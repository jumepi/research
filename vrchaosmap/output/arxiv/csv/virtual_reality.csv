title,summary,authors,published,link,category,keywords
Navigating a maze differently - a user study,"Navigating spaces is an embodied experience. Examples can vary from rescue
workers trying to save people from natural disasters; a tourist finding their
way to the nearest coffee shop, or a gamer solving a maze. Virtual reality
allows these experiences to be simulated in a controlled virtual environment.
However, virtual reality users remain anchored in the real world and the
conventions by which the virtual environment is deployed influence user
performance. There is currently a need to evaluate the degree of influence
imposed by extrinsic factors and virtual reality hardware on its users.
Traditionally, virtual reality experiences have been deployed using
Head-Mounted Displays with powerful computers rendering the graphical content
of the virtual environment; however, user input has been facilitated using an
array of human interface devices including Keyboards, Mice, Trackballs,
Touchscreens, Joysticks, Gamepads, Motion detecting cameras and Webcams. Some
of these HIDs have also been introduced for non-immersive video games and
general computing. Due to this fact, a subset of virtual reality users has
greater familiarity than others in using these HIDs. Virtual reality
experiences that utilize gamepads (controllers) to navigate virtual
environments may introduce a bias towards usability among virtual reality users
previously exposed to video-gaming.
  This article presents an evaluative user study conducted using our ubiquitous
virtual reality framework with general audiences. Among our findings, we reveal
a usability bias among virtual reality users who are predominantly video
gamers. Beyond this, we found a statistical difference in user behavior between
untethered immersive virtual reality experiences compared to untethered
non-immersive virtual reality experiences.","['Aryabrata Basu', 'Kyle Johnsen']",2018-05-23T23:41:42Z,http://arxiv.org/abs/1805.09454v4,['cs.HC'],"Virtual reality,User study,User performance,Human interface devices,Gamepads,Immersive virtual reality,Non-immersive virtual reality,Usability bias,User behavior,Virtual environment"
Immersion Metrics for Virtual Reality,"Technological advances in recent years have promoted the development of
virtual reality systems that have a wide variety of hardware and software
characteristics, providing varying degrees of immersion. Immersion is an
objective property of the virtual reality system that depends on both its
hardware and software characteristics. Virtual reality systems are currently
attempting to improve immersion as much as possible. However, there is no
metric to measure the level of immersion of a virtual reality system based on
its characteristics. To date, the influence of these hardware and software
variables on immersion has only been considered individually or in small
groups. The way these system variables simultaneously affect immersion has not
been analyzed either. In this paper, we propose immersion metrics for virtual
reality systems based on their hardware and software variables, as well as the
development process that led to their formulation. From the conducted
experiment and the obtained data, we followed a methodology to find immersion
models based on the variables of the system. The immersion metrics presented in
this work offer a useful tool in the area of virtual reality and immersive
technologies, not only to measure the immersion of any virtual reality system
but also to analyze the relationship and importance of the variables of these
systems.","['Matias N. Selzer', 'Silvia M. Castro']",2022-06-15T18:21:28Z,http://arxiv.org/abs/2206.07748v1,"['cs.HC', 'cs.GR']","virtual reality,immersion,metrics,hardware,software characteristics,development process,experiment,variables,immersion models,immersive technologies"
"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']",2019-03-07T04:29:50Z,http://arxiv.org/abs/1903.02723v1,['cs.HC'],"physical reality,virtual reality,mixed reality,symmetrical reality,unified framework,augmented reality,inverse virtual reality,observation locations"
The concept of strong and weak virtual reality,"We approach the virtual reality phenomenon by studying its relationship to
set theory, and we investigate the case where this is done using the
wellfoundedness property of sets. Our hypothesis is that non-wellfounded sets
(hypersets) give rise to a different quality of virtual reality than do
familiar wellfounded sets. We initially provide an alternative approach to
virtual reality based on Sommerhoff's idea of first and second order
self-awareness; both categories of self-awareness are considered as necessary
conditions for consciousness in terms of higher cognitive functions. We then
introduce a representation of first and second order self-awareness through
sets, and assume that these sets, which we call events, originally form a
collection of wellfounded sets. Strong virtual reality characterizes virtual
reality environments which have the limited capacity to create only events
associated with wellfounded sets. In contrast, the more general concept of weak
virtual reality characterizes collections of virtual reality mediated events
altogether forming an entirety larger than any collection of wellfounded sets.
By giving reference to Aczel's hyperset theory we indicate that this definition
is not empty, because hypersets encompass wellfounded sets already. Moreover,
we argue that weak virtual reality could be realized in human history through
continued progress in computer technology. Finally, we reformulate our
characterization into a more general framework, and use Baltag's Structural
Theory of Sets (STS) to show that within this general hyperset theory
Sommerhoff's first and second order self-awareness as well as both concepts of
virtual reality admit a consistent mathematical representation.",['A. M. Lisewski'],2003-11-29T14:08:56Z,http://arxiv.org/abs/cs/0312001v3,"['cs.LO', 'nlin.AO', 'physics.comp-ph', 'F.4.1; I.2.4; H.1.2; H.5.1']","virtual reality,set theory,wellfounded sets,hypersets,self-awareness,consciousness,cognitive functions,events,strong virtual reality,weak virtual reality"
Virtual Reality: A Definition History - A Personal Essay,"This essay, written in 1998 by an active participant in both virtual reality
development and the virtual reality definition debate, discusses the definition
of the phrase ""Virtual Reality"" (VR). I start with history from a personal
perspective, concentrating on the debate between the ""Virtual Reality"" and
""Virtual Environment"" labels in the late 1980's and early 1990's. Definitions
of VR based on specific technologies are shown to be unsatisfactory. I propose
the following definition of VR, based on the striking effects of a good VR
system: ""Virtual Reality is the use of computer technology to create the effect
of an interactive three-dimensional world in which the objects have a sense of
spatial presence."" The justification for this definition is discussed in
detail, and is favorably compared with the dictionary definitions of ""virtual""
and ""reality"". The implications of this definition for virtual reality
technology are briefly examined.",['Steve Bryson'],2013-12-16T11:33:06Z,http://arxiv.org/abs/1312.4322v1,['cs.HC'],"Virtual Reality,Definition,History,Debate,Virtual Environment,Computer Technology,Interactive,Three-dimensional,Spatial Presence,Technology"
The virtual reality framework for engineering objects,"A framework for virtual reality of engineering objects has been developed.
This framework may simulate different equipment related to virtual reality.
Framework supports 6D dynamics, ordinary differential equations, finite
formulas, vector and matrix operations. The framework also supports embedding
of external software.","['Petr R. Ivankov', 'Nikolay P. Ivankov']",2006-12-22T19:19:41Z,http://arxiv.org/abs/cs/0612126v1,"['cs.CE', 'cs.MS', 'J.9']","virtual reality,engineering objects,framework,simulation,6D dynamics,ordinary differential equations,finite formulas,vector operations,matrix operations,external software"
DiVRsify: Break the Cycle and Develop VR for Everyone,"Virtual reality technology is biased. It excludes approximately 95% the
world's population by being primarily designed for male, western, educated,
industrial, rich, and democratic populations. This bias may be due to the lack
of diversity in virtual reality researchers, research participants, developers,
and end users, fueling a noninclusive research, development, and usability
cycle. The objective of this paper is to highlight the minimal virtual reality
research involving understudied populations with respect to dimensions of
diversity, such as gender, race, culture, ethnicity, age, disability, and
neurodivergence. Specifically, we highlight numerous differences in virtual
reality usability between underrepresented groups compared to commonly studied
populations. These differences illustrate the lack of generalizability of prior
virtual reality research. Lastly, we present a call to action with the aim
that, over time, will break the cycle and enable virtual reality for everyone.","['Tabitha C. Peck', 'Kyla McMullen', 'John Quarles']",2021-10-01T15:51:59Z,http://arxiv.org/abs/2110.00497v1,['cs.HC'],"virtual reality technology,bias,diversity,research,populations,gender,race,culture,ethnicity,usability"
The Physical World as a Virtual Reality,"This paper explores the idea that the universe is a virtual reality created
by information processing, and relates this strange idea to the findings of
modern physics about the physical world. The virtual reality concept is
familiar to us from online worlds, but our world as a virtual reality is
usually a subject for science fiction rather than science. Yet logically the
world could be an information simulation running on a multi-dimensional
space-time screen. Indeed, if the essence of the universe is information,
matter, charge, energy and movement could be aspects of information, and the
many conservation laws could be a single law of information conservation. If
the universe were a virtual reality, its creation at the big bang would no
longer be paradoxical, as every virtual system must be booted up. It is
suggested that whether the world is an objective reality or a virtual reality
is a matter for science to resolve. Modern information science can suggest how
core physical properties like space, time, light, matter and movement could
derive from information processing. Such an approach could reconcile relativity
and quantum theories, with the former being how information processing creates
space-time, and the latter how it creates energy and matter.",['Brian Whitworth'],2008-01-02T05:04:27Z,http://arxiv.org/abs/0801.0337v2,['cs.OH'],"virtual reality,information processing,modern physics,universe,information simulation,multi-dimensional space-time,conservation laws,big bang,information science,relativity"
"Improved AdaBoost for Virtual Reality Experience Prediction Based on
  Long Short-Term Memory Network","A classification prediction algorithm based on Long Short-Term Memory Network
(LSTM) improved AdaBoost is used to predict virtual reality (VR) user
experience. The dataset is randomly divided into training and test sets in the
ratio of 7:3.During the training process, the model's loss value decreases from
0.65 to 0.31, which shows that the model gradually reduces the discrepancy
between the prediction results and the actual labels, and improves the accuracy
and generalisation ability.The final loss value of 0.31 indicates that the
model fits the training data well, and is able to make predictions and
classifications more accurately. The confusion matrix for the training set
shows a total of 177 correct predictions and 52 incorrect predictions, with an
accuracy of 77%, precision of 88%, recall of 77% and f1 score of 82%. The
confusion matrix for the test set shows a total of 167 correct and 53 incorrect
predictions with 75% accuracy, 87% precision, 57% recall and 69% f1 score. In
summary, the classification prediction algorithm based on LSTM with improved
AdaBoost shows good prediction ability for virtual reality user experience.
This study is of great significance to enhance the application of virtual
reality technology in user experience. By combining LSTM and AdaBoost
algorithms, significant progress has been made in user experience prediction,
which not only improves the accuracy and generalisation ability of the model,
but also provides useful insights for related research in the field of virtual
reality. This approach can help developers better understand user requirements,
optimise virtual reality product design, and enhance user satisfaction,
promoting the wide application of virtual reality technology in various fields.","['Wenhan Fan', 'Zhicheng Ding', 'Ruixin Huang', 'Chang Zhou', 'Xuyang Zhang']",2024-05-17T03:47:30Z,http://arxiv.org/abs/2405.10515v1,['cs.LG'],"AdaBoost,Long Short-Term Memory Network,prediction algorithm,virtual reality,user experience,dataset,training process,confusion matrix,accuracy,precision"
Virtual reality: A human centered tool for improving Manufacturing,"Manufacturing is using Virtual Reality tools to enhance the product life
cycle. Their definitions are still in flux and it is necessary to define their
connections. Thus, firstly, we will introduce more closely some definitions
where we will find that, if the Virtual manufacturing concepts originate from
machining operations and evolve in this manufacturing area, there exist a lot
of applications in different fields such as casting, forging, sheet
metalworking and robotics (mechanisms). From the recent projects in Europe or
in USA, we notice that the human perception or the simulation of mannequin is
more and more needed in both fields. In this context, we have isolated some
applications as ergonomic studies, assembly and maintenance simulation, design
or training where the virtual reality tools can be applied. Thus, we find out a
family of applications where the virtual reality tools give the engineers the
main role in the optimization process. We will illustrate our paper by several
examples where virtual reality interfaces are used and combined with
optimization tools as multi-agent systems.","['Fouad Bennis', 'Damien Chablat', 'Philippe Dépincé']",2007-07-24T14:28:01Z,http://arxiv.org/abs/0707.3563v1,['cs.RO'],"Virtual reality,Manufacturing,Product life cycle,Virtual manufacturing,Machining operations,Casting,Forging,Sheet metalworking,Robotics,Human perception"
Deep Learning Development Environment in Virtual Reality,"Virtual reality (VR) offers immersive visualization and intuitive
interaction. We leverage VR to enable any biomedical professional to deploy a
deep learning (DL) model for image classification. While DL models can be
powerful tools for data analysis, they are also challenging to understand and
develop. To make deep learning more accessible and intuitive, we have built a
virtual reality-based DL development environment. Within our environment, the
user can move tangible objects to construct a neural network only using their
hands. Our software automatically translates these configurations into a
trainable model and then reports its resulting accuracy on a test dataset in
real-time. Furthermore, we have enriched the virtual objects with
visualizations of the model's components such that users can achieve insight
about the DL models that they are developing. With this approach, we bridge the
gap between professionals in different fields of expertise while offering a
novel perspective for model analysis and data interaction. We further suggest
that techniques of development and visualization in deep learning can benefit
by integrating virtual reality.","['Kevin C. VanHorn', 'Meyer Zinn', 'Murat Can Cobanoglu']",2019-06-13T20:53:33Z,http://arxiv.org/abs/1906.05925v1,"['cs.LG', 'cs.HC', 'cs.MM', 'stat.ML']","Deep learning,Virtual reality,Biomedical,Image classification,Neural network,Model development,Visualization,Data interaction,Model analysis,Training model"
"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']",2021-04-17T15:47:48Z,http://arxiv.org/abs/2104.08579v2,['cs.HC'],"Augmented Reality,Virtual Reality,Selective Visualisation,Virtual Environments,Information Filtering"
Security Considerations for Virtual Reality Systems,"There is a growing need for authentication methodology in virtual reality
applications. Current systems assume that the immersive experience technology
is a collection of peripheral devices connected to a personal computer or
mobile device. Hence there is a complete reliance on the computing device with
traditional authentication mechanisms to handle the authentication and
authorization decisions. Using the virtual reality controllers and headset
poses a different set of challenges as it is subject to unauthorized
observation, unannounced to the user given the fact that the headset completely
covers the field of vision in order to provide an immersive experience. As the
need for virtual reality experiences in the commercial world increases, there
is a need to provide other alternative mechanisms for secure authentication. In
this paper, we analyze a few proposed authentication systems and reached a
conclusion that a multidimensional approach to authentication is needed to
address the granular nature of authentication and authorization needs of a
commercial virtual reality applications in the commercial world.","['Karthik Viswanathan', 'Abbas Yazdinejad']",2022-01-07T17:47:09Z,http://arxiv.org/abs/2201.02563v3,"['cs.CR', 'cs.MM']","authentication,virtual reality systems,immersive experience,peripheral devices,computing device,virtual reality controllers,headset,secure authentication,commercial virtual reality applications"
Towards Everyday Virtual Reality through Eye Tracking,"With developments in computer graphics, hardware technology, perception
engineering, and human-computer interaction, virtual reality and virtual
environments are becoming more integrated into our daily lives. Head-mounted
displays, however, are still not used as frequently as other mobile devices
such as smart phones and watches. With increased usage of this technology and
the acclimation of humans to virtual application scenarios, it is possible that
in the near future an everyday virtual reality paradigm will be realized. When
considering the marriage of everyday virtual reality and head-mounted displays,
eye tracking is an emerging technology that helps to assess human behaviors in
a real time and non-intrusive way. Still, multiple aspects need to be
researched before these technologies become widely available in daily life.
Firstly, attention and cognition models in everyday scenarios should be
thoroughly understood. Secondly, as eyes are related to visual biometrics,
privacy preserving methodologies are necessary. Lastly, instead of studies or
applications utilizing limited human participants with relatively homogeneous
characteristics, protocols and use-cases for making such technology more
accessible should be essential. In this work, taking the aforementioned points
into account, a significant scientific push towards everyday virtual reality
has been completed with three main research contributions.",['Efe Bozkir'],2022-03-29T16:09:37Z,http://arxiv.org/abs/2203.15703v1,"['cs.HC', 'cs.AI']","virtual reality,eye tracking,human-computer interaction,head-mounted displays,perception engineering,computer graphics"
"VREd: A Virtual Reality-Based Classroom for Online Education Using
  Unity3D WebGL","Virtual reality is the way of the future. The use of virtual reality is
expanding over time across all sectors, from the entertainment industry to the
military and space. VREd is a similar concept where a virtual reality-based
classroom is used for online education where the user will have better
interaction and more control. Unity3D and WebGL software have been used for
implementation. Students or learners accustomed to contemporary technologies
may find the traditional educational system unappealing because of its flaws.
Incorporating the latest technologies can increase the curiosity and learning
abilities of students. The system architecture of VREd is similar to that of an
actual classroom, allowing both students and teachers to access all of the
course materials and interact with one another using only an internet
connection. The environment and the background are also customizable.
Therefore, all the users can comfortably use the system and feel at home. We
can create an effective educational system that raises educational quality by
utilizing virtual reality.","['Ratun Rahman', 'Md Rafid Islam']",2023-04-20T18:18:47Z,http://arxiv.org/abs/2304.10585v1,['cs.CY'],"Virtual reality,Virtual reality-based classroom,Online education,Unity3D,WebGL,System architecture,Course materials,Interaction,Educational system,Educational quality"
"Virtualization of Classical Reality: Limits and Possibilities in
  Physical Simulation","This study explores the virtualization of classical reality and aims to
establish a clear framework to determine the limits and possibilities of
virtual reality. It addresses two primary questions: whether an observer's
senses can perceive a different reality through appropriate equipment, and
whether it is possible to simulate a reality without the laws of physics. As
virtual and augmented reality are increasingly used in various fields, it is
crucial to provide well-founded responses to these inquiries. Understanding the
limitations and achievability of virtual reality is essential for creating
realistic environments in education, entertainment, and other domains.
Additionally, considering the role of physics and scientific rigor in virtual
contexts is important. The study presents a theoretical framework divided into
three sections: Methods, Results, and Discussion. The Methods section explains
the nature of computers and their ability to create perceived virtual reality.
The Results section introduces the theoretical framework, emphasizing
observable simulation and interactive simulation and highlighting their
distinctions. Finally, the Discussion section builds upon the theoretical
foundation to provide comprehensive insights and answers to the research
questions. This study enhances our understanding of the boundaries and
possibilities of virtual reality, offering concrete answers and valuable
knowledge for the development and application of virtual reality in various
domains.",['Francesco Sisini'],2023-06-12T08:36:13Z,http://arxiv.org/abs/2306.07955v1,['cs.HC'],"virtualization,classical reality,physical simulation,virtual reality,augmented reality,laws of physics,perceived virtual reality,interactive simulation,theoretical framework"
"Application of Artificial Intelligence in Hand Gesture Recognition with
  Virtual Reality: Survey and Analysis of Hand Gesture Hardware Selection","The ongoing usage of artificial intelligence technologies in virtual reality
has led to a large number of researchers exploring immersive virtual reality
interaction. Gesture controllers and head-mounted displays are the primary
pieces of hardware used in virtual reality applications. This article analyzes
the advantages and disadvantages of various hardware tools as well as possible
use cases for hand gesture recognition. Several popular data sets used in data
pre-processing and hand gesture detection systems are also summarized. This
paper's primary objectives are to evaluate the current state of the art in
gesture recognition research and to offer potential routes for future study to
guide researchers working on immersive virtual reality interaction. Comparing
recent study results and taking into account the state of the development of
human-computer interaction technology, a number of potential research
directions are highlighted, including the following: input data noise should be
filtered, a lightweight network should be designed, tactile feedback should be
created, and eye and gesture data should be combined.",['Jindi Wang'],2024-05-25T15:05:49Z,http://arxiv.org/abs/2405.16264v1,['cs.HC'],"Artificial Intelligence,Hand Gesture Recognition,Virtual Reality,Hardware Selection,Gesture Controllers,Head-Mounted Displays,Data Sets,Data Pre-processing,Hand Gesture Detection"
"Preprint Virtual Reality Assistant Technology for Learning Primary
  Geography","This is the preprint version of our paper on ICWL2015. A virtual reality
based enhanced technology for learning primary geography is proposed, which
synthesizes several latest information technologies including virtual
reality(VR), 3D geographical information system(GIS), 3D visualization and
multimodal human-computer-interaction (HCI). The main functions of the proposed
system are introduced, i.e. Buffer analysis, Overlay analysis, Space convex
hull calculation, Space convex decomposition, 3D topology analysis and 3D space
intersection detection. The multimodal technologies are employed in the system
to enhance the immersive perception of the users.","['Zhihan Lv', 'Xiaoming Li']",2015-09-01T07:03:35Z,http://arxiv.org/abs/1509.00159v2,['cs.HC'],"virtual reality,primary geography,3D geographical information system,HCI,buffer analysis,overlay analysis,space convex hull calculation,3D topology analysis,3D space intersection detection"
"Where's My Drink? Enabling Peripheral Real World Interactions While
  Using HMDs","Head Mounted Displays (HMDs) allow users to experience virtual reality with a
great level of immersion. However, even simple physical tasks like drinking a
beverage can be difficult and awkward while in a virtual reality experience. We
explore mixed reality renderings that selectively incorporate the physical
world into the virtual world for interactions with physical objects. We
conducted a user study comparing four rendering techniques that balances
immersion in a virtual world with ease of interaction with the physical world.
Finally, we discuss the pros and cons of each approach, suggesting guidelines
for future rendering techniques that bring physical objects into virtual
reality.","['Pulkit Budhiraja', 'Rajinder Sodhi', 'Brett Jones', 'Kevin Karsch', 'Brian Bailey', 'David Forsyth']",2015-02-16T22:50:03Z,http://arxiv.org/abs/1502.04744v1,['cs.HC'],"HMDs,virtual reality,immersion,mixed reality,renderings,user study,interaction,physical objects,guidelines"
"The Office of the Future: Virtual, Portable and Global","Virtual Reality has the potential to change the way we work. We envision the
future office worker to be able to work productively everywhere solely using
portable standard input devices and immersive head-mounted displays. Virtual
Reality has the potential to enable this, by allowing users to create working
environments of their choice and by relieving them from physical world
limitations such as constrained space or noisy environments. In this article,
we investigate opportunities and challenges for realizing this vision and
discuss implications from recent findings of text entry in virtual reality as a
core office task.","['Jens Grubert', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson']",2018-12-05T19:35:40Z,http://arxiv.org/abs/1812.02197v1,['cs.HC'],"Virtual Reality,Portable devices,Global work,Immersive displays,Working environments,Text entry,Office tasks,Productivity"
Low-cost VR Collaborative System equipped with Haptic Feedback,"In this paper, we present a low-cost virtual reality (VR) collaborative
system equipped with a haptic feedback sensation system. This system is
composed of a Kinect sensor for bodies and gestures detection, a
microcontroller and vibrators to simulate outside interactions, and smartphone
powered cardboard, all of this are put into a network implemented with Unity 3D
game engine. CCS CONCEPTS $\bullet$ Interaction paradigms $\rightarrow$ Virtual
reality; Collaborative interaction; $\bullet$ Hardware $\rightarrow$ Sensors
and actuators; Wireless devices; KEYWORDS collaborative virtual reality, haptic
feedback system.","['Samir Benbelkacem', 'Abdelkader Bellarbi', 'Nadia Zenati-Henda', 'Ahmed Bentaleb', 'Ahmed Bellabaci', 'Samir Otmane']",2019-03-01T07:36:19Z,http://arxiv.org/abs/1903.01219v1,['cs.HC'],"virtual reality,collaborative system,haptic feedback,Kinect sensor,microcontroller,vibrators,Unity 3D,sensors,actuators,wireless devices"
Compact 3 DOF Driving Simulator using Immersive Virtual Reality,"A driving simulator was created using commercially available 3 degree of
freedom motion platform (DOFreality H3) and a virtual reality head-mounted
display (Oculus CV1). Using virtual reality headset as the visual simulation
system with low-cost moving base platform allowed us to create a high-fidelity
driving simulator with minimal cost and space. A custom motion cueing algorithm
was used to minimize visuo-vestibular conflict, and simulator sickness
questionnaire (SSQ) was used to measure progression of simulator sickness over
time while driving on a highway environment.","['Jungsu Pak', 'Uri Maoz']",2019-09-12T17:39:50Z,http://arxiv.org/abs/1909.05833v1,['cs.HC'],"3 DOF,driving simulator,immersive virtual reality,motion platform,virtual reality headset,simulation system,motion cueing algorithm,visuo-vestibular conflict,simulator sickness,highway environment"
Rethinking Immersive Virtual Reality and Empathy,"In this position paper, we aim to spark more discussions surrounding the use
of empathy as the intended outcome of many studies on immersive virtual reality
experiences. As a construct, empathy has many significant flaws that may lead
to unintended and negative outcomes, going against our original goal of
employing these technologies for the betterment of society. We highlight the
possible advantages of designing for rational compassion instead, and propose
alternative research directions and outcome measurements for immersive virtual
reality that urgently warrant our attention.","['Ken Jen Lee', 'Edith Law']",2021-09-09T03:52:04Z,http://arxiv.org/abs/2109.04023v1,"['cs.HC', 'cs.MM', 'H.5.1; J.4; K.4.0']","immersive virtual reality,empathy,construct,negative outcomes,rational compassion,research directions,outcome measurements"
"A virtual reality-based method for examining audiovisual prosody
  perception","Prosody plays a vital role in verbal communication. Acoustic cues of prosody
have been examined extensively. However, prosodic characteristics are not only
perceived auditorily, but also visually based on head and facial movements. The
purpose of this report is to present a method for examining audiovisual prosody
using virtual reality. We show that animations based on a virtual human provide
motion cues similar to those obtained from video recordings of a real talker.
The use of virtual reality opens up new avenues for examining multimodal
effects of verbal communication. We discuss the method in the framework of
examining prosody perception in cochlear implant listeners.","['Hartmut Meister', 'Isa Samira Winter', 'Moritz Waeachtler', 'Pascale Sandmann', 'Khaled Abdellatif']",2022-09-13T06:10:08Z,http://arxiv.org/abs/2209.05745v1,['cs.CL'],"audiovisual prosody,perception,virtual reality,acoustic cues,prosodic characteristics,virtual human,motion cues,video recordings,verbal communication,cochlear implant listeners"
Teleoperated Robot Grasping in Virtual Reality Spaces,"Despite recent advancement in virtual reality technology, teleoperating a
high DoF robot to complete dexterous tasks in cluttered scenes remains
difficult. In this work, we propose a system that allows the user to
teleoperate a Fetch robot to perform grasping in an easy and intuitive way,
through exploiting the rich environment information provided by the virtual
reality space. Our system has the benefit of easy transferability to different
robots and different tasks, and can be used without any expert knowledge. We
tested the system on a real fetch robot, and a video demonstrating the
effectiveness of our system can be seen at https://youtu.be/1-xW2Bx_Cms.","['Jiaheng Hu', 'David Watkins', 'Peter Allen']",2023-01-30T17:07:52Z,http://arxiv.org/abs/2301.13064v1,['cs.RO'],"teleoperated robot,virtual reality,grasping,DoF,cluttered scenes,system,Fetch robot,environment information,transferability,teleoperation"
"The ImmersaDesk3 -- Experiences With A Flat Panel Display for Virtual
  Reality","In this paper we discuss the design and implementation of a plasma display
panel for a wide field of view desktop virtual reality environment. Present
commercial plasma displays are not designed with virtual reality in mind,
leading to several problems in generating stereo imagery and obtaining good
tracking information. Although we developed solutions for a number of these
problems, the limitations of the system preclude its current use in practical
applications, and point to issues that must be resolved for flat panel displays
to be useful for VR.","['Dave Pape', 'Josephine Anstey', 'Mike Bogucki', 'Greg Dawe', 'Tom DeFanti', 'Andy Johnson', 'Dan Sandin']",2023-11-17T04:03:53Z,http://arxiv.org/abs/2311.12037v1,['cs.HC'],"plasma display panel,virtual reality,flat panel display,stereo imagery,tracking information,limitations,practical applications,flat panel displays"
HTC Vive MeVisLab integration via OpenVR for medical applications,"Virtual Reality, an immersive technology that replicates an environment via
computer-simulated reality, gets a lot of attention in the entertainment
industry. However, VR has also great potential in other areas, like the medical
domain, Examples are intervention planning, training and simulation. This is
especially of use in medical operations, where an aesthetic outcome is
important, like for facial surgeries. Alas, importing medical data into Virtual
Reality devices is not necessarily trivial, in particular, when a direct
connection to a proprietary application is desired. Moreover, most researcher
do not build their medical applications from scratch, but rather leverage
platforms like MeVisLab, MITK, OsiriX or 3D Slicer. These platforms have in
common that they use libraries like ITK and VTK, and provide a convenient
graphical interface. However, ITK and VTK do not support Virtual Reality
directly. In this study, the usage of a Virtual Reality device for medical data
under the MeVisLab platform is presented. The OpenVR library is integrated into
the MeVisLab platform, allowing a direct and uncomplicated usage of the head
mounted display HTC Vive inside the MeVisLab platform. Medical data coming from
other MeVisLab modules can directly be connected per drag-and-drop to the
Virtual Reality module, rendering the data inside the HTC Vive for immersive
virtual reality inspection.","['Jan Egger', 'Markus Gall', 'Jürgen Wallner', 'Pedro Boechat', 'Alexander Hann', 'Xing Li', 'Xiaojun Chen', 'Dieter Schmalstieg']",2017-03-22T09:21:00Z,http://arxiv.org/abs/1703.07575v1,"['cs.GR', 'cs.SE']","HTC Vive,MeVisLab,OpenVR,Virtual Reality,medical applications,ITK,VTK,3D Slicer,MITK,immersive technology"
"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']",2022-01-13T16:54:36Z,http://arxiv.org/abs/2201.07003v1,['cs.CY'],"augmented reality,virtual reality,blended learning,general secondary education,mixed reality,educational process,methodology,teaching methods,teaching aids,organizational forms"
Visualization in virtual reality: a systematic review,"Rapidly growing virtual reality (VR) technologies and techniques have gained
importance over the past few years, and academics and practitioners have been
searching for efficient visualizations in VR. To date, emphasis has been on the
employment of game technologies. Despite the growing interest and discussion,
visualization studies have lacked a common baseline in the transition period of
2D visualizations to immersive ones. To this end, the presented study aims to
provide a systematic literature review that explains the state-of-the-art
research and future trends on visualization in virtual reality. The research
framework is grounded in empirical and theoretical works of visualization. We
characterize the reviewed literature based on three dimensions: (a) Connection
with visualization background and theory, (b) Evaluation and design
considerations for virtual reality visualization, and (c) Empirical studies.
The results from this systematic review suggest that: (1) There are only a few
studies that focus on creating standard guidelines for virtual reality, and
each study individually provides a framework or employs previous studies on
traditional 2D visualizations; (2) With the myriad of advantages provided for
visualization and virtual reality, most of the studies prefer to use game
engines; (3) Although game engines are extensively used, they are not
convenient for critical scientific studies; and (4) 3D versions of traditional
statistical visualization techniques, such as bar plots and scatter plots, are
still commonly used in the data visualization context. This systematic review
attempts to add to the literature a clear picture of the emerging contexts,
different elements, and their interdependencies.","['Elif Hilal Korkut', 'Elif Surer']",2022-03-15T03:08:39Z,http://arxiv.org/abs/2203.07616v1,['cs.HC'],"Visualization,Virtual reality,Systematic review,Game technologies,Immersive,2D visualizations,Empirical studies,Game engines,Data visualization,Statistical visualization"
"Immersed in Reality Secured by Design -- A Comprehensive Analysis of
  Security Measures in AR/VR Environments","Virtual reality and related technologies such as mixed and augmented reality
have received extensive coverage in both mainstream and fringe media outlets.
When the subject goes to a new AR headset, another AR device, or AR glasses,
the talk swiftly shifts to the technical and design details. Unfortunately, no
one seemed to care about security. Data theft and other forms of cyberattack
pose serious threats to virtual reality systems. Virtual reality goggles are
just specialist versions of computers or Internet of Things devices, whereas
virtual reality experiences are software packages. As a result, AR systems are
just as vulnerable as any other Internet of Things (IoT) device we use on a
daily basis, such as computers, tablets, and phones. Preventing and responding
to common cybersecurity threats and assaults is crucial. Cybercriminals can
exploit virtual reality headsets just like any other computer system. This
paper analysis the data breach induced by these assaults could result in a
variety of concerns, including but not limited to identity theft, the
unauthorized acquisition of personal information or network credentials, damage
to hardware and software, and so on. Augmented reality (AR) allows for
real-time monitoring and visualization of network activity, system logs, and
security alerts. This allows security professionals to immediately identify
threats, monitor suspicious activities, and fix any issues that develop. This
data can be displayed in an aesthetically pleasing and intuitively structured
format using augmented reality interfaces, enabling for faster analysis and
decision-making.","['Sameer Chauhan', 'Luv Sachdeva']",2024-01-30T21:24:52Z,http://arxiv.org/abs/2404.16839v1,"['cs.CR', 'cs.IT', 'math.IT']","immersed reality,security measures,AR/VR environments,data theft,cyberattack,cybersecurity threats,virtual reality systems,cybercriminals,augmented reality,network activity"
Magic Fairy Tales as Source for Interface Metaphors,"The work is devoted to a problem of search of metaphors for interactive
systems and systems based on Virtual Reality (VR) environments. The analysis of
magic fairy tales as a source of metaphors for interface and virtual reality is
offered. Some results of design process based on magic metaphors are
considered.",['Vladimir L. Averbukh'],2008-11-12T20:31:34Z,http://arxiv.org/abs/0811.1974v1,"['cs.HC', 'H.1.m']","interface metaphors,virtual reality,magic fairy tales,interactive systems,design process"
Virtual Reality,"This paper is focused on the presentation of Virtual Reality principles
together with the main implementation methods and techniques. An overview of
the main development directions is included.","['Dan L. Lacrama', 'Dorina Fera']",2009-03-25T12:16:29Z,http://arxiv.org/abs/0903.4314v1,['cs.MM'],"Virtual Reality,principles,implementation methods,techniques,development directions"
Non-euclidean virtual reality I: explorations of $\mathbb{H}^3$,"We describe our initial explorations in simulating non-euclidean geometries
in virtual reality. Our simulations of three-dimensional hyperbolic space are
available at http://h3.hypernom.com.","['Vi Hart', 'Andrea Hawksley', 'Elisabetta A. Matsumoto', 'Henry Segerman']",2017-02-13T22:48:03Z,http://arxiv.org/abs/1702.04004v1,"['math.HO', 'math.GT', '00A66, 51M10, 53A35, 51M20']","non-euclidean geometries,virtual reality,hyperbolic space,simulation"
"Non-euclidean virtual reality II: explorations of
  $\mathbb{H}^2\times\mathbb{E}$","We describe our initial explorations in simulating non-euclidean geometries
in virtual reality. Our simulation of the product of two-dimensional hyperbolic
space with one-dimensional euclidean space is available at
http://h2xe.hypernom.com.","['Vi Hart', 'Andrea Hawksley', 'Elisabetta A. Matsumoto', 'Henry Segerman']",2017-02-16T05:40:39Z,http://arxiv.org/abs/1702.04862v1,"['math.HO', 'math.GT', 'math.MG', '00A66, 51M10, 53A35, 51M20']","non-euclidean geometries,virtual reality,hyperbolic space,euclidean space,simulation"
A brief chronology of Virtual Reality,"In this article, we are going to review a brief history of the field of
Virtual Reality (VR), VR systems, and applications and discuss how they
evolved. After that, we will familiarize ourselves with the essential
components of VR experiences and common VR terminology. Finally, we discuss the
evolution of ubiquitous VR as a subfield of VR and its current trends.",['Aryabrata Basu'],2019-11-21T17:01:14Z,http://arxiv.org/abs/1911.09605v2,['cs.HC'],"Virtual Reality,VR systems,applications,components,terminology,ubiquitous VR,evolution,trends"
Non-Euclidean Virtual Reality IV: Sol,"This article presents virtual reality software designed to explore the Sol
geometry. The simulation is available on 3-dimensional.space/sol.html","['Rémi Coulon', 'Elisabetta A. Matsumoto', 'Henry Segerman', 'Steve Trettel']",2020-02-02T11:32:30Z,http://arxiv.org/abs/2002.00369v1,"['math.HO', 'cs.GR', 'math.GT', 'math.MG', '00A09, 00A66, 53A35, 57K35, 51-04, 68U05, 37D40']","virtual reality,software,Sol geometry,simulation,3-dimensional space"
"3DMAP-VR, a project to visualize 3-dimensional models of astrophysical
  phenomena in virtual reality","In this research note, we present 3DMAP-VR,(3-Dimensional Modeling of
Astrophysical Phenomena in Virtual Reality), a project aimed at visualizing 3D
MHD models of astrophysical simulations, using virtual reality sets of
equipment. The models account for all the relevant physical processes in
astrophysical phenomena: gravity, magnetic-field-oriented thermal conduction,
energy losses due to radiation, gas viscosity, deviations from proton-electron
temperature equilibration, deviations from the ionization equilibrium, cosmic
rays acceleration, etc.. We realized an excellent synergy between our 3DMAP-VR
project and Sketchfab (one of the largest open access platforms to publish and
share 3D virtual reality and augmented reality content) to promote a wide
dissemination of results for both scientific and public outreach purposes.","['Salvatore Orlando', 'Ignazio Pillitteri', 'Fabrizio Bocchino', 'Laura Daricello', 'Laura Leonardi']",2019-12-01T07:38:11Z,http://arxiv.org/abs/1912.02649v1,['astro-ph.IM'],"3DMAP-VR,astrophysical phenomena,virtual reality,3D MHD models,simulations,gravity,thermal conduction,radiation,gas viscosity,cosmic rays"
Interactive Narrative in Virtual Reality,"Interactive fiction is a literary genre that is rapidly gaining popularity.
In this genre, readers are able to explicitly take actions in order to guide
the course of the story. With the recent popularity of narrative focused games,
we propose to design and develop an interactive narrative tool for content
creators. In this extended abstract, we show how we leverage this interactive
medium to present a tool for interactive storytelling in virtual reality. Using
a simple markup language, content creators and researchers are now able to
create interactive narratives in a virtual reality environment. We further
discuss the potential future directions for a virtual reality storytelling
engine.","['Gilad Ostrin', 'Jérémy Frey', 'Jessica Cauchard']",2019-01-08T08:10:58Z,http://arxiv.org/abs/1901.02198v1,['cs.HC'],"Interactive fiction,literary genre,interactive narrative,content creators,virtual reality,markup language,interactive storytelling,narrative engine,virtual environment,content creation"
"Above Surface Interaction for Multiscale Navigation in Mobile Virtual
  Reality","Virtual Reality enables the exploration of large information spaces. In
physically constrained spaces such as airplanes or buses, controller-based or
mid-air interaction in mobile Virtual Reality can be challenging. Instead, the
input space on and above touch-screen enabled devices such as smartphones or
tablets could be employed for Virtual Reality interaction in those spaces.
  In this context, we compared an above surface interaction technique with
traditional 2D on-surface input for navigating large planar information spaces
such as maps in a controlled user study (n = 20). We find that our proposed
above surface interaction technique results in significantly better performance
and user preference compared to pinch-to-zoom and drag-to-pan when navigating
planar information spaces.","['Tim Menzner', 'Travis Gesslein', 'Alexander Otte', 'Jens Grubert']",2020-02-07T22:38:47Z,http://arxiv.org/abs/2002.03037v1,['cs.HC'],"Mobile Virtual Reality,Above Surface Interaction,Multiscale Navigation,Controller-based Interaction,Touch-screen Devices,Virtual Reality Interaction,User Study,Pinch-to-zoom,Drag-to-pan,Information Spaces"
Affective visualization in Virtual Reality: An integrative review,"A cluster of research in Affective Computing suggests that it is possible to
infer some characteristics of users' affective states by analyzing their
electrophysiological activity in real-time. However, it is not clear how to use
the information extracted from electrophysiological signals to create visual
representations of the affective states of Virtual Reality (VR) users.
Visualization of users' affective states in VR can lead to biofeedback
therapies for mental health care. Understanding how to visualize affective
states in VR requires an interdisciplinary approach that integrates psychology,
electrophysiology, and audio-visual design. Therefore, this review aims to
integrate previous studies from these fields to understand how to develop
virtual environments that can automatically create visual representations of
users' affective states. The manuscript addresses this challenge in four
sections: First, theories related to emotion and affect are summarized. Second,
evidence suggesting that visual and sound cues tend to be associated with
affective states are discussed. Third, some of the available methods for
assessing affect are described. The fourth and final section contains five
practical considerations for the development of virtual reality environments
for affect visualization.","['Andres Pinilla', 'Jaime Garcia', 'William Raffe', 'Jan-Niklas Voigt-Antons', 'Robert Spang', 'Sebastian Möller']",2020-12-16T10:42:40Z,http://arxiv.org/abs/2012.08849v2,['cs.HC'],"Affective Computing,Electrophysiological activity,Virtual Reality,Visualization,Biofeedback therapies,Mental health care,Psychology,Audio-visual design,Emotion,Affect"
"Virtual Reality: A Survey of Enabling Technologies and its Applications
  in IoT","Virtual Reality (VR) has shown great potential to revolutionize the market by
providing users immersive experiences with freedom of movement. Compared to
traditional video streaming, VR is with ultra high-definition and dynamically
changes with users' head and eye movements, which poses significant challenges
for the realization of such potential. In this paper, we provide a detailed and
systematic survey of enabling technologies of virtual reality and its
applications in Internet of Things (IoT). We identify major challenges of
virtual reality on system design, view prediction, computation, streaming, and
quality of experience evaluation. We discuss each of them by extensively
surveying and reviewing related papers in the recent years. We also introduce
several use cases of VR for IoT. Last, issues and future research directions
are also identified and discussed.","['Miao Hu', 'Xianzhuo Luo', 'Jiawen Chen', 'Young Choon Lee', 'Yipeng Zhou', 'Di Wu']",2021-03-11T05:39:10Z,http://arxiv.org/abs/2103.06472v1,['cs.NI'],"Virtual Reality,Enabling Technologies,Applications,IoT,Immersive Experiences,Video Streaming,Head and Eye Movements,System Design,View Prediction,Computation,Streaming,Quality of Experience Evaluation,Use Cases,Future Research Directions"
"Hand tracking for immersive virtual reality: opportunities and
  challenges","Hand tracking has become an integral feature of recent generations of
immersive virtual reality head-mounted displays. With the widespread adoption
of this feature, hardware engineers and software developers are faced with an
exciting array of opportunities and a number of challenges, mostly in relation
to the human user. In this article, I outline what I see as the main
possibilities for hand tracking to add value to immersive virtual reality as
well as some of the potential challenges in the context of the psychology and
neuroscience of the human user. It is hoped that this paper serves as a roadmap
for the development of best practices in the field for the development of
subsequent generations of hand tracking and virtual reality technologies.",['Gavin Buckingham'],2021-03-27T09:28:47Z,http://arxiv.org/abs/2103.14853v1,"['cs.HC', 'cs.CY', 'cs.MM']","hand tracking,immersive virtual reality,opportunities,challenges,hardware engineers,software developers,human user,psychology,neuroscience,best practices"
Deep Connection: Making Virtual Reality Artworks with Medical Scan Data,"Deep Connection is an installation and virtual reality artwork made using
full body 3D and 4D magnetic resonance scan datasets. When the user enters Deep
Connection, they see a scanned body lying prone in mid-air. The user can walk
around the body and inspect it. The user can dive inside and see its inner
workings, its lungs, spine, brain. The user can take hold of the figure's
outstretched hand: holding the hand triggers the 4D dataset, making the heart
beat and the lungs breathe. When the user lets go of the hand, the heart stops
beating and the lungs stop breathing. Deep Connection creates a scenario where
an embodied human becomes the companion for a virtual body. This paper maps the
conceptual and theoretical framework for Deep Connection such as virtual
intimacy and digital mediated companionship. It also reflects on working with
scanned bodies more generally in virtual reality by discussing transparency,
the cyberbody versus the data body, as well as data privacy and data ethics.
The paper also explains the technical and procedural aspects of the Deep
Connection project with respect to acquiring scan data for the creation of
virtual reality artworks.","['Marilene Oliver', 'Gary James Joynes', 'Kumar Punithakumar', 'Peter Seres']",2021-10-01T00:32:07Z,http://arxiv.org/abs/2110.00138v1,"['cs.MM', 'cs.HC']","virtual reality,artwork,medical scan data,3D,4D,magnetic resonance,dataset,cyberbody,data privacy,data ethics."
"Omnidirectional MediA Format (OMAF): Toolbox for Virtual Reality
  Services","This paper provides an overview of the Omnidirectional Media Format (OMAF)
standard, second edition, which has been recently finalized. OMAF specifies the
media format for coding, storage, delivery, and rendering of omnidirectional
media, including video, audio, images, and timed text. Additionally, OMAF
supports multiple viewpoints corresponding to omnidirectional cameras and
overlay images or video rendered over the omnidirectional background image or
video. Many examples of usage scenarios for multiple viewpoints and overlays
are described in the paper. OMAF provides a toolbox of features, which can be
selectively used in virtual reality services. Consequently, the paper presents
the interoperability points specified in the OMAF standard, which enable
signaling which OMAF features are in use or required to be supported in
implementations. Finally, the paper summarizes which OMAF interoperability
points have been taken into use in virtual reality service specifications by
the 3rd Generation Partnership Project (3GPP) and the Virtual Reality Industry
Forum (VRIF).","['Sachin Deshpande', 'Miska M. Hannuksela']",2022-03-02T15:34:11Z,http://arxiv.org/abs/2203.01183v1,"['eess.IV', 'cs.GR', 'cs.HC', 'cs.MM']","Omnidirectional Media Format,OMAF,media format,coding,storage,delivery,rendering,omnidirectional media,video,audio,images,timed text,viewpoints,cameras,overlay,virtual reality,interoperability points"
A Systematic Review on Interactive Virtual Reality Laboratory,"Virtual Reality has become a significant element of education throughout the
years. To understand the quality and advantages of these techniques, it is
important to understand how they were developed and evaluated. Since COVID-19,
the education system has drastically changed a lot. It has shifted from being
in a classroom with a whiteboard and projectors to having your own room in
front of your laptop in a virtual meeting. In this respect, virtual reality in
the laboratory or Virtual Laboratory is the main focus of this research, which
is intended to comprehend the work done in quality education from a distance
using VR. As per the findings of the study, adopting virtual reality in
education can help students learn more effectively and also help them increase
perspective, enthusiasm, and knowledge of complex notions by offering them an
interactive experience in which they can engage and learn more effectively.
This highlights the importance of a significant expansion of VR use in
learning, the majority of which employ scientific comparison approaches to
compare students who use VR to those who use the traditional method for
learning.","['Fozlur Rahman', 'Marium Sana Mim', 'Feekra Baset Baishakhi', 'Mahmudul Hasan', 'Md. Kishor Morol']",2022-03-26T07:16:01Z,http://arxiv.org/abs/2203.15783v1,"['cs.HC', 'cs.AI']","Virtual Reality,Laboratory,Education,Interactive,Distance learning,Perspective,Enthusiasm,Knowledge,Comparison approaches,Traditional method"
"An Explore of Virtual Reality for Awareness of the Climate Change
  Crisis: A Simulation of Sea Level Rise","Virtual Reality (VR) technology has been shown to achieve remarkable results
in multiple fields. Due to the nature of the immersive medium of Virtual
Reality it logically follows that it can be used as a high-quality educational
tool as it offers potentially a higher bandwidth than other mediums such as
text, pictures and videos. This short paper illustrates the development of a
climate change educational awareness application for virtual reality to
simulate virtual scenes of local scenery and sea level rising until 2100 using
prediction data. The paper also reports on the current in progress work of
porting the system to Augmented Reality (AR) and future work to evaluate the
system.","['Zixiang Xu', 'Abraham G. Campbell', 'Soumyabrata Dev', 'Yuan Liang']",2022-05-03T16:02:31Z,http://arxiv.org/abs/2205.01583v1,"['cs.MM', 'cs.HC']","Virtual Reality,Climate Change,Crisis,Simulation,Sea Level Rise,Educational Tool,Immersive Medium,Augmented Reality,Prediction Data,Awareness Application"
Corvo: Visualizing CellxGene Single-Cell Datasets in Virtual Reality,"The CellxGene project has enabled access to single-cell data in the
scientific community, providing tools for browsed-based no-code analysis of
more than 500 annotated datasets. However, single-cell data requires
dimensional reduction to visualize, and 2D embedding does not take full
advantage of three-dimensional human spatial understanding and cognition.
Compared to a 2D visualization that could potentially hide gene expression
patterns, 3D Virtual Reality may enable researchers to make better use of the
information contained within the datasets. For this purpose, we present
\emph{Corvo}, a fully free and open-source software tool that takes the
visualization and analysis of CellxGene single-cell datasets to 3D Virtual
Reality. Similar to CellxGene, Corvo takes a no-code approach for the end user,
but also offers multimodal user input to facilitate fast navigation and
analysis, and is interoperable with the existing Python data science ecosystem.
In this paper, we explain the design goals of Corvo, detail its approach to the
Virtual Reality visualization and analysis of single-cell data, and briefly
discuss limitations and future extensions.","['Luke Hyman', 'Ivo F. Sbalzarini', 'Stephen Quake', 'Ulrik Günther']",2022-12-01T14:13:21Z,http://arxiv.org/abs/2212.00519v1,['cs.HC'],"CellxGene,single-cell data,dimensional reduction,2D embedding,3D Virtual Reality,gene expression patterns,spatial understanding,cognition,multimodal user input,Python data science ecosystem"
Inclusion in Virtual Reality Technology: A Scoping Review,"Despite the significant growth in virtual reality applications and research,
the notion of inclusion in virtual reality is not well studied. Inclusion
refers to the active involvement of different groups of people in the adoption,
use, design, and development of VR technology and applications. In this review,
we provide a scoping analysis of existing virtual reality research literature
about inclusion. We categorize the literature based on target group into
ability, gender, and age, followed by those that study community-based design
of VR experiences. In the latter group, we focus mainly on Indigenous Peoples
as a clearer and more important example. We also briefly review the approaches
to model and consider the role of users in technology adoption and design as a
background for inclusion studies. We identify a series of generic barriers and
research gaps and some specific ones for each group, resulting in suggested
directions for future research.","['Xiaofeng Yong', 'Ali Arya']",2023-10-23T18:55:48Z,http://arxiv.org/abs/2310.15289v1,"['cs.HC', 'cs.CY']","virtual reality technology,inclusion,scoping review,ability,gender,age,community-based design,Indigenous Peoples,technology adoption,research gaps"
Data Integration Framework for Virtual Reality Enabled Digital Twins,"Digital twins are becoming increasingly popular across many industries for
real-time data streaming, processing, and visualization. They allow
stakeholders to monitor, diagnose, and optimize assets. Emerging technologies
used for immersive visualization, such as virtual reality, open many new
possibilities for intuitive access and monitoring of remote assets through
digital twins. This is specifically relevant for floating wind farms, where
access is often limited. However, the integration of data from multiple sources
and access through different devices including virtual reality headsets can be
challenging. In this work, a data integration framework for static and
real-time data from various sources on the assets and their environment is
presented that allows collecting and processing of data in Python and deploying
the data in real-time through Unity on different devices, including virtual
reality headsets. The integration of data from terrain, weather, and asset
geometry is explained in detail. A real-time data stream from the asset to the
clients is implemented and reviewed, and instructions are given on the code
required to connect Python scripts to any Unity application across devices. The
data integration framework is implemented for a digital twin of a floating wind
turbine and an onshore wind farm, and the potential for future research is
discussed.","['Florian Stadtmann', 'Hary Pirajan Mahalingam', 'Adil Rasheed']",2024-01-04T10:54:55Z,http://arxiv.org/abs/2401.02193v1,['eess.SP'],"data integration,virtual reality,digital twins,real-time data,Python,Unity,data stream,immersive visualization,wind farms,assets"
360 virtual reality travel media for elderly,"The objectives of this qualitative research were to study the model of
360-degree virtual reality travel media, to compare appropriateness of moving
360-degree virtual reality travel media for elderly with both still and moving
cameras, and to study satisfaction of elderly in 360-degree virtual reality
travel media. The informants are 10 elders with age above and equal to 60 years
old who live in Bangkok regardless of genders. Data were collected through
documents, detailed interview, and non-participant observation of elders to
360-degree virtual reality travel media with data triangulation. 1. From the
literature review 1. The creation must primarily consider the target consumers
on their physics 2. must have fluidity on changing the view of the camera by
calibrating with the target consumers 3. The image displayed must not move too
fast to prevent dizziness and improve the comfort of the target consumers. It
is also highly recommended to implement a function to customize the movement
rate for the customer. 2. From the in-depth interview with the target
consumers, the results found that 1. They are worried and not used to the
equipment 2. They have no idea where to look 3. They feel excited 5. They are
interested in what is more to see 6. They feel like they did actually travel
there 7. They can hear the sound clearly 8. They do not like when the camera is
moving and find still camera more comfortable. 3. From the non-participant
observation and found that they are always excited, laughed, and smiled when
watching the media. They always asked where this is and why they cannot see
anything when turning around.",['Donlaporn Srifar'],2018-07-24T12:51:25Z,http://arxiv.org/abs/1807.09074v1,"['cs.HC', 'cs.MM', 'H.5.1']","360-degree virtual reality,elderly,travel media,still camera,moving camera,satisfaction,customization,dizziness,comfort,target consumers"
"User Experience of Reading in Virtual Reality -- Finding Values for Text
  Distance, Size and Contrast","Virtual Reality (VR) has an increasing impact on the market in many fields,
from education and medicine to engineering and entertainment, by creating
different applications that replicate or in the case of augmentation enhance
real-life scenarios. Intending to present realistic environments, VR
applications are including text that we are surrounded by every day. However,
text can only add value to the virtual environment if it is designed and
created in such a way that users can comfortably read it. With the aim to
explore what values for text parameters users find comfortable while reading in
virtual reality, a study was conducted allowing participants to manipulate text
parameters such as font size, distance, and contrast. Therefore two different
standalone virtual reality devices were used, Oculus Go and Quest, together
with three different text samples: Short (2 words), medium (21 words), and long
(51 words). Participants had the task of setting text parameters to the best
and worst possible value. Additionally, participants were asked to rate their
experience of reading in virtual reality. Results report mean values for
angular size (the combination of distance and font size) and color contrast
depending on the different device used as well as the varying text length, for
both tasks. Significant differences were found for values of angular size,
depending on the length of the displayed text. However, different device types
had no significant influence on text parameters but on the experiences reported
using the self-assessment manikin (SAM) scale.","['Tanja KojiāE, 'Danish Ali', 'Robert Greinacher', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2020-04-03T13:14:42Z,http://arxiv.org/abs/2004.01545v1,"['cs.MM', 'cs.HC']","user experience,virtual reality,text parameters,font size,distance,contrast,Oculus Go,Oculus Quest,angular size,color contrast"
"Bali Temple VR: The Virtual Reality based Application for the
  Digitalization of Balinese Temples","The aim of this project is the development of Virtual Reality Application in
order to document one kind of Balinese cultural heritages which are Temples.
The Bali Temple VR application will allow users to do the virtual tour and
experience the landscape of the temples and all objects inside the temples. The
application gives on-site tour guide using virtual reality that allow users
experience the visualization of the Balinese culture heritages in this case are
temples. The users can walk through the temples and can see the 3D objects of
temples and also there is narration of every object inside the temples with
background musics. Right now, the project has completed two temples for virtual
reality tour guide application. Those temples are Melanting Temples and Pulaki
Temples. Based on the test results of its functional requirements, this virtual
reality application has been able to run well as expected. All features that
have been developed have been running well. Based on 20 respondents with
various ages and backgrounds, our finding shows that The Bali Temple VR
Application attracts people of all ages to use and experience it. They are
eager to use it and hope that there will be more temples that they can
experience to visit in this application.","['I Gede Mahendra Darmawiguna', 'Gede Aditra Pradnyana', 'I Gede Partha Sindu', 'I Putu Prayoga Susila Karimawan', 'Ni Kadek Risa Ariani Dwiasri']",2020-04-26T05:03:39Z,http://arxiv.org/abs/2004.13853v1,['cs.HC'],"Virtual Reality,Digitalization,Balinese Temples,Application,Landscape,3D objects,Narration,Melanting Temples,Pulaki Temples"
"When Virtual Reality Meets Rate Splitting Multiple Access: A Joint
  Communication and Computation Approach","Rate Splitting Multiple Access (RSMA) has emerged as an effective
interference management scheme for applications that require high data rates.
Although RSMA has shown advantages in rate enhancement and spectral efficiency,
it has yet not to be ready for latency-sensitive applications such as virtual
reality streaming, which is an essential building block of future 6G networks.
Unlike conventional High-Definition streaming applications, streaming virtual
reality applications requires not only stringent latency requirements but also
the computation capability of the transmitter to quickly respond to dynamic
users' demands. Thus, conventional RSMA approaches usually fail to address the
challenges caused by computational demands at the transmitter, let alone the
dynamic nature of the virtual reality streaming applications. To overcome the
aforementioned challenges, we first formulate the virtual reality streaming
problem assisted by RSMA as a joint communication and computation optimization
problem. A novel multicast approach is then proposed to cluster users into
different groups based on a Field-of-View metric and transmit multicast streams
in a hierarchical manner. After that, we propose a deep reinforcement learning
approach to obtain the solution for the optimization problem. Extensive
simulations show that our framework can achieve the millisecond-latency
requirement, which is much lower than other baseline schemes.","['Nguyen Quang Hieu', 'Diep N. Nguyen', 'Dinh Thai Hoang', 'Eryk Dutkiewicz']",2022-07-25T12:25:06Z,http://arxiv.org/abs/2207.12114v1,['cs.NI'],"Rate Splitting Multiple Access,Virtual Reality,Communication,Computation,Latency-sensitive applications,6G networks,High-Definition streaming,Multicast,Field-of-View metric,Deep reinforcement learning"
"Fitted avatars: automatic skeleton adjustment for self-avatars in
  virtual reality","In the era of the metaverse, self-avatars are gaining popularity, as they can
enhance presence and provide embodiment when a user is immersed in Virtual
Reality. They are also very important in collaborative Virtual Reality to
improve communication through gestures. Whether we are using a complex motion
capture solution or a few trackers with inverse kinematics (IK), it is
essential to have a good match in size between the avatar and the user, as
otherwise mismatches in self-avatar posture could be noticeable for the user.
To achieve such a correct match in dimensions, a manual process is often
required, with the need for a second person to take measurements of body limbs
and introduce them into the system. This process can be time-consuming, and
prone to errors. In this paper, we propose an automatic measuring method that
simply requires the user to do a small set of exercises while wearing a
Head-Mounted Display (HMD), two hand controllers, and three trackers. Our work
provides an affordable and quick method to automatically extract user
measurements and adjust the virtual humanoid skeleton to the exact dimensions.
Our results show that our method can reduce the misalignment produced by the IK
system when compared to other solutions that simply apply a uniform scaling to
an avatar based on the height of the HMD, and make assumptions about the
locations of joints with respect to the trackers.","['Jose Luis Ponton', 'Víctor Ceballos', 'Lesly Acosta', 'Alejandro Ríos', 'Eva Monclús', 'Nuria Pelechano']",2023-07-14T08:20:03Z,http://arxiv.org/abs/2307.09558v1,"['cs.HC', 'cs.GR']","avatars,skeleton adjustment,self-avatars,virtual reality,metaverse,embodiment,motion capture,inverse kinematics,measurements,humanoid skeleton"
Live-action Virtual Reality Games,"This paper proposes the concept of ""live-action virtual reality games"" as a
new genre of digital games based on an innovative combination of live-action,
mixed-reality, context-awareness, and interaction paradigms that comprise
tangible objects, context-aware input devices, and embedded/embodied
interactions. Live-action virtual reality games are ""live-action games"" because
a player physically acts out (using his/her real body and senses) his/her
""avatar"" (his/her virtual representation) in the game stage, which is the
mixed-reality environment where the game happens. The game stage is a kind of
""augmented virtuality""; a mixed-reality where the virtual world is augmented
with real-world information. In live-action virtual reality games, players wear
HMD devices and see a virtual world that is constructed using the physical
world architecture as the basic geometry and context information. Physical
objects that reside in the physical world are also mapped to virtual elements.
Live-action virtual reality games keeps the virtual and real-worlds
superimposed, requiring players to physically move in the environment and to
use different interaction paradigms (such as tangible and embodied interaction)
to complete game activities. This setup enables the players to touch physical
architectural elements (such as walls) and other objects, ""feeling"" the game
stage. Players have free movement and may interact with physical objects placed
in the game stage, implicitly and explicitly. Live-action virtual reality games
differ from similar game concepts because they sense and use contextual
information to create unpredictable game experiences, giving rise to emergent
gameplay.","['Luis Valente', 'Esteban Clua', 'Alexandre Ribeiro Silva', 'Bruno Feijó']",2016-01-07T19:30:37Z,http://arxiv.org/abs/1601.01645v1,['cs.HC'],"live-action,virtual reality,mixed-reality,context-awareness,interaction paradigms,tangible objects,HMD devices,embodied interactions,game activities,emergent gameplay"
"Tutorial introduction to Virtual Reality: What possibility are offered
  to our field?","The virtual reality (VR) provides us a three-dimensional, immersive, and
fully interactive visualization environment. To make the best use of the VR's
potential in scientific visualization, a VR visualization software named VFIVE
has been developed for the CAVE-type VR system. VFIVE enables simulation
researchers to analyze three-dimensional scalar and vector fields by various
visualization methods including real time volume rendering in the CAVE's room
sized booth. Some basic visualization tools of VTK have been integrated to
VFIVE, too.","['Akira Kageyama', 'Nobuaki Ohno']",2005-12-08T01:20:24Z,http://arxiv.org/abs/physics/0512066v2,['physics.geo-ph'],"Virtual Reality,Three-dimensional visualization,Immersive environment,Interactive visualization,Scientific visualization,VFIVE,CAVE system,Volume rendering,VTK,Scalar field"
Virtual Reality Visualization by CAVE with VFIVE and VTK,"The CAVE-type virtual reality (VR) system was introduced for scientific
visualization of large scale data in the plasma simulation community about a
decade ago. Since then, we have been developing a VR visualization software,
VFIVE, for general CAVE systems. Recently, we have integrated an open source
visualization library, the Visualization Toolkit (VTK), into VFIVE. Various
visualization methods of VTK can be incorporated and used interactively in
VFIVE.","['Nobuaki Ohno', 'Akira Kageyama', 'Kanya Kusano']",2005-12-27T05:59:07Z,http://arxiv.org/abs/physics/0512247v1,"['physics.geo-ph', 'physics.plasm-ph']","Virtual Reality,Visualization,CAVE,VFIVE,VTK,Scientific Visualization,Plasma Simulation,Open Source,Visualization Library,Interactive"
An Integrated Simulation System for Human Factors Study,"It has been reported that virtual reality can be a useful tool for ergonomics
study. The proposed integrated simulation system aims at measuring operator's
performance in an interactive way for 2D control panel design. By incorporating
some sophisticated virtual reality hardware/software, the system allows natural
human-system and/or human-human interaction in a simulated virtual environment;
enables dynamic objective measurement of human performance; and evaluates the
quality of the system design in human factors perspective based on the
measurement. It can also be for operation training for some 2D control panels.","['Ying Wang', 'Wei Zhang', 'Fouad Bennis', 'Damien Chablat']",2007-09-04T09:20:03Z,http://arxiv.org/abs/0709.0370v1,['cs.HC'],"simulation system,human factors,virtual reality,ergonomics study,performance measurement,2D control panels,human-system interaction,human performance,system design"
Variations of the Turing Test in the Age of Internet and Virtual Reality,"Inspired by Hofstadter's Coffee-House Conversation (1982) and by the science
fiction short story SAM by Schattschneider (1988), we propose and discuss
criteria for non-mechanical intelligence. Firstly, we emphasize the practical
need for such tests in view of massively multiuser online role-playing games
(MMORPGs) and virtual reality systems like Second Life. Secondly, we
demonstrate Second Life as a useful framework for implementing (some iterations
of) that test.","['Florentin Neumann', 'Andrea Reichenberger', 'Martin Ziegler']",2009-04-23T07:56:11Z,http://arxiv.org/abs/0904.3612v1,"['cs.AI', 'cs.HC', 'H.5.1; I.2.0']","Turing Test,Internet,Virtual Reality,Criteria,Intelligence,MMORPGs,Second Life,Implementation,Iterations"
"Applications and a Three-dimensional Desktop Environment for an
  Immersive Virtual Reality System","We developed an application launcher called Multiverse for scientific
visualizations in a CAVE-type virtual reality (VR) system. Multiverse can be
regarded as a type of three-dimensional (3D) desktop environment. In
Multiverse, a user in a CAVE room can browse multiple visualization
applications with 3D icons and explore movies that float in the air. Touching
one of the movies causes ""teleportation"" into the application's VR space. After
analyzing the simulation data using the application, the user can jump back
into Multiverse's VR desktop environment in the CAVE.","['Akira Kageyama', 'Youhei Masada']",2013-01-19T07:01:46Z,http://arxiv.org/abs/1301.4535v1,"['physics.comp-ph', 'cs.GR']","application launcher,scientific visualizations,CAVE-type,virtual reality system,three-dimensional desktop environment,3D icons,movies,teleportation,simulation data,VR space"
A Review Paper on Oculus Rift-A Virtual Reality Headset,"Oculus rift: Virtual reality (VR) is a burgeoning field that has the inherent
potential of manipulating peoples mind with a superlative 3D experience. Oculus
rift is one such application that assists in achieving the same. With the
fleeting enhancements in VR it now seems very feasible to provide the user with
experiences that were earlier thought to be merely a dream or a nightmare.","['Parth Rajesh Desai', 'Pooja Nikhil Desai', 'Komal Deepak Ajmera', 'Khushbu Mehta']",2014-08-06T03:16:21Z,http://arxiv.org/abs/1408.1173v1,['cs.HC'],"Oculus Rift,Virtual Reality,VR,3D experience,Virtual reality headset"
"Preprint: Bringing immersive enjoyment to hyperbaric oxygen chamber
  users using virtual reality glasses","This is the preprint version of our paper on REHAB2015. This paper proposed a
novel immersive entertainment system for the users of hyperbaric oxygen therapy
chamber. The system is a hybrid of hardware and software, the scheme is
described in this paper. The hardware is combined by a HMD (i.e. virtual
reality glasses shell), a smartphone and a waterproof bag. The software is able
to transfer the stereoscopic images of the 3D game to the screen of the
smartphone synchronously. The comparison and selection of the hardware are
discussed according to the practical running scene of the clinical hyperbaric
oxygen treatment. Finally, a preliminary guideline for designing this kind of
system is raised accordingly.",['Zhihan Lv'],2015-09-22T20:35:54Z,http://arxiv.org/abs/1509.06774v1,['cs.HC'],"preprint,immersive entertainment system,hyperbaric oxygen therapy,virtual reality glasses,HMD,smartphone,waterproof bag,stereoscopic images,3D game,guideline"
"Towards the Holodeck: Fully Immersive Virtual Reality Visualisation of
  Scientific and Engineering Data","In this paper, we describe the development and operating principles of an
immersive virtual reality (VR) visualisation environment that is designed
around the use of consumer VR headsets in an existing wide area motion capture
suite. We present two case studies in the application areas of visualisation of
scientific and engineering data. Each of these case studies utilise a different
render engine, namely a custom engine for one case and a commercial game engine
for the other. The advantages and appropriateness of each approach are
discussed along with suggestions for future work.","['Stefan Marks', 'Javier E. Estevez', 'Andy M. Connor']",2016-04-20T02:54:21Z,http://arxiv.org/abs/1604.05797v1,['cs.HC'],"immersive virtual reality,visualisation,scientific data,engineering data,consumer VR headsets,motion capture,render engine,game engine"
"A Statistical Approach to Continuous Self-Calibrating Eye Gaze Tracking
  for Head-Mounted Virtual Reality Systems","We present a novel, automatic eye gaze tracking scheme inspired by smooth
pursuit eye motion while playing mobile games or watching virtual reality
contents. Our algorithm continuously calibrates an eye tracking system for a
head mounted display. This eliminates the need for an explicit calibration step
and automatically compensates for small movements of the headset with respect
to the head. The algorithm finds correspondences between corneal motion and
screen space motion, and uses these to generate Gaussian Process Regression
models. A combination of those models provides a continuous mapping from
corneal position to screen space position. Accuracy is nearly as good as
achieved with an explicit calibration step.","['Subarna Tripathi', 'Brian Guenter']",2016-12-20T23:25:27Z,http://arxiv.org/abs/1612.06919v1,['cs.CV'],"statistical approach,self-calibrating,eye gaze tracking,head-mounted,virtual reality systems,smooth pursuit eye motion,automatic calibration,Gaussian Process Regression,corneal motion,screen space motion"
Streaming Virtual Reality Content,"The recent rise of interest in Virtual Reality (VR) came with the
availability of commodity commercial VR prod- ucts, such as the Head Mounted
Displays (HMD) created by Oculus and other vendors. To accelerate the user
adoption of VR headsets, content providers should focus on producing high
quality immersive content for these devices. Similarly, multimedia streaming
service providers should enable the means to stream 360 VR content on their
platforms. In this study, we try to cover different aspects related to VR
content representation, streaming, and quality assessment that will help
establishing the basic knowledge of how to build a VR streaming system.","['Tarek El-Ganainy', 'Mohamed Hefeeda']",2016-12-26T09:40:45Z,http://arxiv.org/abs/1612.08350v1,['cs.MM'],"Virtual Reality,Streaming,Content,Head Mounted Displays,Immersive,Multimedia,360,Quality Assessment,Platforms"
Robotic Haptic Proxies for Collaborative Virtual Reality,"We propose a new approach for interaction in Virtual Reality (VR) using
mobile robots as proxies for haptic feedback. This approach allows VR users to
have the experience of sharing and manipulating tangible physical objects with
remote collaborators. Because participants do not directly observe the robotic
proxies, the mapping between them and the virtual objects is not required to be
direct. In this paper, we describe our implementation, various scenarios for
interaction, and a preliminary user study.","['Zhenyi He', 'Fengyuan Zhu', 'Aaron Gaudette', 'Ken Perlin']",2017-01-31T00:24:10Z,http://arxiv.org/abs/1701.08879v1,"['cs.HC', 'H.5.1']","Virtual Reality,Haptic Feedback,Mobile Robots,Collaborative Interaction,Tangible Objects,User Study,Robotic Proxies"
"Kinematically Redundant Octahedral Motion Platform for Virtual Reality
  Simulations","We propose a novel design of a parallel manipulator of Stewart Gough type for
virtual reality application of single individuals; i.e. an omni-directional
treadmill is mounted on the motion platform in order to improve VR immersion by
giving feedback to the human body. For this purpose we modify the well-known
octahedral manipulator in a way that it has one degree of kinematical
redundancy; namely an equiform reconfigurability of the base. The instantaneous
kinematics and singularities of this mechanism are studied, where especially
""unavoidable singularities"" are characterized. These are poses of the motion
platform, which can only be realized by singular configurations of the
mechanism despite its kinematic redundancy.","['Georg Nawratil', 'Arvin Rasoulzadeh']",2017-04-15T19:10:46Z,http://arxiv.org/abs/1704.04677v2,['cs.RO'],"kinematically redundant,octahedral,motion platform,virtual reality,simulations,parallel manipulator,Stewart Gough type,omni-directional treadmill,instant kinematics,singularities"
PhyShare: Sharing Physical Interaction in Virtual Reality,"We present PhyShare, a new haptic user interface based on actuated robots.
Virtual reality has recently been gaining wide adoption, and an effective
haptic feedback in these scenarios can strongly support user's sensory in
bridging virtual and physical world. Since participants do not directly observe
these robotic proxies, we investigate the multiple mappings between physical
robots and virtual proxies that can utilize the resources needed to provide a
well rounded VR experience. PhyShare bots can act either as directly touchable
objects or invisible carriers of physical objects, depending on different
scenarios. They also support distributed collaboration, allowing remotely
located VR collaborators to share the same physical feedback.","['Zhenyi He', 'Fengyuan Zhu', 'Ken Perlin']",2017-08-10T21:03:21Z,http://arxiv.org/abs/1708.04139v1,['cs.HC'],"haptic user interface,actuated robots,virtual reality,haptic feedback,sensory,physical world,virtual proxies,VR experience,distributed collaboration,physical feedback"
"Engaging the Public with Supernova and Supernova Remnant Research Using
  Virtual Reality","On 21 April 2018, the citizens of Wako, Japan, interacted in a novel way with
research being carried out at the Astrophysical Big Bang Laboratory (ABBL) at
RIKEN. They were able to explore a model of a supernova and its remnant in an
immersive three-dimentional format by using virtual reality (VR) technology. In
this article, we explain how this experience was developed and delivered to the
public, providing practical tips for and reflecting on the successful
organisation of an event of this kind.","['Gilles Ferrand', 'Don Warren']",2018-11-05T07:48:06Z,http://arxiv.org/abs/1811.01542v1,"['astro-ph.IM', 'physics.pop-ph']","Public engagement,Supernova,Supernova remnant,Virtual reality,Immersive,Three-dimensional,Astrophysical,Big Bang Laboratory,RIKEN,Technology"
"Exploration of Interaction Techniques for Graph-based Modelling in
  Virtual Reality","Editing and manipulating graph-based models within immersive environments is
largely unexplored and certain design activities could benefit from using those
technologies. For example, in the case study of architectural modelling, the 3D
context of Virtual Reality naturally matches the intended output product, i.e.
a 3D architectural geometry. Since both the state of the art and the state of
the practice are lacking, we explore the field of VR-based interactive
modelling, and provide insights as to how to implement proper interactions in
that context, with broadly available devices. We consequently produce several
open-source software prototypes for manipulating graph-based models in VR.","['Adrien Coppens', 'Berat Bicer', 'Naz Yilmaz', 'Serhat Aras']",2020-01-03T17:06:58Z,http://arxiv.org/abs/2001.00892v1,['cs.HC'],"Interaction Techniques,Graph-based Modelling,Virtual Reality,Immersive Environments,Editing,Manipulating,Architectural Modelling,3D Context,Interactive Modelling,Open-source Software"
Knotted Portals in Virtual Reality,"This article describes a software named ""KnotPortal"" for visualization of
branched covers of knots based on an idea by Bill Thurston to view a knot as a
portal to other universes. Our implementation allows users to explore these
knotted portals either on a screen or in virtual reality using a head-mounted
device with room-tracking. This allows users not only to see these glued
different worlds, but experience them by being able to walk through the
portals. This software can be used to enable students to learn about knots,
gluing, (branched) covers, or just to have a fun look at portals.",['Moritz Lucius Sümmermann'],2020-01-09T11:35:27Z,http://arxiv.org/abs/2001.02927v2,"['math.GN', 'math.HO', '(Primary) 97U70, (Secondary) 57M12, 57M25']","virtual reality,software,KnotPortal,visualization,branched covers,knots,Bill Thurston,portals,universes,room-tracking"
Towards a Virtual Reality Home IoT Network Visualizer,"We present an IoT home network visualizer that utilizes virtual reality (VR).
This prototype demonstrates the potential that VR has to aid in the
understanding of home IoT networks. This is particularly important due the
increased number of household devices now connected to the Internet. This
prototype is able to function in a standard display or a VR headset. A
prototype was developed to aid in the understanding of home IoT networks for
homeowners.","['Drew Johnston', 'Jarret Flack', 'Indrakshi Ray', 'Francisco R. Ortega']",2020-01-18T02:31:30Z,http://arxiv.org/abs/2001.06579v1,['cs.HC'],"Virtual Reality,Home IoT,Network Visualizer,Prototype,Household Devices,Internet,Homeowners,Standard Display,VR Headset"
"Exploring an Application of Virtual Reality for Early Detection of
  Dementia","Facing the severe global dementia problem, an exploration was conducted
adopting the technology of virtual reality (VR). This report lays a technical
foundation for further research project ""Early Detection of Dementia Using
Testing Tools in VR Environment"", which illustrates the process of developing a
VR application using Unity 3D software on Oculus Go. This preliminary
exploration is composed of three steps, including 3D virtual scene
construction, VR interaction design and monitoring. The exploration was
recorded to provide basic technical guidance and detailed method for subsequent
research.","['Yiming Zhong', 'Yuan Tian', 'Mira Park', 'Soonja Yeom']",2020-01-15T03:27:36Z,http://arxiv.org/abs/2001.07546v1,['cs.HC'],"virtual reality,early detection,dementia,Unity 3D,Oculus Go"
Virtual Reality based Learning Systems,"This article is based on studies of the existing literature, focusing on the
states-of-the-arts on virtual reality (VR) and its potential uses in learning.
Different platforms have been used to improve the learning effects of VR that
offers exciting opportunities in various fields. As more and more students want
in a distance, part-time, or want to continue their education, VR has attracted
considerable attention in learning, training, and traditional education. VR
based learning enables operators to bring together all disciplinary resources
in a common playground. The VR base multimedia platform has successfully
demonstrated great potential of education and training. In this paper, we will
discuss existing systems and their uses and address the technical challenges
and future directions.",['Yang Cheng'],2016-05-28T19:29:13Z,http://arxiv.org/abs/1605.08928v1,"['cs.CY', 'cs.HC', 'cs.MM']","virtual reality,learning systems,literature,VR,platforms,education,training,multimedia platform,technical challenges"
"The Illusion of Animal Body Ownership and Its Potential for Virtual
  Reality Games","Virtual reality offers the unique possibility to experience a virtual
representation as our own body. In contrast to previous research that
predominantly studied this phenomenon for humanoid avatars, our work focuses on
virtual animals. In this paper, we discuss different body tracking approaches
to control creatures such as spiders or bats and the respective virtual body
ownership effects. Our empirical results demonstrate that virtual body
ownership is also applicable for nonhumanoids and can even outperform
human-like avatars in certain cases. An additional survey confirms the general
interest of people in creating such experiences and allows us to initiate a
broad discussion regarding the applicability of animal embodiment for
educational and entertainment purposes.","['Andrey Krekhov', 'Sebastian Cmentowski', 'Jens Krüger']",2019-07-11T14:12:04Z,http://arxiv.org/abs/1907.05220v1,['cs.HC'],"virtual reality,body ownership,virtual animals,body tracking,nonhumanoids,avatars,virtual embodiment,entertainment,educational purposes"
Lessons Learned from Teaching Astronomy with Virtual Reality,"We report on the initial phase of an ongoing, multi-stage investigation of
how to incorporate Virtual Reality (VR) technology in teaching introductory
astronomy concepts. Our goal was to compare the efficacy of VR vs. conventional
teaching methods using one specific topic, Moon phases and eclipses. After
teaching this topic to an ASTRO 101 lecture class, students were placed into
three groups to experience one of three additional activities: supplemental
lecture, ""hands-on"" activity, or VR experience. All students were tested before
and after their learning activity. Although preliminary, our results can serve
as a useful guide to expanding the role of VR in the astronomy classroom.","['Philip Blanco', 'Gur Windmiller', 'William Welsh', 'Sean Hauze']",2019-12-28T03:51:17Z,http://arxiv.org/abs/1912.12393v1,['physics.ed-ph'],"Virtual Reality,Astronomy,Teaching,Moon phases,Eclipses,ASTRO 101,Lecture class,Conventional teaching methods,Hands-on activity"
"StreamBED: Training Citizen Scientists to Make Qualitative Judgments
  Using Embodied Virtual Reality Training","Environmental citizen science frequently relies on experience-based
assessment, however volunteers are not trained to make qualitative judgments.
Embodied learning in virtual reality (VR) has been explored as a way to train
behavior, but has not fully been considered as a way to train judgment. This
preliminary research explores embodied learning in VR through the design,
evaluation, and redesign of StreamBED, a water quality monitoring training
environment that teaches volunteers to make qualitative assessments by
exploring, assessing and comparing virtual watersheds.","['Alina Striner', 'Jennifer Preece']",2018-04-23T20:57:47Z,http://arxiv.org/abs/1804.08732v1,"['cs.HC', 'Interface Prototype, Qualitative Judgments, Immersion, Citizen\n  Science, Training']","citizen science,qualitative judgments,embodied learning,virtual reality,training,water quality monitoring,volunteers,virtual watersheds,assessment,judgment"
Brown Ring Experiment in Virtual Reality,"Brown Ring Experiment is a very popular test to detect the presence of
Nitrate in salts commonly performed in chemical laboratories with supplies of
required chemicals. Our work clears out the need for a chemical laboratory and
chemicals in order to understand the experiment practically. We have used the
technology of Virtual Reality to fulfill this requirement. Our research work
can be extensively utilized to create virtual environments for conducting other
chemical processes in a virtual environment hence, eliminating the need for a
chemical laboratory. This can help students in remote areas with minimal
resources to fill in the void of practical experiments they have in their
learning process due to space constraints.","['Prithaj Jana', 'Emil Joswin']",2019-10-02T21:10:19Z,http://arxiv.org/abs/1910.04698v1,['cs.HC'],"Brown Ring Experiment,Nitrate,Virtual Reality,Chemical Laboratory,Virtual Environment,Chemical Processes"
Hack.VR: A Programming Game in Virtual Reality,"In this article we describe Hack.VR, an object-oriented programming game in
virtual reality. Hack.VR uses a VR programming language in which nodes
represent functions and node connections represent data flow. Using this
programming framework, players reprogram VR objects such as elevators, robots,
and switches. Hack.VR has been designed to be highly interactable both
physically and semantically.","['Dominic Kao', 'Christos Mousas', 'Alejandra J. Magana', 'D. Fox Harrell', 'Rabindra Ratan', 'Edward F. Melcer', 'Brett Sherrick', 'Paul Parsons', 'Dmitri A. Gusev']",2020-07-09T01:14:25Z,http://arxiv.org/abs/2007.04495v2,['cs.HC'],"programming game,virtual reality,object-oriented,VR programming language,nodes,connections,data flow,reprogram,interactable"
Towards Immersive Virtual Reality Simulations of Bionic Vision,"Bionic vision is a rapidly advancing field aimed at developing visual
neuroprostheses ('bionic eyes') to restore useful vision to people who are
blind. However, a major outstanding challenge is predicting what people 'see'
when they use their devices. The limited field of view of current devices
necessitates head movements to scan the scene, which is difficult to simulate
on a computer screen. In addition, many computational models of bionic vision
lack biological realism. To address these challenges, we propose to embed
biologically realistic models of simulated prosthetic vision (SPV) in immersive
virtual reality (VR) so that sighted subjects can act as 'virtual patients' in
real-world tasks.","['Justin Kasowski', 'Nathan Wu', 'Michael Beyeler']",2021-02-21T20:38:20Z,http://arxiv.org/abs/2102.10678v1,['cs.HC'],"bionic vision,visual neuroprostheses,virtual reality,simulations,immersive,biological realism,prosthetic vision,sighted subjects"
Effectiveness of Social Virtual Reality,"A lot of work in social virtual reality, including our own group's, has
focused on effectiveness of specific social behaviours such as eye-gaze, turn
taking, gestures and other verbal and non-verbal cues. We have built upon these
to look at emergent phenomena such as co-presence, leadership and trust. These
give us good information about the usability issues of specific social VR
systems, but they don't give us much information about the requirements for
such systems going forward. In this short paper we discuss how we are
broadening the scope of our work on social systems, to move out of the
laboratory to more ecologically valid situations and to study groups using
social VR for longer periods of time.","['Lisa Izzouzi', 'Anthony Steed']",2021-04-12T11:34:14Z,http://arxiv.org/abs/2104.05366v1,['cs.HC'],"social virtual reality,effectiveness,social behaviours,eye-gaze,turn taking,gestures,verbal cues,non-verbal cues,co-presence,trust"
Ideals and Virtual Realities,"A main step for world progress is to keep sharing ever-present Ideals for
science and education within today Virtual Realities. On-line education is
transforming human society to new levels in the way people teach and learn
during the ongoing SARS-CoV-2 pandemic. There is an increasing interest in
having more and more reliable, fast and simple apps to communicate and also to
record, assemble and distribute videos and lectures in the fields of Physics &
Maths still using traditional didactic methods. We describe here how to
accurately reproduce chalkboard classes for the popular YouTube video platform
using OpenEyA-YT. The audience can thus be expanded over continents to help
mitigate the effects of physical isolation.","['E. Canessa', 'L. Tenze']",2021-08-29T18:08:13Z,http://arxiv.org/abs/2109.00926v1,"['physics.ed-ph', 'cs.CV']","Ideals,Virtual Realities,On-line education,SARS-CoV-2 pandemic,apps,videos,lectures,Physics,Maths,OpenEyA-YT"
"Hafnia Hands: A Multi-Skin Hand Texture Resource for Virtual Reality
  Research","We created a hand texture resource (with different skin tone versions as well
as non-human hands) for use in virtual reality studies. This makes it easier to
run lab and remote studies where the hand representation is matched to the
participant's own skin tone. We validate that the virtual hands with our
textures align with participants view of their own real hands and allow to
create VR applications where participants have an increased sense of body
ownership. These properties are critical for a range of VR studies, such as of
immersion.","['Henning Pohl', 'Aske Mottelson']",2021-10-07T12:16:54Z,http://arxiv.org/abs/2110.03379v1,['cs.HC'],"hand texture,virtual reality,skin tone,non-human hands,body ownership"
"A Survey on Metaverse: the State-of-the-art, Technologies, Applications,
  and Challenges","Metaverse is a new type of Internet application and social form that
integrates a variety of new technologies. It has the characteristics of
multi-technology, sociality, and hyper spatiotemporality. This paper introduces
the development status of Metaverse, from the five perspectives of network
infrastructure, management technology, basic common technology, virtual reality
object connection, and virtual reality convergence, it introduces the technical
framework of Metaverse. This paper also introduces the nature of Metaverse's
social and hyper spatiotemporality, and discusses the first application areas
of Metaverse and some of the problems and challenges it may face.","['Huansheng Ning', 'Hang Wang', 'Yujia Lin', 'Wenxi Wang', 'Sahraoui Dhelim', 'Fadi Farha', 'Jianguo Ding', 'Mahmoud Daneshmand']",2021-11-18T13:16:34Z,http://arxiv.org/abs/2111.09673v1,['cs.CY'],"Metaverse,Internet application,sociality,spatiotemporality,network infrastructure,management technology,virtual reality,technical framework,application areas,challenges"
From medical imaging to virtual reality for archaeology,"The IRMA project aims to design innovative methodologies for research in the
field of historical and archaeological heritage based on a combination of
medical imaging technologies and interactive 3D restitution modalities (virtual
reality, augmented reality, haptics, additive manufacturing). These tools are
based on recent research results from a collaboration between IRISA, Inrap and
the company Image ET and are intended for cultural heritage professionals such
as museums, curators, restorers and archaeologists.","['Théophane Nicolas', 'Ronan Gaugne', 'Bruno Arnaldi', 'Valérie Gouranton']",2023-01-26T09:39:25Z,http://arxiv.org/abs/2301.11006v1,['cs.GR'],"medical imaging,virtual reality,archaeology,interactive 3D restitution,augmented reality,haptics,additive manufacturing,cultural heritage,museums,archaeologists"
The Economics of Augmented and Virtual Reality,"This paper explores the economics of Augmented Reality (AR) and Virtual
Reality (VR) technologies within decision-making contexts. Two metrics are
proposed: Context Entropy, the informational complexity of an environment, and
Context Immersivity, the value from full immersion. The analysis suggests that
AR technologies assist in understanding complex contexts, while VR technologies
provide access to distant, risky, or expensive environments. The paper provides
a framework for assessing the value of AR and VR applications in various
business sectors by evaluating the pre-existing context entropy and context
immersivity. The goal is to identify areas where immersive technologies can
significantly impact and distinguish those that may be overhyped.","['Joshua Gans', 'Abhishek Nagaraj']",2023-05-26T12:26:14Z,http://arxiv.org/abs/2305.16872v1,"['econ.GN', 'cs.HC', 'q-fin.EC']","Augmented Reality,Virtual Reality,Economics,Decision-making,Context Entropy,Context Immersivity,Immersive technologies,Business sectors,Value assessment,Overhyped."
Let's Give a Voice to Conversational Agents in Virtual Reality,"The dialogue experience with conversational agents can be greatly enhanced
with multimodal and immersive interactions in virtual reality. In this work, we
present an open-source architecture with the goal of simplifying the
development of conversational agents operating in virtual environments. The
architecture offers the possibility of plugging in conversational agents of
different domains and adding custom or cloud-based Speech-To-Text and
Text-To-Speech models to make the interaction voice-based. Using this
architecture, we present two conversational prototypes operating in the digital
health domain developed in Unity for both non-immersive displays and VR
headsets.","['Michele Yin', 'Gabriel Roccabruna', 'Abhinav Azad', 'Giuseppe Riccardi']",2023-08-04T18:51:38Z,http://arxiv.org/abs/2308.02665v1,['cs.AI'],"conversational agents,virtual reality,multimodal interactions,immersive interactions,open-source architecture,Speech-To-Text,Text-To-Speech,digital health domain,Unity,VR headsets."
A Multisensory Approach to Virtual Reality Stress Reduction,"Forest bathing is a nature immersion practice that reduces stress, restores
mental resources, and has a wide variety of use cases in the treatment of
mental illnesses. Since many people who need the benefits of forest bathing
have little access to nature, virtual reality (VR) is being explored as a tool
for delivering accessible immersive nature experiences via virtual nature
environments (VNE's). Research on VNE's mainly utilizes the audiovisual
capabilities of VR, but since forest bathing is a fully multisensory
experience, further investigations into the integration of other sensory
technologies, namely smell and temperature, are essential for the future of VNE
research.","['Rachel Masters', 'Francisco Ortega', 'Victoria Interrante']",2023-09-01T19:51:31Z,http://arxiv.org/abs/2309.00718v1,['cs.HC'],"multisensory approach,virtual reality,stress reduction,forest bathing,nature immersion,mental illnesses,virtual nature environments,VNE,sensory technologies,smell,temperature"
"RetinaVR: Democratizing Vitreoretinal Surgery Training with a Portable
  and Affordable Virtual Reality Simulator in the Metaverse","We developed and validated RetinaVR, an affordable and immersive virtual
reality simulator for vitreoretinal surgery training, using the Meta Quest 2 VR
headset. We focused on four core fundamental skills: core vitrectomy,
peripheral shaving, membrane peeling, and endolaser application. The validation
study involved 10 novice ophthalmology residents and 10 expert vitreoretinal
surgeons. We demonstrated construct validity, as shown by the varying user
performance in a way that correlates with experimental runs, age, sex, and
expertise. RetinaVR shows promise as a portable and affordable simulator, with
potential to democratize surgical simulation access, especially in developing
countries.","['Fares Antaki', 'Cédryk Doucet', 'Daniel Milad', 'Charles-Édouard Giguère', 'Benoit Ozell', 'Karim Hammamji']",2024-01-19T18:54:10Z,http://arxiv.org/abs/2401.10883v1,['cs.HC'],"virtual reality,simulator,vitreoretinal surgery,Meta Quest 2,core vitrectomy,peripheral shaving,membrane peeling,endolaser application,construct validity,surgical simulation"
Exploring Virtual Reality through Ihde's Instrumental Realism,"Based on Ihde's theory, this paper explores the relationship between virtual
reality (VR) as an instrument and phenomenology. It reviews the ""technological
revolution"" spurred by the development of VR technology and discusses how VR
has been used to study subjective experience, explore perception and
embodiment, enhance empathy and perspective, and investigate altered states of
consciousness. The paper emphasizes the role of VR as an instrumental
technology, particularly its ability to expand human perception and cognition.
Reflecting on this in conjunction with the work of Husserl and Ihde, among
others, it revisits the potential of VR to provide new avenues for scientific
inquiry and experience and to transform our understanding of the world through
VR.","['He Zhang', 'John M. Carroll']",2024-01-23T06:31:30Z,http://arxiv.org/abs/2401.12521v1,['cs.HC'],"Ihde's theory,virtual reality,phenomenology,technological revolution,perception,embodiment,empathy,altered states of consciousness,instrumental technology,cognition"
5G Virtual Reality Manipulator Teleoperation using a Mobile Phone,"This paper presents an approach to teleoperate a manipulator using a mobile
phone as a leader device. Using its IMU and camera, the phone estimates its
Cartesian pose which is then used to to control the Cartesian pose of the
robot's tool. The user receives visual feedback in the form of multi-view video
- a point cloud rendered in a virtual reality environment. This enables the
user to observe the scene from any position. To increase immersion, the robot's
estimate of external forces is relayed using the phone's haptic actuator.
Leader and follower are connected through wireless networks such as 5G or
Wi-Fi. The paper describes the setup and analyzes its performance.","['Alexander Werner', 'William Melek']",2024-05-12T01:39:04Z,http://arxiv.org/abs/2405.07128v1,['cs.RO'],"5G,Virtual Reality,Manipulator,Teleoperation,Mobile Phone,IMU,Camera,Cartesian pose,Haptic actuator,Wireless networks"
"Exploring the Pathways of Adaptation an Avatar 3D Animation Procedures
  and Virtual Reality Arenas in Research of Human Courtship Behaviour and
  Sexual Reactivity in Psychological Research","There are many reasons for utilising 3D animation and virtual reality in
sexuality research. Apart from providing a mean with which to (re)experience
certain situations there are four main advantages: a) bespoke animated stimuli
can be created and customized, which is especially important when researching
paraphilia and sexual preferences, b) stimulus production is less expensive and
easier to produce compared to real world stimuli, c) virtual reality allows us
to capture data such as physiological reasons to stimuli, that we would not be
able to otherwise (without resorting to self-report measures which are
especially problematic in this research domain), d) ethical, legal, and health
and safety issues are less complex since neither physical nor psychological
harm is caused to animated characters allowing for the safe presentation of
stimuli involving vulnerable targets. The animation sub-group has been
exploring so far several production quality levels and various animation
procedures in a number of available software. The aim is to develop static as
well as dynamic, interactive sexual stimuli for sexual diagnostic and
therapeutic purposes. We are aware of number of ethical issues related to the
use of virtual reality in proposed research are analysed in this chapter.","['Jakub Binter', 'Kateřina Klapilová', 'Tereza Zikánová', 'Tommy Nilsson', 'Klára Bártová', 'Lucie Krejcová', 'Renata Androvicová', 'Jitka Lindová', 'Denisa Prušová', 'Timothy Wells', 'Daniel Riha']",2016-11-06T18:27:09Z,http://arxiv.org/abs/1611.01817v1,['cs.HC'],"3D animation,virtual reality,stimuli,physiological data,paraphilia,sexual preferences,ethical issues,research,psychological harm"
"UnrealROX: An eXtremely Photorealistic Virtual Reality Environment for
  Robotics Simulations and Synthetic Data Generation","Data-driven algorithms have surpassed traditional techniques in almost every
aspect in robotic vision problems. Such algorithms need vast amounts of quality
data to be able to work properly after their training process. Gathering and
annotating that sheer amount of data in the real world is a time-consuming and
error-prone task. Those problems limit scale and quality. Synthetic data
generation has become increasingly popular since it is faster to generate and
automatic to annotate. However, most of the current datasets and environments
lack realism, interactions, and details from the real world. UnrealROX is an
environment built over Unreal Engine 4 which aims to reduce that reality gap by
leveraging hyperrealistic indoor scenes that are explored by robot agents which
also interact with objects in a visually realistic manner in that simulated
world. Photorealistic scenes and robots are rendered by Unreal Engine into a
virtual reality headset which captures gaze so that a human operator can move
the robot and use controllers for the robotic hands; scene information is
dumped on a per-frame basis so that it can be reproduced offline to generate
raw data and ground truth annotations. This virtual reality environment enables
robotic vision researchers to generate realistic and visually plausible data
with full ground truth for a wide variety of problems such as class and
instance semantic segmentation, object detection, depth estimation, visual
grasping, and navigation.","['Pablo Martinez-Gonzalez', 'Sergiu Oprea', 'Alberto Garcia-Garcia', 'Alvaro Jover-Alvarez', 'Sergio Orts-Escolano', 'Jose Garcia-Rodriguez']",2018-10-16T11:43:50Z,http://arxiv.org/abs/1810.06936v2,"['cs.RO', 'cs.CV', 'cs.MM']","robotics simulations,synthetic data generation,virtual reality environment,Unreal Engine,photorealistic,robotic vision,ground truth annotations,semantic segmentation,object detection,depth estimation"
"Dataset of an EEG-based BCI experiment in Virtual Reality and on a
  Personal Computer","We describe the experimental procedures for a dataset that we have made
publicly available at https://doi.org/10.5281/zenodo.2605204 in mat (Mathworks,
Natick, USA) and csv formats. This dataset contains electroencephalographic
recordings on 21 subjects doing a visual P300 experiment on PC (personal
computer) and VR (virtual reality). The visual P300 is an event-related
potential elicited by a visual stimulation, peaking 240-600 ms after stimulus
onset. The experiment was designed in order to compare the use of a P300-based
brain-computer interface on a PC and with a virtual reality headset, concerning
the physiological, subjective and performance aspects. The brain-computer
interface is based on electroencephalography (EEG). EEG were recorded thanks to
16 electrodes. The virtual reality headset consisted of a passive head-mounted
display, that is, a head-mounted display which does not include any electronics
at the exception of a smartphone. This experiment was carried out at GIPSA-lab
(University of Grenoble Alpes, CNRS, Grenoble-INP) in 2018, and promoted by the
IHMTEK Company (Interaction Homme-Machine Technologie). The study was approved
by the Ethical Committee of the University of Grenoble Alpes (Comit{\'e}
d'Ethique pour la Recherche Non-Interventionnelle). Python code for
manipulating the data is available at
https://github.com/plcrodrigues/py.VR.EEG.2018-GIPSA. The ID of this dataset is
VR.EEG.2018-GIPSA.","['Grégoire Cattan', 'A. Andreev', 'P. Rodrigues', 'M. Congedo']",2019-03-27T08:58:02Z,http://arxiv.org/abs/1903.11297v1,['cs.HC'],"EEG,BCI,Virtual Reality,Personal Computer,P300,Electroencephalography,Head-mounted display,Event-related potential,Experiment,Dataset"
"Architectural Visualization Using Virtual Reality: A User Experience in
  Simulating Buildings of a Community College in Bukidnon, Philippines","The study aims to design and develop a virtual structural design that
simulates the campus and its buildings of a community college in Bukidnon,
Philippines through Virtual Reality. With the immersion of technology, this
project represents the architectural design of the establishment with the use
of Virtual Reality Technology. The project uses a modified Iterative
Development Model which is a guide for the design and development of the 3D
Models and VR Application. TinkerCAD which is a web-based application has been
used to design buildings on the other hand Unity is used to develop the
structural designs of the buildings. The respondents of this study are the
Grade 12 Senior High students from the 4 schools which are geographically near
to the college. With this study, the researchers were able to showcase its VR
Application to the students and later evaluated using a System Usability Scale,
a 10 item questionnaire measuring usability with an overall average of 90% or
Point Score of 4.5 which is interpreted as excellent in a Likert table for
descriptive interpretation. With the use of the VR application potential
students of the college will be able to visualize and experience the present
structures of the college without being physically present in the area. In this
paper, the buildings and structures of NBCC were designed and developed through
a Virtual Reality Platform allowing students from different secondary schools
that are geographically near to the college to experience the feeling to be in
the school without being able to set a step in physically.","['Benzar Glen Grepon', 'Aldwin Lester Martinez']",2021-02-15T03:55:39Z,http://arxiv.org/abs/2103.06238v1,['cs.HC'],"Architectural Visualization,Virtual Reality,User Experience,Community College,Bukidnon,Philippines,3D Models,VR Application,System Usability Scale,Unity"
SURVIVRS: Surround Video-Based Virtual Reality for Surgery Guidance,"There is a strong demand for virtual reality (VR) to bring quality healthcare
to underserved populations. This paper addresses this need with the design and
prototype of SURVIVRS: Surround Video-Based Virtual Reality for Surgery
Guidance. SURVIVRS allows a remote specialist to guide a local surgery team
through a virtual reality (VR) telepresence interface. SURVIVRS is motivated by
a need for medical expertise in remote and hard-to-reach areas, such as
low-to-middle-income countries (LMICs). The remote surgeon interface allows the
live observation of a procedure and combines 3D user interface annotation and
communication tools on streams of the surgical site and the patient vitals
monitor. SURVIVRS also supports debriefing and educational experiences by
offering the ability for users to watch recorded surgeries from the point of
view of the remote expert. The main contributions of this work are: the
feasibility demonstration of the SURVIVRS system through a rigorous 3D user
interface design process; the implementation of a prototype application that
realizes the proposed design; and a usability evaluation of SURVIVRS showing
that the tool was highly favored by users from the general population. The
paper discusses the next steps in this line of research aimed at more equitable
and diverse access to healthcare.","['Amani Taweel', 'Joaquim Jorge', 'Anderson Maciel', 'João Ricardo Nickenig Vissoci', 'Regis Kopper']",2023-02-08T09:24:50Z,http://arxiv.org/abs/2302.03953v1,['cs.HC'],"virtual reality,surgery,guidance,telepresence interface,3D user interface,remote expert,debriefing,educational experiences,usability evaluation,healthcare"
El Sonido como Elemento Clave en Prácticas de Realidad Virtual,"This article discusses the importance of sound for virtual reality systems.
For this, the emotional effects generated by sound are analyzed, and its
contribution to the effect of immersion.",['Yesid Ospitia Medina'],2023-05-21T14:48:25Z,http://arxiv.org/abs/2305.13340v1,['cs.HC'],"sound,virtual reality systems,emotional effects,immersion,practices"
"VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System
  in Virtual Reality","As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain
momentum, there's a growing focus on the development of engagements with 3D
virtual content. Unfortunately, traditional techniques for content creation,
editing, and interaction within these virtual spaces are fraught with
difficulties. They tend to be not only engineering-intensive but also require
extensive expertise, which adds to the frustration and inefficiency in virtual
object manipulation. Our proposed VR-GS system represents a leap forward in
human-centered 3D content interaction, offering a seamless and intuitive user
experience. By developing a physical dynamics-aware interactive Gaussian
Splatting in a Virtual Reality setting, and constructing a highly efficient
two-level embedding strategy alongside deformable body simulations, VR-GS
ensures real-time execution with highly realistic dynamic responses. The
components of our Virtual Reality system are designed for high efficiency and
effectiveness, starting from detailed scene reconstruction and object
segmentation, advancing through multi-view image in-painting, and extending to
interactive physics-based editing. The system also incorporates real-time
deformation embedding and dynamic shadow casting, ensuring a comprehensive and
engaging virtual experience.Our project page is available at:
https://yingjiang96.github.io/VR-GS/.","['Ying Jiang', 'Chang Yu', 'Tianyi Xie', 'Xuan Li', 'Yutao Feng', 'Huamin Wang', 'Minchen Li', 'Henry Lau', 'Feng Gao', 'Yin Yang', 'Chenfanfu Jiang']",2024-01-30T01:28:36Z,http://arxiv.org/abs/2401.16663v2,"['cs.HC', 'cs.CV']","Virtual Reality,Mixed Reality,3D virtual content,interactive,Gaussian Splatting,physical dynamics-aware,deformable body simulations,real-time execution,dynamic responses,object segmentation."
"Visualizing 2D Quantum Field Theory: Geometry and Informatics of
  Mobilevision","This article is devoted to some interesting geometric and informatic
interpretations of peculiarities of 2D quantum field theory, which become re-
vealed after its visualization. Contents.
  I. Geometry of Mobilevision:
  1.1. Interpretational geometry and anomalous virtual realities;
  1.2. Quantum projective field theory and Mobilevision;
  1.3. Quantum conformal and q_R conformal field theories; quantum-field
analogs of Euler-Arnold top;
  1.4. Organizing MV cyberspace;
  1.5. Non-Alexandrian geometry of Mobilevision.
  II. Informatics of Mobilevision:
  2.1. Information transmission via anomalous virtual realities: AVR-photodosy;
  2.2. Information transmission via intentional anomalous virtual realities:
IAVR-teleaesthesy.",['D. Juriev'],1994-01-14T14:34:25Z,http://arxiv.org/abs/hep-th/9401067v4,"['hep-th', 'adap-org', 'nlin.AO', 'nlin.PS', 'patt-sol']","2D quantum field theory,visualization,geometry,informatics,Mobilevision,quantum projective field theory,quantum conformal field theory,q_R conformal field theory,cyberspace,information transmission"
The emergence of the physical world from information processing,"This paper links the conjecture that the physical world is a virtual reality
to the findings of modern physics. What is usually the subject of science
fiction is here proposed as a scientific theory open to empirical evaluation.
We know from physics how the world behaves, and from computing how information
behaves, so whether the physical world arises from ongoing information
processing is a question science can evaluate. A prima facie case for the
virtual reality conjecture is presented. If a photon is a pixel on a
multi-dimensional grid that gives rise to space, the speed of light could
reflect its refresh rate. If mass, charge and energy all arise from processing,
the many conservation laws of physics could reduce to a single law of dynamic
information conservation. If the universe is a virtual reality, then its big
bang creation could be simply when the system was booted up. Deriving core
physics from information processing could reconcile relativity and quantum
theory, with the former how processing creates the space-time operating system
and the latter how it creates energy and matter applications.",['B. Whitworth'],2010-08-15T05:10:38Z,http://arxiv.org/abs/1011.3436v1,['cs.OH'],"physical world,information processing,virtual reality,modern physics,science fiction,scientific theory,empirical evaluation,photon,multi-dimensional grid,speed of light,conservation laws,dynamic information conservation,universe,big bang creation,relativity,quantum theory,space-time operating system,energy,matter applications"
User Interface for Volume Rendering in Virtual Reality Environments,"Volume Rendering applications require sophisticated user interaction for the
definition and refinement of transfer functions. Traditional 2D desktop user
interface elements have been developed to solve this task, but such concepts do
not map well to the interaction devices available in Virtual Reality
environments.
  In this paper, we propose an intuitive user interface for Volume Rendering
specifically designed for Virtual Reality environments. The proposed interface
allows transfer function design and refinement based on intuitive two-handed
operation of Wand-like controllers. Additional interaction modes such as
navigation and clip plane manipulation are supported as well.
  The system is implemented using the Sony PlayStation Move controller system.
This choice is based on controller device capabilities as well as application
and environment constraints.
  Initial results document the potential of our approach.","['Jonathan Klein', 'Dennis Reuling', 'Jan Grimm', 'Andreas Pfau', 'Damien Lefloch', 'Martin Lambers', 'Andreas Kolb']",2013-02-08T13:11:39Z,http://arxiv.org/abs/1302.2024v1,"['cs.GR', 'cs.HC']","Volume Rendering,User Interface,Virtual Reality,Transfer Functions,Interaction Devices,Wand-like Controllers,Navigation,Clip Plane Manipulation,PlayStation Move,User Interaction"
"Post-processing of Engineering Analysis Results for Visualization in VR
  Systems","The applicability of Virtual Reality for evaluating engineering analysis
results is beginning to receive increased appreciation in the last years. The
problem many engineers are still facing is how to import their model together
with the analysis results in a virtual reality environment for exploration and
results validation. In this paper we propose an algorithm for transforming
model data and results from finite element analysis (FEA) solving application
to a format easily interpretable by a virtual reality application. The
algorithm includes also steps for reducing the face-count of the resulting mesh
by eliminating faces from the inner part of the model in the cases when only
the surfaces of the model is analyzed. We also describe a possibility for
simultaneously assessing multiple analysis results relying on multimodal
results presentation by stimulating different senses of the operator.","['Stoyan Maleshkov', 'Dimo Chotrov']",2013-08-27T12:47:34Z,http://arxiv.org/abs/1308.5847v1,['cs.GR'],"Virtual Reality,Engineering Analysis,Post-processing,Visualization,VR Systems,Finite Element Analysis,FEA,Mesh,Algorithm,Multimodal Presentation"
"A Collaborative Untethered Virtual Reality Environment for Interactive
  Social Network Visualization","The increasing prevalence of Virtual Reality technologies as a platform for
gaming and video playback warrants research into how to best apply the current
state of the art to challenges in data visualization. Many current VR systems
are noncollaborative, while data analysis and visualization is often a
multi-person process. Our goal in this paper is to address the technical and
user experience challenges that arise when creating VR environments for
collaborative data visualization. We focus on the integration of multiple
tracking systems and the new interaction paradigms that this integration can
enable, along with visual design considerations that apply specifically to
collaborative network visualization in virtual reality. We demonstrate a system
for collaborative interaction with large 3D layouts of Twitter friend/follow
networks. The system is built by combining a 'Holojam' architecture (multiple
GearVR Headsets within an OptiTrack motion capture stage) and Perception Neuron
motion suits, to offer an untethered, full-room multi-person visualization
experience.","['Sam Royston', 'Connor DeFanti', 'Ken Perlin']",2016-04-27T20:54:37Z,http://arxiv.org/abs/1604.08239v1,"['cs.HC', 'cs.GR']","Virtual Reality,Social Network Visualization,Collaborative,Data Visualization,Tracking Systems,Interaction Paradigms,Visual Design,Network Visualization,Motion Capture,Multi-person"
Virtual Reality: Blessings and Risk Assessment,"Objectives: This paper presents an up-to-date overview of research performed
in the Virtual Reality (VR) environment ranging from definitions, its presence
in the various fields, and existing market players and their projects in the VR
technology. Further an attempt is made to gain an insight on the psychological
mechanism underlying experience in using VR device. Methods: Our literature
survey is based on the research articles, analysis of the projects of various
companies and their findings for different areas of interest. Findings: In our
literature survey we observed that the recent advances in virtual reality
enabling technologies have led to variety of virtual devices that facilitate
people to interact with the digital world. In fact in the past two decades
researchers have tried to integrate reality and VR in the form of intuitive
computer interface. Improvements: This has led to variety of potential benefits
of VR in many applications such as News, Healthcare, Entertainment, Tourism,
Military and Defence etc. However despite the extensive research efforts in
creating virtual system environments it is yet to become apparent in normal
daily life.","['Ayush Sharma', 'Piyush Bajpai', 'Sukhdev Singh', 'Kiran Khatter']",2017-08-31T02:34:22Z,http://arxiv.org/abs/1708.09540v1,['cs.HC'],"Virtual Reality,Research,Technology,Psychological Mechanism,Literature Survey,Virtual Devices,Computer Interface,Applications,Virtual System Environments,Market Players"
"Machine learning architectures to predict motion sickness using a
  Virtual Reality rollercoaster simulation tool","Virtual Reality (VR) can cause an unprecedented immersion and feeling of
presence yet a lot of users experience motion sickness when moving through a
virtual environment. Rollercoaster rides are popular in Virtual Reality but
have to be well designed to limit the amount of nausea the user may feel. This
paper describes a novel framework to get automated ratings on motion sickness
using Neural Networks. An application that lets users create rollercoasters
directly in VR, share them with other users and ride and rate them is used to
gather real-time data related to the in-game behaviour of the player, the track
itself and users' ratings based on a Simulator Sickness Questionnaire (SSQ)
integrated into the application. Machine learning architectures based on deep
neural networks are trained using this data aiming to predict motion sickness
levels. While this paper focuses on rollercoasters this framework could help to
rate any VR application on motion sickness and intensity that involves camera
movement. A new well defined dataset is provided in this paper and the
performance of the proposed architectures are evaluated in a comparative study.","['Stefan Hell', 'Vasileios Argyriou']",2018-11-02T22:02:40Z,http://arxiv.org/abs/1811.01106v1,"['cs.HC', 'cs.AI']","Virtual Reality,motion sickness,Neural Networks,rollercoasters,Simulator Sickness Questionnaire,deep neural networks,dataset,machine learning architectures,predictive modeling,camera movement"
Brain-Computer Interface in Virtual Reality,"We study the performance of brain computer interface (BCI) system in a
virtual reality (VR) environment and compare it to 2D regular displays. First,
we design a headset that consists of three components: a wearable
electroencephalography (EEG) device, a VR headset and an interface. Recordings
of brain and behavior from human subjects, performing a wide variety of tasks
using our device are collected. The tasks consist of object rotation or scaling
in VR using either mental commands or facial expression (smile and eyebrow
movement). Subjects are asked to repeat similar tasks on regular 2D monitor
screens. The performance in 3-D virtual reality environment is considerably
higher compared to the to the 2D screen. Particularly, the median number of
success rate across trials for VR setting is double of that for the 2D setting
(8 successful command in VR setting compared to 4 successful command in 2D
screen in 1 minute trials). Our results suggest that the design of future BCI
systems can remarkably benefit from the VR setting.","['Reza Abbasi-Asl', 'Mohammad Keshavarzi', 'Dorian Yao Chan']",2018-11-13T15:52:21Z,http://arxiv.org/abs/1811.06040v1,['cs.HC'],"Brain-Computer Interface,Virtual Reality,Electroencephalography,EEG,3-D environment,Interface,Human subjects,Mental commands,Facial expression"
Indy: a virtual reality multi-player game for navigation skills training,"Working in complex industrial facilities requires spatial navigation skills
that people build up with time and field experience. Training sessions
consisting in guided tours help discover places but they are insufficient to
become intimately familiar with their layout. They imply passive learning
postures, are time-limited and can be experienced only once because of
organization constraints and potential interferences with ongoing activities in
the buildings. To overcome these limitations and improve the acquisition of
navigation skills, we developed Indy, a virtual reality system consisting in a
collaborative game of treasure hunting. It has several key advantages: it
focuses learners' attention on navigation tasks, implies their active
engagement and provides them with feedbacks on their achievements. Virtual
reality makes it possible to multiply the number and duration of situations
that learners can experience to better consolidate their skills. This paper
discusses the main design principles and a typical usage scenario of Indy.","['Arnaud Mas', 'Idriss Ismaël', 'Nicolas Filliard']",2018-07-11T15:13:06Z,http://arxiv.org/abs/1807.04184v1,"['cs.HC', 'cs.GR']","virtual reality,multi-player game,navigation skills training,spatial navigation,training sessions,collaborative game,treasure hunting,feedbacks,virtual reality system"
Virtual Reality as a Teaching Tool for Moon Phases and Beyond,"A ball on a stick is a common and simple activity for teaching the phases of
the Moon. This activity, like many others in physics and astronomy, gives
students a perspective they otherwise could only imagine. For Moon phases, a
third person view and control over time allows students to rapidly build a
mental model that connects all the moving parts. Computer simulations of many
traditional physics and astronomy activities provide new features, controls, or
vantage points to enhance learning beyond a hands-on activity. Virtual reality
provides the capabilities of computer simulations and embodied cognition
experiences through a hands-on activity making it a natural step to improve
learning. We recreated the traditional ball-and-stick moon phases activity in
virtual reality and compared participant learning using this simulation with
using traditional methods. We found a strong participant preference for VR
relative to the traditional methods. However, we observed no difference across
conditions in average levels of performance on a pre/post knowledge test.","['J. H. Madden', 'A. S. Won', 'J. P. Schuldt', 'B. Kim', 'S. Pandita', 'Y. Sun', 'T. J. Stone', 'N. G. Holmes']",2018-07-30T05:37:06Z,http://arxiv.org/abs/1807.11179v2,['physics.ed-ph'],"Virtual reality,Teaching tool,Moon phases,Physics,Astronomy,Computer simulations,Embodied cognition,Learning,Traditional methods"
Gaze-Contingent Ocular Parallax Rendering for Virtual Reality,"Immersive computer graphics systems strive to generate perceptually realistic
user experiences. Current-generation virtual reality (VR) displays are
successful in accurately rendering many perceptually important effects,
including perspective, disparity, motion parallax, and other depth cues. In
this article, we introduce ocular parallax rendering, a technology that
accurately renders small amounts of gaze-contingent parallax capable of
improving depth perception and realism in VR. Ocular parallax describes the
small amounts of depth-dependent image shifts on the retina that are created as
the eye rotates. The effect occurs because the centers of rotation and
projection of the eye are not the same. We study the perceptual implications of
ocular parallax rendering by designing and conducting a series of user
experiments. Specifically, we estimate perceptual detection and discrimination
thresholds for this effect and demonstrate that it is clearly visible in most
VR applications. Additionally, we show that ocular parallax rendering provides
an effective ordinal depth cue and it improves the impression of realistic
depth in VR.","['Robert Konrad', 'Anastasios Angelopoulos', 'Gordon Wetzstein']",2019-06-24T06:11:30Z,http://arxiv.org/abs/1906.09740v2,"['cs.GR', 'cs.HC', 'J.4; I.3.7; H.5.1']","Immersive computer graphics,Virtual reality (VR),Ocular parallax rendering,Gaze-contingent,Depth perception,Realism,Motion parallax,Depth cues,User experiments,Depth cue"
RadVR: A 6DOF Virtual Reality Daylighting Analysis Tool,"This work introduces RadVR, a virtual reality tool for daylighting analysis
that simultaneously combines qualitative assessments through immersive
real-time renderings with quantitative physically correct daylighting
simulations in a 6DOF virtual environment. By taking a 3D building model with
material properties as input, RadVR allows users to (1) perform
physically-based daylighting simulations via Radiance, (2) study sunlight in
different hours-of-the-year, (3) interact with a 9-point-in-time matrix for the
most representative times of the year, and (4) visualize, compare, and analyze
daylighting simulation results. With an end-to-end workflow, RadVR integrates
with 3D modeling software that is commonly used by building designers.
Additionally, by conducting user experiments we compare the proposed system
with DIVA for Rhino, a Radiance-based tool that uses conventional 2D-displays.
The results show that RadVR can provide promising assistance in spatial
understanding tasks, navigation, and sun position analysis in virtual reality.","['Mohammad Keshavarzi', 'Luisa Caldas', 'Luis Santos']",2019-07-02T21:15:02Z,http://arxiv.org/abs/1907.01652v2,"['cs.GR', 'cs.HC']","6DOF,Virtual Reality,Daylighting Analysis,Radiance,3D Building Model,Physically-Based Simulations,Sunlight,End-to-End Workflow,User Experiments,Spatial Understanding"
"A Short Virtual Reality Mindfulness Meditation Training For Regaining
  Sustained Attention","The ability to focus one's attention underlies success in many everyday
tasks, but voluntary attention cannot be sustained for a long period of time.
Several studies indicate that attention training using computer-based exercises
can lead to improved attention in children and adults. a major goal of recent
research is to create a short (10 minutes) and effective VR Mindfulness
meditation particularly designed for regaining or improving sustained
attention. In this study, we have created a custom virtually relaxing
environment including an archery game with multiple targets. In the experiment,
the attention span of 12 adults are tested before and after the virtual reality
session by a non-action video game ([19]) score and Muse headband EEG-signals.
After the 10-minute virtual reality session participants' game scores increased
(according to game experience): for the beginner by 275%, for intermediate by
107%, and for an expert by 17%. For Muse headband data, calm points increased
by 250% irrespective of the participants gaming experiences. After the
experiment, all participants reported feeling recharged to continue their daily
activities.","['Minkesh Asati', 'Taizo Miyachi']",2019-07-10T02:18:44Z,http://arxiv.org/abs/1907.04487v1,['cs.HC'],"virtual reality,mindfulness meditation,sustained attention,attention training,computer-based exercises,attention span,EEG signals,Muse headband,attention improvement,attention regeneration"
"Development of MirrorShape: High Fidelity Large-Scale Shape Rendering
  Framework for Virtual Reality","Today there is a high variety of haptic devices capable of providing tactile
feedback. Although most of existing designs are aimed at realistic simulation
of the surface properties, their capabilities are limited in attempts of
displaying shape and position of virtual objects. This paper suggests a new
concept of distributed haptic display for realistic interaction with virtual
object of complex shape by a collaborative robot with shape display
end-effector. MirrorShape renders the 3D object in virtual reality (VR) system
by contacting the user hands with the robot end-effector at the calculated
point in real-time. Our proposed system makes it possible to synchronously
merge the position of contact point in VR and end-effector in real world. This
feature provides presentation of different shapes, and at the same time expands
the working area comparing to desktop solutions. The preliminary user study
revealed that MirrorShape was effective at reducing positional error in VR
interactions. Potentially this approach can be used in the virtual systems for
rendering versatile VR objects with wide range of sizes with high fidelity
large-scaleshape experience.","['Aleksey Fedoseev', 'Nikita Chernyadev', 'Dzmitry Tsetserukou']",2019-11-18T03:35:44Z,http://arxiv.org/abs/1911.07408v1,['cs.RO'],"haptic devices,tactile feedback,shape rendering,virtual reality,distributed haptic display,collaborative robot,end-effector,3D object,positional error,large-scale shape"
Exploring the Front Touch Interface for Virtual Reality Headsets,"In this paper, we propose a new interface for virtual reality headset: a
touchpad in front of the headset. To demonstrate the feasibility of the front
touch interface, we built a prototype device, explored VR UI design space
expansion, and performed various user studies. We started with preliminary
tests to see how intuitively and accurately people can interact with the front
touchpad. Then, we further experimented various user interfaces such as a
binary selection, a typical menu layout, and a keyboard. Two-Finger and
Drag-n-Tap were also explored to find the appropriate selection technique. As a
low-cost, light-weight, and in low power budget technology, a touch sensor can
make an ideal interface for mobile headset. Also, front touch area can be large
enough to allow wide range of interaction types such as multi-finger
interactions. With this novel front touch interface, we paved a way to new
virtual reality interaction methods.","['Jihyun Lee', 'Byungmoon Kim', 'Bongwon Suh', 'Eunyee Koh']",2016-08-01T14:34:50Z,http://arxiv.org/abs/1608.00447v1,['cs.HC'],"virtual reality,headset,touchpad,VR UI design,user studies,touch sensor,interaction types,user interface,selection technique,front touch interface"
"Baxter's Homunculus: Virtual Reality Spaces for Teleoperation in
  Manufacturing","Expensive specialized systems have hampered development of telerobotic
systems for manufacturing systems. In this paper we demonstrate a telerobotic
system which can reduce the cost of such system by leveraging commercial
virtual reality(VR) technology and integrating it with existing robotics
control software. The system runs on a commercial gaming engine using off the
shelf VR hardware. This system can be deployed on multiple network
architectures from a wired local network to a wireless network connection over
the Internet. The system is based on the homunculus model of mind wherein we
embed the user in a virtual reality control room. The control room allows for
multiple sensor display, dynamic mapping between the user and robot, does not
require the production of duals for the robot, or its environment. The control
room is mapped to a space inside the robot to provide a sense of co-location
within the robot. We compared our system with state of the art automation
algorithms for assembly tasks, showing a 100% success rate for our system
compared with a 66% success rate for automated systems. We demonstrate that our
system can be used for pick and place, assembly, and manufacturing tasks.","['Jeffrey I Lipton', 'Aidan J Fay', 'Daniela Rus']",2017-03-03T18:03:26Z,http://arxiv.org/abs/1703.01270v1,['cs.RO'],"virtual reality,teleoperation,manufacturing,telerobotic systems,robotics control software,gaming engine,network architectures,homunculus model,sensor display"
"syGlass: Interactive Exploration of Multidimensional Images Using
  Virtual Reality Head-mounted Displays","The quest for deeper understanding of biological systems has driven the
acquisition of increasingly larger multidimensional image datasets. Inspecting
and manipulating data of this complexity is very challenging in traditional
visualization systems. We developed syGlass, a software package capable of
visualizing large scale volumetric data with inexpensive virtual reality
head-mounted display technology. This allows leveraging stereoscopic vision to
significantly improve perception of complex 3D structures, and provides
immersive interaction with data directly in 3D. We accomplished this by
developing highly optimized data flow and volume rendering pipelines, tested on
datasets up to 16TB in size, as well as tools available in a virtual reality
GUI to support advanced data exploration, annotation, and cataloguing.","['Stanislav Pidhorskyi', 'Michael Morehead', 'Quinn Jones', 'George Spirou', 'Gianfranco Doretto']",2018-04-23T00:04:54Z,http://arxiv.org/abs/1804.08197v4,"['cs.GR', 'cs.CV']","multidimensional images,virtual reality,head-mounted displays,volumetric data,volume rendering,stereoscopic vision,immersive interaction,data exploration,annotation,cataloguing"
"Some Experimental Results of Relieving Discomfort in Virtual Reality by
  Disturbing Feedback Loop in Human Brain","Recently, great progress has been made in virtual reality(VR) research and
application. However, virtual reality faces a big problem since its appearance,
i.e. discomfort (nausea, stomach awareness, etc). Discomfort can be relieved by
increasing hardware (sensor, cpu and display) speed. But this will increase
cost. This paper gives another low cost solution. The phenomenon of
cybersickness is explained with the control theory: discomfort arises if
feedback scene differs from expectation, so it can be relieved by disturbing
feedback loop in human brain. A hardware platform is build to test this
explanation. The VR display on a Samsung S6 is blurred while head movement is
detected. The effect is evaluated by comparing responses to the Simulated
Sickness Questionnaire (SSQ) between a control and experimental condition.
Experimental results show that the new method can ease discomfort remarkably
with little extra cost. As a result, VR may be used more widely in teaching
(like foreign language, medicine). It's also reasonable to expect likewise
merits in other VR applications.","['Wei Qionghua', 'Wang Hui', 'Wei Qiang']",2019-03-19T01:59:29Z,http://arxiv.org/abs/1903.12617v1,['cs.HC'],"virtual reality,discomfort,feedback loop,cybersickness,hardware platform,head movement,Simulated Sickness Questionnaire,experimental results,teaching,VR applications"
"Enabling Humans to Plan Inspection Paths Using a Virtual Reality
  Interface","In this work, we investigate whether humans can manually generate
high-quality robot paths for optical inspections. Typically, automated
algorithms are used to solve the inspection planning problem. The use of
automated algorithms implies that specialized knowledge from users is needed to
set up the algorithm. We aim to replace this need for specialized experience,
by entrusting a non-expert human user with the planning task. We augment this
user with intuitive visualizations and interactions in virtual reality. To
investigate if humans can generate high-quality inspection paths, we perform a
user study in which users from different experience categories, generate
inspection paths with the proposed virtual reality interface. From our study,
it can be concluded that users without experience can generate high-quality
inspection paths: The median inspection quality of user generated paths ranged
between 66-81\% of the quality of a state-of-the-art automated algorithm on
various inspection planning scenarios. We noticed however, a sizable variation
in the performance of users, which is a result of some typical user behaviors.
These behaviors are discussed, and possible solutions are provided.","['Boris Bogaerts', 'Seppe Sels', 'Steve Vanlanduit', 'Rudi Penne']",2019-09-13T08:15:36Z,http://arxiv.org/abs/1909.06077v1,"['cs.RO', 'cs.HC']","virtual reality,inspection paths,robot paths,automated algorithms,user study,visualizations,interactions,inspection quality,experience categories"
Keep It Real: a Window to Real Reality in Virtual Reality,"This paper proposed a new interaction paradigm in the virtual reality (VR)
environments, which consists of a virtual mirror or window projected onto a
virtual surface, representing the correct perspective geometry of a mirror or
window reflecting the real world. This technique can be applied to various
videos, live streaming apps, augmented and virtual reality settings to provide
an interactive and immersive user experience. To support such a
perspective-accurate representation, we implemented computer vision algorithms
for feature detection and correspondence matching. To constrain the solutions,
we incorporated an automatically tuning scaling factor upon the homography
transform matrix such that each image frame follows a smooth transition with
the user in sight. The system is a real-time rendering framework where users
can engage their real-life presence with the virtual space.",['Baihan Lin'],2020-04-21T21:33:14Z,http://arxiv.org/abs/2004.10313v3,"['cs.HC', 'cs.CV']","virtual reality,interaction paradigm,virtual mirror,window,computer vision algorithms,feature detection,correspondence matching,homography transform matrix,real-time rendering,immersive experience"
"Eye Tracking Data Collection Protocol for VR for Remotely Located
  Subjects using Blockchain and Smart Contracts","Eye tracking data collection in the virtual reality context is typically
carried out in laboratory settings, which usually limits the number of
participants or consumes at least several months of research time. In addition,
under laboratory settings, subjects may not behave naturally due to being
recorded in an uncomfortable environment. In this work, we propose a
proof-of-concept eye tracking data collection protocol and its implementation
to collect eye tracking data from remotely located subjects, particularly for
virtual reality using Ethereum blockchain and smart contracts. With the
proposed protocol, data collectors can collect high quality eye tracking data
from a large number of human subjects with heterogeneous socio-demographic
characteristics. The quality and the amount of data can be helpful for various
tasks in data-driven human-computer interaction and artificial intelligence.","['Efe Bozkir', 'Shahram Eivazi', 'Mete Akgün', 'Enkelejda Kasneci']",2020-10-23T17:54:38Z,http://arxiv.org/abs/2010.12570v3,['cs.HC'],"eye tracking,data collection,virtual reality,remotely located subjects,blockchain,smart contracts,Ethereum,human subjects,socio-demographic characteristics,data-driven"
"Effect of Gameplay Uncertainty, Display Type, and Age on Virtual Reality
  Exergames","Uncertainty is widely acknowledged as an engaging gameplay element but rarely
used in exergames. In this research, we explore the role of uncertainty in
exergames and introduce three uncertain elements (false-attacks, misses, and
critical hits) to an exergame. We conducted a study under two conditions
(uncertain and certain), with two display types (virtual reality and large
display) and across young and middle-aged adults to measure their effect on
game performance, experience, and exertion. Results show that (1) our designed
uncertain elements are instrumental in increasing exertion levels; (2) when
playing a motion-based first-person perspective exergame, virtual reality can
improve performance, while maintaining the same motion sickness level as a
large display; and (3) exergames for middle-aged adults should be designed with
age-related declines in mind, similar to designing for elderly adults. We also
framed two design guidelines for exergames that have similar features to the
game used in this research.","['Wenge Xu', 'Hai-Ning Liang', 'Kangyou Yu', 'Nilufar Baghaei']",2021-01-15T14:03:50Z,http://arxiv.org/abs/2101.06120v1,['cs.HC'],"gameplay uncertainty,display type,age,virtual reality,exergames,false-attacks,misses,critical hits,game performance,motion sickness"
"""Can I Touch This?"": Survey of Virtual Reality Interactions via Haptic
  Solutions","Haptic feedback has become crucial to enhance the user experiences in Virtual
Reality (VR). This justifies the sudden burst of novel haptic solutions
proposed these past years in the HCI community. This article is a survey of
Virtual Reality interactions, relying on haptic devices. We propose two
dimensions to describe and compare the current haptic solutions: their degree
of physicality, as well as their degree of actuation. We depict a compromise
between the user and the designer, highlighting how the range of required or
proposed stimulation in VR is opposed to the haptic interfaces flexibility and
their deployment in real-life use-cases. This paper (1) outlines the variety of
haptic solutions and provides a novel perspective for analysing their
associated interactions, (2) highlights the limits of the current evaluation
criteria regarding these interactions, and finally (3) reflects the
interaction, operation and conception potentials of ""encountered-type of haptic
devices"".","['Elodie Bouzbib', 'Gilles Bailly', 'Sinan Haliyo', 'Pascal Frey']",2021-01-27T09:16:17Z,http://arxiv.org/abs/2101.11278v1,['cs.HC'],"Haptic feedback,Virtual Reality,Interactions,Haptic solutions,HCI community,Physicality,Actuation,Stimulation,Haptic interfaces,Evaluation criteria"
"The Impact of Virtual Reality and Viewpoints in Body Motion Based Drone
  Teleoperation","The operation of telerobotic systems can be a challenging task, requiring
intuitive and efficient interfaces to enable inexperienced users to attain a
high level of proficiency. Body-Machine Interfaces (BoMI) represent a promising
alternative to standard control devices, such as joysticks, because they
leverage intuitive body motion and gestures. It has been shown that the use of
Virtual Reality (VR) and first-person view perspectives can increase the user's
sense of presence in avatars. However, it is unclear if these beneficial
effects occur also in the teleoperation of non-anthropomorphic robots that
display motion patterns different from those of humans. Here we describe
experimental results on teleoperation of a non-anthropomorphic drone showing
that VR correlates with a higher sense of spatial presence, whereas viewpoints
moving coherently with the robot are associated with a higher sense of
embodiment. Furthermore, the experimental results show that spontaneous body
motion patterns are affected by VR and viewpoint conditions in terms of
variability, amplitude, and robot correlates, suggesting that the design of
BoMIs for drone teleoperation must take into account the use of Virtual Reality
and the choice of the viewpoint.","['Matteo Macchini', 'Manana Lortkipanidze', 'Fabrizio Schiano', 'Dario Floreano']",2021-01-30T13:33:21Z,http://arxiv.org/abs/2102.00226v1,"['cs.RO', 'cs.HC']","Virtual Reality,Viewpoints,Body-Machine Interfaces,Drone,Teleoperation,Spatial presence,Embodiment,Body motion,Experimental results,Robot"
"An ns-3 Implementation of a Bursty Traffic Framework for Virtual Reality
  Sources","Next-generation wireless communication technologies will allow users to
obtain unprecedented performance, paving the way to new and immersive
applications. A prominent application requiring high data rates and low
communication delay is Virtual Reality (VR), whose presence will become
increasingly stronger in the years to come. To the best of our knowledge, we
propose the first traffic model for VR applications based on traffic traces
acquired from a commercial VR streaming software, allowing the community to
further study and improve the technology to manage this type of traffic. This
work implements ns-3 applications able to generate and process large bursts of
packets, enabling the possibility of analyzing APP-level end-to-end metrics,
making the source code as well as the acquired VR traffic traces publicly
available and open-source.","['Mattia Lecci', 'Andrea Zanella', 'Michele Zorzi']",2021-03-08T08:59:58Z,http://arxiv.org/abs/2103.04609v2,"['cs.NI', 'cs.MM']","ns-3,Bursty Traffic Framework,Virtual Reality,traffic model,VR applications,traffic traces,APP-level,end-to-end metrics,source code,open-source."
Implementing Virtual Reality for Teleoperation of a Humanoid Robot,"Our research explores the potential of a humanoid robot for work in
unpredictable environments, but controlling a humanoid robot remains a very
difficult problem. In our previous work, we designed a prototype virtual
reality (VR) interface to allow an operator to command a humanoid robot.
However, while usable, the initial interface was not sufficient for commanding
the robot to perform the tasks; for example, in some cases, there was a lack of
precision available for robot control. The interface was overly cumbersome in
some areas as well. In this paper, we discuss numerous additions, inspired by
traditional interfaces and virtual reality video games, to our prior
implementation, providing additional ways to visualize and command a humanoid
robot to perform difficult tasks within a virtual world.","['Jordan Allspaw', 'Gregory LeMasurier', 'Holly Yanco']",2021-04-23T21:44:25Z,http://arxiv.org/abs/2104.11826v1,"['cs.RO', 'I.2.9']","Humanoid robot,Virtual reality,Teleoperation,Interface,Precision,Control,Visualization,Virtual world"
The Efficacy of a Virtual Reality-Based Mindfulness Intervention,"Mindfulness can be defined as increased awareness of and sustained
attentiveness to the present moment. Recently, there has been a growing
interest in the applications of mindfulness for empirical research in wellbeing
and the use of virtual reality (VR) environments and 3D interfaces as a conduit
for mindfulness training. Accordingly, the current experiment investigated
whether a brief VR-based mindfulness intervention could induce a greater level
of state mindfulness, when compared to an audio-based intervention and control
group. Results indicated two mindfulness interventions, VR-based and
audio-based, induced a greater state of mindfulness, compared to the control
group. Participants in the VR-based mindfulness intervention group reported a
greater state of mindfulness than those in the guided audio group, indicating
the immersive mindfulness intervention was more robust. Collectively, these
results provide empirical support for the efficaciousness of a brief VR-based
mindfulness intervention in inducing a robust state of mindfulness in
laboratory settings.","['Caglar Yildirim', 'Tara OGrady']",2021-05-22T16:12:06Z,http://arxiv.org/abs/2105.10756v1,['cs.HC'],"virtual reality,mindfulness intervention,state mindfulness,3D interfaces,empirical research,wellbeing,audio-based intervention,control group,laboratory settings"
"Real vs Simulated Foveated Rendering to Reduce Visual Discomfort in
  Virtual Reality","In this paper, a study aimed at investigating the effects of real (using eye
tracking to determine the fixation) and simulated foveated blurring in
immersive Virtual Reality is presented. Techniques to reduce the optical flow
perceived at the visual field margins are often employed in immersive Virtual
Reality environments to alleviate discomfort experienced when the visual motion
perception does not correspond to the body's acceleration. Although still
preliminary, our results suggest that for participants with higher
self-declared sensitivity to sickness, there might be an improvement for nausea
when using blurring. The (perceived) difficulty of the task seems to improve
when the real foveated method is used.","['Ariel Caputo', 'Andrea Giachetti', 'Salwa Abkal', 'Chiara Marchesini', 'Massimo Zancanaro']",2021-07-04T15:54:41Z,http://arxiv.org/abs/2107.01669v1,['cs.HC'],"Real Foveated Rendering,Simulated Foveated Rendering,Virtual Reality,Visual Discomfort,Eye Tracking,Immersive Virtual Reality,Nausea,Optical Flow,Visual Motion Perception"
"AIive: Interactive Visualization and Sonification of Neural Networks in
  Virtual Reality","Artificial Intelligence (AI), especially Neural Networks (NNs), has become
increasingly popular. However, people usually treat AI as a tool, focusing on
improving outcome, accuracy, and performance while paying less attention to the
representation of AI itself. We present AIive, an interactive visualization of
AI in Virtual Reality (VR) that brings AI ""alive"". AIive enables users to
manipulate the parameters of NNs with virtual hands and provides auditory
feedback for the real-time values of loss, accuracy, and hyperparameters. Thus,
AIive contributes an artistic and intuitive way to represent AI by integrating
visualization, sonification, and direct manipulation in VR, potentially
targeting a wide range of audiences.","['Zhuoyue Lyu', 'Jiannan Li', 'Bryan Wang']",2021-09-30T15:07:02Z,http://arxiv.org/abs/2109.15193v2,['cs.HC'],"Artificial Intelligence,Neural Networks,Interactive Visualization,Sonification,Virtual Reality,Parameters,Hyperparameters,Accuracy,Performance,Auditory Feedback"
VIRUP : The Virtual Reality Universe Project,"VIRUP is a new C++ open source software that provides an interactive virtual
reality environment to navigate through large scientific astrophysical datasets
obtained from both observations and simulations. It is tailored to visualize
terabytes of data, rendering at 90 frames per second in order to ensure an
optimal immersion experience. While VIRUP has initially been designed to work
with gaming virtual reality headsets, it supports different modern immersive
systems like 3D screens, 180 deg. domes or 360 deg. panorama. VIRUP is
scriptable thanks to the Python language, a feature that allows to immerse
visitors through pre-selected scenes or to pre-render sequences to create
movies. A companion video (https://www.youtube.com/watch?v=KJJXbcf8kxA) to the
last SDSS 2020 release as well as a 21 minute long documentary, The Archaeology
of Light, https://go.epfl.ch/ArchaeologyofLight have been both 100% produced
using VIRUP.","['Florian Cabot', 'Yves Revaz', 'Jean-Paul Kneib', 'Hadrien Gurnel', 'Sarah Kenderdine']",2021-10-08T18:00:02Z,http://arxiv.org/abs/2110.04308v1,"['astro-ph.IM', 'cs.GR', 'cs.HC', 'physics.ed-ph']","C++,open source software,virtual reality,astrophysical datasets,simulations,rendering,frames per second,immersive systems,Python,scriptable"
Ubiq: A System to Build Flexible Social Virtual Reality Experiences,"While they have long been a subject of academic study, social virtual reality
(SVR) systems are now attracting increasingly large audiences on current
consumer virtual reality systems. The design space of SVR systems is very
large, and relatively little is known about how these systems should be
constructed in order to be usable and efficient. In this paper we present Ubiq,
a toolkit that focuses on facilitating the construction of SVR systems. We
argue for the design strategy of Ubiq and its scope. Ubiq is built on the Unity
platform. It provides core functionality of many SVR systems such as connection
management, voice, avatars, etc. However, its design remains easy to extend. We
demonstrate examples built on Ubiq and how it has been successfully used in
classroom teaching. Ubiq is open source (Apache License) and thus enables
several use cases that commercial systems cannot.","['Sebastian Friston', 'Ben Congdon', 'David Swapp', 'Lisa Izzouzi', 'Klara Brandstätter', 'Daniel Archer', 'Otto Olkkonen', 'Felix Thiel', 'Anthony Steed']",2021-12-16T12:49:06Z,http://arxiv.org/abs/2112.08842v1,['cs.HC'],"social virtual reality,Ubiq,toolkit,SVR systems,Unity platform,connection management,voice,avatars,design strategy,classroom teaching"
"ReViVD: Exploration and Filtering of Trajectories in an Immersive
  Environment using 3D Shapes","We present ReViVD, a tool for exploring and filtering large trajectory-based
datasets using virtual reality. ReViVD's novelty lies in using simple 3D shapes
-- such as cuboids, spheres and cylinders -- as queries for users to select and
filter groups of trajectories. Building on this simple paradigm, more complex
queries can be created by combining previously made selection groups through a
system of user-created Boolean operations. We demonstrate the use of ReViVD in
different application domains, from GPS position tracking to simulated data
(e.g., turbulent particle flows and traffic simulation). Our results show the
ease of use and expressiveness of the 3D geometric shapes in a broad range of
exploratory tasks. ReViVD was found to be particularly useful for progressively
refining selections to isolate outlying behaviors. It also acts as a powerful
communication tool for conveying the structure of normally abstract datasets to
an audience.","['François Homps', 'Yohan Beugin', 'Romain Vuillemot']",2022-02-21T21:58:41Z,http://arxiv.org/abs/2202.10545v1,"['cs.HC', 'cs.CV']","virtual reality,trajectories,3D shapes,filtering,exploration,Boolean operations,GPS tracking,particle flows,traffic simulation"
Identifying Fixation and Saccades in Virtual Reality,"Gaze recognition can significantly reduce the amount of eye movement data for
a better understanding of cognitive and visual processing. Gaze recognition is
an essential precondition for eye-based interaction applications in virtual
reality. However, the three-dimensional characteristics of virtual reality
environments also pose new challenges to existing recognition algorithms. Based
on seven evaluation metrics and the Overall score (the mean of the seven
normalized metric values), we obtain optimal parameters of three existing
recognition algorithms (Velocity-Threshold Identification, Dispersion-Threshold
Identification, and Velocity & Dispersion-Threshold Identification) and our
modified Velocity & Dispersion-Threshold Identification algorithm. We compare
the performance of these four algorithms with optimal parameters. The results
show that our modified Velocity & Dispersion-Threshold Identification performs
the best. The impact of interface complexity on classification results is also
preliminarily explored. The results show that the algorithms are not sensitive
to interface complexity.","['Xiao-lin Chen', 'Wen-jun Hou']",2022-05-09T08:40:20Z,http://arxiv.org/abs/2205.04121v1,"['cs.CV', 'cs.HC']","eye movement data,gaze recognition,virtual reality,three-dimensional characteristics,recognition algorithms,Velocity-Threshold Identification,Dispersion-Threshold Identification,interface complexity,classification results"
"Standing Balance Improvement Using Vibrotactile Feedback in Virtual
  Reality","Virtual Reality (VR) users often encounter postural instability, i.e.,
balance issues, which can be a significant impediment to universal usability
and accessibility, particularly for those with balance impairments. Prior
research has validated imbalance issues, but little effort has been made to
mitigate them. We recruited 39 participants (with balance impairments: 18,
without balance impairments: 21) to examine the effect of various vibrotactile
feedback techniques on balance in virtual reality, specifically spatial
vibrotactile, static vibrotactile, rhythmic vibrotactile, and vibrotactile
feedback mapped to the center of pressure (CoP). Participants completed
standing visual exploration and standing reach and grasp tasks. According to
within-subject results, each vibrotactile feedback enhanced balance in VR
significantly (p < .001) for those with and without balance impairments.
Spatial and CoP vibrotactile feedback enhanced balance significantly more (p <
.001) than other vibrotactile feedback. This study presents strategies that
might be used in future virtual environments to enhance standing balance and
bring VR closer to universal usage.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-08-18T22:31:28Z,http://arxiv.org/abs/2208.09082v1,['cs.HC'],"virtual reality,balance impairments,vibrotactile feedback,spatial vibrotactile,static vibrotactile,rhythmic vibrotactile,center of pressure,standing balance,universal usage"
"Security of Virtual Reality Authentication Methods in Metaverse: An
  Overview","The metaverse is said to be the future Internet and will consist of several
worlds called verses. This concept is being discussed a lot lately, however,
the security issues of these virtual worlds are not discussed enough. This
study first discusses the privacy and security concerns of the metaverse.
Virtual reality headsets are the main devices used to access the Metaverse. The
user needs to verify their identity to log in to the metaverse platforms, and
the security of this phase becomes vital. This paper aims to compare the
security of the main authentication methods that are used in virtual reality
environments. Information-based, biometric, and multi-model methods are
compared and analyzed in terms of security. These methods aim to verify the
user with different data types such as 3D patterns, PIN systems, or biometric
data. The pros and cons are discussed. The paper also concludes with what work
can be done to improve the safety of these authentication methods and future
work.","['Pınar Kürtünlüoğlu', 'Beste Akdik', 'Enis Karaarslan']",2022-09-14T06:42:32Z,http://arxiv.org/abs/2209.06447v1,"['cs.CR', 'H.m']","Virtual reality,Authentication methods,Metaverse,Security,Privacy,Virtual reality headsets,Identity verification,Information-based methods,Biometric methods,Multi-model methods"
"Reducing Stress and Anxiety in the Metaverse: A Systematic Review of
  Meditation, Mindfulness and Virtual Reality","Meditation, or mindfulness, is widely used to improve mental health. With the
emergence of Virtual Reality technology, many studies have provided evidence
that meditation with VR can bring health benefits. However, to our knowledge,
there are no guidelines and comprehensive reviews in the literature on how to
conduct such research in virtual reality. In order to understand the role of VR
technology in meditation and future research opportunities, we conducted a
systematic literature review in the IEEE and ACM databases. Our process yielded
19 eligible papers and we conducted a structured analysis. We understand the
state-of-art of meditation type, design consideration and VR and technology
through these papers and conclude research opportunities and challenges for the
future.","['Xian Wang', 'Xiaoyu Mo', 'Mingming Fan', 'Lik-Hang Lee', 'Bertram E. Shi', 'Pan Hui']",2022-09-29T09:08:27Z,http://arxiv.org/abs/2209.14645v1,['cs.HC'],"stress,anxiety,metaverse,meditation,mindfulness,virtual reality,systematic review,technology,mental health,literature"
"Using Immersive Virtual Reality to Enhance Social Interaction among
  Older Adults: A Multi-site Study","Research examining older adults interactions with Virtual Reality (VR) and
the impact of social VR experiences on outcomes such as social engagement has
been limited, especially among older adults. This multi-site pilot study
evaluated the feasibility and acceptability of a novel social virtual reality
(VR) program that paired older adults from different geographic locations (New
York City, Tallahassee, and Ithaca, N.Y) who engaged in virtual travel and
productive engagement activities together. The sample included 36 individuals
aged 60 and older, 25 percent of whom had cognitive impairment (CI). Older
adults with and without CI reported high levels of engagement in the VR
environment and perceived the social VR program to be enjoyable and usable.
Perceived Spatial Presence was a central driver of the positive outcomes. Most
also indicated a willingness to reconnect with their VR partner in the future.
The data also identified important areas for improvement in the program, such
as the use of more realistic and responsive avatars, controllers with larger
controls, and more time for training. Overall, these findings suggest that VR
social applications may foster social engagement among older adults.","['Saleh Kalantari', 'Tong Bill Xu', 'Armin Mostafavi', 'Andrew Dilanchian', 'Benjamin Kim', 'Walter Boot', 'Sara Czaja']",2022-10-10T18:47:58Z,http://arxiv.org/abs/2210.04954v1,['cs.HC'],"Immersive Virtual Reality,Social Interaction,Older Adults,Multi-site Study,Social Engagement,Virtual Travel,Cognitive Impairment,Spatial Presence,Avatars,Controllers"
"Investigating Input Modality and Task Geometry on Precision-first 3D
  Drawing in Virtual Reality","Accurately drawing non-planar 3D curves in immersive Virtual Reality (VR) is
indispensable for many precise 3D tasks. However, due to lack of physical
support, limited depth perception, and the non-planar nature of 3D curves, it
is challenging to adjust mid-air strokes to achieve high precision. Instead of
creating new interaction techniques, we investigated how task geometric shapes
and input modalities affect precision-first drawing performance in a
within-subject study (n = 12) focusing on 3D target tracing in commercially
available VR headsets. We found that compared to using bare hands, VR
controllers and pens yield nearly 30% of precision gain, and that the tasks
with large curvature, forward-backward or left-right orientations perform best.
We finally discuss opportunities for designing novel interaction techniques for
precise 3D drawing. We believe that our work will benefit future research
aiming to create usable toolboxes for precise 3D drawing.","['Chen Chen', 'Matin Yarmand', 'Zhuoqun Xu', 'Varun Singh', 'Yang Zhang', 'Nadir Weibel']",2022-10-21T21:56:43Z,http://arxiv.org/abs/2210.12270v1,"['cs.HC', 'cs.CY']","3D drawing,Input modality,Task geometry,Precision,Virtual Reality,VR controllers,Curvature,Interaction techniques,Depth perception,Toolboxes"
"ChromaCorrect: Prescription Correction in Virtual Reality Headsets
  through Perceptual Guidance","A large portion of today's world population suffer from vision impairments
and wear prescription eyeglasses. However, eyeglasses causes additional bulk
and discomfort when used with augmented and virtual reality headsets, thereby
negatively impacting the viewer's visual experience. In this work, we remedy
the usage of prescription eyeglasses in Virtual Reality (VR) headsets by
shifting the optical complexity completely into software and propose a
prescription-aware rendering approach for providing sharper and immersive VR
imagery. To this end, we develop a differentiable display and visual perception
model encapsulating display-specific parameters, color and visual acuity of
human visual system and the user-specific refractive errors. Using this
differentiable visual perception model, we optimize the rendered imagery in the
display using stochastic gradient-descent solvers. This way, we provide
prescription glasses-free sharper images for a person with vision impairments.
We evaluate our approach on various displays, including desktops and VR
headsets, and show significant quality and contrast improvements for users with
vision impairments.","['Ahmet Güzel', 'Jeanne Beyazian', 'Praneeth Chakravarthula', 'Kaan Akşit']",2022-12-08T13:30:17Z,http://arxiv.org/abs/2212.04264v1,"['cs.HC', 'cs.GR', 'cs.LG', 'I.3.3; I.2.10']","prescription correction,virtual reality headsets,perceptual guidance,prescription eyeglasses,visual impairments,rendering approach,visual perception model,refractive errors,stochastic gradient-descent solvers"
Velocity-Based LOD Reduction in Virtual Reality: A Psychometric Approach,"Virtual Reality headsets enable users to explore the environment by
performing self-induced movements. The retinal velocity produced by such motion
reduces the visual system's ability to resolve fine detail. We measured the
impact of self-induced head rotations on the ability to detect quality changes
of a realistic 3D model in an immersive virtual reality environment. We varied
the Level-of-Detail (LOD) as a function of rotational head velocity with
different degrees of severity. Using a psychophysical method, we asked 17
participants to identify which of the two presented intervals contained the
higher quality model under two different maximum velocity conditions. After
fitting psychometric functions to data relating the percentage of correct
responses to the aggressiveness of LOD manipulations, we identified the
threshold severity for which participants could reliably (75\%) detect the
lower LOD model. Participants accepted an approximately four-fold LOD reduction
even in the low maximum velocity condition without a significant impact on
perceived quality, which suggests that there is considerable potential for
optimisation when users are moving (increased range of perceptual uncertainty).
Moreover, LOD could be degraded significantly more in the maximum head velocity
condition, suggesting these effects are indeed speed dependent.","['David Petrescu', 'Paul A. Warren', 'Zahra Montazeri', 'Stephen Pettifer']",2023-01-23T12:29:39Z,http://arxiv.org/abs/2301.09394v1,"['cs.GR', 'cs.HC', 'I.3']","Virtual Reality,Velocity,LOD,Psychometric,3D model,Head rotations,Quality changes,Psychophysical method,Psychometric functions,Perceived quality"
"Does Adding Physical Realism to Virtual Reality Training Reduce Time
  Compression?","Virtual reality (VR) is known to cause a ""time compression"" effect, where the
time spent in VR feels to pass faster than the effective elapsed time. Our goal
with this research is to investigate if the physical realism of a VR experience
reduces the time compression effect on a gas monitoring training task that
requires precise time estimation. We used physical props and passive haptics in
a VR task with high physical realism and compared it to an equivalent standard
VR task with only virtual objects. We also used an identical real-world task as
a baseline time estimation task. Each scenario includes the user picking up a
device, opening a door, navigating a corridor with obstacles, performing five
short time estimations, and estimating the total time from task start to end.
Contrary to previous work, there was a consistent time dilation effect in all
conditions, including the real world. However, no significant effects were
found comparing the estimated differences between the high and low physical
realism conditions. We discuss implications of the results and limitations of
the study and propose future work that may better address this important
question for virtual reality training.","['Kadir Lofca', 'Jason Jerald', 'Dalton Costa', 'Regis Kopper']",2023-02-07T17:26:46Z,http://arxiv.org/abs/2302.03623v1,['cs.HC'],"Virtual reality,Physical realism,Time compression,Gas monitoring,Time estimation,Physical props,Passive haptics,Real-world task,Time dilation,Virtual objects"
"Unique Identification of 50,000+ Virtual Reality Users from Head & Hand
  Motion Data","With the recent explosive growth of interest and investment in virtual
reality (VR) and the so-called ""metaverse,"" public attention has rightly
shifted toward the unique security and privacy threats that these platforms may
pose. While it has long been known that people reveal information about
themselves via their motion, the extent to which this makes an individual
globally identifiable within virtual reality has not yet been widely
understood. In this study, we show that a large number of real VR users
(N=55,541) can be uniquely and reliably identified across multiple sessions
using just their head and hand motion relative to virtual objects. After
training a classification model on 5 minutes of data per person, a user can be
uniquely identified amongst the entire pool of 50,000+ with 94.33% accuracy
from 100 seconds of motion, and with 73.20% accuracy from just 10 seconds of
motion. This work is the first to truly demonstrate the extent to which
biomechanics may serve as a unique identifier in VR, on par with widely used
biometrics such as facial or fingerprint recognition.","['Vivek Nair', 'Wenbo Guo', 'Justus Mattern', 'Rui Wang', ""James F. O'Brien"", 'Louis Rosenberg', 'Dawn Song']",2023-02-17T15:05:18Z,http://arxiv.org/abs/2302.08927v1,"['cs.CR', 'cs.LG']","Virtual reality,Users,Motion data,Identification,Biomechanics,Biometrics,Security,Privacy,Classification model,Metaverse"
Theoretical limits of Virtual Reality,"In recent years there has been a strong development of the concept of virtual
reality (VR) which from the first video games developed in the 60s has reached
the current immersive systems. VR and the consequent deception of perception
pose an interesting question: is it possible to deceive the mind to the point
of not being able to recognize whether the perceived reality is real or
simulated? In addition to this question, another question arises spontaneously:
is it possible to simulate a non-reality in which the physical laws do not
apply? The answer to the first question is that it would theoretically be
possible to deceive the mind to the point of not being able to recognize
whether the perceived reality is real or simulated, furthermore it is also
possible to simulate a non-real, i.e. magical, world. However, this possibility
is based on hiding degrees of freedom from the observer, therefore it requires
the complicity of the observer to be realised. It can be said that a VR system
can simulate both a real and non-real experience. For an observer, a virtual
reality experience is still an experience of reality. This experience can be
exchanged for a different experience, that is, for a different reality that can
be real or not real.","['Francesco Sisini', 'Valentina Sisini', 'Laura Sisini']",2023-02-18T08:51:09Z,http://arxiv.org/abs/2302.10190v1,['cs.HC'],"Virtual Reality,Perception,Deception,Simulated reality,Physical laws,Immersive systems"
"Results of the 2023 Census of Beat Saber Users: Virtual Reality Gaming
  Population Insights and Factors Affecting Virtual Reality E-Sports
  Performance","The emergence of affordable standalone virtual reality (VR) devices has
allowed VR technology to reach mass-market adoption in recent years, driven
primarily by the popularity of VR gaming applications such as Beat Saber.
However, despite being the top-grossing VR application to date and the most
popular VR e-sport, the population of over 6 million Beat Saber users has not
yet been widely studied. In this report, we present a large-scale comprehensive
survey of Beat Saber players (N=1,006) that sheds light on several important
aspects of this population, including their background, biometrics,
demographics, health information, behavioral patterns, and technical device
specifications. We further provide insights into the emerging field of VR
e-sports by analyzing correlations between responses and an authoritative
measure of in-game performance.","['Vivek Nair', 'Viktor Radulov', ""James F. O'Brien""]",2023-05-23T17:53:29Z,http://arxiv.org/abs/2305.14320v2,['cs.HC'],"Census,Beat Saber,Virtual Reality,Gaming,E-Sports,Performance,Biometrics,Demographics,Health information,Device specifications."
"A Virtual Reality Tool for Representing, Visualizing and Updating Deep
  Learning Models","Deep learning is ubiquitous, but its lack of transparency limits its impact
on several potential application areas. We demonstrate a virtual reality tool
for automating the process of assigning data inputs to different categories. A
dataset is represented as a cloud of points in virtual space. The user explores
the cloud through movement and uses hand gestures to categorise portions of the
cloud. This triggers gradual movements in the cloud: points of the same
category are attracted to each other, different groups are pushed apart, while
points are globally distributed in a way that utilises the entire space. The
space, time, and forces observed in virtual reality can be mapped to
well-defined machine learning concepts, namely the latent space, the training
epochs and the backpropagation. Our tool illustrates how the inner workings of
deep neural networks can be made tangible and transparent. We expect this
approach to accelerate the autonomous development of deep learning applications
by end users in novel areas.","['Hannes Kath', 'Bengt Lüers', 'Thiago S. Gouvêa', 'Daniel Sonntag']",2023-05-24T17:06:59Z,http://arxiv.org/abs/2305.15353v1,"['cs.HC', 'cs.LG']","virtual reality,deep learning models,transparency,data inputs,categories,cloud of points,hand gestures,machine learning concepts,latent space,backpropagation"
"Gotta Go Fast: Measuring Input/Output Latencies of Virtual Reality 3D
  Engines for Cognitive Experiments","Virtual Reality (VR) is seeing increased adoption across many fields. The
field of experimental cognitive science is also testing utilization of the
technology combined with physiological measures such as electroencephalography
(EEG) and eye tracking. Quantitative measures of human behavior and cognition
process, however, are sensitive to minuscule time resolutions that are often
overlooked in the scope of consumer-level VR hardware and software stacks. In
this preliminary study, we implement VR testing environments in two prominent
3D Virtual Reality frameworks (Unity and Unreal Engine) to measure latency
values for stimulus onset execution code to Head-Mount Display (HMD) pixel
change, as well as the latency between human behavioral response input to its
registration in the engine environment under a typical cognitive experiment
hardware setup. We find that whereas the specifics of the latency may further
be influenced by different hardware and software setups, the variations in
consumer hardware is apparent regardless and report detailed statistics on
these latencies. Such consideration should be taken into account when designing
VR-based cognitive experiments that measure human behavior.","['Taeho Kang', 'Christian Wallraven']",2023-06-05T07:12:37Z,http://arxiv.org/abs/2306.02637v1,['cs.HC'],"Virtual Reality,Input/Output Latencies,3D Engines,Cognitive Experiments,Electroencephalography,Eye Tracking,Latency Values,Head-Mount Display,Behavioral Response Input,Hardware Setup"
Power Modeling for Virtual Reality Video Playback Applications,"This paper proposes a method to evaluate and model the power consumption of
modern virtual reality playback and streaming applications on smartphones. Due
to the high computational complexity of the virtual reality processing
toolchain, the corresponding power consumption is very high, which reduces
operating times of battery-powered devices. To tackle this problem, we analyze
the power consumption in detail by performing power measurements. Furthermore,
we construct a model to estimate the true power consumption with a mean error
of less than 3.5%. The model can be used to save power at critical battery
levels by changing the streaming video parameters. Particularly, the results
show that the power consumption is significantly reduced by decreasing the
input video resolution.","['Christian Herglotz', 'Stéphane Coulombe', 'Ahmad Vakili', 'André Kaup']",2023-07-17T09:23:05Z,http://arxiv.org/abs/2307.08338v1,['eess.IV'],"Power modeling,Virtual reality,Video playback,Power consumption,Streaming applications,Smartphones,Computational complexity,Battery-powered devices,Power measurements"
"Motion Matching for Character Animation and Virtual Reality Avatars in
  Unity","Real-time animation of virtual characters has traditionally been accomplished
by playing short sequences of animations structured in the form of a graph.
These methods are time-consuming to set up and scale poorly with the number of
motions required in modern virtual environments. The ever-increasing need for
highly-realistic virtual characters in fields such as entertainment, virtual
reality, or the metaverse has led to significant advances in the field of
data-driven character animation. Techniques like Motion Matching have provided
enough versatility to conveniently animate virtual characters using a selection
of features from an animation database. Data-driven methods retain the quality
of the captured animations, thus delivering smoother and more natural-looking
animations. In this work, we researched and developed a Motion Matching
technique for the Unity game engine. In this thesis, we present our findings on
how to implement an animation system based on Motion Matching. We also
introduce a novel method combining body orientation prediction with Motion
Matching to animate avatars for consumer-grade virtual reality systems.",['Jose Luis Ponton'],2023-10-08T16:12:51Z,http://arxiv.org/abs/2310.05215v1,['cs.GR'],"Motion Matching,Character Animation,Virtual Reality,Avatars,Unity,Data-driven,Animation Database,Animation System,Body Orientation Prediction"
"Designing and Evaluating an Adaptive Virtual Reality System using EEG
  Frequencies to Balance Internal and External Attention States","Virtual reality finds various applications in productivity, entertainment,
and training scenarios requiring working memory and attentional resources.
Working memory relies on prioritizing relevant information and suppressing
irrelevant information through internal attention, which is fundamental for
successful task performance and training. Today, virtual reality systems do not
account for the impact of working memory loads resulting in over or
under-stimulation. In this work, we designed an adaptive system based on EEG
correlates of external and internal attention to support working memory task
performance. Here, participants engaged in a visual working memory N-Back task,
and we adapted the visual complexity of distracting surrounding elements. Our
study first demonstrated the feasibility of EEG frontal theta and parietal
alpha frequency bands for dynamic visual complexity adjustments. Second, our
adaptive system showed improved task performance and diminished perceived
workload compared to a reverse adaptation. Our results show the effectiveness
of the proposed adaptive system, allowing for the optimization of distracting
elements in high-demanding conditions. Adaptive systems based on alpha and
theta frequency bands allow for the regulation of attentional and executive
resources to keep users engaged in a task without resulting in cognitive
overload.","['Francesco Chiossi', 'Changkun Ou', 'Carolina Gerhardt', 'Felix Putze', 'Sven Mayer']",2023-11-17T11:01:17Z,http://arxiv.org/abs/2311.10447v1,['cs.HC'],"virtual reality,EEG frequencies,internal attention,external attention,working memory,task performance,adaptive system,visual complexity,frontal theta,parietal alpha"
"Cost/benefit analysis model for implementing virtual reality in
  construction companies","Immersive technologies (ImT), like Virtual Reality (VR), have several
potential applications in the construction industry. However, the absence of a
cost-benefit analysis discourages construction decision-makers from
implementing these technologies. In this study, we proposed a primary model for
conducting a cost-benefit analysis for implementing virtual reality in
construction companies. The cost and benefit factors were identified through a
literature review and considered input variables for the model, and then using
synthetic data, a Monte Carlo simulation was performed to generate a
distribution of outcome. Given the uncertainty in input parameters, this
distribution reflected the potential range of total net benefit. Considering
synthetic data and input factors obtained only through literature and
assumptions, VR implementation could be a promising decision based on the
results. This study's results would benefit decision-makers in construction
companies about the costs and benefits of implementing VR and other researchers
interested in this field.","['Payam Mohammadi', 'Claudia Garrido Martins']",2023-10-08T00:48:16Z,http://arxiv.org/abs/2311.10726v1,['cs.CY'],"Immersive technologies,Virtual Reality,cost-benefit analysis,construction industry,decision-makers,Monte Carlo simulation,net benefit,input variables,synthetic data."
"VisionaryVR: An Optical Simulation Tool for Evaluating and Optimizing
  Vision Correction Solutions in Virtual Reality","Developing and evaluating vision science methods require robust and efficient
tools for assessing their performance in various real-world scenarios. This
study presents a novel virtual reality (VR) simulation tool that simulates
real-world optical methods while giving high experimental control to the
experiment. The tool incorporates an experiment controller, to smoothly and
easily handle multiple conditions, a generic eye-tracking controller, that
works with most common VR eye-trackers, a configurable defocus simulator, and a
generic VR questionnaire loader to assess participants' behavior in virtual
reality. This VR-based simulation tool bridges the gap between theoretical and
applied research on new optical methods, corrections, and therapies. It enables
vision scientists to increase their research tools with a robust, realistic,
and fast research environment.","['Benedikt W. Hosp', 'Martin Dechant', 'Yannick Sauer', 'Rajat Agarwala', 'Siegfried Wahl']",2023-12-01T16:18:55Z,http://arxiv.org/abs/2312.00692v1,['cs.CV'],"Optical Simulation Tool,Vision Correction Solutions,Virtual Reality,Vision Science Methods,Experiment Controller,Eye-Tracking Controller,Defocus Simulator,Questionnaire Loader,Optical Methods,Vision Scientists"
"Seamless Virtual Reality with Integrated Synchronizer and Synthesizer
  for Autonomous Driving","Virtual reality (VR) is a promising data engine for autonomous driving (AD).
However, data fidelity in this paradigm is often degraded by VR inconsistency,
for which the existing VR approaches become ineffective, as they ignore the
inter-dependency between low-level VR synchronizer designs (i.e., data
collector) and high-level VR synthesizer designs (i.e., data processor). This
paper presents a seamless virtual reality SVR platform for AD, which mitigates
such inconsistency, enabling VR agents to interact with each other in a shared
symbiotic world. The crux to SVR is an integrated synchronizer and synthesizer
IS2 design, which consists of a drift-aware lidar-inertial synchronizer for VR
colocation and a motion-aware deep visual synthesis network for augmented
reality image generation. We implement SVR on car-like robots in two sandbox
platforms, achieving a cm-level VR colocalization accuracy and 3.2% VR image
deviation, thereby avoiding missed collisions or model clippings. Experiments
show that the proposed SVR reduces the intervention times, missed turns, and
failure rates compared to other benchmarks. The SVR-trained neural network can
handle unseen situations in real-world environments, by leveraging its
knowledge learnt from the VR space.","['He Li', 'Ruihua Han', 'Zirui Zhao', 'Wei Xu', 'Qi Hao', 'Shuai Wang', 'Chengzhong Xu']",2024-03-06T08:37:36Z,http://arxiv.org/abs/2403.03541v1,['cs.RO'],"virtual reality,autonomous driving,synchronizer,synthesizer,data fidelity,VR agents,lidar-inertial synchronizer,deep visual synthesis network,augmented reality,neural network"
"Evaluation of Eye Tracking Signal Quality for Virtual Reality
  Applications: A Case Study in the Meta Quest Pro","We present an extensive, in-depth analysis of the eye tracking capabilities
of the Meta Quest Pro virtual reality headset using a dataset of eye movement
recordings collected from 78 participants. In addition to presenting classical
signal quality metrics--spatial accuracy, spatial precision and linearity--in
ideal settings, we also study the impact of background luminance and headset
slippage on device performance. We additionally present a user-centered
analysis of eye tracking signal quality, where we highlight the potential
differences in user experience as a function of device performance. This work
contributes to a growing understanding of eye tracking signal quality in
virtual reality headsets, where the performance of applications such as
gaze-based interaction, foveated rendering, and social gaze are directly
dependent on the quality of eye tracking signal.","['Samantha Aziz', 'Dillon J Lohr', 'Lee Friedman', 'Oleg Komogortsev']",2024-03-11T23:36:55Z,http://arxiv.org/abs/2403.07210v1,['cs.HC'],"eye tracking,signal quality,virtual reality,Meta Quest Pro,spatial accuracy,spatial precision,linearity,background luminance,headset slippage,user experience"
"I Did Not Notice: A Comparison of Immersive Analytics with Augmented and
  Virtual Reality","Immersive environments enable users to engage in embodied interaction,
enhancing the sensemaking processes involved in completing tasks such as
immersive analytics. Previous comparative studies on immersive analytics using
augmented and virtual realities have revealed that users employ different
strategies for data interpretation and text-based analytics depending on the
environment. Our study seeks to investigate how augmented and virtual reality
influences sensemaking processes in quantitative immersive analytics. Our
results, derived from a diverse group of participants, indicate that users
demonstrate comparable performance in both environments. However, it was
observed that users exhibit a higher tolerance for cognitive load in VR and
travel further in AR. Based on our findings, we recommend providing users with
the option to switch between AR and VR, thereby enabling them to select an
environment that aligns with their preferences and task requirements.","['Xiaoyan Zhou', 'Anil Ufuk Batmaz', 'Adam S. Williams', 'Dylan Schreiber', 'Francisco Ortega']",2024-04-04T21:39:49Z,http://arxiv.org/abs/2404.03814v1,['cs.HC'],"Immersive analytics,Augmented reality,Virtual reality,Sensemaking processes,Data interpretation,Embodied interaction,Quantitative analytics,Cognitive load,Immersive environments"
Octonions and Binocular Mobilevision,"This paper is devoted to an interaction of 2 objects: the 1st of them is
octonions, the classical structure of pure mathematics, the 2nd one is
Mobilevision, the recently developped technique of computer graphics. Namely,
it is shown that the binocular Mobilevision maybe elaborated by use of the
octonionic colour space - the 7-dimensional extension of the classical one,
which includes a strange overcolour besides two triples of ordinary ones
(blue,green, red for left and right eyes).
  Contents.
  I. Interpretational geometry, anomalous virtual realities, quantum projective
field theory and Mobilevision:(1.1. Interpretational geometry; 1.2. Anomalous
virtual realities; 1.3. Colours in anomalous virtual realities; 1.4. Quantum
projective field theory; 1.5. Mobilevision).
  II. Quantum conformal and q_R-conformal field theories, an infinite
dimensional quantum group and quantum field analogs of Euler-Arnold top:(2.1.
Quantum conformal field theory; 2.2. Lobachevskii algebra, the quantization of
the Lobachevskii plane; 2.3. Quantum q_R-conformal field theory; 2.4. An
infinite dimensional quantum group; 2.5. Quantum-field Euler-Arnold top and
Virasoro master equation).
  III. Octonionic colour space and binocular Mobilevision:(3.1. Quaternionic
description of ordinary colour space; 3.2. Octonionic colour space and
binocular Mobilevision).",['Denis Juriev'],1994-01-12T15:15:05Z,http://arxiv.org/abs/hep-th/9401047v3,"['hep-th', 'adap-org', 'nlin.AO']","Octonions,Mobilevision,Computer graphics,Octonionic colour space,Quantum projective field theory,Quantum conformal field theory,Lobachevskii algebra,Quantum q_R-conformal field theory,Virasoro master equation,Binocular Mobilevision"
"A basic gesture and motion format for virtual reality multisensory
  applications","The question of encoding movements such as those produced by human gestures
may become central in the coming years, given the growing importance of
movement data exchanges between heterogeneous systems and applications (musical
applications, 3D motion control, virtual reality interaction, etc.). For the
past 20 years, various formats have been proposed for encoding movement,
especially gestures. Though, these formats, at different degrees, were designed
in the context of quite specific applications (character animation, motion
capture, musical gesture, biomechanical concerns...). The article introduce a
new file format, called GMS (for 'Gesture and Motion Signal'), with the aim of
being more low-level and generic, by defining the minimal features a format
carrying movement/gesture information needs, rather than by gathering all the
information generally given by the existing formats. The article argues that,
given its growing presence in virtual reality situations, the ""gesture signal""
itself must be encoded, and that a specific format is needed. The proposed
format features the inner properties of such signals: dimensionality,
structural features, types of variables, and spatial and temporal properties.
The article first reviews the various situations with multisensory virtual
objects in which gesture controls intervene. The proposed format is then
deduced, as a mean to encode such versatile and variable ""gestural and animated
scene"".","['Annie Luciani', 'Matthieu Evrard', 'Damien Couroussé', 'Nicolas Castagné', 'Claude Cadoz', 'Jean-Loup Florens']",2010-05-25T13:16:29Z,http://arxiv.org/abs/1005.4564v1,"['cs.HC', 'cs.GR', 'cs.MM', 'cs.SD']","virtual reality,multisensory applications,movement data exchanges,file format,GMS,Gesture and Motion Signal,gesture signal,virtual reality interaction,spatial properties"
Observing supermassive black holes in virtual reality,"We present a full 360 degree (i.e., 4$\pi$ steradian) general-relativistic
ray-tracing and radiative transfer calculations of accreting supermassive black
holes. We perform state-of-the-art three-dimensional general relativistic
magnetohydrodynamical simulations using the BHAC code, subsequently
post-processing this data with the radiative transfer code RAPTOR. All
relativistic and general-relativistic effects, such as Doppler boosting and
gravitational redshift, as well as geometrical effects due to the local
gravitational field and the observer's changing position and state of motion,
are therefore calculated self-consistently. Synthetic images at four
astronomically-relevant observing frequencies are generated from the
perspective of an observer with a full 360-degree view inside the accretion
flow, who is advected with the flow as it evolves. As an example, we calculated
images based on recent best-fit models of observations of Sagittarius A*. These
images are combined to generate a complete 360-degree Virtual Reality movie of
the surrounding environment of the black hole and its event horizon. Our
approach also enables the calculation of the local luminosity received at a
given fluid element in the accretion flow, providing important applications in,
e.g., radiation feedback calculations onto black hole accretion flows. In
addition to scientific applications, the 360-degree Virtual Reality movies we
present also represent a new medium through which to communicate black hole
physics to a wider audience, serving as a powerful educational tool.","['Jordy Davelaar', 'Thomas Bronzwaer', 'Daniel Kok', 'Ziri Younsi', 'Monika Mościbrodzka', 'Heino Falcke']",2018-11-20T17:05:34Z,http://arxiv.org/abs/1811.08369v1,['astro-ph.HE'],"supermassive black holes,virtual reality,general-relativistic ray-tracing,radiative transfer,magnetohydrodynamical simulations,Doppler boosting,gravitational redshift,accretion flow,event horizon,luminosity"
"A Virtual Reality Game as a Tool to Assess Physiological Correlations of
  Stress","The objective of this study is to develop and use a virtual reality game as a
tool to assess the effects of realistic stress on the behavioral and
physiological responses of participants. The game is based on a popular Steam
game called Keep Talking Nobody Explodes, where the player collaborates with
another person to defuse a bomb. Varying levels of difficulties in solving a
puzzle and time pressures will result in different stress levels that can be
measured in terms of errors, response time lengths, and other physiological
measurements. The game was developed using 3D programming tools including
Blender and virtual reality development kit (VRTK). To measure response times
accurately, we added LSL (Lab Stream Layer) Markers to collect and synchronize
physiological signals, behavioral data, and the timing of game events. We
recorded Electrocardiogram (ECG) data during gameplay to assess heart rate and
heart-rate variability (HRV) that have been shown as reliable indicators of
stress. Our empirical results showed that heart rate increased significantly
while HRV reduced significantly when the participants under high stress, which
are consistent with the prior mainstream stress research. We further
experimented with other tools to enhance communication between two players
under adverse conditions and found that an automatic speech recognition
software effectively enhanced the communication between the players by
displaying keywords into the player's headset that lead to the facilitation of
finding the solution of the puzzles or modules. This VR game framework is
publicly available in Github and allows researchers to measure and synchronize
other physiological signals such as electroencephalogram, electromyogram, and
pupillometry.","['Daniel H. Lee', 'Tzyy-Ping Jung']",2020-09-30T04:20:07Z,http://arxiv.org/abs/2009.14421v1,"['cs.HC', 'H.5.0']","virtual reality game,physiological correlations,stress,EEG,EMG,pupillometry,ECG,heart rate variability,Lab Stream Layer,3D programming"
VENu: The Virtual Environment for Neutrinos,"The Virtual Environment for Neutrinos (VENu) is a virtual reality-based
visualisation of the MicroBooNE detector. MicroBooNE is a liquid-argon-based
neutrino experiment, which is currently operating in Fermilab's Booster
neutrino beam. The new VENu smartphone app provides informative explanations
about neutrinos and uses real MicroBooNE neutrino data that can be visualised
inside a virtual representation of the MicroBooNE detector. Available for both
iOS and Android, the VENu app can be downloaded for free from the Apple and
Google marketplaces. The app enables users to immerse themselves inside the
MicroBooNE particle detector and to see particle tracks inside. This can be
done in Virtual Reality mode, where the users can pair their smartphone with
any consumer virtual reality headset and see the detector in 3D. To encourage
learning in a fun environment, a game is also available, guiding users to learn
about neutrinos and how to detect them. They can also try to ""catch""' neutrinos
themselves in 3D mode. The app is currently being pursued for a QuarkNet
neutrino master class and outreach events at several universities and labs
worldwide.",['Marco Del Tutto'],2017-09-28T18:18:37Z,http://arxiv.org/abs/1709.10120v2,"['physics.pop-ph', 'cs.HC', 'hep-ex', 'physics.ins-det']","Virtual Environment,Neutrinos,MicroBooNE detector,Liquid-argon-based,Fermilab,Booster neutrino beam,Smartphone app,Virtual representation,Particle tracks,Virtual Reality mode"
A controlled study of virtual reality in first-year magnetostatics,"Stereoscopic virtual reality (VR) has experienced a resurgence due to
flagship products such as the Oculus Rift, HTC Vive and smartphone-based VR
solutions like Google Cardboard. This is causing the question to resurface: how
can stereoscopic VR be useful in instruction, if at all, and what are the
pedagogical best practices for its use? To address this, and to continue our
work in this sphere, we performed a study of 289 introductory physics students
who were sorted into three different treatment types: stereoscopic virtual
reality, WebGL simulation, and static 2D images, each designed to provide
information about magnetic fields and forces. Students were assessed using
preliminary items designed to focus on heavily-3D systems. We report on
assessment reliability, and on student performance. Overall, we find that
students who used VR did not significantly outperform students using other
treatment types. There were significant differences between sexes, as other
studies have noted. Dependence on students' self-reported 3D videogame play was
observed, in keeping with previous studies, but this dependence was not
restricted to the VR treatment.","['Chris D. Porter', 'Jonathan Brown', 'Joseph R. Smith', 'Amber Simmons', 'Megan Nieberding', 'Abigail E. Ayers', 'Chris Orban']",2019-07-12T03:46:03Z,http://arxiv.org/abs/1907.05567v1,['physics.ed-ph'],"virtual reality,magnetostatics,stereoscopic,WebGL simulation,2D images,magnetic fields,forces,assessment,3D systems,VR treatment"
The Plausibility Paradox for Scaled-Down Users in Virtual Environments,"This paper identifies a new phenomenon: when users interact with simulated
objects in a virtual environment where the user is much smaller than usual,
there is a mismatch between the object physics that they expect and the object
physics that would be correct at that scale. We report the findings of our
study investigating the relationship between perceived realism and a physically
accurate approximation of reality in a virtual reality experience in which the
user has been scaled down by a factor of ten. We conducted a within-subjects
experiment in which 44 subjects performed a simple interaction task with
objects under two different physics simulation conditions. In one condition,
the objects, when dropped and thrown, behaved accurately according to the
physics that would be correct at that reduced scale in the real world, our true
physics condition. In the other condition, the movie physics condition, the
objects behaved in a similar manner as they would if no scaling of the user had
occurred. We found that a significant majority of the users considered the
latter condition to be the more realistic one. We argue that our findings have
implications for many virtual reality and telepresence applications involving
operation with simulated or physical objects in small scales.","['Matti Pouke', 'Katherine J. Mimnaugh', 'Timo Ojala', 'Steven M. LaValle']",2019-12-03T12:18:53Z,http://arxiv.org/abs/1912.01947v2,"['cs.HC', 'cs.MM', 'H.5.1']","virtual environments,scaled-down users,object physics,perceived realism,physics simulation,virtual reality,interaction task,true physics condition,movie physics condition,telepresence applications"
"Virtual reality analysis of intrinsic protein geometry with applications
  to cis peptide planes","A protein is traditionally visualised as a piecewise linear discrete curve,
and its geometry is conventionally characterised by the extrinsically
determined Ramachandran angles. However, a protein backbone has also two
independent intrinsic geometric structures, due to the peptide planes and the
side chains. Here we adapt and develop modern 3D virtual reality techniques to
scrutinize the atomic geometry along a protein backbone, in the vicinity of a
peptide plane. For this we compare backbone geometry-based (extrinsic) and
structure-based (intrinsic) coordinate systems, and as an example we inspect
the trans and cis peptide planes. We reveal systematics in the way how a cis
peptide plane deforms the neighbouring atomic geometry, and we develop a
virtual reality based visual methodology that can identify the presence of a
cis peptide plane from the arrangement of atoms in its vicinity. Our approach
can easily detect exceptionally placed atoms in crystallographic structures.
Thus it can be employed as a powerful visual refinement tool which is
applicable also in the case when resolution of the protein structure is limited
and whenever refinement is needed. As concrete examples we identify a number of
crystallographic protein structures in Protein Data Bank (PDB) that display
exceptional atomic positions around their cis peptide planes.","['Yanzhen Hou', 'Jin Dai', 'Nevena Ilieva', 'Antti J. Niemi', 'Xubiao Peng', 'Jianfeng He']",2017-06-05T14:24:42Z,http://arxiv.org/abs/1706.01345v2,"['q-bio.BM', 'physics.bio-ph']","virtual reality,protein geometry,intrinsic,cis peptide planes,Ramachandran angles,3D,atomic geometry,crystallographic structures,Protein Data Bank"
"Sampling molecular conformations and dynamics in a multi-user virtual
  reality framework","We describe a framework for interactive molecular dynamics in a multiuser
virtual reality environment, combining rigorous cloud-mounted physical
atomistic simulation with commodity virtual reality hardware, which we have
made accessible to readers (see isci.itch.io/nsb-imd). It allows users to
visualize and sample, with atomic-level precision, the structures and dynamics
of complex molecular structures 'on the fly', and to interact with other users
in the same virtual environment. A series of controlled studies, wherein
participants were tasked with a range of molecular manipulation goals
(threading methane through a nanotube, changing helical screw-sense, and tying
a protein knot), quantitatively demonstrate that users within the interactive
VR environment can complete sophisticated molecular modelling tasks more
quickly than they can using conventional interfaces, especially for molecular
pathways and structural transitions whose conformational choreographies are
intrinsically 3d. This framework should accelerate progress in nanoscale
molecular engineering areas such as drug development, synthetic biology, and
catalyst design. More broadly, our findings highlight VR's potential in
scientific domains where 3d dynamics matter, spanning research and education.","['Michael O Connor', 'Helen M. Deeks', 'Edward Dawn', 'Oussama Metatla', 'Anne Roudaut', 'Matthew Sutton', 'Becca Rose Glowacki', 'Rebecca Sage', 'Philip Tew', 'Mark Wonnacott', 'Phil Bates', 'Adrian J. Mulholland', 'David R. Glowacki']",2018-01-09T11:08:49Z,http://arxiv.org/abs/1801.02884v1,"['physics.chem-ph', 'cs.HC', 'physics.bio-ph', 'physics.ed-ph']","molecular dynamics,virtual reality,atomistic simulation,molecular structures,interactive environment,molecular manipulation,molecular engineering,drug development,synthetic biology,catalyst design"
"Rapid 3D Reconstruction of Indoor Environments to Generate Virtual
  Reality Serious Games Scenarios","Virtual Reality (VR) for Serious Games (SGs) is attracting increasing
attention for training applications due to its potential to provide
significantly enhanced learning to users. Some examples of the application of
VR for SGs are complex training evacuation problems such as indoor earthquake
evacuation or fire evacuation. The indoor 3D geometry of existing buildings can
largely influence evacuees' behaviour, being instrumental in the design of VR
SGs storylines and simulation scenarios. The VR scenarios of existing buildings
can be generated from drawings and models. However, these data may not reflect
the 'as-is' state of the indoor environment and may not be suitable to reflect
dynamic changes of the system (e.g. Earthquakes), resulting in excessive
development efforts to design credible and meaningful user experience. This
paper explores several workflows for the rapid and effective reconstruction of
3D indoor environments of existing buildings that are suitable for earthquake
simulations. These workflows start from Building Information Modelling (BIM),
laser scanning and 360-degree panoramas. We evaluated the feasibility and
efficiency of different approaches by using an earthquake-based case study
developed for VR SGs.","['Zhenan Feng', 'Vicente A. González', 'Ling Ma', 'Mustafa M. A. Al-Adhami', 'Claudio Mourgues']",2018-12-04T21:58:49Z,http://arxiv.org/abs/1812.01706v1,['cs.HC'],"Virtual reality,Serious Games,3D reconstruction,Indoor environments,Building Information Modelling,Laser scanning,Earthquake simulations,Simulation scenarios,Evacuation problems"
"A Visually Plausible Grasping System for Object Manipulation and
  Interaction in Virtual Reality Environments","Interaction in virtual reality (VR) environments is essential to achieve a
pleasant and immersive experience. Most of the currently existing VR
applications, lack of robust object grasping and manipulation, which are the
cornerstone of interactive systems. Therefore, we propose a realistic, flexible
and robust grasping system that enables rich and real-time interactions in
virtual environments. It is visually realistic because it is completely
user-controlled, flexible because it can be used for different hand
configurations, and robust because it allows the manipulation of objects
regardless their geometry, i.e. hand is automatically fitted to the object
shape. In order to validate our proposal, an exhaustive qualitative and
quantitative performance analysis has been carried out. On the one hand,
qualitative evaluation was used in the assessment of the abstract aspects such
as: hand movement realism, interaction realism and motor control. On the other
hand, for the quantitative evaluation a novel error metric has been proposed to
visually analyze the performed grips. This metric is based on the computation
of the distance from the finger phalanges to the nearest contact point on the
object surface. These contact points can be used with different application
purposes, mainly in the field of robotics. As a conclusion, system evaluation
reports a similar performance between users with previous experience in virtual
reality applications and inexperienced users, referring to a steep learning
curve.","['Sergiu Oprea', 'Pablo Martinez-Gonzalez', 'Alberto Garcia-Garcia', 'John Alejandro Castro-Vargas', 'Sergio Orts-Escolano', 'Jose Garcia-Rodriguez']",2019-03-12T22:15:51Z,http://arxiv.org/abs/1903.05238v1,"['cs.GR', 'cs.CV', 'cs.HC']","virtual reality,object manipulation,grasping system,interaction,immersive experience,hand configurations,qualitative analysis,quantitative analysis,error metric,motor control"
"Visualizing biomolecular electrostatics in virtual reality with
  UnityMol-APBS","Virtual reality is a powerful tool with the ability to immerse a user within
a completely external environment. This immersion is particularly useful when
visualizing and analyzing interactions between small organic molecules,
molecular inorganic complexes, and biomolecular systems such as redox proteins
and enzymes. A common tool used in the biomedical community to analyze such
interactions is the APBS software, which was developed to solve the equations
of continuum electrostatics for large biomolecular assemblages. Numerous
applications exist for using APBS in the biomedical community including
analysis of protein ligand interactions and APBS has enjoyed widespread
adoption throughout the biomedical community. Currently, typical use of the
full APBS toolset is completed via the command line followed by visualization
using a variety of two-dimensional external molecular visualization software.
This process has inherent limitations: visualization of three-dimensional
objects using a two-dimensional interface masks important information within
the depth component. Herein, we have developed a single application,
UnityMol-APBS, that provides a dual experience where users can utilize the full
range of the APBS toolset, without the use of a command line interface, by use
of a simple \ac{GUI} for either a standard desktop or immersive virtual reality
experience.","['Joseph Laureanti', 'Juan Brandi', 'Elvis Offor', 'David Engel', 'Robert Rallo', 'Bojana Ginovska', 'Xavier Martinez', 'Marc Baaden', 'Nathan A. Baker']",2019-08-29T14:36:04Z,http://arxiv.org/abs/1908.11261v2,['q-bio.BM'],"virtual reality,biomolecular electrostatics,UnityMol-APBS,APBS,biomedical community,molecular inorganic complexes,redox proteins,enzymes,protein-ligand interactions,GUI"
"Virtual reality for 3D histology: multi-scale visualization of organs
  with interactive feature exploration","Virtual reality (VR) enables data visualization in an immersive and engaging
manner, and it can be used for creating ways to explore scientific data. Here,
we use VR for visualization of 3D histology data, creating a novel interface
for digital pathology. Our contribution includes 3D modeling of a whole organ
and embedded objects of interest, fusing the models with associated
quantitative features and full resolution serial section patches, and
implementing the virtual reality application. Our VR application is multi-scale
in nature, covering two object levels representing different ranges of detail,
namely organ level and sub-organ level. In addition, the application includes
several data layers, including the measured histology image layer and multiple
representations of quantitative features computed from the histology. In this
interactive VR application, the user can set visualization properties, select
different samples and features, and interact with various objects. In this
work, we used whole mouse prostates (organ level) with prostate cancer tumors
(sub-organ objects of interest) as example cases, and included quantitative
histological features relevant for tumor biology in the VR model. Due to
automated processing of the histology data, our application can be easily
adopted to visualize other organs and pathologies from various origins. Our
application enables a novel way for exploration of high-resolution,
multidimensional data for biomedical research purposes, and can also be used in
teaching and researcher training.","['Kaisa Liimatainen', 'Leena Latonen', 'Masi Valkonen', 'Kimmo Kartasalo', 'Pekka Ruusuvuori']",2020-03-24T23:23:41Z,http://arxiv.org/abs/2003.11148v1,"['cs.GR', 'eess.IV']","virtual reality,3D histology,multi-scale visualization,organs,interactive feature exploration,digital pathology,3D modeling,quantitative features,serial section patches,whole organ,sub-organ level"
"Bionic Tracking: Using Eye Tracking to Track Biological Cells in Virtual
  Reality","We present Bionic Tracking, a novel method for solving biological cell
tracking problems with eye tracking in virtual reality using commodity
hardware. Using gaze data, and especially smooth pursuit eye movements, we are
able to track cells in time series of 3D volumetric datasets. The problem of
tracking cells is ubiquitous in developmental biology, where large volumetric
microscopy datasets are acquired on a daily basis, often comprising hundreds or
thousands of time points that span hours or days. The image data, however, is
only a means to an end, and scientists are often interested in the
reconstruction of cell trajectories and cell lineage trees. Reliably tracking
cells in crowded three-dimensional space over many timepoints remains an open
problem, and many current approaches rely on tedious manual annotation and
curation. In our Bionic Tracking approach, we substitute the usual 2D
point-and-click annotation to track cells with eye tracking in a virtual
reality headset, where users simply have to follow a cell with their eyes in 3D
space in order to track it. We detail the interaction design of our approach
and explain the graph-based algorithm used to connect different time points,
also taking occlusion and user distraction into account. We demonstrate our
cell tracking method using the example of two different biological datasets.
Finally, we report on a user study with seven cell tracking experts,
demonstrating the benefits of our approach over manual point-and-click
tracking.","['Ulrik Günther', 'Kyle I. S. Harrington', 'Raimund Dachselt', 'Ivo F. Sbalzarini']",2020-05-01T14:08:40Z,http://arxiv.org/abs/2005.00387v2,"['cs.HC', 'cs.GR']","Bionic Tracking,Eye Tracking,Biological Cells,Virtual Reality,Smooth Pursuit Eye Movements,3D Volumetric Datasets,Developmental Biology,Cell Trajectories,Cell Lineage Trees,Graph-Based Algorithm"
"Trick the Body Trick the Mind: Avatar representation affects the
  perception of available action possibilities in Virtual Reality","In immersive Virtual Reality (VR), your brain can trick you into believing
that your virtual hands are your real hands. Manipulating the representation of
the body, namely the avatar, is a potentially powerful tool for the design of
innovative interactive systems in VR. In this study, we investigated
interactive behavior in VR by using the methods of experimental psychology.
Objects with handles are known to potentiate the afforded action. Participants
tend to respond faster when the handle is on the same side as the responding
hand in bi-manual speed response tasks. In the first experiment, we
successfully replicated this affordance effect in a Virtual Reality (VR)
setting. In the second experiment, we showed that the affordance effect was
influenced by the avatar, which was manipulated by two different hand types: 1)
hand models with full finger tracking that are able to grasp objects, and 2)
capsule-shaped -- fingerless -- hand models that are not able to grasp objects.
We found that less than 5 minutes of adaptation to an avatar, significantly
altered the affordance perception. Counter intuitively, action planning was
significantly shorter with the hand model that is not able to grasp. Possibly,
fewer action possibilities provided an advantage in processing time. The
presence of a handle speeded up the initiation of the hand movement but slowed
down the action completion because of ongoing action planning. The results were
examined from a multidisciplinary perspective and the design implications for
VR applications were discussed.","['Tugce Akkoc', 'Emre Ugur', 'Inci Ayhan']",2020-07-26T03:35:08Z,http://arxiv.org/abs/2007.13048v1,['cs.HC'],"Virtual Reality,Avatar representation,Affordance effect,Experimental psychology,Hand model,Action possibilities"
"Flexible Virtual Reality System for Neurorehabilitation and Quality of
  Life Improvement","As life expectancy is mostly increasing, the incidence of many neurological
disorders is also constantly growing. For improving the physical functions
affected by a neurological disorder, rehabilitation procedures are mandatory,
and they must be performed regularly. Unfortunately, neurorehabilitation
procedures have disadvantages in terms of costs, accessibility and a lack of
therapists. This paper presents Immersive Neurorehabilitation Exercises Using
Virtual Reality (INREX-VR), our innovative immersive neurorehabilitation system
using virtual reality. The system is based on a thorough research methodology
and is able to capture real-time user movements and evaluate joint mobility for
both upper and lower limbs, record training sessions and save electromyography
data. The use of the first-person perspective increases immersion, and the
joint range of motion is calculated with the help of both the HTC Vive system
and inverse kinematics principles applied on skeleton rigs. Tutorial exercises
are demonstrated by a virtual therapist, as they were recorded with real-life
physicians, and sessions can be monitored and configured through tele-medicine.
Complex movements are practiced in gamified settings, encouraging
self-improvement and competition. Finally, we proposed a training plan and
preliminary tests which show promising results in terms of accuracy and user
feedback. As future developments, we plan to improve the system's accuracy and
investigate a wireless alternative based on neural networks.","['Iulia-Cristina Stanica', 'Florica Moldoveanu', 'Giovanni-Paul Portelli', 'Maria-Iuliana Dascalu', 'Alin Moldoveanu', 'Mariana Georgiana Ristea']",2020-11-06T21:04:00Z,http://arxiv.org/abs/2011.03596v1,"['cs.GR', 'I.3.7; I.3.8']","Neurorehabilitation,Virtual Reality,Immersive,Joint mobility,Electromyography,HTC Vive,Inverse kinematics,Tele-medicine,Gamified settings,Neural networks."
"An ecologically valid examination of event-based and time-based
  prospective memory using immersive virtual reality: the effects of delay and
  task type on everyday prospective memory","Recent research has focused on assessing either event- or time-based
prospective memory (PM) using laboratory tasks. Yet, the findings pertaining to
PM performance on laboratory tasks are often inconsistent with the findings on
corresponding naturalistic experiments. Ecologically valid neuropsychological
tasks resemble the complexity and cognitive demands of everyday tasks, offer an
adequate level of experimental control, and allow a generalisation of the
findings to everyday performance. The Virtual Reality Everyday Assessment Lab
(VR-EAL), an immersive virtual reality neuropsychological battery with enhanced
ecological validity, was implemented to comprehensively assess everyday PM
(i.e., focal and non-focal event-based, and time-based). The effects of the
length of delay between encoding and initiating the PM intention and the type
of PM task on everyday PM performance were examined. The results revealed that
everyday PM performance was affected by the length of delay rather than the
type of PM task. The effect of the length of delay differentially affected
performance on the focal, non-focal, and time-based tasks and was proportional
to the PM cue focality (i.e., semantic relationship with the intended action).
This study also highlighted methodological considerations such as the
differentiation between functioning and ability, distinction of cue attributes,
and the necessity of ecological validity.","['Panagiotis Kourtesis', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-02-20T21:24:12Z,http://arxiv.org/abs/2102.10448v1,"['cs.CY', 'cs.HC', 'J.4; J.3; K.4.0']","ecologically valid,event-based,time-based,prospective memory,immersive virtual reality,delay,task type,everyday performance,neuropsychological,cue"
"An ecologically valid examination of event-based and time-based
  prospective memory using immersive virtual reality: the influence of
  attention, memory, and executive function processes on real-world prospective
  memory","Studies on prospective memory (PM) predominantly assess either event- or
time-based PM by implementing non-ecological laboratory-based tasks. The
results deriving from these paradigms have provided findings that are
discrepant with ecologically valid research paradigms that converge on the
complexity and cognitive demands of everyday tasks. The Virtual Reality
Everyday Assessment Lab (VR-EAL), an immersive virtual reality (VR)
neuropsychological battery with enhanced ecological validity, was implemented
to assess everyday event- and time-based PM, as well as the influence of other
cognitive functions on everyday PM functioning. The results demonstrated the
importance of delayed recognition, planning, and visuospatial attention on
everyday PM. Delayed recognition and planning ability were found to be central
in event- and time-based PM respectively. In order of importance, delayed
recognition, visuospatial attention speed, and planning ability were found to
be involved in event-based PM functioning. Comparably, planning, visuospatial
attention accuracy, delayed recognition, and multitasking/task-shifting ability
were found to be involved in time-based PM functioning. These findings further
suggest the importance of ecological validity in the study of PM, which may be
achieved using immersive VR paradigms.","['Panagiotis Kourtesis', 'Sarah E. MacPherson']",2021-02-23T12:19:34Z,http://arxiv.org/abs/2102.11652v1,"['cs.CY', 'cs.HC', 'J.3; J.4; K.4']","prospective memory,immersive virtual reality,attention,memory,executive function,event-based,time-based,visuospatial attention,planning,multitasking"
"The Virtual Emotion Loop: Towards Emotion-Driven Services via Virtual
  Reality","The importance of emotions in service and in product design is well known.
However, it is still not very well understood how users' emotions can be
incorporated in a product or service lifecycle. We argue that this gap is due
to a lack of a methodological framework for an effective investigation of the
emotional response of persons when using products and services. Indeed, the
emotional response of users is generally investigated by means of methods
(e.g., surveys) that are not effective for this purpose. In our view, Virtual
Reality (VR) technologies represent the perfect medium to evoke and recognize
users' emotional response, as well as to prototype products and services (and,
for the latter, even deliver them). In this paper, we first provide our
definition of emotion-driven services, and then we propose a novel
methodological framework, referred to as the Virtual-Reality-Based
Emotion-Elicitation-and-Recognition loop (VEE-loop), that can be exploited to
realize it. Specifically, the VEE-loop consists in a continuous monitoring of
users' emotions, which are then provided to service designers as an implicit
users' feedback. This information is used to dynamically change the content of
the VR environment, until the desired affective state is solicited. Finally, we
discuss issues and opportunities of this VEE-loop, and we also present
potential applications of the VEE-loop in research and in various application
areas.","['Davide Andreoletti', 'Luca Luceri', 'Tiziano Leidi', 'Achille Peternier', 'Silvia Giordano']",2021-02-26T11:36:29Z,http://arxiv.org/abs/2102.13407v3,['cs.HC'],"emotions,service design,product design,methodological framework,Virtual Reality (VR) technologies,emotional response,prototype,users' feedback,affective state,applications"
"Combining the Projective Consciousness Model and Virtual Humans to
  assess ToM capacity in Virtual Reality: a proof-of-concept","Relating explicit psychological mechanisms and observable behaviours is a
central aim of psychological and behavioural science. We implemented the
principles of the Projective Consciousness Model into artificial agents
embodied as virtual humans, as a proof-of-concept for a methodological
framework aimed at simulating behaviours and assessing underlying psychological
parameters, in the context of experiments in virtual reality. We focus on
simulating the role of Theory of Mind (ToM) in the choice of strategic
behaviours of approach and avoidance to optimise the satisfaction of agents'
preferences. We designed an experiment in a virtual environment that could be
used with real humans, allowing us to classify behaviours as a function of
order of ToM, up to the second order. We show that our agents demonstrate
expected behaviours with consistent parameters of ToM in this experiment. We
also show that the agents can be used to estimate correctly each other order of
ToM. A similar approach could be used with real humans in virtual reality
experiments not only to enable human participants to interact with parametric,
virtual humans as stimuli, but also as a mean of inference to derive
model-based psychological assessments of the participants.","['David Rudrauf', 'Grégoire Sergeant-Perthuis', 'Yvain Tisserand', 'Teerawat Monnor', 'Olivier Belli']",2021-04-02T23:55:39Z,http://arxiv.org/abs/2104.07053v2,"['q-bio.NC', 'q-bio.QM']","Projective Consciousness Model,Virtual Humans,ToM capacity,Virtual Reality,proof-of-concept,Theory of Mind,strategic behaviours,artificial agents,virtual environment,psychological parameters."
Metrics for 3D Object Pointing and Manipulation in Virtual Reality,"Assessing the performance of human movements during teleoperation and virtual
reality is a challenging problem, particularly in 3D space due to complex
spatial settings. Despite the presence of a multitude of metrics, a compelling
standardized 3D metric is yet missing, aggravating inter-study comparability
between different studies. Hence, evaluating human performance in virtual
environments is a long-standing research goal, and a performance metric that
combines two or more metrics under one formulation remains largely unexplored,
particularly in higher dimensions. The absence of such a metric is primarily
attributed to the discrepancies between pointing and manipulation, the complex
spatial variables in 3D, and the combination of translational and rotational
movements altogether. In this work, four experiments were designed and
conducted with progressively higher spatial complexity to study and compare
existing metrics thoroughly. The research goal was to quantify the difficulty
of these 3D tasks and model human performance sufficiently in full 3D
peripersonal space. Consequently, a new model extension has been proposed and
its applicability has been validated across all the experimental results,
showing improved modelling and representation of human performance in combined
movements of 3D object pointing and manipulation tasks than existing work.
Lastly, the implications on 3D interaction, teleoperation and object task
design in virtual reality are discussed.","['Eleftherios Triantafyllidis', 'Wenbin Hu', 'Christopher McGreavy', 'Zhibin Li']",2021-06-12T01:05:50Z,http://arxiv.org/abs/2106.06655v1,"['cs.HC', 'cs.RO']","3D object,Pointing,Manipulation,Virtual reality,Metrics,Human performance,Spatial settings,Teleoperation,Inter-study comparability,Rotational movements"
"BIM LOD + Virtual Reality -- Using Game Engine for Visualization in
  Architectural & Construction Education","Architectural Education faces limitations due to its tactile approach to
learning in classrooms with only 2-D and 3-D tools. At a higher level, virtual
reality provides a potential for delivering more information to individuals
undergoing design learning. This paper investigates a hypothesis establishing
grounds towards a new research in Building Information Modeling (BIM) and
Virtual Reality (VR). The hypothesis is projected to determine best practices
for content creation and tactile object virtual interaction, which potentially
can improve learning in architectural & construction education with a less
costly approach and ease of access to well-known buildings. We explored this
hypothesis in a step-by-step game design demonstration in VR, by showcasing the
exploration of the Farnsworth House and reproducing assemblage of the same with
different game levels of difficulty which correspond with varying BIM levels of
development (LODs). The game design prototype equally provides an entry way and
learning style for users with or without a formal architectural or construction
education seeking to understand design tectonics within diverse or
cross-disciplinary study cases. This paper shows that developing geometric
abstract concepts of design pedagogy, using varying LODs for game content and
levels, while utilizing newly developed features such as snap-to-grid,
snap-to-position and snap-to-angle to improve user engagement during assemblage
may provide deeper learning objectives for architectural precedent study.","['Hassan Anifowose', 'Wei Yan', 'Manish Dixit']",2022-01-24T21:12:40Z,http://arxiv.org/abs/2201.09954v1,"['cs.HC', 'cs.GT']","Building Information Modeling,Virtual Reality,Game Engine,Visualization,Architectural Education,Construction Education,BIM Levels of Development,LODs,Design Learning,Design Pedagogy"
"Virtual Reality Assisted Human Perception in ADAS Development: a Munich
  3D Model Study","As the development of autonomous driving (AD) and advanced driver assistance
systems (ADAS) progresses, the relevance of the comfort of users is gaining
increasing interest. It becomes significant to test and validate perceived
comfort performance from the early phase of system development before driving
on roads. Most of the present ADAS test procedures are not efficient in
performing such comfort evaluation. One of the main challenges is to integrate
high-quality, realistic and predictable virtual traffic scenarios into an ADAS
testing framework that has physics-based sensors capable of sensing the virtual
environment. In this paper, we present our development of a virtual reality
based ADAS testing framework that enhances human perception evaluation. The
main contribution relies on three aspects. First, we introduce our development
of a large and high-quality (realism, structure, texture) 3D traffic model of
the Munich city in Germany. Second, we optimize the 3D model for virtual
reality purpose, and real-time capable for human-in-the-loop ADAS testing.
Finally, the model is then integrated into an ADAS framework for testing and
validating ADAS functionalities and perceived comfort performance. The
developed framework components are presented with illustrative examples.","['Felix Bognar', 'Markus Oster', 'Herman Van der Auweraer', 'Tong Duy Son']",2022-08-12T12:07:42Z,http://arxiv.org/abs/2208.07208v3,['cs.HC'],"virtual reality,human perception,ADAS development,3D model,autonomous driving,advanced driver assistance systems,comfort evaluation,virtual traffic scenarios,sensors,Munich"
"""Seeing the Faces Is So Important"" -- Experiences From Online Team
  Meetings on Commercial Virtual Reality Platforms","During the Covid-19 pandemic, online meetings became common for daily
teamwork in the home office. To understand the opportunities and challenges of
meeting in virtual reality (VR) compared to video conferences, we conducted the
weekly team meetings of our human-computer interaction research lab on five
off-the-shelf online meeting platforms over four months. After each of the 12
meetings, we asked the participants (N = 32) to share their experiences,
resulting in 200 completed online questionnaires. We evaluated the ratings of
the overall meeting experience and conducted an exploratory factor analysis of
the quantitative data to compare VR meetings and video calls in terms of
meeting involvement and co-presence. In addition, a thematic analysis of the
qualitative data revealed genuine insights covering five themes: spatial
aspects, meeting atmosphere, expression of emotions, meeting productivity, and
user needs. We reflect on our findings gained under authentic working
conditions, derive lessons learned for running successful team meetings in VR
supporting different kinds of meeting formats, and discuss the team's long-term
platform choice.","['Michael Bonfert', 'Anke V. Reinschluessel', 'Susanne Putze', 'Yenchin Lai', 'Dmitry Alexandrovsky', 'Rainer Malaka', 'Tanja Döring']",2022-10-12T13:19:26Z,http://arxiv.org/abs/2210.06190v2,"['cs.HC', 'H.5.3; H.5.2']","online team meetings,virtual reality,VR platforms,human-computer interaction,video conferences,meeting involvement,co-presence,thematic analysis,meeting productivity,user needs"
"VRContour: Bringing Contour Delineations of Medical Structures Into
  Virtual Reality","Contouring is an indispensable step in Radiotherapy (RT) treatment planning.
However, today's contouring software is constrained to only work with a 2D
display, which is less intuitive and requires high task loads. Virtual Reality
(VR) has shown great potential in various specialties of healthcare and health
sciences education due to the unique advantages of intuitive and natural
interactions in immersive spaces. VR-based radiation oncology integration has
also been advocated as a target healthcare application, allowing providers to
directly interact with 3D medical structures. We present VRContour and
investigate how to effectively bring contouring for radiation oncology into VR.
Through an autobiographical iterative design, we defined three design spaces
focused on contouring in VR with the support of a tracked tablet and VR stylus,
and investigating dimensionality for information consumption and input (either
2D or 2D + 3D). Through a within-subject study (n = 8), we found that
visualizations of 3D medical structures significantly increase precision, and
reduce mental load, frustration, as well as overall contouring effort.
Participants also agreed with the benefits of using such metaphors for learning
purposes.","['Chen Chen', 'Matin Yarmand', 'Varun Singh', 'Michael V. Sherer', 'James D. Murphy', 'Yang Zhang', 'Nadir Weibel']",2022-10-21T23:22:21Z,http://arxiv.org/abs/2210.12298v2,"['cs.HC', 'cs.CY']","Radiotherapy,Contouring,Virtual Reality,Radiation Oncology,3D medical structures,Healthcare,VRContour,Immersive spaces,Iterative design"
"Piloting Virtual Reality Photo-Based Tours among Students of a Filipino
  Language Class: A Case of Emergency Remote Teaching in Japan","The State of Emergency declaration in Japan due to the COVID-19 pandemic
affected many aspects of society in the country, much like the rest of the
world. One sector that felt its disruptive impact was education. As educational
institutions raced to implement emergency remote teaching (ERT) to continue
providing the learning needs of students, some have opened to innovative
interventions. This paper describes a case of ERT where Filipino vocabulary was
taught to a class of Japanese students taking Philippine Studies in a Japanese
university using a cognitive innovation based on virtual reality, an immer-sive
technology often researched for immersion and presence. Students were divided
into three groups to experience six lessons designed around virtual reality
photo-based tours at different immersion levels. While the effect of immersion
on satisfaction was not found to be statistically significant, presence and
satisfaction were found to be correlated. Despite challenges that were
encountered, benefits like enjoyment, increased engagement , and perceived
learning were reported by the students. Our findings exemplify how emerging
multisensory technologies can be used to enhance affective and cognitive
dimensions of human experience while responding to gaps created by the spatial
limitations of remote learning.","['Roberto Bacani Figueroa Jr.', 'Florinda Amparo Adarayan Palma Gil', 'Hiroshi Taniguchi']",2023-01-05T04:26:13Z,http://arxiv.org/abs/2301.01904v1,"['cs.CY', 'cs.MM']","Virtual reality,Photo-based tours,Emergency remote teaching,Filipino language,Cognitive innovation,Immersion,Presence,Satisfaction,Engagement,Multisensory technologies"
"Cybersickness in Virtual Reality Questionnaire (CSQ-VR): A Validation
  and Comparison against SSQ and VRSQ","Cybersickness is a drawback of virtual reality (VR), which also affects the
cognitive and motor skills of the users. The Simulator Sickness Questionnaire
(SSQ), and its variant, the Virtual Reality Sickness Questionnaire (VRSQ) are
two tools that measure cybersickness. However, both tools suffer from important
limitations, which raises concerns about their suitability. Two versions of the
Cybersickness in VR Questionnaire (CSQ-VR), a paper-and-pencil and a 3D-VR
version, were developed. Validation and comparison of CSQ-VR against SSQ and
VRSQ were performed. Thirty-nine participants were exposed to three rides with
linear and angular accelerations in VR. Assessments of cognitive and
psychomotor skills were performed at baseline and after each ride. The validity
of both versions of CSQ-VR was confirmed. Notably, CSQ-VR demonstrated
substantially better internal consistency than both SSQ and VRSQ. Also, CSQ-VR
scores had significantly better psychometric properties in detecting a
temporary decline in performance due to cybersickness. Pupil size was a
significant predictor of cybersickness intensity. In conclusion, the CSQ-VR is
a valid assessment of cybersickness, with superior psychometric properties to
SSQ and VRSQ. The CSQ-VR enables the assessment of cybersickness during VR
exposure, and it benefits from examining pupil size, a biomarker of
cybersickness.","['Panagiotis Kourtesis', 'Josie Linnell', 'Rayaan Amir', 'Ferran Argelaguet', 'Sarah E. MacPherson']",2023-01-30T00:23:18Z,http://arxiv.org/abs/2301.12591v1,"['cs.HC', 'I.3.6; I.3.7; H.5.1; H.5.2; J.4; J.7']","Cybersickness,Virtual Reality,Questionnaire,SSQ,VRSQ,Validation,Comparison,Cognitive skills,Motor skills,Pupil size"
"Fully Immersive Virtual Reality for Skull-base Surgery: Surgical
  Training and Beyond","Purpose: A virtual reality (VR) system, where surgeons can practice
procedures on virtual anatomies, is a scalable and cost-effective alternative
to cadaveric training. The fully digitized virtual surgeries can also be used
to assess the surgeon's skills using measurements that are otherwise hard to
collect in reality. Thus, we present the Fully Immersive Virtual Reality System
(FIVRS) for skull-base surgery, which combines surgical simulation software
with a high-fidelity hardware setup.
  Methods: FIVRS allows surgeons to follow normal clinical workflows inside the
VR environment. FIVRS uses advanced rendering designs and drilling algorithms
for realistic bone ablation. A head-mounted display with ergonomics similar to
that of surgical microscopes is used to improve immersiveness. Extensive
multi-modal data is recorded for post-analysis, including eye gaze, motion,
force, and video of the surgery. A user-friendly interface is also designed to
ease the learning curve of using FIVRS.
  Results: We present results from a user study involving surgeons with various
levels of expertise. The preliminary data recorded by FIVRS differentiates
between participants with different levels of expertise, promising future
research on automatic skill assessment. Furthermore, informal feedback from the
study participants about the system's intuitiveness and immersiveness was
positive.
  Conclusion: We present FIVRS, a fully immersive VR system for skull-base
surgery. FIVRS features a realistic software simulation coupled with modern
hardware for improved realism. The system is completely open-source and
provides feature-rich data in an industry-standard format.","['Adnan Munawar', 'Zhaoshuo Li', 'Nimesh Nagururu', 'Danielle Trakimas', 'Peter Kazanzides', 'Russell H. Taylor', 'Francis X. Creighton']",2023-02-27T15:26:40Z,http://arxiv.org/abs/2302.13878v2,"['cs.RO', 'cs.HC']","Fully Immersive Virtual Reality,Skull-base Surgery,Surgical Training,Virtual Reality System,Cadaveric Training,Surgical Simulation,High-fidelity Hardware,Ablation,Head-mounted Display,User Study"
"A VR-based Priming Framework and Technology Implementation to Improve
  Learning Mindsets and Academic Performance in Post-Secondary Students","Recent research indicates that most post-secondary students in North America
""felt overwhelming anxiety"" in the past few years, negatively affecting
well-being and academic performance. Further research revealed that other
emotions, biases, perceptions, and negative thoughts, can similarly affect
student academic performance. To address this problem, we classify these
counterproductive mindsets, including anxiety, into Scarcity Mindset, a
self-limiting perspective that appropriates cognitive bandwidth required for
essential processes like learning in favour of addressing more critical needs
or perceived insufficiencies. Through a multi-disciplinary literature analysis
of ideas in cognitive science, learning theories and mindsets, and current
technology approaches that are suited to address the limitations of scarcity
thinking, we identify strategies to help transition students to a more positive
Abundance Mindsets. We demonstrate that these priming intervention strategies
can transfer to leading-edge digital environments, particularly Virtual Reality
(VR). Offering further insights into the findings of our two previously
presented studies, we argue that priming interventions related to preparatory
activities and the context priming are transferable to virtual reality
environments. As such, building on our multidisciplinary research insights, we
propose a comprehensive priming model that exploits priming techniques in an
iterative process called Cyclical Priming Methodology (CPM). These intervention
strategies can focus on student preparation, motivation, reflection, the
context of the learning environment, and other aspects of the learning process.
Building on CPM, we further propose a technology implementation within VR
called Virtual Reality Experience Priming (VREP) and discuss the process to
embed CPM/VREP activities within the Experiential Learning Theory (ELT) cycle.","['Dan Hawes', 'Ali Arya']",2023-03-21T02:25:57Z,http://arxiv.org/abs/2303.11547v1,['cs.HC'],"VR-based Priming Framework,Technology Implementation,Learning Mindsets,Academic Performance,Post-Secondary Students,Scarcity Mindset,Abundance Mindsets,Virtual Reality,Cyclical Priming Methodology,Virtual Reality Experience Priming"
"Using a virtual reality interview simulator to explore factors
  influencing people's behavior","Virtual reality interview simulator (VRIS) provides an effective and
manageable approach for candidates prone to being very nervous during
interviews, yet, the major anxiety-inducing elements remain unknown. During an
interview, the anxiety levels, overall experience, and performance of
interviewees might be affected by various circumstances. By analyzing
electrodermal activity and questionnaire, we investigated the influence of five
variables: (I) \textit{Realism}; (II) \textit{Question type}; (III)
\textit{Interviewer attitude}; (IV) \textit{Timing}; and (V)
\textit{Preparation}. As such, an orthogonal design $L_8(4^1 \times 2^4)$ with
eight experiments ($O A_8$ matrix) was implemented, in which 19 college
students took part in the experiments. Considering the anxiety, overall
experience, and performance of the interviewees, results indicate that
\textit{Question type} plays a major role; secondly, \textit{Realism},
\textit{Preparation}, and \textit{Interviewer attitude} all have some degree of
influence; lastly, \textit{Timing} have little to no impact. Specifically,
professional interview questions elicited a greater degree of anxiety than
personal ones among the categories of interview questions. This work
contributes to our understanding of anxiety-stimulating factors during job
interviews in virtual reality and provides cues for designing future VRIS.","['Xinyi Luo', 'Yuyang Wang', 'Lik-Hang Lee', 'Zihan Xing', 'Shan Jin', 'Boya Dong', 'Yuanyi Hu', 'Zeming Chen', 'Jing Yan', 'Pan Hui']",2023-05-13T16:51:31Z,http://arxiv.org/abs/2305.07965v2,['cs.HC'],"Virtual reality interview simulator,Factors,Behavior,Electrodermal activity,Question type,Interviewer attitude,Timing,Preparation,Realism,Anxiety"
"An Analysis of Physiological and Psychological Responses in Virtual
  Reality and Flat Screen Gaming","Recent research has focused on the effectiveness of Virtual Reality (VR) in
games as a more immersive method of interaction. However, there is a lack of
robust analysis of the physiological effects between VR and flatscreen (FS)
gaming. This paper introduces the first systematic comparison and analysis of
emotional and physiological responses to commercially available games in VR and
FS environments. To elicit these responses, we first selected four games
through a pilot study of 6 participants to cover all four quadrants of the
valence-arousal space. Using these games, we recorded the physiological
activity, including Blood Volume Pulse and Electrodermal Activity, and
self-reported emotions of 33 participants in a user study. Our data analysis
revealed that VR gaming elicited more pronounced emotions, higher arousal,
increased cognitive load and stress, and lower dominance than FS gaming. The
Virtual Reality and Flat Screen (VRFS) dataset, containing over 15 hours of
multimodal data comparing FS and VR gaming across different games, is also made
publicly available for research purposes. Our analysis provides valuable
insights for further investigations into the physiological and emotional
effects of VR and FS gaming.","['Ritik Vatsal', 'Shrivatsa Mishra', 'Rushil Thareja', 'Mrinmoy Chakrabarty', 'Ojaswa Sharma', 'Jainendra Shukla']",2023-06-16T08:42:57Z,http://arxiv.org/abs/2306.09690v4,['cs.HC'],"Virtual Reality,Flat Screen,Physiological responses,Psychological responses,Blood Volume Pulse,Electrodermal Activity,User study,Cognitive load,Multimodal data,Emotional effects."
"Multi-Focus Querying of the Human Genome Information on Desktop and in
  Virtual Reality: an Evaluation","The human genome is incredibly information-rich, consisting of approximately
25,000 protein-coding genes spread out over 3.2 billion nucleotide base pairs
contained within 24 unique chromosomes. The genome is important in maintaining
spatial context, which assists in understanding gene interactions and
relationships. However, existing methods of genome visualization that utilize
spatial awareness are inefficient and prone to limitations in presenting gene
information and spatial context. This study proposed an innovative approach to
genome visualization and exploration utilizing virtual reality. To determine
the optimal placement of gene information and evaluate its essentiality in a VR
environment, we implemented and conducted a user study with three different
interaction methods. Two interaction methods were developed in virtual reality
to determine if gene information is better suited to be embedded within the
chromosome ideogram or separate from the ideogram. The final ideogram
interaction method was performed on a desktop and served as a benchmark to
evaluate the potential benefits associated with the use of VR. Our study
findings reveal a preference for VR, despite longer task completion times. In
addition, the placement of gene information within the visualization had a
notable impact on the ability of a user to complete tasks. Specifically, gene
information embedded within the chromosome ideogram was better suited for
single target identification and summarization tasks, while separating gene
information from the ideogram better supported region comparison tasks.","['Gunnar Reiske', 'Sungwon In', 'Yalong Yang']",2023-08-25T16:51:20Z,http://arxiv.org/abs/2308.13487v1,"['cs.HC', 'cs.GR']","human genome,information-rich,protein-coding genes,nucleotide base pairs,chromosomes,genome visualization,spatial context,virtual reality,gene information,ideogram"
"HSVRS: A Virtual Reality System of the Hide-and-Seek Game to Enhance
  Gaze Fixation Ability for Autistic Children","Numerous children diagnosed with Autism Spectrum Disorder (ASD) exhibit
abnormal eye gaze pattern in communication and social interaction. Due to the
high cost of ASD interventions and a shortage of professional therapists,
researchers have explored the use of virtual reality (VR) systems as a
supplementary intervention for autistic children. This paper presents the
design of a novel VR-based system called the Hide and Seek Virtual Reality
System (HSVRS). The HSVRS allows children with ASD to enhance their ocular gaze
abilities while engaging in a hide-and-seek game with a virtual avatar. By
employing face and voice manipulation technology, the HSVRS provides the option
to customize the appearance and voice of the avatar, making it resemble someone
familiar to the child, such as their parents. We conducted a pilot study at the
Third Affiliated Hospital of Sun Yat-sen University, China, to evaluate the
feasibility of HSVRS as an auxiliary intervention for children with autism
(N=24). Through the analysis of subjective questionnaires completed by the
participants' parents and objective eye gaze data, we observed that children in
the VR-assisted intervention group demonstrated better performance compared to
those in the control group. Furthermore, our findings indicate that the
utilization of face and voice manipulation techniques to personalize avatars in
hide-and-seek games can enhance the efficiency and effectiveness of the system.","['Chengyan Yu', 'Shihuan Wang', 'Dong zhang', 'Yingying Zhang', 'Chaoqun Cen', 'Zhixiang you', 'Xiaobing zou', 'Hongzhu Deng', 'Ming Li']",2023-10-20T13:22:40Z,http://arxiv.org/abs/2310.13482v1,"['cs.HC', 'cs.MM']","Virtual reality,Hide-and-seek game,Gaze fixation ability,Autistic children,Autism Spectrum Disorder,Ocular gaze abilities,Avatar,Face and voice manipulation technology,Pilot study,Eye gaze data"
"Simulating Vision Impairment in Virtual Reality -- A Comparison of
  Visual Task Performance with Real and Simulated Tunnel Vision","Purpose: In this work, we explore the potential and limitations of simulating
gaze-contingent tunnel vision conditions using Virtual Reality (VR) with
built-in eye tracking technology. This approach promises an easy and accessible
way of expanding study populations and test groups for visual training, visual
aids, or accessibility evaluations. However, it is crucial to assess the
validity and reliability of simulating these types of visual impairments and
evaluate the extend to which participants with simulated tunnel vision can
represent real patients. Methods: Two age-matched participant groups were
acquired: The first group (n=8 aged 20-60, average 49.1, sd 13.2) consisted of
patients diagnosed with Retinitis pigmentosa (RP). The second group (n=8, aged
27-59, average 46.5, sd 10.8) consisted of visually healthy participants with
simulated tunnel vision. Both groups carried out different visual tasks in a
virtual environment for 30 minutes per day over the course of four weeks. Task
performances as well as gaze characteristics were evaluated in both groups over
the course of the study. Results: Using the ""two one-sided tests for
equivalence"" method, the two groups were found to perform similar in all three
visual tasks. Significant differences between groups were found in different
aspects of their gaze behavior, though most of these aspects seem to converge
over time. Conclusion: Our study evaluates the potential and limitations of
using Virtual Reality technology to simulate the effects of tunnel vision
within controlled virtual environments. We find that the simulation accurately
represents performance of RP patients in the context of group averages, but
fails to fully replicate effects on gaze behavior.","['Alexander Neugebauer', 'Nora Castner', 'Björn Severitt', 'Katarina Stingl', 'Iliya Ivanov', 'Siegfried Wahl']",2023-12-05T14:56:12Z,http://arxiv.org/abs/2312.02812v1,['cs.HC'],"Virtual Reality,Vision Impairment,Tunnel Vision,Eye Tracking,Retinitis Pigmentosa,Visual Tasks,Gaze Characteristics,Simulation,Visual Training,Accessibility"
Stress Management Using Virtual Reality-Based Attention Training,"In this research, we are concerned with the applicability of virtual
reality-based attention training as a tool for stress management. Mental stress
is a worldwide challenge that is still far from being fully managed. This has
maintained a remarkable research attention on developing and validating tools
for detecting and managing stress. Technology-based tools have been at the
heart of these endeavors, including virtual reality (VR) technology.
Nevertheless, the potential of VR lies, to a large part, in the nature of the
content being consumed through such technology. In this study, we investigate
the impact of a special type of content, namely, attention training, on the
feasibility of using VR for stress management. On a group of fourteen
undergraduate engineering students, we conducted a study in which the
participants got exposed twice to a stress inducer while their EEG signals were
being recorded. The first iteration involved VR-based attention training before
starting the stress task while the second time did not. Using multiple features
and various machine learning models, we show that VR-based attention training
has consistently resulted in reducing the number of recognized stress instances
in the recorded EEG signals. This research gives preliminary insights on
adopting VR-based attention training for managing stress, and future studies
are required to replicate the results in larger samples.","['Rojaina Mahmoud', 'Mona Mamdouh', 'Omneya Attallah', 'Ahmad Al-Kabbany']",2023-12-10T22:42:00Z,http://arxiv.org/abs/2312.06025v1,"['eess.SP', 'cs.LG']","virtual reality,attention training,stress management,mental stress,EEG signals,technology-based tools,machine learning models,engineering students,stress inducer,research attention"
"How to Integrate Digital Twin and Virtual Reality in Robotics Systems?
  Design and Implementation for Providing Robotics Maintenance Services in Data
  Centers","In the context of Industry 4.0, the physical and digital worlds are closely
connected, and robots are widely used to achieve system automation. Digital
twin solutions have contributed significantly to the growth of Industry 4.0.
Combining various technologies is a trend that aims to improve system
performance. For example, digital twinning can be combined with virtual reality
in automated systems. This paper proposes a new concept to articulate this
combination, which has mainly been implemented in engineering research
projects. However, there are currently no guidelines, plans, or concepts to
articulate this combination. The concept will be implemented in data centers,
which are crucial for enabling virtual tasks in our daily lives. Due to the
COVID-19 pandemic, there has been a surge in demand for services such as
e-commerce and videoconferencing. Regular maintenance is necessary to ensure
uninterrupted and reliable services. Manual maintenance strategies may not be
sufficient to meet the current high demand, and innovative approaches are
needed to address the problem. This paper presents a novel approach to data
center maintenance: real-time monitoring by an autonomous robot. The robot is
integrated with digital twins of assets and a virtual reality interface that
allows human personnel to control it and respond to alarms. This methodology
enables faster, more cost-effective, and higher quality data center
maintenance. It has been validated in a real data centre and can be used for
intelligent monitoring and management through joint data sources. The method
has potential applications in other automated systems.","['Lin Xie', 'Hanyi Li']",2023-12-20T14:56:09Z,http://arxiv.org/abs/2312.13076v1,"['cs.RO', 'cs.SY', 'eess.SY']","Digital Twin,Virtual Reality,Robotics Systems,Industry 4.0,Automation,Data Centers,Maintenance Services,Autonomous Robot,Real-time Monitoring"
"FocusFlow: 3D Gaze-Depth Interaction in Virtual Reality Leveraging
  Active Visual Depth Manipulation","Gaze interaction presents a promising avenue in Virtual Reality (VR) due to
its intuitive and efficient user experience. Yet, the depth control inherent in
our visual system remains underutilized in current methods. In this study, we
introduce FocusFlow, a hands-free interaction method that capitalizes on human
visual depth perception within the 3D scenes of Virtual Reality. We first
develop a binocular visual depth detection algorithm to understand eye input
characteristics. We then propose a layer-based user interface and introduce the
concept of 'Virtual Window' that offers an intuitive and robust gaze-depth VR
interaction, despite the constraints of visual depth accuracy and precision
spatially at further distances. Finally, to help novice users actively
manipulate their visual depth, we propose two learning strategies that use
different visual cues to help users master visual depth control. Our user
studies on 24 participants demonstrate the usability of our proposed virtual
window concept as a gaze-depth interaction method. In addition, our findings
reveal that the user experience can be enhanced through an effective learning
process with adaptive visual cues, helping users to develop muscle memory for
this brand-new input mechanism. We conclude the paper by discussing strategies
to optimize learning and potential research topics of gaze-depth interaction.","['Chenyang Zhang', 'Tiansu Chen', 'Eric Shaffer', 'Elahe Soltanaghai']",2024-01-23T16:05:02Z,http://arxiv.org/abs/2401.12872v3,['cs.HC'],"Virtual Reality,Gaze interaction,Depth perception,3D scenes,Visual depth detection,User interface,Visual cues,User studies,Learning strategies,Muscle memory."
"From Virtual Reality to the Emerging Discipline of Perception
  Engineering","This paper makes the case that a powerful new discipline, which we term
perception engineering, is steadily emerging. It follows from a progression of
ideas that involve creating illusions, from historical paintings and film, to
video games and virtual reality in modern times. Rather than creating physical
artifacts such as bridges, airplanes, or computers, perception engineers create
illusory perceptual experiences. The scope is defined over any agent that
interacts with the physical world, including both biological organisms (humans,
animals) and engineered systems (robots, autonomous systems). The key idea is
that an agent, called a producer, alters the environment with the intent to
alter the perceptual experience of another agent, called a receiver. Most
importantly, the paper introduces a precise mathematical formulation of this
process, based on the von Neumann-Morgenstern notion of information, to help
scope and define the discipline. It is then applied to the cases of engineered
and biological agents with discussion of its implications on existing fields
such as virtual reality, robotics, and even social media. Finally, open
challenges and opportunities for involvement are identified.","['Steven M. LaValle', 'Evan G. Center', 'Timo Ojala', 'Matti Pouke', 'Nicoletta Prencipe', 'Basak Sakcak', 'Markku Suomalainen', 'Kalle G. Timperi', 'Vadim K. Weinstein']",2024-03-27T14:11:32Z,http://arxiv.org/abs/2403.18588v1,"['cs.HC', 'cs.SY', 'eess.SY']","perception engineering,illusions,virtual reality,perceptual experiences,biological organisms,engineered systems,producer,receiver,mathematical formulation,von Neumann-Morgenstern"
Exploring Bi-Manual Teleportation in Virtual Reality,"Teleportation, a widely-used locomotion technique in Virtual Reality (VR),
allows instantaneous movement within VR environments. Enhanced hand tracking in
modern VR headsets has popularized hands-only teleportation methods, which
eliminate the need for physical controllers. However, these techniques have not
fully explored the potential of bi-manual input, where each hand plays a
distinct role in teleportation: one controls the teleportation point and the
other confirms selections. Additionally, the influence of users' posture,
whether sitting or standing, on these techniques remains unexplored.
Furthermore, previous teleportation evaluations lacked assessments based on
established human motor models such as Fitts' Law. To address these gaps, we
conducted a user study (N=20) to evaluate bi-manual pointing performance in VR
teleportation tasks, considering both sitting and standing postures. We
proposed a variation of the Fitts' Law model to accurately assess users'
teleportation performance. We designed and evaluated various bi-manual
teleportation techniques, comparing them to uni-manual and dwell-based
techniques. Results showed that bi-manual techniques, particularly when the
dominant hand is used for pointing and the non-dominant hand for selection,
enable faster teleportation compared to other methods. Furthermore, bi-manual
and dwell techniques proved significantly more accurate than uni-manual
teleportation. Moreover, our proposed Fitts' Law variation more accurately
predicted users' teleportation performance compared to existing models.
Finally, we developed a set of guidelines for designers to enhance VR
teleportation experiences and optimize user interactions.","['Siddhanth Raja Sindhupathiraja', 'A K M Amanat Ullah', 'William Delamare', 'Khalad Hasan']",2024-04-20T17:35:12Z,http://arxiv.org/abs/2404.13431v1,['cs.HC'],"virtual reality,teleportation,bi-manual input,hand tracking,user study,posture,Fitts' Law,dwell-based techniques,teleportation performance,user interactions"
"The Ability of Virtual Reality Technologies to Improve Comprehension of
  Speech Therapy Device Training","This study evaluates the usage of virtual reality (VR) technologies as a
teaching tool in oral placement therapy, a subset of speech therapy. The
researcher distributed instructional videos using traditional lecture and
modified three-dimensional video to prompt responses. Data was gathered with a
two-part Google Form: In ""Section 1: Knowledge Test"" participants were asked to
determine how well they received the information displayed to them. In ""Section
2: Opinion Test"" participants were asked diagnostic and subjective questions
via Likert scale ranging from 1 (""Strongly Disagree"") to 5 (""Strongly Agree"")
to determine how well they enjoyed viewing the information displayed to them.
Averages for Section 1 were 92.00% for the control group (viewing 2D,
unmodified video) and 77.88% for the experimental group (viewing 3D, VR video).
Almost all participants answered at least 60% of the questions correctly.
Averages for 2D and 3D participants were 4.53/5 and 3.82/5, respectively for
""positive"" prompts. Exactly 50% of participants experiencing VR video preferred
the method to a traditional lecture. This study determines that virtual reality
is viable as a learning tool, but knowledge obtained is not necessarily as high
as using traditional lecture. Further experimentation is required to determine
how well oral placement therapists respond to physically interacting with a
model instead of only viewing it. Copies of the Google Form used to collect
responses, all raw data, and a flowchart outlining each step used to construct
the 3D video can be found in the Appendix.",['Daniel E. Killough'],2024-04-23T21:40:25Z,http://arxiv.org/abs/2404.15534v1,['cs.HC'],"virtual reality,speech therapy,oral placement therapy,instructional videos,3D video,knowledge test,opinion test,Likert scale,traditional lecture,experimental group"
Embedded Reflection Mapping,"Environment maps are used to simulate reflections off curved objects. We
present a technique to reflect a user, or a group of users, in a real
environment, onto a virtual object, in a virtual reality application, using the
live video feeds from a set of cameras, in real-time. Our setup can be used in
a variety of environments ranging from outdoor or indoor scenes.","['Paul Anderson', 'Goncalo Carvalho']",2003-04-08T14:17:53Z,http://arxiv.org/abs/cs/0304011v1,"['cs.GR', 'I.3.7']","Environment maps,Reflections,Curved objects,Embedded,Mapping,Virtual reality,Live video feeds,Cameras,Real-time,Indoor scenes"
The Socceral Force,"We have an audacious dream, we would like to develop a simulation and virtual
reality system to support the decision making in European football (soccer). In
this review, we summarize the efforts that we have made to fulfil this dream
until recently. In addition, an introductory version of FerSML (Footballer and
Football Simulation Markup Language) is presented in this paper.",['Norbert Bátfai'],2010-04-12T16:24:54Z,http://arxiv.org/abs/1004.2003v2,"['cs.AI', 'cs.SE', '68T35', 'H.5.1']","simulation,virtual reality,decision making,European football,soccer,review,FerSML,markup language"
Immersive VR Visualizations by VFIVE. Part 1: Development,"We have been developing a visualization application for CAVE-type virtual
reality (VR) systems for more than a decade. This application, VFIVE, is
currently used in several CAVE systems in Japan for routine visualizations. It
is also used as a base system of further developments of advanced
visualizations. The development of VFIVE is summarized.","['Akira Kageyama', 'Nobuaki Ohno']",2013-01-25T11:03:18Z,http://arxiv.org/abs/1301.6007v1,"['cs.GR', 'physics.comp-ph']","visualization,application,CAVE,virtual reality,VR systems,development,advanced visualizations"
"Machine Learning Distinguishes Neurosurgical Skill Levels in a Virtual
  Reality Tumor Resection Task","Background: Virtual reality simulators and machine learning have the
potential to augment understanding, assessment and training of psychomotor
performance in neurosurgery residents. Objective: This study outlines the first
application of machine learning to distinguish ""skilled"" and ""novice""
psychomotor performance during a virtual reality neurosurgical task. Methods:
Twenty-three neurosurgeons and senior neurosurgery residents comprising the
""skilled"" group and 92 junior neurosurgery residents and medical students the
""novice"" group. The task involved removing a series of virtual brain tumors
without causing injury to surrounding tissue. Over 100 features were extracted
and 68 selected using t-test analysis. These features were provided to 4
classifiers: K-Nearest Neighbors, Parzen Window, Support Vector Machine, and
Fuzzy K-Nearest Neighbors. Equal Error Rate was used to assess classifier
performance. Results: Ratios of train set size to test set size from 10% to 90%
and 5 to 30 features, chosen by the forward feature selection algorithm, were
employed. A working point of 50% train to test set size ratio and 15 features
resulted in an equal error rates as low as 8.3% using the Fuzzy K-Nearest
Neighbors classifier. Conclusion: Machine learning may be one component helping
realign the traditional apprenticeship educational paradigm to a more objective
model based on proven performance standards.
  Keywords: Artificial intelligence, Classifiers, Machine learning,
Neurosurgery skill assessment, Surgical education, Tumor resection, Virtual
reality simulation","['Samaneh Siyar', 'Hamed Azarnoush', 'Saeid Rashidi', 'Alexandre Winkler-Schwartz', 'Vincent Bissonnette', 'Nirros Ponnudurai', 'Rolando F. Del Maestro']",2018-11-20T10:09:02Z,http://arxiv.org/abs/1811.08159v1,"['cs.LG', 'stat.ML']","machine learning,neurosurgical skill levels,virtual reality,tumor resection,classifiers,support vector machine,fuzzy k-nearest neighbors,psychomotor performance,neurosurgery residents,equal error rate"
Non-Euclidean Virtual Reality III: Nil,"We describe a method of rendering real-time scenes in Nil geometry, and use
this to give an expository account of some interesting geometric phenomena. You
can play around with the simulation at www.3-dimensional.space/nil.html.","['Rémi Coulon', 'Elisabetta A. Matsumoto', 'Henry Segerman', 'Steve Trettel']",2020-02-02T23:48:21Z,http://arxiv.org/abs/2002.00513v1,"['math.HO', 'math.GT', 'math.MG', '00A09, 00A66, 53A35, 57K35, 51-04, 68U05, 37D40']","Virtual Reality,Non-Euclidean,Rendering,Real-time scenes,Nil geometry,Geometric phenomena,Simulation"
Raccoons vs Demons: multiclass labeled P300 dataset,"We publish dataset of visual P300 BCI performed in Virtual Reality (VR) game
Raccoons versus Demons (RvD). Data contains reach labels incorporating
information about stimulus chosen enabling us to estimate model's confidence at
each stimulus prediction stage. Data and experiments code are available at
https://gitlab.com/impulse-neiry_public/raccoons-vs-demons","['V. Goncharenko', 'R. Grigoryan', 'A. Samokhina']",2020-04-22T20:10:31Z,http://arxiv.org/abs/2005.02251v2,"['q-bio.NC', 'cs.HC', 'cs.LG', 'stat.ML']","P300 dataset,multiclass labeled,Virtual Reality,BCI,stimulus,model's confidence,experiments code,Raccoons vs Demons"
"Guidelines for the Development of Immersive Virtual Reality Software for
  Cognitive Neuroscience and Neuropsychology: The Development of Virtual
  Reality Everyday Assessment Lab (VR-EAL)","Virtual reality (VR) head-mounted displays (HMD) appear to be effective
research tools, which may address the problem of ecological validity in
neuropsychological testing. However, their widespread implementation is
hindered by VR induced symptoms and effects (VRISE) and the lack of skills in
VR software development. This study offers guidelines for the development of VR
software in cognitive neuroscience and neuropsychology, by describing and
discussing the stages of the development of Virtual Reality Everyday Assessment
Lab (VR-EAL), the first neuropsychological battery in immersive VR. Techniques
for evaluating cognitive functions within a realistic storyline are discussed.
The utility of various assets in Unity, software development kits, and other
software are described so that cognitive scientists can overcome challenges
pertinent to VRISE and the quality of the VR software. In addition, this pilot
study attempts to evaluate VR-EAL in accordance with the necessary criteria for
VR software for research purposes. The VR neuroscience questionnaire (VRNQ;
Kourtesis et al., 2019b) was implemented to appraise the quality of the three
versions of VR-EAL in terms of user experience, game mechanics, in-game
assistance, and VRISE. Twenty-five participants aged between 20 and 45 years
with 12-16 years of full-time education evaluated various versions of VR-EAL.
The final version of VR-EAL achieved high scores in every sub-score of the VRNQ
and exceeded its parsimonious cut-offs. It also appeared to have better in-game
assistance and game mechanics, while its improved graphics substantially
increased the quality of the user experience and almost eradicated VRISE. The
results substantially support the feasibility of the development of effective
VR research and clinical software without the presence of VRISE during a
60-minute VR session.","['Panagiotis Kourtesis', 'Danai Korre', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-01-20T14:55:57Z,http://arxiv.org/abs/2101.08166v1,"['cs.HC', 'cs.CY', 'cs.MM', 'B.8; C.4; D.0; J.4']","Immersive Virtual Reality Software,Cognitive Neuroscience,Neuropsychology,Virtual Reality Everyday Assessment Lab (VR-EAL),Virtual Reality,Head-Mounted Displays (HMD),Ecological Validity,VR Induced Symptoms and Effects (VRISE),Unity,Software Development Kits (SDKs)"
Demonstrating Eye Movement Biometrics in Virtual Reality,"Thanks to the eye-tracking sensors that are embedded in emerging consumer
devices like the Vive Pro Eye, we demonstrate that it is feasible to deliver
user authentication via eye movement biometrics.","['Dillon Lohr', 'Saide Johnson', 'Samantha Aziz', 'Oleg Komogortsev']",2022-07-05T21:34:15Z,http://arxiv.org/abs/2207.02325v1,['cs.HC'],"eye movement,biometrics,virtual reality,eye-tracking sensors,user authentication,emerging consumer devices,Vive Pro Eye,feasibility,demonstration"
"Social Virtual Reality Avatar Biosignal Animations as Availability
  Status Indicators","In this position paper, we outline our research challenges in Affective
Interactive Systems, and present recent work on visualizing avatar biosignals
for social VR entertainment. We highlight considerations for how biosignals
animations in social VR spaces can (falsely) indicate users' availability
status.","['Abdallah El Ali', 'Sueyoon Lee', 'Pablo Cesar']",2023-02-10T11:05:38Z,http://arxiv.org/abs/2302.05172v1,"['cs.HC', 'H.5.m']","Avatar,Biosignal,Animations,Availability,Social Virtual Reality,Affective Interactive Systems,Visualizing,Entertainment,Status Indicators,Biosignals"
Contextual Integrity of A Virtual (Reality) Classroom,"The multicontextual nature of immersive VR makes it difficult to ensure
contextual integrity of VR-generated information flows using existing privacy
design and policy mechanisms. In this position paper, we call on the HCI
community to do away with lengthy disclosures and permissions models and move
towards embracing privacy mechanisms rooted in Contextual Integrity theory.","['Karoline Brehm', 'Yan Shvartzshnaider', 'David Goedicke']",2023-03-23T21:32:01Z,http://arxiv.org/abs/2303.13684v1,['cs.CY'],"immersive VR,contextual integrity,privacy design,policy mechanisms,HCI community"
Enabling immersive experiences in challenging network conditions,"Immersive experiences, such as remote collaboration and augmented and virtual
reality, require delivery of large volumes of data with consistent ultra-low
latency across wireless networks in fluctuating network conditions. We describe
the high-level design behind a data delivery solution that meets these
requirements and provide synthetic simulations and test results running in
network conditions based on real-world measurements demonstrating the efficacy
of the solution.","['Pooja Aggarwal', 'Michael Luby', 'Lorenz Minder']",2023-04-07T16:55:52Z,http://arxiv.org/abs/2304.03732v1,"['cs.NI', 'cs.MM']","immersive experiences,network conditions,data delivery,ultra-low latency,wireless networks,synthetic simulations,test results"
SnB Collaborative Visualization,"We describe a system for visualization and editing of data in a computational
chemistry environment. The system is a collaborative tool allowing researchers
using virtual reality and/or desktop computer displays to work together on
results of the Shake-and-Bake structure determination application.","['Dave Pape', 'Amin Ghadersohi', 'Josephine Anstey', 'Amit Makwana']",2023-10-03T03:39:30Z,http://arxiv.org/abs/2310.01772v1,"['cs.GR', 'I.3.8']","collaborative visualization,data editing,computational chemistry,virtual reality,desktop computer displays,Shake-and-Bake structure determination"
"VR Research at Fraunhofer IGD, Darmstadt, Germany","We present a historical outline of the research and developments of Virtual
Reality at the Fraunhofer Institute for Computer Graphics (IGD) in Darmstadt,
Germany, from 1990 through 2000.","['Wolfgang Felger', 'Martin Göbel', 'Dirk Reiners', 'Gabriel Zachmann']",2024-03-03T22:29:37Z,http://arxiv.org/abs/2403.01629v2,"['cs.GR', 'K.2; I.3.7; I.3.8']","Virtual Reality,Fraunhofer IGD,research,developments,Germany,Darmstadt,historical outline,Institute for Computer Graphics"
"Visual Data Mining of Genomic Databases by Immersive Graph-Based
  Exploration","Biologists are leading current research on genome characterization
(sequencing, alignment, transcription), providing a huge quantity of raw data
about many genome organisms. Extracting knowledge from this raw data is an
important process for biologists, using usually data mining approaches.
However, it is difficult to deals with these genomic information using actual
bioinformatics data mining tools, because data are heterogeneous, huge in
quantity and geographically distributed. In this paper, we present a new
approach between data mining and virtual reality visualization, called visual
data mining. Indeed Virtual Reality becomes ripe, with efficient display
devices and intuitive interaction in an immersive context. Moreover, biologists
use to work with 3D representation of their molecules, but in a desktop
context. We present a software solution, Genome3DExplorer, which addresses the
problem of genomic data visualization, of scene management and interaction.
This solution is based on a well-adapted graphical and interaction paradigm,
where local and global topological characteristics of data are easily visible,
on the contrary to traditional genomic database browsers, always focused on the
zoom and details level.","['Nicolas Férey', 'Pierre-Emmanuel Gros', 'Joan Hérisson', 'Rachid Gherbi']",2007-05-10T19:09:08Z,http://arxiv.org/abs/0705.1535v1,['q-bio.QM'],"genome characterization,sequencing,alignment,transcription,data mining,bioinformatics,virtual reality visualization,3D representation,genomic data visualization,topological characteristics"
"Integration of a Balanced Virtual Manikin in a Virtual Reality Platform
  aimed at Virtual Prototyping","The work presented here is aimed at introducing a virtual human controller in
a virtual prototyping framework. After a brief introduction describing the
problem solved in the paper, we describe the interest as for digital humans in
the context of concurrent engineering. This leads us to draw a control
architecture enabling to drive virtual humans in a real-time immersed way, and
to interact with the product, through motion capture. Unfortunately, we show
this control scheme can lead to unfeasible movements because of the lack of
balance control. Introducing such a controller is a problem that was never
addressed in the context of real-time. We propose an implementation of a
balance controller, that we insert into the previously described control
scheme. Next section is dedicated to show the results we obtained. Finally, we
propose a virtual reality platform into which the digital character controller
is integrated.","['Antoine Rennuit', 'Alain Micaelli', 'Xavier Merlhiot', 'Claude Andriot', 'François Guillaume', 'Nicolas Chevassus', 'Damien Chablat', 'Patrick Chedmail']",2007-07-24T14:23:37Z,http://arxiv.org/abs/0707.3560v1,['cs.RO'],"virtual manikin,virtual reality platform,virtual prototyping,digital humans,control architecture,balance control,real-time,motion capture,virtual character controller"
"Computational Simulation and 3D Virtual Reality Engineering Tools for
  Dynamical Modeling and Imaging of Composite Nanomaterials","An adventure at engineering design and modeling is possible with a Virtual
Reality Environment (VRE) that uses multiple computer-generated media to let a
user experience situations that are temporally and spatially prohibiting. In
this paper, an approach to developing some advanced architecture and modeling
tools is presented to allow multiple frameworks work together while being
shielded from the application program. This architecture is being developed in
a framework of workbench interactive tools for next generation
nanoparticle-reinforced damping/dynamic systems. Through the use of system, an
engineer/programmer can respectively concentrate on tailoring an engineering
design concept of novel system and the application software design while using
existing databases/software outputs.","['L. -V. Bochkareva', 'M. -V. Kireitseu', 'G. R. Tomlinson', 'H. Altenbach', 'V. Kompis', 'D. Hui']",2007-08-14T08:17:45Z,http://arxiv.org/abs/0708.1818v1,"['cs.CE', 'cond-mat.other']","Computational Simulation,3D Virtual Reality,Engineering Tools,Dynamical Modeling,Composite Nanomaterials,Virtual Reality Environment,Architecture,Modeling Tools,Nanoparticle-reinforced,Dynamic Systems"
Virtual Reality Simulation of Fire Fighting Robot Dynamic and Motion,"This paper presents one approach in designing a Fire Fighting Robot which has
been contested annually in a robotic student competition in many countries
following the rules initiated at the Trinity College. The approach makes use of
computer simulation and animation in a virtual reality environment. In the
simulation, the amount of time, starting from home until the flame is
destroyed, can be confirmed. The efficacy of algorithms and parameter values
employed can be easily evaluated. Rather than spending time building the real
robot in a trial and error fashion, now students can explore more variation of
algorithm, parameter and sensor-actuator configuration in the early stage of
design. Besides providing additional excitement during learning process and
enhancing students understanding to the engineering aspects of the design, this
approach could become a useful tool to increase the chance of winning the
contest.","['Joga D. Setiawan', 'Mochamad Subchan', 'Agus Budiyono']",2008-04-24T10:13:53Z,http://arxiv.org/abs/0804.3882v1,"['cs.RO', 'H.5.1']","Virtual Reality,Simulation,Fire Fighting Robot,Dynamic,Motion,Computer Simulation,Animation,Algorithm,Parameter,Sensor-actuator Configuration"
"Student experiences of virtual reality - a case study in learning
  special relativity","We present a study of student learning through the use of virtual reality. A
software package is used to introduce concepts of special relativity to
students in a game-like environment where users experience the effects of
travelling at near light speeds. From this new perspective, space and time are
significantly different to that experienced in everyday life. The study
explores how students have worked with this environment and how these students
have used this experience in their study of special relativity. A mixed method
approach has been taken to evaluate the outcomes of separate implementations of
the package at two universities. Students found the simulation to be a positive
learning experience and described the subject area as being less abstract after
its use. Also, students were more capable of correctly answering concept
questions relating to special relativity, and a small but measurable
improvement was observed in the final exam.","['D. McGrath', 'M. Wegener', 'T. J. McIntyre', 'C. M. Savage', 'M. Williamson']",2009-11-02T02:06:49Z,http://arxiv.org/abs/0911.0226v1,['physics.ed-ph'],"virtual reality,special relativity,student experiences,learning,software package,mixed method approach,simulation,concept questions,final exam"
"Integrating digital human modeling into virtual environment for
  ergonomic oriented design","Virtual human simulation integrated into virtual reality applications is
mainly used for virtual representation of the user in virtual environment or
for interactions between the user and the virtual avatar for cognitive tasks.
In this paper, in order to prevent musculoskeletal disorders, the integration
of virtual human simulation and VR application is presented to facilitate
physical ergonomic evaluation, especially for physical fatigue evaluation of a
given population. Immersive working environments are created to avoid expensive
physical mock-up in conventional evaluation methods. Peripheral motion capture
systems are used to capture natural movements and then to simulate the physical
operations in virtual human simulation. Physical aspects of human's movement
are then analyzed to determine the effort level of each key joint using inverse
kinematics. The physical fatigue level of each joint is further analyzed by
integrating a fatigue and recovery model on the basis of physical task
parameters. All the process has been realized based on VRHIT platform and a
case study is presented to demonstrate the function of the physical fatigue for
a given population and its usefulness for worker selection.","['Liang Ma', 'Damien Chablat', 'Fouad Bennis', 'Bo Hu', 'Wei Zhang']",2010-12-10T08:09:20Z,http://arxiv.org/abs/1012.2197v1,['cs.RO'],"digitall human modeling,virtual environment,ergonomic design,musculoskeletal disorders,virtual human simulation,virtual reality,physical fatigue evaluation,immersive working environments,motion capture systems,inverse kinematics"
Are temperature reconstructions regionally biased?,"Are temperature reconstructions possibly biased due to regionally differing
density of utilized proxy-networks? This question is assessed utilizing a
simple process-based forward model of tree growth in the virtual reality of two
simulations of the climate of the last millennium with different amplitude of
solar forcing variations. The pseudo-tree ring series cluster in high latitudes
of the northern hemisphere and east Asia. Only weak biases are found for the
full network. However, for a strong solar forcing amplitude the high latitudes
indicate a warmer first half of the last millennium while mid-latitudes and
Asia were slightly colder than the extratropical hemispheric average.
Reconstruction skill is weak or non-existent for two simple reconstruction
schemes, and comparison of virtual reality target and reconstructions reveals
strong deficiencies. The temporal resolution of the proxies has an influence on
the reconstruction task and results are sensitive to the construction of the
proxy-network. Existing regional temperature biases can be attenuated or
accentuated by the skill of the reconstruction approach.",['O. Bothe'],2012-04-26T09:51:06Z,http://arxiv.org/abs/1204.5871v1,['physics.ao-ph'],"temperature reconstructions,proxy-networks,forward model,tree growth,solar forcing variations,virtual reality,pseudo-tree ring series,extratropical hemispheric average,reconstruction skill,temporal resolution"
"Affordable Virtual Reality System Architecture for Representation of
  Implicit Object Properties","A flexible, scalable and affordable virtual reality software system
architecture is proposed. This solution can be easily implemented on different
hardware configurations: on a single computer or on a computer cluster. The
architecture is aimed to be integrated in the workflow for solving engineering
tasks and oriented towards presenting implicit object properties through
multiple sensorial channels (visual, audio and haptic). Implicit properties
represent hidden object features (i.e. magnetization, radiation, humidity,
toxicity, etc.) which cannot be perceived by the observer through his or her
senses but require specialized equipment in order to expand the sensory ability
of the observer. Our approach extends the underlying general scene graph
structure incorporating additional effects nodes for implicit properties
representation.","['Stoyan Maleshkov', 'Dimo Chotrov']",2013-08-27T12:44:54Z,http://arxiv.org/abs/1308.5843v1,['cs.GR'],"virtual reality,software system architecture,hardware configurations,engineering tasks,implicit object properties,sensorial channels,general scene graph structure,effects nodes,specialized equipment"
"Embodied social interaction constitutes social cognition in pairs of
  humans: A minimalist virtual reality experiment","Scientists have traditionally limited the mechanisms of social cognition to
one brain, but recent approaches claim that interaction also realizes cognitive
work. Experiments under constrained virtual settings revealed that interaction
dynamics implicitly guide social cognition. Here we show that embodied social
interaction can be constitutive of agency detection and of experiencing
another`s presence. Pairs of participants moved their ""avatars"" along an
invisible virtual line and could make haptic contact with three identical
objects, two of which embodied the other`s motions, but only one, the other`s
avatar, also embodied the other`s contact sensor and thereby enabled responsive
interaction. Co-regulated interactions were significantly correlated with
identifications of the other`s avatar and reports of the clearest awareness of
the other`s presence. These results challenge folk psychological notions about
the boundaries of mind, but make sense from evolutionary and developmental
perspectives: an extendible mind can offload cognitive work into its
environment.","['Tom Froese', 'Hiroyuki Iizuka', 'Takashi Ikegami']",2014-01-16T20:42:32Z,http://arxiv.org/abs/1401.4158v1,"['nlin.AO', 'cs.HC', 'cs.MA', '91B99', 'H.5.1; H.5.2; H.5.3']","Embodied social interaction,Social cognition,Virtual reality,Agency detection,Haptic contact,Avatars,Responsive interaction,Co-regulated interactions,Evolutionary perspectives"
Virtual Reflexes,"Virtual Reality is used successfully to treat people for regular phobias. A
new challenge is to develop Virtual Reality Exposure Training for social
skills. Virtual actors in such systems have to show appropriate social behavior
including emotions, gaze, and keeping distance. The behavior must be realistic
and real-time. Current approaches consist of four steps: 1) trainee social
signal detection, 2) cognitive-affective interpretation, 3) determination of
the appropriate bodily responses, and 4) actuation. The ""cognitive"" detour of
such approaches does not match the directness of human bodily reflexes and
causes unrealistic responses and delay. Instead, we propose virtual reflexes as
concurrent sensory-motor processes to control virtual actors. Here we present a
virtual reflexes architecture, explain how emotion and cognitive modulation are
embedded, detail its workings, and give an example description of an aggression
training application.","['Catholijn Jonker', 'Joost Broekens', 'Aske Plaat']",2014-04-14T14:07:09Z,http://arxiv.org/abs/1404.3920v1,"['cs.CY', 'cs.HC']","Virtual Reality,Virtual Reflexes,Social Skills,Emotions,Gaze,Distance,Real-time,Social Signal Detection,Cognitive-affective Interpretation,Bodily Responses"
"Virtual reality based approach to protein heavy-atom structure
  reconstruction","A commonly recurring problem in structural protein studies, is the
determination of all heavy atom positions from the knowledge of the central
alpha-carbon coordinates. We employ advances in virtual reality to address the
problem. The outcome is a 3D visualisation based technique where all the heavy
backbone and side chain atoms are treated on equal footing, in terms of the
C-alpha coordinates. Each heavy atom can be visualised on the surfaces of the
different two-spheres, that are centered at the other heavy backbone and side
chain atoms. In particular, the rotamers are visible as clusters which display
strong dependence on the underlying backbone secondary structure. Our method
easily detects those atoms in a crystallographic protein structure which have
been been likely misplaced. Our approach forms a basis for the development of a
new generation, visualisation based side chain construction, validation and
refinement tools. The heavy atom positions are identified in a manner which
accounts for the secondary structure environment, leading to improved accuracy
over existing methods.","['Xubiao Peng', 'Alireza Chenani', 'Shuangwei Hu', 'Yifan Zhou', 'Antti J. Niemi']",2014-12-26T19:31:52Z,http://arxiv.org/abs/1412.7975v1,"['q-bio.BM', 'cond-mat.soft', 'physics.bio-ph']","virtual reality,protein structure,heavy atoms,alpha-carbon coordinates,3D visualization,side chain atoms,rotamers,crystallographic structure,secondary structure,refinement"
"Preprint Extending Touch-less Interaction on Vision Based Wearable
  Device","This is the preprint version of our paper on IEEE Virtual Reality Conference
2015. A touch-less interaction technology on vision based wearable device is
designed and evaluated. Users interact with the application with dynamic
hands/feet gestures in front of the camera. Several proof-of-concept prototypes
with eleven dynamic gestures are developed based on the touch-less interaction.
At last, a comparing user study evaluation is proposed to demonstrate the
usability of the touch-less approach, as well as the impact on user's emotion,
running on a wearable framework or Google Glass.","['Zhihan Lv', 'Liangbing Feng', 'Shengzhong Feng', 'Haibo Li']",2015-04-04T17:12:19Z,http://arxiv.org/abs/1504.01025v2,"['cs.HC', 'cs.CV', 'cs.GR', 'H.1.2; H.5.1']","preprint,touch-less interaction,vision based,wearable device,dynamic gestures,proof-of-concept,user study,usability,wearable framework,Google Glass"
"Preprint A Game Based Assistive Tool for Rehabilitation of Dysphonic
  Patients","This is the preprint version of our paper on 3rd International Workshop on
Virtual and Augmented Assistive Technology (VAAT) at IEEE Virtual Reality 2015
(VR2015). An assistive training tool for rehabilitation of dysphonic patients
is designed and developed according to the practical clinical needs. The
assistive tool employs a space flight game as the attractive logic part, and
microphone arrays as input device, which is getting rid of ambient noise by
setting a specific orientation. The therapist can guide the patient to play the
game as well as the voice training simultaneously side by side, while not
interfere the patient voice. The voice information can be recorded and
extracted for evaluating the long-time rehabilitation progress. This paper
outlines a design science approach for the development of an initial useful
software prototype of such a tool, considering 'Intuitive', 'Entertainment',
'Incentive' as main design factors.","['Zhihan Lv', 'Chantal Esteve', 'Javier Chirivella', 'Pablo Gagliardo']",2015-04-04T17:43:54Z,http://arxiv.org/abs/1504.01030v2,"['cs.HC', 'cs.MM', 'I.3.7; H.5.1']","assistive tool,rehabilitation,dysphonic patients,microphone arrays,space flight game,virtual reality,voice training,software prototype,design science"
"Preprint Virtual Reality GIS and Cloud Service Based Traffic Analysis
  Platform","This is the preprint version of our paper on The 23rd International
Conference on Geoinformatics (Geoinformatics2015). City traffic data has
several characteristics, such as large scale, diverse predictable and
real-time, which falls in the range of definition of Big Data. This paper
proposed a cloud service platform which targets for wise transportation is to
carry out unified management and mining analysis of the huge number of the
multivariate and heterogeneous dynamic transportation information, provides
real-time transportation information, increase the utilization efficiency of
transportation, promote transportation management and service level of travel
information and provide decision support of transportation management by
virtual reality as visual.","['Xiaoming Li', 'Zhihan Lv', 'Weixi Wang', 'Chen Wu', 'Jinxing Hu']",2015-05-05T15:57:36Z,http://arxiv.org/abs/1505.01056v2,['cs.OH'],"Virtual Reality,GIS,Cloud Service,Traffic Analysis,Platform,Big Data,Transportation Management,Real-time,Decision Support"
Change Blindness in 3D Virtual Reality,"In the present change blindness study subjects explored stereoscopic three
dimensional (3D) environments through a virtual reality (VR) headset. A novel
method that tracked the subjects' head movements was used for inducing changes
in the scene whenever the changing object was out of the field of view. The
effect of change location (foreground or background in 3D depth) on change
blindness was investigated. Two experiments were conducted, one in the lab (n =
50) and the other online (n = 25). Up to 25% of the changes were undetected and
the mean overall search time was 27 seconds in the lab study. Results indicated
significantly lower change detection success and more change cycles if the
changes occurred in the background, with no differences in overall search
times. The results confirm findings from previous studies and extend them to 3D
environments. The study also demonstrates the feasibility of online VR
experiments.","['Madis Vasser', 'Markus Kängsepp', 'Jaan Aru']",2015-08-24T12:33:10Z,http://arxiv.org/abs/1508.05782v1,"['q-bio.NC', 'cs.HC']","change blindness,3D,virtual reality,headset,stereoscopic,environments,field of view,depth,change location,change cycles"
ATLASrift - a Virtual Reality application,"We present ATLASrift - a Virtual Reality application that provides an
interactive, immersive visit to ATLAS experiment. We envision it being used in
two different ways: first as an educational and outreach tool - for schools,
universities, museums and interested individuals, and secondly as an event
viewer for ATLAS physicists - for them it will provide a much better spatial
awareness of an event, track and jet directions, occupancies and interactions
with detector structures. Using it, one can learn about the experiment as a
whole, visit individual sub-detectors, view real interactions, or take a
scripted walkthrough explaining questions physicists are trying to answer. We
briefly describe our platform of choice - OculusRift VR system, the development
environment - UnrealEngine, and, in detail, the numerous technically demanding
requirements that had to be fulfilled in order to provide a comfortable user
experience. Plans for future versions include making the experience social by
adding multi-user/virtual presence options, event animation, interactive
virtual demonstrations of key high energy physics concepts, or detector
operating principles.","['Ilija Vukotic', 'Edward Moyse', 'Riccardo Maria Bianchi']",2015-10-30T23:18:34Z,http://arxiv.org/abs/1511.00047v1,"['physics.ins-det', 'hep-ex']","Virtual Reality,ATLAS experiment,immersive visit,educational tool,event viewer,spatial awareness,detector structures,Oculus Rift,Unreal Engine,high energy physics concepts"
"Towards Interconnected Virtual Reality: Opportunities, Challenges and
  Enablers","Just recently, the concept of augmented and virtual reality (AR/VR) over
wireless has taken the entire 5G ecosystem by storm spurring an unprecedented
interest from both academia, industry and others. Yet, the success of an
immersive VR experience hinges on solving a plethora of grand challenges
cutting across multiple disciplines. This article underscores the importance of
VR technology as a disruptive use case of 5G (and beyond) harnessing the latest
development of storage/memory, fog/edge computing, computer vision, artificial
intelligence and others. In particular, the main requirements of wireless
interconnected VR are described followed by a selection of key enablers, then,
research avenues and their underlying grand challenges are presented.
Furthermore, we examine three VR case studies and provide numerical results
under various storage, computing and network configurations. Finally, this
article exposes the limitations of current networks and makes the case for more
theory, and innovations to spearhead VR for the masses.","['Ejder BaştuāE, 'Mehdi Bennis', 'Muriel Médard', 'Mérouane Debbah']",2016-11-16T16:50:57Z,http://arxiv.org/abs/1611.05356v2,['cs.NI'],"virtual reality,5G,immersive experience,storage/memory,fog computing,edge computing,computer vision,artificial intelligence,interconnected VR,wireless networks"
"FISF: Better User Experience using Smaller Bandwidth for Panoramic
  Virtual Reality Video","The panoramic video is widely used to build virtual reality (VR) and is
expected to be one of the next generation Killer-Apps. Transmitting panoramic
VR videos is a challenging task because of two problems: 1) panoramic VR videos
are typically much larger than normal videos but they need to be transmitted
with limited bandwidth in mobile networks. 2) high-resolution and fluent views
should be provided to guarantee a superior user experience and avoid
side-effects such as dizziness and nausea. To address these two problems, we
propose a novel interactive streaming technology, namely Focus-based
Interactive Streaming Framework (FISF). FISF consists of three parts: 1) we use
the classic clustering algorithm DBSCAN to analyze real user data for Video
Focus Detection (VFD); 2) we propose a Focus-based Interactive Streaming
Technology (FIST), including a static version and a dynamic version; 3) we
propose two optimization methods: focus merging and prefetch strategy.
Experimental results show that FISF significantly outperforms the
state-of-the-art. The paper is submitted to Sigcomm 2017, VR/AR Network on 31
Mar 2017 at 10:44:04am EDT.","['Lun Wang', 'Damai Dai', 'Jie Jiang', 'Tong Yang', 'Xiaoke Jiang', 'Zekun Cai', 'Yang Li', 'Xiaoming Li']",2017-04-21T08:41:21Z,http://arxiv.org/abs/1704.06444v1,['cs.MM'],"panoramic video,virtual reality,bandwidth,interactive streaming,DBSCAN,user experience,clustering algorithm,optimization methods,FIST,VR/AR Network"
"Dense 3D Facial Reconstruction from a Single Depth Image in
  Unconstrained Environment","With the increasing demands of applications in virtual reality such as 3D
films, virtual Human-Machine Interactions and virtual agents, the analysis of
3D human face analysis is considered to be more and more important as a
fundamental step for those virtual reality tasks. Due to information provided
by an additional dimension, 3D facial reconstruction enables aforementioned
tasks to be achieved with higher accuracy than those based on 2D facial
analysis. The denser the 3D facial model is, the more information it could
provide. However, most existing dense 3D facial reconstruction methods require
complicated processing and high system cost. To this end, this paper presents a
novel method that simplifies the process of dense 3D facial reconstruction by
employing only one frame of depth data obtained with an off-the-shelf RGB-D
sensor. The experiments showed competitive results with real world data.","['Shu Zhang', 'Hui Yu', 'Ting Wang', 'Junyu Dong', 'Honghai Liu']",2017-04-24T10:58:47Z,http://arxiv.org/abs/1704.07142v1,['cs.CV'],"3D facial reconstruction,single depth image,unconstrained environment,virtual reality,human face analysis,dense model,RGB-D sensor,system cost,competitive results"
"Rotation Blurring: Use of Artificial Blurring to Reduce Cybersickness in
  Virtual Reality First Person Shooters","Users of Virtual Reality (VR) systems often experience vection, the
perception of self-motion in the absence of any physical movement. While
vection helps to improve presence in VR, it often leads to a form of motion
sickness called cybersickness. Cybersickness is a major deterrent to large
scale adoption of VR.
  Prior work has discovered that changing vection (changing the perceived speed
or moving direction) causes more severe cybersickness than steady vection
(walking at a constant speed or in a constant direction). Based on this idea,
we try to reduce the cybersickness caused by character movements in a First
Person Shooter (FPS) game in VR. We propose Rotation Blurring (RB), uniformly
blurring the screen during rotational movements to reduce cybersickness. We
performed a user study to evaluate the impact of RB in reducing cybersickness.
We found that the blurring technique led to an overall reduction in sickness
levels of the participants and delayed its onset. Participants who experienced
acute levels of cybersickness benefited significantly from this technique.","['Pulkit Budhiraja', 'Mark Roman Miller', 'Abhishek K Modi', 'David Forsyth']",2017-10-06T22:02:00Z,http://arxiv.org/abs/1710.02599v1,['cs.HC'],"Virtual Reality,Cybersickness,Vection,Motion Sickness,Rotation Blurring,First Person Shooters,User Study,Screen Blurring,Presence,Perception"
"Touching proteins with virtual bare hands: how to visualize protein-drug
  complexes and their dynamics in virtual reality","The ability to precisely visualize the atomic geometry of the interactions
between a drug and its protein target in structural models is critical in
predicting the correct modifications in previously identified inhibitors to
create more effective next generation drugs. It is currently common practice
among medicinal chemists while attempting the above to access the information
contained in three-dimensional structures by using two-dimensional projections,
which can preclude disclosure of useful features. A more precise visualization
of the three-dimensional configuration of the atomic geometry in the models can
be achieved through the implementation of immersive virtual reality (VR). In
this work, we present a freely available software pipeline for visualising
protein structures through VR. New customer hardware, such as the HTC Vive and
the Oculus Rift utilized in this study, are available at reasonable prices.
Moreover, we have combined VR visualization with fast algorithms for simulating
intramolecular motions of protein flexibility, in an effort to further improve
structure-lead drug design by exposing molecular interactions that might be
hidden in the less informative static models.","['Erick Martins Ratamero', 'Dom Bellini', 'Christopher G. Dowson', 'Rudolf A. Roemer']",2017-10-10T15:28:23Z,http://arxiv.org/abs/1710.03655v1,['q-bio.BM'],"protein-drug complexes,visualization,virtual reality,structural models,inhibitors,medicinal chemists,three-dimensional structures,immersive virtual reality,software pipeline"
"Deep Imitation Learning for Complex Manipulation Tasks from Virtual
  Reality Teleoperation","Imitation learning is a powerful paradigm for robot skill acquisition.
However, obtaining demonstrations suitable for learning a policy that maps from
raw pixels to actions can be challenging. In this paper we describe how
consumer-grade Virtual Reality headsets and hand tracking hardware can be used
to naturally teleoperate robots to perform complex tasks. We also describe how
imitation learning can learn deep neural network policies (mapping from pixels
to actions) that can acquire the demonstrated skills. Our experiments showcase
the effectiveness of our approach for learning visuomotor skills.","['Tianhao Zhang', 'Zoe McCarthy', 'Owen Jow', 'Dennis Lee', 'Xi Chen', 'Ken Goldberg', 'Pieter Abbeel']",2017-10-12T17:02:31Z,http://arxiv.org/abs/1710.04615v2,"['cs.LG', 'cs.RO']","Imitation learning,Deep learning,Complex manipulation tasks,Virtual Reality,Teleoperation,Neural network,Visuomotor skills"
"Multi-Path Cooperative Communications Networks for Augmented and Virtual
  Reality Transmission","Augmented and/or virtual reality (AR/VR) are emerging as one of the main
applications in future fifth generation (5G) networks. To meet the requirements
of lower latency and massive data transmission in AR/VR applications, a
solution with software-defined networking (SDN) architecture is proposed for 5G
small cell networks. On this basis, a multi-path cooperative route (MCR) scheme
is proposed to facilitate the AR/VR wireless transmissions in 5G small cell
networks, in which the delay of MCR scheme is analytically studied.
Furthermore, a service effective energy optimal (SEEO) algorithm is developed
for AR/VR wireless transmission in 5G small cell networks. Simulation results
indicate that both the delay and service effective energy (SEE) of the proposed
MCR scheme outperform the delay and SEE of the conventional single path route
scheme in 5G small cell networks.","['Xiaohu Ge', 'Linghui Pan', 'Qiang Li', 'Guoqiang Mao', 'Song Tu']",2017-10-31T14:08:56Z,http://arxiv.org/abs/1710.11486v1,['cs.NI'],"Cooperative communications,Networks,Augmented reality,Virtual reality,5G,Software-defined networking (SDN),Small cell networks,Multi-path,Energy optimal,Wireless transmission"
"Viewport-aware adaptive 360° video streaming using tiles for
  virtual reality","360{\deg} video is attracting an increasing amount of attention in the
context of Virtual Reality (VR). Owing to its very high-resolution
requirements, existing professional streaming services for 360{\deg} video
suffer from severe drawbacks. This paper introduces a novel end-to-end
streaming system from encoding to displaying, to transmit 8K resolution
360{\deg} video and to provide an enhanced VR experience using Head Mounted
Displays (HMDs). The main contributions of the proposed system are about
tiling, integration of the MPEG-Dynamic Adaptive Streaming over HTTP (DASH)
standard, and viewport-aware bitrate level selection. Tiling and adaptive
streaming enable the proposed system to deliver very high-resolution 360{\deg}
video at good visual quality. Further, the proposed viewport-aware bitrate
assignment selects an optimum DASH representation for each tile in a
viewport-aware manner. The quality performance of the proposed system is
verified in simulations with varying network bandwidth using realistic view
trajectories recorded from user experiments. Our results show that the proposed
streaming system compares favorably compared to existing methods in terms of
PSNR and SSIM inside the viewport.","['Cagri Ozcinar', 'Ana De Abreu', 'Aljosa Smolic']",2017-11-07T10:50:13Z,http://arxiv.org/abs/1711.02386v1,['cs.MM'],"360-degree video,Virtual Reality,streaming,tiles,8K resolution,Head Mounted Displays (HMDs),MPEG-Dynamic Adaptive Streaming over HTTP (DASH),viewport-aware,bitrate"
VRGym: A Virtual Testbed for Physical and Interactive AI,"We propose VRGym, a virtual reality testbed for realistic human-robot
interaction. Different from existing toolkits and virtual reality environments,
the VRGym emphasizes on building and training both physical and interactive
agents for robotics, machine learning, and cognitive science. VRGym leverages
mechanisms that can generate diverse 3D scenes with high realism through
physics-based simulation. We demonstrate that VRGym is able to (i) collect
human interactions and fine manipulations, (ii) accommodate various robots with
a ROS bridge, (iii) support experiments for human-robot interaction, and (iv)
provide toolkits for training the state-of-the-art machine learning algorithms.
We hope VRGym can help to advance general-purpose robotics and machine learning
agents, as well as assisting human studies in the field of cognitive science.","['Xu Xie', 'Hangxin Liu', 'Zhenliang Zhang', 'Yuxing Qiu', 'Feng Gao', 'Siyuan Qi', 'Yixin Zhu', 'Song-Chun Zhu']",2019-04-02T22:55:43Z,http://arxiv.org/abs/1904.01698v1,"['cs.HC', 'cs.RO']","virtual reality,testbed,human-robot interaction,robotics,machine learning,cognitive science,physics-based simulation,ROS bridge,training,algorithm"
"Hacking Nonverbal Communication Between Pedestrians and Vehicles in
  Virtual Reality","We use an immersive virtual reality environment to explore the intricate
social cues that underlie non-verbal communication involved in a pedestrian's
crossing decision. We ""hack"" non-verbal communication between pedestrian and
vehicle by engineering a set of 15 vehicle trajectories, some of which follow
social conventions and some that break them. By subverting social expectations
of vehicle behavior we show that pedestrians may use vehicle kinematics to
infer social intentions and not merely as the state of a moving object. We
investigate human behavior in this virtual world by conducting a study of 22
subjects, with each subject experiencing and responding to each of the
trajectories by moving their body, legs, arms, and head in both the physical
and the virtual world. Both quantitative and qualitative responses are
collected and analyzed, showing that, in fact, social cues can be engineered
through vehicle trajectory manipulation. In addition, we demonstrate that
immersive virtual worlds which allow the pedestrian to move around freely,
provide a powerful way to understand both the mechanisms of human perception
and the social signaling involved in pedestrian-vehicle interaction.","['Henri Schmidt', 'Jack Terwilliger', 'Dina AlAdawy', 'Lex Fridman']",2019-04-02T00:34:32Z,http://arxiv.org/abs/1904.01931v1,['cs.HC'],"virtual reality,nonverbal communication,pedestrian,vehicle,social cues,human behavior,immersive virtual worlds,social signaling,vehicle trajectory,perception"
"Brain on the 3D Visual Art through Virtual Reality; Introducing
  Neuro-Art in a Case Investigation","The reciprocal impact of applied neuroscience and cognitive studies on
humanities has been extensive and growing over the past 30 years of research.
Studies on neuroaesthetics have provided novel insights in visual arts, music
as well as abstract and dramatic art. Neuro-Art is an experimental concept in
applied neuroscience where scientists can study the mechanistic pathways
involved for instance in visual art through which creativity and artistic
capacity might receive further empowerment. Based on the existing evidence, at
least 3 large-scale brain networks are involved simultaneously when one is
submitted to a creativity-related task. The question whether the key brain
regions involved in visual art creativity can be identified and receive
neuromodulation to get empowered prompted us to perform the present case
investigation. Virtual reality and functional quantitative
electroencephalography upon 2- vs 3-dimentional painting were employed to study
cortical neurodynamics in a professional painting artist.","['Ali-Mohammad Kamali', 'Mohammad Taghi Najafi', 'Mohammad Nami']",2019-04-14T07:35:47Z,http://arxiv.org/abs/1904.06645v1,['q-bio.NC'],"3D visual art,virtual reality,neuro-art,applied neuroscience,neuroaesthetics,brain networks,creativity,neuromodulation,functional quantitative electroencephalography,cortical neurodynamics"
"Accessibility of Virtual Reality Locomotion Modalities to Adults and
  Minors","Virtual reality (VR) is an important new technology that is fun-damentally
changing the way people experience entertainment and education content. Due to
the fact that most currently available VR products are one size fits all, the
accessibility of the content design and user interface design, even for healthy
children is not well understood. It requires more research to ensure that
children can have equally good user compared to adults in VR. In our study, we
seek to explore accessibility of locomotion in VR between healthy adults and
minors along both objective and subjective dimensions. We performed a user
experience experiment where subjects completed a simple task of moving and
touching underwater animals in VR using one of four different locomotion
modalities, as well as real-world walking without wearing VR headsets as the
baseline. Our results show that physical body movement that mirrors real-world
movement exclusively is the least preferred by both adults and minors. However,
within the different modalities of controller assisted locomotion there are
variations between adults and minors for preference and challenge levels.","['Zhijiong Huang', 'Yu Zhang', 'Kathryn C. Quigley', 'Ramya Sankar', 'Clemence Wormser', 'Xinxin Mo', 'Allen Y. Yang']",2019-04-16T23:12:41Z,http://arxiv.org/abs/1904.08009v1,['cs.HC'],"Virtual reality,Locomotion modalities,Accessibility,Adults,Minors,User interface design,User experience,Controller assisted locomotion,Subjective dimensions,Objective dimensions"
"Improving Usability, Efficiency, and Safety of UAV Path Planning through
  a Virtual Reality Interface","As the capability and complexity of UAVs continue to increase, the
human-robot interface community has a responsibility to design better ways of
specifying the complex 3D flight paths necessary for instructing them.
Immersive interfaces, such as those afforded by virtual reality (VR), have
several unique traits which may improve the user's ability to perceive and
specify 3D information. These traits include stereoscopic depth cues which
induce a sense of physical space as well as six degrees of freedom (DoF)
natural head-pose and gesture interactions. This work introduces an open-source
platform for 3D aerial path planning in VR and compares it to existing UAV
piloting interfaces. Our study has found statistically significant improvements
in safety and subjective usability over a manual control interface, while
achieving a statistically significant efficiency improvement over a 2D
touchscreen interface. The results illustrate that immersive interfaces provide
a viable alternative to touchscreen interfaces for UAV path planning.","['Jesse Paterson', 'Jiwoong Han', 'Tom Cheng', 'Paxtan Laker', 'David McPherson', 'Joseph Menke', 'Allen Yang']",2019-04-18T05:07:34Z,http://arxiv.org/abs/1904.08593v1,"['cs.HC', 'cs.RO']","UAV,Path Planning,Virtual Reality Interface,3D Flight Paths,Stereoscopic Depth Cues,Six Degrees of Freedom (DoF),Safety,Usability,Efficiency,Aerial Path Planning"
"Using CNNs For Users Segmentation In Video See-Through Augmented
  Virtuality","In this paper, we present preliminary results on the use of deep learning
techniques to integrate the users self-body and other participants into a
head-mounted video see-through augmented virtuality scenario. It has been
previously shown that seeing users bodies in such simulations may improve the
feeling of both self and social presence in the virtual environment, as well as
user performance. We propose to use a convolutional neural network for real
time semantic segmentation of users bodies in the stereoscopic RGB video
streams acquired from the perspective of the user. We describe design issues as
well as implementation details of the system and demonstrate the feasibility of
using such neural networks for merging users bodies in an augmented virtuality
simulation.","['Pierre-Olivier Pigny', 'Lionel Dominjon']",2020-01-02T15:22:36Z,http://arxiv.org/abs/2001.00487v1,"['cs.CV', 'I.2.10, I.3.7', 'I.2.10; I.3.7']","CNNs,segmentation,deep learning techniques,self-body,participants,head-mounted,video see-through,augmented virtuality,convolutional neural network,semantic segmentation"
"Cellular-Connected Wireless Virtual Reality: Requirements, Challenges,
  and Solutions","Cellular-connected wireless connectivity provides new opportunities for
virtual reality(VR) to offer seamless user experience from anywhere at anytime.
To realize this vision, the quality-of-service (QoS) for wireless VR needs to
be carefully defined to reflect human perception requirements. In this paper,
we first identify the primary drivers of VR systems, in terms of applications
and use cases. We then map the human perception requirements to corresponding
QoS requirements for four phases of VR technology development. To shed light on
how to provide short/long-range mobility for VR services, we further list four
main use cases for cellular-connected wireless VR and identify their unique
research challenges along with their corresponding enabling technologies and
solutions in 5G systems and beyond. Last but not least, we present a case study
to demonstrate the effectiveness of our proposed solution and the unique QoS
performance requirements of VR transmission compared with that of traditional
video service in cellular networks.","['Fenghe Hu', 'Yansha Deng', 'Walid Saad', 'Mehdi Bennis', 'A. Hamid Aghvami']",2020-01-13T17:45:10Z,http://arxiv.org/abs/2001.06287v2,"['eess.SP', 'eess.IV']","cellular-connected,wireless virtual reality,quality-of-service,human perception,QoS requirements,VR technology,mobility,use cases,5G systems,enabling technologies"
Inspection of histological 3D reconstructions in virtual reality,"3D reconstruction is a challenging current topic in medical research. We
perform 3D reconstructions from serial sections stained by immunohistological
methods. This paper presents an immersive visualisation solution to quality
control (QC), inspect, and analyse such reconstructions. QC is essential to
establish correct digital processing methodologies. Visual analytics, such as
annotation placement, mesh painting, and classification utility, facilitates
medical research insights. We propose a visualisation in virtual reality (VR)
for these purposes. In this manner, we advance the microanatomical research of
human bone marrow and spleen. Both 3D reconstructions and original data are
available in VR. Data inspection is streamlined by subtle implementation
details and general immersion in VR.","['Oleg Lobachev', 'Moritz Berthold', 'Henriette Pfeffer', 'Michael Guthe', 'Birte S. Steiniger']",2020-09-02T08:21:59Z,http://arxiv.org/abs/2009.00887v2,"['cs.GR', '68U05, 68U35, 92C55, 94A08', 'I.3.7']","3D reconstruction,virtual reality,histological,immunohistological methods,quality control,visual analytics,annotation placement,mesh painting,classification utility,microanatomical"
Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality,"Complex 3D curves can be created by directly drawing mid-air in immersive
environments (Augmented and Virtual Realities). Drawing mid-air strokes
precisely on the surface of a 3D virtual object, however, is difficult;
necessitating a projection of the mid-air stroke onto the user ""intended""
surface curve. We present the first detailed investigation of the fundamental
problem of 3D stroke projection in VR. An assessment of the design requirements
of real-time drawing of curves on 3D objects in VR is followed by the
definition and classification of multiple techniques for 3D stroke projection.
We analyze the advantages and shortcomings of these approaches both
theoretically and via practical pilot testing. We then formally evaluate the
two most promising techniques spraycan and mimicry with 20 users in VR. The
study shows a strong qualitative and quantitative user preference for our novel
stroke mimicry projection algorithm. We further illustrate the effectiveness
and utility of stroke mimicry, to draw complex 3D curves on surfaces for
various artistic and functional design applications.","['Rahul Arora', 'Karan Singh']",2020-09-18T19:01:08Z,http://arxiv.org/abs/2009.09029v2,"['cs.GR', 'cs.HC']","Virtual reality,3D surfaces,Mid-air drawing,Curves,Stroke projection,Techniques,Pilot testing,User preference,Algorithm"
"Eye Movement Feature Classification for Soccer Goalkeeper Expertise
  Identification in Virtual Reality","The latest research in expertise assessment of soccer players has affirmed
the importance of perceptual skills (especially for decision making) by
focusing either on high experimental control or on a realistic presentation. To
assess the perceptual skills of athletes in an optimized manner, we captured
omnidirectional in-field scenes and showed these to 12 expert, 10 intermediate
and 13 novice soccer goalkeepers on virtual reality glasses. All scenes were
shown from the same natural goalkeeper perspective and ended after the return
pass to the goalkeeper. Based on their gaze behavior we classified their
expertise with common machine learning techniques. This pilot study shows
promising results for objective classification of goalkeepers expertise based
on their gaze behaviour and provided valuable insight to inform the design of
training systems to enhance perceptual skills of athletes.","['Benedikt Hosp', 'Florian Schultz', 'Oliver Höner', 'Enkelejda Kasneci']",2020-09-23T12:18:41Z,http://arxiv.org/abs/2009.11676v2,"['cs.HC', 'cs.AI', 'cs.CV', 'cs.LG']","eye movement,feature classification,soccer,goalkeeper,expertise identification,virtual reality,perceptual skills,machine learning techniques,training systems"
"Development of a wheelchair simulator for children with multiple
  disabilities","Virtual reality allows to create situations which can be experimented under
the control of the user, without risks, in a very flexible way. This allows to
develop skills and to have confidence to work in real conditions with real
equipment. VR is then widely used as a training and learning tool. More
recently, VR has also showed its potential in rehabilitation and therapy fields
because it provides users with the ability of repeat their actions several
times and to progress at their own pace. In this communication, we present our
work in the development of a wheelchair simulator designed to allow children
with multiple disabilities to familiarize themselves with the wheelchair.",['Nancy Rodriguez'],2016-01-18T09:23:46Z,http://arxiv.org/abs/1601.04436v1,"['cs.HC', 'cs.GR']","wheelchair simulator,children,multiple disabilities,virtual reality,rehabilitation,therapy,training,learning,skills,confidence"
"Die Zukunft sehen: Die Chancen und Herausforderungen der Erweiterten und
  Virtuellen Realität für industrielle Anwendungen","Digitalization offers chances as well as risks for industrial companies. This
article describes how the area of Mixed Reality, with its manifestations
Augmented and Virtual Reality, can support industrial applications in the age
of digitalization. Starting from a historical perspective on Augmented and
Virtual Reality, this article surveys recent developments in the domain of
Mixed Reality, relevant for industrial use cases.
  ---
  Die Digitalisierung bietet f\""ur Industrieunternehmen neue Chancen, stellt
diese jedoch auch vor Herausforderungen. Dieser Artikel beleuchtet wie das
Gebiet der vermischten Realit\""at mit seinen Auspr\""agungen der erweiterten
Realit\""at und der virtuellen Realit\""at f\""ur industriellen Anwendungen im
Zeitalter der Digitalisierung Vorteile schaffen kann. Ausgehend von einer
historischen Betrachtung, werden aktuelle Entwicklungen auf dem Gebiet der
erweiterten und virtuellen Realit\""at diskutiert.",['Jens Grubert'],2017-09-04T16:07:35Z,http://arxiv.org/abs/1709.01020v1,['cs.HC'],"Digitalization,Mixed Reality,Augmented Reality,Virtual Reality,Industrial applications,Chances,Challenges,Industrial use cases"
"Light Virtual Reality systems for the training of conditionally
  automated vehicle drivers","In conditionally automated vehicles, drivers can engage in secondary
activities while traveling to their destination. However, drivers are required
to appropriately respond, in a limited amount of time, to a take-over request
when the system reaches its functional boundaries. In this context, Virtual
Reality systems represent a promising training and learning tool to properly
familiarize drivers with the automated vehicle and allow them to interact with
the novel equipment involved. In this study, the effectiveness of an
Head-Mounted display (HMD)-based training program for acquiring interaction
skills in automated cars was compared to a user manual and a fixed-base
simulator. Results show that the training system affects the take-over
performances evaluated in a test drive in a high-end driving simulator.
Moreover, self-reported measures indicate that the HMD-based training is
preferred with respect to the other systems.","['Daniele Sportillo', 'Alexis Paljic', 'Luciano Ojeda', 'Philippe Fuchs', 'Vincent Roussarie']",2018-03-13T12:43:00Z,http://arxiv.org/abs/1803.04968v1,['cs.HC'],"conditionally automated vehicles,Virtual Reality systems,training,Head-Mounted display (HMD),automated cars,interaction skills,user manual,fixed-base simulator,driving simulator,take-over performances"
"Manifest the Invisible: Design for Situational Awareness of Physical
  Environments in Virtual Reality","Virtual Reality (VR) provides immersive experiences in the virtual world, but
it may reduce users' awareness of physical surroundings and cause safety
concerns and psychological discomfort. Hence, there is a need of an ambient
information design to increase users' situational awareness (SA) of physical
elements when they are immersed in VR environment. This is challenging, since
there is a tradeoff between the awareness in reality and the interference with
users' experience in virtuality. In this paper, we design five representations
(indexical, symbolic, and iconic with three emotions) based on two dimensions
(vividness and emotion) to address the problem. We conduct an empirical study
to evaluate participants' SA, perceived breaks in presence (BIPs), and
perceived engagement through VR tasks that require movement in space. Results
show that designs with higher vividness evoke more SA, designs that are more
consistent with the virtual environment can mitigate the BIP issue, and
emotion-evoking designs are more engaging.","['Zhenyi He', 'Fengyuan Zhu', 'Ken Perlin', 'Xiaojuan Ma']",2018-09-16T08:43:32Z,http://arxiv.org/abs/1809.05837v1,"['cs.HC', '68U05', 'H.5.m']","Virtual Reality,Situational Awareness,Ambient Information Design,Immersive Experiences,Physical Environments,Empirical Study,Perceived Breaks in Presence,Vividness,Emotion-evoking Designs"
Interactive Camera Network Design using a Virtual Reality Interface,"Traditional literature on camera network design focuses on constructing
automated algorithms. These require problem specific input from experts in
order to produce their output. The nature of the required input is highly
unintuitive leading to an unpractical workflow for human operators. In this
work we focus on developing a virtual reality user interface allowing human
operators to manually design camera networks in an intuitive manner. From real
world practical examples we conclude that the camera networks designed using
this interface are highly competitive with, or superior to those generated by
automated algorithms, but the associated workflow is much more intuitive and
simple. The competitiveness of the human-generated camera networks is
remarkable because the structure of the optimization problem is a well known
combinatorial NP-hard problem. These results indicate that human operators can
be used in challenging geometrical combinatorial optimization problems given an
intuitive visualization of the problem.","['Boris Bogaerts', 'Seppe Sels', 'Steve Vanlanduit', 'Rudi Penne']",2018-09-20T12:33:30Z,http://arxiv.org/abs/1809.07593v1,['cs.HC'],"camera network design,virtual reality interface,human operators,automated algorithms,intuitive manner,optimization problem"
"The use of Virtual Reality in Enhancing Interdisciplinary Research and
  Education","Virtual Reality (VR) is increasingly being recognized for its educational
potential and as an effective way to convey new knowledge to people, it
supports interactive and collaborative activities. Affordable VR powered by
mobile technologies is opening a new world of opportunities that can transform
the ways in which we learn and engage with others. This paper reports our study
regarding the application of VR in stimulating interdisciplinary communication.
It investigates the promises of VR in interdisciplinary education and research.
The main contributions of this study are (i) literature review of theories of
learning underlying the justification of the use of VR systems in education,
(ii) taxonomy of the various types and implementations of VR systems and their
application in supporting education and research (iii) evaluation of
educational applications of VR from a broad range of disciplines, (iv)
investigation of how the learning process and learning outcomes are affected by
VR systems, and (v) comparative analysis of VR and traditional methods of
teaching in terms of quality of learning. This study seeks to inspire and
inform interdisciplinary researchers and learners about the ways in which VR
might support them and also VR software developers to push the limits of their
craft.","['Tiffany Leung', 'Farhana Zulkernine', 'Haruna Isah']",2018-09-23T12:22:11Z,http://arxiv.org/abs/1809.08585v1,"['cs.HC', 'cs.AI', 'F.2.2, I.2.7']","Virtual Reality,Interdisciplinary Research,Education,VR systems,Mobile technologies,Learning theories,Taxonomy,Education applications,Learning outcomes,Traditional teaching"
"A System for Acquiring, Processing, and Rendering Panoramic Light Field
  Stills for Virtual Reality","We present a system for acquiring, processing, and rendering panoramic light
field still photography for display in Virtual Reality (VR). We acquire
spherical light field datasets with two novel light field camera rigs designed
for portable and efficient light field acquisition. We introduce a novel
real-time light field reconstruction algorithm that uses a per-view geometry
and a disk-based blending field. We also demonstrate how to use a light field
prefiltering operation to project from a high-quality offline reconstruction
model into our real-time model while suppressing artifacts. We introduce a
practical approach for compressing light fields by modifying the VP9 video
codec to provide high quality compression with real-time, random access
decompression.
  We combine these components into a complete light field system offering
convenient acquisition, compact file size, and high-quality rendering while
generating stereo views at 90Hz on commodity VR hardware. Using our system, we
built a freely available light field experience application called Welcome to
Light Fields featuring a library of panoramic light field stills for consumer
VR which has been downloaded over 15,000 times.","['Ryan S. Overbeck', 'Daniel Erickson', 'Daniel Evangelakos', 'Matt Pharr', 'Paul Debevec']",2018-10-20T22:26:27Z,http://arxiv.org/abs/1810.08860v1,['cs.GR'],"Acquiring,Processing,Rendering,Panoramic Light Field,Virtual Reality,Light Field Camera,Real-time Reconstruction Algorithm,Prefiltering,Compression,VP9 Video Codec"
"Usability of Virtual Reality Application Through the Lens of the User
  Community: A Case Study","The increasing availability and diversity of virtual reality (VR)
applications highlighted the importance of their usability. Function-oriented
VR applications posed new challenges that are not well studied in the
literature. Moreover, user feedback becomes readily available thanks to modern
software engineering tools, such as app stores and open source platforms. Using
Firefox Reality as a case study, we explored the major types of VR usability
issues raised in these platforms. We found that 77% of usability feedbacks can
be mapped to Nielsen's heuristics while few were mappable to VR-specific
heuristics. This result indicates that Nielsen's heuristics could potentially
help developers address the usability of this VR application in its early
development stage. This work paves the road for exploring tools leveraging the
community effort to promote the usability of function-oriented VR applications.","['Wenting Wang', 'Jinghui Cheng', 'Jin L. C. Guo']",2019-02-20T18:57:26Z,http://arxiv.org/abs/1902.07705v1,['cs.HC'],"virtual reality,usability,user feedback,software engineering,app stores,open source platforms,Firefox Reality,Nielsen's heuristics,VR-specific heuristics,function-oriented VR applications"
"Monopulse-based THz Beam Tracking for Indoor Virtual Reality
  Applications","Terahertz spectrum is being researched upon to provide ultra-high throughput
radio links for indoor applications, e.g., virtual reality (VR), etc. as well
as outdoor applications, e.g., backhaul links, etc. This paper investigates a
monopulse-based beam tracking approach for limited mobility users relying on
sparse massive multiple input multiple output (MIMO) wireless channels. Owing
to the sparsity, beamforming is realized using digitally-controlled radio
frequency (RF) / intermediate-frequency (IF) phase shifters with constant
amplitude constraint for transmit power compliance. A monopulse-based beam
tracking technique, using received signal strength indi-cation (RSSI) is
adopted to avoid feedback overheads for obvious reasons of efficacy and
resource savings. The Matlab implementation of the beam tracking algorithm is
also reported. This Matlab implementation has been kept as general purpose as
possible using functions wherein the channel, beamforming codebooks, monopulse
comparator, etc. can easily be updated for specific requirements and with
minimum code amendments.","['Krishan Kumar Tiwari', 'Vladica Sark', 'Eckhard Grass', 'Rolf Kraemer']",2019-06-04T20:53:37Z,http://arxiv.org/abs/1906.01722v1,"['eess.SP', 'cs.IT', 'math.IT']","THz,beam tracking,indoor,virtual reality,MIMO,RF,IF,phase shifters,RSSI,Matlab"
"Identifying Data And Information Streams In Cyberspace: A
  Multi-Dimensional Perspective","Cyberspace has gradually replaced the physical reality, its role evolving
from a simple enabler of daily live processes to a necessity for modern
existence. As a result of this convergence of physical and virtual realities,
for all processes being critically dependent on networked communications,
information representative of our physical, logical and social thoughts is
constantly being generated in cyberspace. The interconnection and integration
of links between our physical and virtual realities creates a new hyperspace as
a source of data and information. Additionally, significant studies in cyber
analysis have predominantly revolved around a single linear analysis of
information from a single source of evidence (The Network). These studies are
limited in their ability to understand the dynamics of relationships across the
multiple dimensions of cyberspace. This paper introduces a multi-dimensional
perspective for data identification in cyberspace. It provides critical
discussions for identifying entangled relationships amongst entities across
cyberspace.",['Ruth Ikwu'],2019-06-10T01:32:32Z,http://arxiv.org/abs/1906.03757v2,['cs.CY'],"Cyberspace,Data streams,Information streams,Multi-dimensional perspective,Networked communications,Virtual realities,Hyperspace,Cyber analysis"
"Parametric Modelling Within Immersive Environments: Building a Bridge
  Between Existing Tools and Virtual Reality Headsets","Even though architectural modelling radically evolved over the course of its
history, the current integration of Augmented Reality (AR) and Virtual
Reality(VR) components in the corresponding design tasks is mostly limited to
enhancing visualisation. Little to none of these tools attempt to tackle the
challenge of modelling within immersive environments, that calls for new input
modalities in order to move away from the traditional mouse and keyboard
combination. In fact, relying on 2D devices for 3D manipulations does not seem
to be effective as it does not offer the same degrees of freedom. We therefore
present a solution that brings VR modelling capabilities to Grasshopper, a
popular parametric design tool. Together with its associated proof-of-concept
application, our extension offers a glimpse at new perspectives in that field.
By taking advantage of them,one can edit geometries with real-time feedback on
the generated models, without ever leaving the virtual environment. The
distinctive characteristics of VR applications provide a range of benefits
without obstructing design activities. The designer can indeed experience the
architectural models at full scale from a realistic point-of-view and truly
feels immersed right next to them.","['Adrien Coppens', 'Tom Mens', 'Mohamed-Anis Gallas']",2019-06-13T07:58:48Z,http://arxiv.org/abs/1906.05532v1,"['cs.HC', 'D.2.2; H.5.2; J.6']","Parametric modelling,Immersive environments,Virtual Reality,Augmented Reality,Grasshopper,Input modalities,3D manipulation,Real-time feedback,Design activities."
"The DREAMS Project: Improving the Intensive Care Patient Experience with
  Virtual Reality","Purpose: Preliminarily evaluate the feasibility and efficacy of using
meditative virtual reality (VR) to improve the hospital experience of intensive
care unit (ICU) patients.
  Methods: Effects of VR were examined in a non-randomized, single-center
cohort. Fifty-nine patients admitted to the surgical or trauma ICU of the
University of Florida Health Shands Hospital participated. A Google Daydream
headset was used to expose ICU patients to commercially available VR
applications focused on calmness and relaxation (Google Spotlight Stories and
RelaxVR). Sessions were conducted once daily for up to seven days. Outcome
measures included pain level, anxiety, depression, medication administration,
sleep quality, heart rate, respiratory rate, blood pressure, delirium status,
and patient ratings of the VR system. Comparisons were made using paired
t-tests and mixed models where appropriate.
  Results: The VR meditative intervention was found to improve patients' ICU
experience with reduced levels of anxiety and depression; however, there was no
evidence suggesting that VR had any significant effects on physiological
measures, pain, or sleep.
  Conclusion: The use of VR technology in the ICU was shown to be easily
implemented and well-received by patients.","['Triton Ong', 'Matthew Ruppert', 'Parisa Rashidi', 'Tezcan Ozrazgat-Baslanti', 'Azra Bihorac', 'Marko Suvajdzic']",2019-06-27T14:55:05Z,http://arxiv.org/abs/1906.11706v2,['cs.HC'],"Virtual reality,Intensive care,Patient experience,Meditative intervention,Anxiety,Depression,Physiological measures,Pain,Sleep,ICU"
Instant Motion Tracking and Its Applications to Augmented Reality,"Augmented Reality (AR) brings immersive experiences to users. With recent
advances in computer vision and mobile computing, AR has scaled across
platforms, and has increased adoption in major products. One of the key
challenges in enabling AR features is proper anchoring of the virtual content
to the real world, a process referred to as tracking. In this paper, we present
a system for motion tracking, which is capable of robustly tracking planar
targets and performing relative-scale 6DoF tracking without calibration. Our
system runs in real-time on mobile phones and has been deployed in multiple
major products on hundreds of millions of devices.","['Jianing Wei', 'Genzhi Ye', 'Tyler Mullen', 'Matthias Grundmann', 'Adel Ahmadyan', 'Tingbo Hou']",2019-07-16T00:13:09Z,http://arxiv.org/abs/1907.06796v1,['cs.CV'],"Augmented Reality,Motion Tracking,Computer Vision,Mobile Computing,Anchoring,Tracking,Real-Time,6DoF tracking,Calibration"
"Beyond Human: Animals as an Escape from Stereotype Avatars in Virtual
  Reality Games","Virtual reality setups are particularly suited to create a tight bond between
players and their avatars up to a degree where we start perceiving the virtual
representation as our own body. We hypothesize that such an illusion of virtual
body ownership (IVBO) has a particularly high, yet overlooked potential for
nonhumanoid avatars. To validate our claim, we use the example of three very
different creatures---a scorpion, a rhino, and a bird---to explore possible
avatar controls and game mechanics based on specific animal abilities. A
quantitative evaluation underpins the high game enjoyment arising from
embodying such nonhuman morphologies, including additional body parts and
obtaining respective superhuman skills, which allows us to derive a set of
novel design implications. Furthermore, the experiment reveals a correlation
between IVBO and game enjoyment, which is a further indication that nonhumanoid
creatures offer a meaningful design space for VR games worth further
investigation.","['Andrey Krekhov', 'Sebastian Cmentowski', 'Katharina Emmerich', 'Jens Krüger']",2019-07-17T12:18:33Z,http://arxiv.org/abs/1907.07466v1,['cs.HC'],"virtual reality,avatars,illusion of virtual body ownership,nonhumanoid,animal abilities,game mechanics,superhuman skills,design implications,VR games"
A View on Edge caching Applications,"Devices with the ability to connect to the internet are growing in numbers
day by day thus creating the need for a new way of manag-ing the way the
produced traffic travels through data networks. Smart Cities, Vehicular Content
Networks, Healthcare and Virtual Reality Videos are a few examples that require
high volume data while maintaining low latency. Edge caching practices are a
prom-ising solution in such cases in order meet the requirements of low latency
in high volume traffic. This paper is a survey on four indic-ative areas, Smart
Cities, Vehicular Content Networks, Healthcare and Virtual Reality Videos that
make use of edge caching.","['D. Antonogiorgakis', 'A. Britzolakis', 'P. Chatziadam', 'A. Dimitriadis', 'S. Gikas', 'E. Michalodimitrakis', 'M. Oikonomakis', 'N. Siganos', 'E. Tzagkarakis', 'Y. Nikoloudakis', 'S. Panagiotakis', 'E. Pallis', 'E. K. Markakis']",2019-07-18T11:56:01Z,http://arxiv.org/abs/1907.12359v1,['cs.NI'],"Edge caching,Applications,Smart Cities,Vehicular Content Networks,Healthcare,Virtual Reality Videos,Data networks,Low latency,High volume traffic"
"Attack Trees for Security and Privacy in Social Virtual Reality Learning
  Environments","Social Virtual Reality Learning Environment (VRLE) is a novel edge computing
platform for collaboration amongst distributed users. Given that VRLEs are used
for critical applications (e.g., special education, public safety training), it
is important to ensure security and privacy issues. In this paper, we present a
novel framework to obtain quantitative assessments of threats and
vulnerabilities for VRLEs. Based on the use cases from an actual social VRLE
viz., vSocial, we first model the security and privacy using the attack trees.
Subsequently, these attack trees are converted into stochastic timed automata
representations that allow for rigorous statistical model checking. Such an
analysis helps us adopt pertinent design principles such as hardening,
diversity and principle of least privilege to enhance the resilience of social
VRLEs. Through experiments in a vSocial case study, we demonstrate the
effectiveness of our attack tree modeling with a reduction of 26% in
probability of loss of integrity (security) and 80% in privacy leakage
(privacy) in before and after scenarios pertaining to the adoption of the
design principles.","['Samaikya Valluripally', 'Aniket Gulhane', 'Reshmi Mitra', 'Khaza Anuarul Hoque', 'Prasad Calyam']",2019-11-08T22:25:21Z,http://arxiv.org/abs/1911.03563v1,"['cs.CR', 'cs.CY', 'cs.DC']","Attack Trees,Security,Privacy,Social Virtual Reality Learning Environment,Edge Computing,Threats,Vulnerabilities,Stochastic Timed Automata,Design Principles,Model Checking."
"Enhancing User Experience in Virtual Reality with Radial Basis Function
  Interpolation Based Stereoscopic Camera Control","Providing a depth-rich Virtual Reality (VR) experience to users without
causing discomfort remains to be a challenge with today's commercially
available head-mounted displays (HMDs), which enforce strict measures on
stereoscopic camera parameters for the sake of keeping visual discomfort to a
minimum. However, these measures often lead to an unimpressive VR experience
with shallow depth feeling. We propose the first method ready to be used with
existing consumer HMDs for automated stereoscopic camera control in virtual
environments (VEs). Using radial basis function interpolation and projection
matrix manipulations, our method makes it possible to significantly enhance
user experience in terms of overall perceived depth while maintaining visual
discomfort on a par with the default arrangement. In our implementation, we
also introduce the first immersive interface for authoring a unique 3D
stereoscopic cinematography for any VE to be experienced with consumer HMDs. We
conducted a user study that demonstrates the benefits of our approach in terms
of superior picture quality and perceived depth. We also investigated the
effects of using depth of field (DoF) in combination with our approach and
observed that the addition of our DoF implementation was seen as a degraded
experience, if not similar.","['Emre Avan', 'Ufuk Celikcan', 'Tolga K. Capin', 'Hasmet Gurcay']",2019-11-11T18:49:49Z,http://arxiv.org/abs/1911.04446v1,"['cs.GR', 'cs.HC']","Virtual Reality,Radial Basis Function Interpolation,Stereoscopic Camera Control,Head-Mounted Displays,Projection Matrix Manipulations,Immersive Interface,3D Stereoscopic Cinematography,Depth of Field"
Exploring Configurations for Multi-user Communication in Virtual Reality,"Virtual Reality (VR) enables users to collaborate while exploring scenarios
not realizable in the physical world. We propose CollabVR, a distributed
multi-user collaboration environment, to explore how digital content improves
expression and understanding of ideas among groups. To achieve this, we
designed and examined three possible configurations for participants and shared
manipulable objects. In configuration (1), participants stand side-by-side. In
(2), participants are positioned across from each other, mirrored face-to-face.
In (3), called ""eyes-free,"" participants stand side-by-side looking at a shared
display, and draw upon a horizontal surface. We also explored a ""telepathy""
mode, in which participants could see from each other's point of view. We
implemented ""3DSketch"" visual objects for participants to manipulate and move
between virtual content boards in the environment. To evaluate the system, we
conducted a study in which four people at a time used each of the three
configurations to cooperate and communicate ideas with each other. We have
provided experimental results and interview responses.","['Zhenyi He', 'Karl Rosenberg', 'Ken Perlin']",2019-11-15T21:15:46Z,http://arxiv.org/abs/1911.06877v1,"['cs.HC', 'cs.GR']","Virtual Reality,Multi-user communication,Collaboration environment,Digital content,Configurations,Manipulable objects,3DSketch,Virtual content boards,Evaluation,Experimental results"
A Comparative Analysis of Virtual Reality Head-Mounted Display Systems,"With recent advances of Virtual Reality (VR) technology, the deployment of
such will dramatically increase in non-entertainment environments, such as
professional education and training, manufacturing, service, or low
frequency/high risk scenarios. Clinical education is an area that especially
stands to benefit from VR technology due to the complexity, high cost, and
difficult logistics. The effectiveness of the deployment of VR systems, is
subject to factors that may not be necessarily considered for devices targeting
the entertainment market. In this work, we systematically compare a wide range
of VR Head-Mounted Displays (HMDs) technologies and designs by defining a new
set of metrics that are 1) relevant to most generic VR solutions and 2) are of
paramount importance for VR-based education and training. We evaluated ten HMDs
based on various criteria, including neck strain, heat development, and color
accuracy. Other metrics such as text readability, comfort, and contrast
perception were evaluated in a multi-user study on three selected HMDs, namely
Oculus Rift S, HTC Vive Pro and Samsung Odyssey+. Results indicate that the HTC
Vive Pro performs best with regards to comfort, display quality and
compatibility with glasses.","['Arian Mehrfard', 'Javad Fotouhi', 'Giacomo Taylor', 'Tess Forster', 'Nassir Navab', 'Bernhard Fuerst']",2019-12-05T23:01:33Z,http://arxiv.org/abs/1912.02913v1,"['cs.HC', 'cs.CV']","Virtual Reality,Head-Mounted Display Systems,VR technology,education,training,metrics,HMDs,Oculus Rift S,HTC Vive Pro,Samsung Odyssey+"
Position Tracking for Virtual Reality Using Commodity WiFi,"Today, experiencing virtual reality (VR) is a cumbersome experience which
either requires dedicated infrastructure like infrared cameras to track the
headset and hand-motion controllers (e.g., Oculus Rift, HTC Vive), or provides
only 3-DoF (Degrees of Freedom) tracking which severely limits the user
experience (e.g., Samsung Gear). To truly enable VR everywhere, we need
position tracking to be available as a ubiquitous service. This paper presents
WiCapture, a novel approach which leverages commodity WiFi infrastructure,
which is ubiquitous today, for tracking purposes. We prototype WiCapture using
off-the-shelf WiFi radios and show that it achieves an accuracy of 0.88 cm
compared to sophisticated infrared based tracking systems like the Oculus,
while providing much higher range, resistance to occlusion, ubiquity and ease
of deployment.","['Manikanta Kotaru', 'Sachin Katti']",2017-03-09T21:19:48Z,http://arxiv.org/abs/1703.03468v2,"['cs.CV', 'cs.NI']","Position Tracking,Virtual Reality,Commodity WiFi,Infrared Cameras,3-DoF,Degrees of Freedom,WiCapture,Ubiquitous Service,Accuracy,Tracking Systems"
"A controlled study of stereoscopic virtual reality in freshman
  electrostatics","Virtual reality (VR) has long promised to revolutionize education, but with
little follow-through. Part of the reason for this is the prohibitive cost of
immersive VR headsets or caves. This has changed with the advent of
smartphone-based VR (along the lines of Google cardboard) which allows students
to use smartphones and inexpensive plastic or cardboard viewers to enjoy
stereoscopic VR simulations. We have completed the largest-ever such study on
627 students enrolled in calculus-based freshman physics at The Ohio State
University. This initial study focused on student understanding of electric
fields. Students were split into three treatments groups: VR, video, and static
2D images. Students were asked questions before, during, and after treatment.
Here we present a preliminary analysis including overall post-pre improvement
among the treatment groups, dependence of improvement on gender, and previous
video game experience. Results on select questions are discussed. Several
electric field visualizations similar to those used in this study are freely
available on Google Play http://go.osu.edu/BuckeyeVR","['Joseph R. Smith', 'Amber Byrum', 'Timothy M. McCormick', 'Nick Young', 'Chris Orban', 'Chris D. Porter']",2017-07-05T19:16:20Z,http://arxiv.org/abs/1707.01544v1,['physics.ed-ph'],"stereoscopic virtual reality,freshman electrostatics,VR simulations,electric fields,treatment groups,static 2D images,preliminary analysis,Google Play"
Towards a Social Virtual Reality Learning Environment in High Fidelity,"Virtual Learning Environments (VLEs) are spaces designed to educate students
remotely via online platforms. Although traditional VLEs such as iSocial have
shown promise in educating students, they offer limited immersion that
diminishes learning effectiveness. This paper outlines a virtual reality
learning environment (VRLE) over a high-speed network, which promotes
educational effectiveness and efficiency via our creation of flexible content
and infrastructure which meet established VLE standards with improved
immersion. This paper further describes our implementation of multiple learning
modules developed in High Fidelity, a ""social VR"" platform. Our experiment
results show that the VR mode of content delivery better stimulates the
generalization of lessons to the real world than non-VR lessons and provides
improved immersion when compared to an equivalent desktop version.","['Chiara Zizza', 'Adam Starr', 'Devin Hudson', 'Sai Shreya Nuguri', 'Prasad Calyam', 'Zhihai He']",2017-07-18T21:12:37Z,http://arxiv.org/abs/1707.05859v2,"['cs.HC', 'cs.MM']","Virtual Learning Environments,VLEs,Virtual Reality Learning Environment,VRLE,High Fidelity,social VR,educational effectiveness,immersion,content delivery"
"Real-Time Head Gesture Recognition on Head-Mounted Displays using
  Cascaded Hidden Markov Models","Head gesture is a natural means of face-to-face communication between people
but the recognition of head gestures in the context of virtual reality and use
of head gesture as an interface for interacting with virtual avatars and
virtual environments have been rarely investigated. In the current study, we
present an approach for real-time head gesture recognition on head-mounted
displays using Cascaded Hidden Markov Models. We conducted two experiments to
evaluate our proposed approach. In experiment 1, we trained the Cascaded Hidden
Markov Models and assessed the offline classification performance using
collected head motion data. In experiment 2, we characterized the real-time
performance of the approach by estimating the latency to recognize a head
gesture with recorded real-time classification data. Our results show that the
proposed approach is effective in recognizing head gestures. The method can be
integrated into a virtual reality system as a head gesture interface for
interacting with virtual worlds.","['Jingbo Zhao', 'Robert S. Allison']",2017-07-20T19:46:13Z,http://arxiv.org/abs/1707.06691v2,['cs.HC'],"head gesture recognition,head-mounted displays,Cascaded Hidden Markov Models,virtual reality,interface,avatars,real-time,classification,latency,virtual environments"
"Eyemotion: Classifying facial expressions in VR using eye-tracking
  cameras","One of the main challenges of social interaction in virtual reality settings
is that head-mounted displays occlude a large portion of the face, blocking
facial expressions and thereby restricting social engagement cues among users.
Hence, auxiliary means of sensing and conveying these expressions are needed.
We present an algorithm to automatically infer expressions by analyzing only a
partially occluded face while the user is engaged in a virtual reality
experience. Specifically, we show that images of the user's eyes captured from
an IR gaze-tracking camera within a VR headset are sufficient to infer a select
subset of facial expressions without the use of any fixed external camera.
Using these inferences, we can generate dynamic avatars in real-time which
function as an expressive surrogate for the user. We propose a novel data
collection pipeline as well as a novel approach for increasing CNN accuracy via
personalization. Our results show a mean accuracy of 74% ($F1$ of 0.73) among 5
`emotive' expressions and a mean accuracy of 70% ($F1$ of 0.68) among 10
distinct facial action units, outperforming human raters.","['Steven Hickson', 'Nick Dufour', 'Avneesh Sud', 'Vivek Kwatra', 'Irfan Essa']",2017-07-22T19:39:19Z,http://arxiv.org/abs/1707.07204v2,['cs.CV'],"facial expressions,VR,eye-tracking cameras,social interaction,virtual reality,algorithm,IR gaze-tracking camera,dynamic avatars,CNN accuracy"
"Interpatient Respiratory Motion Model Transfer for Virtual Reality
  Simulations of Liver Punctures","Current virtual reality (VR) training simulators of liver punctures often
rely on static 3D patient data and use an unrealistic (sinusoidal) periodic
animation of the respiratory movement. Existing methods for the animation of
breathing motion support simple mathematical or patient-specific, estimated
breathing models. However with personalized breathing models for each new
patient, a heavily dose relevant or expensive 4D data acquisition is mandatory
for keyframe-based motion modeling. Given the reference 4D data, first a model
building stage using linear regression motion field modeling takes place. Then
the methodology shown here allows the transfer of existing reference
respiratory motion models of a 4D reference patient to a new static 3D patient.
This goal is achieved by using non-linear inter-patient registration to warp
one personalized 4D motion field model to new 3D patient data. This cost- and
dose-saving new method is shown here visually in a qualitative proof-of-concept
study.","['Andre Mastmeyer', 'Matthias Wilms', 'Heinz Handels']",2017-07-26T17:34:09Z,http://arxiv.org/abs/1707.08554v2,['cs.CV'],"Virtual reality,Simulator,Liver punctures,Respiratory motion,4D data,Motion modeling,Inter-patient registration,Linear regression,Transfer,Proof-of-concept"
"3D Face Reconstruction with Region Based Best Fit Blending Using Mobile
  Phone for Virtual Reality Based Social Media","The use of virtual reality (VR) is exponentially increasing and due to that
many researchers has started to work on developing new VR based social media.
For this purpose it is important to have an avatar of the users which look like
them to be easily generated by the devices which are accessible, such as mobile
phone. In this paper, we propose a novel method of recreating a 3D human face
model captured with a phone camera image or video data. The method focuses more
on model shape than texture in order to make the face recognizable. We detect
68 facial feature points and use them to separate a face into four regions. For
each area the best fitting models are found and are further morphed combined to
find the best fitting models for each area. These are then combined and further
morphed in order to restore the original facial proportions. We also present a
method of texturing the resulting model, where the aforementioned feature
points are used to generate a texture for the resulting model","['Gholamreza Anbarjafari', 'Rain Eric Haamer', 'Iiris Lusi', 'Toomas Tikk', 'Lembit Valgma']",2017-12-12T07:46:17Z,http://arxiv.org/abs/1801.01089v1,['cs.CV'],"3D face reconstruction,region based,best fit blending,mobile phone,virtual reality,social media,avatar,facial feature points,texture,model shape"
Towards Low-Latency and Ultra-Reliable Virtual Reality,"Virtual Reality (VR) is expected to be one of the killer-applications in 5G
networks. However, many technical bottlenecks and challenges need to be
overcome to facilitate its wide adoption. In particular, VR requirements in
terms of high-throughput, low-latency and reliable communication call for
innovative solutions and fundamental research cutting across several
disciplines. In view of this, this article discusses the challenges and
enablers for ultra-reliable and low-latency VR. Furthermore, in an interactive
VR gaming arcade case study, we show that a smart network design that leverages
the use of mmWave communication, edge computing and proactive caching can
achieve the future vision of VR over wireless.","['Mohammed S. Elbamby', 'Cristina Perfecto', 'Mehdi Bennis', 'Klaus Doppler']",2018-01-23T14:50:02Z,http://arxiv.org/abs/1801.07587v1,"['cs.IT', 'cs.NI', 'math.IT']","Virtual Reality,5G networks,low-latency,ultra-reliable communication,high-throughput,edge computing,mmWave communication,proactive caching"
Effects of Hand Representations for Typing in Virtual Reality,"Alphanumeric text entry is a challenge for Virtual Reality (VR) applications.
VR enables new capabilities, impossible in the real world, such as an
unobstructed view of the keyboard, without occlusion by the user's physical
hands. Several hand representations have been proposed for typing in VR on
standard physical keyboards. However, to date, these hand representations have
not been compared regarding their performance and effects on presence for VR
text entry. Our work addresses this gap by comparing existing hand
representations with minimalistic fingertip visualization. We study the effects
of four hand representations (no hand representation, inverse kinematic model,
fingertip visualization using spheres and video inlay) on typing in VR using a
standard physical keyboard with 24 participants. We found that the fingertip
visualization and video inlay both resulted in statistically significant lower
text entry error rates compared to no hand or inverse kinematic model
representations. We found no statistical differences in text entry speed.","['Jens Grubert', 'Lukas Witzani', 'Eyal Ofek', 'Michel Pahud', 'Matthias Kranz', 'Per Ola Kristensson']",2018-02-02T09:39:13Z,http://arxiv.org/abs/1802.00613v1,"['cs.HC', 'H.5.2']","Typing,Virtual Reality,Hand representations,Alphanumeric text entry,Presence,Inverse kinematic model,Fingertip visualization,Video inlay,Error rates,Text entry speed"
"Text Entry in Immersive Head-Mounted Display-based Virtual Reality using
  Standard Keyboards","We study the performance and user experience of two popular mainstream text
entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual
Reality (VR) applications. We discuss the limitations arising from limited
visual feedback, and examine the efficiency of different strategies of use. We
analyze a total of 24 hours of typing data in VR from 24 participants and find
that novice users are able to retain about 60% of their typing speed on a
desktop keyboard and about 40-45\% of their typing speed on a touchscreen
keyboard. We also find no significant learning effects, indicating that users
can transfer their typing skills fast into VR. Besides investigating baseline
performances, we study the position in which keyboards and hands are rendered
in space. We find that this does not adversely affect performance for desktop
keyboard typing and results in a performance trade-off for touchscreen keyboard
typing.","['Jens Grubert', 'Lukas Witzani', 'Eyal Ofek', 'Michel Pahud', 'Matthias Kranz', 'Per Ola Kristensson']",2018-02-02T10:28:44Z,http://arxiv.org/abs/1802.00626v1,"['cs.HC', 'H.5.2']","Text entry,Immersive Head-Mounted Display,Virtual Reality,Standard Keyboards,Touchscreen Keyboards,Typing speed,Learning effects,Positioning,Performance trade-off"
"VR IQA NET: Deep Virtual Reality Image Quality Assessment using
  Adversarial Learning","In this paper, we propose a novel virtual reality image quality assessment
(VR IQA) with adversarial learning for omnidirectional images. To take into
account the characteristics of the omnidirectional image, we devise deep
networks including novel quality score predictor and human perception guider.
The proposed quality score predictor automatically predicts the quality score
of distorted image using the latent spatial and position feature. The proposed
human perception guider criticizes the predicted quality score of the predictor
with the human perceptual score using adversarial learning. For evaluation, we
conducted extensive subjective experiments with omnidirectional image dataset.
Experimental results show that the proposed VR IQA metric outperforms the 2-D
IQA and the state-of-the-arts VR IQA.","['Heoun-taek Lim', 'Hak Gu Kim', 'Yong Man Ro']",2018-04-11T11:45:56Z,http://arxiv.org/abs/1804.03943v1,['cs.CV'],"virtual reality,image quality assessment,adversarial learning,omnidirectional images,deep networks,quality score predictor,human perception,spatial feature,position feature"
Visualization and Labeling of Point Clouds in Virtual Reality,"We present a Virtual Reality (VR) application for labeling and handling point
cloud data sets. A series of room-scale point clouds are recorded as a video
sequence using a Microsoft Kinect. The data can be played and paused, and
frames can be skipped just like in a video player. The user can walk around and
inspect the data while it is playing or paused. Using the tracked hand-held
controller, the user can select and label individual parts of the point cloud.
The points are highlighted with a color when they are labeled. With a tracking
algorithm, the labeled points can be tracked from frame to frame to ease the
labeling process. Our sample data is an RGB point cloud recording of two people
juggling with pins. Here, the user can select and label, for example, the
juggler pins as shown in Figure 1. Each juggler pin is labeled with various
colors to indicate di erent labels.","['Jonathan Dyssel Stets', 'Yongbin Sun', 'Wiley Corning', 'Scott Greenwald']",2018-04-11T17:35:26Z,http://arxiv.org/abs/1804.04111v1,"['cs.HC', 'cs.GR']","Virtual Reality,Point Clouds,Labeling,Microsoft Kinect,Hand-held Controller,Tracking Algorithm,RGB,Juggling,Color Labels"
Immersive Virtual Reality Experiences for All-Sky Data,"Spherical coordinate systems, which are ubiquitous in astronomy, cannot be
shown without distortion on flat, two-dimensional surfaces. This poses
challenges for the two complementary phases of visual exploration -- making
discoveries in data by looking for relationships, patterns or anomalies -- and
publication -- where the results of an exploration are made available for
scientific scrutiny or communication. This is a long-standing problem, and many
practical solutions have been developed. Our allskyVR approach provides a
workflow for experimentation with commodity virtual reality head-mounted
displays. Using the free, open source S2PLOT programming library, and the
A-Frame WebVR browser-based framework, we provide a straightforward way to
visualise all-sky catalogues on a user-centred, virtual celestial sphere. The
allskyVR distribution contains both a quickstart option, complete with a
gaze-based menu system, and a fully customisable mode for those who need more
control of the immersive experience.
  The software is available for download from:
https://github.com/cfluke/allskyVR","['C. J. Fluke', 'D. G. Barnes']",2018-05-09T02:39:38Z,http://arxiv.org/abs/1805.03354v1,"['astro-ph.IM', 'astro-ph.CO', 'astro-ph.SR']","immersive virtual reality,all-sky data,spherical coordinate systems,astronomy,data visualization,virtual reality head-mounted displays,S2PLOT,A-Frame,WebVR,celestial sphere"
Towards Multifocal Displays with Dense Focal Stacks,"We present a virtual reality display that is capable of generating a dense
collection of depth/focal planes. This is achieved by driving a focus-tunable
lens to sweep a range of focal lengths at a high frequency and, subsequently,
tracking the focal length precisely at microsecond time resolutions using an
optical module. Precise tracking of the focal length, coupled with a high-speed
display, enables our lab prototype to generate 1600 focal planes per second.
This enables a novel first-of-its-kind virtual reality multifocal display that
is capable of resolving the vergence-accommodation conflict endemic to today's
displays.","['Jen-Hao Rick Chang', 'B. V. K. Vijaya Kumar', 'Aswin C. Sankaranarayanan']",2018-05-27T17:51:07Z,http://arxiv.org/abs/1805.10664v3,"['cs.CV', 'cs.HC']","multifocal displays,dense focal stacks,depth/focal planes,focus-tunable lens,focal lengths,tracking,microsecond time resolutions,high-speed display,virtual reality,vergence-accommodation conflict"
"Scene Synchronization for Real-Time Interaction in Distributed Mixed
  Reality and Virtual Reality Environments","Advances in computer networks and rendering systems facilitate the creation
of distributed collaborative environments in which the distribution of
information at remote locations allows efficient communication. One of the
challenges in networked virtual environments is maintaining a consistent view
of the shared state in the presence of inevitable network latency and jitter. A
consistent view in a shared scene may significantly increase the sense of
presence among participants and facilitate their interactivity. The dynamic
shared state is directly affected by the frequency of actions applied on the
objects in the scene. Mixed Reality (MR) and Virtual Reality (VR) environments
contain several types of action producers including human users, a wide range
of electronic motion sensors, and haptic devices. In this paper, the authors
propose a novel criterion for categorization of distributed MR/VR systems and
present an adaptive synchronization algorithm for distributed MR/VR
collaborative environments. In spite of significant network latency, results
show that for low levels of update frequencies the dynamic shared state can be
maintained consistent at multiple remotely located sites.","['Felix G. Hamza-Lup', 'Jannick P. Rolland']",2018-12-08T14:01:49Z,http://arxiv.org/abs/1812.03322v1,"['cs.NI', 'cs.MM']","distributed mixed reality,virtual reality,scene synchronization,interaction,real-time,network latency,jitter,collaborative environments,synchronization algorithm"
Virtual replicas of real places: Experimental investigations,"The emergence of social virtual reality (VR) experiences, such as Facebook
Spaces, Oculus Rooms, and Oculus Venues, will generate increased interest from
users who want to share real places (both personal and public) with their
fellow users in VR. At the same time, advances in scanning and reconstruction
technology are making the realistic capture of real places more and more
feasible. These complementary pressures mean that the representation of real
places in virtual reality will be an increasingly common use case for VR.
Despite this, there has been very little research into how users perceive such
replicated spaces. This paper reports the results from a series of three user
studies investigating this topic. Taken together, these results show that
getting the scale of the space correct is the most important factor for
generating a ""feeling of reality"", that it is important to avoid incoherent
behaviors (such as floating objects), and that lighting makes little difference
to perceptual similarity.","['Richard Skarbez', 'Doug A. Bowman', 'J. Todd Ogle', 'Thomas Tucker', 'Joseph L. Gabbard']",2018-12-09T08:04:00Z,http://arxiv.org/abs/1812.03441v1,"['cs.HC', 'I.3.7']","virtual reality,virtual replicas,real places,scanning technology,reconstruction technology,user perception,scale,incoherent behaviors,lighting"
"Walking Through an Exploded Star: Rendering Supernova Remnant Cassiopeia
  A into Virtual Reality","NASA and other astrophysical data of the Cassiopeia A supernova remnant have
been rendered into a three-dimensional virtual reality (VR) and augmented
reality (AR) program, the first of its kind. This data-driven experience of a
supernova remnant allows viewers to walk inside the leftovers from the
explosion of a massive star, select the parts of the supernova remnant to
engage with, and access descriptive texts on what the materials are. The basis
of this program is a unique 3D model of the 340-year old remains of a stellar
explosion, made by combining data from the NASA Chandra X-ray Observatory,
Spitzer Space Telescope, and ground-based facilities. A collaboration between
the Smithsonian Astrophysical Observatory and Brown University allowed the 3D
astronomical data collected on Cassiopeia A to be featured in the VR/AR
program, which is an innovation in digital technologies with public, education,
and research-based impacts.","['Kimberly K. Arcand', 'Elaine Jiang', 'Sara Price', 'Megan Watzke', 'Tom Sgouros', 'Peter Edmonds']",2018-12-15T05:29:04Z,http://arxiv.org/abs/1812.06237v1,"['astro-ph.IM', 'astro-ph.HE', 'cs.HC']","supernova remnant,virtual reality,augmented reality,3D model,NASA,Chandra X-ray Observatory,Spitzer Space Telescope,astronomical data,Cassiopeia A"
Optimal Multi-Quality Multicast for 360 Virtual Reality Video,"A 360 virtual reality (VR) video, recording a scene of interest in every
direction, provides VR users with immersive viewing experience. However,
transmission of a 360 VR video which is of a much larger size than a
traditional video to mobile users brings a heavy burden to a wireless network.
In this paper, we consider multi-quality multicast of a 360 VR video from a
single server to multiple users using time division multiple access (TDMA). To
improve transmission efficiency, tiling is adopted, and each tile is
pre-encoded into multiple representations with different qualities. We optimize
the quality level selection, transmission time allocation and transmission
power allocation to maximize the total utility of all users under the
transmission time and power allocation constraints as well as the quality
smoothness constraints for mixed-quality tiles. The problem is a challenging
mixed discrete-continuous opti-mization problem. We propose two low-complexity
algorithms to obtain two suboptimal solutions, using continuous relaxation and
DC programming, respectively. Finally, numerical results demonstrate the
advantage of the proposed solutions.","['Kaixuan Long', 'Chencheng Ye', 'Ying Cui', 'Zhi Liu']",2019-01-08T08:24:52Z,http://arxiv.org/abs/1901.02203v1,"['cs.IT', 'math.IT']","360 virtual reality,multicast,time division multiple access (TDMA),tiling,optimization,transmission efficiency,quality level selection,transmission power allocation,mixed-quality tiles"
"Utilization of Virtual Reality Visualizations on Heavy Mobile Crane
  Planning for Modular Construction","Many kinds of industrial projects involve the use of prefabricated modules
built offsite, and installation on-site using mobile cranes. Due to their
costly operation and safety concerns, utilization of such heavy lift mobile
cranes requires a precise heavy lift planning. Traditional heavy lift path
planning methods on congested industrial job sites are ineffective,
time-consuming and non-precise in many cases, whereas computer-based simulation
models and visualization can be a substantial improving tool. This paper
provides a Virtual Reality (VR) environment in which the user can experience
lifting process in an immerse virtual environment. Providing such a VR model
not only facilitates planning for critical lifts (e.g. modules, heavy vessels),
but also it provides a training environment to enhance safe climate prior to
the actual lift. The developed VR model is implemented successfully on an
actual construction site of a petrochemical plant on a modular basis in which
heavy lift mobile cranes are employed.","['Navid Kayhani', 'Hosein Taghaddos', 'Mojtaba Noghabaee', 'Ulrich', 'Hermann']",2019-01-12T21:22:57Z,http://arxiv.org/abs/1901.06248v1,['cs.CY'],"Virtual Reality,Visualizations,Heavy Mobile Crane,Planning,Modular Construction,Industrial Projects,Prefabricated Modules,Simulation Models,Training Environment,Petrochemical Plant"
Omnipotent Virtual Giant for Remote Human-Swarm Interaction,"This paper proposes an intuitive human-swarm interaction framework inspired
by our childhood memory in which we interacted with living ants by changing
their positions and environments as if we were omnipotent relative to the ants.
In virtual reality, analogously, we can be a super-powered virtual giant who
can supervise a swarm of mobile robots in a vast and remote environment by
flying over or resizing the world and coordinate them by picking and placing a
robot or creating virtual walls. This work implements this idea by using
Virtual Reality along with Leap Motion, which is then validated by
proof-of-concept experiments using real and virtual mobile robots in mixed
reality. We conduct a usability analysis to quantify the effectiveness of the
overall system as well as the individual interfaces proposed in this work. The
results revealed that the proposed method is intuitive and feasible for
interaction with swarm robots, but may require appropriate training for the new
end-user interface device.","['Inmo Jang', 'Junyan Hu', 'Farshad Arvin', 'Joaquin Carrasco', 'Barry Lennox']",2019-03-24T21:39:49Z,http://arxiv.org/abs/1903.10064v2,['cs.RO'],"virtual reality,human-swarm interaction,mobile robots,remote environment,virtual giant"
"Would Gaze-Contingent Rendering Improve Depth Perception in Virtual and
  Augmented Reality?","Near distances are overestimated in virtual reality, and far distances are
underestimated, but an explanation for these distortions remains elusive. One
potential concern is that whilst the eye rotates to look at the virtual scene,
the virtual cameras remain static. Could using eye-tracking to change the
perspective of the virtual cameras as the eye rotates improve depth perception
in virtual reality? This paper identifies 14 distinct perspective distortions
that could in theory occur from keeping the virtual cameras fixed whilst the
eye rotates in the context of near-eye displays. However, the impact of eye
movements on the displayed image depends on the optical, rather than physical,
distance of the display. Since the optical distance of most head-mounted
displays is over 1m, most of these distortions will have only a negligible
effect. The exception are 'gaze-contingent disparities', which will leave near
virtual objects looking displaced from physical objects that are meant to be at
the same distance in augmented reality.",['Paul Linton'],2019-05-24T11:47:08Z,http://arxiv.org/abs/1905.10366v1,['cs.HC'],"gaze-contingent rendering,depth perception,virtual reality,augmented reality,eye-tracking,perspective distortions,near-eye displays,optical distance,head-mounted displays,gaze-contingent disparities"
"Outstanding: A Multi-Perspective Travel Approach for Virtual Reality
  Games","In virtual reality games, players dive into fictional environments and can
experience a compelling and immersive world. State-of-the-art VR systems allow
for natural and intuitive navigation through physical walking. However, the
tracking space is still limited, and viable alternatives are required to reach
further virtual destinations. Our work focuses on the exploration of vast open
worlds - an area where existing local navigation approaches such as the
arc-based teleport are not ideally suited and world-in-miniature techniques
potentially reduce presence. We present a novel alternative for open
environments: Our idea is to equip players with the ability to switch from
first-person to a third-person bird's eye perspective on demand. From above,
players can command their avatar and initiate travels over large distance. Our
evaluation reveals a significant increase in spatial orientation while avoiding
cybersickness and preserving presence, enjoyment, and competence. We summarize
our findings in a set of comprehensive design guidelines to help developers
integrate our technique.","['Sebastian Cmentowski', 'Andrey Krekhov', 'Jens Krüger']",2019-08-01T13:22:37Z,http://arxiv.org/abs/1908.00379v2,['cs.HC'],"Virtual reality,Games,Navigation,Presence,Spatial orientation,Third-person perspective,Design guidelines,Cybersickness,Immersive world,Avatar"
Toward a Taxonomy of Inventory Systems for Virtual Reality Games,"Virtual reality (VR) games are gradually becoming more elaborated and
feature-rich, but fail to reach the complexity of traditional digital games.
One common feature that is used to extend and organize complex gameplay is the
in-game inventory, which allows players to obtain and carry new tools and items
throughout their journey. However, VR imposes additional requirements and
challenges that impede the implementation of this important feature and hinder
games to unleash their full potential. Our current work focuses on the design
space of inventories in VR games. We introduce this sparsely researched topic
by constructing a first taxonomy of the underlying design considerations and
building blocks. Furthermore, we present three different inventories that were
designed using our taxonomy and evaluate them in an early qualitative study.
The results underline the importance of our research and reveal promising
insights that show the huge potential for VR games.","['Sebastian Cmentowski', 'Andrey Krekhov', 'Ann-Marie Müller', 'Jens Krüger']",2019-08-09T18:35:00Z,http://arxiv.org/abs/1908.03591v2,['cs.HC'],"taxonomy,inventory systems,virtual reality games,in-game inventory,design considerations,building blocks,qualitative study,VR games,digital games"
"A Research Framework for Virtual Reality Neurosurgery Based on
  Open-Source Tools","Fully immersive virtual reality (VR) has the potential to improve
neurosurgical planning. For example, it may offer 3D visualizations of relevant
anatomical structures with complex shapes, such as blood vessels and tumors.
However, there is a lack of research tools specifically tailored for this area.
We present a research framework for VR neurosurgery based on open-source tools
and preliminary evaluation results. We showcase the potential of such a
framework using clinical data of two patients and research data of one subject.
As a first step toward practical evaluations, two certified senior
neurosurgeons positively assessed the usefulness of the VR visualizations using
head-mounted displays. The methods and findings described in our study thus
provide a foundation for research and development aiming at versatile and
user-friendly VR tools for improving neurosurgical planning and training.","['Lukas D. J. Fiederer', 'Hisham Alwanni', 'Martin Völker', 'Oliver Schnell', 'Jürgen Beck', 'Tonio Ball']",2019-08-14T16:04:40Z,http://arxiv.org/abs/1908.05188v1,"['cs.HC', 'eess.IV']","virtual reality,neurosurgery,open-source tools,3D visualizations,anatomical structures,blood vessels,tumors,research framework,clinical data,head-mounted displays"
"A Proposed Framework for Interactive Virtual Reality In Situ
  Visualization of Parallel Numerical Simulations","As computer simulations progress to increasingly complex, non-linear, and
three-dimensional systems and phenomena, intuitive and immediate visualization
of their results is becoming crucial. While Virtual Reality (VR) and Natural
User Interfaces (NUIs) have been shown to improve understanding of complex 3D
data, their application to live in situ visualization and computational
steering is hampered by performance requirements. Here, we present the design
of a software framework for interactive VR in situ visualization of parallel
numerical simulations, as well as a working prototype implementation. Our
design is targeted towards meeting the performance requirements for VR, and our
work is packaged in a framework that allows for easy instrumentation of
simulations. Our preliminary results inform about the technical feasibility of
the architecture, as well as the challenges that remain.","['Aryaman Gupta', 'Ulrik Günther', 'Pietro Incardona', 'Ata Deniz Aydin', 'Raimund Dachselt', 'Stefan Gumhold', 'Ivo F. Sbalzarini']",2019-09-06T16:03:12Z,http://arxiv.org/abs/1909.02986v1,"['cs.DC', 'cs.GR']","Virtual Reality,In Situ Visualization,Parallel Numerical Simulations,Natural User Interfaces,Computational Steering,Performance Requirements,Software Framework,Instrumentation,Architecture."
Virtual Reality for Robots,"This paper applies the principles of Virtual Reality (VR) to robots, rather
than living organisms. A simulator, of either physical states or information
states, renders outputs to custom displays that fool the robot's sensors. This
enables a robot to experience a combination of real and virtual sensor inputs,
combining the efficiency of simulation and the benefits of real world sensor
inputs. Thus, the robot can be taken through targeted experiences that are more
realistic than pure simulation, yet more feasible and controllable than pure
real-world experiences. We define two distinctive methods for applying VR to
robots, namely black box and white box; based on these methods we identify
potential applications, such as testing and verification procedures that are
better than simulation, the study of spoofing attacks and anti-spoofing
techniques, and sample generation for machine learning. A general mathematical
framework is presented, along with a simple experiment, detailed examples, and
discussion of the implications.","['Markku Suomalainen', 'Alexandra Q. Nilles', 'Steven M. LaValle']",2019-09-16T09:58:42Z,http://arxiv.org/abs/1909.07096v3,['cs.RO'],"Virtual Reality,Robots,Simulator,Sensors,Simulation,Real-world,Black box,White box,Spoofing attacks,Machine learning"
"Predictive Simulation: Using Regression and Artificial Neural Networks
  to Negate Latency in Networked Interactive Virtual Reality","Current virtual reality systems are typically limited by performance/cost,
usability (size), or a combination of both. By using a networked client/server
environment, we have solved these limitations for the client. However, in doing
so we have introduced a new problem, namely increased latency. Interactive
networked virtual environments such as games and simulations have existed for
nearly as long as the Internet and have consistently faced latency issues. We
propose a solution for negating the effects of latency for interactive
networked virtual environments with lightweight clients, with respect to the
server being used. The proposed method extrapolates future client states to be
incorporated in the server's updates, which helps to synchronize actions on the
client-side and the results coming from the server. We refer to this approach
as predictive simulation. In addition to describing our method, in this paper,
we look at extrapolation methods because the success of our predictive
simulation method is dependent on strong predictions. We focus on regression
methods and briefly examine the use of artificial neural networks.","['Gregory Gutmann', 'Akihiko Konagaya']",2019-10-04T06:29:39Z,http://arxiv.org/abs/1910.04703v1,"['cs.HC', 'I.2.1; I.6.7']","Predictive simulation,Regression,Artificial neural networks,Latency,Networked interactive virtual reality,Client-server environment,Lightweight clients,Extrapolation methods,Synchronization,Interactive networked virtual environments"
"An Automatic Digital Terrain Generation Technique for Terrestrial
  Sensing and Virtual Reality Applications","The identification and modeling of the terrain from point cloud data is an
important component of Terrestrial Remote Sensing (TRS) applications. The main
focus in terrain modeling is capturing details of complex geological features
of landforms. Traditional terrain modeling approaches rely on the user to exert
control over terrain features. However, relying on the user input to manually
develop the digital terrain becomes intractable when considering the amount of
data generated by new remote sensing systems capable of producing massive
aerial and ground-based point clouds from scanned environments. This article
provides a novel terrain modeling technique capable of automatically generating
accurate and physically realistic Digital Terrain Models (DTM) from a variety
of point cloud data. The proposed method runs efficiently on large-scale point
cloud data with real-time performance over large segments of terrestrial
landforms. Moreover, generated digital models are designed to effectively
render within a Virtual Reality (VR) environment in real time. The paper
concludes with an in-depth discussion of possible research directions and
outstanding technical and scientific challenges to improve the proposed
approach.","['Lee Easson', 'Alireza Tavakkoli', 'Jonathan Greenberg']",2019-10-11T02:26:01Z,http://arxiv.org/abs/1910.04944v1,"['cs.CV', 'cs.MM']","Terrain modeling,Digital Terrain Generation,Terrestrial Remote Sensing,Point cloud data,Digital Terrain Models (DTM),Virtual Reality,Remote sensing systems."
"Immersive Analytics of Large Dynamic Networks via Overview and Detail
  Navigation","Analysis of large dynamic networks is a thriving research field, typically
relying on 2D graph representations. The advent of affordable head mounted
displays however, sparked new interest in the potential of 3D visualization for
immersive network analytics. Nevertheless, most solutions do not scale well
with the number of nodes and edges and rely on conventional fly- or
walk-through navigation. In this paper, we present a novel approach for the
exploration of large dynamic graphs in virtual reality that interweaves two
navigation metaphors: overview exploration and immersive detail analysis. We
thereby use the potential of state-of-the-art VR headsets, coupled with a
web-based 3D rendering engine that supports heterogeneous input modalities to
enable ad-hoc immersive network analytics. We validate our approach through a
performance evaluation and a case study with experts analyzing a co-morbidity
network.","['Johannes Sorger', 'Manuela Waldner', 'Wolfgang Knecht', 'Alessio Arleo']",2019-10-15T14:42:55Z,http://arxiv.org/abs/1910.06825v2,['cs.HC'],"dynamic networks,immersive analytics,overview navigation,detail navigation,3D visualization,virtual reality,large graphs,network exploration,web-based rendering,heterogeneous input modalities"
"Ready Student One: Exploring the predictors of student learning in
  virtual reality","Immersive virtual reality (VR) has enormous potential for education, but
classroom resources are limited. Thus, it is important to identify whether and
when VR provides sufficient advantages over other modes of learning to justify
its deployment. In a between-subjects experiment, we compared three methods of
teaching Moon phases (a hands-on activity, VR, and a desktop simulation) and
measured student improvement on existing learning and attitudinal measures.
While a substantial majority of students preferred the VR experience, we found
no significant differences in learning between conditions. However, we found
differences between conditions based on gender, which was highly correlated
with experience with video games. These differences may indicate certain groups
have an advantage in the VR setting.","['J. Madden', 'S. Pandita', 'J. P. Schuldt', 'B. Kim', 'A. S. Won', 'N. G. Holmes']",2019-10-24T06:47:46Z,http://arxiv.org/abs/1910.10939v3,['physics.ed-ph'],"virtual reality,predictors,student learning,education,immersive,classroom resources,advantages,deployment,Moon phases,desktop simulation"
Visual-Inertial Telepresence for Aerial Manipulation,"This paper presents a novel telepresence system for enhancing aerial
manipulation capabilities. It involves not only a haptic device, but also a
virtual reality that provides a 3D visual feedback to a remotely-located
teleoperator in real-time. We achieve this by utilizing onboard visual and
inertial sensors, an object tracking algorithm and a pre-generated object
database. As the virtual reality has to closely match the real remote scene, we
propose an extension of a marker tracking algorithm with visual-inertial
odometry. Both indoor and outdoor experiments show benefits of our proposed
system in achieving advanced aerial manipulation tasks, namely grasping,
placing, force exertion and peg-in-hole insertion.","['Jongseok Lee', 'Ribin Balachandran', 'Yuri S. Sarkisov', 'Marco De Stefano', 'Andre Coelho', 'Kashmira Shinde', 'Min Jun Kim', 'Rudolph Triebel', 'Konstantin Kondak']",2020-03-25T17:26:03Z,http://arxiv.org/abs/2003.11509v2,"['cs.RO', 'cs.CV']","visual-inertial,telepresence,aerial manipulation,haptic device,virtual reality,3D visual feedback,object tracking algorithm,visual-inertial sensors,marker tracking algorithm"
"Levitation Simulator: Prototyping Ultrasonic Levitation Interfaces in
  Virtual Reality","We present the Levitation Simulator, a system that enables researchers and
designers to iteratively develop and prototype levitation interface ideas in
Virtual Reality. This includes user tests and formal experiments. We derive a
model of the movement of a levitating particle in such an interface. Based on
this, we develop an interactive simulation of the levitation interface in VR,
which exhibits the dynamical properties of the real interface. The results of a
Fitts' Law pointing study show that the Levitation Simulator enables
performance, comparable to the real prototype. We developed the first two
interactive games, dedicated for levitation interfaces: LeviShooter and
BeadBounce, in the Levitation Simulator, and then implemented them on the real
interface. Our results indicate that participants experienced similar levels of
user engagement when playing the games, in the two environments. We share our
Levitation Simulator as Open Source, thereby democratizing levitation research,
without the need for a levitation apparatus.","['Viktorija Paneva', 'Myroslav Bachynskyi', 'Jörg Müller']",2020-05-13T12:51:19Z,http://arxiv.org/abs/2005.06291v1,"['cs.HC', 'cs.SY', 'eess.SY', 'math.OC']","levitation,simulator,ultrasonic,virtual reality,interface,prototyping,interactive simulation,Fitts' Law,user engagement,Open Source"
"Learning-based Prediction, Rendering and Association Optimization for
  MEC-enabled Wireless Virtual Reality (VR) Network","Wireless-connected Virtual Reality (VR) provides immersive experience for VR
users from any-where at anytime. However, providing wireless VR users with
seamless connectivity and real-time VR video with high quality is challenging
due to its requirements in high Quality of Experience (QoE) and low VR
interaction latency under limited computation capability of VR device. To
address these issues,we propose a MEC-enabled wireless VR network, where the
field of view (FoV) of each VR user can be real-time predicted using Recurrent
Neural Network (RNN), and the rendering of VR content is moved from VR device
to MEC server with rendering model migration capability. Taking into account
the geographical and FoV request correlation, we propose centralized and
distributed decoupled Deep Reinforcement Learning (DRL) strategies to maximize
the long-term QoE of VR users under the VR interaction latency constraint.
Simulation results show that our proposed MEC rendering schemes and DRL
algorithms substantially improve the long-term QoE of VR users and reduce the
VR interaction latency compared to rendering at VR devices","['Xiaonan Liu', 'Yansha Deng']",2020-05-17T18:17:46Z,http://arxiv.org/abs/2005.08332v1,['eess.SP'],"Prediction,Rendering,Association Optimization,MEC,Wireless Virtual Reality,Quality of Experience (QoE),Latency,Recurrent Neural Network (RNN),Deep Reinforcement Learning (DRL),Simulation"
User Attention and Behaviour in Virtual Reality Art Encounter,"With the proliferation of consumer virtual reality (VR) headsets and creative
tools, content creators have started to experiment with new forms of
interactive audience experience using immersive media. Understanding user
attention and behaviours in virtual environment can greatly inform creative
processes in VR. We developed an abstract VR painting and an experimentation
system to study audience encounters through eye gaze and movement tracking. The
data from a user experiment with 35 participants reveal a range of user
activity patterns in art exploration. Deep learning models are used to study
the connections between behavioural data and audience background. New
integrated methods to visualise user attention as part of the artwork are also
developed as a feedback loop to the content creator.","['Mu Mu', 'Murtada Dohan', 'Alison Goodyear', 'Gary Hill', 'Cleyon Johns', 'Andreas Mauthe']",2020-05-20T16:09:57Z,http://arxiv.org/abs/2005.10161v1,"['cs.HC', 'cs.MA', 'cs.MM']","user attention,behaviour,virtual reality,art encounter,immersive media,eye gaze tracking,movement tracking,user experiment,deep learning models,visualisation techniques"
V-Dream: Immersive Exploration of Generative Design Solution Space,"Generative Design workflows have introduced alternative paradigms in the
domain of computational design, allowing designers to generate large pools of
valid solutions by defining a set of goals and constraints. However, analyzing
and narrowing down the generated solution space, which usually consists of
various high-dimensional properties, has been a major challenge in current
generative workflows. By taking advantage of the interactive unbounded spatial
exploration, and the visual immersion offered in virtual reality platforms, we
propose V-Dream, a virtual reality generative analysis framework for exploring
large-scale solution spaces. V-Dream proposes a hybrid search workflow in which
a spatial stochastic search approach is combined with a recommender system
allowing users to pick desired candidates and eliminate the undesired ones
iteratively. In each cycle, V-Dream reorganizes the remaining options in
clusters based on the defined features. Moreover, our framework allows users to
inspect design solutions and evaluate their performance metrics in various
hierarchical levels, assisting them in narrowing down the solution space
through iterative cycles of search/select/re-clustering of the solutions in an
immersive fashion. Finally, we present a prototype of our proposed framework,
illustrating how users can navigate and narrow down desired solutions from a
pool of over 16000 monitor stands generated by Autodesk's Dreamcatcher
software.","['Mohammad Keshavarzi', 'Ardavan Bidgoli', 'Hans Kellner']",2020-06-19T09:51:27Z,http://arxiv.org/abs/2006.11044v1,['cs.HC'],"Generative Design,Computational Design,Solution Space,Virtual Reality,Immersive Exploration,Stochastic Search,Recommender System,Performance Metrics,Prototype,Autodesk."
Ray-VR: Ray Tracing Virtual Reality in Falcor,"NVidia RTX platform has been changing and extending the possibilities for
real time Computer Graphics applications. It is the first time in history that
retail graphics cards have full hardware support for ray tracing primitives. It
still a long way to fully understand and optimize its use and this task itself
is a fertile field for scientific progression. However, another path is to
explore the platform as an expansion of paradigms for other problems. For
example, the integration of real time Ray Tracing and Virtual Reality can
result in interesting applications for visualization of Non-Euclidean Geometry
and 3D Manifolds. In this paper we present Ray-VR, a novel algorithm for real
time stereo ray tracing, constructed on top of Falcor, NVidia's scientific
prototyping framework.","['Vinicius da Silva', 'Luiz Velho']",2020-06-19T19:54:50Z,http://arxiv.org/abs/2006.11348v1,"['cs.GR', 'I.3.7']","Ray tracing,Virtual Reality,Falcor,NVidia RTX platform,Computer Graphics,Real time,3D Manifolds,Stereo ray tracing,Non-Euclidean Geometry,Scientific prototyping framework"
"Training atomic neural networks using fragment-based data generated in
  virtual reality","The ability to understand and engineer molecular structures relies on having
accurate descriptions of the energy as a function of atomic coordinates. Here
we outline a new paradigm for deriving energy functions of hyperdimensional
molecular systems, which involves generating data for low-dimensional systems
in virtual reality (VR) to then efficiently train atomic neural networks
(ANNs). This generates high quality data for specific areas of interest within
the hyperdimensional space that characterizes a molecule's potential energy
surface (PES). We demonstrate the utility of this approach by gathering data
within VR to train ANNs on chemical reactions involving fewer than 8 heavy
atoms. This strategy enables us to predict the energies of much
higher-dimensional systems, e.g. containing nearly 100 atoms. Training on
datasets containing only 15K geometries, this approach generates mean absolute
errors around 2 kcal/mol. This represents one of the first times that an
ANN-PES for a large reactive radical has been generated using such a small
dataset. Our results suggest VR enables the intelligent curation of
high-quality data, which accelerates the learning process.","['Silvia Amabilino', 'Lars A. Bratholm', 'Simon J. Bennie', ""Michael B. O'Connor"", 'David R. Glowacki']",2020-05-30T13:56:19Z,http://arxiv.org/abs/2007.02824v1,['physics.chem-ph'],"atomic neural networks,fragment-based data,virtual reality,energy functions,hyperdimensional molecular systems,potential energy surface (PES),chemical reactions,heavy atoms,geometries,mean absolute errors"
"Towards Secure and Usable Authentication for Augmented and Virtual
  Reality Head-Mounted Displays","Immersive technologies, including augmented and virtual reality (AR & VR)
devices, have enhanced digital communication along with a considerable increase
in digital threats. Thus, authentication becomes critical in AR & VR
technology, particularly in shared spaces. In this paper, we propose applying
the ZeTA protocol that allows secure authentication even in shared spaces for
the AR & VR context. We explain how it can be used with the available
interaction methods provided by Head-Mounted Displays. In future work, our
research goal is to evaluate different designs of ZeTA (e.g., interaction
modes) concerning their usability and users' risk perception regarding their
security - while using a cross-cultural approach.","['Reyhan Duezguen', 'Peter Mayer', 'Sanchari Das', 'Melanie Volkamer']",2020-07-22T20:34:14Z,http://arxiv.org/abs/2007.11663v2,"['cs.CR', 'cs.CY']","Secure authentication,Usable authentication,Augmented reality,Virtual reality,Head-Mounted Displays,ZeTA protocol,Interaction methods,Usability,Security perception"
Content Format and Quality of Experience in Virtual Reality,"In this paper, we investigate three forms of virtual reality content
production and consumption. Namely, 360 stereoscopic video, the combination of
a 3D environment with a video billboard for dynamic elements, and a full 3D
rendered scene. On one hand, video based techniques facilitate the acquisition
of content, but they can limit the experience of the user since the content is
captured from a fixed point of view. On the other hand, 3D content allows for
point of view translation, but real-time photorealistic rendering is not
trivial and comes at high production and processing costs. We also compare the
two extremes with an approach that combines dynamic video elements with a 3D
virtual environment. We discuss the advantages and disadvantages of these
systems, and present the result of a user study with 24 participants. In the
study, we evaluated the quality of experience, including presence, simulation
sickness and participants' assessment of content quality, of three versions of
a cinematic segment with two actors. We found that, in this context, mixing
video and 3D content produced the best experience.","['Henrique Galvan Debarba', 'Mario Montagud', 'Sylvain Chagué', 'Javier Lajara', 'Ignacio Lacosta', 'Sergi Fernandez Langa', 'Caecilia Charbonnier']",2020-08-11T04:56:53Z,http://arxiv.org/abs/2008.04511v1,['cs.MM'],"Content Format,Quality of Experience,Virtual Reality,Stereoscopic Video,3D Environment,Video Billboard,3D Rendered Scene,Photorealistic Rendering,User Study,Simulation Sickness"
"Breaking the Screen: Interaction Across Touchscreen Boundaries in
  Virtual Reality for Mobile Knowledge Workers","Virtual Reality (VR) has the potential to transform knowledge work. One
advantage of VR knowledge work is that it allows extending 2D displays into the
third dimension, enabling new operations, such as selecting overlapping objects
or displaying additional layers of information. On the other hand, mobile
knowledge workers often work on established mobile devices, such as tablets,
limiting interaction with those devices to a small input space. This challenge
of a constrained input space is intensified in situations when VR knowledge
work is situated in cramped environments, such as airplanes and touchdown
spaces.
  In this paper, we investigate the feasibility of interacting jointly between
an immersive VR head-mounted display and a tablet within the context of
knowledge work. Specifically, we 1) design, implement and study how to interact
with information that reaches beyond a single physical touchscreen in VR; 2)
design and evaluate a set of interaction concepts; and 3) build example
applications and gather user feedback on those applications.","['Verena Biener', 'Daniel Schneider', 'Travis Gesslein', 'Alexander Otte', 'Bastian Kuth', 'Per Ola Kristensson', 'Eyal Ofek', 'Michel Pahud', 'Jens Grubert']",2020-08-11T07:21:20Z,http://arxiv.org/abs/2008.04559v1,"['cs.HC', 'I.3.7']","Virtual Reality,Knowledge Work,Interaction,Touchscreen Boundaries,Mobile Devices,Immersive VR,Head-mounted Display,Interaction Concepts,Example Applications,User Feedback."
Audio- and Gaze-driven Facial Animation of Codec Avatars,"Codec Avatars are a recent class of learned, photorealistic face models that
accurately represent the geometry and texture of a person in 3D (i.e., for
virtual reality), and are almost indistinguishable from video. In this paper we
describe the first approach to animate these parametric models in real-time
which could be deployed on commodity virtual reality hardware using audio
and/or eye tracking. Our goal is to display expressive conversations between
individuals that exhibit important social signals such as laughter and
excitement solely from latent cues in our lossy input signals. To this end we
collected over 5 hours of high frame rate 3D face scans across three
participants including traditional neutral speech as well as expressive and
conversational speech. We investigate a multimodal fusion approach that
dynamically identifies which sensor encoding should animate which parts of the
face at any time. See the supplemental video which demonstrates our ability to
generate full face motion far beyond the typically neutral lip articulations
seen in competing work:
https://research.fb.com/videos/audio-and-gaze-driven-facial-animation-of-codec-avatars/","['Alexander Richard', 'Colin Lea', 'Shugao Ma', 'Juergen Gall', 'Fernando de la Torre', 'Yaser Sheikh']",2020-08-11T22:28:48Z,http://arxiv.org/abs/2008.05023v1,['cs.CV'],"Facial Animation,Codec Avatars,Audio,Gaze,Parametric Models,Virtual Reality,Eye Tracking,Multimodal Fusion,3D Face Scans,Social Signals"
"Towards Ultra-Low-Latency mmWave Wi-Fi for Multi-User Interactive
  Virtual Reality","The need for cables with high-fidelity Virtual Reality (VR) headsets remains
a stumbling block on the path towards interactive multi-user VR. Due to strict
latency constraints, designing fully wireless headsets is challenging, with the
few commercially available solutions being expensive. These solutions use
proprietary millimeter wave (mmWave) communications technologies, as extremely
high frequencies are needed to meet the throughput and latency requirements of
VR applications. In this work, we investigate whether such a system could be
built using specification-compliant IEEE 802.11ad hardware, which would
significantly reduce the cost of wireless mmWave VR solutions. We present a
theoretical framework to calculate attainable live VR video bitrates for
different IEEE 802.11ad channel access methods, using 1 or more head-mounted
displays connected to a single Access Point (AP). Using the ns-3 simulator, we
validate our theoretical framework, and demonstrate that a properly configured
IEEE 802.11ad AP can support at least 8 headsets receiving a 4K video stream
for each eye, with transmission latency under 1 millisecond.","['Jakob Struye', 'Filip Lemic', 'Jeroen Famaey']",2020-08-27T12:34:01Z,http://arxiv.org/abs/2008.12086v2,['cs.NI'],"mmWave,Wi-Fi,Virtual Reality,latency,IEEE 802.11ad,channel access methods,head-mounted displays,Access Point,ns-3 simulator"
"Virtual Smartphone: High Fidelity Interaction with Proxy Objects in
  Virtual Reality","This workshop paper presents two proxy objects for high fidelity interaction
in virtual reality (VR): a paper map and a smartphone. We showcase how our
virtual paper map can increase interactivity and orientation, while our virtual
smartphone extends the use of a proxy object, as it allows for actual touch
input on a real phone leading to an almost infinite set of possible
(inter-)actions (e.g. snapping pictures in the virtual world). Observations
showed that participants were very precise in holding and interacting with both
the paper map and the smartphone even though they did not see their hands in
VR. The interaction in general was very intuitive which was mostly attributed
to the realistic size of the virtual objects. Using our findings we discuss the
trade off between adaptivity and high fidelity of proxy objects in VR.",['Gian-Luca Savino'],2020-10-02T12:07:29Z,http://arxiv.org/abs/2010.00942v1,['cs.HC'],"Virtual Reality,Proxy Objects,High Fidelity Interaction,Smartphone,Interactivity,Orientation,Touch Input,Virtual Objects,Adaptivity,Trade-off"
Exploration of Hands-free Text Entry Techniques For Virtual Reality,"Text entry is a common activity in virtual reality (VR) systems. There is a
limited number of available hands-free techniques, which allow users to carry
out text entry when users' hands are busy such as holding items or hand-based
devices are not available. The most used hands-free text entry technique is
DwellType, where a user selects a letter by dwelling over it for a specific
period. However, its performance is limited due to the fixed dwell time for
each character selection. In this paper, we explore two other hands-free text
entry mechanisms in VR: BlinkType and NeckType, which leverage users' eye
blinks and neck's forward and backward movements to select letters. With a user
study, we compare the performance of the two techniques with DwellType. Results
show that users can achieve an average text entry rate of 13.47, 11.18 and
11.65 words per minute with BlinkType, NeckType, and DwellType, respectively.
Users' subjective feedback shows BlinkType as the preferred technique for text
entry in VR.","['Xueshi Lu', 'Difeng Yu', 'Hai-Ning Liang', 'Wenge Xu', 'Yuzheng Chen', 'Xiang Li', 'Khalad Hasan']",2020-10-07T07:59:31Z,http://arxiv.org/abs/2010.03247v1,"['cs.HC', 'H.5.2']","text entry,virtual reality,hands-free,DwellType,BlinkType,NeckType,user study,performance,user feedback"
"Evaluating the Effect of Audience in a Virtual Reality Presentation
  Training Tool","Public speaking is an essential skill in everyone's professional or academic
career. Nevertheless, honing this skill is often tricky because training in
front of a mirror does not give feedback or inspire the same anxiety as
present-ing in front of an audience. Further, most people do not always have
access to the place where the presentation will happen. In this research, we
developed a Virtual Reality (VR) environment to assist in improving people's
presentation skills. Our system uses 3D scanned people to create more realistic
scenarios. We conducted a study with twelve participants who had no prior
experience with VR. We validated our virtual environment by analyzing whether
it was preferred to no VR system and accepted regardless of the existence of a
virtual audience. Our results show that users overwhelmingly prefer to use the
VR system as a tool to help them improve their public speaking skills than
training in an empty environment. However, the preference for an audience is
mixed.","['Diego Monteiro', 'Hai-Ning Liang', 'Hongji Li', 'Yu Fu', 'Xian Wang']",2020-10-12T23:28:11Z,http://arxiv.org/abs/2010.06077v1,['cs.HC'],"Virtual Reality,Presentation Skills,Audience,Training Tool,3D Scanning,Study,Feedback,Anxiety,Public Speaking,User Preference"
"Real-Time Detection of Simulator Sickness in Virtual Reality Games Based
  on Players' Psychophysiological Data during Gameplay","Virtual Reality (VR) technology has been proliferating in the last decade,
especially in the last few years. However, Simulator Sickness (SS) still
represents a significant problem for its wider adoption. Currently, the most
common way to detect SS is using the Simulator Sickness Questionnaire (SSQ).
SSQ is a subjective measurement and is inadequate for real-time applications
such as VR games. This research aims to investigate how to use machine learning
techniques to detect SS based on in-game characters' and users' physiological
data during gameplay in VR games. To achieve this, we designed an experiment to
collect such data with three types of games. We trained a Long Short-Term
Memory neural network with the dataset eye-tracking and character movement data
to detect SS in real-time. Our results indicate that, in VR games, our model is
an accurate and efficient way to detect SS in real-time.","['Jialin Wang', 'Hai-Ning Liang', 'Diego Monteiro', 'Wenge Xu', 'Hao Chen', 'Qiwen Chen']",2020-10-13T03:53:07Z,http://arxiv.org/abs/2010.06152v1,['cs.HC'],"virtual reality,simulator sickness,psychophysiological data,machine learning techniques,neural network,eye-tracking,character movement data,real-time detection"
"Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic
  Video with Applications for Virtual Reality","We introduce a convolutional neural network model for unsupervised learning
of depth and ego-motion from cylindrical panoramic video. Panoramic depth
estimation is an important technology for applications such as virtual reality,
3D modeling, and autonomous robotic navigation. In contrast to previous
approaches for applying convolutional neural networks to panoramic imagery, we
use the cylindrical panoramic projection which allows for the use of the
traditional CNN layers such as convolutional filters and max pooling without
modification. Our evaluation of synthetic and real data shows that unsupervised
learning of depth and ego-motion on cylindrical panoramic images can produce
high-quality depth maps and that an increased field-of-view improves ego-motion
estimation accuracy. We create two new datasets to evaluate our approach: a
synthetic dataset created using the CARLA simulator, and Headcam, a novel
dataset of panoramic video collected from a helmet-mounted camera while biking
in an urban setting. We also apply our network to the problem of converting
monocular panoramas to stereo panoramas.","['Alisha Sharma', 'Ryan Nett', 'Jonathan Ventura']",2020-10-14T16:41:33Z,http://arxiv.org/abs/2010.07704v2,"['cs.CV', 'cs.LG', 'cs.RO']","Unsupervised learning,Depth estimation,Ego-motion,Cylindrical panoramic video,Virtual reality,Convolutional neural network,CNN layers,Synthetic dataset,Real data,Field-of-view"
"When Virtual Therapy and Art Meet: A Case Study of Creative Drawing Game
  in Virtual Environments","There have been a resurge lately on virtual therapy and other virtual- and
tele-medicine services due to the new normal of practicing 'shelter at home'.
In this paper, we propose a creative drawing game for virtual therapy and
investigate user's comfort and movement freedom in a pilot study. In a
mixed-design study, healthy participants (N=16, 8 females) completed one of the
easy or hard trajectories of the virtual therapy game in standing and seated
arrangements using a virtual-reality headset. The results from participants'
movement accuracy, task completion time, and usability questionnaires indicate
that participants had significant performance differences on two levels of the
game based on its difficulty (between-subjects factor), but no difference in
seated and standing configurations (within-subjects factor). Also, the hard
mode was more favorable among participants. This work offers implications on
virtual reality and 3D-interactive systems, with specific contributions to
virtual therapy, and serious games for healthcare applications.","['Lauren Baron', 'Brian Cohn', 'Roghayeh Barmaki']",2020-10-16T02:08:03Z,http://arxiv.org/abs/2010.08100v1,['cs.HC'],"virtual therapy,art therapy,creative drawing game,virtual environments,virtual reality,tele-medicine,pilot study,mixed-design study,usability questionnaires,serious games"
"Teleoperated aerial manipulator and its avatar. Part 1: Communication,
  system's interconnection, control, and virtual world","The tasks that an aerial manipulator can perform are incredibly diverse.
However, nowadays the technology is not completely developed to achieve complex
tasks autonomously. That's why we propose a human-in-the-loop system that can
control a semi-autonomous aerial manipulator to accomplish these kinds of
tasks. Furthermore, motivated by the growing trend of virtual reality systems,
together with teleoperation, we develop a system composed of: an aerial
manipulator model programmed in PX4 and modeled in Gazebo, a virtual reality
immersion with an interactive controller, and the interconnection between the
aforementioned systems via the Internet. This research is the first part of a
broader project. In this part, we present experiments in the software in the
loop simulation. The code of this work is liberated on our GitHub page. Also, a
video shows the conducted experiments.","['Rodolfo Verdín', 'Germán Ramírez', 'Carlos Rivera', 'Gerardo Flores']",2020-10-19T22:41:16Z,http://arxiv.org/abs/2010.09903v1,"['cs.RO', 'cs.SY', 'eess.SY']","teleoperated,aerial manipulator,avatar,communication,system interconnection,control,virtual world,PX4,Gazebo"
Body coherence in curved-space virtual reality games,"Virtual-reality simulations of curved space are most effective and most fun
when presented as a game (for example, curved-space billiards), so the user not
only has something to see in the curved space, but also has something fun to do
there. However, such simulations encounter a geometrical problem: they must
track the player's hands as well as her head, and in curved space the effects
of holonomy would quickly lead to violations of ""body coherence"". That is, what
the player sees with her eyes would disagree with what she feels with her
hands. This article presents a solution to the body coherence problem, as well
as several other questions that arise in interactive VR simulations in curved
space (radians vs. meters, visualization of the projection transformation,
native-inhabitant view vs. tourist view, and mental models of curved space).",['Jeff Weeks'],2020-11-01T14:20:11Z,http://arxiv.org/abs/2011.00510v2,"['physics.ed-ph', 'cs.GR', 'math.HO', '51M10, 51-08']","virtual reality,curved space,body coherence,simulations,holonomy,interactive,VR,radians,meters,visualization"
"Immersive Interactive Quantum Mechanics for Teaching and Learning
  Chemistry","The impossibility of experiencing the molecular world with our senses hampers
teaching and understanding chemistry because very abstract concepts (such as
atoms, chemical bonds, molecular structure, reactivity) are required for this
process. Virtual reality, especially when based on explicit physical modeling
(potentially in real time), offers a solution to this dilemma. Chemistry
teaching can make use of advanced technologies such as virtual-reality
frameworks and haptic devices. We show how an immersive learning setting could
be applied to help students understand the core concepts of typical chemical
reactions by offering a much more intuitive approach than traditional learning
settings. Our setting relies on an interactive exploration and manipulation of
a chemical system; this system is simulated in real-time with quantum chemical
methods, and therefore, behaves in a physically meaningful way.","['Thomas Weymuth', 'Markus Reiher']",2020-11-06T09:37:04Z,http://arxiv.org/abs/2011.03256v1,"['physics.ed-ph', 'physics.chem-ph', 'physics.pop-ph', 'quant-ph']","Quantum mechanics,Chemistry,Virtual reality,Physical modeling,Molecular structure,Reactive,Virtual-reality frameworks,Haptic devices,Chemical reactions,Quantum chemical methods"
"Student and Teacher Meet in a Shared Virtual Reality: A one-on-one
  Tutoring System for Anatomy Education","We introduce a Virtual Reality (VR) one-on-one tutoring system to support
anatomy education. A student uses a fully immersive VR headset to explore the
anatomy of the base of the human skull. A teacher guides the student by using
the semi-immersive zSpace. Both systems are connected via network and each
action is synchronized between both systems.
  The teacher is provided with various features to direct the student through
the immersive learning experience. She can influence the student's navigation
or provide annotations on the fly and, hereby, improve the students learning
experience. The system is implemented using the \textit{Unity} game engine. A
qualitative user study demonstrates that the one-on-one tutoring approach is
feasible and sets a solid base for future research in the area of shared
virtual environments for anatomy education.","['Patrick Saalfeld', 'Anna Schmeier', ""Wolfgang D'Hanis"", 'Hermann-Josef Rothkötter', 'Bernhard Preim']",2020-11-16T13:17:52Z,http://arxiv.org/abs/2011.07926v1,['cs.HC'],"Virtual Reality,Tutoring System,Anatomy Education,Immersive Learning,zSpace,Network Connectivity,Unity,User Study,Shared Virtual Environments,Human Skull"
"Traffic Characteristics of Virtual Reality over Edge-enabled Wi-Fi
  Networks","Virtual reality (VR) is becoming prevalent with a plethora of applications in
education, healthcare, entertainment, etc. To increase the user mobility, and
to reduce the energy consumption and production cost of VR head mounted
displays (HMDs), wireless VR with edge-computing has been the focus of both
industry and academia. However, transferring large video frames of VR
applications with their stringent Quality of Service (QoS) requirements over
wireless network requires innovations and optimizations across different
network layers. In order to develop efficient architectures, protocols and
scheduling mechanisms, the traffic characteristics of various types of VR
applications are required. In this paper, we first compute the theoretical
throughput requirements of an ideal VR experience as well as a popular VR HMD.
We then examine the traffic characteristics of a set of VR applications using
an edge-enabled Wi-Fi network. Our results reveal interesting findings that can
be considered in developing new optimizations, protocols, access mechanisms and
scheduling algorithms.","['Seyedmohammad Salehi', 'Abdullah Alnajim', 'Xiaoqing Zhu', 'Malcolm Smith', 'Chien-Chung Shen', 'Leonard Cimini']",2020-11-18T01:56:36Z,http://arxiv.org/abs/2011.09035v1,['cs.NI'],"Virtual reality,Edge computing,Wi-Fi networks,Quality of Service,Traffic characteristics,VR applications,Throughput requirements,Scheduling mechanisms,Network layers,Protocols"
"Learning-based Prediction and Uplink Retransmission for Wireless Virtual
  Reality (VR) Network","Wireless Virtual Reality (VR) users are able to enjoy immersive experience
from anywhere at anytime. However, providing full spherical VR video with high
quality under limited VR interaction latency is challenging. If the viewpoint
of the VR user can be predicted in advance, only the required viewpoint is
needed to be rendered and delivered, which can reduce the VR interaction
latency. Therefore, in this paper, we use offline and online learning
algorithms to predict viewpoint of the VR user using real VR dataset. For the
offline learning algorithm, the trained learning model is directly used to
predict the viewpoint of VR users in continuous time slots. While for the
online learning algorithm, based on the VR user's actual viewpoint delivered
through uplink transmission, we compare it with the predicted viewpoint and
update the parameters of the online learning algorithm to further improve the
prediction accuracy. To guarantee the reliability of the uplink transmission,
we integrate the Proactive retransmission scheme into our proposed online
learning algorithm. Simulation results show that our proposed online learning
algorithm for uplink wireless VR network with the proactive retransmission
scheme only exhibits about 5% prediction error.","['Xiaonan Liu', 'Xinyu Li', 'Yansha Deng']",2020-12-16T18:31:05Z,http://arxiv.org/abs/2012.12725v1,"['eess.SP', 'cs.LG']","learning-based prediction,uplink retransmission,wireless virtual reality,VR network,immersive experience,viewpoint prediction,offline learning,online learning,real VR dataset,proactive retransmission"
"Remote VR Studies -- A Framework for Running Virtual Reality Studies
  Remotely Via Participant-Owned HMDs","We investigate the opportunities and challenges of running virtual reality
(VR) studies remotely. Today, many consumers own head-mounted displays (HMDs),
allowing them to participate in scientific studies from their homes using their
own equipment. Researchers can benefit from this approach by being able to
reach a more diverse study population and to conduct research at times when it
is difficult to get people into the lab (cf. the COVID pandemic). We first
conducted an online survey (N=227), assessing HMD owners' demographics, their
VR setups, and their attitudes towards remote participation. We then identified
different approaches to running remote studies and conducted two case studies
for an in-depth understanding. We synthesize our findings into a framework for
remote VR studies, discuss the strengths and weaknesses of the different
approaches, and derive best practices. Our work is valuable for HCI researchers
conducting VR studies outside labs.","['Radiah Rivu', 'Ville Mäkelä', 'Sarah Prange', 'Sarah Delgado Rodriguez', 'Robin Piening', 'Yumeng Zhou', 'Kay Köhle', 'Ken Pfeuffer', 'Yomna Abdelrahman', 'Matthias Hoppe', 'Albrecht Schmidt', 'Florian Alt']",2021-02-22T17:36:15Z,http://arxiv.org/abs/2102.11207v1,['cs.HC'],"Remote VR Studies,Virtual Reality,HMDs,Participant-Owned,Framework,Remote Participation,Case Studies,Best Practices,HCI Researchers"
vrCAPTCHA: Exploring CAPTCHA Designs in Virtual Reality,"With the popularity of online access in virtual reality (VR) devices, it will
become important to investigate exclusive and interactive CAPTCHA (Completely
Automated Public Turing test to tell Computers and Humans Apart) designs for VR
devices. In this paper, we first present four traditional two-dimensional (2D)
CAPTCHAs (i.e., text-based, image-rotated, image-puzzled, and image-selected
CAPTCHAs) in VR. Then, based on the three-dimensional (3D) interaction
characteristics of VR devices, we propose two vrCAPTCHA design prototypes
(i.e., task-driven and bodily motion-based CAPTCHAs). We conducted a user study
with six participants for exploring the feasibility of our two vrCAPTCHAs and
traditional CAPTCHAs in VR. We believe that our two vrCAPTCHAs can be an
inspiration for the further design of CAPTCHAs in VR.","['Xiang Li', 'Yuzheng Chen', 'Rakesh Patibanda', ""Florian 'Floyd' Mueller""]",2021-02-24T14:43:11Z,http://arxiv.org/abs/2102.12313v2,['cs.HC'],"virtual reality,CAPTCHA,two-dimensional,three-dimensional,interaction characteristics,user study,design prototypes,text-based,image-rotated,image-puzzled"
Domain and View-point Agnostic Hand Action Recognition,"Hand action recognition is a special case of action recognition with
applications in human-robot interaction, virtual reality or life-logging
systems. Building action classifiers able to work for such heterogeneous action
domains is very challenging. There are very subtle changes across different
actions from a given application but also large variations across domains (e.g.
virtual reality vs life-logging). This work introduces a novel skeleton-based
hand motion representation model that tackles this problem. The framework we
propose is agnostic to the application domain or camera recording view-point.
When working on a single domain (intra-domain action classification) our
approach performs better or similar to current state-of-the-art methods on
well-known hand action recognition benchmarks. And, more importantly, when
performing hand action recognition for action domains and camera perspectives
which our approach has not been trained for (cross-domain action
classification), our proposed framework achieves comparable performance to
intra-domain state-of-the-art methods. These experiments show the robustness
and generalization capabilities of our framework.","['Alberto Sabater', 'Iñigo Alonso', 'Luis Montesano', 'Ana C. Murillo']",2021-03-03T10:32:36Z,http://arxiv.org/abs/2103.02303v3,['cs.CV'],"hand action recognition,domain agnostic,view-point agnostic,skeleton-based,motion representation,intra-domain action classification,cross-domain action classification,state-of-the-art methods,generalization capabilities"
"VXSlate: Combining Head Movement and Mobile Touch for Large Virtual
  Display Interaction","Virtual Reality (VR) headsets can open opportunities for users to accomplish
complex tasks on large virtual displays, using compact setups. However,
interacting with large virtual displays using existing interaction techniques
might cause fatigue, especially for precise manipulations, due to the lack of
physical surfaces. We designed VXSlate, an interaction technique that uses a
large virtual display, as an expansion of a tablet. VXSlate combines a user's
headmovement, as tracked by the VR headset, and touch interaction on the
tablet. The user's headmovement position both a virtual representation of the
tablet and of the user's hand on the large virtual display. The user's
multi-touch interactions perform finely-tuned content manipulations.","['Khanh-Duy Le', 'Tanh Quang Tran', 'Karol Chlasta', 'Krzysztof Krejtz', 'Morten Fjeld', 'Andreas Kunz']",2021-03-14T16:18:01Z,http://arxiv.org/abs/2103.07964v2,"['cs.HC', '68M99', 'H.5.2; B.4.2; J.7']","Virtual Reality,Head Movement,Mobile Touch,Large Virtual Display,Interaction Technique,Tablet,User's Hand,Multi-touch Interactions,Content Manipulations"
"Virtual Reality and Immersive Collaborative Environments: the New
  Frontier for Big Data Visualisation","The IDIA Visualisation Laboratory based at the University of Cape Town is
exploring the use of virtual reality technology to visualise and analyse
astronomical data. The iDaVIE software suite currently under development reads
from both volumetric data cubes and sparse multi-dimensional catalogs,
rendering them in a room-scale immersive environment that allows the user to
intuitively view, navigate around and interact with features in three
dimensions. This paper will highlight how the software imports from common
astronomy data formats and processes the information for loading into the Unity
game engine. It will also describe what tools are currently available to the
user and the various performance optimisations made for seamless use.
Applications by astronomers will be reviewed in addition to the features we
plan to include in future releases.","['Alexander K. Sivitilli', 'Angus Comrie', 'Lucia Marchetti', 'Thomas H. Jarrett']",2021-03-26T11:02:39Z,http://arxiv.org/abs/2103.14397v1,"['astro-ph.IM', 'astro-ph.GA']","Virtual reality,Immersive collaborative environments,Big data visualization,Astronomical data,Volumetric data cubes,Multi-dimensional catalogs,Unity game engine,Performance optimizations,Astronomy data formats"
"Congruence and Plausibility, not Presence?! Pivotal Conditions for XR
  Experiences and Effects, a Novel Model","Presence often is considered the most important quale describing the
subjective feeling of being in a computer-generated and/or computer-mediated
virtual environment. The identification and separation of orthogonal presence
components, i.e., the place illusion and the plausibility illusion, has been an
accepted theoretical model describing Virtual Reality (VR) experiences for some
time. This perspective article challenges this presence-oriented VR theory.
First, we argue that a place illusion cannot be the major construct to describe
the much wider scope of Virtual, Augmented, and Mixed Reality (VR, AR, MR: or
XR for short). Second, we argue that there is no plausibility illusion but
merely plausibility, and we derive the place illusion caused by congruent and
plausible generation of spatial cues, and similarly for all the current model's
so-defined illusions. Finally, we propose congruence and plausibility to become
the central essential conditions in a novel theoretical model describing XR
experiences and effects.","['Marc Erich Latoschik', 'Carolin Wienrich']",2021-04-10T19:25:17Z,http://arxiv.org/abs/2104.04846v5,"['cs.HC', 'H.5.1']","Congruence,Plausibility,Presence,XR,Virtual Reality,Augmented Reality,Mixed Reality,Spatial cues,Illusions"
"Social Virtual Reality: Ethical Considerations and Future Directions for
  An Emerging Research Space","The boom of commercial social virtual reality (VR) platforms in recent years
has signaled the growth and wide-spread adoption of consumer VR. Social VR
platforms draw aspects from traditional 2D virtual worlds where users engage in
various immersive experiences, interactive activities, and choices in
avatar-based representation. However, social VR also demonstrates specific
nuances that extend traditional 2D virtual worlds and other online social
spaces, such as full/partial body tracked avatars, experiencing mundane
everyday activities in a new way (e.g., sleeping), and an immersive means to
explore new and complex identities. The growing popularity has signaled
interest and investment from top technology companies who each have their own
social VR platforms. Thus far, social VR has become an emerging research space,
mainly focusing on design strategies, communication and interaction modalities,
nuanced activities, self-presentation, harassment, privacy, and
self-disclosure. These recent works suggest that many questions still remain in
social VR scholarship regarding how to ethically conduct research on these
sites and which research areas require additional attention. Therefore, in this
paper, we provide an overview of modern Social VR, critically review current
scholarship in the area, raise ethical considerations for conducting research
on these sites, and highlight unexplored areas.","['Divine Maloney', 'Guo Freeman', 'Andrew Robb']",2021-04-11T15:34:59Z,http://arxiv.org/abs/2104.05030v1,['cs.HC'],"Social Virtual Reality,Ethical Considerations,Future Directions,Consumer VR,Avatar-Based Representation,Body Tracked Avatars,Immersive Experiences,Interaction Modalities,Self-Presentation,Privacy"
"Inverse design enables large-scale high-performance meta-optics
  reshaping virtual reality","Meta-optics has achieved major breakthroughs in the past decade; however,
conventional forward design faces challenges as functionality complexity and
device size scale up. Inverse design aims at optimizing meta-optics design but
has been currently limited by expensive brute-force numerical solvers to small
devices, which are also difficult to realize experimentally. Here, we present a
general inverse design framework for aperiodic large-scale complex meta-optics
in three dimensions, which alleviates computational cost for both simulation
and optimization via a fast-approximate solver and an adjoint method,
respectively. Our framework naturally accounts for fabrication constraints via
a surrogate model. In experiments, we demonstrate, for the first time,
aberration-corrected metalenses working in the visible with high numerical
aperture, poly-chromatic focusing, and large diameter up to centimeter scale.
Such large-scale meta-optics opens a new paradigm for applications, and we
demonstrate its potential for future virtual-reality platforms by using a
meta-eyepiece and a laser back-illuminated micro-Liquid Crystal Display.","['Zhaoyi Li', 'Raphaël Pestourie', 'Joon-Suh Park', 'Yao-Wei Huang', 'Steven G. Johnson', 'Federico Capasso']",2021-04-20T00:55:17Z,http://arxiv.org/abs/2104.09702v1,['physics.optics'],"Meta-optics,Inverse design,Aperiodic,Large-scale,Complex,Three dimensions,Numerical aperture,Aberration-corrected,Virtual reality"
Semi-Autonomous Planning and Visualization in Virtual Reality,"Virtual reality (VR) interfaces for robots provide a three-dimensional (3D)
view of the robot in its environment, which allows people to better plan
complex robot movements in tight or cluttered spaces. In our prior work, we
created a VR interface to allow for the teleoperation of a humanoid robot. As
detailed in this paper, we have now focused on a human-in-the-loop planner
where the operator can send higher level manipulation and navigation goals in
VR through functional waypoints, visualize the results of a robot planner in
the 3D virtual space, and then deny, alter or confirm the plan to send to the
robot. In addition, we have adapted our interface to also work for a mobile
manipulation robot in addition to the humanoid robot. For a video demonstration
please see the accompanying video at https://youtu.be/wEHZug_fxrA.","['Gregory LeMasurier', 'Jordan Allspaw', 'Holly A. Yanco']",2021-04-23T21:48:05Z,http://arxiv.org/abs/2104.11827v1,"['cs.RO', 'I.2.9']","Semi-Autonomous Planning,Visualization,Virtual Reality,3D view,Robot,Teleoperation,Human-in-the-loop planner,Waypoints,Manipulation goals,Navigation goals"
"Millimeter-Wave Beamforming with Continuous Coverage for Mobile
  Interactive Virtual Reality","Contemporary Virtual Reality (VR) setups commonly consist of a Head-Mounted
Display (HMD) tethered to a content-generating server. ""Cutting the wire"" in
such setups and going truly wireless will require a wireless network capable of
delivering enormous amounts of video data at an extremely low latency. Higher
frequencies, such as the millimeter-wave (mmWave) band, can support these
requirements. Due to high attenuation and path loss in the mmWave frequencies,
beamforming is essential. For VR setups, beamforming must adapt in real-time to
the user's head rotations, but can rely on the HMD's built-in sensors providing
accurate orientation estimates. In this work, we present coVRage, a beamforming
solution tailored for VR HMDs. Based on past and current head orientations, the
HMD predicts how the Angle of Arrival (AoA) from the access point will change
in the near future, and covers this AoA trajectory with a dynamically shaped
beam, synthesized using sub-arrays. We show that this solution can cover such
trajectories with consistently high gain, unlike regular single-beam solutions.","['Jakob Struye', 'Filip Lemic', 'Jeroen Famaey']",2021-05-25T09:50:48Z,http://arxiv.org/abs/2105.11793v1,['cs.NI'],"Millimeter-wave,Beamforming,Virtual Reality,Head-Mounted Display,Latency,Frequencies,Path loss,Sensors,Angle of Arrival,Sub-arrays"
"Latency and Information Freshness in Multipath Communications for
  Virtual Reality","Wireless Virtual Reality (VR) and Augmented Reality (AR) will contribute to
people increasingly working and socializing remotely. However, the VR/AR
experience is very susceptible to various delays and timing discrepancies,
which can lead to motion sickness and discomfort. This paper models and
exploits the existence of multiple paths and redundancy to improve the timing
performance of wireless VR communications. We consider Multiple Description
Coding (MDC), a scheme where the video stream is encoded in Q streams (Q = 2 in
this paper) known as descriptors and delivered independently over multiple
paths. We also consider an alternating scheme, that simply switches between the
paths. We analyze the full distribution of two relevant metrics: the packet
delay and the Peak Age of Information (PAoI), which measures the freshness of
the information at the receiver. The results show interesting trade-offs
between picture quality, frame rate, and latency: full duplication results in
fewer lost frames, but a higher latency than schemes with less redundancy. Even
the simple alternating scheme can outperform duplication in terms of PAoI, but
MDC can exploit the independent decodability of the descriptors to deliver a
basic version of the frames faster, while still getting the full-quality frames
with a slightly higher delay.","['Federico Chiariotti', 'Beatriz Soret', 'Petar Popovski']",2021-06-10T10:47:54Z,http://arxiv.org/abs/2106.05652v1,['cs.NI'],"Latency,Information Freshness,Multipath Communications,Virtual Reality,Wireless,Augmented Reality,Multiple Description Coding,Peak Age of Information,Redundancy,Packet Delay"
Evaluating Foveated Video Quality Using Entropic Differencing,"Virtual Reality is regaining attention due to recent advancements in hardware
technology. Immersive images / videos are becoming widely adopted to carry
omnidirectional visual information. However, due to the requirements for higher
spatial and temporal resolution of real video data, immersive videos require
significantly larger bandwidth consumption. To reduce stresses on bandwidth,
foveated video compression is regaining popularity, whereby the space-variant
spatial resolution of the retina is exploited. Towards advancing the progress
of foveated video compression, we propose a full reference (FR) foveated image
quality assessment algorithm, which we call foveated entropic differencing
(FED), which employs the natural scene statistics of bandpass responses by
applying differences of local entropies weighted by a foveation-based error
sensitivity function. We evaluate the proposed algorithm by measuring the
correlations of the predictions that FED makes against human judgements on the
newly created 2D and 3D LIVE-FBT-FCVR databases for Virtual Reality (VR). The
performance of the proposed algorithm yields state-of-the-art as compared with
other existing full reference algorithms. Software for FED has been made
available at: http://live.ece.utexas.edu/research/Quality/FED.zip","['Yize Jin', 'Anjul Patney', 'Alan Bovik']",2021-06-12T16:29:13Z,http://arxiv.org/abs/2106.06817v1,"['eess.IV', 'cs.CV']","Foveated video quality,Entropic differencing,Virtual Reality,Bandwidth consumption,Foveated video compression,Full reference algorithm,Natural scene statistics,Entropies,Error sensitivity function,State-of-the-art"
"Virtual Reality based Digital Twin System for remote laboratories and
  online practical learning","There is a need for remote learning and virtual learning applications such as
virtual reality (VR) and tablet-based solutions which the current pandemic has
demonstrated. Creating complex learning scenarios by developers is highly
time-consuming and can take over a year. There is a need to provide a simple
method to enable lecturers to create their own content for their laboratory
tutorials. Research is currently being undertaken into developing generic
models to enable the semi-automatic creation of a virtual learning application.
A case study describing the creation of a virtual learning application for an
electrical laboratory tutorial is presented.","['Claire Palmer', 'Ben Roullier', 'Muhammad Aamir', 'Leonardo Stella', 'Uchenna Diala', 'Ashiq Anjum', 'Frank Mcquade', 'Keith Cox', 'Alex Calvert']",2021-06-17T09:38:24Z,http://arxiv.org/abs/2106.09344v1,['cs.AI'],"Virtual Reality,Digital Twin System,remote laboratories,online practical learning,pandemic,learning scenarios,developers,laboratory tutorials,generic models,virtual learning application"
"Mixed reality technologies for people with dementia: Participatory
  evaluation methods","Technologies can support people with early onset dementia (PwD) to aid them
in Instrumental Activities of Daily Living (IADL). The integration of physical
and virtual realities in Mixed reality technologies (MRTs) could provide
scalable and deployable options in developing prompting systems for PwD.
However, these emerging technologies should be evaluated and investigated for
feasibility with PwD. Survey instruments such as SUS, SUPR-Q and ethnographic
methods that are used for usability evaluation of websites and apps are used to
evaluate and study MRTs. However, PwD who cannot provide written and verbal
feedback are unable to participate in these studies. MRTs also present
challenges due to different ways in which physical and virtual realities could
be coupled. Experiences with physical, virtual and the couplings between the
two are to be considered in evaluating MRTs.","['Shital Desai', 'Arlene Astell']",2021-06-07T00:00:15Z,http://arxiv.org/abs/2107.07336v1,['cs.HC'],"dementia,Mixed reality technologies,participatory evaluation methods,instrumental activities of daily living,prompting systems,feasibility,survey instruments,usability evaluation,ethnographic methods,physical and virtual realities"
"Evaluating Performance and Gameplay of Virtual Reality Sickness
  Techniques in a First-Person Shooter Game","In virtual reality (VR) games, playability and immersion levels are important
because they affect gameplay, enjoyment, and performance. However, they can be
adversely affected by VR sickness (VRS) symptoms. VRS can be minimized by
manipulating users' perception of the virtual environment via the head-mounted
display (HMD). One extreme example is the Teleport mitigation technique, which
lets users navigate discretely, skipping sections of the virtual space. Other
techniques are less extreme but still rely on controlling what and how much
users see via the HMD. This research examines the effect on players'
performance and gameplay of these mitigation techniques in fast-paced VR games.
Our focus is on two types of visual reduction techniques. This study aims to
identify specifically the trade-offs these techniques have in a first-person
shooter game regarding immersion, performance, and VRS. The main contributions
in this paper are (1) a deeper understanding of one of the most popular
techniques (Teleport) when it comes to gameplay; (2) the replication and
validation of a novel VRS mitigation technique based on visual reduction; and
(3) a comparison of their effect on players' performance and gameplay.","['Diego Monteiro', 'Hao Chen', 'Hai-Ning Liang', 'Huawei Tu', 'Henry Dub']",2021-07-18T13:00:22Z,http://arxiv.org/abs/2107.08432v1,['cs.HC'],"Virtual reality,Sickness,Gameplay,Performance,First-person shooter,Immersion,Teleport,Mitigation technique,Head-mounted display,Visual reduction."
"Jarvis for Aeroengine Analytics: A Speech Enhanced Virtual Reality
  Demonstrator Based on Mining Knowledge Databases","In this paper, we present a Virtual Reality (VR) based environment where the
engineer interacts with incoming data from a fleet of aeroengines. This data
takes the form of 3D computer-aided design (CAD) engine models coupled with
characteristic plots for the subsystems of each engine. Both the plots and
models can be interacted with and manipulated using speech or gestural input.
The characteristic data is ported to a knowledge-based system underpinned by a
knowledge-graph storing complex domain knowledge. This permits the system to
respond to queries about the current state and health of each aeroengine asset.
Responses to these questions require some degree of analysis, which is handled
by a semantic knowledge representation layer managing information on aeroengine
subsystems. This paper represents a significant step forward for aeroengine
analysis in a bespoke VR environment and brings us a step closer to a
Jarvis-like system for aeroengine analytics.","['Sławomir Konrad Tadeja', 'Krzysztof Kutt', 'Yupu Lu', 'Pranay Seshadri', 'Grzegorz J. Nalepa', 'Per Ola Kristensson']",2021-07-28T14:46:16Z,http://arxiv.org/abs/2107.13403v1,['cs.HC'],"Virtual Reality,Aeroengines,Knowledge Databases,Speech Enhancement,Data Analysis,Knowledge-based System,Knowledge Graph,Semantic Representation,CAD Models,Gestural Input."
"An examination of skill requirements for Augmented Reality and Virtual
  Reality job advertisements","The field of Augmented Reality (AR) and Virtual Reality (VR) has seen massive
growth in recent years. Numerous degree programs have started to redesign their
curricula to meet the high market demand of such job positions. In this paper,
we performed a content analysis of online job postings hosted on Indeed.com and
provided a skill classification framework for AR/VR job positions. Furthermore,
we present a ranking of the relevant skills for the job position. Overall, we
noticed that technical skills like UI/UX design, software design, asset design
and graphics rendering are highly desirable for AR/VR positions. Our findings
regarding prominent skill categories could be beneficial for the human resource
departments as well as enhancing existing course curricula to tailor to the
high market demand.","['Amit Verma', 'Pratibha Purohit', 'Timothy Thornton', 'Kamal Lamsal']",2021-08-10T22:06:10Z,http://arxiv.org/abs/2108.04946v1,['cs.CY'],"Augmented Reality,Virtual Reality,job advertisements,skill requirements,content analysis,skill classification framework,UI/UX design,software design,asset design,graphics rendering"
"The ""Kinesthetic HMD"": Enhancing Self-Motion Sensations in VR with
  Head-Based Force Feedback","The sensation of self-motion is essential in many virtual reality
applications, from entertainment to training, such as flying and driving
simulators. If the common approach used in amusement parks is to actuate the
seats with cumbersome systems, multisensory integration can also be leveraged
to get rich effects from lightweight solutions. In this short paper, we
introduce a novel approach called the ""Kinesthetic HMD"": actuating a
head-mounted display with force feedback in order to provide sensations of
self-motion. We discuss its design considerations and demonstrate an augmented
flight simulator use case with a proof-of-concept prototype. We conducted a
user study assessing our approach's ability to enhance self-motion sensations.
Taken together, our results show that our Kinesthetic HMD provides
significantly stronger and more egocentric sensations than a visual-only
self-motion experience. Thus, by providing congruent vestibular and
proprioceptive cues related to balance and self-motion, the Kinesthetic HMD
represents a promising approach for a variety of virtual reality applications
in which motion sensations are prominent.","['Antoine Costes', 'Anatole Lécuyer']",2021-08-23T14:26:34Z,http://arxiv.org/abs/2108.10196v1,['cs.HC'],"Kinesthetic HMD,Self-motion sensations,VR,Head-based force feedback,Multisensory integration,Augmented flight simulator,User study,Vestibular cues,Proprioceptive cues"
"Rule-based Adaptations to Control Cybersickness in Social Virtual
  Reality Learning Environments","Social virtual reality learning environments (VRLEs) provide immersive
experience to users with increased accessibility to remote learning. Lack of
maintaining high-performance and secured data delivery in critical VRLE
application domains (e.g., military training, manufacturing) can disrupt
application functionality and induce cybersickness. In this paper, we present a
novel rule-based 3QS-adaptation framework that performs risk and cost aware
trade-off analysis to control cybersickness due to performance/security anomaly
events during a VRLE session. Our framework implementation in a social VRLE
viz., vSocial monitors performance/security anomaly events in network/session
data. In the event of an anomaly, the framework features rule-based adaptations
that are triggered by using various decision metrics. Based on our experimental
results, we demonstrate the effectiveness of our rule-based 3QS-adaptation
framework in reducing cybersickness levels, while maintaining application
functionality. Using our key findings, we enlist suitable practices for
addressing performance and security issues towards a more high-performing and
robust social VRLE.","['Samaikya Valluripally', 'Vaibhav Akashe', 'Michael Fisher', 'David Falana', 'Khaza Anuarul Hoque', 'Prasad Calyam']",2021-08-27T14:49:04Z,http://arxiv.org/abs/2108.12315v1,"['cs.HC', 'cs.CR', 'cs.DC']","social virtual reality learning environments,cybersickness,rule-based adaptations,3QS-adaptation framework,performance,security,anomaly events,network data,decision metrics"
"Towards Retina-Quality VR Video Streaming: 15ms Could Save You 80% of
  Your Bandwidth","Virtual reality systems today cannot yet stream immersive, retina-quality
virtual reality video over a network. One of the greatest challenges to this
goal is the sheer data rates required to transmit retina-quality video frames
at high resolutions and frame rates. Recent work has leveraged the decay of
visual acuity in human perception in novel gaze-contingent video compression
techniques. In this paper, we show that reducing the motion-to-photon latency
of a system itself is a key method for improving the compression ratio of
gaze-contingent compression. Our key finding is that a client and streaming
server system with sub-15ms latency can achieve 5x better compression than
traditional techniques while also using simpler software algorithms than
previous work.","['Luke Hsiao', 'Brooke Krajancich', 'Philip Levis', 'Gordon Wetzstein', 'Keith Winstein']",2021-08-28T23:29:45Z,http://arxiv.org/abs/2108.12720v3,"['cs.NI', 'cs.GR', 'cs.MM']","virtual reality,video streaming,bandwidth,data rates,visual acuity,gaze-contingent compression,latency,compression ratio,software algorithms"
"RelicVR: A Virtual Reality Game for Active Exploration of Archaeological
  Relics","Digitalization is changing how people visit museums and explore the artifacts
they house. Museums, as important educational venues outside classrooms, need
to actively explore the application of digital interactive media, including
games that can balance entertainment and knowledge acquisition. In this paper,
we introduce RelicVR, a virtual reality (VR) game that encourages players to
discover artifacts through physical interaction in a game-based approach.
Players need to unearth artifacts hidden in a clod enclosure by using available
tools and physical movements. The game relies on the dynamic voxel deformation
technique to allow players to chip away earth covering the artifacts. We added
uncertainty in the exploration process to bring it closer to how archaeological
discovery happens in real life. Players do not know the shape or features of
the hidden artifact and have to take away the earth gradually but strategically
without hitting the artifact itself. From playtesting sessions with eight
participants, we found that the uncertainty elements are conducive to their
engagement and exploration experience. Overall, RelicVR is an innovative game
that can improve players' learning motivation and outcomes of ancient
artifacts.","['Yilin Liu', 'Yiming Lin', 'Rongkai Shi', 'Yiming Luo', 'Hai-Ning Liang']",2021-09-29T04:11:40Z,http://arxiv.org/abs/2109.14185v1,['cs.HC'],"Virtual Reality,Game,Archaeological,Relics,Digitalization,Interactive Media,Voxel Deformation,Uncertainty,Playtesting"
"Reducing the Human Factor in Virtual Reality Research to Increase
  Reproducibility and Replicability","The replication crisis is real, and awareness of its existence is growing
across disciplines. We argue that research in human-computer interaction (HCI),
and especially virtual reality (VR), is vulnerable to similar challenges due to
many shared methodologies, theories, and incentive structures. For this reason,
in this work, we transfer established solutions from other fields to address
the lack of replicability and reproducibility in HCI and VR. We focus on
reducing errors resulting from the so-called human factor and adapt established
solutions to the specific needs of VR research. In addition, we present a
toolkit to support the setup, execution, and evaluation of VR research. Some of
the features aim to reduce human errors and thus improve replicability and
reproducibility. Finally, the identified chances are applied to a typical
scientific process in VR.","['Daniel Hepperle', 'Tobias Dienlin', 'Matthias Wölfel']",2021-10-29T11:26:37Z,http://arxiv.org/abs/2110.15687v1,"['cs.HC', 'stat.ME']","Human-computer interaction,Virtual reality,Reproducibility,Replicability,Toolkit,Methodologies,Theories,Errors,Scientific process"
Virtual Reality for Emotion Elicitation -- A Review,"Emotions are multifaceted phenomena that affect our behaviour, perception,
and cognition. Increasing evidence indicates that induction mechanisms play a
crucial role in triggering emotions by simulating the sensations required for
an experimental design. Over the years, many reviews have evaluated a passive
elicitation mechanism where the user is an observer, ignoring the importance of
self-relevance in emotional experience. So, in response to the gap in the
literature, this study intends to explore the possibility of using Virtual
Reality (VR) as an active mechanism for emotion induction. Furthermore, for the
success and quality of research settings, VR must select the appropriate
material to effectively evoke emotions. Therefore, in the present review, we
evaluated to what extent VR visual and audio-visual stimuli, games, and tasks,
and 360-degree panoramas and videos can elicit emotions based on the current
literature. Further, we present public datasets generated by VR and
emotion-sensing interfaces that can be used in VR based research. The
conclusions of this survey reveal that VR has a great potential to evoke
emotions effectively and naturally by generating motivational and empathy
mechanisms which makes it an ecologically valid paradigm to study emotions.","['Rukshani Somarathna', 'Tomasz Bednarz', 'Gelareh Mohammadi']",2021-10-31T10:45:26Z,http://arxiv.org/abs/2111.04461v1,['cs.HC'],"Virtual Reality,Emotion Elicitation,Induction Mechanisms,Self-Relevance,Visual Stimuli,Audio-visual Stimuli,360-Degree Panoramas,Public Datasets,Emotion-sensing Interfaces,Ecologically Valid"
"ENI: Quantifying Environment Compatibility for Natural Walking in
  Virtual Reality","We present a novel metric to analyze the similarity between the physical
environment and the virtual environment for natural walking in virtual reality.
Our approach is general and can be applied to any pair of physical and virtual
environments. We use geometric techniques based on conforming constrained
Delaunay triangulations and visibility polygons to compute the Environment
Navigation Incompatibility (ENI) metric that can be used to measure the
complexity of performing simultaneous navigation. We demonstrate applications
of ENI for highlighting regions of incompatibility for a pair of environments,
guiding the design of the virtual environments to make them more compatible
with a fixed physical environment, and evaluating the performance of different
redirected walking controllers. We validate the ENI metric using simulations
and two user studies. Results of our simulations and user studies show that in
the environment pair that our metric identified as more navigable, users were
able to walk for longer before colliding with objects in the physical
environment. Overall, ENI is the first general metric that can automatically
identify regions of high and low compatibility in physical and virtual
environments. Our project website is available at https://gamma.umd.edu/eni/.","['Niall L. Williams', 'Aniket Bera', 'Dinesh Manocha']",2022-01-04T17:49:07Z,http://arxiv.org/abs/2201.01261v3,['cs.GR'],"Environment Compatibility,Natural Walking,Virtual Reality,Geometric Techniques,Delaunay Triangulations,Visibility Polygons,Navigation Incompatibility,Redirected Walking,User Studies"
"DReyeVR: Democratizing Virtual Reality Driving Simulation for
  Behavioural & Interaction Research","Simulators are an essential tool for behavioural and interaction research on
driving, due to the safety, cost, and experimental control issues of on-road
driving experiments. The most advanced simulators use expensive 360 degree
projections systems to ensure visual fidelity, full field of view, and
immersion. However, similar visual fidelity can be achieved affordably using a
virtual reality (VR) based visual interface. We present DReyeVR, an open-source
VR based driving simulator platform designed with behavioural and interaction
research priorities in mind. DReyeVR (read ""driver"") is based on Unreal Engine
and the CARLA autonomous vehicle simulator and has features such as eye
tracking, a functional driving heads-up display (HUD) and vehicle audio, custom
definable routes and traffic scenarios, experimental logging, replay
capabilities, and compatibility with ROS. We describe the hardware required to
deploy this simulator for under $5000$ USD, much cheaper than commercially
available simulators. Finally, we describe how DReyeVR may be leveraged to
answer an interaction research question in an example scenario.","['Gustavo Silvera', 'Abhijat Biswas', 'Henny Admoni']",2022-01-06T05:47:08Z,http://arxiv.org/abs/2201.01931v2,"['cs.HC', 'cs.AI', 'cs.GR', 'cs.RO']","virtual reality,driving simulator,behavioural research,interaction research,Unreal Engine,CARLA simulator,eye tracking,heads-up display (HUD),logging,ROS"
An Open Platform for Research about Cognitive Load in Virtual Reality,"The cognitive load can be used to assess if someone is struggling while
performing a task. It can be used in many different situations such as in
driving, piloting, studying, playing, working, etc. This information can help
to design better systems and even to create interactive systems that can be
aware of the user's cognitive load and adapt itself to the user. We propose an
open source platform that can be used for doing research about cognitive load
in virtual reality (VR). Our platform can be used for stimulating cognitive
load through several VR scenes and for analyzing cognitive load through
objective and subjective measurements.","['Olivier Augereau', 'Gabriel Brocheton', 'Pedro Paulo Do Prado Neto']",2022-01-17T08:31:02Z,http://arxiv.org/abs/2201.06273v1,"['cs.GR', 'cs.HC']","cognitive load,virtual reality,research,open platform,interactive systems,VR scenes,objective measurements,subjective measurements,driving,piloting"
"VibroWeight: Simulating Weight and Center of Gravity Changes of Objects
  in Virtual Reality for Enhanced Realism","Haptic feedback in virtual reality (VR) allows users to perceive the physical
properties of virtual objects (e.g., their weight and motion patterns).
However, the lack of haptic sensations deteriorates users' immersion and
overall experience. In this work, we designed and implemented a low-cost
hardware prototype with liquid metal, VibroWeight, which can work in
complementarity with commercial VR handheld controllers. VibroWeight is
characterized by bimodal feedback cues in VR, driven by adaptive absolute mass
(weights) and gravity shift. To our knowledge, liquid metal is used in a VR
haptic device for the first time. Our 29 participants show that VibroWeight
delivers significantly better VR experiences in realism and comfort.","['Xian Wang', 'Diego Monteiro', 'Lik-Hang Lee', 'Pan Hui', 'Hai-Ning Liang']",2022-01-18T16:01:38Z,http://arxiv.org/abs/2201.07078v1,['cs.HC'],"haptic feedback,virtual reality,physical properties,weight,motion patterns,immersion,hardware prototype,liquid metal,bimodal feedback cues,gravity shift"
Auditory Feedback for Standing Balance Improvement in Virtual Reality,"Virtual Reality (VR) users often experience postural instability, i.e.,
balance problems, which could be a major barrier to universal usability and
accessibility for all, especially for persons with balance impairments. Prior
research has confirmed the imbalance effect, but minimal research has been
conducted to reduce this effect. We recruited 42 participants (with balance
impairments: 21, without balance impairments: 21) to investigate the impact of
several auditory techniques on balance in VR, specifically spatial audio,
static rest frame audio, rhythmic audio, and audio mapped to the center of
pressure (CoP). Participants performed two types of tasks - standing visual
exploration and standing reach and grasp. Within-subject results showed that
each auditory technique improved balance in VR for both persons with and
without balance impairments. Spatial and CoP audio improved balance
significantly more than other auditory conditions. The techniques presented in
this research could be used in future virtual environments to improve standing
balance and help push VR closer to universal usability.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-02-09T22:11:42Z,http://arxiv.org/abs/2202.04743v1,['cs.HC'],"auditory feedback,standing balance,virtual reality,spatial audio,static rest frame audio,rhythmic audio,center of pressure,balance impairments,universal usability"
The Dark Side of Perceptual Manipulations in Virtual Reality,"""Virtual-Physical Perceptual Manipulations"" (VPPMs) such as redirected
walking and haptics expand the user's capacity to interact with Virtual Reality
(VR) beyond what would ordinarily physically be possible. VPPMs leverage
knowledge of the limits of human perception to effect changes in the user's
physical movements, becoming able to (perceptibly and imperceptibly) nudge
their physical actions to enhance interactivity in VR. We explore the risks
posed by the malicious use of VPPMs. First, we define, conceptualize and
demonstrate the existence of VPPMs. Next, using speculative design workshops,
we explore and characterize the threats/risks posed, proposing mitigations and
preventative recommendations against the malicious use of VPPMs. Finally, we
implement two sample applications to demonstrate how existing VPPMs could be
trivially subverted to create the potential for physical harm. This paper aims
to raise awareness that the current way we apply and publish VPPMs can lead to
malicious exploits of our perceptual vulnerabilities.","['Wen-Jie Tseng', 'Elise Bonnail', 'Mark McGill', 'Mohamed Khamis', 'Eric Lecolinet', 'Samuel Huron', 'Jan Gugenheimer']",2022-02-26T17:45:34Z,http://arxiv.org/abs/2202.13200v1,['cs.HC'],"Perceptual Manipulations,Virtual Reality,Redirected Walking,Haptics,Human Perception,Interactivity,Risks,Malicious Use,Mitigations,Preventative Recommendations"
"Virtual Reality Digital Twin and Environment for Troubleshooting
  Lunar-based Infrastructure Assembly Failures","Humans and robots will need to collaborate in order to create a sustainable
human lunar presence by the end of the 2020s. This includes cases in which a
human will be required to teleoperate an autonomous rover that has encountered
an instrument assembly failure. To aid teleoperators in the troubleshooting
process, we propose a virtual reality digital twin placed in a simulated
environment. Here, the operator can virtually interact with a digital version
of the rover and mechanical arm that uses the same controls and kinematic
model. The user can also adopt the egocentric (a first person view through
using stereoscopic passthrough) and exocentric (a third person view where the
operator can virtually walk around the environment and rover as if they were on
site) view. We also discuss our metrics for evaluating the differences between
our digital and physical robot, as well as the experimental concept based on
real and applicable missions, and future work that would compare our platform
to traditional troubleshooting methods.","['Phaedra S. Curlin', 'Madaline A. Muniz', 'Mason M. Bell', 'Alexis A. Muniz', 'Jack O. Burns']",2022-03-05T19:36:16Z,http://arxiv.org/abs/2203.02810v1,['cs.RO'],"virtual reality,digital twin,troubleshooting,lunar-based infrastructure,assembly failures,teleoperate,autonomous rover,kinematic model,egocentric view,exocentric view"
Immersive Virtual Reality Simulations of Bionic Vision,"Bionic vision uses neuroprostheses to restore useful vision to people living
with incurable blindness. However, a major outstanding challenge is predicting
what people 'see' when they use their devices. The limited field of view of
current devices necessitates head movements to scan the scene, which is
difficult to simulate on a computer screen. In addition, many computational
models of bionic vision lack biological realism. To address these challenges,
we present VR-SPV, an open-source virtual reality toolbox for simulated
prosthetic vision that uses a psychophysically validated computational model to
allow sighted participants to 'see through the eyes' of a bionic eye user. To
demonstrate its utility, we systematically evaluated how clinically reported
visual distortions affect performance in a letter recognition and an immersive
obstacle avoidance task. Our results highlight the importance of using an
appropriate phosphene model when predicting visual outcomes for bionic vision.","['Justin Kasowski', 'Michael Beyeler']",2022-03-09T02:30:42Z,http://arxiv.org/abs/2203.05675v1,['cs.HC'],"Immersive Virtual Reality,Bionic Vision,Neuroprostheses,Field of View,Computational Model,Virtual Reality Toolbox,Prosthetic Vision,Visual Distortions,Phosphene Model,Bionic Vision"
"Effect of Render Resolution on Gameplay Experience, Performance, and
  Simulator Sickness in Virtual Reality Games","Higher resolution is one of the main directions and drivers in the
development of virtual reality (VR) head-mounted displays (HMDs). However,
given its associated higher cost, it is important to determine the benefits of
having higher resolution on user experience. For non-VR games, higher
resolution is often thought to lead to a better experience, but it is
unexplored in VR games. This research aims to investigate the resolution
tradeoff in gameplay experience, performance, and simulator sickness (SS) for
VR games, particularly first-person shooter (FPS) games. To this end, we
designed an experiment to collect gameplay experience, SS, and player
performance data with a popular VR FPS game, Half-Life: Alyx. Our results
indicate that 2K resolution is an important threshold for an enhanced gameplay
experience without affecting performance and increasing SS levels. Moreover,
the resolution from 1K to 4K has no significant difference in player
performance. Our results can inform game developers and players in determining
the type of HMD they want to use to balance the tradeoff between costs and
benefits and achieve a more optimal experience.","['Jialin Wang', 'Rongkai Shi', 'Zehui Xiao', 'Xueying Qin', 'Hai-Ning Liang']",2022-03-23T09:37:48Z,http://arxiv.org/abs/2203.12294v1,['cs.HC'],"Render resolution,Gameplay experience,Performance,Simulator sickness,Virtual reality games,First-person shooter games,HMD,Half-Life: Alyx,Player performance,2K resolution"
"Learning Personalized Human-Aware Robot Navigation Using Virtual Reality
  Demonstrations from a User Study","For the most comfortable, human-aware robot navigation, subjective user
preferences need to be taken into account. This paper presents a novel
reinforcement learning framework to train a personalized navigation controller
along with an intuitive virtual reality demonstration interface. The conducted
user study provides evidence that our personalized approach significantly
outperforms classical approaches with more comfortable human-robot experiences.
We achieve these results using only a few demonstration trajectories from
non-expert users, who predominantly appreciate the intuitive demonstration
setup. As we show in the experiments, the learned controller generalizes well
to states not covered in the demonstration data, while still reflecting user
preferences during navigation. Finally, we transfer the navigation controller
without loss in performance to a real robot.","['Jorge de Heuvel', 'Nathan Corral', 'Lilli Bruckschen', 'Maren Bennewitz']",2022-03-28T13:31:12Z,http://arxiv.org/abs/2203.14741v2,"['cs.RO', 'cs.AI']","reinforcement learning,personalized navigation,virtual reality,user study,human-aware robot,demonstration interface"
"Beyond Being Real: A Sensorimotor Control Perspective on Interactions in
  Virtual Reality","We can create Virtual Reality (VR) interactions that have no equivalent in
the real world by remapping spacetime or altering users' body representation,
such as stretching the user's virtual arm for manipulation of distant objects
or scaling up the user's avatar to enable rapid locomotion. Prior research has
leveraged such approaches, what we call beyond-real techniques, to make
interactions in VR more practical, efficient, ergonomic, and accessible. We
present a survey categorizing prior movement-based VR interaction literature as
reality-based, illusory, or beyond-real interactions. We survey relevant
conferences (CHI, IEEE VR, VRST, UIST, and DIS) while focusing on selection,
manipulation, locomotion, and navigation in VR. For beyond-real interactions,
we describe the transformations that have been used by prior works to create
novel remappings. We discuss open research questions through the lens of the
human sensorimotor control system and highlight challenges that need to be
addressed for effective utilization of beyond-real interactions in future VR
applications, including plausibility, control, long-term adaptation, and
individual differences.","['Parastoo Abtahi', 'Sidney Q. Hough', 'James A. Landay', 'Sean Follmer']",2022-04-18T21:28:34Z,http://arxiv.org/abs/2204.08566v1,['cs.HC'],"Virtual Reality,Sensorimotor Control,Interactions,Beyond-real techniques,VR interaction,Spacetime,Body representation,Movement-based,Remappings"
Security and Privacy in Virtual Reality -- A Literature Survey,"Virtual Reality (VR) is a multibillionaire market that keeps growing, year
after year. As VR is becoming prevalent in households and small businesses, it
is critical to address the effects that this technology might have on the
privacy and security of its users. In this paper, we explore the
state-of-the-art in VR privacy and security, we categorise potential issues and
threats, and we analyse causes and effects of the identified threats. Besides,
we focus on the research previously conducted in the field of authentication in
VR, as it stands as the most investigated area in the topic. We also provide an
overview of other interesting uses of VR in the field of cybersecurity, such as
the use of VR to teach cybersecurity or evaluate the usability of security
solutions.",['Alberto Giaretta'],2022-04-30T08:45:09Z,http://arxiv.org/abs/2205.00208v2,['cs.CR'],"Virtual Reality,Privacy,Security,Threats,Authentication,Cybersecurity,Usability,Research,Literature Survey,Market"
Shared-Control Robotic Manipulation in Virtual Reality,"In this paper, we present the implementation details of a Virtual Reality
(VR)-based teleoperation interface for moving a robotic manipulator. We propose
an iterative human-in-the-loop design where the user sets the next task-space
waypoint for the robot's end effector and executes the action on the physical
robot before setting the next waypoints. Information from the robot's
surroundings is provided to the user in two forms: as a point cloud in 3D space
and a video stream projected on a virtual wall. The feasibility of the selected
end effector pose is communicated to the user by the color of the virtual end
effector. The interface is demonstrated to successfully work for a pick and
place scenario, however, our trials showed that the fluency of the interaction
and the autonomy level of the system can be increased.","['Shiyu Xu', 'Scott Moore', 'Akansel Cosgun']",2022-05-21T11:09:43Z,http://arxiv.org/abs/2205.10564v1,['cs.RO'],"Shared-Control,Robotic Manipulation,Virtual Reality,Teleoperation,Human-in-the-loop,Task-space waypoint,End effector,Point cloud,Video stream,Pick and place."
"Learning Effect of Lay People in Gesture-Based Locomotion in Virtual
  Reality","Locomotion in Virtual Reality (VR) is an important part of VR applications.
Many scientists are enriching the community with different variations that
enable locomotion in VR. Some of the most promising methods are gesture-based
and do not require additional handheld hardware. Recent work focused mostly on
user preference and performance of the different locomotion techniques. This
ignores the learning effect that users go through while new methods are being
explored. In this work, it is investigated whether and how quickly users can
adapt to a hand gesture-based locomotion system in VR. Four different
locomotion techniques are implemented and tested by participants. The goal of
this paper is twofold: First, it aims to encourage researchers to consider the
learning effect in their studies. Second, this study aims to provide insight
into the learning effect of users in gesture-based systems.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2022-06-16T10:44:16Z,http://arxiv.org/abs/2206.08076v1,"['cs.HC', 'cs.CV']","Virtual reality,Locomotion,Gesture-based,Learning effect,Adaptation,User preference,Performance,Handheld hardware,Locomotion techniques,Researchers"
"A guideline proposal for minimizing cybersickness in VR-based serious
  games and applications","Head-mounted displays (HMDs) are popular immersive tools in general, not
limited to entertainment but also for education, military, and serious games
for health. While these displays have strong popularity, they still have user
experience issues, triggering possible symptoms of discomfort to users. This
condition is known as cybersickness (CS) and is one of the most popular
research topics tied to virtual reality (VR) issues. We first present the main
strategies focused on minimizing cybersickness problems in virtual reality.
Following this, we propose a guideline framework based on CS causes such as
locomotion, acceleration, the field of view, depth of field, degree of freedom,
exposition use time, latency-lag, static rest frame, and camera rotation.
Additionally, serious games applications and broader categories of games can
also adopt it. Additionally, we categorized the imminent challenges for CS
minimization into four different items. Conclusively, this work contributes as
a consulting reference to enable VR developers and designers to optimize their
VR users' experience and VR serious games.","['Thiago Porcino', 'Derek Reilly', 'Esteban Clua', 'Daniela Trevisan']",2022-07-13T17:01:28Z,http://arxiv.org/abs/2207.06346v1,['cs.HC'],"cybersickness,VR-based serious games,head-mounted displays,discomfort,user experience,virtual reality,locomotion,acceleration,field of view,latency-lag"
Design of VR Engine Assembly Teaching System,"Virtual reality(VR) is a hot research topic, and it has been effectively
applied in military, education and other fields. The application prospect of
virtual reality in education is very broad. It can effectively reduce labor
cost, resource consumption, stimulate students' interest in learning, and
improve students' knowledge level. New energy vehicles have also been widely
promoted in recent years, and the production of new energy vehicles has played
a key role in it. However, the teaching of car engine disassembly and assembly
still retains a more traditional way. That's why applying VR technology has
high significance. This project uses the Unity 3D engine to develop a VR-based
engine teaching software, which aims to allow users to use VR headsets, handles
and other accessories to simulate the disassembly and assembly of car engines
in a virtual environment. We design a modular system framework and divided the
software into two layers, the system layer and the function layer. The system
layer includes a message system and a data configuration system. The functional
layer includes the user interface system, disassembly and assembly function,
and data module. In addition to fulfilling functional requirements , we used
the Unity UPR tool to check out performance issues, and optimized product
performance by turning off vertical sync and turning on static switches for
some scene objects.",['Zhang Jiayu'],2022-07-12T02:23:22Z,http://arxiv.org/abs/2207.07119v1,['cs.HC'],"VR,virtual reality,education,new energy vehicles,Unity 3D,engine assembly,VR headset,modular system framework,disassembly,assembly"
"Path Tracing in 2D, 3D, and Physicalized Networks","It is common to advise against using 3D to visualize abstract data such as
networks, however Ware and Mitchell's 2008 study showed that path tracing in a
network is less error prone in 3D than in 2D. It is unclear, however, if 3D
retains its advantage when the 2D presentation of a network is improved using
edge-routing, and when simple interaction techniques for exploring the network
are available. We address this with two studies of path tracing under new
conditions. The first study was preregistered, involved 34 users, and compared
2D and 3D layouts that the user could rotate and move in virtual reality with a
handheld controller. Error rates were lower in 3D than in 2D, despite the use
of edge-routing in 2D and the use of mouse-driven interactive highlighting of
edges. The second study involved 12 users and investigated data
physicalization, comparing 3D layouts in virtual reality versus physical 3D
printouts of networks augmented with a Microsoft HoloLens headset. No
difference was found in error rate, but users performed a variety of actions
with their fingers in the physical condition which can inform new interaction
techniques.","['Michael J. McGuffin', 'Ryan Servera', 'Marie Forest']",2022-07-23T19:35:53Z,http://arxiv.org/abs/2207.11586v1,['cs.HC'],"path tracing,2D,3D,networks,edge-routing,virtual reality,physicalization,interactive highlighting,Microsoft HoloLens"
"Virtual Reality Therapy for the Psychological Well-being of Palliative
  Care Patients in Hong Kong","In this paper we introduce novel Virtual Reality (VR) and Augmented Reality
(AR) treatments to improve the psychological well being of patients in
palliative care, based on interviews with a clinical psychologist who has
successfully implemented VR assisted interventions on palliative care patients
in the Hong Kong hospital system. Our VR and AR assisted interventions are
adaptations of traditional palliative care therapies which simultaneously
facilitate patients communication with family and friends while isolated in
hospital due to physical weakness and COVID-19 related restrictions. The first
system we propose is a networked, metaverse platform for palliative care
patients to create customized virtual environments with therapists, family and
friends which function as immersive and collaborative versions of 'life review'
and 'reminiscence therapy'. The second proposed system will investigate the use
of Mixed Reality telepresence and haptic touch in an AR environment, which will
allow palliative care patients to physically feel friends and family in a
virtual space, adding to the sense of presence and immersion in that
environment.","['Daniel Eckhoff', 'Royce Ng', 'Alvaro Cassinelli']",2022-07-24T14:31:52Z,http://arxiv.org/abs/2207.11754v1,['cs.HC'],"Virtual Reality,Augmented Reality,palliative care,psychological well-being,therapy,Hong Kong,virtual environments,life review,reminiscence therapy"
"Voice Analysis for Stress Detection and Application in Virtual Reality
  to Improve Public Speaking in Real-time: A Review","Stress during public speaking is common and adversely affects performance and
self-confidence. Extensive research has been carried out to develop various
models to recognize emotional states. However, minimal research has been
conducted to detect stress during public speaking in real time using voice
analysis. In this context, the current review showed that the application of
algorithms was not properly explored and helped identify the main obstacles in
creating a suitable testing environment while accounting for current
complexities and limitations. In this paper, we present our main idea and
propose a stress detection computational algorithmic model that could be
integrated into a Virtual Reality (VR) application to create an intelligent
virtual audience for improving public speaking skills. The developed model,
when integrated with VR, will be able to detect excessive stress in real time
by analysing voice features correlated to physiological parameters indicative
of stress and help users gradually control excessive stress and improve public
speaking performance","['Arushi', 'Roberto Dillon', 'Ai Ni Teoh', 'Denise Dillon']",2022-08-01T03:51:43Z,http://arxiv.org/abs/2208.01041v1,"['eess.AS', 'cs.HC', 'cs.LG', 'cs.MM', 'cs.SD', 'I.6; K.3; K.4; A.2']","Voice analysis,Stress detection,Virtual reality,Public speaking,Real-time,Computational algorithmic model,Virtual audience,Physiological parameters,Intelligent virtual audience,Voice features"
"Vibrotactile Feedback to Make Real Walking in Virtual Reality More
  Accessible","This research aims to examine the effects of various vibrotactile feedback
techniques on gait (i.e., walking patterns) in virtual reality (VR). Prior
studies have demonstrated that gait disturbances in VR users are significant
usability barriers. However, adequate research has not been performed to
address this problem. In our study, 39 participants (with mobility impairments:
18, without mobility impairments: 21) performed timed walking tasks in a
real-world environment and identical activities in a VR environment with
different forms of vibrotactile feedback (spatial, static, and rhythmic).
Within-group results revealed that each form of vibrotactile feedback improved
gait performance in VR significantly (p < .001) relative to the no vibrotactile
condition in VR for individuals with and without mobility impairments.
Moreover, spatial vibrotactile feedback increased gait performance
significantly (p < .001) in both participant groups compared to other
vibrotactile conditions. The findings of this research will help to make real
walking in VR more accessible for those with and without mobility impairments.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-08-04T02:13:58Z,http://arxiv.org/abs/2208.02403v1,['cs.HC'],"Vibrotactile feedback,Gait,Virtual reality,Mobility impairments,Usability,Walking tasks,Spatial feedback,Static feedback,Rhythmic feedback"
"Drive Right: Shaping Public's Trust, Understanding, and Preference
  Towards Autonomous Vehicles Using a Virtual Reality Driving Simulator","Autonomous vehicles are increasingly introduced into our lives. Yet, people's
misunderstanding and mistrust have become the major obstacles to the use of
these technologies. In response to this problem, proper work must be done to
increase public's understanding and awareness and help drivers rationally
evaluate the system. The method proposed in this paper is a virtual reality
driving simulator which serves as a low-cost platform for autonomous vehicle
demonstration and education. To test the validity of the platform, we recruited
36 participants and conducted a test training drive using three different
scenarios. The results show that our simulator successfully increased
participants' understanding while favorably changing their attitude towards the
autonomous system. The methodology and findings presented in this paper can be
further explored by driving schools, auto manufacturers, and policy makers, to
improve training for autonomous vehicles.","['Zhijie Qiao', 'Xiatao Sun', 'Helen Loeb', 'Rahul Mangharam']",2022-08-05T00:30:18Z,http://arxiv.org/abs/2208.02939v2,['cs.HC'],"Autonomous vehicles,Trust,Understanding,Preference,Virtual reality,Driving simulator,Awareness,Evaluation,Training,Attitude"
Remote Assistance with Mixed Reality for Procedural Tasks,"We present a volumetric communication system that is designed for remote
assistance of procedural tasks. The system allows a remote expert to visually
guide a local operator. The two parties share a view that is spatially
identical, but for the local operator it is of the object on which they
operate, while for the remote expert, the object is presented as a mixed
reality ""hologram"". Guidance is provided by voice, gestures, and annotations
performed directly on the object of interest or its hologram. At each end of
the communication, spatial is visualized using mixed-reality glasses.","['Manuel Rebol', 'Colton Hood', 'Claudia Ranniger', 'Adam Rutenberg', 'Neal Sikka', 'Erin Maria Horan', 'Christian Gütl', 'Krzysztof Pietroszek']",2022-08-05T16:29:43Z,http://arxiv.org/abs/2208.03261v1,['cs.HC'],"Mixed Reality,Remote Assistance,Volumetric Communication System,Procedural Tasks,Hologram,Voice Guidance,Gestures,Annotations,Mixed-Reality Glasses"
"Towards Enabling Next Generation Societal Virtual Reality Applications
  for Virtual Human Teleportation","Virtual reality (VR) is an emerging technology of great societal potential.
Some of its most exciting and promising use cases include remote scene content
and untethered lifelike navigation. This article first highlights the relevance
of such future societal applications and the challenges ahead towards enabling
them. It then provides a broad and contextual high-level perspective of several
emerging technologies and unconventional techniques and argues that only by
their synergistic integration can the fundamental performance bottlenecks of
hyper-intensive computation, ultra-high data rate, and ultra-low latency be
overcome to enable untethered and lifelike VR-based remote scene immersion. A
novel future system concept is introduced that embodies this holistic
integration, unified with a rigorous analysis, to capture the fundamental
synergies and interplay between communications, computation, and signal
scalability that arise in this context, and advance its performance at the same
time. Several representative results highlighting these trade-offs and the
benefits of the envisioned system are presented at the end.","['Jacob Chakareski', 'Mahmudur Khan', 'Murat Yuksel']",2022-08-09T18:45:57Z,http://arxiv.org/abs/2208.04998v1,"['cs.NI', 'cs.MM', 'cs.SY', 'eess.IV', 'eess.SY', 'stat.AP']","Virtual reality,Societal applications,Virtual human teleportation,Emerging technologies,Untethered navigation,Remote scene immersion,Hyper-intensive computation,Ultra-low latency,Data rate,Performance bottlenecks"
Auditory Feedback to Make Walking in Virtual Reality More Accessible,"The objective of this study is to investigate the impact of several auditory
feedback modalities on gait (i.e., walking patterns) in virtual reality (VR).
Prior research has substantiated gait disturbances in VR users as one of the
primary obstacles to VR usability. However, minimal research has been done to
mitigate this issue. We recruited 39 participants (with mobility impairments:
18, without mobility impairments: 21) who completed timed walking tasks in a
real-world environment and the same tasks in a VR environment with various
types of auditory feedback. Within-subject results showed that each auditory
condition significantly improved gait performance while in VR (p < .001)
compared to the no auditory condition in VR for both groups of participants
with and without mobility impairments. Moreover, spatial audio improved gait
performance significantly (p < .001) compared to other auditory conditions for
both groups of participants. This research could help to make walking in VR
more accessible for people with and without mobility impairments.","['M. Rasel Mahmud', 'Michael Stewart', 'Alberto Cordova', 'John Quarles']",2022-08-17T16:29:27Z,http://arxiv.org/abs/2208.08390v1,['cs.HC'],"auditory feedback,gait,virtual reality,mobility impairments,spatial audio"
"Evaluation of Postural Muscle Synergies during a Complex Motor Task in a
  Virtual Reality Environment","In this study, we investigate how the central nervous system (CNS) organizes
postural control synergies when individuals perform a complex catch-and-throw
task in a virtual reality (VR) environment. A Robotic Upright Stand Trainer
(RobUST) platform, including surface electromyography and kinematics, was used
to investigate how the CNS fine-tunes postural synergies with perturbative and
assist-as-needed force fields. A control group without assistive forces was
recruited to elucidate the effect of force fields on motor performance and
postural synergy organization after the perturbation and during the VR reaching
task. We found that the application of assistive forces significantly improved
reaching and balance control. The group receiving assistive forces displayed
four postural control synergies characterized by higher complexity (i.e.,
greater number of muscles involved). However, control subjects displayed eight
synergies that recruited less number of muscles. In conclusion, assistive
forces reduce the number of postural synergies while increasing the complexity
of muscle module composition.","['Xupeng Ai', 'Victor Santamaria', 'Isirame Babajide Omofuma', 'Sunil K. Agrawal']",2022-08-18T18:07:11Z,http://arxiv.org/abs/2208.09009v1,"['cs.RO', 'physics.data-an']","central nervous system,postural control,synergies,virtual reality,electromyography,kinematics,assistive forces,muscle module,motor performance,balance control"
"VRBubble: Enhancing Peripheral Awareness of Avatars for People with
  Visual Impairments in Social Virtual Reality","Social Virtual Reality (VR) is growing for remote socialization and
collaboration. However, current social VR applications are not accessible to
people with visual impairments (PVI) due to their focus on visual experiences.
We aim to facilitate social VR accessibility by enhancing PVI's peripheral
awareness of surrounding avatar dynamics. We designed VRBubble, an audio-based
VR technique that provides surrounding avatar information based on social
distances. Based on Hall's proxemic theory, VRBubble divides the social space
with three Bubbles -- Intimate, Conversation, and Social Bubble -- generating
spatial audio feedback to distinguish avatars in different bubbles and provide
suitable avatar information. We provide three audio alternatives: earcons,
verbal notifications, and real-world sound effects. PVI can select and combine
their preferred feedback alternatives for different avatars, bubbles, and
social contexts. We evaluated VRBubble and an audio beacon baseline with 12 PVI
in a navigation and a conversation context. We found that VRBubble
significantly enhanced participants' avatar awareness during navigation and
enabled avatar identification in both contexts. However, VRBubble was shown to
be more distracting in crowded environments.","['Tiger Ji', 'Brianna R. Cochran', 'Yuhang Zhao']",2022-08-23T16:27:17Z,http://arxiv.org/abs/2208.11071v1,['cs.HC'],"Social Virtual Reality,Visual Impairments,Avatar,Peripheral Awareness,Spatial Audio,Hall's proxemic theory,Earcons,Verbal notifications,Sound effects,Accessibility"
"Scalably manufactured high-index atomic layer-polymer hybrid
  metasurfaces for high-efficiency virtual reality metaoptics in the visible","Metalenses, which exhibit superior light-modulating performance with
sub-micrometer-scale thicknesses, are suitable alternatives to conventional
bulky refractive lenses. However, fabrication limitations, such as a high cost,
low throughput, and small patterning area, hinder their mass production. Here,
we demonstrate the mass production of low-cost, high-throughput, and
large-aperture visible metalenses using an argon fluoride immersion scanner and
wafer-scale nanoimprint lithography. Once a 12-inch master stamp is imprinted,
hundreds of centimeter-scale metalenses can be fabricated. To enhance light
confinement, the printed metasurface is thinly coated with a high-index film,
resulting in drastic increase of conversion efficiency. As a proof of concept,
a prototype of a virtual reality device with ultralow thickness is demonstrated
with the fabricated metalens.","['Joohoon Kim', 'Junhwa Seong', 'Wonjoong Kim', 'Gun-Yeal Lee', 'Hongyoon Kim', 'Seong-Won Moon', 'Jaehyuck Jang', 'Yeseul Kim', 'Younghwan Yang', 'Dong Kyo Oh', 'Chanwoong Park', 'Hojung Choi', 'Hyeongjin Jeon', 'Kyung-Il Lee', 'Byoungho Lee', 'Heon Lee', 'Junsuk Rho']",2022-08-26T13:36:44Z,http://arxiv.org/abs/2208.12665v1,"['physics.optics', 'physics.app-ph']","metalenses,high-index film,atomic layer,polymer hybrid,metasurfaces,nanoimprint lithography,virtual reality,conversion efficiency,wafer-scale,mass production"
"MiCellAnnGELo: Annotate microscopy time series of complex cell surfaces
  with 3D Virtual Reality","Summary: Advances in 3D live cell microscopy are enabling high-resolution
capture of previously unobserved processes. Unleashing the power of modern
machine learning methods to fully benefit from these technologies is, however,
frustrated by the difficulty of manually annotating 3D training data.
MiCellAnnGELo virtual reality software offers an immersive environment for
viewing and interacting with 4D microscopy data, including efficient tools for
annotation. We present tools for labelling cell surfaces with a wide range of
applications, including cell motility, endocytosis, and transmembrane
signalling. Availability and implementation: MiCellAnnGELo employs the cross
platform (Mac/Unix/Windows) Unity game engine and is available under the MIT
licence at https://github.com/CellDynamics/MiCellAnnGELo.git, together with
sample data and demonstration movies. MiCellAnnGELo can be run in desktop mode
on a 2D screen or in 3D using a standard VR headset with compatible GPU.","['Adam Platt', 'E. Josiah Lutton', 'Edward Offord', 'Till Bretschneider']",2022-09-23T16:02:00Z,http://arxiv.org/abs/2209.11672v2,['cs.HC'],"microscopy,time series,cell surfaces,3D Virtual Reality,machine learning,annotation,cell motility,endocytosis,transmembrane signalling"
"Communication in Immersive Social Virtual Reality: A Systematic Review
  of 10 Years' Studies","As virtual reality (VR) technologies have improved in the past decade, more
research has investigated how they could support more effective communication
in various contexts to improve collaboration and social connectedness. However,
there was no literature to summarize the uniqueness VR provided and put forward
guidance for designing social VR applications for better communication. To
understand how VR has been designed and used to facilitate communication in
different contexts, we conducted a systematic review of the studies
investigating communication in social VR in the past ten years by following the
PRISMA guidelines. We highlight current practices and challenges and identify
research opportunities to improve the design of social VR to better support
communication and make social VR more accessible.","['Xiaoying Wei', 'Xiaofu Jin', 'Mingming Fan']",2022-10-04T04:10:54Z,http://arxiv.org/abs/2210.01365v1,['cs.HC'],"immersive social virtual reality,systematic review,communication,virtual reality technologies,collaboration,social connectedness,social VR applications,PRISMA guidelines,research opportunities"
"Learning Depth Vision-Based Personalized Robot Navigation From Dynamic
  Demonstrations in Virtual Reality","For the best human-robot interaction experience, the robot's navigation
policy should take into account personal preferences of the user. In this
paper, we present a learning framework complemented by a perception pipeline to
train a depth vision-based, personalized navigation controller from user
demonstrations. Our virtual reality interface enables the demonstration of
robot navigation trajectories under motion of the user for dynamic interaction
scenarios. The novel perception pipeline enrolls a variational autoencoder in
combination with a motion predictor. It compresses the perceived depth images
to a latent state representation to enable efficient reasoning of the learning
agent about the robot's dynamic environment. In a detailed analysis and
ablation study, we evaluate different configurations of the perception
pipeline. To further quantify the navigation controller's quality of
personalization, we develop and apply a novel metric to measure preference
reflection based on the Fr\'echet Distance. We discuss the robot's navigation
performance in various virtual scenes and demonstrate the first personalized
robot navigation controller that solely relies on depth images. A supplemental
video highlighting our approach is available online.","['Jorge de Heuvel', 'Nathan Corral', 'Benedikt Kreis', 'Jacobus Conradi', 'Anne Driemel', 'Maren Bennewitz']",2022-10-04T15:30:16Z,http://arxiv.org/abs/2210.01683v3,['cs.RO'],"personalized robot navigation,depth vision,virtual reality,perception pipeline,variational autoencoder,motion predictor,learning agent,preference reflection,Fr\'echet Distance"
"VR-SFT: Reproducing Swinging Flashlight Test in Virtual Reality to
  Detect Relative Afferent Pupillary Defect","The relative afferent asymmetry between two eyes can be diagnosed using
swinging flashlight test, also known as the alternating light test. This
remains one of the most used clinical tests to this day. Despite the swinging
flashlight test's straightforward approach, a number of factors can add
variability into the clinical methodology and reduce the measurement's validity
and reliability. This includes small and poorly responsive pupils, dark iris,
anisocoria, uneven illumination in both eyes. Due to these limitations, the
true condition of relative afferent asymmetry may create confusion and various
observers may quantify the relative afferent pupillary defect differently.
Consequently, the results of the swinging flashlight test are subjective and
ambiguous. In order to eliminate the limitations of traditional swinging
flashlight test and introduce objectivity, we propose a novel approach to the
swinging flashlight exam, VR-SFT, by making use of virtual reality (VR). We
suggest that the clinical records of the subjects and the results of VR-SFT are
comparable. In this paper, we describe how we exploit the features of immersive
VR experience to create a reliable and objective swinging flashlight test.","['Prithul Sarker', 'Nasif Zaman', 'Alireza Tavakkoli']",2022-10-12T00:06:14Z,http://arxiv.org/abs/2210.06474v1,"['cs.HC', 'cs.AI', 'cs.CV']","virtual reality,VR-SFT,swinging flashlight test,relative afferent pupillary defect,clinical test,afferent asymmetry,validity,reliability,methodology,subjective"
"GazeBaseVR, a large-scale, longitudinal, binocular eye-tracking dataset
  collected in virtual reality","We present GazeBaseVR, a large-scale, longitudinal, binocular eye-tracking
(ET) dataset collected at 250 Hz with an ET-enabled virtual-reality (VR)
headset. GazeBaseVR comprises 5,020 binocular recordings from a diverse
population of 407 college-aged participants. Participants were recorded up to
six times each over a 26-month period, each time performing a series of five
different ET tasks: (1) a vergence task, (2) a horizontal smooth pursuit task,
(3) a video-viewing task, (4) a self-paced reading task, and (5) a random
oblique saccade task. Many of these participants have also been recorded for
two previously published datasets with different ET devices, and some
participants were recorded before and after COVID-19 infection and recovery.
GazeBaseVR is suitable for a wide range of research on ET data in VR devices,
especially eye movement biometrics due to its large population and longitudinal
nature. In addition to ET data, additional participant details are provided to
enable further research on topics such as fairness.","['Dillon Lohr', 'Samantha Aziz', 'Lee Friedman', 'Oleg V Komogortsev']",2022-10-14T05:29:03Z,http://arxiv.org/abs/2210.07533v1,['cs.HC'],"GazeBaseVR,eye-tracking,dataset,virtual reality,longitudinal,binocular,population,tasks,biometrics,COVID-19"
"Using Virtual Reality to Simulate Human-Robot Emergency Evacuation
  Scenarios","This paper describes our recent effort to use virtual reality to simulate
threatening emergency evacuation scenarios in which a robot guides a person to
an exit. Our prior work has demonstrated that people will follow a robot's
guidance, even when the robot is faulty, during an emergency evacuation. Yet,
because physical in-person emergency evacuation experiments are difficult and
costly to conduct and because we would like to evaluate many different factors,
we are motivated to develop a system that immerses people in the simulation
environment to encourage genuine subject reactions. We are working to complete
experiments verifying the validity of our approach.","['Alan R. Wagner', 'Colin Holbrook', 'Daniel Holman', 'Brett Sheeran', 'Vidullan Surendran', 'Jared Armagost', 'Savanna Spazak', 'Yinxuan Yin']",2022-10-16T02:29:30Z,http://arxiv.org/abs/2210.08414v1,"['cs.RO', 'cs.AI', 'cs.HC']","Virtual reality,Human-robot interaction,Emergency evacuation,Simulation,Robot guidance"
"Hierarchical Reinforcement Learning for Furniture Layout in Virtual
  Indoor Scenes","In real life, the decoration of 3D indoor scenes through designing furniture
layout provides a rich experience for people. In this paper, we explore the
furniture layout task as a Markov decision process (MDP) in virtual reality,
which is solved by hierarchical reinforcement learning (HRL). The goal is to
produce a proper two-furniture layout in the virtual reality of the indoor
scenes. In particular, we first design a simulation environment and introduce
the HRL formulation for a two-furniture layout. We then apply a hierarchical
actor-critic algorithm with curriculum learning to solve the MDP. We conduct
our experiments on a large-scale real-world interior layout dataset that
contains industrial designs from professional designers. Our numerical results
demonstrate that the proposed model yields higher-quality layouts as compared
with the state-of-art models.","['Xinhan Di', 'Pengqian Yu']",2022-10-19T09:58:10Z,http://arxiv.org/abs/2210.10431v1,"['cs.CV', 'cs.AI']","Hierarchical Reinforcement Learning,Furniture Layout,Virtual Reality,Markov Decision Process,Simulation Environment,Actor-Critic Algorithm,Curriculum Learning,Interior Layout Dataset,Industrial Designs"
Facial De-occlusion Network for Virtual Telepresence Systems,"To see what is not in the image is one of the broader missions of computer
vision. Technology to inpaint images has made significant progress with the
coming of deep learning. This paper proposes a method to tackle occlusion
specific to human faces. Virtual presence is a promising direction in
communication and recreation for the future. However, Virtual Reality (VR)
headsets occlude a significant portion of the face, hindering the
photo-realistic appearance of the face in the virtual world. State-of-the-art
image inpainting methods for de-occluding the eye region does not give usable
results. To this end, we propose a working solution that gives usable results
to tackle this problem enabling the use of the real-time photo-realistic
de-occluded face of the user in VR settings.","['Surabhi Gupta', 'Ashwath Shetty', 'Avinash Sharma']",2022-10-23T05:34:17Z,http://arxiv.org/abs/2210.12622v1,['cs.CV'],"computer vision,inpainting,deep learning,occlusion,Virtual Reality,VR headsets,image inpainting,photo-realistic,de-occlusion,real-time"
"An Image-Space Split-Rendering Approach to Accelerate Low-Powered
  Virtual Reality","Virtual Reality systems provide many opportunities for scientific research
and consumer enjoyment; however, they are more demanding than traditional
desktop applications and require a wired connection to desktops in order to
enjoy maximum quality. Standalone options that are not connected to computers
exist, yet they are powered by mobile GPUs, which provide limited power in
comparison to desktop rendering. Alternative approaches to improve performance
on mobile devices use server rendering to render frames for a client and treat
the client largely as a display device. However, current streaming solutions
largely suffer from high end-to-end latency due to processing and networking
requirements, as well as underutilization of the client. We propose a networked
split-rendering approach to achieve faster end-to-end image presentation rates
on the mobile device while preserving image quality. Our proposed solution uses
an image-space division of labour between the server-side GPU and the mobile
client, and achieves a significantly faster runtime than client-only rendering
and than using a thin-client approach, which is mostly reliant on the server.","['Ville Cantory', 'Nathan Ringo']",2022-11-04T15:44:53Z,http://arxiv.org/abs/2211.02529v2,['cs.GR'],"Virtual Reality,Low-Powered,Split-Rendering Approach,Mobile GPUs,Server Rendering,End-to-End Latency,Image Quality,Client,Networked Split-Rendering"
"Virtual Reality in University Teaching: Experiences from a Computer
  Science Seminar","Due to the corona pandemic, numerous courses were held using digital
solutions in order to be able to continue teaching. Conventional collaboration
tools (Zoom, Big Blue Button, etc.) were used in particular to digitally map a
synchronous session for teaching and learning purposes. While these
conventional collaboration tools offer a solid basis for communication between
learners and teachers, aspects such as presence or a realistic type of
interaction are neglected. In this work, we report on the experiences from a
computer science seminar where virtual reality (VR) technology was used as an
alternative solution for teaching and group work. The benefits of VR compared
to conventional collaboration tools were examined using questionnaires and
interviews with the participants. On the one hand, the results show the high
potential of VR to increase the clarity and experienceability of learning
content and to promote cooperation through social presence. On the other hand,
the use of VR brings with it some technical and organizational difficulties
that should be taken into account in the didactic implementation.",['Enes Yigitbas'],2022-11-22T12:28:18Z,http://arxiv.org/abs/2211.12221v1,['cs.HC'],"Virtual reality,University teaching,Computer science seminar,Digital solutions,Collaboration tools,Synchronous session,Presence,Interaction,Group work,Didactic implementation."
"Multi-Finger Haptics: Analysis of Human Hand Grasp towards a Tripod
  Three-Finger Haptic Grasp model","Grasping is an incredible ability of animals using their arms and limbs in
their daily life. The human hand is an especially astonishing multi-fingered
tool for precise grasping, which helped humans to develop the modern world. The
implementation of the human grasp to virtual reality and telerobotics is always
interesting and challenging at the same time. In this work, authors surveyed,
studied, and analyzed the human hand-grasping behavior for the possibilities of
haptic grasping in the virtual and remote environment. This work is focused on
the motion and force analysis of fingers in human hand grasping scenarios and
the paper describes the transition of the human hand grasping towards a tripod
haptic grasp model for effective interaction in virtual reality.",['Jose James'],2022-12-30T20:37:16Z,http://arxiv.org/abs/2301.00049v1,"['cs.RO', 'cs.HC']","multi-finger haptics,human hand grasp,tripod,haptic grasp model,virtual reality,telerobotics,motion analysis,force analysis,human hand grasping behavior"
"Virtual reality for the analysis and visualization of scientific
  numerical models","The complexity of the data generated by (magneto)-hydrodynamic (HD/MHD)
simulations requires advanced tools for their analysis and visualization. The
dramatic improvements in virtual reality (VR) technologies have inspired us to
seek the long-term goal of creating VR tools for scientific model analysis and
visualization that would allow researchers to study and perform data analysis
on their models within an immersive environment. Here, we report the results
obtained at INAF-Osservatorio Astronomico di Palermo in the development of
these tools, which would allow for the exploration of 3D models interactively,
resulting in highly detailed analysis that cannot be performed with traditional
data visualization and analysis platforms. Additionally, these VR-based tools
offer the ability to produce high-impact VR content for efficient audience
engagement and awareness.","['S. Orlando', 'M. Miceli', 'U. Lo Cicero', 'S. Ustamujic']",2023-01-26T08:42:03Z,http://arxiv.org/abs/2301.11334v1,['cs.HC'],"virtual reality,analysis,visualization,scientific models,numerical models,(magneto)-hydrodynamic simulations,immersive environment,3D models,data analysis"
"Never Skip Leg Day Again: Training the Lower Body with Vertical Jumps in
  a Virtual Reality Exergame","Virtual Reality (VR) exergames can increase engagement in and motivation for
physical activities. Most VR exergames focus on the upper body because many VR
setups only track the users' heads and hands. To become a serious alternative
to existing exercise programs, VR exergames must provide a balanced workout and
train the lower limbs, too. To address this issue, we built a VR exergame
focused on vertical jump training to explore full-body exercise applications.
To create a safe and effective training, nine domain experts participated in
our prototype design. Our mixed-methods study confirms that the jump-centered
exercises provided a worthy challenge and positive player experience,
indicating long-term retention. Based on our findings, we present five design
implications to guide future work: avoid an unintended forward drift, consider
technical constraints, address safety concerns in full-body VR exergames,
incorporate rhythmic elements with fluent movement patterns, adapt difficulty
to players' fitness progression status.","['Sebastian Cmentowski', 'Sukran Karaosmanoglu', 'Lennart Nacke', 'Frank Steinicke', 'Jens Krüger']",2023-02-06T14:25:44Z,http://arxiv.org/abs/2302.02803v4,['cs.HC'],"Virtual Reality,Exergames,Lower Body Training,Vertical Jumps,Full-body Exercise,Prototype Design,Design Implications,Safety Concerns,Fitness Progression,Fluent Movement Patterns"
"FingerMapper: Mapping Finger Motions onto Virtual Arms to Enable Safe
  Virtual Reality Interaction in Confined Spaces","Whole-body movements enhance the presence and enjoyment of Virtual Reality
(VR) experiences. However, using large gestures is often uncomfortable and
impossible in confined spaces (e.g., public transport). We introduce
FingerMapper, mapping small-scale finger motions onto virtual arms and hands to
enable whole-body virtual movements in VR. In a first target selection study
(n=13) comparing FingerMapper to hand tracking and ray-casting, we found that
FingerMapper can significantly reduce physical motions and fatigue while having
a similar degree of precision. In a consecutive study (n=13), we compared
FingerMapper to hand tracking inside a confined space (the front passenger seat
of a car). The results showed participants had significantly higher perceived
safety and fewer collisions with FingerMapper while preserving a similar degree
of presence and enjoyment as hand tracking. Finally, we present three example
applications demonstrating how FingerMapper could be applied for locomotion and
interaction for VR in confined spaces.","['Wen-Jie Tseng', 'Samuel Huron', 'Eric Lecolinet', 'Jan Gugenheimer']",2023-02-23T09:11:38Z,http://arxiv.org/abs/2302.11865v1,['cs.HC'],"Virtual Reality,FingerMapper,finger motions,virtual arms,confined spaces,hand tracking,presence,enjoyment,locomotion,interaction"
"Bridging the Generational Gap: Exploring How Virtual Reality Supports
  Remote Communication Between Grandparents and Grandchildren","When living apart, grandparents and grandchildren often use audio-visual
communication approaches to stay connected. However, these approaches seldom
provide sufficient companionship and intimacy due to a lack of co-presence and
spatial interaction, which can be fulfilled by immersive virtual reality (VR).
To understand how grandparents and grandchildren might leverage VR to
facilitate their remote communication and better inform future design, we
conducted a user-centered participatory design study with twelve pairs of
grandparents and grandchildren. Results show that VR affords casual and equal
communication by reducing the generational gap, and promotes conversation by
offering shared activities as bridges for connection. Participants preferred
resemblant appearances on avatars for conveying well-being but created ideal
selves for gaining playfulness. Based on the results, we contribute eight
design implications that inform future VR-based grandparent-grandchild
communications.","['Xiaoying Wei', 'Yizheng Gu', 'Emily Kuang', 'Xian Wang', 'Beiyan Cao', 'Xiaofu Jin', 'Mingming Fan']",2023-02-28T16:30:45Z,http://arxiv.org/abs/2302.14717v1,['cs.HC'],"virtual reality,remote communication,grandparents,grandchildren,immersive,user-centered design,avatars,communication,design implications"
"A Large-Scale Study of Personal Identifiability of Virtual Reality
  Motion Over Time","In recent years, social virtual reality (VR), sometimes described as the
""metaverse,"" has become widely available. With its potential comes risks,
including risks to privacy. To understand these risks, we study the
identifiability of participants' motion in VR in a dataset of 232 VR users with
eight weekly sessions of about thirty minutes each, totaling 764 hours of
social interaction. The sample is unique as we are able to study the effect of
user, session, and time independently. We find that the number of sessions
recorded greatly increases identifiability, and duration per session increases
identifiability as well, but to a lesser degree. We also find that greater
delay between training and testing sessions reduces identifiability.
Ultimately, understanding the identifiability of VR activities will help
designers, security professionals, and consumer advocates make VR safer.","['Mark Roman Miller', 'Eugy Han', 'Cyan DeVeaux', 'Eliot Jones', 'Ryan Chen', 'Jeremy N. Bailenson']",2023-03-02T17:34:58Z,http://arxiv.org/abs/2303.01430v1,['cs.CR'],"virtual reality,personal identifiability,motion,social interaction,dataset,privacy,user,session,time,identifiability"
"Virtual Reality in Metaverse over Wireless Networks with User-centered
  Deep Reinforcement Learning","The Metaverse and its promises are fast becoming reality as maturing
technologies are empowering the different facets. One of the highlights of the
Metaverse is that it offers the possibility for highly immersive and
interactive socialization. Virtual reality (VR) technologies are the backbone
for the virtual universe within the Metaverse as they enable a hyper-realistic
and immersive experience, and especially so in the context of socialization. As
the virtual world 3D scenes to be rendered are of high resolution and frame
rate, these scenes will be offloaded to an edge server for computation.
Besides, the metaverse is user-center by design, and human users are always the
core. In this work, we introduce a multi-user VR computation offloading over
wireless communication scenario. In addition, we devised a novel user-centered
deep reinforcement learning approach to find a near-optimal solution. Extensive
experiments demonstrate that our approach can lead to remarkable results under
various requirements and constraints.","['Wenhan Yu', 'Terence Jie Chua', 'Jun Zhao']",2023-03-08T03:10:41Z,http://arxiv.org/abs/2303.04349v1,"['cs.NI', 'cs.AI']","Virtual Reality,Metaverse,Wireless Networks,Deep Reinforcement Learning,User-centered,Edge Server,Multi-user,Computation Offloading,Immersive Experience,Socialization"
"Towards Driving Policies with Personality: Modeling Behavior and Style
  in Risky Scenarios via Data Collection in Virtual Reality","Autonomous driving research currently faces data sparsity in representation
of risky scenarios. Such data is both difficult to obtain ethically in the real
world, and unreliable to obtain via simulation. Recent advances in virtual
reality (VR) driving simulators lower barriers to tackling this problem in
simulation. We propose the first data collection framework for risky scenario
driving data from real humans using VR, as well as accompanying numerical
driving personality characterizations. We validate the resulting dataset with
statistical analyses and model driving behavior with an eight-factor
personality vector based on the Multi-dimensional Driving Style Inventory
(MDSI). Our method, dataset, and analyses show that realistic driving
personalities can be modeled without deep learning or large datasets to
complement autonomous driving research.","['Laura Zheng', 'Julio Poveda', 'James Mullen', 'Shreelekha Revankar', 'Ming C. Lin']",2023-03-08T21:38:24Z,http://arxiv.org/abs/2303.04901v1,"['cs.RO', 'cs.HC']","data collection,virtual reality,risky scenarios,driving behavior,personality modeling,driving simulators,driving personality characterizations,Multi-dimensional Driving Style Inventory (MDSI),autonomous driving"
CAstelet in Virtual reality for shadOw AVatars (CAVOAV),"After an overview of the use of digital shadows in computing science research
projects with cultural and social impacts and a focus on recent researches and
insights on virtual theaters, this paper introduces a research mixing the
manipulation of shadow avatars and the building of a virtual theater setup
inspired by traditional shadow theater (or ``castelet'' in french) in a mixed
reality environment. It describes the virtual 3D setup, the nature of the
shadow avatars and the issues of directing believable interactions between
virtual avatars and physical performers on stage. Two modalities of shadow
avatars direction are exposed. Some results of the research are illustrated in
two use cases: the development of theatrical creativity in mixed reality
through pedagogical workshops; and an artistic achievement in ''The Shadow''
performance, after H. C. Andersen.","['Georges Gagneré', 'Anastasiia Ternova']",2023-03-13T10:31:09Z,http://arxiv.org/abs/2303.06981v1,['cs.GR'],"digital shadows,virtual theaters,shadow avatars,virtual reality,mixed reality environment,shadow theater,3D setup,interactions,shadow avatars direction"
"Digital twin in virtual reality for human-vehicle interactions in the
  context of autonomous driving","This paper presents the results of tests of interactions between real humans
and simulated vehicles in a virtual scenario. Human activity is inserted into
the virtual world via a virtual reality interface for pedestrians. The
autonomous vehicle is equipped with a virtual Human-Machine interface (HMI) and
drives through the digital twin of a real crosswalk. The HMI was combined with
gentle and aggressive braking maneuvers when the pedestrian intended to cross.
The results of the interactions were obtained through questionnaires and
measurable variables such as the distance to the vehicle when the pedestrian
initiated the crossing action. The questionnaires show that pedestrians feel
safer whenever HMI is activated and that varying the braking maneuver does not
influence their perception of danger as much, while the measurable variables
show that both HMI activation and the gentle braking maneuver cause the
pedestrian to cross earlier.","['Sergio Martín Serrano', 'Rubén Izquierdo', 'Iván García Daza', 'Miguel Ángel Sotelo', 'David Fernández Llorca']",2023-03-20T21:45:16Z,http://arxiv.org/abs/2303.11463v3,['cs.RO'],"Digital twin,virtual reality,human-vehicle interactions,autonomous driving,virtual scenario,Human-Machine interface (HMI),braking maneuvers,crosswalk,pedestrian perception"
"Towards a Virtual Reality Visualization of Hand-Object Interactions to
  Support Remote Physical Therapy","Improving object manipulation skills through hand-object interaction
exercises is crucial for rehabilitation. Despite limited healthcare resources,
physical therapists propose remote exercise routines followed up by remote
monitoring. However, remote motor skills assessment remains challenging due to
the lack of effective motion visualizations. Therefore, exploring innovative
ways of visualization is crucial, and virtual reality (VR) has shown the
potential to address this limitation. However, it is unclear how VR
visualization can represent understandable hand-object interactions. To address
this gap, in this paper, we present VRMoVi, a VR visualization system that
incorporates multiple levels of 3D visualization layers to depict movements. In
a 2-stage study, we showed VRMoVi's potential in representing hand-object
interactions, with its visualization outperforming traditional representations,
and detailed features improved the hand-object interactions understanding. This
study takes the initial step in developing VR visualization of hand-object
interaction to support remote physical therapy.","['Trudi Di Qi', 'LouAnne Boyd', 'Scott Fitzpatrick', 'Meghna Raswan', 'Farnceli Cibrian']",2023-03-22T21:34:18Z,http://arxiv.org/abs/2303.12920v2,"['cs.HC', 'cs.GR', 'H.5.0']","Virtual reality,Visualization,Hand-object interactions,Remote physical therapy,Rehabilitation,Motion visualizations,3D visualization,VRMoVi,Remote monitoring"
"Tangible Web: An Interactive Immersion Virtual RealityCreativity System
  that Travels Across Reality","With the advancement of virtual reality (VR) technology, virtual displays
have become integral to how museums, galleries, and other tourist destinations
present their collections to the public. However, the current lack of immersion
in virtual reality displays limits the user's ability to experience and
appreciate its aesthetics. This paper presents a case study of a creative
approach taken by a tourist attraction venue in developing a physical network
system that allows visitors to enhance VR's aesthetic aspects based on
environmental parameters gathered by external sensors. Our system was
collaboratively developed through interviews and sessions with twelve
stakeholder groups interested in art and exhibitions. This paper demonstrates
how our technological advancements in interaction, immersion, and visual
attractiveness surpass those of earlier virtual display generations. Through
multimodal interaction, we aim to encourage innovation on the Web and create
more visually appealing and engaging virtual displays. It is hoped that the
greater online art community will gain fresh insight into how people interact
with virtual worlds as a result of this work.","['Simin Yang', 'Ze Gao', 'Reza Hadi Mogavi', 'Pan Hui', 'Tristan Braud']",2023-04-05T07:37:55Z,http://arxiv.org/abs/2304.02274v1,"['cs.HC', 'cs.MM']","virtual reality,creativity system,immersion,virtual displays,aesthetics,environmental parameters,sensors,interaction,visual attractiveness,multimodal interaction"
The future of hearing aid technology,"Background. Hearing aid technology has proven successful in the
rehabilitation of hearing loss, but its performance is still limited in
difficult everyday conditions characterized by noise and reverberation.
  Objectives. Introduction to the current state of hearing aid technology and
presentation of the current state of research and future development.
  Methods. Current literature is analyzed and several specific new developments
are presented.
  Results. Both objective and subjective data from empirical studies show the
limitation of current technology. Examples of current research show the
potential of machine-learning based algorithms and multi-modal signal
processing for improving speech processing and perception, of using virtual
reality for improving hearing device fitting and of mobile health technology
for improving hearing-health services.
  Conclusions. Hearing device technology will remain a key factor in the
rehabilitation of hearing impairment. New technology such as machine learning,
and multi-modal signal processing, virtual reality and mobile health technology
will improve speech enhancement, individual fitting and communication training.",['Volker Hohmann'],2023-04-13T19:11:52Z,http://arxiv.org/abs/2304.06786v2,"['eess.AS', 'cs.SD']","hearing aid technology,rehabilitation,noise,reverberation,machine learning,multi-modal signal processing,virtual reality,mobile health technology,speech processing,perception"
"A Virtual Reality Framework for Human-Robot Collaboration in Cloth
  Folding","We present a virtual reality (VR) framework to automate the data collection
process in cloth folding tasks. The framework uses skeleton representations to
help the user define the folding plans for different classes of garments,
allowing for replicating the folding on unseen items of the same class. We
evaluate the framework in the context of automating garment folding tasks. A
quantitative analysis is performed on 3 classes of garments, demonstrating that
the framework reduces the need for intervention by the user. We also compare
skeleton representations with RGB and binary images in a classification task on
a large dataset of clothing items, motivating the use of the framework for
other classes of garments.","['Marco Moletta', 'Maciej K. Wozniak', 'Michael C. Welle', 'Danica Kragic']",2023-05-12T14:08:46Z,http://arxiv.org/abs/2305.07493v2,['cs.RO'],"virtual reality,human-robot collaboration,cloth folding,framework,data collection,skeleton representations,garment folding,classification task,RGB images,binary images"
"A Virtual Reality Teleoperation Interface for Industrial Robot
  Manipulators","We address the problem of teleoperating an industrial robot manipulator via a
commercially available Virtual Reality (VR) interface. Previous works on VR
teleoperation for robot manipulators focus primarily on collaborative or
research robot platforms (whose dynamics and constraints differ from industrial
robot arms), or only address tasks where the robot's dynamics are not as
important (e.g: pick and place tasks). We investigate the usage of commercially
available VR interfaces for effectively teleoeprating industrial robot
manipulators in a variety of contact-rich manipulation tasks. We find that
applying standard practices for VR control of robot arms is challenging for
industrial platforms because torque and velocity control is not exposed, and
position control is mediated through a black-box controller. To mitigate these
problems, we propose a simplified filtering approach to process command signals
to enable operators to effectively teleoperate industrial robot arms with VR
interfaces in dexterous manipulation tasks. We hope our findings will help
robot practitioners implement and setup effective VR teleoperation interfaces
for robot manipulators. The proposed method is demonstrated on a variety of
contact-rich manipulation tasks which can also involve very precise movement of
the robot during execution (videos can be found at
https://www.youtube.com/watch?v=OhkCB9mOaBc)","['Eric Rosen', 'Devesh K. Jha']",2023-05-18T13:26:23Z,http://arxiv.org/abs/2305.10960v1,"['cs.RO', 'cs.AI']","Virtual Reality,Teleoperation,Industrial Robot,Manipulators,VR interface,Robot dynamics,Contact-rich manipulation tasks,Torque control,Velocity control"
Intuitive Robot Integration via Virtual Reality Workspaces,"As robots become increasingly prominent in diverse industrial settings, the
desire for an accessible and reliable system has correspondingly increased.
Yet, the task of meaningfully assessing the feasibility of introducing a new
robotic component, or adding more robots into an existing infrastructure,
remains a challenge. This is due to both the logistics of acquiring a robot and
the need for expert knowledge in setting it up. In this paper, we address these
concerns by developing a purely virtual simulation of a robotic system. Our
proposed framework enables natural human-robot interaction through a visually
immersive representation of the workspace. The main advantages of our approach
are the following: (i) independence from a physical system, (ii) flexibility in
defining the workspace and robotic tasks, and (iii) an intuitive interaction
between the operator and the simulated environment. Not only does our system
provide an enhanced understanding of 3D space to the operator, but it also
encourages a hands-on way to perform robot programming. We evaluate the
effectiveness of our method in applying novel automation assignments by
training a robot in virtual reality and then executing the task on a real
robot.","['Minh Q. Tram', 'Joseph M. Cloud', 'William J. Beksi']",2023-05-25T02:06:24Z,http://arxiv.org/abs/2305.15657v1,['cs.RO'],"virtual reality,robot integration,human-robot interaction,simulation,workspace,robotic system,automation,robot programming,3D space,virtual simulation"
"Happily Error After: Framework Development and User Study for Correcting
  Robot Perception Errors in Virtual Reality","While we can see robots in more areas of our lives, they still make errors.
One common cause of failure stems from the robot perception module when
detecting objects. Allowing users to correct such errors can help improve the
interaction and prevent the same errors in the future. Consequently, we
investigate the effectiveness of a virtual reality (VR) framework for
correcting perception errors of a Franka Panda robot. We conducted a user study
with 56 participants who interacted with the robot using both VR and screen
interfaces. Participants learned to collaborate with the robot faster in the VR
interface compared to the screen interface. Additionally, participants found
the VR interface more immersive, enjoyable, and expressed a preference for
using it again. These findings suggest that VR interfaces may offer advantages
over screen interfaces for human-robot interaction in erroneous environments.","['Maciej K. Wozniak', 'Rebecca Stower', 'Patric Jensfelt', 'Andre Pereira']",2023-06-26T10:55:17Z,http://arxiv.org/abs/2306.14589v1,"['cs.RO', 'cs.HC']","Framework Development,User Study,Robot Perception,Virtual Reality,Interaction,Errors,Franka Panda,Immersive,Human-Robot"
Visualization of AI Systems in Virtual Reality: A Comprehensive Review,"This study provides a comprehensive review of the utilization of Virtual
Reality (VR) for visualizing Artificial Intelligence (AI) systems, drawing on
18 selected studies. The results illuminate a complex interplay of tools,
methods, and approaches, notably the prominence of VR engines like Unreal
Engine and Unity. However, despite these tools, a universal solution for
effective AI visualization remains elusive, reflecting the unique strengths and
limitations of each technique. We observed the application of VR for AI
visualization across multiple domains, despite challenges such as high data
complexity and cognitive load. Moreover, it briefly discusses the emerging
ethical considerations pertaining to the broad integration of these
technologies. Despite these challenges, the field shows significant potential,
emphasizing the need for dedicated research efforts to unlock the full
potential of these immersive technologies. This review, therefore, outlines a
roadmap for future research, encouraging innovation in visualization
techniques, addressing identified challenges, and considering the ethical
implications of VR and AI convergence.","['Medet Inkarbekov', 'Rosemary Monahan', 'Barak A. Pearlmutter']",2023-06-27T15:15:38Z,http://arxiv.org/abs/2306.15545v1,['cs.HC'],"Virtual Reality,Artificial Intelligence,Visualization,VR engines,Unreal Engine,Unity,data complexity,cognitive load,ethical considerations,immersive technologies"
Generating Animatable 3D Cartoon Faces from Single Portraits,"With the booming of virtual reality (VR) technology, there is a growing need
for customized 3D avatars. However, traditional methods for 3D avatar modeling
are either time-consuming or fail to retain similarity to the person being
modeled. We present a novel framework to generate animatable 3D cartoon faces
from a single portrait image. We first transfer an input real-world portrait to
a stylized cartoon image with a StyleGAN. Then we propose a two-stage
reconstruction method to recover the 3D cartoon face with detailed texture,
which first makes a coarse estimation based on template models, and then
refines the model by non-rigid deformation under landmark supervision. Finally,
we propose a semantic preserving face rigging method based on manually created
templates and deformation transfer. Compared with prior arts, qualitative and
quantitative results show that our method achieves better accuracy, aesthetics,
and similarity criteria. Furthermore, we demonstrate the capability of
real-time facial animation of our 3D model.","['Chuanyu Pan', 'Guowei Yang', 'Taijiang Mu', 'Yu-Kun Lai']",2023-07-04T04:12:50Z,http://arxiv.org/abs/2307.01468v1,['cs.CV'],"virtual reality,3D avatars,StyleGAN,3D cartoon faces,landmark supervision,non-rigid deformation,face rigging,deformation transfer,facial animation"
"A Case for VR Briefings: Comparing Communication in Daily Audio and VR
  Mission Control in a Simulated Lunar Mission","Alpha-XR Mission conducted by XR Lab PJAIT focused on research related to
individual and crew well-being and participatory team collaboration in ICE
(isolated, confined and extreme) conditions. In this two-week mission within an
analog space habitat, collaboration, objective execution and leisure was
facilitated and studied by virtual reality (VR) tools. The mission commander
and first officer, both experienced with virtual reality, took part in daily
briefings with mission control. In the first week the briefings were voice-only
conducted via a channel on Discord. During the following week last briefings
were conducted in VR, using Horizon Workrooms. This qualitative pilot study
employing participatory observation revealed that VR facilitates communication,
especially on complex problems and experiences, providing the sense of
emotional connection and shared understanding, that may be lacking in audio
calls. The study points to the need to further explore VR-facilitated
communication in high-stake environments as it may improve relationships,
well-being, and communication outcomes.","['Kinga Skorupska', 'Maciej Grzeszczuk', 'Anna Jaskulska', 'Monika Kornacka', 'Grzegorz Pochwatko', 'Wiesław KopeāE]",2023-07-17T15:59:11Z,http://arxiv.org/abs/2307.08589v1,"['cs.HC', 'H.5.1']","VR briefings,Communication,Virtual reality,Mission control,Participatory observation,ICE conditions,Horizon Workrooms,Analog space habitat,Emotional connection,High-stake environments"
"Assessing the Effects of Illuminance and Correlated Color Temperature on
  Emotional Responses and Lighting Preferences Using Virtual Reality","This paper presents a novel approach to assessing human lighting adjustment
behavior and preference in diverse lighting conditions through the evaluation
of emotional feedback and behavioral data using VR. Participants (n= 27) were
exposed to different lighting (n=17) conditions with different levels of
illuminance and correlated color temperature (CCT) with a randomized order in a
virtual office environment. Results from this study significantly advanced our
understanding of preferred lighting conditions in virtual reality environments,
influenced by a variety of factors such as illuminance, color temperature,
order of presentation, and participant demographics. Through a comprehensive
analysis of user adjustment profiles, we obtained insightful data that can
guide the optimization of lighting design across various settings.","['Armin Mostafavi', 'Tong Bill Xu', 'Saleh Kalantari']",2023-07-20T15:54:41Z,http://arxiv.org/abs/2307.10969v1,['cs.HC'],"illuminance,correlated color temperature,emotional responses,lighting preferences,virtual reality,lighting conditions,behavioral data,user adjustment profiles,lighting design,virtual office environment"
Virtual Reality Based Robot Teleoperation via Human-Scene Interaction,"Robot teleoperation gains great success in various situations, including
chemical pollution rescue, disaster relief, and long-distance manipulation. In
this article, we propose a virtual reality (VR) based robot teleoperation
system to achieve more efficient and natural interaction with humans in
different scenes. A user-friendly VR interface is designed to help users
interact with a desktop scene using their hands efficiently and intuitively. To
improve user experience and reduce workload, we simulate the process in the
physics engine to help build a preview of the scene after manipulation in the
virtual scene before execution. We conduct experiments with different users and
compare our system with a direct control method across several teleoperation
tasks. The user study demonstrates that the proposed system enables users to
perform operations more instinctively with a lighter mental workload. Users can
perform pick-and-place and object-stacking tasks in a considerably short time,
even for beginners. Our code is available at
https://github.com/lingxiaomeng/VR_Teleoperation_Gen3.","['Lingxiao Meng', 'Jiangshan Liu', 'Wei Chai', 'Jiankun Wang', 'Max Q. -H. Meng']",2023-08-02T14:08:10Z,http://arxiv.org/abs/2308.01164v1,['cs.RO'],"robot teleoperation,virtual reality,human-scene interaction,user-friendly interface,physics engine,user study,pick-and-place,object-stacking,teleoperation tasks,code availability"
"BalanceVR: Balance Training to Increase Tolerance to Cybersickness in
  Immersive Virtual Reality","Cybersickness is a serious usability problem in virtual reality. Postural (or
balance) instability theory has emerged as one of the major hypotheses for the
cause of cybersickness. In this paper, we conducted a two-week-long experiment
to observe the trends in user balance learning and sickness tolerance under
different experimental conditions to analyze the potential inter-relationship
between them. The experimental results have shown, aside from the obvious
improvement in balance performance itself, that accompanying balance training
had a stronger effect of increasing tolerance to cybersickness than mere
exposure to VR. In addition, training in immersive VR was found to be more
effective than using the 2D-based non-immersive medium, especially for the
transfer effect to other non-training VR content.","['Seonghoon Kang', 'Yechan Yang', 'Gerard Jounghyun Kim', 'Hanseob Kim']",2023-08-10T01:27:53Z,http://arxiv.org/abs/2308.05276v3,['cs.HC'],"Balance training,Tolerance,Cybersickness,Immersive Virtual Reality,Postural instability theory,User balance learning,Sickness tolerance,Experimental conditions,Balance performance,VR content"
"Open Medical Gesture: An Open-Source Experiment in Naturalistic Physical
  Interactions for Mixed and Virtual Reality Simulations","Mixed Reality (MR) and Virtual Reality (VR) simulations are hampered by
requirements for hand controllers or attempts to perseverate in use of
two-dimensional computer interface paradigms from the 1980s. From our efforts
to produce more naturalistic interactions for combat medic training for the
military, USC has developed an open-source toolkit that enables direct hand
controlled responsive interactions that is sensor independent and can function
with depth sensing cameras, webcams or sensory gloves. Natural approaches we
have examined include the ability to manipulate virtual smart objects in a
similar manner to how they are used in the real world. From this research and
review of current literature, we have discerned several best approaches for
hand-based human computer interactions which provide intuitive, responsive,
useful, and low frustration experiences for VR users.","['Thomas B Talbot', 'Chinmay Chinara']",2023-08-14T21:56:41Z,http://arxiv.org/abs/2308.07472v1,['cs.HC'],"Mixed Reality,Virtual Reality,Simulation,Open-Source,Naturalistic Interactions,Hand Controlled,Depth Sensing Cameras,Sensor Independent,Human-Computer Interactions,Virtual Smart Objects"
"Comparative Analysis of Change Blindness in Virtual Reality and
  Augmented Reality Environments","Change blindness is a phenomenon where an individual fails to notice
alterations in a visual scene when a change occurs during a brief interruption
or distraction. Understanding this phenomenon is specifically important for the
technique that uses a visual stimulus, such as Virtual Reality (VR) or
Augmented Reality (AR). Previous research had primarily focused on 2D
environments or conducted limited controlled experiments in 3D immersive
environments. In this paper, we design and conduct two formal user experiments
to investigate the effects of different visual attention-disrupting conditions
(Flickering and Head-Turning) and object alternative conditions (Removal, Color
Alteration, and Size Alteration) on change blindness detection in VR and AR
environments. Our results reveal that participants detected changes more
quickly and had a higher detection rate with Flickering compared to
Head-Turning. Furthermore, they spent less time detecting changes when an
object disappeared compared to changes in color or size. Additionally, we
provide a comparison of the results between VR and AR environments.","['DongHoon Kim', 'Dongyun Han', 'Isaac Cho']",2023-08-24T00:08:39Z,http://arxiv.org/abs/2308.12476v3,['cs.HC'],"Change blindness,Virtual Reality,Augmented Reality,3D immersive environments,Visual attention,Flickering,Head-Turning,Object alternative,Removal,Color Alteration,Size Alteration"
Expanding Targets in Virtual Reality Environments: A Fitts' Law Study,"Target pointing selection is a fundamental task. According to Fitts' law,
users need more time to select targets with smaller sizes. Expanding the target
to a larger size is a practical approach that can facilitate pointing
selection. It has been well-examined and -deployed in 2D user interfaces.
However, limited research has investigated target expansion methods using an
immersive virtual reality (VR) head-mounted display (HMD). In this work, we
aimed to fill this gap by conducting a user study using ISO 9241-411
multi-directional pointing task to examine the effect of target expansion on
target selection performance in VR HMD. Based on our results, we found that
compared to not expanding the target, expanding the target width by 1.5 and 2.5
times during the movement can significantly reduce the selection time. We hope
that the design and results derived from the study can help frame future work.","['Rongkai Shi', 'Yushi Wei', 'Yue Li', 'Lingyun Yu', 'Hai-Ning Liang']",2023-08-24T02:57:48Z,http://arxiv.org/abs/2308.12515v1,['cs.HC'],"Fitts' Law,Target selection,Virtual Reality,Target expansion,User study,ISO 9241-411,Pointing task,Head-mounted display,Selection time,Target width"
"Designing Loving-Kindness Meditation in Virtual Reality for
  Long-Distance Romantic Relationships","Loving-kindness meditation (LKM) is used in clinical psychology for couples'
relationship therapy, but physical isolation can make the relationship more
strained and inaccessible to LKM. Virtual reality (VR) can provide immersive
LKM activities for long-distance couples. However, no suitable commercial VR
applications for couples exist to engage in LKM activities of long-distance.
This paper organized a series of workshops with couples to build a prototype of
a couple-preferred LKM app. Through analysis of participants' design works and
semi-structured interviews, we derived design considerations for such VR apps
and created a prototype for couples to experience. We conducted a study with
couples to understand their experiences of performing LKM using the VR
prototype and a traditional video conferencing tool. Results show that LKM
session utilizing both tools has a positive effect on the intimate relationship
and the VR prototype is a more preferable tool for long-term use. We believe
our experience can inform future researchers.","['Xian Wang', 'Xiaoyu Mo', 'Lik-Hang Lee', 'Xiaoying Wei', 'Xiaofu Jin', 'Mingming Fan', 'Pan Hui']",2023-09-21T06:38:47Z,http://arxiv.org/abs/2309.11816v1,['cs.HC'],"Loving-kindness meditation,Virtual reality,Long-distance relationships,Couples,Clinical psychology,Therapy,Immersive activities,Design considerations,Prototype"
"This is the Table I Want! Interactive Data Transformation on Desktop and
  in Virtual Reality","Data transformation is an essential step in data science. While experts
primarily use programming to transform their data, there is an increasing need
to support non-programmers with user interface-based tools. With the rapid
development in interaction techniques and computing environments, we report our
empirical findings about the effects of interaction techniques and environments
on performing data transformation tasks. Specifically, we studied the potential
benefits of direct interaction and virtual reality (VR) for data
transformation. We compared gesture interaction versus a standard WIMP user
interface, each on the desktop and in VR. With the tested data and tasks, we
found time performance was similar between desktop and VR. Meanwhile, VR
demonstrates preliminary evidence to better support provenance and sense-making
throughout the data transformation process. Our exploration of performing data
transformation in VR also provides initial affirmation for enabling an
iterative and fully immersive data science workflow.","['Sungwon In', 'Tica Lin', 'Chris North', 'Hanspeter Pfister', 'Yalong Yang']",2023-09-21T15:25:46Z,http://arxiv.org/abs/2309.12168v1,['cs.HC'],"Data transformation,Programming,User interface,Interaction techniques,Computing environments,Gesture interaction,WIMP user interface,Virtual reality,Provenance,Sense-making."
Drone Flight Path Architecture,"This project was built from a pre-existing architecture that facilitates the
planning and automatic execution of drone routes in a known space through a 3D
virtual reality environment. Our work consisted in extending this architecture
by integrating a new web component, making use of a 3D map API, to facilitate
to people who do not have access to virtual reality hardware, the possibility
of planning flight routes that have as parameters the <latitude, longitude>
with respect to the globe and also a component in meters that represents the
height at which the drone rises in a certain point. Additionally, the
configuration possibilities of a route were extended in order to take advantage
of one of the components that gives more value and potential to unmanned
aircrafts: the use of the camera in multiple contexts and scenarios. The
extension of this solution allows the user to assign different camera tasks
along the route, see in real time what the camera is capturing and, after the
flight, retrieve the multimedia content that was created","['Andres Forero Osorio', 'Carlos Andrés Torres Echeverría']",2023-08-02T23:50:01Z,http://arxiv.org/abs/2309.12319v1,['cs.HC'],"drone,flight path,architecture,3D map API,latitude,longitude,camera,unmanned aircraft,virtual reality,multimedia content"
"Virtual Reality as a Tool for Studying Diversity and Inclusion in
  Human-Robot Interaction: Advantages and Challenges","This paper investigates the potential of Virtual Reality (VR) as a research
tool for studying diversity and inclusion characteristics in the context of
human-robot interactions (HRI). Some exclusive advantages of using VR in HRI
are discussed, such as a controllable environment, the possibility to
manipulate the variables related to the robot and the human-robot interaction,
flexibility in the design of the robot and the environment, and advanced
measurement methods related e.g. to eye tracking and physiological data. At the
same time, the challenges of researching diversity and inclusion in HRI are
described, especially in accessibility, cyber sickness and bias when developing
VR-environments. Furthermore, solutions to these challenges are being discussed
to fully harness the benefits of VR for the studying of diversity and
inclusion.","['André Helgert', 'Sabrina C. Eimler', 'Carolin Straßmann']",2023-09-26T13:48:30Z,http://arxiv.org/abs/2309.14937v1,['cs.RO'],"Virtual Reality,Diversity,Inclusion,Human-Robot Interaction,Controllable environment,Variables manipulation,Design flexibility,Eye tracking,Physiological data,Bias."
"Voice2Action: Language Models as Agent for Efficient Real-Time
  Interaction in Virtual Reality","Large Language Models (LLMs) are trained and aligned to follow natural
language instructions with only a handful of examples, and they are prompted as
task-driven autonomous agents to adapt to various sources of execution
environments. However, deploying agent LLMs in virtual reality (VR) has been
challenging due to the lack of efficiency in online interactions and the
complex manipulation categories in 3D environments. In this work, we propose
Voice2Action, a framework that hierarchically analyzes customized voice signals
and textual commands through action and entity extraction and divides the
execution tasks into canonical interaction subsets in real-time with error
prevention from environment feedback. Experiment results in an urban
engineering VR environment with synthetic instruction data show that
Voice2Action can perform more efficiently and accurately than approaches
without optimizations.",['Yang Su'],2023-09-29T19:06:52Z,http://arxiv.org/abs/2310.00092v1,"['cs.CL', 'cs.AI', 'cs.HC']","Large Language Models,Virtual Reality,Interaction,Agent,Hierarchical analysis,Voice signals,Textual commands,Entity extraction,Real-time,Error prevention"
"Exploiting Human Color Discrimination for Memory- and Energy-Efficient
  Image Encoding in Virtual Reality","Virtual Reality (VR) has the potential of becoming the next ubiquitous
computing platform. Continued progress in the burgeoning field of VR depends
critically on an efficient computing substrate. In particular, DRAM access
energy is known to contribute to a significant portion of system energy.
Today's framebuffer compression system alleviates the DRAM traffic by using a
numerically lossless compression algorithm. Being numerically lossless,
however, is unnecessary to preserve perceptual quality for humans. This paper
proposes a perceptually lossless, but numerically lossy, system to compress
DRAM traffic. Our idea builds on top of long-established psychophysical studies
that show that humans cannot discriminate colors that are close to each other.
The discrimination ability becomes even weaker (i.e., more colors are
perceptually indistinguishable) in our peripheral vision. Leveraging the color
discrimination (in)ability, we propose an algorithm that adjusts pixel colors
to minimize the bit encoding cost without introducing visible artifacts. The
algorithm is coupled with lightweight architectural support that, in real-time,
reduces the DRAM traffic by 66.9\% and outperforms existing framebuffer
compression mechanisms by up to 20.4\%. Psychophysical studies on human
participants show that our system introduce little to no perceptual fidelity
degradation.","['Nisarg Ujjainkar', 'Ethan Shahan', 'Kenneth Chen', 'Budmonde Duinkharjav', 'Qi Sun', 'Yuhao Zhu']",2023-09-30T17:28:59Z,http://arxiv.org/abs/2310.00441v1,"['cs.GR', 'cs.HC']","Virtual Reality,DRAM access energy,compression algorithm,psychophysical studies,color discrimination,perceptually lossless,numerically lossy,bit encoding cost,perceptual fidelity,framebuffer compression"
"Exploring Users Pointing Performance on Large Displays with Different
  Curvatures in Virtual Reality","Large curved displays inside Virtual Reality environments are becoming
popular for visualizing high-resolution content during analytical tasks, gaming
or entertainment. Prior research showed that such displays provide a wide field
of view and offer users a high level of immersion. However, little is known
about users' performance (e.g., pointing speed and accuracy) on them. We
explore users' pointing performance on large virtual curved displays. We
investigate standard pointing factors (e.g., target width and amplitude) in
combination with relevant curve-related factors, namely display curvature and
both linear and angular measures. Our results show that the less curved the
display, the higher the performance, i.e., faster movement time. This result
holds for pointing tasks controlled via their visual properties (linear widths
and amplitudes) or their motor properties (angular widths and amplitudes).
Additionally, display curvatures significantly affect the error rate for both
linear and angular conditions. Furthermore, we observe that curved displays
perform better or similar to flat displays based on throughput analysis.
Finally, we discuss our results and provide suggestions regarding pointing
tasks on large curved displays in VR.","['A K M Amanat Ullah', 'William Delamare', 'Khalad Hasan']",2023-10-10T04:12:34Z,http://arxiv.org/abs/2310.06296v1,['cs.HC'],"Large displays,Curvatures,Virtual Reality,Pointing performance,Target width,Amplitude,Display curvature,Linear measures,Angular measures,Error rate"
"Who's Watching Me?: Exploring the Impact of Audience Familiarity on
  Player Performance, Experience, and Exertion in Virtual Reality Exergames","Familiarity with audiences plays a significant role in shaping individual
performance and experience across various activities in everyday life. This
study delves into the impact of familiarity with non-playable character (NPC)
audiences on player performance and experience in virtual reality (VR)
exergames. By manipulating of NPC appearance (face and body shape) and voice
familiarity, we explored their effect on game performance, experience, and
exertion. The findings reveal that familiar NPC audiences have a positive
impact on performance, creating a more enjoyable gaming experience, and leading
players to perceive less exertion. Moreover, individuals with higher levels of
self-consciousness exhibit heightened sensitivity to the familiarity with NPC
audiences. Our results shed light on the role of familiar NPC audiences in
enhancing player experiences and provide insights for designing more engaging
and personalized VR exergame environments.","['Zixuan Guo', 'Wenge Xu', 'Jialin Zhang', 'Hongyu Wang', 'Cheng-Hung Lo', 'Hai-Ning Liang']",2023-10-23T12:37:02Z,http://arxiv.org/abs/2310.14867v1,['cs.HC'],"audience familiarity,player performance,experience,exertion,virtual reality,exergames,non-playable character (NPC),appearance,voice familiarity"
Can Virtual Reality Protect Users from Keystroke Inference Attacks?,"Virtual Reality (VR) has gained popularity by providing immersive and
interactive experiences without geographical limitations. It also provides a
sense of personal privacy through physical separation. In this paper, we show
that despite assumptions of enhanced privacy, VR is unable to shield its users
from side-channel attacks that steal private information. Ironically, this
vulnerability arises from VR's greatest strength, its immersive and interactive
nature. We demonstrate this by designing and implementing a new set of
keystroke inference attacks in shared virtual environments, where an attacker
(VR user) can recover the content typed by another VR user by observing their
avatar. While the avatar displays noisy telemetry of the user's hand motion, an
intelligent attacker can use that data to recognize typed keys and reconstruct
typed content, without knowing the keyboard layout or gathering labeled data.
We evaluate the proposed attacks using IRB-approved user studies across
multiple VR scenarios. For 13 out of 15 tested users, our attacks accurately
recognize 86%-98% of typed keys, and the recovered content retains up to 98% of
the meaning of the original typed content. We also discuss potential defenses.","['Zhuolin Yang', 'Zain Sarwar', 'Iris Hwang', 'Ronik Bhaskar', 'Ben Y. Zhao', 'Haitao Zheng']",2023-10-24T21:19:38Z,http://arxiv.org/abs/2310.16191v1,['cs.CR'],"Virtual Reality,Keystroke Inference Attacks,Side-Channel Attacks,Shared Virtual Environments,User Studies"
"Haptic-Enhanced Virtual Reality Simulator for Robot-Assisted Femur
  Fracture Surgery","In this paper, we develop a virtual reality (VR) simulator for the Robossis
robot-assisted femur fracture surgery. Due to the steep learning curve for such
procedures, a VR simulator is essential for training surgeon(s) and staff. The
Robossis Surgical Simulator (RSS) is designed to immerse user(s) in a realistic
surgery setting using the Robossis system as completed in a previous real-world
cadaveric procedure. The RSS is designed to interface the Sigma-7 Haptic
Controller with the Robossis Surgical Robot (RSR) and the Meta Quest VR
headset. Results show that the RSR follows user commands in 6 DOF and prevents
the overlapping of bone segments. This development demonstrates a promising
avenue for future implementation of the Robossis system.","['Fayez H. Alruwaili', 'David W. Halim-Banoub', 'Jessica Rodgers', 'Adam Dalkilic', 'Christopher Haydel', 'Javad Parvizi', 'Iulian I. Iordachita', 'Mohammad H. Abedin-Nasab']",2023-10-29T23:07:51Z,http://arxiv.org/abs/2310.19187v1,['cs.RO'],"virtual reality,simulator,robot-assisted surgery,femur fracture,haptic controller,surgical robot,VR headset,DOF,bone segments,Robossis"
"Clonemator: Composing Spatiotemporal Clones to Create Interactive
  Automators in Virtual Reality","Clonemator is a virtual reality (VR) system allowing users to create their
avatar clones and configure them spatially and temporally, forming automators
to accomplish complex tasks. In particular, clones can (1) freeze at a user's
body pose as static objects, (2) synchronously mimic the user's movement, and
(3) replay a sequence of the user's actions in a period of time later. Combined
with traditional techniques such as scaling, positional rearrangement, group
selection, and duplication, Clonemator enables users to iteratively develop
customized and reusable solutions by breaking down complex tasks into a
sequence of collaborations with clones. This bypasses implementing dedicated
interaction techniques or scripts while allowing flexible interactions in VR
applications. We demonstrate the flexibility of Clonemator with several
examples and validate its usability and effectiveness through a preliminary
user study. Finally, we discuss the potential of Clonemator in VR applications
such as gaming mechanisms, spatial interaction techniques, and multi-robot
control and provide our insights for future research.","['Yi-Shuo Lin', 'Ching-Yi Tsai', 'Lung-Pan Cheng']",2023-11-08T02:00:58Z,http://arxiv.org/abs/2311.04427v1,"['cs.HC', 'H.5.2; D.1.7']","virtual reality,avatar,clones,spatiotemporal,automators,interactions,user study,gaming mechanisms,spatial interaction techniques"
"Deep Motion Masking for Secure, Usable, and Scalable Real-Time
  Anonymization of Virtual Reality Motion Data","Virtual reality (VR) and ""metaverse"" systems have recently seen a resurgence
in interest and investment as major technology companies continue to enter the
space. However, recent studies have demonstrated that the motion tracking
""telemetry"" data used by nearly all VR applications is as uniquely identifiable
as a fingerprint scan, raising significant privacy concerns surrounding
metaverse technologies. Although previous attempts have been made to anonymize
VR motion data, we present in this paper a state-of-the-art VR identification
model that can convincingly bypass known defensive countermeasures. We then
propose a new ""deep motion masking"" approach that scalably facilitates the
real-time anonymization of VR telemetry data. Through a large-scale user study
(N=182), we demonstrate that our method is significantly more usable and
private than existing VR anonymity systems.","['Vivek Nair', 'Wenbo Guo', ""James F. O'Brien"", 'Louis Rosenberg', 'Dawn Song']",2023-11-09T01:34:22Z,http://arxiv.org/abs/2311.05090v1,"['cs.HC', 'cs.CR']","Deep motion masking,Secure,Scalable,Real-time,Anonymization,Virtual Reality,Motion data,Telemetry,Identification model,User study"
"Smell of Fire Increases Behavioural Realism in Virtual Reality: A Case
  Study on a Recreated MGM Grand Hotel Fire","Virtual reality allows creating highly immersive visual and auditory
experiences, making users feel physically present in the environment. This
makes it an ideal platform to simulate dangerous scenarios, including fire
evacuation, and study human behaviour without exposing users to harmful
elements. However, human perception of the surroundings is based on the
integration of multiple sensory cues (visual, auditory, tactile, or/and
olfactory) present in the environment. When some of the sensory stimuli are
missing in the virtual experience, it can break the illusion of being there in
the environment and could lead to actions that deviate from normal behaviour.
In this work, we added an olfactory cue in a well-documented historic hotel
fire scenario that was recreated in VR, and examined the effects of the
olfactory cue on human behaviour. We conducted a between subject study on 40
naive participants. Our results show that the addition of the olfactory cue
could increase behavioural realism. We found that 80% of the studied actions
for the VR with olfactory cue condition matched the ones performed by the
survivors. In comparison, only 40% of the participants' actions for VR only
condition were similar to the survivors.","['Humayun Khan', 'Daniel Nilsson']",2023-11-13T20:55:56Z,http://arxiv.org/abs/2311.09246v1,['cs.HC'],"virtual reality,behavioural realism,olfactory cue,fire evacuation,human behaviour,sensory cues,immersive experiences,dangerous scenarios,historic hotel fire,between subject study"
"Exploring User Perceptions of Virtual Reality Scene Design in Metaverse
  Learning Environments","Metaverse learning environments allow for a seamless and intuitive transition
between activities compared to Virtual Reality (VR) learning environments, due
to their interconnected design. The design of VR scenes is important for
creating effective learning experiences in the Metaverse. However, there is
limited research on the impact of different design elements on user's learning
experiences in VR scenes. To address this, a study was conducted with 16
participants who interacted with two VR scenes, each with varying design
elements such as style, color, texture, object, and background, while watching
a short tutorial. Participant rankings of the scenes for learning were obtained
using a seven-point Likert scale, and the Mann-Whitney U test was used to
validate differences in preference between the scenes. The results showed a
significant difference in preference between the scenes. Further analysis using
the NASA TLX questionnaire was conducted to examine the impact of this
difference on cognitive load, and participant feedback was also considered. The
study emphasizes the importance of careful VR scene design to improve the
user's learning experience.","['Rahatara Ferdousi', 'Mohammed Faisal', 'Fedwa Laamarti', 'Chunsheng Yang', 'Abdulmotaleb El Saddik']",2023-11-17T00:56:55Z,http://arxiv.org/abs/2311.10256v2,"['cs.HC', 'cs.MM', 'K.3; J.7']","Virtual Reality,Metaverse,Learning Environments,Design Elements,Likert Scale,Mann-Whitney U Test,Cognitive Load,Participant Feedback"
"Necknasium: A Virtual Reality Rehabilitation Game for Managing Faulty
  Neck Posture","This study is concerned with the application of virtual reality (VR) in
rehabilitation programs for faulty neck posture which is a primary source of
neck pain (NP). The latter is a highly prevalent musculoskeletal disorder that
is associated with serious societal and economic burden. VR has been shown to
be effective in the physical rehabilitation of various diseases. Specifically,
it has been shown to improve the adherence of patients and engagement to carry
out physical exercises on a regular basis. Many games have been used to manage
NP with different immersion levels. Towards this goal, we present a VR-based
system that targets a specific neck problem, the so called forward head posture
(FHP), which is a faulty head position that abnormally stresses neck
structures. The system can also generalize well to other neck-related disorders
and rehabilitation goals. We show the steps for designing and developing the
system, and we highlight the aspects of interaction between usability and
various game elements. Using a three-point scale for user experience, we also
present preliminary insights on the evaluation of the system prototype, and we
discuss future enhancement directions based on the feedback from users.","['Aliaa Rehan Youssef', 'Mohammed Gumaa', 'Ahmad Al-Kabbany']",2023-12-22T01:46:42Z,http://arxiv.org/abs/2312.14371v1,['cs.HC'],"virtual reality,rehabilitation,neck posture,neck pain,musculoskeletal disorder,forward head posture,engagement,physical exercises,game elements,user experience"
"Decoding Fear: Exploring User Experiences in Virtual Reality Horror
  Games","This preliminary study investigated user experiences in VR horror games,
highlighting fear-triggering and gender-based differences in perception. By
utilizing a scientifically validated and specially designed questionnaire, we
successfully collected questionnaire data from 23 subjects for an early
empirical study of fear induction in a virtual reality gaming environment. The
early findings suggest that visual restrictions and ambient sound-enhanced
realism may be more effective in intensifying the fear experience. Participants
exhibited a tendency to avoid playing alone or during nighttime, underscoring
the significant psychological impact of VR horror games. The study also
revealed a distinct gender difference in fear perception, with female
participants exhibiting a higher sensitivity to fear stimuli. However, the
preference for different types of horror games was not solely dominated by
males; it varied depending on factors such as the game's pace, its objectives,
and the nature of the fear stimulant.","['He Zhang', 'Xinyang Li', 'Christine Qiu', 'Xinyi Fu']",2023-12-25T01:40:38Z,http://arxiv.org/abs/2312.15582v1,['cs.HC'],"user experiences,virtual reality,horror games,fear induction,perception differences,questionnaire data,fear experience,visual restrictions,ambient sound,psychological impact,gender difference"
"Near Real-Time Data-Driven Control of Virtual Reality Traffic in Open
  Radio Access Network","In mobile networks, Open Radio Access Network (ORAN) provides a framework for
implementing network slicing that interacts with the resources at the lower
layers. Both monitoring and Radio Access Network (RAN) control is feasible for
both 4G and 5G systems. In this work, we consider how data-driven resource
allocation in a 4G context can enable adaptive slice allocation to steer the
experienced latency of Virtual Reality (VR) traffic towards a requested
latency. We develop an xApp for the near real-time RAN Intelligent Controller
(RIC) that embeds a heuristic algorithm for latency control, aiming to: (1)
maintain latency of a VR stream around a requested value; and (2) improve the
available RAN allocation to offer higher bit rate to another user. We have
experimentally demonstrated the proposed approach in an ORAN testbed. Our
results show that the data-driven approach can dynamically follow the variation
of the traffic load while satisfying the required latency. This results in
15.8% more resources to secondary users than a latency-equivalent static
allocation.","['Andreas Casparsen', 'Beatriz Soret', 'Jimmy Jessen Nielsen', 'Petar Popovski']",2024-01-03T10:18:55Z,http://arxiv.org/abs/2401.01652v1,['cs.NI'],"Open Radio Access Network,Network slicing,Radio Access Network,Virtual Reality,xApp,RAN Intelligent Controller,latency control,heuristic algorithm,resource allocation,traffic load"
"Testing Human-Robot Interaction in Virtual Reality: Experience from a
  Study on Speech Act Classification","In recent years, an increasing number of Human-Robot Interaction (HRI)
approaches have been implemented and evaluated in Virtual Reality (VR), as it
allows to speed-up design iterations and makes it safer for the final user to
evaluate and master the HRI primitives. However, identifying the most suitable
VR experience is not straightforward. In this work, we evaluate how, in a smart
agriculture scenario, immersive and non-immersive VR are perceived by users
with respect to a speech act understanding task. In particular, we collect
opinions and suggestions from the 81 participants involved in both experiments
to highlight the strengths and weaknesses of these different experiences.","['Sara Kaszuba', 'Sandeep Reddy Sabbella', 'Francesco Leotta', 'Pascal Serrarens', 'Daniele Nardi']",2024-01-09T13:08:13Z,http://arxiv.org/abs/2401.04534v1,"['cs.RO', 'cs.HC']","Human-Robot Interaction,Virtual Reality,Speech Act Classification,Smart Agriculture,Immersive VR,Non-immersive VR,User Perception,Design Iterations,HRI primitives,Evaluation"
Evaluating Gesture Recognition in Virtual Reality,"Human-Robot Interaction (HRI) has become increasingly important as robots are
being integrated into various aspects of daily life. One key aspect of HRI is
gesture recognition, which allows robots to interpret and respond to human
gestures in real-time. Gesture recognition plays an important role in
non-verbal communication in HRI. To this aim, there is ongoing research on how
such non-verbal communication can strengthen verbal communication and improve
the system's overall efficiency, thereby enhancing the user experience with the
robot. However, several challenges need to be addressed in gesture recognition
systems, which include data generation, transferability, scalability,
generalizability, standardization, and lack of benchmarking of the gestural
systems. In this preliminary paper, we want to address the challenges of data
generation using virtual reality simulations and standardization issues by
presenting gestures to some commands that can be used as a standard in ground
robots.","['Sandeep Reddy Sabbella', 'Sara Kaszuba', 'Francesco Leotta', 'Pascal Serrarens', 'Daniele Nardi']",2024-01-09T13:35:09Z,http://arxiv.org/abs/2401.04545v1,"['cs.HC', 'cs.RO']","Gesture recognition,Virtual reality,Human-Robot Interaction,Non-verbal communication,Data generation,Standardization,Scalability,Generalizability,Benchmarking,Ground robots"
AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey,"With the rapid advance of computer graphics and artificial intelligence
technologies, the ways we interact with the world have undergone a
transformative shift. Virtual Reality (VR) technology, aided by artificial
intelligence (AI), has emerged as a dominant interaction media in multiple
application areas, thanks to its advantage of providing users with immersive
experiences. Among those applications, medicine is considered one of the most
promising areas. In this paper, we present a comprehensive examination of the
burgeoning field of AI-enhanced VR applications in medical care and services.
By introducing a systematic taxonomy, we meticulously classify the pertinent
techniques and applications into three well-defined categories based on
different phases of medical diagnosis and treatment: Visualization Enhancement,
VR-related Medical Data Processing, and VR-assisted Intervention. This
categorization enables a structured exploration of the diverse roles that
AI-powered VR plays in the medical domain, providing a framework for a more
comprehensive understanding and evaluation of these technologies. To our best
knowledge, this is the first systematic survey of AI-powered VR systems in
medical settings, laying a foundation for future research in this
interdisciplinary domain.","['Yixuan Wu', 'Kaiyuan Hu', 'Danny Z. Chen', 'Jian Wu']",2024-02-05T15:24:13Z,http://arxiv.org/abs/2402.03093v1,"['cs.CV', 'cs.HC']","artificial intelligence,virtual reality,medicine,computer graphics,immersive experiences,medical diagnosis,treatment,visualization enhancement,medical data processing,VR-assisted intervention"
"Stepping into the Right Shoes: The Effects of User-Matched Avatar
  Ethnicity and Gender on Sense of Embodiment in Virtual Reality","In many consumer virtual reality (VR) applications, users embody predefined
characters that offer minimal customization options, frequently emphasizing
storytelling over user choice. We explore whether matching a user's physical
characteristics, specifically ethnicity and gender, with their virtual
self-avatar affects their sense of embodiment in VR. We conducted a 2 x 2
within-subjects experiment (n=32) with a diverse user population to explore the
impact of matching or not matching a user's self-avatar to their ethnicity and
gender on their sense of embodiment. Our results indicate that matching the
ethnicity of the user and their self-avatar significantly enhances sense of
embodiment regardless of gender, extending across various aspects, including
appearance, response, and ownership. We also found that matching gender
significantly enhanced ownership, suggesting that this aspect is influenced by
matching both ethnicity and gender. Interestingly, we found that matching
ethnicity specifically affects self-location while matching gender specifically
affects one's body ownership.","['Tiffany D. Do', 'Camille Isabella Protko', 'Ryan P. McMahan']",2024-02-05T18:36:11Z,http://arxiv.org/abs/2402.03279v3,['cs.HC'],"virtual reality,avatar,embodiment,ethnicity,gender,user-matched,sense of embodiment,experiment,self-avatar,ownership"
"BioNet-XR: Biological Network Visualization Framework for Virtual
  Reality and Mixed Reality Environments","Protein-protein interaction networks (PPIN) enable the study of cellular
processes in organisms. Visualizing PPINs in extended reality (XR), including
virtual reality (VR) and mixed reality (MR), is crucial for exploring
subnetworks, evaluating protein positions, and collaboratively analyzing and
discussing on networks with the help of recent technological advancements.
Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in
VR and MR environments. BioNet-XR was developed with the Unity3D game engine.
Our framework provides state-of-the-art methods and visualization features
including teleportation between nodes, general and first-person view to explore
the network, subnetwork construction via PageRank, Steiner tree, and all-pair
shortest path algorithms for a given set of initial nodes. We used usability
tests to gather feedback from both specialists (bioinformaticians) and
generalists (multidisciplinary groups), addressing the need for usability
evaluations of visualization tools. In the MR version of BioNet-XR, users can
seamlessly transition to real-world environments and interact with protein
interaction networks. BioNet-XR is highly modular and adaptable for
visualization of other biological networks, such as metabolic and regulatory
networks, and extension with additional network methods.","['Busra Senderin', 'Nurcan Tuncbag', 'Elif Surer']",2024-02-06T12:20:10Z,http://arxiv.org/abs/2402.03946v1,['cs.MM'],"Biological Network Visualization,Protein-protein interaction networks,Extended Reality,Virtual Reality,Mixed Reality,Unity3D game engine,Usability tests,PageRank algorithm,Steiner tree algorithm"
MERP: Metaverse Extended Realtiy Portal,"A standardized control system called Metaverse Extended Reality Portal (MERP)
is presented as a solution to the issues with conventional VR eyewear. The MERP
system improves user awareness of the physical world while offering an
immersive 3D view of the metaverse by using a shouldermounted projector to
display a Heads-Up Display (HUD) in a designated Metaverse Experience Room. To
provide natural and secure interaction inside the metaverse, a compass module
and gyroscope integration enable accurate mapping of real-world motions to
avatar actions. Through user tests and research, the MERP system shows that it
may reduce mishaps brought on by poor spatial awareness, offering an improved
metaverse experience and laying the groundwork for future developments in
virtual reality technology. MERP, which is compared with existing Virtual
Reality (VR) glasses used to traverse the metaverse, is projected to become a
seamless, novel and better alternative. Existing VR headsets and AR glasses
have well-known drawbacks that making them ineffective for prolonged usage as
it causes harm to the eyes.","['Anisha Ghosh', 'Aditya Mitra', 'Anik Saha', 'Sibi Chakkaravarthy Sethuraman', 'Anitha Subramanian']",2024-02-08T11:46:03Z,http://arxiv.org/abs/2402.05592v1,['cs.HC'],"Metaverse Extended Reality Portal,VR eyewear,Heads-Up Display,compass module,gyroscope integration,avatar actions,spatial awareness,virtual reality technology,Virtual Reality glasses,AR glasses"
"Springboard, Roadblock or ""Crutch""?: How Transgender Users Leverage
  Voice Changers for Gender Presentation in Social Virtual Reality","Social virtual reality (VR) serves as a vital platform for transgender
individuals to explore their identities through avatars and foster personal
connections within online communities. However, it presents a challenge: the
disconnect between avatar embodiment and voice representation, often leading to
misgendering and harassment. Prior research acknowledges this issue but
overlooks the potential solution of voice changers. We interviewed 13
transgender and gender-nonconforming users of social VR platforms, focusing on
their experiences with and without voice changers. We found that using a voice
changer not only reduces voice-related harassment, but also allows them to
experience gender euphoria through both hearing their modified voice and the
reactions of others to their modified voice, motivating them to pursue voice
training and medication to achieve desired voices. Furthermore, we identified
the technical barriers to current voice changer technology and potential
improvements to alleviate the problems that transgender and
gender-nonconforming users face.","['Kassie Povinelli', 'Yuhang Zhao']",2024-02-13T05:10:04Z,http://arxiv.org/abs/2402.08217v1,"['cs.HC', 'cs.SD', 'eess.AS']","transgender,voice changers,gender presentation,social virtual reality,avatar embodiment,misgendering,harassment,voice training,medication,technical barriers"
"Penetration Vision through Virtual Reality Headsets: Identifying
  360-degree Videos from Head Movements","In this paper, we present the first contactless side-channel attack for
identifying 360 videos being viewed in a Virtual Reality (VR) Head Mounted
Display (HMD). Although the video content is displayed inside the HMD without
any external exposure, we observe that user head movements are driven by the
video content, which creates a unique side channel that does not exist in
traditional 2D videos. By recording the user whose vision is blocked by the HMD
via a malicious camera, an attacker can analyze the correlation between the
user's head movements and the victim video to infer the video title.
  To exploit this new vulnerability, we present INTRUDE, a system for
identifying 360 videos from recordings of user head movements. INTRUDE is
empowered by an HMD-based head movement estimation scheme to extract a head
movement trace from the recording and a video saliency-based trace-fingerprint
matching framework to infer the video title. Evaluation results show that
INTRUDE achieves over 96% of accuracy for video identification and is robust
under different recording environments. Moreover, INTRUDE maintains its
effectiveness in the open-world identification scenario.","['Anh Nguyen', 'Xiaokuan Zhang', 'Zhisheng Yan']",2024-02-18T04:06:42Z,http://arxiv.org/abs/2402.11446v2,['cs.HC'],"Virtual reality headsets,360-degree videos,side-channel attack,head movements,video identification,HMD-based,video saliency,trace-fingerprint matching,vulnerability,recording environments"
"Touching the Moon: Leveraging Passive Haptics, Embodiment and Presence
  for Operational Assessments in Virtual Reality","Space agencies are in the process of drawing up carefully thought-out
Concepts of Operations (ConOps) for future human missions on the Moon. These
are typically assessed and validated through costly and logistically demanding
analogue field studies. While interactive simulations in Virtual Reality (VR)
offer a comparatively cost-effective alternative, they have faced criticism for
lacking the fidelity of real-world deployments. This paper explores the
applicability of passive haptic interfaces in bridging the gap between
simulated and real-world ConOps assessments. Leveraging passive haptic props
(equipment mockup and astronaut gloves), we virtually recreated the Apollo 12
mission procedure and assessed it with experienced astronauts and other space
experts. Quantitative and qualitative findings indicate that haptics increased
presence and embodiment, thus improving perceived simulation fidelity and
validity of user reflections. We conclude by discussing the potential role of
passive haptic modalities in facilitating early-stage ConOps assessments for
human endeavours on the Moon and beyond.","['Florian Dufresne', 'Tommy Nilsson', 'Geoffrey Gorisse', 'Enrico Guerra', 'André Zenner', 'Olivier Christmann', 'Leonie Bensch', 'Nikolai Anton Callus', 'Aidan Cowley']",2024-02-24T02:48:55Z,http://arxiv.org/abs/2402.15694v1,"['cs.HC', '93B51, 97M50', 'H.1.2; I.3.8; J.4; J.m; K.8.2; J.6']","passive haptics,embodiment,presence,operational assessments,virtual reality,ConOps,haptic interfaces,simulation fidelity,astronaut gloves"
CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency,"Neural Radiance Field (NeRF) has shown impressive results in novel view
synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),
thanks to its ability to represent scenes continuously. However, when just a
few input view images are available, NeRF tends to overfit the given views and
thus make the estimated depths of pixels share almost the same value. Unlike
previous methods that conduct regularization by introducing complex priors or
additional supervisions, we propose a simple yet effective method that
explicitly builds depth-aware consistency across input views to tackle this
challenge. Our key insight is that by forcing the same spatial points to be
sampled repeatedly in different input views, we are able to strengthen the
interactions between views and therefore alleviate the overfitting problem. To
achieve this, we build the neural networks on layered representations
(\textit{i.e.}, multiplane images), and the sampling point can thus be
resampled on multiple discrete planes. Furthermore, to regularize the unseen
target views, we constrain the rendered colors and depths from different input
views to be the same. Although simple, extensive experiments demonstrate that
our proposed method can achieve better synthesis quality over state-of-the-art
methods.","['Hanxin Zhu', 'Tianyu He', 'Zhibo Chen']",2024-02-26T09:04:04Z,http://arxiv.org/abs/2402.16407v1,"['cs.CV', 'cs.GR']","Neural Radiance Field,Novel View Synthesis,Cross-view Multiplane Consistency,Virtual Reality,Augmented Reality,Depth-aware Consistency,Neural Networks,Multiplane Images,Synthesis Quality"
"The Interaction Fidelity Model: A Taxonomy to Distinguish the Aspects of
  Fidelity in Virtual Reality","Fidelity describes how closely a replication resembles the original. It can
be helpful to analyze how faithful interactions in virtual reality (VR) are to
a reference interaction. In prior research, fidelity has been restricted to the
simulation of reality - also called realism. Our definition includes other
reference interactions, such as superpowers or fiction. Interaction fidelity is
a multilayered concept. Unfortunately, different aspects of fidelity have
either not been distinguished in scientific discourse or referred to with
inconsistent terminology. Therefore, we present the Interaction Fidelity Model
(IntFi Model). Based on the human-computer interaction loop, it systematically
covers all stages of VR interactions. The conceptual model establishes a clear
structure and precise definitions of eight distinct components. It was reviewed
through interviews with fourteen VR experts. We provide guidelines, diverse
examples, and educational material to universally apply the IntFi Model to any
VR experience. We identify common patterns and propose foundational research
opportunities.","['Michael Bonfert', 'Thomas Muender', 'Ryan P. McMahan', 'Frank Steinicke', 'Doug Bowman', 'Rainer Malaka', 'Tanja Döring']",2024-02-26T15:38:08Z,http://arxiv.org/abs/2402.16665v1,"['cs.HC', 'cs.GR', 'cs.MM', 'H.5.2; H.5.1; I.3.7; H.1.2']","Fidelity,Virtual Reality,Interaction fidelity,Simulation,Realism,Reference interaction,Superpowers,Fiction,Human-computer interaction,Conceptual model"
"Using Virtual Reality for Detection and Intervention of Depression -- A
  Systematic Literature Review","The use of emerging technologies like Virtual Reality (VR) in therapeutic
settings has increased in the past few years. By incorporating VR, a mental
health condition like depression can be assessed effectively, while also
providing personalized motivation and meaningful engagement for treatment
purposes. The integration of external sensors further enhances the engagement
of the subjects with the VR scenes. This paper presents a comprehensive review
of existing literature on the detection and treatment of depression using VR.
It explores various types of VR scenes, external hardware, innovative metrics,
and targeted user studies conducted by researchers and professionals in the
field. The paper also discusses potential requirements for designing VR scenes
specifically tailored for depression assessment and treatment, with the aim of
guiding future practitioners in this area.","['Mohammad Waqas', 'Y Pawankumar Gururaj', 'V D Shanmukha Mitra', 'Sai Anirudh Karri', 'Raghu Reddy', 'Syed Azeemuddin']",2024-03-04T09:44:37Z,http://arxiv.org/abs/2403.01882v1,['cs.HC'],"Virtual Reality,Depression,Detection,Intervention,Literature Review,Therapeutic Settings,Mental Health,External Sensors,Engagement,Treatment"
"Mitigating Ageism through Virtual Reality: Intergenerational
  Collaborative Escape Room Design","As virtual reality (VR) becomes more popular for intergenerational
collaboration, there is still a significant gap in research regarding
understanding the potential for reducing ageism. Our study aims to address this
gap by analyzing ageism levels before and after VR escape room collaborative
experiences. We recruited 28 participants to collaborate with an older player
in a challenging VR escape room game. To ensure consistent and reliable
performance data of older players, our experimenters simulated older
participants following specific guidelines. After completing the game, we found
a significant reduction in ageism among younger participants. Furthermore, we
introduce a new game mechanism that encourages intergenerational collaboration.
Our research highlights the potential of VR collaborative games as a practical
tool for mitigating ageism. It provides valuable insights for designing
immersive VR experiences that foster enhanced intergenerational collaboration.","['Ruotong Zou', 'Shuyu Yin', 'Tianqi Song', 'Peinuan Qin', 'Yi-Chieh Lee']",2024-03-06T14:32:01Z,http://arxiv.org/abs/2403.03742v1,['cs.HC'],"virtual reality,intergenerational collaboration,ageism,escape room,collaborative design,immersive experiences,research,older players,game mechanism"
"Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A
  GRU LSTM Hybrid Approach","Accurate classification of objects in 3D point clouds is a significant
problem in several applications, such as autonomous navigation and
augmented/virtual reality scenarios, which has become a research hot spot. In
this paper, we presented a deep learning strategy for 3D object classification
in augmented reality. The proposed approach is a combination of the GRU and
LSTM. LSTM networks learn longer dependencies well, but due to the number of
gates, it takes longer to train; on the other hand, GRU networks have a weaker
performance than LSTM, but their training speed is much higher than GRU, which
is The speed is due to its fewer gates. The proposed approach used the
combination of speed and accuracy of these two networks. The proposed approach
achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes
eight classes (unlabeled, man-made terrain, natural terrain, high vegetation,
low vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the
traditional machine learning approaches could achieve a maximum accuracy of
0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,
Hybrid Model, GRULSTM, GRU, LSTM","['Ramin Mousa', 'Mitra Khezli', 'Mohamadreza Azadi', 'Vahid Nikoofard', 'Saba Hesaraki']",2024-03-09T16:05:31Z,http://arxiv.org/abs/2403.05950v2,"['cs.CV', 'cs.AI']","3D Point Clouds,Recurrent Neural Network,GRU,LSTM,Object Classification,Augmented Reality,Deep Learning,Hybrid Approach,Machine Learning"
"A Review of Virtual Reality Studies on Autonomous Vehicle--Pedestrian
  Interaction","An increasing number of studies employ virtual reality (VR) to evaluate
interactions between autonomous vehicles (AVs) and pedestrians. VR simulators
are valued for their cost-effectiveness, flexibility in developing various
traffic scenarios, safe conduct of user studies, and acceptable ecological
validity. Reviewing the literature between 2010 and 2020, we found 31 empirical
studies using VR as a testing apparatus for both implicit and explicit
communication. By performing a systematic analysis, we identified current
coverage of critical use cases, obtained a comprehensive account of factors
influencing pedestrian behavior in simulated traffic scenarios, and assessed
evaluation measures. Based on the findings, we present a set of recommendations
for implementing VR pedestrian simulators and propose directions for future
research.","['Tram Thi Minh Tran', 'Callum Parker', 'Martin Tomitsch']",2024-03-18T00:08:04Z,http://arxiv.org/abs/2403.11378v1,['cs.HC'],"Virtual reality,Autonomous vehicles,Pedestrian interaction,VR simulators,Traffic scenarios,User studies,Ecological validity,Use cases,Pedestrian behavior,Evaluation measures"
"User-customizable Shared Control for Fine Teleoperation via Virtual
  Reality","Shared control can ease and enhance a human operator's ability to teleoperate
robots, particularly for intricate tasks demanding fine control over multiple
degrees of freedom. However, the arbitration process dictating how much
autonomous assistance to administer in shared control can confuse novice
operators and impede their understanding of the robot's behavior. To overcome
these adverse side-effects, we propose a novel formulation of shared control
that enables operators to tailor the arbitration to their unique capabilities
and preferences. Unlike prior approaches to customizable shared control where
users could indirectly modify the latent parameters of the arbitration function
by issuing a feedback command, we instead make these parameters observable and
directly editable via a virtual reality (VR) interface. We present our
user-customizable shared control method for a teleoperation task in SE(3),
known as the buzz wire game. A user study is conducted with participants
teleoperating a robotic arm in VR to complete the game. The experiment spanned
two weeks per subject to investigate longitudinal trends. Our findings reveal
that users allowed to interactively tune the arbitration parameters across
trials generalize well to adaptations in the task, exhibiting improvements in
precision and fluency over direct teleoperation and conventional shared
control.","['Rui Luo', 'Mark Zolotas', 'Drake Moore', 'Taskin Padir']",2024-03-19T22:06:37Z,http://arxiv.org/abs/2403.13177v1,['cs.RO'],"shared control,teleoperation,virtual reality,arbitration process,robotic arm,user study,longitudinal trends,precision,fluency"
"Perception and Control of Surfing in Virtual Reality using a 6-DoF
  Motion Platform","The paper presents a system for simulating surfing in Virtual Reality (VR),
emphasizing the recreation of aquatic motions and user-initiated propulsive
forces using a 6-Degree of Freedom (DoF) motion platform. We present an
algorithmic approach to accurately render surfboard kinematics and interactive
paddling dynamics, validated through experimental evaluation with \(N=17\)
participants. Results indicate that the system effectively reproduces various
acceleration levels, the perception of which is independent of users' body
posture. We additionally found that the presence of ocean ripples amplifies the
perception of acceleration. This system aims to enhance the realism and
interactivity of VR surfing, laying a foundation for future advancements in
surf therapy and interactive aquatic VR experiences.","['Premankur Banerjee', 'Jason Cherin', 'Jayati Upadhyay', 'Jason Kutch', 'Heather Culbertson']",2024-03-23T20:05:34Z,http://arxiv.org/abs/2403.15924v1,"['cs.HC', 'cs.SY', 'eess.SY']","Virtual Reality,Surfing,6-DoF,Motion Platform,Algorithm,Kinematics,Dynamics,Acceleration,Posture,Waves"
Understanding Physical Breakdowns in Virtual Reality,"Virtual Reality (VR) moves away from well-controlled laboratory environments
into public and personal spaces. As users are visually disconnected from the
physical environment, interacting in an uncontrolled space frequently leads to
collisions and raises safety concerns. In my thesis, I investigate this
phenomenon which I define as the physical breakdown in VR. The goal is to
understand the reasons for physical breakdowns, provide solutions, and explore
future mechanisms that could perpetuate safety risks. First, I explored the
reasons for physical breakdowns by investigating how people interact with the
current VR safety mechanism (e.g., Oculus Guardian). Results show one reason
for breaking out of the safety boundary is when interacting with large motions
(e.g., swinging arms), the user does not have enough time to react although
they see the safety boundary. I proposed a solution, FingerMapper, that maps
small-scale finger motions onto virtual arms and hands to enable whole-body
virtual arm motions in VR to avoid physical breakdowns. To demonstrate future
safety risks, I explored the malicious use of perceptual manipulations (e.g.,
redirection techniques) in VR, which could deliberately create physical
breakdowns without users noticing. Results indicate further open challenges
about the cognitive process of how users comprehend their physical environment
when they are blindfolded in VR.",['Wen-Jie Tseng'],2024-03-20T18:03:54Z,http://arxiv.org/abs/2404.00025v1,['cs.HC'],"Virtual Reality,Physical Breakdown,Safety Mechanism,Oculus Guardian,FingerMapper,Perceptual Manipulations,Redirection Techniques,Whole-body motion,Safety Risks,Cognitive Process"
"Integrating Large Language Models with Multimodal Virtual Reality
  Interfaces to Support Collaborative Human-Robot Construction Work","In the construction industry, where work environments are complex,
unstructured and often dangerous, the implementation of Human-Robot
Collaboration (HRC) is emerging as a promising advancement. This underlines the
critical need for intuitive communication interfaces that enable construction
workers to collaborate seamlessly with robotic assistants. This study
introduces a conversational Virtual Reality (VR) interface integrating
multimodal interaction to enhance intuitive communication between construction
workers and robots. By integrating voice and controller inputs with the Robot
Operating System (ROS), Building Information Modeling (BIM), and a game engine
featuring a chat interface powered by a Large Language Model (LLM), the
proposed system enables intuitive and precise interaction within a VR setting.
Evaluated by twelve construction workers through a drywall installation case
study, the proposed system demonstrated its low workload and high usability
with succinct command inputs. The proposed multimodal interaction system
suggests that such technological integration can substantially advance the
integration of robotic assistants in the construction industry.","['Somin Park', 'Carol C. Menassa', 'Vineet R. Kamat']",2024-04-04T14:56:41Z,http://arxiv.org/abs/2404.03498v1,"['cs.RO', 'cs.HC']","Large Language Models,Multimodal Virtual Reality,Human-Robot Collaboration,Construction Work,Virtual Reality Interface,Robot Operating System,Building Information Modeling,Game Engine,Chat Interface,Technological Integration"
"Evaluating Navigation and Comparison Performance of Computational
  Notebooks on Desktop and in Virtual Reality","The computational notebook serves as a versatile tool for data analysis.
However, its conventional user interface falls short of keeping pace with the
ever-growing data-related tasks, signaling the need for novel approaches. With
the rapid development of interaction techniques and computing environments,
there is a growing interest in integrating emerging technologies in data-driven
workflows. Virtual reality, in particular, has demonstrated its potential in
interactive data visualizations. In this work, we aimed to experiment with
adapting computational notebooks into VR and verify the potential benefits VR
can bring. We focus on the navigation and comparison aspects as they are
primitive components in analysts' workflow. To further improve comparison, we
have designed and implemented a Branching&Merging functionality. We tested
computational notebooks on the desktop and in VR, both with and without the
added Branching&Merging capability. We found VR significantly facilitated
navigation compared to desktop, and the ability to create branches enhanced
comparison.","['Sungwon In', 'Erick Krokos', 'Kirsten Whitley', 'Chris North', 'Yalong Yang']",2024-04-10T16:54:07Z,http://arxiv.org/abs/2404.07161v1,['cs.HC'],"computational notebooks,data analysis,user interface,interaction techniques,computing environments,virtual reality,data visualization,branching,merging,comparison"
"AipanVR: A Virtual Reality Experience for Preserving Uttarakhand's
  Traditional Art Form","This paper presents a demonstration of the developed prototype showcasing a
way to preserve the Intangible Cultural Heritage of Uttarakhand, India. Aipan
is a traditional art form practiced in the Kumaon region in the state of
Uttarakhand. It is typically used to decorate floors and walls at places of
worship or entrances of homes and is considered auspicious to begin any work or
event. This art is associated with a great degree of social, cultural as well
as religious significance and is passed from generation to generation. However,
in the present era of modernization and technological advancements, this art
form now stands on the verge of depletion. This study presents a humble attempt
to preserve this vanishing art form through the use of Virtual Reality (VR).
Ethnographic studies were conducted in Almora, Nainital, and Haldwani regions
of Uttarakhand to trace the origins as well as to gain a deeper understanding
of this art form. A total of ten (N =10) Aipan designers were interviewed.
Several interesting insights are revealed through these studies that show the
potential to be incorporated as a VR experience.","['Nishant Chaudhary', 'Mihir Raj', 'Richik Bhattacharjee', 'Anmol Srivastava', 'Rakesh Sah', 'Pankaj Badoni']",2024-04-19T05:53:10Z,http://arxiv.org/abs/2404.12643v1,['cs.HC'],"Virtual Reality,Intangible Cultural Heritage,Uttarakhand,Aipan,Traditional Art,Cultural Significance,Ethnographic Studies,VR Experience,Preservation,Modernization"
"Training Attention Skills in Individuals with Neurodevelopmental
  Disorders using Virtual Reality and Eye-tracking technology","Neurodevelopmental disorders (NDD), encompassing conditions like Intellectual
Disability, Attention Deficit Hyperactivity Disorder, and Autism Spectrum
Disorder, present challenges across various cognitive capacities. Attention
deficits are often common in individuals with NDD due to the sensory system
dysfunction that characterizes these disorders. Consequently, limited attention
capability can affect the overall quality of life and the ability to transfer
knowledge from one circumstance to another. The literature has increasingly
recognized the potential benefits of virtual reality (VR) in supporting NDD
learning and rehabilitation due to its interactive and engaging nature, which
is critical for consistent practice. In previous studies, we explored the usage
of a VR application called Wildcard to enhance attention skills in persons with
NDD. The application has been redesigned in this study, exploiting eye-tracking
technology to enable novel and more fine-grade interactions. A four-week
experiment with 38 NDD participants was conducted to evaluate its usability and
effectiveness in improving Visual Attention Skills. Results show the usability
and effectiveness of Wildcard in enhancing attention skills, advocating for
continued exploration of VR and eye-tracking technology's potential in NDD
interventions.","['Alberto Patti', 'Francesco Vona', 'Anna Barberio', 'Marco Domenico Buttiglione', 'Ivan Crusco', 'Marco Mores', 'Franca Garzotto']",2024-04-24T16:26:37Z,http://arxiv.org/abs/2404.15960v1,['cs.HC'],"Neurodevelopmental disorders,Attention deficits,Virtual reality,Eye-tracking technology,Intellectual Disability,Attention Deficit Hyperactivity Disorder,Autism Spectrum Disorder,Usability,Visual Attention Skills,Intervention"
"Evaluating Eye Movement Biometrics in Virtual Reality: A Comparative
  Analysis of VR Headset and High-End Eye-Tracker Collected Dataset","Previous studies have shown that eye movement data recorded at 1000 Hz can be
used to authenticate individuals. This study explores the effectiveness of eye
movement-based biometrics (EMB) by utilizing data from an eye-tracking
(ET)-enabled virtual reality (VR) headset (GazeBaseVR) and compares it to the
performance using data from a high-end eye tracker (GazeBase) that has been
downsampled to 250 Hz. The research also aims to assess the biometric potential
of both binocular and monocular eye movement data. GazeBaseVR dataset achieves
an equal error rate (EER) of 1.67% and a false rejection rate (FRR) at 10^-4
false acceptance rate (FAR) of 22.73% in a binocular configuration. This study
underscores the biometric viability of data obtained from eye-tracking-enabled
VR headset.","['Mehedi Hasan Raju', 'Dillon J Lohr', 'Oleg V Komogortsev']",2024-05-06T09:05:06Z,http://arxiv.org/abs/2405.03287v1,['cs.HC'],"eye movement biometrics,virtual reality,VR headset,high-end eye-tracker,dataset,authentication,biometric potential,binocular,monocular"
"VR-GPT: Visual Language Model for Intelligent Virtual Reality
  Applications","The advent of immersive Virtual Reality applications has transformed various
domains, yet their integration with advanced artificial intelligence
technologies like Visual Language Models remains underexplored. This study
introduces a pioneering approach utilizing VLMs within VR environments to
enhance user interaction and task efficiency. Leveraging the Unity engine and a
custom-developed VLM, our system facilitates real-time, intuitive user
interactions through natural language processing, without relying on visual
text instructions. The incorporation of speech-to-text and text-to-speech
technologies allows for seamless communication between the user and the VLM,
enabling the system to guide users through complex tasks effectively.
Preliminary experimental results indicate that utilizing VLMs not only reduces
task completion times but also improves user comfort and task engagement
compared to traditional VR interaction methods.","['Mikhail Konenkov', 'Artem Lykov', 'Daria Trinitatova', 'Dzmitry Tsetserukou']",2024-05-19T12:56:00Z,http://arxiv.org/abs/2405.11537v1,"['cs.RO', 'cs.AI', 'cs.ET']","Virtual Reality,Visual Language Model,Artificial Intelligence,Unity engine,Natural Language Processing,Speech-to-text,Text-to-speech,User Interaction,Task Efficiency,Immersive Virtual Reality"
Launching Your VR Neuroscience Laboratory,"The proliferation and refinement of affordable virtual reality (VR)
technologies and wearable sensors have opened new frontiers in cognitive and
behavioral neuroscience. This chapter offers a broad overview of VR for anyone
interested in leveraging it as a research tool. In the first section, it
examines the fundamental functionalities of VR and outlines important
considerations that inform the development of immersive content that stimulates
the senses. In the second section, the focus of the discussion shifts to the
implementation of VR in the context of the neuroscience lab. Practical advice
is offered on adapting commercial, off-theshelf devices to specific research
purposes. Further, methods are explored for recording, synchronizing, and
fusing heterogeneous forms of data obtained through the VR system or add-on
sensors, as well as for labeling events and capturing game play.","['Ying Choon Wu', 'Christopher Maymon', 'Jonathon Paden', 'Weichen Liu']",2024-05-21T19:37:09Z,http://arxiv.org/abs/2405.13171v1,"['cs.HC', 'q-bio.NC']","virtual reality,neuroscience,laboratory,wearable sensors,immersive content,research tool,off-the-shelf devices,data recording,event labeling"
"Development of a Virtual Reality Application for Oculomotor Examination
  Education Based on Student-Centered Pedagogy","This work-in-progress paper discusses the use of student-centered pedagogy to
teach clinical oculomotor examination via Virtual Reality (VR). Traditional
methods, such as PowerPoint slides and lab activities, are often insufficient
for providing hands-on experience due to the high cost of clinical equipment.
To address this, a VR-based application was developed using Unity and the HTC
Vive Pro headset, offering a cost-effective solution for practical learning.
The VR app allows students to engage in oculomotor examinations at their own
pace, accommodating diverse backgrounds and learning preferences. This
application enables students to collect and analyze data, providing a realistic
simulation of clinical practice. The user study results from Doctor of Physical
Therapy students indicate a high preference for the flexibility offered by the
VR app, suggesting its potential as a valuable educational tool. Additionally,
the paper explores the broader implications of using VR in engineering and
computing education, highlighting the benefits of immersive, interactive
learning environments.","['Austin Finlayson', 'Rui Wu', 'Chia-Cheng Lin', 'Brian Sylcott']",2024-05-26T00:53:19Z,http://arxiv.org/abs/2405.16392v1,"['cs.CE', 'cs.CY']","student-centered pedagogy,oculomotor examination,Virtual Reality,Unity,HTC Vive Pro,hands-on experience,VR-based application,practical learning,user study,educational tool"
The Virtual Reality Conjecture,"We take our world to be an objective reality, but is it? The assumption that
the physical world exists in and of itself has struggled to assimilate the
findings of modern physics for some time now. For example, an objective space
and time would just ""be"", but in relativity, space contracts and time dilates.
Likewise objective ""things"" should just inherently exist, but the entities of
quantum theory are probability of existence smears, that spread, tunnel,
superpose and entangle in physically impossible ways. Cosmology even tells us
that our entire physical universe just ""popped up"", from nowhere, about 14
billion years ago. This is not how an objectively real world should behave! Yet
traditional alternatives don't work much better. That the world is just an
illusion of the mind doesn't explain its consistent realism and Descartes
dualism, that another reality beyond the physical exists, just doubles the
existential problem. It is time to consider an option we might normally dismiss
out of hand. This essay explores the virtual reality conjecture, that the
physical world is the digital output of non-physical quantum processing. It
finds it neither illogical, nor unscientific, nor incompatible with current
physics. In this model, quantum entities are programs, movement is the transfer
of processing, interactions are processing overloads and the fields of physics
are network properties. It has no empty space, no singularities and all the
conservations of physics just conserve processing. Its prediction, that the
collision of high frequency light in a vacuum can create permanent matter, will
test it. If the physical world has the properties of a processing output,
physics must rewrite the story behind its equations.",['Brian Whitworth'],2011-10-13T03:13:54Z,http://arxiv.org/abs/1110.3307v1,['physics.gen-ph'],"virtual reality,objective reality,modern physics,relativity,quantum theory,cosmology,dualism,processing,conservation,equations"
"Initial validation of a virtual-reality learning environment for
  prostate biopsies: realism matters!",": Introduction-objectives: A virtual-reality learning environment dedicated
to prostate biopsies was designed to overcome the limitations of current
classical teaching methods. The aim of this study was to validate reliability,
face, content and construct of the simulator. Materials and methods: The
simulator is composed of a) a laptop computer, b) a haptic device with a stylus
that mimics the ultrasound probe, c) a clinical case database including three
dimensional (3D) ultrasound volumes and patient data and d) a learning
environment with a set of progressive exercises including a randomized 12-core
biopsy procedure. Both visual (3D biopsy mapping) and numerical (score)
feedback are given to the user. The simulator evaluation was conducted in an
academic urology department on 7 experts and 14 novices who each performed a
virtual biopsy procedure and completed a face and content validity
questionnaire. Results: The overall realism of the biopsy procedure was rated
at a median of 9/10 by non-experts (7.1-9.8). Experts rated the usefulness of
the simulator for the initial training of urologists at 8.2/10 (7.9-8.3), but
reported the range of motion and force feedback as significantly less realistic
than novices (p=0.01 and 0.03 respectively). Pearson's r correlation
coefficient between correctly placed biopsies on the right and left side of the
prostate for each user was 0.79 (p<0.001). The 7 experts had a median score of
64% (59-73), and the 14 novices a median score of 52% (43-67), without reaching
statistical significance (p=0,19). Conclusion: The newly designed virtual
reality learning environment proved its versatility and its reliability, face
and content were validated. Demonstrating the construct validity will require
improvements to the realism and scoring system used.","['Gaelle Fiard', 'Sonia-Yuki Selmi', 'Emmanuel Promayon', 'Lucile Vadcard', 'Jean-Luc Descotes', 'Jocelyne Troccaz']",2013-11-04T18:29:54Z,http://arxiv.org/abs/1311.0806v1,['cs.CY'],"virtual-reality,learning environment,prostate biopsies,haptic device,ultrasound probe,3D ultrasound volumes,simulator evaluation,urologists,force feedback"
Safe Walking In VR using Augmented Virtuality,"New technologies allow ordinary people to access Virtual Reality at
affordable prices in their homes. One of the most important tasks when
interacting with immersive Virtual Reality is to navigate the virtual
environments (VEs). Arguably, the best methods to accomplish this use of direct
control interfaces. Among those, natural walking (NW) makes for enjoyable user
experience. However, common techniques to support direct control interfaces in
VEs feature constraints that make it difficult to use those methods in cramped
home environments. Indeed, NW requires unobstructed and open space. To approach
this problem, we propose a new virtual locomotion technique, Combined Walking
in Place (CWIP). CWIP allows people to take advantage of the available physical
space and empowers them to use NW to navigate in the virtual world. For longer
distances, we adopt Walking in Place (WIP) to enable them to move in the
virtual world beyond the confines of a cramped real room. However, roaming in
immersive alternate reality, while moving in the confines of a cluttered
environment can lead people to stumble and fall. To approach these problems, we
developed Augmented Virtual Reality (AVR), to inform users about real-world
hazards, such as chairs, drawers, walls via proxies and signs placed in the
virtual world. We propose thus CWIP-AVR as a way to safely explore VR in the
cramped confines of your own home. To our knowledge, this is the first approach
to combined different locomotion modalities in a safe manner. We evaluated it
in a user study with 20 participants to validate their ability to navigate a
virtual world while walking in a confined and cluttered real space. Our results
show that CWIP-AVR allows people to navigate VR safely, switching between
locomotion modes flexibly while maintaining a good immersion.","['Maurício Sousa', 'Daniel Mendes', 'Joaquim Jorge']",2019-11-29T10:09:19Z,http://arxiv.org/abs/1911.13032v1,"['cs.HC', 'cs.GR']","Virtual Reality,Augmented Virtuality,Natural Walking,Walking in Place,Combined Walking in Place,Locomotion,User Study,Immersion,Virtual Environments"
"Learned human-agent decision-making, communication and joint action in a
  virtual reality environment","Humans make decisions and act alongside other humans to pursue both
short-term and long-term goals. As a result of ongoing progress in areas such
as computing science and automation, humans now also interact with non-human
agents of varying complexity as part of their day-to-day activities;
substantial work is being done to integrate increasingly intelligent machine
agents into human work and play. With increases in the cognitive, sensory, and
motor capacity of these agents, intelligent machinery for human assistance can
now reasonably be considered to engage in joint action with humans---i.e., two
or more agents adapting their behaviour and their understanding of each other
so as to progress in shared objectives or goals. The mechanisms, conditions,
and opportunities for skillful joint action in human-machine partnerships is of
great interest to multiple communities. Despite this, human-machine joint
action is as yet under-explored, especially in cases where a human and an
intelligent machine interact in a persistent way during the course of
real-time, daily-life experience. In this work, we contribute a virtual reality
environment wherein a human and an agent can adapt their predictions, their
actions, and their communication so as to pursue a simple foraging task. In a
case study with a single participant, we provide an example of human-agent
coordination and decision-making involving prediction learning on the part of
the human and the machine agent, and control learning on the part of the
machine agent wherein audio communication signals are used to cue its human
partner in service of acquiring shared reward. These comparisons suggest the
utility of studying human-machine coordination in a virtual reality
environment, and identify further research that will expand our understanding
of persistent human-machine joint action.","['Patrick M. Pilarski', 'Andrew Butcher', 'Michael Johanson', 'Matthew M. Botvinick', 'Andrew Bolt', 'Adam S. R. Parker']",2019-05-07T16:53:48Z,http://arxiv.org/abs/1905.02691v1,"['cs.AI', 'cs.HC', 'cs.LG']","decision-making,communication,joint action,virtual reality,human-agent,intelligent machinery,prediction learning,control learning,coordination"
Illustrations of non-Euclidean geometry in virtual reality,"Mathematical objects are generally abstract and not very approachable.
Illustrations and interactive visualizations help both students and
professionals to comprehend mathematical material and to work with it. This
approach lends itself particularly well to geometrical objects. An example for
this category of mathematical objects are hyperbolic geometric spaces. When
Euclid lay down the foundations of mathematics, his formulation of geometry
reflected the surrounding space, as humans perceive it. For about two
millennia, it remained unclear whether there are alternative geometric spaces
that carry their own, unique mathematical properties and that do not reflect
human every-day perceptions. Finally, in the early 19th century, several
mathematicians described such geometries, which do not follow Euclid's rules
and which were at first interesting solely from a pure mathematical point of
view. These descriptions were not very accessible as mathematicians approached
the geometries via complicated collections of formulae. Within the following
decades, visualization aided the new concepts and two-dimensional versions of
these illustrations even appeared in artistic works. Furthermore, certain
aspects of Einstein's theory of relativity provided applications for
non-Euclidean geometric spaces. With the rise of computer graphics towards the
end of the twentieth century, three-dimensional illustrations became available
to explore these geometries and their non-intuitive properties. However, just
as the canvas confines the two-dimensional depictions, the computer monitor
confines these three-dimensional visualizations. Only virtual reality recently
made it possible to present immersive experiences of non-Euclidean geometries.
In virtual reality, users have completely new opportunities to encounter
geometric properties and effects that are not present in their surrounding
Euclidean world.",['Martin Skrodzki'],2020-08-04T06:39:11Z,http://arxiv.org/abs/2008.01363v2,"['math.HO', 'cs.GR', 'math.DG', '51M10', 'I.3.7; J.5']","Non-Euclidean geometry,Virtual reality,Geometric spaces,Visualization,Mathematical objects,Hyperbolic geometry,Computer graphics,Immersive experiences,Euclidean world,Interactive visualizations"
ASIAVR: Asian Studies Virtual Reality Game a Learning Tool,"The study aims to develop an application that will serve as an alternative
learning tool for learning Asian Studies. The delivery of lessons into a
virtual reality game depends on the pace of students. The developed application
comprises several more features that enable users to get valuable information
from an immersive environment. The researchers used Rapid Application
Development (RAD) in developing the application. It follows phases such as
requirement planning, user design, construction, and cutover. Two sets of
questionnaires were developed, one for the teachers and another for the
students. Then, testing and evaluation were conducted through purposive
sampling to select the respondents. The application was overall rated as 3.56
which is verbally interpreted as very good. The result was based on the system
evaluation using ISO 9126 in terms of functionality, usability, content,
reliability, and performance. The developed application meets the objectives to
provide an alternative learning tool for learning Asian Studies. The
application is well commended and accepted by the end-users to provide an
interactive and immersive environment for students to learn at their own pace.
Further enhancement of the audio, gameplay, and graphics of the tool. Schools
should take into consideration the adoption of the Asian Studies Virtual
Reality is a good alternative tool for their teachers and students to teach and
learn Asian Studies. The use of more 3D objects relevant to the given
information to enhance the game experience may be considered. A databank for
the quiz questions that will be loaded into the game should also be considered.","['Kenn Migan Vincent C. Gumonan', 'Aleta C. Fabregas']",2020-11-23T04:24:03Z,http://arxiv.org/abs/2012.01162v1,"['cs.CY', 'cs.HC', 'I.3.7']","Asian Studies,Virtual Reality,Learning Tool,Rapid Application Development,ISO 9126,Immersive Environment,3D Objects,Quiz Questions,Game Experience"
Exploring and Interrogating Astrophysical Data in Virtual Reality,"Scientists across all disciplines increasingly rely on machine learning
algorithms to analyse and sort datasets of ever increasing volume and
complexity. Although trends and outliers are easily extracted, careful and
close inspection will still be necessary to explore and disentangle detailed
behavior, as well as identify systematics and false positives. We must
therefore incorporate new technologies to facilitate scientific analysis and
exploration. Astrophysical data is inherently multi-parameter, with the
spatial-kinematic dimensions at the core of observations and simulations. The
arrival of mainstream virtual-reality (VR) headsets and increased GPU power, as
well as the availability of versatile development tools for video games, has
enabled scientists to deploy such technology to effectively interrogate and
interact with complex data. In this paper we present development and results
from custom-built interactive VR tools, called the iDaVIE suite, that are
informed and driven by research on galaxy evolution, cosmic large-scale
structure, galaxy-galaxy interactions, and gas/kinematics of nearby galaxies in
survey and targeted observations. In the new era of Big Data ushered in by
major facilities such as the SKA and LSST that render past analysis and
refinement methods highly constrained, we believe that a paradigm shift to new
software, technology and methods that exploit the power of visual perception,
will play an increasingly important role in bridging the gap between
statistical metrics and new discovery. We have released a beta version of the
iDaVIE software system that is free and open to the community.","['T. H. Jarrett', 'A. Comrie', 'L. Marchetti', 'A. Sivitilli', 'S. Macfarlane', 'F. Vitello', 'U. Becciani', 'A. R. Taylor', 'J. M. van der Hulst', 'P. Serra', 'N. Katz', 'M. Cluver']",2020-12-18T16:40:32Z,http://arxiv.org/abs/2012.10342v3,"['astro-ph.IM', 'astro-ph.GA']","machine learning algorithms,astrophysical data,virtual reality,GPU power,spatial-kinematic dimensions,interactive VR tools,galaxy evolution,cosmic large-scale structure,Big Data,visual perception"
"WestDrive X LoopAR: An open-access virtual reality project in Unity for
  evaluating user interaction methods during TOR","With the further development of highly automated vehicles, drivers will
engage in non-related tasks while being driven. Still, drivers have to take
over control when requested by the car. Here the question arises, how
potentially distracted drivers get back into the control-loop quickly and
safely when the car requests a takeover. To investigate effective human-machine
interactions in mobile, versatile, and cost-efficient setup is needed. We
developed a virtual reality toolkit for the Unity 3D game engine containing all
necessary code and assets to enable fast adaptations to various human-machine
interaction experiments, including close monitoring of the subject. The
presented project contains all needed functionalities for realistic traffic
behavior, cars, and pedestrians, as well as a large, open-source, scriptable,
and modular VR environment. It covers roughly 25 square km, a package of 125
animated pedestrians and numerous vehicles, including motorbikes, trucks, and
cars. It also contains all needed nature assets to make it both highly dynamic
and realistic. The presented repository contains a C++ library made for LoopAR
that enables force feedback for gaming steering wheels as a fully supported
component. It also includes All necessary scripts for eye-tracking in the used
devices. All main functions are integrated into the graphical user interface of
the Unity Editor or are available as prefab variants to ease the use of the
embedded functionalities. The primary purpose of this project is to serve as
open access, cost-efficient toolkit that enables interested researchers to
conduct realistic virtual reality research studies without costly and immobile
simulators.","['Farbod N. Nezami', 'Maximilian A. Wächter', 'Nora Maleki', 'Philipp Spaniol', 'Lea M. Kühne', 'Anke Haas', 'Johannes M. Pingel', 'Linus Tiemann', 'Frederik Nienhaus', 'Lynn Keller', 'Sabine König', 'Peter König', 'Gordon Pipa']",2020-12-22T14:27:53Z,http://arxiv.org/abs/2012.12041v1,['cs.HC'],"highly automated vehicles,human-machine interaction,virtual reality,Unity 3D,LoopAR,force feedback,eye-tracking,graphical user interface,open access,cost-efficient"
Beyond the Metaverse: XV (eXtended meta/uni/Verse),"We propose the term and concept XV (eXtended meta/omni/uni/Verse) as an
alternative to, and generalization of, the shared/social virtual reality widely
known as ``metaverse''. XV is shared/social XR. We, and many others, use XR
(eXtended Reality) as a broad umbrella term and concept to encompass all the
other realities, where X is an ``anything'' variable, like in mathematics, to
denote any reality, X $\in$ \{physical, virtual, augmented, \ldots \} reality.
Therefore XV inherits this generality from XR. We begin with a very simple
organized taxonomy of all these realities in terms of two simple building
blocks: (1) physical reality (PR) as made of ``atoms'', and (2) virtual reality
(VR) as made of ``bits''. Next we introduce XV as combining all these realities
with extended society as a three-dimensional space and taxonomy of (1)
``atoms'' (physical reality), (2) ``bits'' (virtuality), and (3) ``genes''
(sociality). Thus those working in the liminal space between Virtual Reality
(VR), Augmented Reality (AR), metaverse, and their various extensions, can
describe their work and research as existing in the new field of XV. XV
includes the metaverse along with extensions of reality itself like shared
seeing in the infrared, ultraviolet, and shared seeing of electromagnetic radio
waves, sound waves, and electric currents in motors. For example, workers in a
mechanical room can look at a pump and see a superimposed time-varying waveform
of the actual rotating magnetic field inside its motor, in real time, while
sharing this vision across multiple sites.
  Presented at IEEE Standards Association, Behind and Beyond the Metaverse: XV
(eXtended meta/uni/Verse), Thurs. Dec. 8, 2022, 2:15-3:30pm, EST.","['Steve Mann', 'Yu Yuan', 'Tom Furness', 'Joseph Paradiso', 'Thomas Coughlin']",2022-12-15T16:49:32Z,http://arxiv.org/abs/2212.07960v1,"['eess.SY', 'cs.HC', 'cs.SY', 'eess.IV']","XV,eXtended Reality,Metaverse,Virtual Reality,Augmented Reality,Shared reality,Society,Taxonomy,Electromagnetic waves,IEEE Standard"
"Modeling novel physics in virtual reality labs: An affective analysis of
  student learning","We report on a study of the effects of laboratory activities that model
fictitious laws of physics in a virtual reality environment on (1) students'
epistemology about the role of experimental physics in class and in the world;
(2) students' self-efficacy; and (3) the quality of student engagement with the
lab activities. We create opportunities for students to practice physics as a
means of creating and validating new knowledge by simulating real and
fictitious physics in virtual reality (VR). This approach seeks to steer
students away from a confirmation mindset in labs by eliminating any form of
prior or outside models to confirm. We refer to the activities using this
approach as Novel Observations in Mixed Reality (NOMR) labs. We examined NOMR's
effects in 100-level and 200-level undergraduate courses. Using pre-post
measurements we find that after NOMR labs, students in both populations were
more expertlike in their epistemology about experimental physics and held
stronger self-efficacy about their abilities to do the kinds of things
experimental physicists do. Through the lens of the psychological theory of
flow, we found that students engage as productively with NOMR labs as with
traditional hands-on labs. This engagement persisted after the novelty of VR in
the classroom wore off, suggesting that these effects are due to the
pedagogical design rather than the medium of the intervention. We conclude that
these NOMR labs offer an approach to physics laboratory instruction that
centers the development of students' understanding of and comfort with the
authentic practice of science.","['Jared P. Canright', 'Suzanne White Brahmia']",2023-10-12T00:27:24Z,http://arxiv.org/abs/2310.07952v1,['physics.ed-ph'],"virtual reality,laboratory activities,physics,epistemology,self-efficacy,engagement,novel observations,mixed reality,expertlike,pedagogical design"
Virtual Reality in the World of Holograms,"If we assume that the initial conditions for the universe were such that
there was no volume-extensive entropy `at the beginning of time' (which is true
in Linde's chaotic inflation), we can formulate a covariant holographic bound
on the entanglement entropy inside or outside closed space-like surfaces. This
bound should hold even for regions where the coarse-grained entropy exceeds the
surface area. We find that Bousso's bound gives strong support for this
conjecture. We also present a speculative interpretation of the entropy bound,
according to which any observer of interest can be surrounded by a holographic
screen, providing a non-redundant description of the rest of the universe.",['Michal Fabinger'],2001-04-11T21:44:59Z,http://arxiv.org/abs/hep-th/0104111v1,['hep-th'],"Virtual Reality,Holograms,Entropy,Covariant,Holographic Bound,Entanglement,Speculative Interpretation,Universe,Observer"
Narratives within immersive technologies,"The main goal of this project is to research technical advances in order to
enhance the possibility to develop narratives within immersive mediated
environments. An important part of the research is concerned with the question
of how a script can be written, annotated and realized for an immersive
context. A first description of the main theoretical framework and the ongoing
work and a first script example is provided. This project is part of the
program for presence research, and it will exploit physiological feedback and
Computational Intelligence within virtual reality.",['Joan Llobera'],2007-04-19T14:27:25Z,http://arxiv.org/abs/0704.2542v1,['cs.HC'],"immersive technologies,narratives,technical advances,immersive mediated environments,script,annotated,theoretical framework,physiological feedback,Computational Intelligence,virtual reality"
DNA Nanorobotics,"This paper presents a molecular mechanics study for new nanorobotic
structures using molecular dynamics (MD) simulations coupled to virtual reality
(VR) techniques. The operator can design and characterize through molecular
dynamics simulation the behavior of bionanorobotic components and structures
through 3-D visualization. The main novelty of the proposed simulations is
based on the mechanical characterization of passive/active robotic devices
based on double stranded DNA molecules. Their use as new DNA-based nanojoint
and nanotweezer are simulated and results discussed.","['M. Hamdi', 'A. Ferreira']",2007-08-10T15:12:22Z,http://arxiv.org/abs/0708.1458v1,['cond-mat.mtrl-sci'],"DNA Nanorobotics,Molecular Dynamics,Virtual Reality,Bionanorobotic Components,Nanojoint,Nanotweezer,Mechanical Characterization,Double Stranded DNA,Simulation,Molecular Mechanics"
Teaching Physics Using Virtual Reality,"We present an investigation of game-like simulations for physics teaching. We
report on the effectiveness of the interactive simulation ""Real Time
Relativity"" for learning special relativity. We argue that the simulation not
only enhances traditional learning, but also enables new types of learning that
challenge the traditional curriculum. The lessons drawn from this work are
being applied to the development of a simulation for enhancing the learning of
quantum mechanics.","['C. M. Savage', 'D. McGrath', 'T. J. McIntyre', 'M. Wegener', 'M. Williamson']",2009-10-30T03:08:32Z,http://arxiv.org/abs/0910.5776v1,['physics.ed-ph'],"Physics teaching,Virtual Reality,Game-like simulations,Real Time Relativity,Special relativity,Interactive simulation,Traditional learning,Quantum mechanics,Curriculum,Lessons"
Using Coloured Petri Nets for design of parallel raytracing environment,"This paper deals with the parallel raytracing part of virtual-reality system
PROLAND, developed at the home institution of authors. It describes an actual
implementation of the raytracing part and introduces a Coloured Petri Nets
model of the implementation. The model is used for an evaluation of the
implementation by means of simulation-based performance analysis and also forms
the basis for future improvements of its parallelization strategy.","['Stefan Korecko', 'Branislav Sobota']",2010-03-06T16:24:29Z,http://arxiv.org/abs/1003.1397v1,"['cs.DC', '68U20, 68U05', 'I.6.3']","Coloured Petri Nets,parallel raytracing,virtual-reality system,PROLAND,implementation,simulation-based performance analysis,parallelization strategy"
Work Integrated Learning (WIL) In Virtual Reality (VR),"The focus of this report is to initially discuss the concepts WIL and VR,
their main characteristics and current applications. Moreover, the pros and
cons of VWIL are also analyzed. Finally, the report presents some
recommendation including further researches into areas where VWIL has potential
to be successful in the future.",['Waleed Abdullah Al Shehri'],2012-11-11T12:36:36Z,http://arxiv.org/abs/1211.2412v1,['cs.HC'],"Work Integrated Learning,Virtual Reality,WIL,VR,characteristics,applications,pros,cons,VWIL,recommendation"
Adaptive Scheduling in Real-Time Systems Through Period Adjustment,"Real time system technology traditionally developed for safety critical
systems, has now been extended to support multimedia systems and virtual
reality. A large number of real-time application, related to multimedia and
adaptive control system, require more flexibility than classical real-time
theory usually permits. This paper proposes an efficient adaptive scheduling
framework in real-time systems based on period adjustment. Under this model
periodic task can change their execution rates based on their importance value
to keep the system underloaded. We propose Period_Adjust algorithm, which
consider the tasks whose periods are bounded as well as the tasks whose periods
are not bounded.",['Shri Prakash Dwivedi'],2012-12-14T15:33:45Z,http://arxiv.org/abs/1212.3502v1,['cs.OS'],"Adaptive Scheduling,Real-Time Systems,Period Adjustment,Multimedia Systems,Virtual Reality,Real-Time Application,Adaptive Control System,Scheduling Framework,Periodic Task"
Immersive VR Visualizations by VFIVE. Part 2: Applications,"VFIVE is a scientific visualization application for CAVE-type immersive
virtual reality systems. The source codes are freely available. VFIVE is used
as a research tool in various VR systems. It also lays the groundwork for
developments of new visualization software for CAVEs. In this paper, we pick up
five CAVE systems in four different institutions in Japan. Applications of
VFIVE in each CAVE system are summarized. Special emphases will be placed on
scientific and technical achievements made possible by VFIVE.","['Akira Kageyama', 'Nobuaki Ohno', 'Shintaro Kawahara', 'Kazuo Kashiyama', 'Hiroaki Ohtani']",2013-01-25T11:07:37Z,http://arxiv.org/abs/1301.6008v1,"['cs.GR', 'physics.comp-ph']","immersive virtual reality,VFIVE,scientific visualization,CAVE,research tool,visualization software,Japan,technical achievements"
Introduction to Clifford's Geometric Algebra,"Geometric algebra was initiated by W.K. Clifford over 130 years ago. It
unifies all branches of physics, and has found rich applications in robotics,
signal processing, ray tracing, virtual reality, computer vision, vector field
processing, tracking, geographic information systems and neural computing. This
tutorial explains the basics of geometric algebra, with concrete examples of
the plane, of 3D space, of spacetime, and the popular conformal model.
Geometric algebras are ideal to represent geometric transformations in the
general framework of Clifford groups (also called versor or Lipschitz groups).
Geometric (algebra based) calculus allows, e.g., to optimize learning
algorithms of Clifford neurons, etc.
  Keywords: Hypercomplex algebra, hypercomplex analysis, geometry, science,
engineering.",['Eckhard Hitzer'],2013-06-07T09:01:33Z,http://arxiv.org/abs/1306.1660v1,"['math.RA', '15A66, 11E88']","Clifford's Geometric Algebra,Physics,Robotics,Signal Processing,Ray Tracing,Virtual Reality,Computer Vision,Vector Field Processing,Geographic Information Systems,Neural Computing"
Preprint Big City 3D Visual Analysis,"This is the preprint version of our paper on EUROGRAPHICS 2015. A big city
visual analysis platform based on Web Virtual Reality Geographical Information
System (WEBVRGIS) is presented. Extensive model editing functions and spatial
analysis functions are available, including terrain analysis, spatial analysis,
sunlight analysis, traffic analysis, population analysis and community
analysis.","['Zhihan Lv', 'Xiaoming Li', 'Baoyun Zhang', 'Weixi Wang', 'Shengzhong Feng', 'Jinxing Hu']",2015-04-06T15:53:09Z,http://arxiv.org/abs/1504.01379v2,"['cs.GR', 'cs.HC', 'I.3.7']","preprint,big city,3D visual analysis,Web Virtual Reality Geographical Information System,WEBVRGIS,model editing,spatial analysis,terrain analysis,sunlight analysis,traffic analysis"
"RDF annotation of Second Life objects: Knowledge Representation meets
  Social Virtual reality","We have designed and implemented an application running inside Second Life
that supports user annotation of graphical objects and graphical visualization
of concept ontologies, thus providing a formal, machine-accessible description
of objects. As a result, we offer a platform that combines the graphical
knowledge representation that is expected from a MUVE artifact with the
semantic structure given by the Resource Framework Description (RDF)
representation of information.","['Carlo Bernava', 'Giacomo Fiumara', 'Dario Maggiorini', 'Alessandro Provetti', 'Laura Ripamonti']",2015-04-09T15:52:59Z,http://arxiv.org/abs/1504.02358v1,"['cs.AI', 'cs.HC', 'H.5.1; I.2.4']","RDF,annotation,Second Life,objects,knowledge representation,social virtual reality,concept ontologies,machine-accessible description,semantic structure"
Preprint Virtual Reality Based GIS Analysis Platform,"This is the preprint version of our paper on ICONIP2015. The proposed
platform supports the integrated VRGIS functions including 3D spatial analysis
functions, 3D visualization for spatial process and serves for 3D globe and
digital city. The 3D analysis and visualization of the concerned city massive
information are conducted in the platform. The amount of information that can
be visualized with this platform is overwhelming, and the GIS based
navigational scheme allows to have great flexibility to access the different
available data sources.","['Weixi Wang', 'Zhihan Lv', 'Xiaoming Li', 'Weiping Xu', 'Baoyun Zhang']",2015-08-09T14:14:33Z,http://arxiv.org/abs/1508.02024v1,['cs.HC'],"preprint,virtual reality,GIS,analysis platform,3D spatial analysis functions,3D visualization,spatial process,3D globe,digital city"
ASIST: Automatic Semantically Invariant Scene Transformation,"We present ASIST, a technique for transforming point clouds by replacing
objects with their semantically equivalent counterparts. Transformations of
this kind have applications in virtual reality, repair of fused scans, and
robotics. ASIST is based on a unified formulation of semantic labeling and
object replacement; both result from minimizing a single objective. We present
numerical tools for the efficient solution of this optimization problem. The
method is experimentally assessed on new datasets of both synthetic and real
point clouds, and is additionally compared to two recent works on object
replacement on data from the corresponding papers.","['Or Litany', 'Tal Remez', 'Daniel Freedman', 'Lior Shapira', 'Alex Bronstein', 'Ran Gal']",2015-12-04T19:14:57Z,http://arxiv.org/abs/1512.01515v1,['cs.CV'],"ASIST,Automatic,Semantically Invariant,Scene Transformation,Point Clouds,Semantic Labeling,Object Replacement,Optimization Problem,Robotics,Virtual Reality"
"Deep3D: Fully Automatic 2D-to-3D Video Conversion with Deep
  Convolutional Neural Networks","As 3D movie viewing becomes mainstream and Virtual Reality (VR) market
emerges, the demand for 3D contents is growing rapidly. Producing 3D videos,
however, remains challenging. In this paper we propose to use deep neural
networks for automatically converting 2D videos and images to stereoscopic 3D
format. In contrast to previous automatic 2D-to-3D conversion algorithms, which
have separate stages and need ground truth depth map as supervision, our
approach is trained end-to-end directly on stereo pairs extracted from 3D
movies. This novel training scheme makes it possible to exploit orders of
magnitude more data and significantly increases performance. Indeed, Deep3D
outperforms baselines in both quantitative and human subject evaluations.","['Junyuan Xie', 'Ross Girshick', 'Ali Farhadi']",2016-04-13T04:35:07Z,http://arxiv.org/abs/1604.03650v1,['cs.CV'],"Deep Convolutional Neural Networks,2D-to-3D Video Conversion,3D contents,Virtual Reality (VR),3D movies,depth map,stereo pairs,end-to-end training,human subject evaluations"
Solving Poisson's Equation on the Microsoft HoloLens,"We present a mixed reality application (HoloFEM) for the Microsoft HoloLens.
The application lets a user define and solve a physical problem governed by
Poisson's equation with the surrounding real world geometry as input data.
Holograms are used to visualise both the problem and the solution. The finite
element method is used to solve Poisson's equation. Solving and visualising
partial differential equations in mixed reality could have potential usage in
areas such as building planning and safety engineering.","['Anders Logg', 'Carl Lundholm', 'Magne Nordaas']",2017-11-17T13:44:33Z,http://arxiv.org/abs/1711.07790v1,"['cs.GR', 'cs.MS']","Poisson's equation,Microsoft HoloLens,mixed reality,HoloFEM,finite element method,partial differential equations,holograms,building planning,safety engineering"
Inverse Augmented Reality: A Virtual Agent's Perspective,"We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']",2018-08-10T05:23:37Z,http://arxiv.org/abs/1808.03413v1,['cs.HC'],"inverse augmented reality,virtual agent,virtual world,virtual objects,real objects,traditional augmented reality,virtual reality,mixed reality,human-centered,physical structure"
Implementation of a Daemon for OpenBCI,"This document describes a technical study of the electroencephalographic
(EEG) headset OpenBCI (New York, US). In comparison to research grade EEG, the
OpenBCI headset is affordable thus suitable for the general public use. In this
study we designed a daemon, that is, a background and continuous task
communicating with the headset, acquiring, filtering and analyzing the EEG
data. This study was promoted by the IHMTEK Company (Vienne, France) in 2016
within a thesis on the integration of EEG-based brain-computer interfaces in
virtual reality for the general public.","['Maxime Chabance', 'Grégoire Cattan', 'Bastien Maureille']",2019-04-08T12:45:51Z,http://arxiv.org/abs/1904.04015v1,['cs.HC'],"daemon,OpenBCI,electroencephalographic,EEG headset,continuous task,filtering,analyzing,IHMTEK Company,brain-computer interfaces,virtual reality"
SalNet360: Saliency Maps for omni-directional images with CNN,"The prediction of Visual Attention data from any kind of media is of valuable
use to content creators and used to efficiently drive encoding algorithms. With
the current trend in the Virtual Reality (VR) field, adapting known techniques
to this new kind of media is starting to gain momentum. In this paper, we
present an architectural extension to any Convolutional Neural Network (CNN) to
fine-tune traditional 2D saliency prediction to Omnidirectional Images (ODIs)
in an end-to-end manner. We show that each step in the proposed pipeline works
towards making the generated saliency map more accurate with respect to ground
truth data.","['Rafael Monroy', 'Sebastian Lutz', 'Tejo Chalasani', 'Aljosa Smolic']",2017-09-19T16:21:09Z,http://arxiv.org/abs/1709.06505v2,['cs.CV'],"Saliency Maps,Omni-directional images,CNN,Visual Attention,Encoding algorithms,Virtual Reality,Convolutional Neural Network,2D saliency prediction,Omnidirectional Images,Ground truth data"
"Population-based Respiratory 4D Motion Atlas Construction and its
  Application for VR Simulations of Liver Punctures","Virtual reality (VR) training simulators of liver needle insertion in the
hepatic area of breathing virtual patients currently need 4D data acquisitions
as a prerequisite. Here, first a population-based breathing virtual patient 4D
atlas can be built and second the requirement of a dose-relevant or expensive
acquisition of a 4D data set for a new static 3D patient can be mitigated by
warping the mean atlas motion. The breakthrough contribution of this work is
the construction and reuse of population-based learned 4D motion models.","['Andre Mastmeyer', 'Matthias Wilms', 'Heinz Handels']",2017-12-05T19:59:20Z,http://arxiv.org/abs/1712.01893v2,['cs.CV'],"population-based,respiratory,4D motion atlas,VR simulations,liver punctures,virtual reality,training simulators,data acquisitions,4D atlas,motion models."
Real-time Egocentric Gesture Recognition on Mobile Head Mounted Displays,"Mobile virtual reality (VR) head mounted displays (HMD) have become popular
among consumers in recent years. In this work, we demonstrate real-time
egocentric hand gesture detection and localization on mobile HMDs. Our main
contributions are: 1) A novel mixed-reality data collection tool to automatic
annotate bounding boxes and gesture labels; 2) The largest-to-date egocentric
hand gesture and bounding box dataset with more than 400,000 annotated frames;
3) A neural network that runs real time on modern mobile CPUs, and achieves
higher than 76% precision on gesture recognition across 8 classes.","['Rohit Pandey', 'Marie White', 'Pavel Pidlypenskyi', 'Xue Wang', 'Christine Kaeser-Chen']",2017-12-13T19:06:37Z,http://arxiv.org/abs/1712.04961v1,['cs.CV'],"real-time,egocentric gesture recognition,mobile head mounted displays,virtual reality,hand gesture detection,localization,neural network,bounding box,dataset,precision"
"Immersion on the Edge: A Cooperative Framework for Mobile Immersive
  Computing","Immersive computing (IC) technologies such as virtual reality and augmented
reality are gaining tremendous popularity. In this poster, we present CoIC, a
Cooperative framework for mobile Immersive Computing. The design of CoIC is
based on a key insight that IC tasks among different applications or users
might be similar or redundant. CoIC enhances the performance of mobile IC
applications by caching and sharing computation-intensive IC results on the
edge. Our preliminary evaluation results on an AR application show that CoIC
can reduce the recognition and rendering latency by up to 52.28% and 75.86%
respectively on current mobile devices.","['Zeqi Lai', 'Yong Cui', 'Ziyi Wang', 'Xiaoyu Hu']",2018-07-12T12:32:59Z,http://arxiv.org/abs/1807.04572v1,['cs.NI'],"Immersive computing,Cooperative framework,Mobile,Virtual reality,Augmented reality,Edge computing,Performance enhancement,Caching,Computation-intensive."
"A man with a computer face (to the 80th anniversary of Ivan Edward
  Sutherland)","The article presents the main milestones of the science and technology
biography of Ivan Edward Sutherland. The influence of the family and the school
on the development of its research competencies is shown, and little-known
biographical facts explaining the evolution of his scientific interests is
presented: from dynamic object-oriented graphic systems through systems of
virtual reality to asynchronous circuits.","['S. O. Semerikov', 'A. M. Striuk', 'K. I. Slovak', 'N. V. Rashevska', 'Yu. V. Yechkalo']",2018-07-03T18:00:40Z,http://arxiv.org/abs/1807.07824v1,"['cs.GL', 'A.0; K.2']","Ivan Edward Sutherland,computer face,milestones,science and technology biography,research competencies,biographical facts,dynamic object-oriented graphic systems,virtual reality,asynchronous circuits"
Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs,"We present an end-to-end neural network-based model for inferring an
approximate 3D mesh representation of a human face from single camera input for
AR applications. The relatively dense mesh model of 468 vertices is well-suited
for face-based AR effects. The proposed model demonstrates super-realtime
inference speed on mobile GPUs (100-1000+ FPS, depending on the device and
model variant) and a high prediction quality that is comparable to the variance
in manual annotations of the same image.","['Yury Kartynnik', 'Artsiom Ablavatski', 'Ivan Grishchenko', 'Matthias Grundmann']",2019-07-15T20:08:17Z,http://arxiv.org/abs/1907.06724v1,['cs.CV'],"neural network,3D mesh,human face,AR applications,mobile GPUs,real-time,inference speed,prediction quality,manual annotations,image"
"WiredSwarm: High Resolution Haptic Feedback Provided by a Swarm of
  Drones to the User's Fingers for VR interaction","We propose a concept of a novel interaction strategy for providing rich
haptic feedback in Virtual Reality (VR), when each user's finger is connected
to micro-quadrotor with a wire. Described technology represents the first
flying wearable haptic interface. The solution potentially is able to deliver
high resolution force feedback to each finger during fine motor interaction in
VR. The tips of tethers are connected to the centers of quadcopters under their
bottom. Therefore, flight stability is increasing and the interaction forces
are becoming stronger which allows to use smaller drones.","['Evgeny Tsykunov', 'Dzmitry Tsetserukou']",2019-11-12T04:25:37Z,http://arxiv.org/abs/1911.04667v1,"['cs.RO', 'cs.HC']","Haptic feedback,Swarm,Drones,Virtual Reality,Wearable,Force feedback,Quadcopters,Interaction strategy,Fine motor interaction,Flight stability"
Haptic Sketches on the Arm for Manipulation in Virtual Reality,"We propose a haptic system that applies forces or skin deformation to the
user's arm, rather than at the fingertips, for believable interaction with
virtual objects as an alternative to complex thimble devices. Such a haptic
system would be able to convey information to the arm instead of the
fingertips, even though the user manipulates virtual objects using their hands.
We developed a set of haptic sketches to determine which directions of skin
deformation are deemed more believable during a grasp and lift task. Subjective
reports indicate that normal forces were the most believable feedback to
represent this interaction.","['Mine Sarac', 'Allison M. Okamura', 'Massimiliano Di Luca']",2019-11-14T22:44:14Z,http://arxiv.org/abs/1911.08528v1,['cs.RO'],"haptic system,skin deformation,virtual reality,manipulation,virtual objects,haptic sketches,grasp,lift,normal forces,interaction"
Feature Extraction in Augmented Reality,"Augmented Reality (AR) is used for various applications associated with the
real world. In this paper, first, describe characteristics and essential
services of AR. Brief history on Virtual Reality (VR) and AR is also mentioned
in the introductory section. Then, AR Technologies along with its workflow is
depicted, which includes the complete AR Process consisting of the stages of
Image Acquisition, Feature Extraction, Feature Matching, Geometric
Verification, and Associated Information Retrieval. Feature extraction is the
essence of AR hence its details are furnished in the paper.","['Jekishan K. Parmar', 'Ankit Desai']",2019-11-09T14:24:56Z,http://arxiv.org/abs/1911.09177v1,"['cs.GR', 'cs.CV', 'eess.IV']","Augmented Reality,Feature Extraction,AR Technologies,Image Acquisition,Feature Matching,Geometric Verification,Associated Information Retrieval,Virtual Reality,Workflow"
Intelligent Physiotherapy Through Procedural Content Generation,"This paper describes an avenue for artificial and computational intelligence
techniques applied within games research to be deployed for purposes of
physical therapy. We provide an overview of prototypical research focussed on
the application of motion sensor input devices and virtual reality equipment
for rehabilitation of motor impairment an issue typical of patient's of
traumatic brain injuries. We highlight how advances in procedural content
generation and player modelling can stimulate development in this area by
improving quality of rehabilitation programmes and measuring patient
performance.","['Shabnam Sadeghi Esfahlani', 'Tommy Thompson']",2018-04-25T10:24:41Z,http://arxiv.org/abs/1804.09465v1,['cs.AI'],"artificial intelligence,computational intelligence,games research,physical therapy,motion sensor input devices,virtual reality equipment,rehabilitation,motor impairment,procedural content generation,player modelling"
Haptic Simulator for Liver Diagnostics through Palpation,"Mechanical properties of biological tissue for both histological and
pathological considerations are often required in disease diagnostics. Such
properties can be simulated and explored with haptic technology. Development of
cost effective haptic-based simulators and their introduction in the minimally
invasive surgery learning cycle is still in its infancy. Receiving pretraining
in a core set of surgical skills can reduce skill acquisition time and risks.
We present the development of a visuo-haptic simulator module designed to train
internal organs disease diagnostics through palpation. The module is part of a
set of tools designed to train and improve basic surgical skills for minimally
invasive surgery.","['Felix G. Hamza-Lup', 'Crenguta M. Bogdan', 'Adrian Seitan']",2019-03-08T03:50:21Z,http://arxiv.org/abs/1903.03268v1,['cs.HC'],"Haptic technology,Liver diagnostics,Mechanical properties,Biological tissue,Minimally invasive surgery,Visuo-haptic simulator,Disease diagnostics,Palpation,Surgical skills,Simulator module."
"A 6-DOF haptic manipulation system to verify assembly procedures on CAD
  models","During the design phase of products and before going into production, it is
necessary to verify the presence of mechanical plays, tolerances, and
encumbrances on production mockups. This work introduces a multi-modal system
that allows verifying assembly procedures of products in Virtual Reality
starting directly from CAD models. Thus leveraging the costs and speeding up
the assessment phase in product design. For this purpose, the design of a novel
6-DOF Haptic device is presented. The achieved performance of the system has
been validated in a demonstration scenario employing state-of-the-art
volumetric rendering of interaction forces together with a stereoscopic
visualization setup.","['Paolo Tripicchio', 'Carlo Alberto Avizzano', 'Massimo Bergamasco']",2019-09-27T14:37:59Z,http://arxiv.org/abs/1909.12714v1,['cs.RO'],"haptic manipulation system,assembly procedures,CAD models,mechanical plays,tolerances,encumbrances,Virtual Reality,6-DOF Haptic device,volumetric rendering,stereoscopic visualization"
"Artificial Intelligence, connected products, virtual reality: potential
  impacts on consumer safety in terms of their physical and psychological
  ability or well-being","With the progressive digitalisation of a majority of services to communities
and individuals, humankind is facing new challenges. While energy sources are
rapidly dwindling and rigorous choices have to be made to ensure the
sustainability of our environment, there is increasing concern in science and
society about the safety of connected products and technology for the
individual user. This essay provides a first basis for further inquiry into the
risks in terms of potentially negative, short and long-term, effects of
connected technologies and massive digitalisation on the psychological and/or
physical abilities and well-being of users or consumers.",['Birgitta Dresp-Langley'],2020-02-14T15:43:20Z,http://arxiv.org/abs/2002.06086v1,['cs.CY'],"Artificial Intelligence,connected products,virtual reality,consumer safety,psychological effects,physical effects,well-being."
"Deep Learning for Content-based Personalized Viewport Prediction of
  360-Degree VR Videos","In this paper, the problem of head movement prediction for virtual reality
videos is studied. In the considered model, a deep learning network is
introduced to leverage position data as well as video frame content to predict
future head movement. For optimizing data input into this neural network, data
sample rate, reduced data, and long-period prediction length are also explored
for this model. Simulation results show that the proposed approach yields
16.1\% improvement in terms of prediction accuracy compared to a baseline
approach that relies only on the position data.","['Xinwei Chen', 'Ali Taleb Zadeh Kasgari', 'Walid Saad']",2020-03-01T07:31:50Z,http://arxiv.org/abs/2003.00429v1,"['cs.CV', 'cs.LG', 'stat.ML']","deep learning,personalized,viewport prediction,360-degree,VR videos,head movement,neural network,data input,prediction accuracy"
Wireless VR/Haptic Open Platform for Multimodal Teleoperation,"With emerging trends in the fifth generation and robotics, the Internet of
Skills will enable us to deliver skills or expertise anywhere over the
Internet. In this paper, we propose a wireless connected virtual reality and
haptic communication open platform to show the proof of concept for multimodal
teleoperation systems in real-time. We focus on a practical implementation with
commercial products to facilitate the access and modification of the system.
The performance of the system is measured in terms of system latency and
user-centric metrics.","['Tae Hun Jung', 'Hanju Yoo', 'Yuna Jin', 'Chae Eun Rhee', 'Chan-Byoung Chae']",2020-04-27T02:12:57Z,http://arxiv.org/abs/2004.12545v1,"['cs.NI', 'eess.SP']","wireless,VR,haptic communication,multimodal teleoperation,Internet of Skills,fifth generation,virtual reality,real-time,system latency,user-centric metrics"
Exploiting Social Networks. Technological Trends (Habilitation Thesis),"The habilitation thesis presents two main directions:
  1. Exploiting data from social networks (Twitter, Facebook, Flickr, etc.) -
creating resources for text and image processing (classification, retrieval,
credibility, diversification, etc.);
  2. Creating applications with new technologies : augmented reality
(eLearning, games, smart museums, gastronomy, etc.), virtual reality (eLearning
and games), speech processing with Amazon Alexa (eLearning, entertainment, IoT,
etc.).
  The work was validated with good results in evaluation campaigns like CLEF
(Question Answering, Image CLEF, LifeCLEF, etc.), SemEval (Sentiment and
Emotion in text, Anorexia, etc.).",['Adrian Iftene'],2020-04-30T06:01:51Z,http://arxiv.org/abs/2004.14386v1,['cs.SI'],"social networks,data exploitation,image processing,augmented reality,virtual reality,speech processing,Amazon Alexa,evaluation campaigns,CLEF,SemEval"
MediaPipe Hands: On-device Real-time Hand Tracking,"We present a real-time on-device hand tracking pipeline that predicts hand
skeleton from single RGB camera for AR/VR applications. The pipeline consists
of two models: 1) a palm detector, 2) a hand landmark model. It's implemented
via MediaPipe, a framework for building cross-platform ML solutions. The
proposed model and pipeline architecture demonstrates real-time inference speed
on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at
https://mediapipe.dev.","['Fan Zhang', 'Valentin Bazarevsky', 'Andrey Vakunov', 'Andrei Tkachenka', 'George Sung', 'Chuo-Ling Chang', 'Matthias Grundmann']",2020-06-18T00:19:13Z,http://arxiv.org/abs/2006.10214v1,['cs.CV'],"hand tracking,on-device,real-time,RGB camera,AR/VR applications,pipeline,palm detector,hand landmark model,MediaPipe,mobile GPUs"
Attention Mesh: High-fidelity Face Mesh Prediction in Real-time,"We present Attention Mesh, a lightweight architecture for 3D face mesh
prediction that uses attention to semantically meaningful regions. Our neural
network is designed for real-time on-device inference and runs at over 50 FPS
on a Pixel 2 phone. Our solution enables applications like AR makeup, eye
tracking and AR puppeteering that rely on highly accurate landmarks for eye and
lips regions. Our main contribution is a unified network architecture that
achieves the same accuracy on facial landmarks as a multi-stage cascaded
approach, while being 30 percent faster.","['Ivan Grishchenko', 'Artsiom Ablavatski', 'Yury Kartynnik', 'Karthik Raveendran', 'Matthias Grundmann']",2020-06-19T05:07:38Z,http://arxiv.org/abs/2006.10962v1,['cs.CV'],"Attention Mesh,Face Mesh Prediction,Real-time,3D,Neural Network,On-device Inference,FPS,AR Makeup,Eye Tracking"
Actors in VR storytelling,"Virtual Reality (VR) storytelling enhances the immersion of users into
virtual environments (VE). Its use in virtual cultural heritage presentations
helps the revival of the genius loci (the spirit of the place) of cultural
monuments. This paper aims to show that the use of actors in VR storytelling
adds to the quality of user experience and improves the edutainment value of
virtual cultural heritage applications. We will describe the Baiae dry visit
application which takes us to a time travel in the city considered by the Roman
elite as ""Little Rome (Pusilla Roma)"" and presently is only partially preserved
under the sea.","['Selma Rizvic', 'Dusanka Boskovic', 'Fabio Bruno', 'Barbara Davidde Petriaggi', 'Sanda Sljivo', 'Marco Cozza']",2020-10-05T12:14:37Z,http://arxiv.org/abs/2010.01944v1,"['cs.HC', 'cs.GR', 'cs.MM']","Virtual Reality,VR storytelling,immersion,user experience,actors,edutainment,cultural heritage,application,time travel,Baiae"
"Temporally-smooth Antialiasing and Lens Distortion with Rasterization
  Map","Current GPU rasterization procedure is limited to narrow views in rectilinear
perspective. While industries demand curvilinear perspective in wide-angle
views, like Virtual Reality and Virtual Film Production industry. This paper
delivers new rasterization method using industry-standard STMaps. Additionally
new antialiasing rasterization method is proposed, which outperforms MSAA in
both quality and performance. It is an improvement upon previous solutions
found in paper Perspective picture from Visual Sphere by yours truly.",['Jakub Maximilian Fober'],2020-10-08T15:59:52Z,http://arxiv.org/abs/2010.04077v2,"['cs.GR', '68U05', 'I.3.3; I.3.7']","Rasterization,Antialiasing,Lens Distortion,STMaps,MSAA,Virtual Reality,Virtual Film Production,Curvilinear Perspective,Wide-angle views,Rectilinear perspective"
An Immersive Virtual Environment for Collaborative Geovisualization,"This paper presents an immersive virtual reality environment that can be used
to develop collaborative educational applications. Multiple users can
collaborate within the virtual shared space and communicate with each other
through voice. To asses the feasibility of the collaborative environment a
novel case-study concerned the education of a geography was developed and
evaluated. The geovisualization experiment scenario explores the possibility of
learning geography in a collaborative virtual environment. A user-study with 30
participants was performed. Participants evaluated and commented on the
usability and interaction methods used within the virtual environment.","['Milan Dolezal', 'Jiri Chmelik', 'Fotis Liarokapis']",2020-10-13T10:45:16Z,http://arxiv.org/abs/2010.06279v1,['cs.HC'],"Immersive virtual environment,Collaborative,Geovisualization,Virtual reality,Educational applications,Shared space,Voice communication,Case study,Geography education,User study"
"Automated acquisition of structured, semantic models of manipulation
  activities from human VR demonstration","In this paper we present a system capable of collecting and annotating, human
performed, robot understandable, everyday activities from virtual environments.
The human movements are mapped in the simulated world using off-the-shelf
virtual reality devices with full body, and eye tracking capabilities. All the
interactions in the virtual world are physically simulated, thus movements and
their effects are closely relatable to the real world. During the activity
execution, a subsymbolic data logger is recording the environment and the human
gaze on a per-frame basis, enabling offline scene reproduction and replays.
Coupled with the physics engine, online monitors (symbolic data loggers) are
parsing (using various grammars) and recording events, actions, and their
effects in the simulated world.","['Andrei Haidu', 'Michael Beetz']",2020-11-27T11:58:32Z,http://arxiv.org/abs/2011.13689v1,['cs.AI'],"automated acquisition,structured,semantic models,manipulation activities,human VR demonstration,virtual environments,full body tracking,eye tracking,subsymbolic data logger,physics engine"
Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.",['Nico Feld'],2021-01-07T14:43:51Z,http://arxiv.org/abs/2101.02565v1,['cs.HC'],"Augmented reality,Asymmetric teleteaching,Education,Virtual reality,Teaching system,Workflow,User roles,Student,Teacher,Investigation"
Mixed Reality Interaction Techniques,"This chapter gives an overview of interaction techniques for mixed reality
including augmented and virtual reality (AR/VR). Various modalities for input
and output are discussed. Specifically, techniques for tangible and
surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and
mouse-based, as well as haptic interaction are discussed. Furthermore, the
combination of multiple modalities in multisensory and multimodal interaction,
as well as interaction using multiple physical or virtual displays, are
presented. Finally, interaction with intelligent virtual agents is considered.",['Jens Grubert'],2021-03-10T10:47:10Z,http://arxiv.org/abs/2103.05984v1,['cs.HC'],"mixed reality,interaction techniques,augmented reality,virtual reality,input modalities,output modalities,tangible interaction,surface-based interaction,gesture-based interaction,pen-based interaction"
Flow-based Video Segmentation for Human Head and Shoulders,"Video segmentation for the human head and shoulders is essential in creating
elegant media for videoconferencing and virtual reality applications. The main
challenge is to process high-quality background subtraction in a real-time
manner and address the segmentation issues under motion blurs, e.g., shaking
the head or waving hands during conference video. To overcome the motion blur
problem in video segmentation, we propose a novel flow-based encoder-decoder
network (FUNet) that combines both traditional Horn-Schunck optical-flow
estimation technique and convolutional neural networks to perform robust
real-time video segmentation. We also introduce a video and image segmentation
dataset: ConferenceVideoSegmentationDataset. Code and pre-trained models are
available on our GitHub repository:
\url{https://github.com/kuangzijian/Flow-Based-Video-Matting}.","['Zijian Kuang', 'Xinran Tie']",2021-04-20T04:05:36Z,http://arxiv.org/abs/2104.09752v1,['cs.CV'],"Flow-based video segmentation,Human head,Shoulders,Background subtraction,Motion blur,Optical-flow estimation,Convolutional neural networks,Real-time,Dataset"
A Way to a Universal VR Accessibility Toolkit,"Virtual Reality (VR) has become more and more popular with dropping prices
for systems and a growing number of users. However, the issue of accessibility
in VR has been hardly addressed so far and no uniform approach or standard
exists at this time. In this position paper, we propose a customisable toolkit
implemented at the system-level and discuss the potential benefits of this
approach and challenges that will need to be overcome for a successful
implementation.","['Felix J. Thiel', 'Anthony Steed']",2021-06-01T08:50:46Z,http://arxiv.org/abs/2106.00321v1,"['cs.CY', 'K.4.2; H.5.1']","Universal VR,Accessibility,Toolkit,Virtual Reality,System-level,Customisable,Benefits,Challenges,Implementation,Standard"
Pluto: Motion Detection for Navigation in a VR Headset,"Untethered, inside-out tracking is considered a new goalpost for virtual
reality, which became attainable with advent of machine learning in SLAM. Yet
computer vision-based navigation is always at risk of a tracking failure due to
poor illumination or saliency of the environment. An extension for a navigation
system is proposed, which recognizes agents motion and stillness states with
87% accuracy from accelerometer data. 40% reduction in navigation drift is
demonstrated in a repeated tracking failure scenario on a challenging dataset.","['Dmitri Kovalenko', 'Artem Migukin', 'Svetlana Ryabkova', 'Vitaly Chernov']",2021-07-26T08:38:50Z,http://arxiv.org/abs/2107.12030v2,['cs.RO'],"Pluto,Motion Detection,Navigation,VR Headset,Inside-out tracking,Machine learning,SLAM,Computer vision,Accelerometer data,Navigation drift"
Rectangle-based Approximation for Rendering Glossy Interreflections,"This study introduces an approximation for rendering one bounce glossy
interreflection in real time. The solution is based on the most representative
point (MRP) and extends to a sampling disk near the MRP. Our algorithm
represents geometry as rectangle proxies and specular reflections using a
spherical Gaussian. The reflected radiance from the disk was efficiently
approximated by selecting a representative attenuation axis in the sampling
disk. We provide an efficient approximation of the glossy interreflection and
can efficiently perform the approximation at runtime. Our method uses forward
rendering (without using GBuffer), which is more suitable for platforms that
favor forward rendering, such as mobile applications and virtual reality.",['Chunbiao Guo'],2021-09-13T09:30:56Z,http://arxiv.org/abs/2109.05805v1,['cs.GR'],"rendering,glossy interreflections,approximation,representative point,geometry,specular reflections,spherical Gaussian,radiance,sampling disk,forward rendering"
"MolecuSense: Using Force-Feedback Gloves for Creating and Interacting
  with Ball-and-Stick Molecules in VR","We contribute MolecuSense, a virtual version of a physical molecule
construction kit, based on visualization in Virtual Reality (VR) and
interaction with force-feedback gloves. Targeting at chemistry education, our
goal is to make virtual molecule structures more tangible. Results of an
initial user study indicate that the VR molecular construction kit was
positively received. Compared to a physical construction kit, the VR molecular
construction kit is on the same level in terms of natural interaction. Besides,
it fosters the typical digital advantages though, such as saving, exporting,
and sharing of molecules. Feedback from the study participants has also
revealed potential future avenues for tangible molecule visualizations.","['Patrick Gebhardt', 'Xingyao Yu', 'Andreas Köhn', 'Michael Sedlmair']",2022-03-17T19:35:42Z,http://arxiv.org/abs/2203.09577v1,['cs.HC'],"Force-feedback gloves,Ball-and-stick molecules,Virtual Reality (VR),MolecuSense,Chemistry education,Molecule structures,User study,Natural interaction,Digital advantages,Molecule visualizations"
"Volkit: A Performance-Portable Computer Vision Library for 3D Volumetric
  Data","We present volkit, an open source library with high performance
implementations of image manipulation and computer vision algorithms that focus
on 3D volumetric representations. Volkit implements a cross-platform,
performance-portable API targeting both CPUs and GPUs that defers data and
resource movement and hides them from the application developer using a managed
API. We use volkit to process medical and simulation data that is rendered in
VR and consequently integrated the library into the C++ virtual reality
software CalVR. The paper presents case studies and performance results and by
that demonstrates the library's effectiveness and the efficiency of this
approach.","['Stefan Zellmann', 'Giovanni Aguirre', 'Jürgen P. Schulze']",2022-03-19T01:52:08Z,http://arxiv.org/abs/2203.10213v1,"['cs.CV', 'cs.GR']","performance-portable,computer vision,library,3D volumetric data,CPU,GPU,API,image manipulation,virtual reality,CalVR"
"Synergy between 6G and AI: Open Future Horizons and Impending Security
  Risks","This paper investigates the synergy between 6G and AI. It argues that they
can unlock future horizons, by discussing how they can address future
challenges in healthcare, transportation, virtual reality, education, resource
management, robotics, in addition to public safety and warfare. However, these
great opportunities come also with greater risk. Therefore, the paper provides
an overview of the security risks and challenges, along with possible
mitigation techniques.",['Elias Yaacoub'],2022-03-20T11:59:24Z,http://arxiv.org/abs/2203.10534v1,"['cs.CR', 'cs.CY', 'cs.SY', 'eess.SY']","6G,AI,synergy,future horizons,security risks,healthcare,transportation,virtual reality,education,robotics."
Towards Immersive Humanitarian Visualizations,"This paper introduces immersive humanitarian visualization as a promising
research area in information visualization. Humanitarian visualizations are
data visualizations designed to promote human welfare. This paper explains why
immersive display technologies taken broadly (e.g, virtual reality, augmented
reality, ambient displays and physical representations) open up a range of
opportunities for humanitarian visualization. In particular, immersive displays
offer ways to make remote and hidden human suffering more salient. They also
offer ways to communicate quantitative facts together with qualitative
information and visceral experiences, in order to provide a holistic
understanding of humanitarian issues that could support more informed
humanitarian decisions. But despite some promising preliminary work, immersive
humanitarian visualization has not taken off as a research topic yet. The goal
of this paper is to encourage, motivate, and inspire future research in this
area.",['Pierre Dragicevic'],2022-04-04T08:35:49Z,http://arxiv.org/abs/2204.01313v1,['cs.HC'],"humanitarian visualization,immersive display technologies,virtual reality,augmented reality,ambient displays,physical representations,quantitative facts,qualitative information,visceral experiences,informed humanitarian decisions"
Rule-based Procedural Tree Modeling Approach,"In some entertainment and virtual reality applications, it is necessary to
model and draw the real world realistically, so as to improve the fidelity of
natural scenes and make users have a better sense of immersion. However, due to
the morphological structure of trees The complexity and variety present many
challenges for photorealistic modeling and rendering of trees. This paper
reviews the progress achieved in photorealistic modeling and rendering of tree
branches, leaves, and bark over the past few decades. The main achievement is
mainly a rule-based procedural tree modeling method.","['Yinhui Yang', 'Rui Wang', 'Yuchi Huo']",2022-04-07T06:04:59Z,http://arxiv.org/abs/2204.03237v1,['cs.GR'],"procedural modeling,tree modeling,rule-based,photorealistic,rendering,natural scenes,immersion,morphological structure,tree branches,leaves"
"Ordered-logit pedestrian stress model for traffic flow with automated
  vehicles","An ordered-logit model is developed to study the effects of Automated
Vehicles (AVs) in the traffic mix on the average stress level of a pedestrian
when crossing an urban street at mid-block. Information collected from a
galvanic skin resistance sensor and virtual reality experiments are transformed
into a dataset with interpretable average stress levels (low, medium, and high)
and geometric, traffic, and environmental conditions. Modelling results
indicate a decrease in average stress level with the increase in the percentage
of AVs in the traffic mix.","['Kimia Kamal', 'Bilal Farooq', 'Mahwish Mudassar', 'Arash Kalatian']",2022-04-24T21:59:47Z,http://arxiv.org/abs/2204.11367v1,"['cs.HC', 'stat.AP']","ordered-logit model,pedestrian stress,traffic flow,automated vehicles"
The Metaverse from a Multimedia Communications Perspective,"eXtended reality (XR) technologies such as virtual reality and 360{\deg}
stereoscopic streaming enable the concept of the Metaverse, an immersive
virtual space for collaboration and interaction. To ensure high fidelity
display of immersive media, the bandwidth, latency and network traffic patterns
will need to be considered to ensure a user's Quality of Experience (QoE). In
this article, examples and calculations are explored to demonstrate the
requirements of the abovementioned parameters. Additionally, future methods
such as network-awareness using reinforcement learning (RL) and XR content
awareness using spatial or temporal difference in the frames could be explored
from a multimedia communications perspective.","['Haiwei Dong', 'Jeannie S. A. Lee']",2023-01-18T19:10:28Z,http://arxiv.org/abs/2301.07740v1,"['cs.MM', 'cs.NI']","eXtended reality,XR technologies,virtual reality,360° stereoscopic streaming,Metaverse,bandwidth,latency,network traffic patterns,Quality of Experience (QoE),reinforcement learning (RL)"
"Extended Reality and Internet of Things for Hyper-Connected Metaverse
  Environments","The Metaverse encompasses technologies related to the internet, virtual and
augmented reality, and other domains toward smart interfaces that are
hyper-connected, immersive, and engaging. However, Metaverse applications face
inherent disconnects between virtual and physical components and interfaces.
This work explores how an Extended Metaverse framework can be used to increase
the seamless integration of interoperable agents between virtual and physical
environments. It contributes an early theory and practice toward the synthesis
of virtual and physical smart environments anticipating future designs and
their potential for connected experiences.","['Jie Guan', 'Jay Irizawa', 'Alexis Morris']",2023-01-21T00:28:03Z,http://arxiv.org/abs/2301.08835v1,['cs.HC'],"Extended Reality,Internet of Things,Hyper-Connected,Metaverse,Virtual Reality,Augmented Reality,Smart Interfaces,Interoperable Agents,Seamless Integration,Smart Environments"
"Teaching Color Science to EECS Students Using Interactive Tutorials:
  Tools and Lessons","Teaching color science to Electrical Engineering and Computer Science (EECS)
students is critical to preparing them for advanced topics such as graphics,
visualization, imaging, Augmented/Virtual Reality. Color historically receive
little attention in EECS curriculum; students find it difficult to grasp basic
concepts. This is because today's pedagogical approaches are nonintuitive and
lack rigor for teaching color science. We develop a set of interactive
tutorials that teach color science to EECS students. Each tutorial is backed up
by a mathematically rigorous narrative, but is presented in a form that invites
students to participate in developing each concept on their own through
visualization tools. This paper describes the tutorial series we developed and
discusses the design decisions we made.",['Yuhao Zhu'],2023-01-24T02:39:32Z,http://arxiv.org/abs/2301.09788v1,['physics.ed-ph'],"Color science,EECS students,interactive tutorials,graphics,visualization,imaging,Augmented Reality,Virtual Reality,pedagogical approaches"
Privacy concerns from variances in spatial navigability in VR,"Current Virtual Reality (VR) input devices make it possible to navigate a
virtual environment and record immersive, personalized data regarding the
user's movement and specific behavioral habits, which brings the question of
the user's privacy concern to the forefront. In this article, the authors
propose to investigate Machine Learning driven learning algorithms that try to
learn with human users co-operatively and can be used to countermand existing
privacy concerns in VR but could also be extended to Augmented Reality (AR)
platforms.","['Aryabrata Basu', 'Mohammad Jahed Murad Sunny', 'Jayasri Sai Nikitha Guthula']",2023-02-06T01:48:59Z,http://arxiv.org/abs/2302.02525v1,"['cs.HC', 'cs.CY']","Virtual Reality,VR,privacy concerns,spatial navigability,input devices,immersive data,personalized data,Machine Learning,learning algorithms,Augmented Reality"
Dataset for predicting cybersickness from a virtual navigation task,"This work presents a dataset collected to predict cybersickness in virtual
reality environments. The data was collected from navigation tasks in a virtual
environment designed to induce cybersickness. The dataset consists of many data
points collected from diverse participants, including physiological responses
(EDA and Heart Rate) and self-reported cybersickness symptoms. The paper will
provide a detailed description of the dataset, including the arranged
navigation task, the data collection procedures, and the data format. The
dataset will serve as a valuable resource for researchers to develop and
evaluate predictive models for cybersickness and will facilitate more research
in cybersickness mitigation.","['Yuyang Wang', 'Ruichen Li', 'Jean-Rémy Chardonnet', 'Pan Hui']",2023-02-07T03:57:56Z,http://arxiv.org/abs/2303.13527v1,"['cs.HC', 'cs.AI', 'I.2; J.0; J.3']","cybersickness,dataset,virtual navigation task,virtual reality environments,physiological responses,EDA,Heart Rate,self-reported symptoms,data collection procedures,predictive models"
"The Exploration and Evaluation of Generating Affective 360$^\circ$
  Panoramic VR Environments Through Neural Style Transfer","Affective virtual reality (VR) environments with varying visual style can
impact users' valence and arousal responses. We applied Neural Style Transfer
(NST) to generate 360$^\circ$ VR environments that elicited users' varied
valence and arousal responses. From a user study with 30 participants, findings
suggested that generative VR environments changed participants' arousal
responses but not their valence levels. The generated visual features, e.g.,
textures and colors, also altered participants' affective perceptions. Our work
contributes novel insights about how users respond to generative VR
environments and provided a strategy for creating affective VR environments
without altering content.","['Yanheng Li', 'Long Bai', 'Yaxuan Mao', 'Xuening Peng', 'Zehao Zhang', 'Xin Tong', 'Ray LC']",2023-02-14T10:34:58Z,http://arxiv.org/abs/2303.13535v1,['cs.HC'],"Affective virtual reality,Neural Style Transfer,360$^\circ$ VR environments,valence,arousal responses,generative VR environments,textures,colors"
"Pedestrian Behavior Interacting with Autonomous Vehicles: Role of AV
  Operation and Signal Indication and Roadway Infrastructure","Interacting with pedestrians is challenging for Autonomous vehicles (AVs).
This study evaluates how AV operations /associated signaling and roadway
infrastructure affect pedestrian behavior in virtual reality. AVs were designed
with different operations and signal indications, including negotiating with no
signal, negotiating with a yellow signal, and yellow/blue negotiating/no-yield
indications. Results show that AV signal significantly impacts pedestrians'
accepted gap, walking time, and waiting time. Pedestrians chose the largest
open gap between cars with AV showing no signal, and had the slowest crossing
speed with AV showing a yellow signal indication. Roadway infrastructure
affects pedestrian walking time and waiting time.","['Fengjiao Zou', 'Jennifer Ogle', 'Weimin Jin', 'Patrick Gerard', 'Daniel Petty', 'Andrew Robb']",2023-03-27T16:09:38Z,http://arxiv.org/abs/2303.15352v1,['cs.RO'],"Pedestrian behavior,Autonomous vehicles,AV operations,Signal indication,Roadway infrastructure"
Dynamic Scene Adjustment for Player Engagement in VR Game,"Virtual reality (VR) produces a highly realistic simulated environment with
controllable environment variables. This paper proposes a Dynamic Scene
Adjustment (DSA) mechanism based on the user interaction status and
performance, which aims to adjust the VR experiment variables to improve the
user's game engagement. We combined the DSA mechanism with a musical rhythm VR
game. The experimental results show that the DSA mechanism can improve the
user's game engagement (task performance).","['Zhitao Liu', 'Yi Li', 'Ning Xie', 'YouTeng Fan', 'Haolan Tang', 'Wei Zhang']",2023-05-07T10:35:06Z,http://arxiv.org/abs/2305.04242v1,['cs.HC'],"player engagement,dynamic scene adjustment,VR game,user interaction,performance,virtual reality,environment variables,experimental results,task performance"
TauBench 1.1: A Dynamic Benchmark for Graphics Rendering,"Many graphics rendering algorithms used in both real-time games and virtual
reality applications can get performance boosts by temporally reusing previous
computations. However, algorithms based on temporal reuse are typically
measured using trivial benchmarks with very limited dynamic features. To this
end, in [1] we presented TauBench 1.0, a benchmark designed to stress temporal
reuse algorithms. Now, we release TauBench version 1.1, which improves the
usability of the original benchmark. In particular, these improvements reduce
the size of the dataset significantly, resulting in faster loading and
rendering times, and in better compatibility with 3D software that impose
strict size limits for the scenes.","['Erfan Momeni Yazdi', 'Markku Mäkitalo', 'Julius Ikkala', 'Pekka Jääskeläinen']",2023-05-08T16:02:43Z,http://arxiv.org/abs/2305.04804v1,['cs.GR'],"graphics rendering,benchmark,TauBench,dynamic features,temporal reuse,algorithms,dataset,rendering times,3D software"
Realization RGBD Image Stylization,"This research paper explores the application of style transfer in computer
vision using RGB images and their corresponding depth maps. We propose a novel
method that incorporates the depth map and a heatmap of the RGB image to
generate more realistic style transfer results. We compare our method to the
traditional neural style transfer approach and find that our method outperforms
it in terms of producing more realistic color and style. The proposed method
can be applied to various computer vision applications, such as image editing
and virtual reality, to improve the realism of generated images. Overall, our
findings demonstrate the potential of incorporating depth information and
heatmap of RGB images in style transfer for more realistic results.","['Bhavya Sehgal', 'Vaishnavi Mendu', 'Aparna Mendu']",2023-05-11T04:49:37Z,http://arxiv.org/abs/2305.06565v1,"['cs.CV', 'eess.IV']","RGBD images,stylization,style transfer,computer vision,depth map,heatmap,neural style transfer,image editing,virtual reality,realism"
OpenVR: Teleoperation for Manipulation,"Across the robotics field, quality demonstrations are an integral part of
many control pipelines. However, collecting high-quality demonstration
trajectories remains time-consuming and difficult, often resulting in the
number of demonstrations being the performance bottleneck. To address this
issue, we present a method of Virtual Reality (VR) Teleoperation that uses an
Oculus VR headset to teleoperate a Franka Emika Panda robot. Although other VR
teleoperation methods exist, our code is open source, designed for readily
available consumer hardware, easy to modify, agnostic to experimental setup,
and simple to use.","['Abraham George', 'Alison Bartsch', 'Amir Barati Farimani']",2023-05-16T19:34:05Z,http://arxiv.org/abs/2305.09765v1,"['cs.RO', 'cs.HC', 'cs.LG']","OpenVR,Teleoperation,Manipulation,Robotics,Demonstration,Trajectories,Virtual Reality,Oculus VR,Franka Emika Panda,Code"
Development of a Metaverse Platform for Tourism Promotion in Apulia,"Metaverse is an engaging way to recreate in a digital environment the real
world. It allows people to connect not by just browsing a website, but by using
headsets and virtual reality techniques. The metaverse is actually in a rapid
development phase, thanks to the advances in different topics. This paper
proposes a smart tourism platform in which tourists can interact with guides
and different kinds of suppliers, without the need to phisically visit the city
they are in. We propose some techniques to scan the real world and transpose it
in a metaverse platform, using the recreation of an Italian city, Bari, as a
real life scenario.","['Enrico Carmine Ciliberti', 'Marco Fiore', 'Marina Mongiello']",2023-05-05T14:02:33Z,http://arxiv.org/abs/2305.11877v1,"['cs.HC', 'cs.CY']","Metaverse,Tourism Promotion,Virtual Reality,Smart Tourism Platform,Virtual Reality Techniques,Digital Environment,Italian City,Recreation,Tourism Platform,Real World"
"Cross-Reality for Extending the Metaverse: Designing Hyper-Connected
  Immersive Environments with XRI","The Metaverse comprises technologies to enable virtual twins of the real
world, via mixed reality, internet of things, and others. As it matures unique
challenges arise such as a lack of strong connections between virtual and
physical worlds. This work presents design frameworks for cross-reality hybrid
spaces. Contributions include: i) clarifying the metaverse ""disconnect"", ii)
extended metaverse design frameworks, iii) prototypes, and iv) discussions
toward new metaverse smart environments.","['Jie Guan', 'Alexis Morris', 'Jay Irizawa']",2023-06-01T19:55:34Z,http://arxiv.org/abs/2306.01113v1,['cs.HC'],"Metaverse,Cross-Reality,Immersive Environments,XRI,Mixed Reality,Internet of Things,Design Frameworks,Prototypes,Smart Environments"
"Efficient coding of 360° videos exploiting inactive regions in
  projection formats","This paper presents an efficient method for encoding common projection
formats in 360$^\circ$ video coding, in which we exploit inactive regions.
These regions are ignored in the reconstruction of the equirectangular format
or the viewport in virtual reality applications. As the content of these pixels
is irrelevant, we neglect the corresponding pixel values in ratedistortion
optimization, residual transformation, as well as inloop filtering and achieve
bitrate savings of up to 10%.","['Christian Herglotz', 'Mohammadreza Jamali', 'Stéphane Coulombe', 'Carlos Vazquez', 'Ahmad Vakili']",2023-07-17T09:33:46Z,http://arxiv.org/abs/2307.08344v1,['eess.IV'],"360° videos,encoding,projection formats,inactive regions,equirectangular format,viewport,virtual reality,ratedistortion optimization,residual transformation,bitrate savings"
Gaze Estimation on Spresense,"Gaze estimation is a valuable technology with numerous applications in fields
such as human-computer interaction, virtual reality, and medicine. This report
presents the implementation of a gaze estimation system using the Sony
Spresense microcontroller board and explores its performance in latency,
MAC/cycle, and power consumption. The report also provides insights into the
system's architecture, including the gaze estimation model used. Additionally,
a demonstration of the system is presented, showcasing its functionality and
performance. Our lightweight model TinyTrackerS is a mere 169Kb in size, using
85.8k parameters and runs on the Spresense platform at 3 FPS.","['Thomas Ruegg', 'Pietro Bonazzi', 'Andrea Ronco']",2023-08-23T07:11:58Z,http://arxiv.org/abs/2308.12313v2,"['cs.CV', 'cs.HC', 'cs.RO']","Gaze estimation,Spresense,technology,human-computer interaction,virtual reality,medicine,microcontroller board,latency,power consumption,architecture."
Immersive ExaBrick: Visualizing Large AMR Data in the CAVE,"Rendering large adaptive mesh refinement (AMR) data in real-time in virtual
reality (VR) environments is a complex challenge that demands sophisticated
techniques and tools. The proposed solution harnesses the ExaBrick framework
and integrates it as a plugin in COVISE, a robust visualization system equipped
with the VR-centric OpenCOVER render module. This setup enables direct
navigation and interaction within the rendered volume in a VR environment. The
user interface incorporates rendering options and functions, ensuring a smooth
and interactive experience. We show that high-quality volume rendering of AMR
data in VR environments at interactive rates is possible using GPUs.","['Zhaoyang Wang', 'Stefan Wesner', 'Stefan Zellmann']",2023-10-04T15:17:36Z,http://arxiv.org/abs/2310.02881v1,['cs.GR'],"adaptive mesh refinement,VR,ExaBrick,visualization,CAVE,rendering,COVISE,OpenCOVER,volume rendering,GPUs"
Quality Evaluation of Projection-Based VR Displays,"We present a collection of heuristics and simple tests for evaluating the
quality of a projection-based virtual reality display. A typical VR system
includes numerous potential sources of error. By understanding the
characteristics of a correctly working system, and the types of errors that are
likely to occur, users can quickly determine if their display is inaccurate and
what components may need correction.","['Dave Pape', 'Dan Sandin']",2023-11-12T20:53:08Z,http://arxiv.org/abs/2311.09244v1,"['cs.HC', 'cs.GR']","Projection-based VR displays,Quality evaluation,Heuristics,Tests,Virtual reality system"
Virtual Heritage at iGrid 2000,"As part of the iGrid Research Demonstration at INET 2000, we created two
Virtual Cultural Heritage environments - ""Virtual Harlem"" and ""Shared Miletus"".
The purpose of these applications was to explore possibilities in using the
combination of high-speed international networks and virtual reality (VR)
displays for cultural heritage education. Our ultimate goal is to enable the
construction of tele-immersive museums and classes. In this paper we present an
overview of the infrastructure used for these applications, and some details of
their construction.","['Dave Pape', 'Josephine Anstey', 'Bryan Carter', 'Jason Leigh', 'Maria Roussou', 'Tim Portlock']",2023-11-17T03:25:53Z,http://arxiv.org/abs/2311.10303v1,['cs.HC'],"Virtual Heritage,iGrid 2000,Virtual Cultural Heritage,Virtual Reality,high-speed international networks,tele-immersive museums,tele-immersive classes"
"Diffusion Attack: Leveraging Stable Diffusion for Naturalistic Image
  Attacking","In Virtual Reality (VR), adversarial attack remains a significant security
threat. Most deep learning-based methods for physical and digital adversarial
attacks focus on enhancing attack performance by crafting adversarial examples
that contain large printable distortions that are easy for human observers to
identify. However, attackers rarely impose limitations on the naturalness and
comfort of the appearance of the generated attack image, resulting in a
noticeable and unnatural attack. To address this challenge, we propose a
framework to incorporate style transfer to craft adversarial inputs of natural
styles that exhibit minimal detectability and maximum natural appearance, while
maintaining superior attack capabilities.","['Qianyu Guo', 'Jiaming Fu', 'Yawen Lu', 'Dongming Gan']",2024-03-21T18:49:20Z,http://arxiv.org/abs/2403.14778v1,"['cs.CV', 'eess.IV']","diffusion attack,stable diffusion,naturalistic image,adversarial attack,deep learning,adversarial examples,style transfer,naturalness,comfort,attack image"
"Playing With Neuroscience: Past, Present and Future of Neuroimaging and
  Games","Videogames have been a catalyst for advances in many research fields, such as
artificial intelligence, human-computer interaction or virtual reality. Over
the years, research in fields such as artificial intelligence has enabled the
design of new types of games, while games have often served as a powerful tool
for testing and simulation. Can this also happen with neuroscience? What is the
current relationship between neuroscience and games research? what can we
expect from the future? In this article, we'll try to answer these questions,
analysing the current state-of-the-art at the crossroads between neuroscience
and games and envisioning future directions.","['Paolo Burelli', 'Laurits Dixen']",2024-03-06T12:38:18Z,http://arxiv.org/abs/2403.15413v1,"['q-bio.NC', 'cs.AI']","Neuroscience,Neuroimaging,Games,Artificial intelligence,Human-computer interaction,Virtual reality,Research,Simulation,Relationship,State-of-the-art"
The Hall of Singularity: VR Experience of Prophecy by AI,"""The Hall of Singularity"" is an immersive art that creates personalized
experiences of receiving prophecies from an AI deity through an integration of
Artificial Intelligence (AI) and Virtual Reality (VR). As a metaphor for the
mythologizing of AI in our society, ""The Hall of Singularity"" offers an
immersive quasi-religious experience where individuals can encounter an AI that
has the power to make prophecies. This journey enables users to experience and
imagine a world with an omnipotent AI deity.","['Jisu Kim', 'Kirak Kim']",2024-03-22T16:31:44Z,http://arxiv.org/abs/2404.00033v1,['cs.HC'],"AI,Virtual Reality,Prophecy,Immersive,Artificial Intelligence,Deity,Mythologizing,Omnipotent,Society,Experience"
"Toward Improving Binary Program Comprehension via Embodied Immersion: A
  Survey","Binary program comprehension is critical for many use cases but is difficult,
suffering from compounded uncertainty and lack of full automation. We seek
methods to improve the effectiveness of the human-machine joint cognitive
system performing binary PC. We survey three research areas to perform an
indirect cognitive task analysis: cognitive models of the PC process, related
elements of cognitive theory, and applicable affordances of virtual reality.
Based on common elements in these areas, we identify three overarching themes:
enhancing abductive iteration, augmenting working memory, and supporting
information organization. These themes spotlight several affordances of VR to
exploit in future studies of immersive tools for binary PC.","['Dennis Brown', 'Emily Mulder', 'Samuel Mulder']",2024-04-25T21:19:20Z,http://arxiv.org/abs/2404.17051v1,"['cs.HC', 'H.1.2; H.5.1; D.2.7']","Binary program comprehension,Embodied immersion,Cognitive system,Cognitive models,Virtual reality,Abductive iteration,Working memory,Information organization,Immersive tools."
"Enhancing Autonomous Vehicle Design and Testing: A Comprehensive Review
  of AR and VR Integration","This comprehensive literature review explores the potential of Augmented
Reality and Virtual Reality technologies to enhance the design and testing of
autonomous vehicles. By analyzing existing research, the review aims to
identify how AR and VR can be leveraged to improve various aspects of
autonomous vehicle development, including: creating more realistic and
comprehensive testing environments, facilitating the design of user centered
interfaces, and safely evaluating driver behavior in complex scenarios.
Ultimately, the review highlights AR and VR utilization as a key driver in the
development of adaptable testing environments, fostering more dependable
autonomous vehicle technology, and ultimately propelling significant
advancements within the field.","['Emanuella Ejichukwu', 'Lauren Tong', 'Gadir Hazime', 'Bochen Jia']",2024-04-29T18:04:30Z,http://arxiv.org/abs/2404.19021v1,['cs.HC'],"Autonomous vehicles,Design,Testing,Augmented Reality,Virtual Reality,User-centered interfaces,Driver behavior,Testing environments,Technology,Advancements."
"MetaSpace II: Object and full-body tracking for interaction and
  navigation in social VR","MetaSpace II (MS2) is a social Virtual Reality (VR) system where multiple
users can not only see and hear but also interact with each other, grasp and
manipulate objects, walk around in space, and get tactile feedback. MS2 allows
walking in physical space by tracking each user's skeleton in real-time and
allows users to feel by employing passive haptics i.e., when users touch or
manipulate an object in the virtual world, they simultaneously also touch or
manipulate a corresponding object in the physical world. To enable these
elements in VR, MS2 creates a correspondence in spatial layout and object
placement by building the virtual world on top of a 3D scan of the real world.
Through the association between the real and virtual world, users are able to
walk freely while wearing a head-mounted device, avoid obstacles like walls and
furniture, and interact with people and objects. Most current virtual reality
(VR) environments are designed for a single user experience where interactions
with virtual objects are mediated by hand-held input devices or hand gestures.
Additionally, users are only shown a representation of their hands in VR
floating in front of the camera as seen from a first person perspective. We
believe, representing each user as a full-body avatar that is controlled by
natural movements of the person in the real world (see Figure 1d), can greatly
enhance believability and a user's sense immersion in VR.","['Misha Sra', 'Chris Schmandt']",2015-12-09T16:13:34Z,http://arxiv.org/abs/1512.02922v1,"['cs.HC', 'H.5.1']","MetaSpace II,Object tracking,Full-body tracking,Interaction,Navigation,Social VR,Skeleton tracking,Passive haptics,Virtual world,Real world"
"Ownership and Agency of an Independent Supernumerary Hand Induced by an
  Imitation Brain-Computer Interface","To study body ownership and control, illusions that elicit these feelings in
non-body objects are widely used. Classically introduced with the Rubber Hand
Illusion, these illusions have been replicated more recently in virtual reality
and by using brain-computer interfaces. Traditionally these illusions
investigate the replacement of a body part by an artificial counterpart,
however as brain-computer interface research develops it offers us the
possibility to explore the case where non-body objects are controlled in
addition to movements of our own limbs. Therefore we propose a new illusion
designed to test the feeling of ownership and control of an independent
supernumerary hand. Subjects are under the impression they control a virtual
reality hand via a brain-computer interface, but in reality there is no causal
connection between brain activity and virtual hand movement but correct
movements are observed with 80% probability. These imitation brain-computer
interface trials are interspersed with movements in both the subjects' real
hands, which are in view throughout the experiment. We show that subjects
develop strong feelings of ownership and control over the third hand, despite
only receiving visual feedback with no causal link to the actual brain signals.
Our illusion is crucially different from previously reported studies as we
demonstrate independent ownership and control of the third hand without loss of
ownership in the real hands.","['Luke Bashford', 'Carsten Mehring']",2015-12-16T15:55:14Z,http://arxiv.org/abs/1512.05220v2,"['q-bio.NC', 'cs.HC']","body ownership,control,illusions,brain-computer interface,virtual reality,supernumerary hand,ownership,agency,imitation,experimental"
Emotional Qualities of VR Space,"The emotional response a person has to a living space is predominantly
affected by light, color and texture as space-making elements. In order to
verify whether this phenomenon could be replicated in a simulated environment,
we conducted a user study in a six-sided projected immersive display that
utilized equivalent design attributes of brightness, color and texture in order
to assess to which extent the emotional response in a simulated environment is
affected by the same parameters affecting real environments. Since emotional
response depends upon the context, we evaluated the emotional responses of two
groups of users: inactive (passive) and active (performing a typical daily
activity). The results from the perceptual study generated data from which
design principles for a virtual living space are articulated. Such a space, as
an alternative to expensive built dwellings, could potentially support new,
minimalist lifestyles of occupants, defined as the neo-nomads, aligned with
their work experience in the digital domain through the generation of emotional
experiences of spaces. Data from the experiments confirmed the hypothesis that
perceivable emotional aspects of real-world spaces could be successfully
generated through simulation of design attributes in the virtual space. The
subjective response to the virtual space was consistent with corresponding
responses from real-world color and brightness emotional perception. Our data
could serve the virtual reality (VR) community in its attempt to conceive of
further applications of virtual spaces for well-defined activities.","['Asma Naz', 'Regis Kopper', 'Ryan P. McMahan', 'Mihai Nadin']",2017-01-18T23:58:54Z,http://arxiv.org/abs/1701.06412v1,['cs.HC'],"VR space,emotional response,light,color,texture,user study,immersive display,design attributes,perceptual study,virtual reality (VR)"
Spatiotemporal Rate Adaptive Tiled Scheme for 360 Sports Events,"The recent rise of interest in Virtual Reality (VR) came with the
availability of commodity commercial VR products, such as the Head Mounted
Displays (HMD) created by Oculus and other vendors. One of the main
applications of virtual reality that has been recently adopted is streaming
sports events. For instance, the last olympics held in Rio De Janeiro was
streamed over the Internet for users to view on VR headsets or using 360 video
players. A big challenge for streaming VR sports events is the users limited
bandwidth and the amount of data required to transmit 360 videos. While 360
video demands high bandwidth, at any time instant users are only viewing a
small portion of the video according to the HMD field of view (FOV). Many
approaches have been proposed in the literature such as proposing new
representations (e.g. pyramid and offset-cubemap) and tiling the video and
streaming the tiles currently being viewed. In this paper, we propose a tiled
streaming framework, where we provide a degrading quality model similar to the
state-of-the-art offset-cubemap while minimizing its storage requirements at
the server side. We conduct objective studies showing the effectiveness of our
approach providing smooth degradation of quality from the user FOV to the back
of the 360 space. In addition, we conduct subjective studies showing that users
tend to prefer our proposed scheme over offset-cubemap in low bandwidth
connections, and they don't feel difference for higher bandwidth connections.
That is, we achieve better perceived quality with huge storage savings up to
670%.",['Tarek El-Ganainy'],2017-05-14T02:46:06Z,http://arxiv.org/abs/1705.04911v1,['cs.MM'],"Virtual Reality,Head Mounted Displays,Streaming,Bandwidth,360 Video,Tiling,Quality Model,Offset-Cubemap,Storage Requirements,Perceived Quality"
"Optimal Task Scheduling in Communication-Constrained Mobile Edge
  Computing Systems for Wireless Virtual Reality","Mobile edge computing (MEC) is expected to be an effective solution to
deliver 360-degree virtual reality (VR) videos over wireless networks. In
contrast to previous computation-constrained MEC framework, which reduces the
computation-resource consumption at the mobile VR device by increasing the
communication-resource consumption, we develop a communications-constrained MEC
framework to reduce communication-resource consumption by increasing the
computation-resource consumption and exploiting the caching resources at the
mobile VR device in this paper. Specifically, according to the task
modularization, the MEC server can only deliver the components which have not
been stored in the VR device, and then the VR device uses the received
components and the corresponding cached components to construct the task,
resulting in low communication-resource consumption but high delay. The MEC
server can also compute the task by itself to reduce the delay, however, it
consumes more communication-resource due to the delivery of entire task.
Therefore, we then propose a task scheduling strategy to decide which
computation model should the MEC server operates, in order to minimize the
communication-resource consumption under the delay constraint. Finally, we
discuss the tradeoffs between communications, computing, and caching in the
proposed system.","['Xiao Yang', 'Zhiyong Chen', 'Kuikui Li', 'Yaping Sun', 'Hongming Zheng']",2017-08-02T05:33:36Z,http://arxiv.org/abs/1708.00606v1,"['cs.IT', 'cs.NI', 'math.IT']","Task Scheduling,Communication-Constrained,Mobile Edge Computing,Virtual Reality,Wireless Networks,Computation-Resource Consumption,Communication-Resource Consumption,Caching Resources,Task Modularization"
"Echo State Learning for Wireless Virtual Reality Resource Allocation in
  UAV-enabled LTE-U Networks","In this paper, the problem of resource management is studied for a network of
wireless virtual reality (VR) users communicating using an unmanned aerial
vehicle (UAV)-enabled LTE-U network. In the studied model, the UAVs act as VR
control centers that collect tracking information from the VR users over the
wireless uplink and, then, send the constructed VR images to the VR users over
an LTE-U downlink. Therefore, resource allocation in such a UAV-enabled LTE-U
network must jointly consider the uplink and downlink links over both licensed
and unlicensed bands. In such a VR setting, the UAVs can dynamically adjust the
image quality and format of each VR image to change the data size of each VR
image, then meet the delay requirement. Therefore, resource allocation must
also take into account the image quality and format. This VR-centric resource
allocation problem is formulated as a noncooperative game that enables a joint
allocation of licensed and unlicensed spectrum bands, as well as a dynamic
adaptation of VR image quality and format. To solve this game, a learning
algorithm based on the machine learning tools of echo state networks (ESNs)
with leaky integrator neurons is proposed. Unlike conventional ESN based
learning algorithms that are suitable for discrete-time systems, the proposed
algorithm can dynamically adjust the update speed of the ESN's state and,
hence, it can enable the UAVs to learn the continuous dynamics of their
associated VR users. Simulation results show that the proposed algorithm
achieves up to 14% and 27.1% gains in terms of total VR QoE for all users
compared to Q-learning using LTE-U and Q-learning using LTE.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin']",2017-08-02T20:28:41Z,http://arxiv.org/abs/1708.00921v1,"['cs.IT', 'math.IT']","Echo State Learning,Wireless Virtual Reality,Resource Allocation,UAV-enabled,LTE-U Networks,Noncooperative Game,Machine Learning,Echo State Networks,Licensed Spectrum,Unlicensed Spectrum"
"Echo State Transfer Learning for Data Correlation Aware Resource
  Allocation in Wireless Virtual Reality","In this paper, the problem of data correlation-aware resource management is
studied for a network of wireless virtual reality (VR) users communicating over
cloud-based small cell networks (SCNs). In the studied model, small base
stations (SBSs) with limited computational resources act as VR control centers
that collect the tracking information from VR users over the cellular uplink
and send them to the VR users over the downlink. In such a setting, VR users
may send or request correlated or similar data (panoramic images and tracking
data). This potential spatial data correlation can be factored into the
resource allocation problem to reduce the traffic load in both uplink and
downlink. This VR resource allocation problem is formulated as a noncooperative
game that allows jointly optimizing the computational and spectrum resources,
while being cognizant of the data correlation. To solve this game, a transfer
learning algorithm based on the machine learning framework of echo state
networks (ESNs) is proposed. Unlike conventional reinforcement learning
algorithms that must be executed each time the environment changes, the
proposed algorithm can intelligently transfer information on the learned
utility, across time, to rapidly adapt to environmental dynamics due to factors
such as changes in the users' content or data correlation. Simulation results
show that the proposed algorithm achieves up to 16.7% and 18.2% gains in terms
of delay compared to the Q-learning with data correlation and Q-learning
without data correlation. The results also show that the proposed algorithm has
a faster convergence time than Q-learning and can guarantee low delays.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin', 'Merouane Debbah']",2017-11-25T01:09:10Z,http://arxiv.org/abs/1711.09173v1,"['cs.IT', 'math.IT']","data correlation,resource allocation,wireless virtual reality,small cell networks,echo state networks,transfer learning,spectrum resources,computational resources,reinforcement learning,noncooperative game"
Efficient Measuring of Congruence on High Dimensional Time Series,"A time series is a sequence of data items; typical examples are streams of
temperature measurements, stock ticker data, or gestures recorded with modern
virtual reality motion controllers. Quite some research has been devoted to
comparing and indexing time series. Especially, when the comparison should not
be affected by time warping, the ubiquitous Dynamic Time Warping distance
function ($\texttt{DTW}$) is one of the most analyzed time series distance
functions. The Dog-Keeper distance ($\texttt{DK}$) is another example for a
distance function on time series which is truely invariant under time warping.
  For many application scenarios (e.$\,$g. motion gesture recognition in
virtual reality), the invariance under isometric spatial transformations
(i.$\,$e. rotation, translation, and mirroring) is as important as the
invariance under time warping. Distance functions on time series which are
invariant under isometric transformations can be seen as measurements for the
congruency of two time series. The congruence distance ($\texttt{CD}$) is an
example for such a distance function. However, it is very hard to compute and
it is not invariant under time warpings.
  In this work, we are taking one step towards developing a feasable distance
function which is invariant under isometric spatial transformations and time
warping: We develop four approximations for $\texttt{CD}$. Two of these even
satisfy the triangle inequality and can thus be used with metric indexing
structures. We show that all approximations serve as a lower bound to
$\texttt{CD}$. Our evaluation shows that they achieve remarkable tightness
while providing a speedup of more than two orders of magnitude to the
congruence distance.","['Jörg P. Bachmann', 'Johann-Christoph Freytag']",2018-11-27T16:42:21Z,http://arxiv.org/abs/1811.11856v1,"['cs.DS', 'cs.DM']","High Dimensional,Time Series,Dynamic Time Warping distance,Dog-Keeper distance,Isometric Spatial Transformations,Congruence distance,Metric Indexing Structures,Approximations,Triangle Inequality,Speedup"
"Security, Privacy and Safety Risk Assessment for Virtual Reality
  Learning Environment Applications","Social Virtual Reality based Learning Environments (VRLEs) such as vSocial
render instructional content in a three-dimensional immersive computer
experience for training youth with learning impediments. There are limited
prior works that explored attack vulnerability in VR technology, and hence
there is a need for systematic frameworks to quantify risks corresponding to
security, privacy, and safety (SPS) threats. The SPS threats can adversely
impact the educational user experience and hinder delivery of VRLE content. In
this paper, we propose a novel risk assessment framework that utilizes attack
trees to calculate a risk score for varied VRLE threats with rate and duration
of threats as inputs. We compare the impact of a well-constructed attack tree
with an adhoc attack tree to study the trade-offs between overheads in managing
attack trees, and the cost of risk mitigation when vulnerabilities are
identified. We use a vSocial VRLE testbed in a case study to showcase the
effectiveness of our framework and demonstrate how a suitable attack tree
formalism can result in a more safer, privacy-preserving and secure VRLE
system.","['Aniket Gulhane', 'Akhil Vyas', 'Reshmi Mitra', 'Roland Oruche', 'Gabriela Hoefer', 'Samaikya Valluripally', 'Prasad Calyam', 'Khaza Anuarul Hoque']",2018-11-29T20:46:35Z,http://arxiv.org/abs/1811.12476v1,"['cs.HC', 'cs.CR']","Security,Privacy,Safety Risk Assessment,Virtual Reality,Learning Environment,Attack Vulnerability,Attack Trees,Risk Score,Risk Mitigation"
SurfaceBrush: From Virtual Reality Drawings to Manifold Surfaces,"Popular Virtual Reality (VR) tools allow users to draw varying-width,
ribbon-like 3D brush strokes by moving a hand-held controller in 3D space.
Artists frequently use dense collections of such strokes to draw virtual 3D
shapes. We propose SurfaceBrush, a surfacing method that converts such VR
drawings into user-intended manifold free-form 3D surfaces, providing a novel
approach for modeling 3D shapes. The inputs to our method consist of dense
collections of artist-drawn stroke ribbons described by the positions and
normals of their central polylines, and ribbon widths. These inputs are highly
distinct from those handled by existing surfacing frameworks and exhibit
different sparsity and error patterns, necessitating a novel surfacing
approach. We surface the input stroke drawings by identifying and leveraging
local coherence between nearby artist strokes. In particular, we observe that
strokes intended to be adjacent on the artist imagined surface often have
similar tangent directions along their respective polylines. We leverage this
local stroke direction consistency by casting the computation of the
user-intended manifold surface as a constrained matching problem on stroke
polyline vertices and edges. We first detect and smoothly connect adjacent
similarly-directed sequences of stroke edges producing one or more manifold
partial surfaces. We then complete the surfacing process by identifying and
connecting adjacent similarly directed edges along the borders of these partial
surfaces. We confirm the usability of the SurfaceBrush interface and the
validity of our drawing analysis via an observational study. We validate our
stroke surfacing algorithm by demonstrating an array of manifold surfaces
computed by our framework starting from a range of inputs of varying
complexity, and by comparing our outputs to reconstructions computed using
alternative means.","['Enrique Rosales', 'Jafet Rodriguez', 'Alla Sheffer']",2019-04-28T10:23:06Z,http://arxiv.org/abs/1904.12297v1,['cs.GR'],"Virtual Reality,3D brush strokes,Manifold surfaces,Surfacing method,3D shapes,Central polylines,Local coherence,Matching problem,Partial surfaces,Drawing analysis"
"Data Visceralization: Enabling Deeper Understanding of Data Using
  Virtual Reality","A fundamental part of data visualization is transforming data to map abstract
information onto visual attributes. While this abstraction is a powerful basis
for data visualization, the connection between the representation and the
original underlying data (i.e., what the quantities and measurements actually
correspond with in reality) can be lost. On the other hand, virtual reality
(VR) is being increasingly used to represent real and abstract models as
natural experiences to users. In this work, we explore the potential of using
VR to help restore the basic understanding of units and measures that are often
abstracted away in data visualization in an approach we call data
visceralization. By building VR prototypes as design probes, we identify key
themes and factors for data visceralization. We do this first through a
critical reflection by the authors, then by involving external participants. We
find that data visceralization is an engaging way of understanding the
qualitative aspects of physical measures and their real-life form, which
complements analytical and quantitative understanding commonly gained from data
visualization. However, data visceralization is most effective when there is a
one-to-one mapping between data and representation, with transformations such
as scaling affecting this understanding. We conclude with a discussion of
future directions for data visceralization.","['Benjamin Lee', 'Dave Brown', 'Bongshin Lee', 'Christophe Hurter', 'Steven Drucker', 'Tim Dwyer']",2020-08-31T18:55:28Z,http://arxiv.org/abs/2009.00059v2,['cs.HC'],"Data visualization,Virtual reality,Data visceralization,Units,Measures,Design probes,Qualitative aspects,Analytical understanding,Quantitative understanding"
"Improving the Usability of Virtual Reality Neuron Tracing with
  Topological Elements","Researchers in the field of connectomics are working to reconstruct a map of
neural connections in the brain in order to understand at a fundamental level
how the brain processes information. Constructing this wiring diagram is done
by tracing neurons through high-resolution image stacks acquired with
fluorescence microscopy imaging techniques. While a large number of automatic
tracing algorithms have been proposed, these frequently rely on local features
in the data and fail on noisy data or ambiguous cases, requiring time-consuming
manual correction. As a result, manual and semi-automatic tracing methods
remain the state-of-the-art for creating accurate neuron reconstructions. We
propose a new semi-automatic method that uses topological features to guide
users in tracing neurons and integrate this method within a virtual reality
(VR) framework previously used for manual tracing. Our approach augments both
visualization and interaction with topological elements, allowing rapid
understanding and tracing of complex morphologies. In our pilot study,
neuroscientists demonstrated a strong preference for using our tool over prior
approaches, reported less fatigue during tracing, and commended the ability to
better understand possible paths and alternatives. Quantitative evaluation of
the traces reveals that users' tracing speed increased, while retaining similar
accuracy compared to a fully manual approach.","['Torin McDonald', 'Will Usher', 'Nate Morrical', 'Attila Gyulassy', 'Steve Petruzza', 'Frederick Federer', 'Alessandra Angelucci', 'Valerio Pascucci']",2020-09-03T19:20:50Z,http://arxiv.org/abs/2009.01891v1,['cs.GR'],"connectomics,neural connections,neuron tracing,virtual reality,topological elements,fluorescence microscopy imaging,automatic tracing algorithms,semi-automatic tracing methods,neuron reconstructions,pilot study"
"Quantifying Data Rate and Bandwidth Requirements for Immersive 5G
  Experience","The proliferation of smartphones/mobile devices that support a wide range of
broadband applications and services has driven the volume of mobile data
traffic to an unprecedented high level, requiring a next generation mobile
communication system, i.e., the fifth generation (5G). Millimeter wave bands,
due to the large available spectrum bandwidth, are considered as one of the
most promising approaches to significantly boost the capacity. In this paper,
we define a typical use case envisaged in the early stage of the 5G system
rollout, where users can experience 100M+ data rate in the target area and at
the same time enjoy services demanding extremely high data rates, such as
virtual reality and ultra-high definition video. We then break down the use
case into four different traffic types: web browsing, content sharing, virtual
reality experience and ultra-high definition video, and derive and analyze the
distributions of the required instantaneous data rates for each individual
traffic type. Finally, we consider the case where multiple users with mixture
of traffic types are simultaneously active and analyze the overall data rate
and bandwidth requirements to support such a scenario.","['Yinan Qi', 'Mythri Hunukumbure', 'Maziar Nekovee', 'Javier Lorca', 'Victoria Sgardoni']",2016-05-11T08:40:07Z,http://arxiv.org/abs/1605.03331v1,"['cs.IT', 'cs.NI', 'math.IT']","data rate,bandwidth requirements,5G,millimeter wave bands,spectrum bandwidth,virtual reality,ultra-high definition video,traffic types,mobile communication system"
Vision-based Engagement Detection in Virtual Reality,"User engagement modeling for manipulating actions in vision-based interfaces
is one of the most important case studies of user mental state detection. In a
Virtual Reality environment that employs camera sensors to recognize human
activities, we have to know when user intends to perform an action and when
not. Without a proper algorithm for recognizing engagement status, any kind of
activities could be interpreted as manipulating actions, called ""Midas Touch""
problem. Baseline approach for solving this problem is activating gesture
recognition system using some focus gestures such as waiving or raising hand.
However, a desirable natural user interface should be able to understand user's
mental status automatically. In this paper, a novel multi-modal model for
engagement detection, DAIA, is presented. using DAIA, the spectrum of mental
status for performing an action is quantized in a finite number of engagement
states. For this purpose, a Finite State Transducer (FST) is designed. This
engagement framework shows how to integrate multi-modal information from user
biometric data streams such as 2D and 3D imaging. FST is employed to make the
state transition smoothly using combination of several boolean expressions. Our
FST true detection rate is 92.3% in total for four different states. Results
also show FST can segment user hand gestures more robustly.","['Ghassem Tofighi', 'Kaamraan Raahemifar', 'Maria Frank', 'Haisong Gu']",2016-09-05T22:24:08Z,http://arxiv.org/abs/1609.01344v1,['cs.CV'],"engagement detection,virtual reality,vision-based interfaces,mental state detection,camera sensors,gesture recognition,user interface,multi-modal model,Finite State Transducer,biometric data streams"
EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras,"Marker-based and marker-less optical skeletal motion-capture methods use an
outside-in arrangement of cameras placed around a scene, with viewpoints
converging on the center. They often create discomfort by possibly needed
marker suits, and their recording volume is severely restricted and often
constrained to indoor scenes with controlled backgrounds. Alternative
suit-based systems use several inertial measurement units or an exoskeleton to
capture motion. This makes capturing independent of a confined volume, but
requires substantial, often constraining, and hard to set up body
instrumentation. We therefore propose a new method for real-time, marker-less
and egocentric motion capture which estimates the full-body skeleton pose from
a lightweight stereo pair of fisheye cameras that are attached to a helmet or
virtual reality headset. It combines the strength of a new generative pose
estimation framework for fisheye views with a ConvNet-based body-part detector
trained on a large new dataset. Our inside-in method captures full-body motion
in general indoor and outdoor scenes, and also crowded scenes with many people
in close vicinity. The captured user can freely move around, which enables
reconstruction of larger-scale activities and is particularly useful in virtual
reality to freely roam and interact, while seeing the fully motion-captured
virtual body.","['Helge Rhodin', 'Christian Richardt', 'Dan Casas', 'Eldar Insafutdinov', 'Mohammad Shafiei', 'Hans-Peter Seidel', 'Bernt Schiele', 'Christian Theobalt']",2016-09-23T10:46:19Z,http://arxiv.org/abs/1609.07306v1,['cs.CV'],"egocentric,marker-less,motion capture,fisheye cameras,pose estimation,ConvNet,indoor scenes,outdoor scenes,virtual reality,skeletal"
"A Survey of Calibration Methods for Optical See-Through Head-Mounted
  Displays","Optical see-through head-mounted displays (OST HMDs) are a major output
medium for Augmented Reality, which have seen significant growth in popularity
and usage among the general public due to the growing release of
consumer-oriented models, such as the Microsoft Hololens. Unlike Virtual
Reality headsets, OST HMDs inherently support the addition of
computer-generated graphics directly into the light path between a user's eyes
and their view of the physical world. As with most Augmented and Virtual
Reality systems, the physical position of an OST HMD is typically determined by
an external or embedded 6-Degree-of-Freedom tracking system. However, in order
to properly render virtual objects, which are perceived as spatially aligned
with the physical environment, it is also necessary to accurately measure the
position of the user's eyes within the tracking system's coordinate frame. For
over 20 years, researchers have proposed various calibration methods to
determine this needed eye position. However, to date, there has not been a
comprehensive overview of these procedures and their requirements. Hence, this
paper surveys the field of calibration methods for OST HMDs. Specifically, it
provides insights into the fundamentals of calibration techniques, and presents
an overview of both manual and automatic approaches, as well as evaluation
methods and metrics. Finally, it also identifies opportunities for future
research. % relative to the tracking coordinate system, and, hence, its
position in 3D space.","['Jens Grubert', 'Yuta Itoh', 'Kenneth Moser', 'J. Edward Swan II']",2017-09-13T12:55:45Z,http://arxiv.org/abs/1709.04299v1,"['cs.HC', 'cs.CV']","Calibration methods,Optical see-through head-mounted displays,Augmented Reality,Virtual Reality,6-Degree-of-Freedom tracking system,Eye position,Calibration techniques,Manual approaches,Automatic approaches"
Learning Historical and Chronological Time: Practical Applications,"In the present article the necessity of introducing the teaching of time from
the first educative stages is defended through a didactic process in which a
feedback is produced between its chronological dimension (perception and
measuring of physical time) and the historical (knowledge of time in the
history of humanity). It has as its fundamental objectives the offering of
pedagogical strategies which will contribute to the development of this
competence in children, forming the basis of their future education as well as
revising the classic educative theories about the learning of time. In
particular, in relation to historical time where the employment of hyper-media
timelines and the most original technology of improved virtual reality and 3D
systems can contribute to an approach being given to their introduction for
comprehension at an earlier age. Concurrently, the correct integration of those
novel tools and telematic and computational systems in education, especially
from the humanistic and analytical viewpoint, will permit the adoption of
critical attitudes in the school with regard to the fictitious and virtual
reality produced. It will contribute, in this manner, to facilitating a better
comprehension of the modern world and the presence and functions that these
play in society, in a constructivist context of the development of knowledge,
science and culture.",['Jose Gomez-Galan'],2018-03-02T17:20:40Z,http://arxiv.org/abs/1803.01680v1,"['physics.ed-ph', 'physics.hist-ph']","teaching,time,historical time,chronological dimension,pedagogical strategies,hyper-media timelines,virtual reality,3D systems,telematic systems,computational systems"
Quantifying and attenuating pathologic tremor in virtual reality,"We present a virtual reality (VR) experience that creates a research-grade
benchmark in assessing patients with active upper-limb tremor, while
simultaneously offering the opportunity for patients to engage with VR
experiences without their pathologic tremor. Accurate and precise use of
handheld motion controllers in VR gaming applications may be limited for
patients with upper limb tremor. In parallel, objective tools measuring tremor
are not in widespread, routine clinical use. We used a commercially available
VR system and designed a challenging virtual-balloon-popping test mimicking a
common nose-to-target pointing task used by medical practitioners to
subjectively evaluate tremor in the exam room. Within our VR experience, we
offer a software mode which uses a low-pass filter to adjust hand position and
pointing orientation over a series of past data points. This digital filter
creates a smoothing function for hand movement which effectively removes the
patient's tremor in the VR representation. While the patient completes trials
of the reaching task, quantitative data on the pathologic tremor is digitally
recorded. With speed, accuracy, and the tremor components computed across three
axes of movement, patients can be evaluated for their tremor amplitudes in a
quantitative, replicable, and enjoyable manner. Removal of tremor in digital
space may allow patients having significant upper limb tremor to have both an
objective clinical measurement of symptoms while providing patients positive
feedback and interaction.","['Brian A. Cohn', 'Dilan D. Shah', 'Ali Marjaninejad', 'Martin Shapiro', 'Serhan Ulkumen', 'Christopher M. Laine', 'Francisco J. Valero-Cuevas', 'Kenneth H. Hayashida', 'Sarah Ingersoll']",2018-09-16T22:23:04Z,http://arxiv.org/abs/1809.05970v1,"['q-bio.QM', 'cs.HC']","virtual reality,pathologic tremor,handheld motion controllers,virtual-balloon-popping test,low-pass filter,tremor amplitudes,replicable,upper limb tremor,objective clinical measurement,digital space"
"Novel Approach to Measure Motion-To-Photon and Mouth-To-Ear Latency in
  Distributed Virtual Reality Systems","Distributed Virtual Reality systems enable globally dispersed users to
interact with each other in a shared virtual environment. In such systems,
different types of latencies occur. For a good VR experience, they need to be
controlled. The time delay between the user's head motion and the corresponding
display output of the VR system might lead to adverse effects such as a reduced
sense of presence or motion sickness. Additionally, high network latency among
worldwide locations makes collaboration between users more difficult and leads
to misunderstandings. To evaluate the performance and optimize dispersed VR
solutions it is therefore important to measure those delays. In this work, a
novel, easy to set up, and inexpensive method to measure local and remote
system latency will be described. The measuring setup consists of a
microcontroller, a microphone, a piezo buzzer, a photosensor, and a
potentiometer. With these components, it is possible to measure
motion-to-photon and mouth-to-ear latency of various VR systems. By using
GPS-receivers for timecode-synchronization it is also possible to obtain the
end-to-end delays between different worldwide locations. The described system
was used to measure local and remote latencies of two HMD based distributed VR
systems.","['Armin Becher', 'Jens Angerer', 'Thomas Grauschopf']",2018-09-17T16:48:28Z,http://arxiv.org/abs/1809.06320v1,"['cs.GR', 'cs.HC']","Distributed Virtual Reality systems,Latency,Motion-to-Photon,Mouth-to-Ear,VR systems,Network latency,Performance evaluation,Microcontroller,Timecode synchronization"
"Multisensory cues facilitate coordination of stepping movements with a
  virtual reality avatar","The effectiveness of simple sensory cues for retraining gait have been
demonstrated, yet the feasibility of humanoid avatars for entrainment have yet
to be investigated. Here, we describe the development of a novel method of
visually cued training, in the form of a virtual partner, and investigate its
ability to provide movement guidance in the form of stepping. Real stepping
movements were mapped onto an avatar using motion capture data. The trajectory
of one of the avatar step cycles was then accelerated or decelerated by 15% to
create a perturbation. Healthy participants were motion captured while
instructed to step in time to the avatar's movements, as viewed through a
virtual reality headset. Step onset times were used to measure the timing
errors (asynchronies) between them. Participants completed either a visual-only
condition, or auditory-visual with footstep sounds included. Participants'
asynchronies exhibited slow drift in the Visual-Only condition, but became
stable in the Auditory-Visual condition. Moreover, we observed a clear
corrective response to the phase perturbation in both auditory-visual
conditions. We conclude that an avatar's movements can be used to influence a
person's own gait, but should include relevant auditory cues congruent with the
movement to ensure a suitable accuracy is achieved.","['Omar Khan', 'Imran Ahmed', 'Joshua Cottingham', 'Musa Rahhal', 'Theodoros N Arvanitis', 'Mark Elliott']",2019-06-24T11:08:42Z,http://arxiv.org/abs/1906.09850v1,['cs.HC'],"multisensory cues,coordination,stepping movements,virtual reality avatar,sensory cues,gait retraining,humanoid avatars,visually cued training,motion capture,timing errors"
ReconViguRation: Reconfiguring Physical Keyboards in Virtual Reality,"Physical keyboards are common peripherals for personal computers and are
efficient standard text entry devices. Recent research has investigated how
physical keyboards can be used in immersive head-mounted display-based Virtual
Reality (VR). So far, the physical layout of keyboards has typically been
transplanted into VR for replicating typing experiences in a standard desktop
environment.
  In this paper, we explore how to fully leverage the immersiveness of VR to
change the input and output characteristics of physical keyboard interaction
within a VR environment. This allows individual physical keys to be
reconfigured to the same or different actions and visual output to be
distributed in various ways across the VR representation of the keyboard.
  We explore a set of input and output mappings for reconfiguring the virtual
presentation of physical keyboards and probe the resulting design space by
specifically designing, implementing and evaluating nine VR-relevant
applications: emojis, languages and special characters, application shortcuts,
virtual text processing macros, a window manager, a photo browser, a
whack-a-mole game, secure password entry and a virtual touch bar. We
investigate the feasibility of the applications in a user study with 20
participants and find that, among other things, they are usable in VR. We
discuss the limitations and possibilities of remapping the input and output
characteristics of physical keyboards in VR based on empirical findings and
analysis and suggest future research directions in this area.","['Daniel Schneider', 'Alexander Otte', 'Travis Gesslein', 'Philipp Gagel', 'Bastian Kuth', 'Mohamad Shahm Damlakhi', 'Oliver Dietz', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson', 'Jörg Müller', 'Jens Grubert']",2019-07-18T16:55:32Z,http://arxiv.org/abs/1907.08153v1,"['cs.HC', 'H.5.2']","physical keyboards,virtual reality,reconfiguration,immersive,input mappings,output mappings,VR applications,user study,limitations,future research"
"E0102-VR: exploring the scientific potential of Virtual Reality for
  observational astrophysics","Virtual Reality (VR) technology has been subject to a rapid democratization
in recent years, driven in large by the entertainment industry, and epitomized
by the emergence of consumer-grade, plug-and-play, room-scale VR devices. To
explore the scientific potential of this technology for the field of
observational astrophysics, we have created an experimental VR application:
E0102-VR. The specific scientific goal of this application is to facilitate the
characterization of the 3D structure of the oxygen-rich ejecta in the young
supernova remnant 1E 0102.2-7219 in the Small Magellanic Cloud. Using E0102-VR,
we measure the physical size of two large cavities in the system, including a
(7.0$\pm$0.5) pc-long funnel structure on the far-side of the remnant. The
E0102-VR application, albeit experimental, demonstrates the benefits of using
human depth perception for a rapid and accurate characterization of complex 3D
structures. Given the implementation costs (time-wise) of a dedicated VR
application like E0102-VR, we conclude that the future of VR for scientific
purposes in astrophysics most likely resides in the development of a robust,
generic application dedicated to the exploration and visualization of 3D
observational datasets, akin to a ``ds9-VR''.","['E. Baracaglia', 'F. P. A. Vogt']",2019-11-11T19:00:01Z,http://arxiv.org/abs/1911.04500v1,"['astro-ph.IM', 'astro-ph.HE', 'astro-ph.SR']","Virtual Reality,observational astrophysics,3D structure,oxygen-rich ejecta,supernova remnant,Small Magellanic Cloud,depth perception,3D structures,visualization,observational datasets"
"PhotoTwinVR: An Immersive System for Manipulation, Inspection and
  Dimension Measurements of the 3D Photogrammetric Models of Real-Life
  Structures in Virtual Reality","Photogrammetry is a science dealing with obtaining reliable information about
physical objects using their imagery description. Recent advancements in the
development of Virtual Reality (VR) can help to unlock the full potential
offered by the digital 3D-reality models generated using the state-of-art
photogrammetric technologies. These models are becoming a viable alternative
for providing high-quality content for such immersive environment.
Simultaneously, their analyses in VR could bring added-value to professionals
working in various engineering and non-engineering settings and help in
extracting useful information about physical objects. However, there is little
research published to date on feasible interaction methods in the VR-based
systems augmented with the 3D photogrammetric models, especially concerning
gestural input interfaces. Consequently, this paper presents the PhotoTwinVR --
an immersive, gesture-controlled system for manipulation and inspection of 3D
photogrammetric models of physical objects in VR. Our system allows the user to
perform basic engineering operations on the model subjected to the off-line
inspection process. An observational study with a group of three domain-expert
participants was completed to verify its feasibility. The system was populated
with a 3D photogrammetric model of an existing pipe-rack generated using a
commercial software package. The participants were asked to carry out a survey
measurement of the object using the measurement toolbox offered by PhotoTwinVR.
The study revealed a potential of such immersive tool to be applied in
practical real-words cases of off-line inspections of pipelines.","['Slawomir Konrad Tadeja', 'Wojciech Rydlewicz', 'Yupu Lu', 'Per Ola Kristensson', 'Tomasz Bubas', 'Maciej Rydlewicz']",2019-11-22T10:30:12Z,http://arxiv.org/abs/1911.09958v1,['cs.HC'],"Photogrammetry,Virtual Reality,3D Photogrammetric Models,Immersive System,Dimension Measurements,Interaction Methods,Gesture-Controlled System,Engineering Operations,Observational Study,Measurement Toolbox"
"Navigating in Virtual Reality using Thought: The Development and
  Assessment of a Motor Imagery based Brain-Computer Interface","Brain-computer interface (BCI) systems have potential as assistive
technologies for individuals with severe motor impairments. Nevertheless,
individuals must first participate in many training sessions to obtain adequate
data for optimizing the classification algorithm and subsequently acquiring
brain-based control. Such traditional training paradigms have been dubbed
unengaging and unmotivating for users. In recent years, it has been shown that
the synergy of virtual reality (VR) and a BCI can lead to increased user
engagement. This study created a 3-class BCI with a rather elaborate EEG signal
processing pipeline that heavily utilizes machine learning. The BCI initially
presented sham feedback but was eventually driven by EEG associated with motor
imagery. The BCI tasks consisted of motor imagery of the feet and left and
right hands, which were used to navigate a single-path maze in VR. Ten of the
eleven recruited participants achieved online performance superior to chance (p
< 0.01), while the majority successfully completed more than 70% of the
prescribed navigational tasks. These results indicate that the proposed
paradigm warrants further consideration as neurofeedback BCI training tool. A
paradigm that allows users, from their perspective, control from the outset
without the need for prior data collection sessions.","['Behnam Reyhani-Masoleh', 'Tom Chau']",2019-12-10T17:14:34Z,http://arxiv.org/abs/1912.04828v1,"['eess.SP', 'q-bio.NC']","Brain-computer interface,Motor imagery,Virtual reality,EEG signal processing,Machine learning,Motor impairments,Neurofeedback,Navigational tasks,Training sessions,Classification algorithm"
Predictive Scheduling for Virtual Reality,"A significant challenge for future virtual reality (VR) applications is to
deliver high quality-of-experience, both in terms of video quality and
responsiveness, over wireless networks with limited bandwidth. This paper
proposes to address this challenge by leveraging the predictability of user
movements in the virtual world. We consider a wireless system where an access
point (AP) serves multiple VR users. We show that the VR application process
consists of two distinctive phases, whereby during the first (proactive
scheduling) phase the controller has uncertain predictions of the demand that
will arrive at the second (deadline scheduling) phase. We then develop a
predictive scheduling policy for the AP that jointly optimizes the scheduling
decisions in both phases.
  In addition to our theoretical study, we demonstrate the usefulness of our
policy by building a prototype system. We show that our policy can be
implemented under Furion, a Unity-based VR gaming software, with minor
modifications. Experimental results clearly show visible difference between our
policy and the default one. We also conduct extensive simulation studies, which
show that our policy not only outperforms others, but also maintains excellent
performance even when the prediction of future user movements is not accurate.","['I-Hong Hou', 'Narges Zarnaghi Naghsh', 'Sibendu Paul', 'Y. Charlie Hu', 'Atilla Eryilmaz']",2019-12-29T15:22:06Z,http://arxiv.org/abs/1912.12672v1,['cs.NI'],"Predictive Scheduling,Virtual Reality,Quality-of-Experience,Video Quality,Responsiveness,Wireless Networks,Bandwidth,Scheduling Policy,Deadline Scheduling,User Movements"
"Measuring Cognitive Conflict in Virtual Reality with Feedback-Related
  Negativity","As virtual reality (VR) emerges as a mainstream platform, designers have
started to experiment new interaction techniques to enhance the user
experience. This is a challenging task because designers not only strive to
provide designs with good performance but also carefully ensure not to disrupt
users' immersive experience. There is a dire need for a new evaluation tool
that extends beyond traditional quantitative measurements to assist designers
in the design process. We propose an EEG-based experiment framework that
evaluates interaction techniques in VR by measuring intentionally elicited
cognitive conflict. Through the analysis of the feedback-related negativity
(FRN) as well as other quantitative measurements, this framework allows
designers to evaluate the effect of the variables of interest. We studied the
framework by applying it to the fundamental task of 3D object selection using
direct 3D input, i.e. tracked hand in VR. The cognitive conflict is
intentionally elicited by manipulating the selection radius of the target
object. Our first behavior experiment validated the framework in line with the
findings of conflict-induced behavior adjustments like those reported in other
classical psychology experiment paradigms. Our second EEG-based experiment
examines the effect of the appearance of virtual hands. We found that the
amplitude of FRN correlates with the level of realism of the virtual hands,
which concurs with the Uncanny Valley theory.","['Avinash Kumar Singh', 'Hsiang-Ting Chen', 'Jung-Tai King', 'Chin-Teng Lin']",2017-03-16T02:56:34Z,http://arxiv.org/abs/1703.05462v1,"['cs.HC', 'H.5.2']","virtual reality,feedback-related negativity,EEG,cognitive conflict,interaction techniques,quantitative measurements,3D object selection,tracked hand,virtual hands,Uncanny Valley theory"
Multi-Stream Switching for Interactive Virtual Reality Video Streaming,"Virtual reality (VR) video provides an immersive 360 viewing experience to a
user wearing a head-mounted display: as the user rotates his head,
correspondingly different fields-of-view (FoV) of the 360 video are rendered
for observation. Transmitting the entire 360 video in high quality over
bandwidth-constrained networks from server to client for real-time playback is
challenging. In this paper we propose a multi-stream switching framework for VR
video streaming: the server pre-encodes a set of VR video streams covering
different view ranges that account for server-client round trip time (RTT)
delay, and during streaming the server transmits and switches streams according
to a user's detected head rotation angle. For a given RTT, we formulate an
optimization to seek multiple VR streams of different view ranges and the
head-angle-to-stream mapping function simultaneously, in order to minimize the
expected distortion subject to bandwidth and storage constraints. We propose an
alternating algorithm that, at each iteration, computes the optimal streams
while keeping the mapping function fixed and vice versa. Experiments show that
for the same bandwidth, our multi-stream switching scheme outperforms a
non-switching single-stream approach by up to 2.9dB in PSNR.","['Gene Cheung', 'Zhi Liu', 'Zhiyou Ma', 'Jack Z. G. Tan']",2017-03-27T14:09:12Z,http://arxiv.org/abs/1703.09090v1,"['cs.MM', 'cs.NI']","Virtual reality,VR video,360 video,streaming,multi-stream switching,head rotation,round trip time (RTT),optimization,distortion,bandwidth"
"Merging real and virtual worlds: An analysis of the state of the art and
  practical evaluation of Microsoft Hololens","Achieving a symbiotic blending between reality and virtuality is a dream that
has been lying in the minds of many people for a long time. Advances in various
domains constantly bring us closer to making that dream come true. Augmented
reality as well as virtual reality are in fact trending terms and are expected
to further progress in the years to come.
  This master's thesis aims to explore these areas and starts by defining
necessary terms such as augmented reality (AR) or virtual reality (VR). Usual
taxonomies to classify and compare the corresponding experiences are then
discussed.
  In order to enable those applications, many technical challenges need to be
tackled, such as accurate motion tracking with 6 degrees of freedom (positional
and rotational), that is necessary for compelling experiences and to prevent
user sickness. Additionally, augmented reality experiences typically rely on
image processing to position the superimposed content. To do so, ""paper""
markers or features extracted from the environment are often employed. Both
sets of techniques are explored and common solutions and algorithms are
presented.
  After investigating those technical aspects, I carry out an objective
comparison of the existing state-of-the-art and state-of-the-practice in those
domains, and I discuss present and potential applications in these areas. As a
practical validation, I present the results of an application that I have
developed using Microsoft HoloLens, one of the more advanced affordable
technologies for augmented reality that is available today. Based on the
experience and lessons learned during this development, I discuss the
limitations of current technologies and present some avenues of future
research.",['Adrien Coppens'],2017-06-25T13:10:39Z,http://arxiv.org/abs/1706.08096v1,"['cs.HC', 'cs.CV']","augmented reality,virtual reality,motion tracking,6 degrees of freedom,image processing,markers,Microsoft HoloLens,state-of-the-art,state-of-the-practice"
"Prototyping Virtual Reality Serious Games for Building Earthquake
  Preparedness: The Auckland City Hospital Case Study","Enhancing evacuee safety is a key factor in reducing the number of injuries
and deaths that result from earthquakes. One way this can be achieved is by
training occupants. Virtual Reality (VR) and Serious Games (SGs), represent
novel techniques that may overcome the limitations of traditional training
approaches. VR and SGs have been examined in the fire emergency context,
however, their application to earthquake preparedness has not yet been
extensively examined. We provide a theoretical discussion of the advantages and
limitations of using VR SGs to investigate how building occupants behave during
earthquake evacuations and to train building occupants to cope with such
emergencies. We explore key design components for developing a VR SG framework:
(a) what features constitute an earthquake event, (b) which building types can
be selected and represented within the VR environment, (c) how damage to the
building can be determined and represented, (d) how non-player characters (NPC)
can be designed, and (e) what level of interaction there can be between NPC and
the human participants. We illustrate the above by presenting the Auckland City
Hospital, New Zealand as a case study, and propose a possible VR SG training
tool to enhance earthquake preparedness in public buildings.","['Ruggiero Lovreglio', 'Vicente Gonzalez', 'Zhenan Feng', 'Robert Amor', 'Michael Spearpoint', 'Jared Thomas', 'Margaret Trotter', 'Rafael Sacks']",2018-02-26T01:08:51Z,http://arxiv.org/abs/1802.09119v1,['cs.AI'],"Virtual Reality,Serious Games,Earthquake Preparedness,Building Occupants,Evacuation Behavior,Building Types,Damage Representation,Non-Player Characters (NPC),Interaction Level,Public Buildings"
"Immersive Virtual Reality Serious Games for Evacuation Training and
  Research: A Systematic Literature Review","An appropriate and safe behavior for exiting a facility is key to reducing
injuries and increasing survival when facing an emergency evacuation in a
building. Knowledge on the best evacuation practice is commonly delivered by
traditional training approaches such as videos, posters, or evacuation drills,
but they may become ineffective in terms of knowledge acquisition and
retention. Serious games (SGs) are an innovative approach devoted to training
and educating people in a gaming environment. Recently, increasing attention
has been paid to immersive virtual reality (IVR)-based SGs for evacuation
knowledge delivery and behavior assessment because they are highly engaging and
promote greater cognitive learning.
  This paper aims to understand the development and implementation of IVR SGs
in the context of building evacuation training and research, applied to various
indoor emergencies such as fire and earthquake. Thus, a conceptual framework
for effective design and implementation through the systematic literature
review method was developed. As a result, this framework integrates critical
aspects and provides connections between them, including pedagogical and
behavioral impacts, gaming environment development, and outcome and
participation experience measures.","['Zhenan Feng', 'Vicente A. González', 'Robert Amor', 'Ruggiero Lovreglio', 'Guillermo Cabrera']",2018-05-14T04:01:48Z,http://arxiv.org/abs/1805.09138v2,"['cs.CY', 'cs.GT']","immersive virtual reality,serious games,evacuation training,research,systematic literature review,evacuation practice,cognitive learning,indoor emergencies,conceptual framework"
"Impact of Smartphone Distraction on Pedestrians' Crossing Behaviour: An
  Application of Head-Mounted Immersive Virtual Reality","A novel head-mounted virtual immersive/interactive reality environment (VIRE)
is utilized to evaluate the behaviour of participants in three pedestrian road
crossing conditions while 1) not distracted, 2) distracted with a smartphone,
and 3) distracted with a smartphone with a virtually implemented safety measure
on the road. Forty-two volunteers participated in our research who completed
thirty successful (complete crossing) trials in blocks of ten trials for each
crossing condition. For the two distracted conditions, pedestrians are engaged
in a maze-solving game on a virtual smartphone, while at the same time checking
the traffic for a safe crossing gap. For the proposed safety measure, smart
flashing and color changing LED lights are simulated on the crosswalk to warn
the distracted pedestrian who initiates crossing. Surrogate safety measures as
well as speed information and distraction attributes such as direction and
orientation of participant's head were collected and evaluated by employing a
Multinomial Logit (MNL) model. Results from the model indicate that females
have more dangerous crossing behaviour especially in distracted conditions;
however, the smart LED treatment reduces this negative impact. Moreover, the
number of times and the percentage of duration the head was facing the
smartphone during a trial and a waiting time respectively increase the
possibility of unsafe crossings; though, the proposed treatment reduces the
safety crossing rate. Hence, our study shows that the smart LED light safety
treatment indeed improves the safety of distracted pedestrians and enhances the
successful crossing rate.","['Anae Sobhani', 'Bilal Farooq']",2018-06-17T21:54:11Z,http://arxiv.org/abs/1806.06454v1,['cs.HC'],"Smartphone distraction,Pedestrians,Crossing behavior,Head-mounted immersive virtual reality,Virtual reality environment,Safety measure,LED lights,Multinomial Logit model,Surrogate safety measures,Distraction attributes"
"Gamification for Education of the Digitally Native Generation by Means
  of Virtual Reality, Augmented Reality, Machine Learning, and Brain-Computing
  Interfaces in Museums","Particularly close attention is being paid today among researchers in social
science disciplines to aspects of learning in the digital age, especially for
the Digitally Native Generation. In the context of museums, the question is:
how can rich learning experiences be provided for increasingly technologically
advanced young visitors in museums? Which high-tech platforms and solutions do
museums need to focus on? At the same time, the software games business is
growing fast and now finding its way into non-entertainment contexts, helping
to deliver substantial benefits, particularly in education, training, research,
and health. This article outlines some aspects facing Digitally Native learners
in museums through an analysis of several radically new key technologies:
Interactivity, Wearables, Virtual Reality, and Augmented Reality. Special
attention is paid to use cases for application of games-based scenarios via
these technologies in non-leisure contexts and specifically for educational
purposes in museums.","['Olga Barkova', 'Natalia Pysarevska', 'Oleg Allenin', 'Serhii Hamotsky', 'Nikita Gordienko', 'Vladyslav Sarnatskyi', 'Vadym Ovcharenko', 'Mariia Tkachenko', 'Yurii Gordienko', 'Sergei Stirenko']",2018-06-20T17:03:52Z,http://arxiv.org/abs/1806.07842v1,"['cs.CY', 'cs.DL', 'cs.HC']","Gamification,Education,Digitally Native Generation,Virtual Reality,Augmented Reality,Machine Learning,Brain-Computing Interfaces,Museums,Interactivity,Wearables"
"Communications, Caching and Computing for Mobile Virtual Reality:
  Modeling and Tradeoff","Virtual reality (VR) over wireless is emerging as an important use case of 5G
networks. Immersive VR experience requires the delivery of huge data at
ultra-low latency, thus demanding ultra-high transmission rate. This challenge
can be largely addressed by the recent network architecture known as mobile
edge computing (MEC), which enables caching and computing capabilities at the
edge of wireless networks. This paper presents a novel MEC-based mobile VR
delivery framework that is able to cache parts of the field of views (FOVs) in
advance and run certain post-processing procedures at the mobile VR device. To
optimize resource allocation at the mobile VR device, we formulate a joint
caching and computing decision problem to minimize the average required
transmission rate while meeting a given latency constraint. When FOVs are
homogeneous, we obtain a closed-form expression for the optimal joint policy
which reveals interesting communications-caching-computing tradeoffs. When FOVs
are heterogeneous, we obtain a local optima of the problem by transforming it
into a linearly constrained indefinite quadratic problem then applying concave
convex procedure. Numerical results demonstrate great promises of the proposed
mobile VR delivery framework in saving communication bandwidth while meeting
low latency requirement.","['Yaping Sun', 'Zhiyong Chen', 'Meixia Tao', 'Hui Liu']",2018-06-23T08:22:08Z,http://arxiv.org/abs/1806.08928v1,"['cs.IT', 'math.IT']","Communications,Caching,Computing,Mobile Virtual Reality,Modeling,Tradeoff,Edge Computing,Transmission Rate,Resource Allocation"
Wireless Access to Ultimate Virtual Reality 360-Degree Video At Home,"Virtual reality 360-degree videos will become the first prosperous online VR
application. VR 360 videos are data-hungry and latency-sensitive that pose
unique challenges to the networking infrastructure. In this paper, we focus on
the ultimate VR 360 that satisfies human eye fidelity. The ultimate VR 360
requires downlink 1.5 Gbps for viewing and uplink 6.6 Gbps for live
broadcasting, with round-trip time of less than 8.3 ms. On the other hand,
wireless access to VR 360 services is preferred over wire-line transmission
because of the better user experience and the safety concern (e.g., tripping
hazard). We explore in this paper whether the most advanced wireless
technologies from both cellular communications and WiFi communications support
the ultimate VR 360. Specifically, we consider 5G in cellular communications,
IEEE 802.11ac (operating in 5GHz) and IEEE 802.11ad (operating in 60GHz) in
WiFi communications. According to their performance specified in their
standards and/or empirical measurements, we have the following findings: (1)
Only 5G has the potential to support both the the ultimate VR 360 viewing and
live broadcasting. However, it is difficult for 5G to support multiple users of
the ultimate VR live broadcasting at home; (2) IEEE 802.11ac supports the
ultimate VR 360 viewing but fails to support the ultimate VR 360 live
broadcasting because it does not meet the data rate requirement of the ultimate
VR 360 live broadcasting; (3) IEEE 802.11ad fails to support the ultimate VR
360, because its current implementation incurs very high latency. Our
preliminary results indicate that more advanced wireless technologies are
needed to fully support multiple ultimate VR 360 users at home.","['Huanle Zhang', 'Ahmed Elmokashfi', 'Zhicheng Yang', 'Prasant Mohapatra']",2018-12-05T01:54:53Z,http://arxiv.org/abs/1812.01777v2,"['cs.HC', 'cs.MM']","Wireless access,Virtual reality,360-degree videos,Networking infrastructure,5G,IEEE 802.11ac,IEEE 802.11ad,Data rate,Latency,User experience"
Cognitive Analysis of 360 degree Surround Photos,"360 degrees surround photography or photospheres have taken the world by
storm as the new media for content creation providing viewers rich, immersive
experience compared to conventional photography. With the emergence of Virtual
Reality as a mainstream trend, the 360 degrees photography is increasingly
important to offer a practical approach to the general public to capture
virtual reality ready content from their mobile phones without explicit tool
support or knowledge. Even though the amount of 360-degree surround content
being uploaded to the Internet continues to grow, there is no proper way to
index them or to process them for further information. This is because of the
difficulty in image processing the photospheres due to the distorted nature of
objects embedded. This challenge lies in the way 360-degree panoramic
photospheres are saved. This paper presents a unique, and innovative technique
named Photosphere to Cognition Engine (P2CE), which allows cognitive analysis
on 360-degree surround photos using existing image cognitive analysis
algorithms and APIs designed for conventional photos. We have optimized the
system using a wide variety of indoor and outdoor samples and extensive
evaluation approaches. On average, P2CE provides up-to 100% growth in accuracy
on image cognitive analysis of Photospheres over direct use of conventional
non-photosphere based Image Cognition Systems.","['Madhawa Vidanapathirana', 'Lakmal Meegahapola', 'Indika Perera']",2019-01-17T05:50:00Z,http://arxiv.org/abs/1901.05634v1,['cs.CV'],"Cognitive Analysis,360-degree Surround Photos,Photospheres,Virtual Reality,Image Processing,Photosphere to Cognition Engine (P2CE),Image Cognitive Analysis,Image Cognition Systems"
"On the Reliability of Wireless Virtual Reality at Terahertz (THz)
  Frequencies","Guaranteeing ultra reliable low latency communications (URLLC) with high data
rates for virtual reality (VR) services is a key challenge to enable a dual VR
perception: visual and haptic. In this paper, a terahertz (THz) cellular
network is considered to provide high-rate VR services, thus enabling a
successful visual perception. For this network, guaranteeing URLLC with high
rates requires overcoming the uncertainty stemming from the THz channel. To
this end, the achievable reliability and latency of VR services over THz links
are characterized. In particular, a novel expression for the probability
distribution function of the transmission delay is derived as a function of the
system parameters. Subsequently, the end-to-end (E2E) delay distribution that
takes into account both processing and transmission delay is found and a
tractable expression of the reliability of the system is derived as a function
of the THz network parameters such as the molecular absorption loss and noise,
the transmitted power, and the distance between the VR user and its respective
small base station (SBS). Numerical results show the effects of various system
parameters such as the bandwidth and the region of non-negligible interference
on the reliability of the system. In particular, the results show that THz can
deliver rates up to 16.4 Gbps and a reliability of 99.999% (with a delay
threshold of 30 ms) provided that the impact of the molecular absorption on the
THz links, which substantially limits the communication range of the SBS, is
alleviated by densifying the network accordingly.","['Christina Chaccour', 'Ramy Amer', 'Bo Zhou', 'Walid Saad']",2019-05-18T23:35:09Z,http://arxiv.org/abs/1905.07656v1,"['cs.IT', 'math.IT']","Wireless,Virtual Reality,Terahertz,THz frequencies,Reliability,Latency,Ultra reliable low latency communications (URLLC),End-to-end delay distribution"
"An Immersive Virtual Reality Serious Game to Enhance Earthquake
  Behavioral Responses and Post-earthquake Evacuation Preparedness in Buildings","Enhancing the earthquake behavioral responses and post-earthquake evacuation
preparedness of building occupants is beneficial to increasing their chances of
survival and reducing casualties after the main shock of an earthquake.
Traditionally, training approaches such as seminars, posters, videos or drills
are applied to enhance preparedness. However, they are not highly engaging and
have limited sensory capabilities to mimic life-threatening scenarios for the
purpose of training potential participants. Immersive Virtual Reality (IVR) and
Serious Games (SG) as innovative digital technologies can be used to create
training tools to overcome these limitations. In this study, we propose an IVR
SG-based training system to improve earthquake behavioral responses and
post-earthquake evacuation preparedness. Auckland City Hospital was chosen as a
case study to test our IVR SG training system. A set of learning outcomes based
on best evacuation practice has been identified and embedded into several
training scenarios of the IVR SG. Hospital staff (healthcare and administrative
professionals) and visitors were recruited as participants to be exposed to
these training scenarios. Participants' preparedness has been measured along
two dimensions: 1) Knowledge about best evacuation practice; 2) Self-efficacy
in dealing with earthquake emergencies. Assessment results showed that there
was a significant knowledge and self-efficacy increase after the training. And
participants acknowledged that it was easy and engaging to learn best
evacuation practice knowledge through the IVR SG training system.","['Zhenan Feng', 'Vicente A. González', 'Robert Amor', 'Michael Spearpoint', 'Jared Thomas', 'Rafael Sacks', 'Ruggiero Lovreglio', 'Guillermo Cabrera-Guerrero']",2019-05-27T09:44:44Z,http://arxiv.org/abs/1905.11082v1,['cs.CY'],"Virtual reality,Serious games,Earthquake preparedness,Behavioral responses,Evacuation,Training tools"
"FlightGoggles: A Modular Framework for Photorealistic Camera,
  Exteroceptive Sensor, and Dynamics Simulation","FlightGoggles is a photorealistic sensor simulator for perception-driven
robotic vehicles. The key contributions of FlightGoggles are twofold. First,
FlightGoggles provides photorealistic exteroceptive sensor simulation using
graphics assets generated with photogrammetry. Second, it provides the ability
to combine (i) synthetic exteroceptive measurements generated in silico in real
time and (ii) vehicle dynamics and proprioceptive measurements generated in
motio by vehicle(s) in a motion-capture facility. FlightGoggles is capable of
simulating a virtual-reality environment around autonomous vehicle(s). While a
vehicle is in flight in the FlightGoggles virtual reality environment,
exteroceptive sensors are rendered synthetically in real time while all complex
extrinsic dynamics are generated organically through the natural interactions
of the vehicle. The FlightGoggles framework allows for researchers to
accelerate development by circumventing the need to estimate complex and
hard-to-model interactions such as aerodynamics, motor mechanics, battery
electrochemistry, and behavior of other agents. The ability to perform
vehicle-in-the-loop experiments with photorealistic exteroceptive sensor
simulation facilitates novel research directions involving, e.g., fast and
agile autonomous flight in obstacle-rich environments, safe human interaction,
and flexible sensor selection. FlightGoggles has been utilized as the main test
for selecting nine teams that will advance in the AlphaPilot autonomous drone
racing challenge. We survey approaches and results from the top AlphaPilot
teams, which may be of independent interest.","['Winter Guerra', 'Ezra Tal', 'Varun Murali', 'Gilhyun Ryou', 'Sertac Karaman']",2019-05-27T17:59:38Z,http://arxiv.org/abs/1905.11377v2,['cs.RO'],"FlightGoggles,Sensor simulation,Photorealistic,Exteroceptive,Dynamics simulation,Photogrammetry,Proprioceptive,Virtual reality,Autonomous vehicle,Motion-capture"
Maps and Globes in Virtual Reality,"This paper explores different ways to render world-wide geographic maps in
virtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's
viewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c)
an egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved
map, created by projecting the map onto a section of a sphere which curves
around the user. In all four visualisations the geographic centre can be
smoothly adjusted with a standard handheld VR controller and the user, through
a head-tracked headset, can physically move around the visualisation. For
distance comparison, exocentric globe is more accurate than egocentric globe
and flat map. For area comparison, more time is required with exocentric and
egocentric globes than with flat and curved maps. For direction estimation, the
exocentric globe is more accurate and faster than the other visual
presentations. Our study participants had a weak preference for the exocentric
globe. Generally, the curved map had benefits over the flat map. In almost all
cases the egocentric globe was found to be the least effective visualisation.
Overall, our results provide support for the use of exocentric globes for
geographic visualisation in mixed-reality.","['Yalong Yang', 'Bernhard Jenny', 'Tim Dwyer', 'Kim Marriott', 'Haohui Chen', 'Maxime Cordeil']",2019-08-06T11:45:51Z,http://arxiv.org/abs/1908.02088v1,"['cs.HC', 'cs.GR', 'cs.MM']","geographic maps,virtual reality,3D globe,flat map,curved map,exocentric globe,egocentric globe,handheld VR controller,head-tracked headset,mixed-reality"
Towards Generating Ambisonics Using Audio-Visual Cue for Virtual Reality,"Ambisonics i.e., a full-sphere surround sound, is quintessential with
360-degree visual content to provide a realistic virtual reality (VR)
experience. While 360-degree visual content capture gained a tremendous boost
recently, the estimation of corresponding spatial sound is still challenging
due to the required sound-field microphones or information about the
sound-source locations. In this paper, we introduce a novel problem of
generating Ambisonics in 360-degree videos using the audio-visual cue. With
this aim, firstly, a novel 360-degree audio-visual video dataset of 265 videos
is introduced with annotated sound-source locations. Secondly, a pipeline is
designed for an automatic Ambisonic estimation problem. Benefiting from the
deep learning-based audio-visual feature-embedding and prediction modules, our
pipeline estimates the 3D sound-source locations and further use such locations
to encode to the B-format. To benchmark our dataset and pipeline, we
additionally propose evaluation criteria to investigate the performance using
different 360-degree input representations. Our results demonstrate the
efficacy of the proposed pipeline and open up a new area of research in
360-degree audio-visual analysis for future investigations.","['Aakanksha Rana', 'Cagri Ozcinar', 'Aljoscha Smolic']",2019-08-16T14:49:30Z,http://arxiv.org/abs/1908.06752v1,"['cs.SD', 'cs.CV', 'cs.LG', 'cs.MM', 'eess.AS']","Ambisonics,Audio-Visual Cue,Virtual Reality,360-degree,Sound-field microphones,Sound-source locations,Deep learning,B-format,Dataset,Evaluation criteria"
"Interactive molecular dynamics in virtual reality for accurate flexible
  protein-ligand docking","Simulating drug binding and unbinding is a challenge, as the rugged energy
landscapes that separate bound and unbound states require extensive sampling
that consumes significant computational resources. Here, we describe the use of
interactive molecular dynamics in virtual reality (iMD-VR) as an accurate
low-cost strategy for flexible protein-ligand docking. We outline an
experimental protocol which enables expert iMD-VR users to guide ligands into
and out of the binding pockets of trypsin, neuraminidase, and HIV-1 protease,
and recreate their respective crystallographic protein-ligand binding poses
within 5 - 10 minutes. Following a brief training phase, our studies shown that
iMD-VR novices were able to generate unbinding and rebinding pathways on
similar timescales as iMD-VR experts, with the majority able to recover binding
poses within 2.15 Angstrom RMSD of the crystallographic binding pose. These
results indicate that iMD-VR affords sufficient control for users to carry out
the detailed atomic manipulations required to dock flexible ligands into
dynamic enzyme active sites and recover crystallographic poses, offering an
interesting new approach for simulating drug docking and generating binding
hypotheses.","['Helen M. Deeks', 'Rebecca K. Walters', 'Stephanie R. Hare', ""Michael B. O'Connor"", 'Adrian J. Mulholland', 'David R. Glowacki']",2019-08-20T14:26:08Z,http://arxiv.org/abs/1908.07395v2,"['physics.bio-ph', 'physics.comp-ph', 'q-bio.BM']","interactive molecular dynamics,virtual reality,protein-ligand docking,flexible ligands,binding pockets,trypsin,neuraminidase,HIV-1 protease,crystallographic binding pose,drug docking"
"Design, Assembly, Calibration, and Measurement of an Augmented Reality
  Haploscope","A haploscope is an optical system which produces a carefully controlled
virtual image. Since the development of Wheatstone's original stereoscope in
1838, haploscopes have been used to measure perceptual properties of human
stereoscopic vision. This paper presents an augmented reality (AR) haploscope,
which allows the viewing of virtual objects superimposed against the real
world. Our lab has used generations of this device to make a careful series of
perceptual measurements of AR phenomena, which have been described in
publications over the previous 8 years. This paper systematically describes the
design, assembly, calibration, and measurement of our AR haploscope. These
methods have been developed and improved in our lab over the past 10 years.
Despite the fact that 180 years have elapsed since the original report of
Wheatstone's stereoscope, we have not previously found a paper that describes
these kinds of details.","['Nate Phillips', 'Kristen Massey', 'Mohammed Safayet Arefin', 'J. Edward Swan II']",2019-08-21T23:32:57Z,http://arxiv.org/abs/1908.08532v1,"['cs.GR', 'cs.HC']","Augmented Reality,Haploscope,Optical System,Virtual Image,Stereoscopic Vision,Perceptual Measurements,Design,Assembly,Calibration,Measurement"
Study of 3D Virtual Reality Picture Quality,"Virtual Reality (VR) and its applications have attracted significant and
increasing attention. However, the requirements of much larger file sizes,
different storage formats, and immersive viewing conditions pose significant
challenges to the goals of acquiring, transmitting, compressing and displaying
high quality VR content. Towards meeting these challenges, it is important to
be able to understand the distortions that arise and that can affect the
perceived quality of displayed VR content. It is also important to develop ways
to automatically predict VR picture quality. Meeting these challenges requires
basic tools in the form of large, representative subjective VR quality
databases on which VR quality models can be developed and which can be used to
benchmark VR quality prediction algorithms. Towards making progress in this
direction, here we present the results of an immersive 3D subjective image
quality assessment study. In the study, 450 distorted images obtained from 15
pristine 3D VR images modified by 6 types of distortion of varying severities
were evaluated by 42 subjects in a controlled VR setting. Both the subject
ratings as well as eye tracking data were recorded and made available as part
of the new database, in hopes that the relationships between gaze direction and
perceived quality might be better understood. We also evaluated several
publicly available IQA models on the new database, and also report a
statistical evaluation of the performances of the compared IQA models.","['Meixu Chen', 'Yize Jin', 'Todd Goodall', 'Xiangxu Yu', 'Alan C. Bovik']",2019-10-07T20:42:54Z,http://arxiv.org/abs/1910.03074v4,['eess.IV'],"3D Virtual Reality,Picture Quality,VR content,File sizes,Storage formats,Immersive viewing,VR quality,Distortions,Subjective image quality assessment,IQA models"
Shooting Labels: 3D Semantic Labeling by Virtual Reality,"Availability of a few, large-size, annotated datasets, like ImageNet, Pascal
VOC and COCO, has lead deep learning to revolutionize computer vision research
by achieving astonishing results in several vision tasks.We argue that new
tools to facilitate generation of annotated datasets may help spreading
data-driven AI throughout applications and domains. In this work we propose
Shooting Labels, the first 3D labeling tool for dense 3D semantic segmentation
which exploits Virtual Reality to render the labeling task as easy and fun as
playing a video-game. Our tool allows for semantically labeling large scale
environments very expeditiously, whatever the nature of the 3D data at hand
(e.g. point clouds, mesh). Furthermore, Shooting Labels efficiently integrates
multiusers annotations to improve the labeling accuracy automatically and
compute a label uncertainty map. Besides, within our framework the 3D
annotations can be projected into 2D images, thereby speeding up also a
notoriously slow and expensive task such as pixel-wise semantic labeling. We
demonstrate the accuracy and efficiency of our tool in two different scenarios:
an indoor workspace provided by Matterport3D and a large-scale outdoor
environment reconstructed from 1000+ KITTI images.","['Pierluigi Zama Ramirez', 'Claudio Paternesi', 'Luca De Luigi', 'Luigi Lella', 'Daniele De Gregorio', 'Luigi Di Stefano']",2019-10-11T08:11:27Z,http://arxiv.org/abs/1910.05021v2,['cs.CV'],"3D semantic labeling,Virtual Reality,annotated datasets,deep learning,computer vision,3D data,point clouds,mesh,multiuser annotations,label uncertainty map"
"AeroVR: Virtual Reality-based Teleoperation with Tactile Feedback for
  Aerial Manipulation","Drone application for aerial manipulation is tested in such areas as
industrial maintenance, supporting the rescuers in emergencies, and e-commerce.
Most of such applications require teleoperation. The operator receives visual
feedback from the camera installed on a robot arm or drone. As aerial
manipulation requires delicate and precise motion of robot arm, the camera data
delay, narrow field of view, and blurred images caused by drone dynamics can
lead the UAV to crash. The paper focuses on the development of a novel
teleoperation system for aerial manipulation using Virtual Reality (VR). The
controlled system consists of UAV with a 4-DoF robotic arm and embedded
sensors. VR application presents the digital twin of drone and remote
environment to the user through a head-mounted display (HMD). The operator
controls the position of the robotic arm and gripper with VR trackers worn on
the arm and tracking glove with vibrotactile feedback. Control data is
translated directly from VR to the real robot in real-time. The experimental
results showed a stable and robust teleoperation mediated by the VR scene. The
proposed system can considerably improve the quality of aerial manipulations.","['Grigoriy A. Yashin', 'Daria Trinitatova', 'Ruslan T. Agishev', 'Roman Ibrahimov', 'Dzmitry Tsetserukou']",2019-10-25T10:21:03Z,http://arxiv.org/abs/1910.11604v1,['cs.RO'],"Aerial manipulation,Teleoperation,Tactile feedback,Virtual Reality,UAV,Robotic arm,VR trackers,Vibrotactile feedback,Drone dynamics,Industrial maintenance"
Mixing realities for sketch retrieval in Virtual Reality,"Drawing tools for Virtual Reality (VR) enable users to model 3D designs from
within the virtual environment itself. These tools employ sketching and
sculpting techniques known from desktop-based interfaces and apply them to
hand-based controller interaction. While these techniques allow for mid-air
sketching of basic shapes, it remains difficult for users to create detailed
and comprehensive 3D models. In our work, we focus on supporting the user in
designing the virtual environment around them by enhancing sketch-based
interfaces with a supporting system for interactive model retrieval. Through
sketching, an immersed user can query a database containing detailed 3D models
and replace them into the virtual environment. To understand supportive
sketching within a virtual environment, we compare different methods of sketch
interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D
sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet.
%using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and
3D mid-air sketching. Our results show that 3D mid-air sketching is considered
to be a more intuitive method to search a collection of models while the
addition of physical devices creates confusion due to the complications of
their inclusion within a virtual environment. While we pose our work as a
retrieval problem for 3D models of chairs, our results can be extrapolated to
other sketching tasks for virtual environments.","['Daniele Giunchi', 'Stuart james', 'Donald Degraen', 'Anthony Steed']",2019-10-25T11:52:25Z,http://arxiv.org/abs/1910.11637v2,"['cs.HC', 'cs.CV']","Virtual Reality,Sketch retrieval,3D designs,Sketching techniques,Interactive model retrieval,Sketch interaction,Mid-air sketching,Virtual tablet,Virtual whiteboard,Physical tablet"
"Decoding pedestrian and automated vehicle interactions using immersive
  virtual reality and interpretable deep learning","To ensure pedestrian friendly streets in the era of automated vehicles,
reassessment of current policies, practices, design, rules and regulations of
urban areas is of importance. This study investigates pedestrian crossing
behaviour, as an important element of urban dynamics that is expected to be
affected by the presence of automated vehicles. For this purpose, an
interpretable machine learning framework is proposed to explore factors
affecting pedestrians' wait time before crossing mid-block crosswalks in the
presence of automated vehicles. To collect rich behavioural data, we developed
a dynamic and immersive virtual reality experiment, with 180 participants from
a heterogeneous population in 4 different locations in the Greater Toronto Area
(GTA). Pedestrian wait time behaviour is then analyzed using a data-driven Cox
Proportional Hazards (CPH) model, in which the linear combination of the
covariates is replaced by a flexible non-linear deep neural network. The
proposed model achieved a 5% improvement in goodness of fit, but more
importantly, enabled us to incorporate a richer set of covariates. A game
theoretic based interpretability method is used to understand the contribution
of different covariates to the time pedestrians wait before crossing. Results
show that the presence of automated vehicles on roads, wider lane widths, high
density on roads, limited sight distance, and lack of walking habits are the
main contributing factors to longer wait times. Our study suggested that, to
move towards pedestrian-friendly urban areas, national level educational
programs for children, enhanced safety measures for seniors, promotion of
active modes of transportation, and revised traffic rules and regulations
should be considered.","['Arash Kalatian', 'Bilal Farooq']",2020-02-18T01:30:29Z,http://arxiv.org/abs/2002.07325v2,"['cs.HC', 'cs.LG']","automated vehicles,pedestrian crossing behavior,machine learning framework,wait time,mid-block crosswalks,virtual reality experiment,Cox Proportional Hazards model,deep neural network,game theoretic,interpretability"
"Risk-Based Optimization of Virtual Reality over Terahertz Reconfigurable
  Intelligent Surfaces","In this paper, the problem of associating reconfigurable intelligent surfaces
(RISs) to virtual reality (VR) users is studied for a wireless VR network. In
particular, this problem is considered within a cellular network that employs
terahertz (THz) operated RISs acting as base stations. To provide a seamless VR
experience, high data rates and reliable low latency need to be continuously
guaranteed. To address these challenges, a novel risk-based framework based on
the entropic value-at-risk is proposed for rate optimization and reliability
performance. Furthermore, a Lyapunov optimization technique is used to
reformulate the problem as a linear weighted function, while ensuring that
higher order statistics of the queue length are maintained under a threshold.
To address this problem, given the stochastic nature of the channel, a
policy-based reinforcement learning (RL) algorithm is proposed. Since the state
space is extremely large, the policy is learned through a deep-RL algorithm. In
particular, a recurrent neural network (RNN) RL framework is proposed to
capture the dynamic channel behavior and improve the speed of conventional RL
policy-search algorithms. Simulation results demonstrate that the maximal queue
length resulting from the proposed approach is only within 1% of the optimal
solution. The results show a high accuracy and fast convergence for the RNN
with a validation accuracy of 91.92%.","['Christina Chaccour', 'Mehdi Naderi Soorki', 'Walid Saad', 'Mehdi Bennis', 'Petar Popovski']",2020-02-20T22:41:10Z,http://arxiv.org/abs/2002.09052v1,"['cs.IT', 'eess.SP', 'math.IT']","Virtual reality,Terahertz,Reconfigurable intelligent surfaces,Entropic value-at-risk,Lyapunov optimization,Queue length,Reinforcement learning,Deep RL,Recurrent neural network,Simulation."
On the Effectiveness of Virtual Reality-based Training for Robotic Setup,"Virtual Reality (VR) is rapidly increasing in popularity as a teaching tool.
It allows for the creation of a highly immersive, three-dimensional virtual
environment intended to simulate real-life environments. With more robots
saturating the industry - from manufacturing to healthcare, there is a need to
train end-users on how to set up, operate, tear down, and troubleshoot the
robot. Even though VR has become widely used in training surgeons on the
psychomotor skills associated with operating the robot, little research has
been done to see how the benefits of VR could translate to teaching the bedside
staff, tasked with supporting the robot during the full end-to-end surgical
procedure. We trained 30 participants on how to set up a robotic arm in an
environment mimicking clinical setup. We divided these participants equally
into 3 groups with one group trained with paper-based instructions, one with
video-based instructions and one with VR-based instructions. We then compared
and contrasted these three different training methods. VR and paper-based were
highly favored training mediums over video-based. VR-trained participants
achieved slightly higher fidelity of individual robotic joint angles,
suggesting better comprehension of the spatial awareness skills necessary to
achieve desired arm positioning. In addition, VR resulted in higher
reproducibility of setup fidelity and more consistency in user confidence
levels as compared to paper and video-based training.","['Arian Mehrfard', 'Javad Fotouhi', 'Tess Forster', 'Giacomo Taylor', 'Danyal Fer', 'Deborah Nagle', 'Nassir Navab', 'Bernhard Fuerst']",2020-03-03T14:42:25Z,http://arxiv.org/abs/2003.01540v1,['cs.RO'],"Virtual Reality,Robotic Setup,Training,Immersive Environment,Surgical Procedure,Psychomotor Skills,Spatial Awareness,Fidelity,Reproducibility"
"The Security-Utility Trade-off for Iris Authentication and Eye Animation
  for Social Virtual Avatars","The gaze behavior of virtual avatars is critical to social presence and
perceived eye contact during social interactions in Virtual Reality. Virtual
Reality headsets are being designed with integrated eye tracking to enable
compelling virtual social interactions. This paper shows that the near
infra-red cameras used in eye tracking capture eye images that contain iris
patterns of the user. Because iris patterns are a gold standard biometric, the
current technology places the user's biometric identity at risk. Our first
contribution is an optical defocus based hardware solution to remove the iris
biometric from the stream of eye tracking images. We characterize the
performance of this solution with different internal parameters. Our second
contribution is a psychophysical experiment with a same-different task that
investigates the sensitivity of users to a virtual avatar's eye movements when
this solution is applied. By deriving detection threshold values, our findings
provide a range of defocus parameters where the change in eye movements would
go unnoticed in a conversational setting. Our third contribution is a
perceptual study to determine the impact of defocus parameters on the perceived
eye contact, attentiveness, naturalness, and truthfulness of the avatar. Thus,
if a user wishes to protect their iris biometric, our approach provides a
solution that balances biometric protection while preventing their conversation
partner from perceiving a difference in the user's virtual avatar. This work is
the first to develop secure eye tracking configurations for VR/AR/XR
applications and motivates future work in the area.","['Brendan John', 'Sophie Jörg', 'Sanjeev Koppal', 'Eakta Jain']",2020-03-09T16:48:25Z,http://arxiv.org/abs/2003.04250v1,['cs.HC'],"Iris Authentication,Eye Animation,Virtual Reality,Eye Tracking,Biometric Identity,Optical Defocus,Psychophysical Experiment,Detection Threshold,Perceptual Study,Secure Eye Tracking"
Assessment of Empathy in an Affective VR Environment using EEG Signals,"With the advancements in social robotics and virtual avatars, it becomes
increasingly important that these agents adapt their behavior to the mood,
feelings and personality of their users. One such aspect of the user is
empathy. Whereas many studies measure empathy through offline measures that are
collected after empathic stimulation (e.g. post-hoc questionnaires), the
current study aimed to measure empathy online, using brain activity collected
during the experience. Participants watched an affective 360 video of a child
experiencing domestic violence in a virtual reality headset while their EEG
signals were recorded. Results showed a significant attenuation of alpha, theta
and delta asymmetry in the frontal and central areas of the brain. Moreover, a
significant relationship between participants' empathy scores and their frontal
alpha asymmetry at baseline was found. These results demonstrate specific brain
activity alterations when participants are exposed to an affective virtual
reality environment, with the level of empathy as a personality trait being
visible in brain activity during a baseline measurement. These findings suggest
the potential of EEG measurements for development of passive brain-computer
interfaces that assess the user's affective responses in real-time and
consequently adapt the behavior of socially intelligent agents for a
personalized interaction.","['Maryam Alimardani', 'Annabella Hermans', 'Angelica M. Tinga']",2020-03-24T14:35:27Z,http://arxiv.org/abs/2003.10886v1,['cs.HC'],"Empathy,Affective,VR Environment,EEG Signals,Brain Activity,Alpha Asymmetry,Theta,Delta,Personality Trait,Brain-Computer Interfaces"
"Influence of Hand Tracking as a way of Interaction in Virtual Reality on
  User Experience","With the rising interest in Virtual Reality and the fast development and
improvement of available devices, new features of interactions are becoming
available. One of them that is becoming very popular is hand tracking, as the
idea to replace controllers for interactions in virtual worlds. This experiment
aims to compare different interaction types in VR using either controllers or
hand tracking. Participants had to play two simple VR games with various types
of tasks in those games - grabbing objects or typing numbers. While playing,
they were using interactions with different visualizations of hands and
controllers. The focus of this study was to investigate user experience of
varying interactions (controller vs. hand tracking) for those two simple tasks.
Results show that different interaction types statistically significantly
influence reported emotions with Self-Assessment Manikin (SAM), where for hand
tracking participants were feeling higher valence, but lower arousal and
dominance. Additionally, task type of grabbing was reported to be more
realistic, and participants experienced a higher presence. Surprisingly,
participants rated the interaction type with controllers where both where hands
and controllers were visualized as statistically most preferred. Finally, hand
tracking for both tasks was rated with the System Usability Scale (SUS) scale,
and hand tracking for the task typing was rated as statistically significantly
more usable. These results can drive further research and, in the long term,
contribute to help selecting the most matching interaction modality for a task.","['Jan-Niklas Voigt-Antons', 'Tanja KojiāE, 'Danish Ali', 'Sebastian Möller']",2020-04-27T08:43:40Z,http://arxiv.org/abs/2004.12642v1,"['cs.HC', 'cs.MM']","Virtual Reality,Hand tracking,Interaction,User experience,Controllers,Virtual worlds,Self-Assessment Manikin,Presence,System Usability Scale,Usability"
OpenEDS2020: Open Eyes Dataset,"We present the second edition of OpenEDS dataset, OpenEDS2020, a novel
dataset of eye-image sequences captured at a frame rate of 100 Hz under
controlled illumination, using a virtual-reality head-mounted display mounted
with two synchronized eye-facing cameras. The dataset, which is anonymized to
remove any personally identifiable information on participants, consists of 80
participants of varied appearance performing several gaze-elicited tasks, and
is divided in two subsets: 1) Gaze Prediction Dataset, with up to 66,560
sequences containing 550,400 eye-images and respective gaze vectors, created to
foster research in spatio-temporal gaze estimation and prediction approaches;
and 2) Eye Segmentation Dataset, consisting of 200 sequences sampled at 5 Hz,
with up to 29,500 images, of which 5% contain a semantic segmentation label,
devised to encourage the use of temporal information to propagate labels to
contiguous frames. Baseline experiments have been evaluated on OpenEDS2020, one
for each task, with average angular error of 5.37 degrees when performing gaze
prediction on 1 to 5 frames into the future, and a mean intersection over union
score of 84.1% for semantic segmentation. As its predecessor, OpenEDS dataset,
we anticipate that this new dataset will continue creating opportunities to
researchers in eye tracking, machine learning and computer vision communities,
to advance the state of the art for virtual reality applications. The dataset
is available for download upon request at
http://research.fb.com/programs/openeds-2020-challenge/.","['Cristina Palmero', 'Abhishek Sharma', 'Karsten Behrendt', 'Kapil Krishnakumar', 'Oleg V. Komogortsev', 'Sachin S. Talathi']",2020-05-08T06:53:05Z,http://arxiv.org/abs/2005.03876v1,"['cs.CV', 'cs.LG']","OpenEDS2020,dataset,eye-image sequences,frame rate,virtual-reality,eye-facing cameras,gaze prediction,spatio-temporal gaze estimation,eye segmentation,semantic segmentation."
"Facial Electromyography-based Adaptive Virtual Reality Gaming for
  Cognitive Training","Cognitive training has shown promising results for delivering improvements in
human cognition related to attention, problem solving, reading comprehension
and information retrieval. However, two frequently cited problems in cognitive
training literature are a lack of user engagement with the training programme,
and a failure of developed skills to generalise to daily life. This paper
introduces a new cognitive training (CT) paradigm designed to address these two
limitations by combining the benefits of gamification, virtual reality (VR),
and affective adaptation in the development of an engaging, ecologically valid,
CT task. Additionally, it incorporates facial electromyography (EMG) as a means
of determining user affect while engaged in the CT task. This information is
then utilised to dynamically adjust the game's difficulty in real-time as users
play, with the aim of leading them into a state of flow. Affect recognition
rates of 64.1% and 76.2%, for valence and arousal respectively, were achieved
by classifying a DWT-Haar approximation of the input signal using kNN. The
affect-aware VR cognitive training intervention was then evaluated with a
control group of older adults. The results obtained substantiate the notion
that adaptation techniques can lead to greater feelings of competence and a
more appropriate challenge of the user's skills.","['Lorcan Reidy', 'Dennis Chan', 'Charles Nduka', 'Hatice Gunes']",2020-04-27T10:01:52Z,http://arxiv.org/abs/2005.05023v3,"['cs.HC', 'cs.LG', 'I.2; K.8']","Facial Electromyography,Virtual Reality,Cognitive Training,Gamification,Affective Adaptation,Facial EMG,Dynamic Difficulty Adjustment,Affect Recognition,DWT-Haar,kNN"
"Automatic Recommendation of Strategies for Minimizing Discomfort in
  Virtual Environments","Virtual reality (VR) is an imminent trend in games, education, entertainment,
military, and health applications, as the use of head-mounted displays is
becoming accessible to the mass market. Virtual reality provides immersive
experiences but still does not offer an entirely perfect situation, mainly due
to Cybersickness (CS) issues. In this work, we first present a detailed review
about possible causes of CS. Following, we propose a novel CS prediction
solution. Our system is able to suggest if the user may be entering in the next
moments of the application into an illness situation. We use Random Forest
classifiers, based on a dataset we have produced. The CSPQ (Cybersickness
Profile Questionnaire) is also proposed, which is used to identify the player's
susceptibility to CS and the dataset construction. In addition, we designed two
immersive environments for empirical studies where participants are asked to
complete the questionnaire and describe (orally) the degree of discomfort
during their gaming experience. Our data was achieved through 84 individuals on
different days, using VR devices. Our proposal also allows us to identify which
are the most frequent attributes (causes) in the observed discomfort
situations.","['Thiago Porcino', 'Esteban Clua', 'Daniela Trevisan', 'Érick Rodrigues', 'Alexandre Silva']",2020-06-27T19:28:48Z,http://arxiv.org/abs/2006.15432v1,"['cs.HC', 'cs.GR', 'cs.LG']","Virtual reality,Head-mounted displays,Cybersickness,Random Forest classifiers,Dataset,Cybersickness Profile Questionnaire,Immersive environments,Empirical studies,Discomfort,Attributes"
"Building Trust in Autonomous Vehicles: Role of Virtual Reality Driving
  Simulators in HMI Design","The investigation of factors contributing at making humans trust Autonomous
Vehicles (AVs) will play a fundamental role in the adoption of such technology.
The user's ability to form a mental model of the AV, which is crucial to
establish trust, depends on effective user-vehicle communication; thus, the
importance of Human-Machine Interaction (HMI) is poised to increase. In this
work, we propose a methodology to validate the user experience in AVs based on
continuous, objective information gathered from physiological signals, while
the user is immersed in a Virtual Reality-based driving simulation. We applied
this methodology to the design of a head-up display interface delivering visual
cues about the vehicle' sensory and planning systems. Through this approach, we
obtained qualitative and quantitative evidence that a complete picture of the
vehicle's surrounding, despite the higher cognitive load, is conducive to a
less stressful experience. Moreover, after having been exposed to a more
informative interface, users involved in the study were also more willing to
test a real AV. The proposed methodology could be extended by adjusting the
simulation environment, the HMI and/or the vehicle's Artificial Intelligence
modules to dig into other aspects of the user experience.","['Lia Morra', 'Fabrizio Lamberti', 'F. Gabriele Pratticó', 'Salvatore La Rosa', 'Paolo Montuschi']",2020-07-27T08:42:07Z,http://arxiv.org/abs/2007.13371v1,"['cs.HC', 'cs.AI', 'cs.GR']","autonomous vehicles,virtual reality,driving simulators,HMI design,human-machine interaction,physiological signals,head-up display,cognitive load,artificial intelligence"
"Mesh Processing Strategies and Fractals for Three Dimensional
  Morphological Analysis of a Granitic Terrain using IRS LISS IV and Carto DEM","Virtual Reality (VR) enabled applications are becoming very important to
visualize the terrain features in 3D. In general 3D datasets generated from
high-resolution satellites and DEM occupy large volumes of data. However,
lightweight datasets are required to create better user experiences on VR
platforms. So, the present study develops a methodology to generate datasets
compatible with VR using Indian Remote Sensing satellite (IRS) sensors. A
Linear Imaging Self-Scanning System - IV (LISS IV) with 5.8 m spatial
resolution and Carto DEM are used for generating the 3D view using the Arc
environment and then converted into virtual reality modeling language (VRML)
format. In order to reduce the volume of the VRML dataset a quadratic edge
collapse decimation method is applied which reduces the number of faces in the
mesh while preserving the boundary and/or normal. A granitic terrain in the
south-west part of Hyderabad comprising of dyke intrusion is considered for the
generation of 3D VR dataset, as it has high elevation differences thus
rendering it most suitable for the present study. Further, the enhanced
geomorphological features such as hills and valleys, geological structures such
as fractures, intrusive (dykes) are studied and found suitable for better
interpretation.","['K. Seshadri', 'M. Naresh Kumar']",2020-07-15T10:51:31Z,http://arxiv.org/abs/2008.01174v1,['cs.GR'],"Mesh Processing Strategies,Fractals,Three Dimensional,Morphological Analysis,Granitic Terrain,IRS LISS IV,Carto DEM,Virtual Reality,VR Platforms,VRML"
Pen-based Interaction with Spreadsheets in Mobile Virtual Reality,"Virtual Reality (VR) can enhance the display and interaction of mobile
knowledge work and in particular, spreadsheet applications. While spreadsheets
are widely used yet are challenging to interact with, especially on mobile
devices, using them in VR has not been explored in depth. A special uniqueness
of the domain is the contrast between the immersive and large display space
afforded by VR, contrasted by the very limited interaction space that may be
afforded for the information worker on the go, such as an airplane seat or a
small work-space. To close this gap, we present a tool-set for enhancing
spreadsheet interaction on tablets using immersive VR headsets and pen-based
input. This combination opens up many possibilities for enhancing the
productivity for spreadsheet interaction. We propose to use the space around
and in front of the tablet for enhanced visualization of spreadsheet data and
meta-data. For example, extending sheet display beyond the bounds of the
physical screen, or easier debugging by uncovering hidden dependencies between
sheet's cells. Combining the precise on-screen input of a pen with spatial
sensing around the tablet, we propose tools for the efficient creation and
editing of spreadsheets functions such as off-the-screen layered menus,
visualization of sheets dependencies, and gaze-and-touch-based switching
between spreadsheet tabs. We study the feasibility of the proposed tool-set
using a video-based online survey and an expert-based assessment of indicative
human performance potential.","['Travis Gesslein', 'Verena Biener', 'Philipp Gagel', 'Daniel Schneider', 'Per Ola Kristensson', 'Eyal Ofek', 'Michel Pahud', 'Jens Grubert']",2020-08-11T06:39:35Z,http://arxiv.org/abs/2008.04543v1,"['cs.HC', 'I.3.7']","mobile virtual reality,pen-based interaction,spreadsheets,immersive VR headsets,tablet,spreadsheet applications,spreadsheet interaction,pen-based input,spatial sensing,off-the-screen layered menus"
"Facial Expression Recognition Under Partial Occlusion from Virtual
  Reality Headsets based on Transfer Learning","Facial expressions of emotion are a major channel in our daily
communications, and it has been subject of intense research in recent years. To
automatically infer facial expressions, convolutional neural network based
approaches has become widely adopted due to their proven applicability to
Facial Expression Recognition (FER) task.On the other hand Virtual Reality (VR)
has gained popularity as an immersive multimedia platform, where FER can
provide enriched media experiences. However, recognizing facial expression
while wearing a head-mounted VR headset is a challenging task due to the upper
half of the face being completely occluded. In this paper we attempt to
overcome these issues and focus on facial expression recognition in presence of
a severe occlusion where the user is wearing a head-mounted display in a VR
setting. We propose a geometric model to simulate occlusion resulting from a
Samsung Gear VR headset that can be applied to existing FER datasets. Then, we
adopt a transfer learning approach, starting from two pretrained networks,
namely VGG and ResNet. We further fine-tune the networks on FER+ and RAF-DB
datasets. Experimental results show that our approach achieves comparable
results to existing methods while training on three modified benchmark datasets
that adhere to realistic occlusion resulting from wearing a commodity VR
headset. Code for this paper is available at:
https://github.com/bita-github/MRP-FER","['Bita Houshmand', 'Naimul Khan']",2020-08-12T20:25:07Z,http://arxiv.org/abs/2008.05563v1,['cs.CV'],"Facial Expression Recognition,Transfer Learning,Convolutional Neural Network,Virtual Reality Headsets,Occlusion,Geometric Model,Pretrained Networks,FER+ dataset,RAF-DB dataset"
"A survey on applications of augmented, mixed and virtual reality for
  nature and environment","Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are
technologies of great potential due to the engaging and enriching experiences
they are capable of providing. Their use is rapidly increasing in diverse
fields such as medicine, manufacturing or entertainment. However, the
possibilities that AR, VR and MR offer in the area of environmental
applications are not yet widely explored. In this paper we present the outcome
of a survey meant to discover and classify existing AR/VR/MR applications that
can benefit the environment or increase awareness on environmental issues. We
performed an exhaustive search over several online publication access platforms
and past proceedings of major conferences in the fields of AR/VR/MR. Identified
relevant papers were filtered based on novelty, technical soundness, impact and
topic relevance, and classified into different categories. Referring to the
selected papers, we discuss how the applications of each category are
contributing to environmental protection, preservation and sensitization
purposes. We further analyse these approaches as well as possible future
directions in the scope of existing and upcoming AR/VR/MR enabling
technologies.","['Jason Rambach', 'Gergana Lilligreen', 'Alexander Schäfer', 'Ramya Bankanal', 'Alexander Wiebel', 'Didier Stricker']",2020-08-27T09:59:27Z,http://arxiv.org/abs/2008.12024v2,"['cs.HC', 'cs.CV', 'cs.CY', 'cs.GT']","augmented reality,virtual reality,mixed reality,environmental applications,survey,classification,awareness,environmental protection,preservation,sensitization"
Comparing Pedestrian Navigation Methods in Virtual Reality and Real Life,"Mobile navigation apps are among the most used mobile applications and are
often used as a baseline to evaluate new mobile navigation technologies in
field studies. As field studies often introduce external factors that are hard
to control for, we investigate how pedestrian navigation methods can be
evaluated in virtual reality (VR). We present a study comparing navigation
methods in real life (RL) and VR to evaluate if VR environments are a viable
alternative to RL environments when it comes to testing these. In a series of
studies, participants navigated a real and a virtual environment using a paper
map and a navigation app on a smartphone. We measured the differences in
navigation performance, task load and spatial knowledge acquisition between RL
and VR. From these we formulate guidelines for the improvement of pedestrian
navigation systems in VR like improved legibility for small screen devices. We
furthermore discuss appropriate low-cost and low-space VR-locomotion techniques
and discuss more controllable locomotion techniques.","['Gian-Luca Savino', 'Niklas Emanuel', 'Steven Kowalzik', 'Felix A. Kroll', 'Marvin C. Lange', 'Matthis Laudan', 'Rieke Leder', 'Zhanhua Liang', 'Dayana Markhabayeva', 'Martin Schmeißer', 'Nicolai Schütz', 'Carolin Stellmacher', 'Zihe Xu', 'Kerstin Bub', 'Thorsten Kluss', 'Jaime Maldonado', 'Ernst Kruijff', 'Johannes Schöning']",2020-10-06T09:08:23Z,http://arxiv.org/abs/2010.02561v1,['cs.HC'],"pedestrian navigation,virtual reality,real life,field studies,mobile applications,navigation methods,spatial knowledge acquisition,VR environments,navigation performance"
An Evaluation Testbed for Locomotion in Virtual Reality,"A common operation performed in Virtual Reality (VR) environments is
locomotion. Although real walking can represent a natural and intuitive way to
manage displacements in such environments, its use is generally limited by the
size of the area tracked by the VR system (typically, the size of a room) or
requires expensive technologies to cover particularly extended settings. A
number of approaches have been proposed to enable effective explorations in VR,
each characterized by different hardware requirements and costs, and capable to
provide different levels of usability and performance. However, the lack of a
well-defined methodology for assessing and comparing available approaches makes
it difficult to identify, among the various alternatives, the best solutions
for selected application domains. To deal with this issue, this paper
introduces a novel evaluation testbed which, by building on the outcomes of
many separate works reported in the literature, aims to support a comprehensive
analysis of the considered design space. An experimental protocol for
collecting objective and subjective measures is proposed, together with a
scoring system able to rank locomotion approaches based on a weighted set of
requirements. Testbed usage is illustrated in a use case requesting to select
the technique to adopt in a given application scenario.","['Alberto Cannavò', 'Davide Calandra', 'F. Gabriele Pratticò', 'Valentina Gatteschi', 'Fabrizio Lamberti']",2020-10-20T10:21:15Z,http://arxiv.org/abs/2010.10178v1,"['cs.HC', 'cs.GR']","Virtual Reality,Locomotion,Evaluation Testbed,VR system,Hardware requirements,Usability,Performance,Methodology,Objective measures,Subjective measures"
"Unmasking Communication Partners: A Low-Cost AI Solution for Digitally
  Removing Head-Mounted Displays in VR-Based Telepresence","Face-to-face conversation in Virtual Reality (VR) is a challenge when
participants wear head-mounted displays (HMD). A significant portion of a
participant's face is hidden and facial expressions are difficult to perceive.
Past research has shown that high-fidelity face reconstruction with personal
avatars in VR is possible under laboratory conditions with high-cost hardware.
In this paper, we propose one of the first low-cost systems for this task which
uses only open source, free software and affordable hardware. Our approach is
to track the user's face underneath the HMD utilizing a Convolutional Neural
Network (CNN) and generate corresponding expressions with Generative
Adversarial Networks (GAN) for producing RGBD images of the person's face. We
use commodity hardware with low-cost extensions such as 3D-printed mounts and
miniature cameras. Our approach learns end-to-end without manual intervention,
runs in real time, and can be trained and executed on an ordinary gaming
computer. We report evaluation results showing that our low-cost system does
not achieve the same fidelity of research prototypes using high-end hardware
and closed source software, but it is capable of creating individual facial
avatars with person-specific characteristics in movements and expressions.","['Philipp Ladwig', 'Alexander Pech', 'Ralf Dörner', 'Christian Geiger']",2020-11-06T23:17:12Z,http://arxiv.org/abs/2011.03630v1,"['cs.GR', 'cs.CV']","Virtual Reality,Head-Mounted Display,Face Reconstruction,Convolutional Neural Network,Generative Adversarial Networks,RGBD images,3D-printed mounts,Miniature cameras,End-to-end learning,Gaming computer"
"A Review of Deep Learning Approaches to EEG-Based Classification of
  Cybersickness in Virtual Reality","Cybersickness is an unpleasant side effect of exposure to a virtual reality
(VR) experience and refers to such physiological repercussions as nausea and
dizziness triggered in response to VR exposure. Given the debilitating effect
of cybersickness on the user experience in VR, academic interest in the
automatic detection of cybersickness from physiological measurements has
crested in recent years. Electroencephalography (EEG) has been extensively used
to capture changes in electrical activity in the brain and to automatically
classify cybersickness from brainwaves using a variety of machine learning
algorithms. Recent advances in deep learning (DL) algorithms and increasing
availability of computational resources for DL have paved the way for a new
area of research into the application of DL frameworks to EEG-based detection
of cybersickness. Accordingly, this review involved a systematic review of the
peer-reviewed papers concerned with the application of DL frameworks to the
classification of cybersickness from EEG signals. The relevant literature was
identified through exhaustive database searches, and the papers were
scrutinized with respect to experimental protocols for data collection, data
preprocessing, and DL architectures. The review revealed a limited number of
studies in this nascent area of research and showed that the DL frameworks
reported in these studies (i.e., DNN, CNN, and RNN) could classify
cybersickness with an average accuracy rate of 93%. This review provides a
summary of the trends and issues in the application of DL frameworks to the
EEG-based detection of cybersickness, with some guidelines for future research.",['Caglar Yildirim'],2020-12-01T21:50:53Z,http://arxiv.org/abs/2012.00855v1,['cs.HC'],"Deep learning,EEG-based classification,Cybersickness,Virtual reality,Electroencephalography,Machine learning algorithms,DL frameworks,DNN,CNN"
"Immersive Anatomical Scenes that Enable Multiple Users to Occupy the
  Same Virtual Space: A Tool for Surgical Planning and Education","3D modeling is becoming a well-developed field of medicine, but its
applicability can be limited due to the lack of software allowing for easy
utilizations of generated 3D visualizations. By leveraging recent advances in
virtual reality, we can rapidly create immersive anatomical scenes as well as
allow multiple users to occupy the same virtual space: i.e., over a local or
distributed network. This setup is ideal for pre-surgical planning and
education, allowing users to identify and study structures of interest. I
demonstrate here such a pipeline on a broad spectrum of anatomical models and
discuss its applicability to the medical field and its future prospects.3D
modeling is becoming a well-developed field of medicine, but its applicability
can be limited due to the lack of software allowing for easy utilizations of
generated 3D visualizations. By leveraging recent advances in virtual reality,
we can rapidly create immersive anatomical scenes as well as allow multiple
users to occupy the same virtual space: i.e., over a local or distributed
network. This setup is ideal for pre-surgical planning and education, allowing
users to identify and study structures of interest. I demonstrate here such a
pipeline on a broad spectrum of anatomical models and discuss its applicability
to the medical field and its future prospects.","['Alex J. Deakyne', 'Erik N. Gaasedelen', 'Tinen L. Iles', 'Paul A. Iaizzo']",2020-11-05T15:57:06Z,http://arxiv.org/abs/2012.02596v1,"['cs.HC', 'cs.GR']","3D modeling,medicine,software,virtual reality,immersive,anatomical scenes,surgical planning,education,structures,pipeline"
"Digital Reconstruction of Elmina Castle for Mobile Virtual Reality via
  Point-based Detail Transfer","Reconstructing 3D models from large, dense point clouds is critical to enable
Virtual Reality (VR) as a platform for entertainment, education, and heritage
preservation. Existing 3D reconstruction systems inevitably make trade-offs
between three conflicting goals: the efficiency of reconstruction (e.g., time
and memory requirements), the visual quality of the constructed scene, and the
rendering speed on the VR device. This paper proposes a reconstruction system
that simultaneously meets all three goals. The key idea is to avoid the
resource-demanding process of reconstructing a high-polygon mesh altogether.
Instead, we propose to directly transfer details from the original point cloud
to a low polygon mesh, which significantly reduces the reconstruction time and
cost, preserves the scene details, and enables real-time rendering on mobile VR
devices.
  While our technique is general, we demonstrate it in reconstructing cultural
heritage sites. We for the first time digitally reconstruct the Elmina Castle,
a UNESCO world heritage site at Ghana, from billions of laser-scanned points.
The reconstruction process executes on low-end desktop systems without
requiring high processing power, making it accessible to the broad community.
The reconstructed scenes render on Oculus Go in 60 FPS, providing a real-time
VR experience with high visual quality. Our project is part of the Digital
Elmina effort (http://digitalelmina.org/) between University of Rochester and
University of Ghana.","['Sifan Ye', 'Ting Wu', 'Michael Jarvis', 'Yuhao Zhu']",2020-12-19T17:11:43Z,http://arxiv.org/abs/2012.10739v3,['cs.MM'],"3D models,point clouds,Virtual Reality (VR),reconstruction system,high-polygon mesh,low polygon mesh,real-time rendering,mobile VR devices,cultural heritage sites,Elmina Castle"
"OpenUVR: an Open-Source System Framework for Untethered Virtual Reality
  Applications","Advancements in heterogeneous computing technologies enable the significant
potential of virtual reality (VR) applications. To offer the best user
experience (UX), a system should adopt an untethered, wireless-network-based
architecture to transfer VR content between the user and the content generator.
However, modern wireless network technologies make implementing such an
architecture challenging, as VR applications require superior video quality --
with high resolution, high frame rates, and very low latency.
  This paper presents OpenUVR, an open-source framework that uses commodity
hardware components to satisfy the demands of interactive, real-time VR
applications. OpenUVR significantly improves UX through a redesign of the
system stack and addresses the most time-sensitive issues associated with
redundant memory copying in modern computing systems. OpenUVR presents a
cross-layered VR datapath to avoid redundant data operations and computation
among system components, OpenUVR customizes the network stack to eliminate
unnecessary memory operations incurred by mismatching data formats in each
layer, and OpenUVR uses feedback from mobile devices to remove memory buffers.
  Together, these modifications allow OpenUVR to reduce VR application delays
to 14.32 ms, meeting the 20 ms minimum latency in avoiding motion sickness. As
an open-source system that is fully compatible with commodity hardware, OpenUVR
offers the research community an opportunity to develop, investigate, and
optimize applications for untethered, high-performance VR architectures.","['Alec Rohloff', 'Zackary Allen', 'Kung-Min Lin', 'Joshua Okrend', 'Chengyi Nie', 'Yu-Chia Liu', 'Hung-Wei Tseng']",2021-01-18T21:02:16Z,http://arxiv.org/abs/2101.07327v1,"['cs.NI', 'cs.HC', 'cs.OS']","Untethered,Virtual Reality,Open-Source,Framework,Wireless Network,Latency,Commodity Hardware,System Stack,Data Path,VR Application"
"Technological Competence is a Precondition for Effective Implementation
  of Virtual Reality Head Mounted Displays in Human Neuroscience: A
  Technological Review and Meta-analysis","Immersive virtual reality (VR) emerges as a promising research and clinical
tool. However, several studies suggest that VR induced adverse symptoms and
effects (VRISE) may undermine the health and safety standards, and the
reliability of the scientific results. In the current literature review, the
technical reasons for the adverse symptomatology are investigated to provide
suggestions and technological knowledge for the implementation of VR
head-mounted display (HMD) systems in cognitive neuroscience. The technological
systematic literature indicated features pertinent to display, sound, motion
tracking, navigation, ergonomic interactions, user experience, and computer
hardware that should be considered by the researchers. Subsequently, a
meta-analysis of 44 neuroscientific or neuropsychological studies involving VR
HMD systems was performed. The meta-analysis of the VR studies demonstrated
that new generation HMDs induced significantly less VRISE and marginally fewer
dropouts.Importantly, the commercial versions of the new generation HMDs with
ergonomic interactions had zero incidents of adverse symptomatology and
dropouts. HMDs equivalent to or greater than the commercial versions of
contemporary HMDs accompanied with ergonomic interactions are suitable for
implementation in cognitive neuroscience. In conclusion, researchers
technological competency, along with meticulous methods and reports pertinent
to software, hardware, and VRISE, are paramount to ensure the health and safety
standards and the reliability of neuroscientific results.","['Panagiotis Kourtesis', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-01-20T13:48:11Z,http://arxiv.org/abs/2101.08123v1,"['cs.HC', 'cs.CY', 'cs.MM', 'B.8; C.4; D.0; J.4']","technological competence,virtual reality,head-mounted displays,cognitive neuroscience,adverse symptoms,meta-analysis,motion tracking,user experience,computer hardware,ergonomic interactions"
"Electrotactile Feedback For Enhancing Contact Information in Virtual
  Reality","This paper presents a wearable electrotactile feedback system to enhance
contact information for mid-air interactions with virtual objects. In
particular, we propose the use of electrotactile feedback to render the
interpenetration distance between the user's finger and the virtual content is
touched. Our approach consists of modulating the perceived intensity (frequency
and pulse width modulation) of the electrotactile stimuli according to the
registered interpenetration distance. In a user study (N=21), we assessed the
performance of four different interpenetration feedback approaches:
electrotactile-only, visual-only, electrotactile and visual, and no
interpenetration feedback. First, the results showed that contact precision and
accuracy were significantly improved when using interpenetration feedback.
Second, and more interestingly, there were no significant differences between
visual and electrotactile feedback when the calibration was optimized and the
user was familiarized with electrotactile feedback. Taken together, these
results suggest that electrotactile feedback could be an efficient replacement
of visual feedback for enhancing contact information in virtual reality
avoiding the need of active visual focus and the rendering of additional visual
artefacts.","['Sebastian Vizcay', 'Panagiotis Kourtesis', 'Ferran Argelaguet', 'Claudio Pacchierotti', 'Maud Marchal']",2021-01-30T16:05:29Z,http://arxiv.org/abs/2102.00259v2,"['cs.HC', 'cs.RO', 'I.3.7; H.5.2; J.3; J.7']","Electrotactile feedback,Virtual reality,Interpenetration distance,Wearable system,User study,Contact precision,Visual feedback,Calibration,Virtual objects,Mid-air interactions."
The Plausibility Paradox for Resized Users in Virtual Environments,"This paper identifies and confirms a perceptual phenomenon: when users
interact with simulated objects in a virtual environment where the users' scale
deviates greatly from normal, there is a mismatch between the object physics
they consider realistic and the object physics that would be correct at that
scale. We report the findings of two studies investigating the relationship
between perceived realism and a physically accurate approximation of reality in
a virtual reality experience in which the user has been scaled by a factor of
ten. Study 1 investigated perception of physics when scaled-down by a factor of
ten, whereas Study 2 focused on enlargement by a similar amount. Studies were
carried out as within-subjects experiments in which a total of 84 subjects
performed simple interaction tasks with objects under two different physics
simulation conditions. In the true physics condition, the objects, when dropped
and thrown, behaved accurately according to the physics that would be correct
at that either reduced or enlarged scale in the real world. In the movie
physics condition, the objects behaved in a similar manner as they would if no
scaling of the user had occurred. We found that a significant majority of the
users considered the movie physics condition to be the more realistic one.
However, at enlarged scale, many users considered true physics to match their
expectations even if they ultimately believed movie physics to be the realistic
condition. We argue that our findings have implications for many virtual
reality and telepresence applications involving operation with simulated or
physical objects in abnormal and especially small scales.","['Matti Pouke', 'Katherine J. Mimnaugh', 'Alexis Chambers', 'Timo Ojala', 'Steven M. LaValle']",2021-02-05T13:55:31Z,http://arxiv.org/abs/2102.03179v1,"['cs.HC', 'cs.MM']","virtual environments,perceptual phenomenon,object physics,scaled users,virtual reality,physics simulation,true physics,movie physics,telepresence applications,small scales"
3D Virtual Reality vs. 2D Desktop Registration User Interface Comparison,"Working with organs and extracted tissue blocks is an essential task in
surgery and anatomy environments. To prepare specimens from human donors for
analysis, wet-bench workers must dissect human tissue and collect metadata for
downstream analysis, including information about the spatial origin of tissue.
The Registration User Interface (RUI) was developed to allow stakeholders in
the Human Biomolecular Atlas Program (HuBMAP) to register tissue blocks, i.e.,
to record the size, position, and orientation of human tissue data with regard
to reference organs. In this paper, we compare three setups for registering one
3D tissue block object to another 3D reference organ (target) object. The first
setup is a 2D Desktop implementation featuring a traditional screen, mouse, and
keyboard interface. The remaining setups are both virtual reality (VR) versions
of the RUI: VR Tabletop, where users sit at a physical desk which is replicated
in virtual space; VR Standup, where users stand upright while performing their
tasks. We then ran a user study for these three setups involving 42 human
subjects completing 14 increasingly difficult and then 30 identical tasks in
sequence and reporting position accuracy, rotation accuracy, completion time,
and satisfaction. While VR Tabletop and VR Standup users are about three times
as fast and about a third more accurate in terms of rotation than 2D Desktop
users (for the sequence of 30 identical tasks), there are no significant
differences between the three setups for position accuracy when normalized by
the height of the virtual kidney across setups.","['Andreas Bueckle', 'Kilian Buehling', 'Patrick C. Shih', 'Katy Borner']",2021-02-24T02:30:35Z,http://arxiv.org/abs/2102.12030v2,['cs.HC'],"3D,Virtual Reality,2D,Registration User Interface,Human Biomolecular Atlas Program,Tissue Blocks,Reference Organs,User Study,Rotation Accuracy"
"Q-VR: System-Level Design for Future Mobile Collaborative Virtual
  Reality","High Quality Mobile Virtual Reality (VR) is what the incoming graphics
technology era demands: users around the world, regardless of their hardware
and network conditions, can all enjoy the immersive virtual experience.
However, the state-of-the-art software-based mobile VR designs cannot fully
satisfy the realtime performance requirements due to the highly interactive
nature of user's actions and complex environmental constraints during VR
execution. Inspired by the unique human visual system effects and the strong
correlation between VR motion features and realtime hardware-level information,
we propose Q-VR, a novel dynamic collaborative rendering solution via
software-hardware co-design for enabling future low-latency high-quality mobile
VR. At software-level, Q-VR provides flexible high-level tuning interface to
reduce network latency while maintaining user perception. At hardware-level,
Q-VR accommodates a wide spectrum of hardware and network conditions across
users by effectively leveraging the computing capability of the increasingly
powerful VR hardware. Extensive evaluation on real-world games demonstrates
that Q-VR can achieve an average end-to-end performance speedup of 3.4x (up to
6.7x) over the traditional local rendering design in commercial VR devices, and
a 4.1x frame rate improvement over the state-of-the-art static collaborative
rendering.","['Chenhao Xie', 'Xie Li', 'Yang Hu', 'Huwan Peng', 'Michael Taylor', 'Shuaiwen Leon Song']",2021-02-25T21:56:05Z,http://arxiv.org/abs/2102.13191v1,"['cs.AR', 'cs.GR']","Mobile Collaborative Virtual Reality,System-Level Design,Software-Hardware Co-Design,Realtime Performance,Virtual Reality,Network Latency,Hardware-Level Information,End-to-End Performance,Frame Rate,Rendering Solution"
Reducing cybersickness in 360-degree virtual reality,"Despite the technological advancements in Virtual Reality (VR), users are
constantly combating feelings of nausea and disorientation, the so called
cybersickness. Cybersickness symptoms cause severe discomfort and hinder the
immersive VR experience. Here we investigated cybersickness in 360-degree
head-mounted display VR. In traditional 360-degree VR experiences,
translational movement in the real world is not reflected in the virtual world,
and therefore self-motion information is not corroborated by matching visual
and vestibular cues, which may trigger symptoms of cybersickness. We have
evaluated whether a new Artificial Intelligence (AI) software designed to
supplement the 360-degree VR experience with artificial 6-degrees-of-freedom
motion may reduce cybersickness. Explicit (simulator sickness questionnaire and
fast motion sickness rating) and implicit (heart rate) measurements were used
to evaluate cybersickness symptoms during and after 360-degree VR exposure.
Simulator sickness scores showed a significant reduction in feelings of nausea
during the AI supplemented 6-degrees-of-freedom motion VR compared to
traditional 360-degree VR. However, 6-degrees-of-freedom motion VR did not
reduce oculomotor or disorientation measures of sickness. No changes have been
observed in fast motion sickness and heart rate measures. Improving the
congruency between visual and vestibular cues in 360-degree VR, as provided by
the AI supplemented 6-degrees-of-freedom motion system considered, is essential
to provide a more engaging, immersive and safe VR, which is critical for
educational, cultural and entertainment applications.","['Iqra Arshad', 'Paulo De Mello', 'Martin Ender', 'Jason D. McEwen', 'Elisa R. Ferré']",2021-03-05T19:06:15Z,http://arxiv.org/abs/2103.03898v2,['cs.HC'],"Virtual Reality,Cybersickness,360-degree,Artificial Intelligence,6-degrees-of-freedom,Vestibular cues,Simulator sickness,Oculomotor,Disorientation."
"Virtual Reality Sickness Mitigation Methods: A Comparative Study in a
  Racing Game","Using virtual reality (VR) head-mounted displays (HMDs) can induce VR
sickness. VR sickness can cause strong discomfort, decrease users' presence and
enjoyment, especially in games, shorten the duration of the VR experience, and
can even pose health risks. Previous research has explored different VR
sickness mitigation methods by adding visual effects or elements. Field of View
(FOV) reduction, Depth of Field (DOF) blurring, and adding a rest frame into
the virtual environment are examples of such methods. Although useful in some
cases, they might result in information loss. This research is the first to
compare VR sickness, presence, workload to complete a search task, and
information loss of these three VR sickness mitigation methods in a racing game
with two levels of control. To do this, we conducted a mixed factorial user
study (N = 32) with degree of control as the between-subjects factor and the VR
sickness mitigation techniques as the within-subjects factor. Participants were
required to find targets with three difficulty levels while steering or not
steering a car in a virtual environment. Our results show that there are no
significant differences in VR sickness, presence and workload among these
techniques under two levels of control in our VR racing game. We also found
that changing FOV dynamically or using DOF blur effects would result in
information loss while adding a target reticule as a rest frame would not.","['Rongkai Shi', 'Hai-Ning Liang', 'Yu Wu', 'Difeng Yu', 'Wenge Xu']",2021-03-09T03:31:12Z,http://arxiv.org/abs/2103.05200v1,['cs.HC'],"Virtual Reality,Sickness,Mitigation Methods,Racing Game,Field of View,Depth of Field,VR experience,User Study,Information Loss,Presence"
"Reliving the Dataset: Combining the Visualization of Road Users'
  Interactions with Scenario Reconstruction in Virtual Reality","One core challenge in the development of automated vehicles is their
capability to deal with a multitude of complex trafficscenarios with many, hard
to predict traffic participants. As part of the iterative development process,
it is necessary to detect criticalscenarios and generate knowledge from them to
improve the highly automated driving (HAD) function. In order to tackle this
challenge,numerous datasets have been released in the past years, which act as
the basis for the development and testing of such algorithms.Nevertheless, the
remaining challenges are to find relevant scenes, such as safety-critical
corner cases, in these datasets and tounderstand them completely.Therefore,
this paper presents a methodology to process and analyze naturalistic motion
datasets in two ways: On the one hand, ourapproach maps scenes of the datasets
to a generic semantic scene graph which allows for a high-level and objective
analysis. Here,arbitrary criticality measures, e.g. TTC, RSS or SFF, can be set
to automatically detect critical scenarios between traffic participants.On the
other hand, the scenarios are recreated in a realistic virtual reality (VR)
environment, which allows for a subjective close-upanalysis from multiple,
interactive perspectives.","['Lars Töttel', 'Maximilian Zipfl', 'Daniel Bogdoll', 'Marc René Zofka', 'J. Marius Zöllner']",2021-05-04T16:39:06Z,http://arxiv.org/abs/2105.01610v3,['cs.GR'],"automated vehicles,traffic scenarios,datasets,critical scenarios,semantic scene graph,virtual reality,motion datasets,traffic participants,scenario reconstruction"
"Immersive virtual reality methods in cognitive neuroscience and
  neuropsychology: Meeting the criteria of the National Academy of
  Neuropsychology and American Academy of Clinical Neuropsychology","Clinical tools involving immersive virtual reality (VR) may bring several
advantages to cognitive neuroscience and neuropsychology. However, there are
some technical and methodological pitfalls. The American Academy of Clinical
Neuropsychology (AACN) and the National Academy of Neuropsychology (NAN) raised
8 key issues pertaining to Computerized Neuropsychological Assessment Devices.
These issues pertain to: (1) the safety and effectivity; (2) the identity of
the end-user; (3) the technical hardware and software features; (4) privacy and
data security; (5) the psychometric properties; (6) examinee issues; (7) the
use of reporting services; and (8) the reliability of the responses and
results. The VR Everyday Assessment Lab (VR-EAL) is the first immersive VR
neuropsychological battery with enhanced ecological validity for the assessment
of everyday cognitive functions by offering a pleasant testing experience
without inducing cybersickness. The VR-EAL meets the criteria of the NAN and
AACN, addresses the methodological pitfalls, and brings advantages for
neuropsychological testing. However, there are still shortcomings of the
VR-EAL, which should be addressed. Future iterations should strive to improve
the embodiment illusion in VR-EAL and the creation of an open access VR
software library should be attempted. The discussed studies demonstrate the
utility of VR methods in cognitive neuroscience and neuropsychology.","['Panagiotis Kourtesis', 'Sarah E. MacPherson']",2021-05-25T13:15:57Z,http://arxiv.org/abs/2105.11909v2,"['cs.HC', 'cs.CY', 'B.8; C.4; D.0; J.4; J.3; K.4.0']","immersive virtual reality,cognitive neuroscience,neuropsychology,National Academy of Neuropsychology,American Academy of Clinical Neuropsychology,Computerized Neuropsychological Assessment Devices,psychometric properties,examinee issues,reporting services"
"Augmenting Teleportation in Virtual Reality With Discrete Rotation
  Angles","Locomotion is one of the most essential interaction tasks in virtual reality
(VR) with teleportation being widely accepted as the state-of-the-art
locomotion technique at the time of this writing. A major draw-back of
teleportation is the accompanying physical rotation that is necessary to adjust
the users' orientation either before or after teleportation. This is a limiting
factor for tethered head-mounted displays (HMDs) and static body postures and
can induce additional simulator sickness for HMDs with three degrees-of-freedom
(DOF) due to missing parallax cues. To avoid physical rotation, previous work
proposed discrete rotation at fixed intervals (InPlace) as a controller-based
technique with low simulator sickness, yet the impact of varying intervals on
spatial disorientation, user presence and performance remains to be explored.
An unevaluated technique found in commercial VR games is reorientation during
the teleportation process (TeleTurn), which prevents physical rotation but
potentially increases interaction time due to its continuous orientation
selection. In an exploratory user study, where participants were free to apply
both techniques, we evaluated the impact of rotation parameters of either
technique on user performance and preference. Our results indicate that
discrete InPlace rotation introduced no significant spatial disorientation,
while user presence scores were increased. Discrete TeleTurn and teleportation
without rotation was ranked higher and achieved a higher presence score than
continuous TeleTurn, which is the current state-of-the-art found in VR games.
Based on observations, that participants avoided TeleTurn rotation when
discrete InPlace rotation was available, we distilled guidelines for designing
teleportation without physical rotation.","['Dennis Wolf', 'Michael Rietzler', 'Laura Bottner', 'Enrico Rukzio']",2021-06-08T11:30:26Z,http://arxiv.org/abs/2106.04257v1,"['cs.HC', 'H.5.2; H.5.1']","virtual reality,teleportation,rotation,locomotion,head-mounted displays,simulator sickness,degrees-of-freedom,spatial disorientation,user study,presence"
"Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets
  Deep Reinforcement Learning","This paper investigates the problem of providing ultra-reliable and
energy-efficient virtual reality (VR) experiences for wireless mobile users. To
ensure reliable ultra-high-definition (UHD) video frame delivery to mobile
users and enhance their immersive visual experiences, a coordinated multipoint
(CoMP) transmission technique and millimeter wave (mmWave) communications are
exploited. Owing to user movement and time-varying wireless channels, the
wireless VR experience enhancement problem is formulated as a
sequence-dependent and mixed-integer problem with a goal of maximizing users'
feeling of presence (FoP) in the virtual world, subject to power consumption
constraints on access points (APs) and users' head-mounted displays (HMDs). The
problem, however, is hard to be directly solved due to the lack of users'
accurate tracking information and the sequence-dependent and mixed-integer
characteristics. To overcome this challenge, we develop a parallel echo state
network (ESN) learning method to predict users' tracking information by
training fresh and historical tracking samples separately collected by APs.
With the learnt results, we propose a deep reinforcement learning (DRL) based
optimization algorithm to solve the formulated problem. In this algorithm, we
implement deep neural networks (DNNs) as a scalable solution to produce integer
decision variables and solving a continuous power control problem to criticize
the integer decision variables. Finally, the performance of the proposed
algorithm is compared with various benchmark algorithms, and the impact of
different design parameters is also discussed. Simulation results demonstrate
that the proposed algorithm is more 4.14% energy-efficient than the benchmark
algorithms.","['Peng Yang', 'Tony Q. S. Quek', 'Jingxuan Chen', 'Chaoqun You', 'Xianbin Cao']",2021-06-03T08:35:10Z,http://arxiv.org/abs/2107.01001v2,"['cs.NI', 'cs.AI', 'cs.HC', 'cs.LG']","Presence Maximization,mmWave,Virtual Reality,Deep Reinforcement Learning,Coordinated Multipoint Transmission,Ultra-High-Definition Video,Millimeter Wave Communications,Power Consumption Constraints,Access Points,Head-Mounted Displays"
"Multi-level Stress Assessment from ECG in a Virtual Reality Environment
  using Multimodal Fusion","ECG is an attractive option to assess stress in serious Virtual Reality (VR)
applications due to its non-invasive nature. However, the existing Machine
Learning (ML) models perform poorly. Moreover, existing studies only perform a
binary stress assessment, while to develop a more engaging biofeedback-based
application, multi-level assessment is necessary. Existing studies annotate and
classify a single experience (e.g. watching a VR video) to a single stress
level, which again prevents design of dynamic experiences where real-time
in-game stress assessment can be utilized. In this paper, we report our
findings on a new study on VR stress assessment, where three stress levels are
assessed. ECG data was collected from 9 users experiencing a VR roller coaster.
The VR experience was then manually labeled in 10-seconds segments to three
stress levels by three raters. We then propose a novel multimodal deep fusion
model utilizing spectrogram and 1D ECG that can provide a stress prediction
from just a 1-second window. Experimental results demonstrate that the proposed
model outperforms the classical HRV-based ML models (9% increase in accuracy)
and baseline deep learning models (2.5% increase in accuracy). We also report
results on the benchmark WESAD dataset to show the supremacy of the model.","['Zeeshan Ahmad', 'Suha Rabbani', 'Muhammad Rehman Zafar', 'Syem Ishaque', 'Sridhar Krishnan', 'Naimul Khan']",2021-07-09T17:34:42Z,http://arxiv.org/abs/2107.04566v1,"['cs.LG', 'cs.HC', 'eess.SP']","ECG,Virtual Reality,Multimodal Fusion,Machine Learning,Stress Assessment,Biofeedback,Spectrogram,Deep Learning,HRV-based,WESAD dataset"
Collaborative Software Modeling in Virtual Reality,"Modeling is a key activity in conceptual design and system design. Through
collaborative modeling, end-users, stakeholders, experts, and entrepreneurs are
able to create a shared understanding of a system representation. While the
Unified Modeling Language (UML) is one of the major conceptual modeling
languages in object-oriented software engineering, more and more concerns arise
from the modeling quality of UML and its tool support. Among them, the
limitation of the two-dimensional presentation of its notations and lack of
natural collaborative modeling tools are reported to be significant. In this
paper, we explore the potential of using Virtual Reality (VR) technology for
collaborative UML software design by comparing it with classical collaborative
software design using conventional devices (Desktop PC, Laptop). For this
purpose, we have developed a VR modeling environment that offers a natural
collaborative modeling experience for UML Class Diagrams. Based on a user study
with 24 participants, we have compared collaborative VR modeling with
conventional modeling with regard to efficiency, effectiveness, and user
satisfaction. Results show that the use of VR has some disadvantages concerning
efficiency and effectiveness, but the user's fun, the feeling of being in the
same room with a remote collaborator, and the naturalness of collaboration were
increased.","['Enes Yigitbas', 'Simon Gorissen', 'Nils Weidmann', 'Gregor Engels']",2021-07-27T12:34:54Z,http://arxiv.org/abs/2107.12772v1,"['cs.SE', 'cs.HC']","Collaborative software modeling,Virtual reality,Unified Modeling Language (UML),Conceptual design,System design,Object-oriented software engineering,Two-dimensional presentation,Collaborative modeling tools,User study"
"Learning-based Prediction, Rendering and Transmission for Interactive
  Virtual Reality in RIS-Assisted Terahertz Networks","The quality of experience (QoE) requirements of wireless Virtual Reality (VR)
can only be satisfied with high data rate, high reliability, and low VR
interaction latency. This high data rate over short transmission distances may
be achieved via abundant bandwidth in the terahertz (THz) band. However, THz
waves suffer from severe signal attenuation, which may be compensated by the
reconfigurable intelligent surface (RIS) technology with programmable
reflecting elements. Meanwhile, the low VR interaction latency may be achieved
with the mobile edge computing (MEC) network architecture due to its high
computation capability. Motivated by these considerations, in this paper, we
propose a MEC-enabled and RIS-assisted THz VR network in an indoor scenario, by
taking into account the uplink viewpoint prediction and position transmission,
MEC rendering, and downlink transmission. We propose two methods, which are
referred to as centralized online Gated Recurrent Unit (GRU) and distributed
Federated Averaging (FedAvg), to predict the viewpoints of VR users. In the
uplink, an algorithm that integrates online Long-short Term Memory (LSTM) and
Convolutional Neural Networks (CNN) is deployed to predict the locations and
the line-of-sight and non-line-of-sight statuses of the VR users over time. In
the downlink, we further develop a constrained deep reinforcement learning
algorithm to select the optimal phase shifts of the RIS under latency
constraints. Simulation results show that our proposed learning architecture
achieves near-optimal QoE as that of the genie-aided benchmark algorithm, and
about two times improvement in QoE compared to the random phase shift selection
scheme.","['Xiaonan Liu', 'Yansha Deng', 'Chong Han', 'Marco Di Renzo']",2021-07-27T16:59:00Z,http://arxiv.org/abs/2107.12943v1,['eess.SP'],"Prediction,Rendering,Transmission,Interactive Virtual Reality,Terahertz Networks,Reconfigurable Intelligent Surface (RIS),Mobile Edge Computing (MEC),Gated Recurrent Unit (GRU),Long-short Term Memory (LSTM),Convolutional Neural Networks (CNN)"
Exploring Head-based Mode-Switching in Virtual Reality,"Mode-switching supports multilevel operations using a limited number of input
methods. In Virtual Reality (VR) head-mounted displays (HMD), common approaches
for mode-switching use buttons, controllers, and users' hands. However, they
are inefficient and challenging to do with tasks that require both hands (e.g.,
when users need to use two hands during drawing operations). Using head
gestures for mode-switching can be an efficient and cost-effective way,
allowing for a more continuous and smooth transition between modes. In this
paper, we explore the use of head gestures for mode-switching especially in
scenarios when both users' hands are performing tasks. We present a first user
study that evaluated eight head gestures that could be suitable for VR HMD with
a dual-hand line-drawing task. Results show that move forward, move backward,
roll left, and roll right led to better performance and are preferred by
participants. A second study integrating these four gestures in Tilt Brush, an
open-source painting VR application, is conducted to further explore the
applicability of these gestures and derive insights. Results show that Tilt
Brush with head gestures allowed users to change modes with ease and led to
improved interaction and user experience. The paper ends with a discussion on
some design recommendations for using head-based mode-switching in VR HMD.","['Rongkai Shi', 'Nan Zhu', 'Hai-Ning Liang', 'Shengdong Zhao']",2021-08-12T05:09:20Z,http://arxiv.org/abs/2108.05538v1,['cs.HC'],"mode-switching,Virtual Reality,head gestures,head-mounted displays,dual-hand,user study,Tilt Brush,interaction,user experience"
"VR Sickness Prediction from Integrated HMD's Sensors using Multimodal
  Deep Fusion Network","Virtual Reality (VR) sickness commonly known as cybersickness is one of the
major problems for the comfortable use of VR systems. Researchers have proposed
different approaches for predicting cybersickness from bio-physiological data
(e.g., heart rate, breathing rate, electroencephalogram). However, collecting
bio-physiological data often requires external sensors, limiting locomotion and
3D-object manipulation during the virtual reality (VR) experience. Limited
research has been done to predict cybersickness from the data readily available
from the integrated sensors in head-mounted displays (HMDs) (e.g.,
head-tracking, eye-tracking, motion features), allowing free locomotion and
3D-object manipulation. This research proposes a novel deep fusion network to
predict cybersickness severity from heterogeneous data readily available from
the integrated HMD sensors. We extracted 1755 stereoscopic videos,
eye-tracking, and head-tracking data along with the corresponding self-reported
cybersickness severity collected from 30 participants during their VR gameplay.
We applied several deep fusion approaches with the heterogeneous data collected
from the participants. Our results suggest that cybersickness can be predicted
with an accuracy of 87.77\% and a root-mean-square error of 0.51 when using
only eye-tracking and head-tracking data. We concluded that eye-tracking and
head-tracking data are well suited for a standalone cybersickness prediction
framework.","['Rifatul Islam', 'Kevin Desai', 'John Quarles']",2021-08-14T01:28:15Z,http://arxiv.org/abs/2108.06437v1,['cs.HC'],"VR,cybersickness,HMD,sensors,multimodal,deep fusion network,head-tracking,eye-tracking,locomotion,3D-object manipulation"
"Dynamic Difficulty Adjustment in Virtual Reality Exergames through
  Experience-driven Procedural Content Generation","Virtual Reality (VR) games that feature physical activities have been shown
to increase players' motivation to do physical exercise. However, for such
exercises to have a positive healthcare effect, they have to be repeated
several times a week. To maintain player motivation over longer periods of
time, games often employ Dynamic Difficulty Adjustment (DDA) to adapt the
game's challenge according to the player's capabilities. For exercise games,
this is mostly done by tuning specific in-game parameters like the speed of
objects. In this work, we propose to use experience-driven Procedural Content
Generation for DDA in VR exercise games by procedurally generating levels that
match the player's current capabilities. Not only finetuning specific
parameters but creating completely new levels has the potential to decrease
repetition over longer time periods and allows for the simultaneous adaptation
of the cognitive and physical challenge of the exergame. As a proof-of-concept,
we implement an initial prototype in which the player must traverse a maze that
includes several exercise rooms, whereby the generation of the maze is realized
by a neural network. Passing those exercise rooms requires the player to
perform physical activities. To match the player's capabilities, we use Deep
Reinforcement Learning to adjust the structure of the maze and to decide which
exercise rooms to include in the maze. We evaluate our prototype in an
exploratory user study utilizing both biodata and subjective questionnaires.","['Tobias Huber', 'Silvan Mertes', 'Stanislava Rangelova', 'Simon Flutura', 'Elisabeth André']",2021-08-19T16:06:16Z,http://arxiv.org/abs/2108.08762v1,"['cs.HC', 'cs.AI', 'cs.LG', 'cs.MM']","Dynamic Difficulty Adjustment,Virtual Reality,Exergames,Procedural Content Generation,Physical exercise,Game challenge,In-game parameters,Procedurally generating levels,Cognitive challenge,Physical challenge,Deep Reinforcement Learning"
"Using Trajectory Compression Rate to Predict Changes in Cybersickness in
  Virtual Reality Games","Identifying cybersickness in virtual reality (VR) applications such as games
in a fast, precise, non-intrusive, and non-disruptive way remains challenging.
Several factors can cause cybersickness, and their identification will help
find its origins and prevent or minimize it. One such factor is virtual
movement. Movement, whether physical or virtual, can be represented in
different forms. One way to represent and store it is with a temporally
annotated point sequence. Because a sequence is memory-consuming, it is often
preferable to save it in a compressed form. Compression allows redundant data
to be eliminated while still preserving changes in speed and direction. Since
changes in direction and velocity in VR can be associated with cybersickness,
changes in compression rate can likely indicate changes in cybersickness
levels. In this research, we explore whether quantifying changes in virtual
movement can be used to estimate variation in cybersickness levels of VR users.
We investigate the correlation between changes in the compression rate of
movement data in two VR games with changes in players' cybersickness levels
captured during gameplay. Our results show (1) a clear correlation between
changes in compression rate and cybersickness, and(2) that a machine learning
approach can be used to identify these changes. Finally, results from a second
experiment show that our approach is feasible for cybersickness inference in
games and other VR applications that involve movement.","['Diego Monteiro', 'Hai-Ning Liang', 'Xiaohang Tang', 'Pourang Irani']",2021-08-21T16:26:04Z,http://arxiv.org/abs/2108.09538v1,['cs.HC'],"cybersickness,virtual reality,trajectory compression rate,virtual movement,compression,direction,velocity,machine learning,inference,VR games"
"A Multi-Sensor Interface to Improve the Learning Experience in Arc
  Welding Training Tasks","This paper presents the development of a multi-sensor user interface to
facilitate the instruction of arc welding tasks. Traditional methods to acquire
hand-eye coordination skills are typically conducted through one-to-one
instruction where trainees must wear protective helmets and conduct several
tests. This approach is inefficient as the harmful light emitted from the
electric arc impedes the close monitoring of the process; Practitioners can
only observe a small bright spot. To tackle these problems, recent training
approaches have leveraged virtual reality to safely simulate the process and
visualize the geometry of the workpieces. However, the synthetic nature of
these types of simulation platforms reduces their effectiveness as they fail to
comprise actual welding interactions with the environment, which hinders the
trainees' learning process. To provide users with a real welding experience, we
have developed a new multi-sensor extended reality platform for arc welding
training. Our system is composed of: (1) An HDR camera, monitoring the real
welding spot in real-time; (2) A depth sensor, capturing the 3D geometry of the
scene; and (3) A head-mounted VR display, visualizing the process safely. Our
innovative platform provides users with a ""bot trainer"", virtual cues of the
seam geometry, automatic spot tracking, and performance scores. To validate the
platform's feasibility, we conduct extensive experiments with several welding
training tasks. We show that compared with the traditional training practice
and recent virtual reality approaches, our automated multi-sensor method
achieves better performances in terms of accuracy, learning curve, and
effectiveness.","['Hoi-Yin Lee', 'Peng Zhou', 'Anqing Duan', 'Jiangliu Wang', 'Victor Wu', 'David Navarro-Alarcon']",2021-09-03T08:56:32Z,http://arxiv.org/abs/2109.01383v3,"['cs.HC', 'cs.RO']","multi-sensor interface,arc welding,hand-eye coordination,virtual reality,extended reality,HDR camera,depth sensor,head-mounted VR display"
"A Virtual Reality-based Training and Assessment System for Bridge
  Inspectors with an Assistant Drone","Over 600,000 bridges in the U.S. must be inspected every two years to
identify flaws, defects, or potential problems that may need follow-up
maintenance. Bridge inspection has adopted unmanned aerial vehicles (or drones)
for improving safety, efficiency, and cost-effectiveness. Although drones can
operate in an autonomous mode, keeping inspectors in the loop is critical for
complex tasks in bridge inspection. Therefore, inspectors need to develop the
skill and confidence to operate drones in their jobs. This paper presents the
design and development of a virtual reality-based training and assessment
system for inspectors assisted by a drone in bridge inspection. The system is
composed of four integrated modules: a simulated bridge inspection developed in
Unity, an interface that allows a trainee to operate the drone in simulation
using a remote controller, data monitoring and analysis to provide real-time,
in-task feedback to trainees to assist their learning, and a post-study
assessment supporting personalized training. The paper also conducts a
proof-of-concept pilot study to illustrate the functionality of this system.
The study demonstrated that TASBID, as a tool for the early-stage training, can
objectively identify the training needs of individuals in detail and, further,
help them develop the skill and confidence in collaborating with a drone in
bridge inspection. The system has built a modeling and analysis platform for
exploring advanced solutions to the human-drone cooperative inspection of civil
infrastructure.","['Yu Li', 'Muhammad Monjurul Karim', 'Ruwen Qin']",2021-09-06T19:29:37Z,http://arxiv.org/abs/2109.02705v3,['cs.RO'],"Virtual Reality,Training,Assessment System,Bridge Inspectors,Assistant Drone,Unmanned Aerial Vehicles,Simulation,Data Monitoring,Pilot Study,Human-Drone Cooperative Inspection"
Virtual Reality Gaming on the Cloud: A Reality Check,"Cloud virtual reality (VR) gaming traffic characteristics such as frame size,
inter-arrival time, and latency need to be carefully studied as a first step
toward scalable VR cloud service provisioning. To this end, in this paper we
analyze the behavior of VR gaming traffic and Quality of Service (QoS) when VR
rendering is conducted remotely in the cloud. We first build a VR testbed
utilizing a cloud server, a commercial VR headset, and an off-the-shelf WiFi
router. Using this testbed, we collect and process cloud VR gaming traffic data
from different games under a number of network conditions and fixed and
adaptive video encoding schemes. To analyze the application-level
characteristics such as video frame size, frame inter-arrival time, frame loss
and frame latency, we develop an interval threshold based identification method
for video frames. Based on the frame identification results, we present two
statistical models that capture the behaviour of the VR gaming video traffic.
The models can be used by researchers and practitioners to generate VR traffic
models for simulations and experiments - and are paramount in designing
advanced radio resource management (RRM) and network optimization for cloud VR
gaming services. To the best of the authors' knowledge, this is the first
measurement study and analysis conducted using a commercial cloud VR gaming
platform, and under both fixed and adaptive bitrate streaming. We make our VR
traffic data-sets publicly available for further research by the community.","['Sihao Zhao', 'Hatem Abou-zeid', 'Ramy Atawia', 'Yoga Suhas Kuruba Manjunath', 'Akram Bin Sediq', 'Xiao-Ping Zhang']",2021-09-21T11:52:40Z,http://arxiv.org/abs/2109.10114v1,"['eess.SP', 'cs.NI']","Cloud computing,Virtual reality gaming,Traffic characteristics,Quality of Service (QoS),Video encoding,Frame size,Latency,Radio resource management (RRM),Network optimization,Bitrate streaming"
"WebAssembly enables low latency interoperable augmented and virtual
  reality software","There is a clear difference in runtime performance between native
applications that use augmented/virtual reality (AR/VR) device-specific
hardware and comparable web-based implementations. Here we show that
WebAssembly (Wasm) offers a promising developer solution that can bring
near-native low latency performance to web-based applications, enabling
hardware-agnostic interoperability at scale through portable bytecode that runs
on any WiFi or cellular data network-enabled AR/VR device. Many software
application areas have begun to realize Wasm's potential as a key enabling
technology, but it has yet to establish a robust presence in the AR/VR domain.
When considering the limitations of current web-based AR/VR development
technologies such as WebXR, which provides an existing application programming
interface (API) that enables AR/VR capabilities for web-based programs, Wasm
can resolve critical issues faced with just-in-time (JIT) compilation, slow
run-times, large file sizes and big data, among other challenges. Existing
applications using Wasm-based WebXR are sparse but growing, and the potential
for porting native applications to use this emerging framework will benefit the
web-based AR/VR application space and bring it closer to its native
counterparts in terms of performance. Taken together, this kind of standardized
""write-once-deploy-everywhere"" software framework for AR/VR applications has
the potential to consolidate user experiences across different head-mounted
displays and other compatible hardware devices to ultimately create an
interoperable AR/VR ecosystem.",['Bohdan B. Khomtchouk'],2021-10-14T03:17:07Z,http://arxiv.org/abs/2110.07128v1,['cs.HC'],"WebAssembly,low latency,interoperable,augmented reality,virtual reality,bytecode,AR/VR device,JIT compilation,WebXR,performance"
"WareVR: Virtual Reality Interface for Supervision of Autonomous Robotic
  System Aimed at Warehouse Stocktaking","WareVR is a novel human-robot interface based on a virtual reality (VR)
application to interact with a heterogeneous robotic system for automated
inventory management. We have created an interface to supervise an autonomous
robot remotely from a secluded workstation in a warehouse that could benefit
during the current pandemic COVID-19 since the stocktaking is a necessary and
regular process in warehouses, which involves a group of people. The proposed
interface allows regular warehouse workers without experience in robotics to
control the heterogeneous robotic system consisting of an unmanned ground
vehicle (UGV) and unmanned aerial vehicle (UAV). WareVR provides visualization
of the robotic system in a digital twin of the warehouse, which is accompanied
by a real-time video stream from the real environment through an on-board UAV
camera. Using the WareVR interface, the operator can conduct different levels
of stocktaking, monitor the inventory process remotely, and teleoperate the
drone for a more detailed inspection. Besides, the developed interface includes
remote control of the UAV for intuitive and straightforward human interaction
with the autonomous robot for stocktaking. The effectiveness of the VR-based
interface was evaluated through the user study in a ""visual inspection""
scenario.","['Ivan Kalinov', 'Daria Trinitatova', 'Dzmitry Tsetserukou']",2021-10-21T10:57:19Z,http://arxiv.org/abs/2110.11052v1,"['cs.RO', 'cs.HC']","Virtual Reality,Interface,Supervision,Autonomous Robotic System,Warehouse,Stocktaking,Heterogeneous Robotic System,Unmanned Ground Vehicle (UGV),Unmanned Aerial Vehicle (UAV)"
"Investigating Exit Choice in Built Environment Evacuation combining
  Immersive Virtual Reality and Discrete Choice Modelling","In the event of a fire emergency in the built environment, occupants face a
range of evacuation decisions, including the choice of exits. An important
question from the standpoint of evacuation safety is how evacuees make these
choices and what factors affect their choices. Understanding how humans weigh
these (often) competing factors is essential knowledge for evacuation planning
and safe design. Here, we use immersive Virtual Reality (VR) experiments to
investigate, in controlled settings, how these trade-offs are made using
empirical data and econometric choice models. In each VR scenario, participants
are confronted with trade-offs between choosing exits that are familiar to
them, exits that are less occupied, exits that are nearer to them and exits to
which visibility is less affected by fire smoke. The marginal role of these
competing factors on their decisions is quantified in a discrete choice model.
Post-experiment questionnaires also determine factors such as their perceived
realism and emotion evoked by the VR evacuation experience. Results indicate
that none of the investigated factors dominated the others in terms of their
influence on exit choices. The participants exhibited patterns of
multi-attribute conjoint decision-making, consistent with the recent findings
in the literature. While lack of familiarity and the presence of smoke both
negatively affected the desirability of an exit to evacuees, neither solely
determined exit choice. It was also observed that prioritisation of the said
factors by participants changed during the repeated scenarios when compared to
the first scenario that they experienced. Results have implications for both
fire safety designs and future VR evacuation experiment designs. These
empirical models can also be employed as input in computer simulations of
building evacuation.","['R Lovreglio', 'E Dillies', 'E Kuligowski', 'A Rahouti', 'M Haghani']",2021-10-22T04:06:52Z,http://arxiv.org/abs/2110.11577v1,['cs.HC'],"built environment,evacuation,immersive Virtual Reality,discrete choice modelling,evacuees,trade-offs,econometric,VR scenario,exit choices,multi-attribute decision-making"
Teaching Math with the help of Virtual Reality,"In the present work we intend to introduce a system based on VR (Virtual
Reality) for examining analytical-geometric structures that occur in the study
of mathematics and physics concepts in the last high school classes. In our
opinion, an immersive study environment has several advantages over traditional
two-dimensional environments (such as a book or the simple screen of a PC or
tablet), such as the spatial understanding of the concepts exposed, more
peripheral awareness and moreover an evident decreasing in the information
dispersion phenomenon. This does not mean that our pedagogical approach is a
substitute for traditional pedagogical approaches, but is simply meant to be a
robust support. In the first phase of our research we have tried to understand
which mathematical objects and which tools to use to enhance mathematical
teaching, to demonstrate that the use of VR techniques significantly increase
the level of understanding of the mathematical subject investigated by the
students.The system which provides for the integration of two machine levels,
hardware and software, was subsequently tested by a representative sample of
students who returned various food for thought through a questionnaire.","['Marco Simonetti', 'Damiano Perri', 'Natale Amato', 'Osvaldo Gervasi']",2021-11-03T01:55:06Z,http://arxiv.org/abs/2111.01973v1,"['physics.ed-ph', 'cs.GR', 'cs.HC', 'math.HO']","Virtual Reality,Analytical-geometric structures,Mathematics,Physics concepts,Immersive study environment,Two-dimensional environments,Mathematical objects,VR techniques,Hardware,Software"
"Wireless Edge-Empowered Metaverse: A Learning-Based Incentive Mechanism
  for Virtual Reality","The Metaverse is regarded as the next-generation Internet paradigm that
allows humans to play, work, and socialize in an alternative virtual world with
immersive experience, for instance, via head-mounted display for Virtual
Reality (VR) rendering. With the help of ubiquitous wireless connections and
powerful edge computing technologies, VR users in wireless edge-empowered
Metaverse can immerse in the virtual through the access of VR services offered
by different providers. However, VR applications are computation- and
communication-intensive. The VR service providers (SPs) have to optimize the VR
service delivery efficiently and economically given their limited communication
and computation resources. An incentive mechanism can be thus applied as an
effective tool for managing VR services between providers and users. Therefore,
in this paper, we propose a learning-based Incentive Mechanism framework for VR
services in the Metaverse. First, we propose the quality of perception as the
metric for VR users immersing in the virtual world. Second, for quick trading
of VR services between VR users (i.e., buyers) and VR SPs (i.e., sellers), we
design a double Dutch auction mechanism to determine optimal pricing and
allocation rules in this market. Third, for auction communication reduction, we
design a deep reinforcement learning-based auctioneer to accelerate this
auction process. Experimental results demonstrate that the proposed framework
can achieve near-optimal social welfare while reducing at least half of the
auction information exchange cost than baseline methods.","['Minrui Xu', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Chunyan Miao', 'Dong In Kim']",2021-11-07T13:02:52Z,http://arxiv.org/abs/2111.03776v1,"['cs.GT', 'cs.CY']","Virtual Reality,Metaverse,Wireless Edge,Edge Computing,Incentive Mechanism,VR Service Providers,Quality of Perception,Dutch Auction Mechanism,Deep Reinforcement Learning"
"A Virtual Reality Simulation Pipeline for Online Mental Workload
  Modeling","Seamless human robot interaction (HRI) and cooperative human-robot (HR)
teaming critically rely upon accurate and timely human mental workload (MW)
models. Cognitive Load Theory (CLT) suggests representative physical
environments produce representative mental processes; physical environment
fidelity corresponds with improved modeling accuracy. Virtual Reality (VR)
systems provide immersive environments capable of replicating complicated
scenarios, particularly those associated with high-risk, high-stress scenarios.
Passive biosignal modeling shows promise as a noninvasive method of MW
modeling. However, VR systems rarely include multimodal psychophysiological
feedback or capitalize on biosignal data for online MW modeling. Here, we
develop a novel VR simulation pipeline, inspired by the NASA Multi-Attribute
Task Battery II (MATB-II) task architecture, capable of synchronous collection
of objective performance, subjective performance, and passive human biosignals
in a simulated hazardous exploration environment. Our system design extracts
and publishes biofeatures through the Robot Operating System (ROS),
facilitating real time psychophysiology-based MW model integration into
complete end-to-end systems. A VR simulation pipeline capable of evaluating MWs
online could be foundational for advancing HR systems and VR experiences by
enabling these systems to adaptively alter their behaviors in response to
operator MW.","['Robert L. Wilson', 'Daniel Browne', 'Jonathan Wagstaff', 'Steve McGuire']",2021-11-07T00:50:39Z,http://arxiv.org/abs/2111.03977v2,"['cs.HC', 'cs.RO']","Virtual Reality,Simulation Pipeline,Mental Workload Modeling,Cognitive Load Theory,Biosignal Modeling,Psychophysiological Feedback,Human-Robot Interaction,Human-Robot Teaming,Multi-Attribute Task Battery II,Robot Operating System (ROS)"
Virtual Reality for Synergistic Surgical Training and Data Generation,"Surgical simulators not only allow planning and training of complex
procedures, but also offer the ability to generate structured data for
algorithm development, which may be applied in image-guided computer assisted
interventions. While there have been efforts on either developing training
platforms for surgeons or data generation engines, these two features, to our
knowledge, have not been offered together. We present our developments of a
cost-effective and synergistic framework, named Asynchronous Multibody
Framework Plus (AMBF+), which generates data for downstream algorithm
development simultaneously with users practicing their surgical skills. AMBF+
offers stereoscopic display on a virtual reality (VR) device and haptic
feedback for immersive surgical simulation. It can also generate diverse data
such as object poses and segmentation maps. AMBF+ is designed with a flexible
plugin setup which allows for unobtrusive extension for simulation of different
surgical procedures. We show one use case of AMBF+ as a virtual drilling
simulator for lateral skull-base surgery, where users can actively modify the
patient anatomy using a virtual surgical drill. We further demonstrate how the
data generated can be used for validating and training downstream computer
vision algorithms","['Adnan Munawar', 'Zhaoshuo Li', 'Punit Kunjam', 'Nimesh Nagururu', 'Andy S. Ding', 'Peter Kazanzides', 'Thomas Looi', 'Francis X. Creighton', 'Russell H. Taylor', 'Mathias Unberath']",2021-11-15T21:46:21Z,http://arxiv.org/abs/2111.08097v1,['cs.RO'],"virtual reality,surgical training,data generation,algorithm development,stereoscopic display,haptic feedback,surgical simulation,segmentation maps,plugin setup,computer vision algorithms"
Taxonomy of Virtual and Augmented Reality Applications in Education,"This paper presents and analyses existing taxonomies of virtual and augmented
reality and demonstrates knowledge gaps and mixed terminology which may cause
confusion among educators, researchers, and developers. Several such occasions
of confusion are presented. A methodology is then presented to construct a
taxonomy of virtual reality and augmented reality applications based on a
combination of: a faceted analysis approach for the overall design of the
taxonomy; an existing taxonomy of educational objectives to derive the
educational purpose; an information systems analysis to establish important
facets of the taxonomy; and two systematic mapping studies to identify
categories within each facet. Based onUsing thisthe methodology a new taxonomy
is proposed and the implications of its facets (and their combinations of
facets)are demonstrated. The taxonomy focuses on technology used to provide the
virtual or augmented reality as well as the content presented to the user,
including the type of gamification and how it is operated. It also takes into
accountaccommodates a large number of devices and approaches developed
throughout the years and for multiple industries, and proposes and
developsprovides a way to categorize them in order to clarify communication
between researchers, developers and as well as educators. Use of the taxonomy
and implications of choices made during their development is then demonstrated
ion two case studies:, a virtual reality chemical plant for use in chemical
engineering education and an augmented reality dog for veterinary education.","['Jiri Motejlek', 'Esat Alpay']",2021-12-08T23:15:11Z,http://arxiv.org/abs/2112.04619v1,['cs.HC'],"virtual reality,augmented reality,taxonomy,education,technology,gamification,devices,communication,developers,researchers"
"Assessing Human Interaction in Virtual Reality With Continually Learning
  Prediction Agents Based on Reinforcement Learning Algorithms: A Pilot Study","Artificial intelligence systems increasingly involve continual learning to
enable flexibility in general situations that are not encountered during system
training. Human interaction with autonomous systems is broadly studied, but
research has hitherto under-explored interactions that occur while the system
is actively learning, and can noticeably change its behaviour in minutes. In
this pilot study, we investigate how the interaction between a human and a
continually learning prediction agent develops as the agent develops
competency. Additionally, we compare two different agent architectures to
assess how representational choices in agent design affect the human-agent
interaction. We develop a virtual reality environment and a time-based
prediction task wherein learned predictions from a reinforcement learning (RL)
algorithm augment human predictions. We assess how a participant's performance
and behaviour in this task differs across agent types, using both quantitative
and qualitative analyses. Our findings suggest that human trust of the system
may be influenced by early interactions with the agent, and that trust in turn
affects strategic behaviour, but limitations of the pilot study rule out any
conclusive statement. We identify trust as a key feature of interaction to
focus on when considering RL-based technologies, and make several
recommendations for modification to this study in preparation for a
larger-scale investigation. A video summary of this paper can be found at
https://youtu.be/oVYJdnBqTwQ .","['Dylan J. A. Brenneis', 'Adam S. Parker', 'Michael Bradley Johanson', 'Andrew Butcher', 'Elnaz Davoodi', 'Leslie Acker', 'Matthew M. Botvinick', 'Joseph Modayil', 'Adam White', 'Patrick M. Pilarski']",2021-12-14T22:46:44Z,http://arxiv.org/abs/2112.07774v2,"['cs.AI', 'cs.HC', 'cs.MA']","virtual reality,continually learning,prediction agents,reinforcement learning algorithms,human interaction,autonomous systems,agent architectures,representational choices,trust,pilot study"
Controlling camera movement in VR colonography,"Immersive Colonography allows medical professionals to navigate inside the
intricate tubular geometries of subject-specific 3D colon images using Virtual
Reality displays. Typically, camera travel is performed via Fly-Through or
Fly-Over techniques that enable semi-automatic traveling through a constrained,
well-defined path at user-controlled speeds. However, Fly-Through is known to
limit the visibility of lesions located behind or inside haustral folds. At the
same time, Fly-Over requires splitting the entire colon visualization into two
specific halves. In this paper, we study the effect of immersive Fly-Through
and Fly-Over techniques on lesion detection and introduce a camera travel
technique that maintains a fixed camera orientation throughout the entire
medial axis path. While these techniques have been studied in non-VR desktop
environments, their performance is not well understood in VR setups. We
performed a comparative study to ascertain which camera travel technique is
more appropriate for constrained path navigation in Immersive Colonography and
validated our conclusions with two radiologists. To this end, we asked 18
participants to navigate inside a 3D colon to find specific marks. Our results
suggest that the Fly-Over technique may lead to enhanced lesion detection at
the cost of higher task completion times. Nevertheless, the Fly-Through method
may offer a more balanced trade-off between speed and effectiveness, whereas
the fixed camera orientation technique provided seemingly inferior performance
results. Our study further provides design guidelines and informs future work.","['Soraia F Paulo', 'Daniel Medeiros', 'Daniel Lopes', 'Joaquim Jorge']",2022-01-08T09:07:39Z,http://arxiv.org/abs/2201.02795v1,['cs.HC'],"VR colonography,camera movement,Fly-Through,Fly-Over,lesions,haustral folds,medial axis path,constrained path navigation,immersive colonography,radiologists"
"Using a Nature-based Virtual Reality Environment for Improving Mood
  States and Cognitive Engagement in Older Adults: A Mixed-method Feasibility
  Study","Engaging with natural environments and representations of nature has been
shown to improve mood states and reduce cognitive decline in older adults. The
current study evaluated the use of virtual reality (VR) for presenting
immersive 360 degree nature videos and a digitally designed interactive garden
for this purpose. Fifty participants (age 60 plus), with varied cognitive and
physical abilities, were recruited. Data were collected through
pre/post-intervention surveys, standardized observations during the
interventions, and post-intervention semi structured interviews. The results
indicated significant improvements in attitudes toward VR and in some aspects
of mood and engagement. The responses to the environment did not significantly
differ among participants with different cognitive abilities; however, those
with physical disabilities expressed stronger positive reactions on some
metrics compared to participants without disabilities. Almost no negative
impacts (cybersickness, task frustration) were found. In the interviews some
participants expressed resistance to the technology, in particular the digital
garden, indicating that it felt cartoonish or unappealing and that it could not
substitute for real nature. However, the majority felt that the VR experiences
could be a beneficial activity in situations when real-world contact with
nature was not immediately feasible.","['Saleh Kalantari', 'Tong Bill Xu', 'Armin Mostafavi', 'Angella Lee', 'Ruth Barankevich', 'Walter Boot', 'Sara Czaja']",2022-01-09T04:15:14Z,http://arxiv.org/abs/2201.02921v1,['cs.HC'],"virtual reality,nature-based,mood states,cognitive engagement,older adults,feasibility study,immersive,360 degree,interactive garden,cybersickness"
"Effects of Virtual Room Size and Objects on Relative Translation Gain
  Thresholds in Redirected Walking","This paper investigates how the size of virtual space and objects within it
affect the threshold range of relative translation gains, a Redirected Walking
(RDW) technique that scales the user's movement in virtual space in different
ratios for the width and depth. While previous studies assert that a virtual
room's size affects relative translation gain thresholds on account of the
virtual horizon's location, additional research is needed to explore this
assumption through a structured approach to visual perception in Virtual
Reality (VR). We estimate the relative translation gain thresholds in six
spatial conditions configured by three room sizes and the presence of virtual
objects (3 X 2), which were set according to differing Angles of Declination
(AoDs) between eye-gaze and the forward-gaze. Results show that both size and
virtual objects significantly affect the threshold range, it being greater in
the large-sized condition and furnished condition. This indicates that the
effect of relative translation gains can be further increased by constructing a
perceived virtual movable space that is even larger than the adjusted virtual
movable space and placing objects in it. Our study can be applied to adjust
virtual spaces in synchronizing heterogeneous spaces without coordinate
distortion where real and virtual objects can be leveraged to create realistic
mutual spaces.","['Dooyoung Kim', 'Jinwook Kim', 'Jae-eun Shin', 'Boram Yoon', 'Jeongmi Lee', 'Woontack Woo']",2022-01-12T02:17:43Z,http://arxiv.org/abs/2201.04273v1,['cs.HC'],"Virtual Reality,Redirected Walking,Relative Translation Gain,Thresholds,Virtual Space,Virtual Objects,Angles of Declination,Visual Perception,Virtual Movable Space,Heterogeneous Spaces"
"nuReality: A VR environment for research of pedestrian and autonomous
  vehicle interactions","We present nuReality, a virtual reality 'VR' environment designed to test the
efficacy of vehicular behaviors to communicate intent during interactions
between autonomous vehicles 'AVs' and pedestrians at urban intersections. In
this project we focus on expressive behaviors as a means for pedestrians to
readily recognize the underlying intent of the AV's movements. VR is an ideal
tool to use to test these situations as it can be immersive and place subjects
into these potentially dangerous scenarios without risk. nuReality provides a
novel and immersive virtual reality environment that includes numerous visual
details (road and building texturing, parked cars, swaying tree limbs) as well
as auditory details (birds chirping, cars honking in the distance, people
talking). In these files we present the nuReality environment, its 10 unique
vehicle behavior scenarios, and the Unreal Engine and Autodesk Maya source
files for each scenario. The files are publicly released as open source at
www.nuReality.org, to support the academic community studying the critical
AV-pedestrian interaction.","['Paul Schmitt', 'Nicholas Britten', 'JiHyun Jeong', 'Amelia Coffey', 'Kevin Clark', 'Shweta Sunil Kothawade', 'Elena Corina Grigore', 'Adam Khaw', 'Christopher Konopka', 'Linh Pham', 'Kim Ryan', 'Christopher Schmitt', 'Aryaman Pandya', 'Emilio Frazzoli']",2022-01-12T23:54:09Z,http://arxiv.org/abs/2201.04742v1,['cs.RO'],"virtual reality,pedestrian,autonomous vehicles,vehicular behaviors,interactions,urban intersections,immersive,expressive behaviors"
PoVRPoint: Authoring Presentations in Mobile Virtual Reality,"Virtual Reality (VR) has the potential to support mobile knowledge workers by
complementing traditional input devices with a large three-dimensional output
space and spatial input. Previous research on supporting VR knowledge work
explored domains such as text entry using physical keyboards and spreadsheet
interaction using combined pen and touch input. Inspired by such work, this
paper probes the VR design space for authoring presentations in mobile
settings. We propose PoVRPoint -- a set of tools coupling pen- and touch-based
editing of presentations on mobile devices, such as tablets, with the
interaction capabilities afforded by VR. We study the utility of extended
display space to, for example, assist users in identifying target slides,
supporting spatial manipulation of objects on a slide, creating animations, and
facilitating arrangements of multiple, possibly occluded, shapes. Among other
things, our results indicate that 1) the wide field of view afforded by VR
results in significantly faster target slide identification times compared to a
tablet-only interface for visually salient targets; and 2) the
three-dimensional view in VR enables significantly faster object reordering in
the presence of occlusion compared to two baseline interfaces. A user study
further confirmed that the interaction techniques were found to be usable and
enjoyable.","['Verena Biener', 'Travis Gesslein', 'Daniel Schneider', 'Felix Kawala', 'Alexander Otte', 'Per Ola Kristensson', 'Michel Pahud', 'Eyal Ofek', 'Cuauhtli Campos', 'Matjaž Kljun', 'Klen ČopiāEPucihar', 'Jens Grubert']",2022-01-17T10:50:01Z,http://arxiv.org/abs/2201.06337v1,"['cs.HC', 'I.3.7']","Virtual Reality,Mobile devices,Authoring,Presentations,Pen-based editing,Touch input,Spatial manipulation,Extended display space,Three-dimensional view,User study"
"Interactivity: the missing link between virtual reality technology and
  drug discovery pipelines","The potential of virtual reality (VR) to contribute to drug design and
development has been recognised for many years. Hardware and software
developments now mean that this potential is beginning to be realised, and VR
methods are being actively used in this sphere. A recent advance is to use VR
not only to visualise and interact with molecular structures, but also to
interact with molecular dynamics simulations of 'on the fly' (interactive
molecular dynamics in VR, IMD-VR), which is useful not only for flexible
docking but also to examine binding processes and conformational changes.
iMD-VR has been shown to be useful for creating complexes of ligands bound to
target proteins, e.g., recently applied to peptide inhibitors of the SARS-CoV-2
main protease. In this review, we use the term 'interactive VR' to refer to
software where interactivity is an inherent part of the user VR experience
e.g., in making structural modifications or interacting with a physically
rigorous molecular dynamics (MD) simulation, as opposed to simply using VR
controllers to rotate and translate the molecule for enhanced visualisation.
Here, we describe these methods and their application to problems relevant to
drug discovery, highlighting the possibilities that they offer in this arena.
We suggest that the ease of viewing and manipulating molecular structures and
dynamics, and the ability to modify structures on the fly (e.g., adding or
deleting atoms) makes modern interactive VR a valuable tool to add to the
armoury of drug development methods.","['Rebecca K. Walters', 'Ella M. Gale', 'Jonathan Barnoud', 'David R. Glowacki', 'Adrian J. Mulholland']",2022-02-08T16:03:32Z,http://arxiv.org/abs/2202.03953v1,['q-bio.BM'],"interactivity,virtual reality,drug discovery,VR methods,molecular dynamics,interactive VR,IMD-VR,flexible docking,conformational changes,drug development"
"Resize Me! Exploring the User Experience of Embodied Realistic
  Modulatable Avatars for Body Image Intervention in Virtual Reality","Obesity is a serious disease that can affect both physical and psychological
well-being. Due to weight stigmatization, many affected individuals suffer from
body image disturbances whereby they perceive their body in a distorted way,
evaluate it negatively, or neglect it. Beyond established interventions such as
mirror exposure, recent advancements aim to complement body image treatments by
the embodiment of visually altered virtual bodies in virtual reality (VR). We
present a high-fidelity prototype of an advanced VR system that allows users to
embody a rapidly generated personalized, photorealistic avatar and to
realistically modulate its body weight in real-time within a carefully designed
virtual environment. In a formative multi-method approach, a total of 12
participants rated the general user experience (UX) of our system during body
scan and VR experience using semi-structured qualitative interviews and
multiple quantitative UX measures. By using body weight modification tasks, we
further compared three different interaction methods for real-time body weight
modification and measured our system's impact on the body image relevant
measures body awareness and body weight perception. From the feedback received,
demonstrating an already solid UX of our overall system and providing
constructive input for further improvement, we derived a set of design
guidelines to guide future development and evaluation processes of systems
supporting body image interventions.","['Nina Döllinger', 'Erik Wolf', 'David Mal', 'Stephan Wenninger', 'Mario Botsch', 'Marc Erich Latoschik', 'Carolin Wienrich']",2022-03-09T21:44:01Z,http://arxiv.org/abs/2203.05060v1,['cs.HC'],"body image,intervention,virtual reality,avatar,embodiment,user experience,body weight,virtual environment,body awareness,design guidelines"
"Using Virtual Reality to Design and Evaluate a Lunar Lander: The EL3
  Case Study","The European Large Logistics Lander (EL3) is being designed to carry out
cargo delivery missions in support of future lunar ground crews. The capacity
of virtual reality (VR) to visualize and interactively simulate the unique
lunar environment makes it a potentially powerful design tool during the early
development stages of such solutions. Based on input from the EL3 development
team, we have produced a VR-based operational scenario featuring a hypothetical
configuration of the lander. Relying on HCI research methods, we have
subsequently evaluated this scenario with relevant experts (n=10). Qualitative
findings from this initial pilot study have demonstrated the usefulness of VR
as a design tool in this context, but likewise surfaced a number of limitations
in the form of potentially impaired validity and generalizability. We conclude
by outlining our future research plan and reflect on the potential use of
physical stimuli to improve the validity of VR-based simulations in forthcoming
design activities.","['Tommy Nilsson', 'Flavie Rometsch', 'Andrea E. M. Casini', 'Enrico Guerra', 'Leonie Becker', 'Andreas Treuer', 'Paul de Medeiros', 'Hanjo Schnellbaecher', 'Anna Vock', 'Aidan Cowley']",2022-03-25T23:52:10Z,http://arxiv.org/abs/2203.13941v1,"['cs.HC', 'cs.MM', '93B51', 'H.1.2; I.3.8; J.4; J.m; K.8.2']","Virtual Reality,Lunar Lander,EL3,Operational Scenario,HCI,Pilot Study,Design Tool,Validity,Generalizability,Physical Stimuli"
"Virtual Reality Applications in Software Engineering Education: A
  Systematic Review","Requirement Engineering (RE) is a Software Engineering (SE) process of
defining, documenting, and maintaining the requirements from a problem. It is
one of the most complex processes of SE because it addresses the relation
between customer and developer. RE learning may be abstract and complex for
most students because many of them cannot visualize the subject directly
applied. Through the advancement of technology, Virtual Reality (VR) hardware
is becoming increasingly more accessible, and it is not rare to use it in
education. Little research and systematic studies explain the integration
between SE and VR, and even less between RE and VR. Hence, this systematic
review proposes to select and present studies that relate the use of VR
applications to teach SE and RE concepts. We selected nine studies to include
in this review. Despite the lack of articles addressing the topic, the results
from this study showed that the use of VR technologies for learning SE is still
very seminal. The projects based essentially on visualization. There are lack
of tasks to build modeling artifacts, and also interaction with stakeholders
and other software engineers. Learning tasks and the monitoring of students'
progress by teachers also need to be considered.","['Gustavo Vargas de Andrade', 'André Luiz Cordeiro Gomes', 'Felipe Rohr Hoinoski', 'Marília Guterres Ferreira', 'Pablo Schoeffel', 'Adilson Vahldick']",2022-04-26T00:30:34Z,http://arxiv.org/abs/2204.12008v1,['cs.SE'],"Requirement Engineering,Software Engineering,Virtual Reality,Systematic Review,VR applications,SE concepts,Modeling artifacts,Interaction,Stakeholders"
"Assessing visual acuity in visual prostheses through a virtual-reality
  system","Current visual implants still provide very low resolution and limited field
of view, thus limiting visual acuity in implanted patients. Developments of new
strategies of artificial vision simulation systems by harnessing new
advancements in technologies are of upmost priorities for the development of
new visual devices. In this work, we take advantage of virtual-reality software
paired with a portable head-mounted display and evaluated the performance of
normally sighted participants under simulated prosthetic vision with variable
field of view and number of pixels. Our simulated prosthetic vision system
allows simple experimentation in order to study the design parameters of future
visual prostheses. Ten normally sighted participants volunteered for a visual
acuity study. Subjects were required to identify computer-generated Landolt-C
gap orientation and different stimulus based on light perception,
time-resolution, light location and motion perception commonly used for visual
acuity examination in the sighted. Visual acuity scores were recorded across
different conditions of number of electrodes and size of field of view. Our
results showed that of all conditions tested, a field of view of 20{\deg} and
1000 phosphenes of resolution proved the best, with a visual acuity of 1.3
logMAR. Furthermore, performance appears to be correlated with phosphene
density, but showing a diminishing return when field of view is less than
20{\deg}. The development of new artificial vision simulation systems can be
useful to guide the development of new visual devices and the optimization of
field of view and resolution to provide a helpful and valuable visual aid to
profoundly or totally blind patients.","['Melani Sanchez-Garcia', 'Roberto Morollon-Ruiz', 'Ruben Martinez-Cantin', 'Jose J. Guerrero', 'Eduardo Fernandez-Jover']",2022-05-20T18:24:15Z,http://arxiv.org/abs/2205.10395v1,['cs.CV'],"visual acuity,visual prostheses,virtual-reality system,artificial vision simulation systems,technologies,field of view,number of pixels,Landolt-C gap orientation,phosphenes,logMAR"
"SaccadeNet: Towards Real-time Saccade Prediction for Virtual Reality
  Infinite Walking","Modern Redirected Walking (RDW) techniques significantly outperform classical
solutions. Nevertheless, they are often limited by their heavy reliance on
eye-tracking hardware embedded within the VR headset to reveal redirection
opportunities.
  We propose a novel RDW technique that leverages the temporary blindness
induced due to saccades for redirection. However, unlike the state-of-the-art,
our approach does not impose additional eye-tracking hardware requirements.
Instead, SaccadeNet, a deep neural network, is trained on head rotation data to
predict saccades in real-time during an apparent head rotation. Rigid
transformations are then applied to the virtual environment for redirection
during the onset duration of these saccades. However, SaccadeNet is only
effective when combined with moderate cognitive workload that elicits repeated
head rotations.
  We present three user studies. The relationship between head and gaze
directions is confirmed in the first user study, followed by the training data
collection in our second user study. Then, after some fine-tuning experiments,
the performance of our RDW technique is evaluated in a third user study.
Finally, we present the results demonstrating the efficacy of our approach. It
allowed users to walk up a straight virtual distance of at least 38 meters from
within a $3.5 x 3.5m^2$ of the physical tracked space. Moreover, our system
unlocks saccadic redirection on widely used consumer-grade hardware without
eye-tracking.","['Yashas Joshi', 'Charalambos Poullis']",2022-05-31T14:50:24Z,http://arxiv.org/abs/2205.15846v1,"['cs.GR', 'cs.MM']","SaccadeNet,Real-time,Prediction,Virtual Reality,Redirected Walking,Eye-tracking,Neural network,Head rotation,Cognitive workload"
"Multi-party Holomeetings: Toward a New Era of Low-Cost Volumetric
  Holographic Meetings in Virtual Reality","Fueled by advances in multi-party communications, increasingly mature
immersive technologies being adopted, and the COVID-19 pandemic, a new wave of
social virtual reality (VR) platforms have emerged to support socialization,
interaction, and collaboration among multiple remote users who are integrated
into shared virtual environments. Social VR aims to increase levels of
(co-)presence and interaction quality by overcoming the limitations of 2D
windowed representations in traditional multi-party video conferencing tools,
although most existing solutions rely on 3D avatars to represent users. This
article presents a social VR platform that supports real-time volumetric
holographic representations of users that are based on point clouds captured by
off-the-shelf RGB-D sensors, and it analyzes the platform's potential for
conducting interactive holomeetings (i.e., holoconferencing scenarios). This
work evaluates such a platform's performance and readiness for conducting
meetings with up to four users, and it provides insights into aspects of the
user experience when using single-camera and low-cost capture systems in
scenarios with both frontal and side viewpoints. Overall, the obtained results
confirm the platform's maturity and the potential of holographic communications
for conducting interactive multi-party meetings, even when using low-cost
systems and single-camera capture systems in scenarios where users are sitting
or have a limited translational movement along the X, Y, and Z axes within the
3D virtual environment (commonly known as 3 Degrees of Freedom plus, 3DoF+)","['Sergi Fernández', 'Mario Montagud', 'Gianluca Cernigliaro', 'David Rincón']",2022-06-11T05:33:03Z,http://arxiv.org/abs/2206.05426v1,"['cs.MM', 'cs.HC']","multi-party communications,immersive technologies,virtual reality,social virtual reality,3D avatars,holographic representations,point clouds,RGB-D sensors,holomeetings,3DoF+"
Perceptual Quality Assessment of Virtual Reality Videos in the Wild,"Investigating how people perceive virtual reality (VR) videos in the wild
(i.e., those captured by everyday users) is a crucial and challenging task in
VR-related applications due to complex authentic distortions localized in space
and time. Existing panoramic video databases only consider synthetic
distortions, assume fixed viewing conditions, and are limited in size. To
overcome these shortcomings, we construct the VR Video Quality in the Wild
(VRVQW) database, containing $502$ user-generated videos with diverse content
and distortion characteristics. Based on VRVQW, we conduct a formal
psychophysical experiment to record the scanpaths and perceived quality scores
from $139$ participants under two different viewing conditions. We provide a
thorough statistical analysis of the recorded data, observing significant
impact of viewing conditions on both human scanpaths and perceived quality.
Moreover, we develop an objective quality assessment model for VR videos based
on pseudocylindrical representation and convolution. Results on the proposed
VRVQW show that our method is superior to existing video quality assessment
models. We have made the database and code available at
https://github.com/limuhit/VR-Video-Quality-in-the-Wild.","['Wen Wen', 'Mu Li', 'Yiru Yao', 'Xiangjie Sui', 'Yabin Zhang', 'Long Lan', 'Yuming Fang', 'Kede Ma']",2022-06-13T02:22:57Z,http://arxiv.org/abs/2206.08751v3,"['cs.CV', 'eess.IV']","Virtual reality,VR videos,Perceptual quality assessment,User-generated videos,Panoramic video databases,Psychophysical experiment,Scanpaths,Objective quality assessment,Pseudocylindrical representation,Convolution."
"Latents2Segments: Disentangling the Latent Space of Generative Models
  for Semantic Segmentation of Face Images","With the advent of an increasing number of Augmented and Virtual Reality
applications that aim to perform meaningful and controlled style edits on
images of human faces, the impetus for the task of parsing face images to
produce accurate and fine-grained semantic segmentation maps is more than ever
before. Few State of the Art (SOTA) methods which solve this problem, do so by
incorporating priors with respect to facial structure or other face attributes
such as expression and pose in their deep classifier architecture. Our
endeavour in this work is to do away with the priors and complex pre-processing
operations required by SOTA multi-class face segmentation models by reframing
this operation as a downstream task post infusion of disentanglement with
respect to facial semantic regions of interest (ROIs) in the latent space of a
Generative Autoencoder model. We present results for our model's performance on
the CelebAMask-HQ and HELEN datasets. The encoded latent space of our model
achieves significantly higher disentanglement with respect to semantic ROIs
than that of other SOTA works. Moreover, it achieves a 13% faster inference
rate and comparable accuracy with respect to the publicly available SOTA for
the downstream task of semantic segmentation of face images.","['Snehal Singh Tomar', 'A. N. Rajagopalan']",2022-07-05T08:09:15Z,http://arxiv.org/abs/2207.01871v2,"['cs.CV', '68T45', 'I.2; I.4']","latent space,generative models,semantic segmentation,face images,augmented reality,virtual reality,deep learning,disentanglement,CelebAMask-HQ,HELEN"
"Virtual reality (VR) as a testing bench for consumer optical solutions:
  A machine learning approach (GBR) to visual comfort under simulated
  progressive addition lenses (PALS) distortions","For decades, manufacturers have attempted to reduce or eliminate the optical
aberrations that appear on the progressive addition lens' surfaces during
manufacturing. Besides every effort made, some of these distortions are
inevitable given how lenses are fabricated, where in fact, astigmatism appears
on the surface and cannot be entirely removed or where non-uniform
magnification becomes inherent to the power change across the lens. Some
presbyopes may refer to certain discomfort when wearing these lenses for the
first time, and a subset of them might never adapt. Developing, prototyping,
testing and purveying those lenses into the market come at a cost, which is
usually reflected in the retail price. This study aims to test the feasibility
of virtual reality for testing customers' satisfaction with these lenses, even
before getting them onto production. VR offers a controlled environment where
different parameters affecting progressive lens comforts, such as distortions,
image displacement or optical blurring, can be analysed separately. In this
study, the focus was set on the distortions and image displacement, not taking
blur into account. Behavioural changes (head and eye movements) were recorded
using the built-in eye tracker. Participants were significantly more displeased
in the presence of highly distorted lens simulations. In addition, a gradient
boosting regressor was fitted to the data, so predictors of discomfort could be
unveiled, and ratings could be predicted without performing additional
measurements.","['Miguel García García', 'Yannick Sauer', 'Tamara Watson', 'Siegfried Wahl']",2022-07-14T09:26:39Z,http://arxiv.org/abs/2207.06769v1,['cs.HC'],"Virtual reality,Machine learning,Visual comfort,Progressive addition lenses,Distortions,Astigmatism,Presbyopes,Eye tracker,Gradient boosting regressor,Optical solutions"
"Short-Term Trajectory Prediction for Full-Immersive Multiuser Virtual
  Reality with Redirected Walking","Full-immersive multiuser Virtual Reality (VR) envisions supporting
unconstrained mobility of the users in the virtual worlds, while at the same
time constraining their physical movements inside VR setups through redirected
walking. For enabling delivery of high data rate video content in real-time,
the supporting wireless networks will leverage highly directional communication
links that will ""track"" the users for maintaining the Line-of-Sight (LoS)
connectivity. Recurrent Neural Networks (RNNs) and in particular Long
Short-Term Memory (LSTM) networks have historically presented themselves as a
suitable candidate for near-term movement trajectory prediction for natural
human mobility, and have also recently been shown as applicable in predicting
VR users' mobility under the constraints of redirected walking. In this work,
we extend these initial findings by showing that Gated Recurrent Unit (GRU)
networks, another candidate from the RNN family, generally outperform the
traditionally utilized LSTMs. Second, we show that context from a virtual world
can enhance the accuracy of the prediction if used as an additional input
feature in comparison to the more traditional utilization of solely the
historical physical movements of the VR users. Finally, we show that the
prediction system trained on a static number of coexisting VR users be scaled
to a multi-user system without significant accuracy degradation.","['Filip Lemic', 'Jakob Struye', 'Jeroen Famaey']",2022-07-15T15:09:07Z,http://arxiv.org/abs/2207.07520v1,"['cs.NI', 'cs.LG', 'cs.MM']","Full-immersive,Multiuser,Virtual Reality,Redirected Walking,Recurrent Neural Networks,Long Short-Term Memory,Gated Recurrent Unit,Prediction,Mobility,Wireless Networks"
Virtual Reality Simulator for Fetoscopic Spina Bifida Repair Surgery,"Spina Bifida (SB) is a birth defect developed during the early stage of
pregnancy in which there is incomplete closing of the spine around the spinal
cord. The growing interest in fetoscopic Spina-Bifida repair, which is
performed in fetuses who are still in the pregnant uterus, prompts the need for
appropriate training. The learning curve for such procedures is steep and
requires excellent procedural skills. Computer-based virtual reality (VR)
simulation systems offer a safe, cost-effective, and configurable training
environment free from ethical and patient safety issues. However, to the best
of our knowledge, there are currently no commercial or experimental VR training
simulation systems available for fetoscopic SB-repair procedures. In this
paper, we propose a novel VR simulator for core manual skills training for
SB-repair. An initial simulation realism validation study was carried out by
obtaining subjective feedback (face and content validity) from 14 clinicians.
The overall simulation realism was on average marked 4.07 on a 5-point Likert
scale (1 - very unrealistic, 5 - very realistic). Its usefulness as a training
tool for SB-repair as well as in learning fundamental laparoscopic skills was
marked 4.63 and 4.80, respectively. These results indicate that VR simulation
of fetoscopic procedures may contribute to surgical training without putting
fetuses and their mothers at risk. It could also facilitate wider adaptation of
fetoscopic procedures in place of much more invasive open fetal surgeries.","['Przemysław Korzeniowski', 'Szymon Płotka', 'Robert Brawura-Biskupski-Samaha', 'Arkadiusz Sitek']",2022-07-30T08:51:11Z,http://arxiv.org/abs/2208.00169v1,"['cs.CV', 'cs.MM', 'cs.RO']","Virtual reality,Simulator,Fetoscopic surgery,Spina bifida,Training,Simulation systems,Laparoscopic skills,Surgical training,Simulation realism,Fetal surgeries"
"The Relative Importance of Depth Cues and Semantic Edges for Indoor
  Mobility Using Simulated Prosthetic Vision in Immersive Virtual Reality","Visual neuroprostheses (bionic eyes) have the potential to treat degenerative
eye diseases that often result in low vision or complete blindness. These
devices rely on an external camera to capture the visual scene, which is then
translated frame-by-frame into an electrical stimulation pattern that is sent
to the implant in the eye. To highlight more meaningful information in the
scene, recent studies have tested the effectiveness of deep-learning based
computer vision techniques, such as depth estimation to highlight nearby
obstacles (DepthOnly mode) and semantic edge detection to outline important
objects in the scene (EdgesOnly mode). However, nobody has attempted to combine
the two, either by presenting them together (EdgesAndDepth) or by giving the
user the ability to flexibly switch between them (EdgesOrDepth). Here, we used
a neurobiologically inspired model of simulated prosthetic vision (SPV) in an
immersive virtual reality (VR) environment to test the relative importance of
semantic edges and relative depth cues to support the ability to avoid
obstacles and identify objects. We found that participants were significantly
better at avoiding obstacles using depth-based cues as opposed to relying on
edge information alone, and that roughly half the participants preferred the
flexibility to switch between modes (EdgesOrDepth). This study highlights the
relative importance of depth cues for SPV mobility and is an important first
step towards a visual neuroprosthesis that uses computer vision to improve a
user's scene understanding.","['Alex Rasla', 'Michael Beyeler']",2022-08-09T22:47:51Z,http://arxiv.org/abs/2208.05066v2,['cs.HC'],"depth cues,semantic edges,prosthetic vision,immersive virtual reality,neuroprostheses,computer vision techniques,deep-learning,obstacles,objects,scene understanding"
"Virtual Reality Platform to Develop and Test Applications on Human-Robot
  Social Interaction","Robotics simulation has been an integral part of research and development in
the robotics area. The simulation eliminates the possibility of harm to
sensors, motors, and the physical structure of a real robot by enabling
robotics application testing to be carried out quickly and affordably without
being subjected to mechanical or electronic errors. Simulation through virtual
reality (VR) offers a more immersive experience by providing better visual cues
of environments, making it an appealing alternative for interacting with
simulated robots. This immersion is crucial, particularly when discussing
sociable robots, a subarea of the human-robot interaction (HRI) field. The
widespread use of robots in daily life depends on HRI. In the future, robots
will be able to interact effectively with people to perform a variety of tasks
in human civilization. It is crucial to develop simple and understandable
interfaces for robots as they begin to proliferate in the personal workspace.
Due to this, in this study, we implement a VR robotic framework with
ready-to-use tools and packages to enhance research and application development
in social HRI. Since the entire VR interface is an open-source project, the
tests can be conducted in an immersive environment without needing a physical
robot.","['Jair A. Bottega', 'Raul Steinmetz', 'Alisson H. Kolling', 'Victor A. Kich', 'Junior C. de Jesus', 'Ricardo B. Grando', 'Daniel F. T. Gamarra']",2022-08-13T19:03:19Z,http://arxiv.org/abs/2208.06711v1,['cs.RO'],"virtual reality,platform,human-robot social interaction,robotics simulation,sensors,motors,immersive experience,sociable robots,human-robot interaction,VR robotic framework"
Deep Billboards towards Lossless Real2Sim in Virtual Reality,"An aspirational goal for virtual reality (VR) is to bring in a rich diversity
of real world objects losslessly. Existing VR applications often convert
objects into explicit 3D models with meshes or point clouds, which allow fast
interactive rendering but also severely limit its quality and the types of
supported objects, fundamentally upper-bounding the ""realism"" of VR. Inspired
by the classic ""billboards"" technique in gaming, we develop Deep Billboards
that model 3D objects implicitly using neural networks, where only 2D image is
rendered at a time based on the user's viewing direction. Our system,
connecting a commercial VR headset with a server running neural rendering,
allows real-time high-resolution simulation of detailed rigid objects, hairy
objects, actuated dynamic objects and more in an interactive VR world,
drastically narrowing the existing real-to-simulation (real2sim) gap.
Additionally, we augment Deep Billboards with physical interaction capability,
adapting classic billboards from screen-based games to immersive VR. At our
pavilion, the visitors can use our off-the-shelf setup for quickly capturing
their favorite objects, and within minutes, experience them in an immersive and
interactive VR world with minimal loss of reality. Our project page:
https://sites.google.com/view/deepbillboards/","['Naruya Kondo', 'So Kuroki', 'Ryosuke Hyakuta', 'Yutaka Matsuo', 'Shixiang Shane Gu', 'Yoichi Ochiai']",2022-08-08T16:16:29Z,http://arxiv.org/abs/2208.08861v1,"['cs.CV', 'cs.GR']","virtual reality,real world objects,3D models,meshes,point clouds,neural networks,neural rendering,physical interaction,immersive VR,real-to-simulation gap"
Automatic Calibration of a Six-Degrees-of-Freedom Pose Estimation System,"Systems for estimating the six-degrees-of-freedom human body pose have been
improving for over two decades. Technologies such as motion capture cameras,
advanced gaming peripherals and more recently both deep learning techniques and
virtual reality systems have shown impressive results. However, most systems
that provide high accuracy and high precision are expensive and not easy to
operate. Recently, research has been carried out to estimate the human body
pose using the HTC Vive virtual reality system. This system shows accurate
results while keeping the cost under a 1000 USD. This system uses an optical
approach. Two transmitter devices emit infrared pulses and laser planes are
tracked by use of photo diodes on receiver hardware. A system using these
transmitter devices combined with low-cost custom-made receiver hardware was
developed previously but requires manual measurement of the position and
orientation of the transmitter devices. These manual measurements can be time
consuming, prone to error and not possible in particular setups. We propose an
algorithm to automatically calibrate the poses of the transmitter devices in
any chosen environment with custom receiver/calibration hardware. Results show
that the calibration works in a variety of setups while being more accurate
than what manual measurements would allow. Furthermore, the calibration
movement and speed has no noticeable influence on the precision of the results.","['Wouter Jansen', 'Dennis Laurijssen', 'Walter Daems', 'Jan Steckel']",2022-08-23T09:40:23Z,http://arxiv.org/abs/2208.10837v1,"['cs.RO', 'eess.SP']","Six-degrees-of-freedom,Pose estimation,Calibration,Human body pose,Virtual reality,Motion capture,Deep learning,Transmitter devices,Receiver hardware,Optical approach"
"""It's Just Part of Me:"" Understanding Avatar Diversity and
  Self-presentation of People with Disabilities in Social Virtual Reality","In social Virtual Reality (VR), users are embodied in avatars and interact
with other users in a face-to-face manner using avatars as the medium. With the
advent of social VR, people with disabilities (PWD) have shown an increasing
presence on this new social media. With their unique disability identity, it is
not clear how PWD perceive their avatars and whether and how they prefer to
disclose their disability when presenting themselves in social VR. We fill this
gap by exploring PWD's avatar perception and disability disclosure preferences
in social VR. Our study involved two steps. We first conducted a systematic
review of fifteen popular social VR applications to evaluate their avatar
diversity and accessibility support. We then conducted an in-depth interview
study with 19 participants who had different disabilities to understand their
avatar experiences. Our research revealed a number of disability disclosure
preferences and strategies adopted by PWD (e.g., reflect selective
disabilities, present a capable self). We also identified several challenges
faced by PWD during their avatar customization process. We discuss the design
implications to promote avatar accessibility and diversity for future social VR
platforms.","['Kexin Zhang', 'Elmira Deldari', 'Zhicong Lu', 'Yaxing Yao', 'Yuhang Zhao']",2022-08-23T19:56:26Z,http://arxiv.org/abs/2208.11170v1,['cs.HC'],"Avatar diversity,Self-presentation,People with disabilities,Social Virtual Reality,Disability identity,Avatar perception,Disability disclosure preferences,Accessibility support,Avatar customization,Social VR platforms"
"Collaborative Remote Control of Unmanned Ground Vehicles in Virtual
  Reality","Virtual reality (VR) technology is commonly used in entertainment
applications; however, it has also been deployed in practical applications in
more serious aspects of our lives, such as safety. To support people working in
dangerous industries, VR can ensure operators manipulate standardized tasks and
work collaboratively to deal with potential risks. Surprisingly, little
research has focused on how people can collaboratively work in VR environments.
Few studies have paid attention to the cognitive load of operators in their
collaborative tasks. Once task demands become complex, many researchers focus
on optimizing the design of the interaction interfaces to reduce the cognitive
load on the operator. That approach could be of merit; however, it can actually
subject operators to a more significant cognitive load and potentially more
errors and a failure of collaboration. In this paper, we propose a new
collaborative VR system to support two teleoperators working in the VR
environment to remote control an uncrewed ground vehicle. We use a compared
experiment to evaluate the collaborative VR systems, focusing on the time spent
on tasks and the total number of operations. Our results show that the total
number of processes and the cognitive load during operations were significantly
lower in the two-person group than in the single-person group. Our study sheds
light on designing VR systems to support collaborative work with respect to the
flow of work of teleoperators instead of simply optimizing the design outcomes.","['Ziming Li', 'Yiming Luo', 'Jialin Wang', 'Yushan Pan', 'Lingyun Yu', 'Hai-Ning Liang']",2022-08-24T04:18:08Z,http://arxiv.org/abs/2208.11294v1,"['cs.HC', 'cs.RO']","- Collaborative remote control
- Unmanned ground vehicles
- Virtual reality (VR)
- Cognitive load
- Interaction interfaces
- Teleoperators
- Collaborative tasks
- VR systems
- Remote control
- Collaborative work"
"Evaluation of Text Selection Techniques in Virtual Reality Head-Mounted
  Displays","Text selection is an essential activity in interactive systems, including
virtual reality (VR) head-mounted displays (HMDs). It is useful for: sharing
information across apps or platforms, highlighting and making notes while
reading articles, and text editing tasks. Despite its usefulness, the space of
text selection interaction is underexplored in VR HMDs. In this research, we
performed a user study with 24 participants to investigate the performance and
user preference of six text selection techniques (Controller+Dwell,
Controller+Click, Head+Dwell, Head+Click, Hand+Dwell, Hand+Pinch). Results
reveal that Head+Click is ranked first since it has excellent speed-accuracy
performance (2nd fastest task completion speed with 3rd lowest total error
rate), provides the best user experience, and produces a very low workload --
followed by Controller+Click, which has the fastest speed and comparable
experience with Head+Click, but much higher total error rate. Other methods can
also be useful depending on the goals of the system or the users. As a first
systematic evaluation of pointing*selection techniques for text selection in
VR, the results of this work provide a strong foundation for further research
in this area of growing importance to the future of VR to help it become a more
ubiquitous and pervasive platform.","['Wenge Xu', 'Xuanru Meng', 'Kangyou Yu', 'Sayan Sacar', 'Hai-Ning Liang']",2022-09-14T08:52:36Z,http://arxiv.org/abs/2209.06498v2,"['cs.HC', 'cs.MM', 'H.5.1; I.3.7']","Text selection,Virtual reality,Head-mounted displays,User study,Text selection techniques,Controller,Dwell,Click,Hand,Pinch"
"An Exploration of Hands-free Text Selection for Virtual Reality
  Head-Mounted Displays","Hand-based interaction, such as using a handheld controller or making hand
gestures, has been widely adopted as the primary method for interacting with
both virtual reality (VR) and augmented reality (AR) head-mounted displays
(HMDs). In contrast, hands-free interaction avoids the need for users' hands
and although it can afford additional benefits, there has been limited research
in exploring and evaluating hands-free techniques for these HMDs. As VR HMDs
become ubiquitous, people will need to do text editing, which requires
selecting text segments. Similar to hands-free interaction, text selection is
underexplored. This research focuses on both, text selection via hands-free
interaction. Our exploration involves a user study with 24 participants to
investigate the performance, user experience, and workload of three hands-free
selection mechanisms (Dwell, Blink, Voice) to complement head-based pointing.
Results indicate that Blink outperforms Dwell and Voice in completion time.
Users' subjective feedback also shows that Blink is the preferred technique for
text selection. This work is the first to explore hands-free interaction for
text selection in VR HMDs. Our results provide a solid platform for further
research in this important area.","['Xuanru Meng', 'Wenge Xu', 'Hai-Ning Liang']",2022-09-14T09:25:54Z,http://arxiv.org/abs/2209.06825v2,"['cs.HC', 'H.5.1; I.3.7']","Hands-free interaction,Text selection,Virtual reality,Augmented reality,Head-mounted displays,User study,Dwell,Blink,Voice"
"HyperGuider: Virtual Reality Framework for Interactive Path Planning of
  Quadruped Robot in Cluttered and Multi-Terrain Environments","Quadruped platforms have become an active topic of research due to their high
mobility and traversability in rough terrain. However, it is highly challenging
to determine whether the clattered environment could be passed by the robot and
how exactly its path should be calculated. Moreover, the calculated path may
pass through areas with dynamic objects or environments that are dangerous for
the robot or people around. Therefore, we propose a novel conceptual approach
of teaching quadruped robots navigation through user-guided path planning in
virtual reality (VR). Our system contains both global and local path planners,
allowing robot to generate path through iterations of learning. The VR
interface allows user to interact with environment and to assist quadruped
robot in challenging scenarios. The results of comparison experiments show that
cooperation between human and path planning algorithms can increase the
computational speed of the algorithm by 35.58% in average, and non-critically
increasing of the path length (average of 6.66%) in test scenario.
Additionally, users described VR interface as not requiring physical demand
(2.3 out of 10) and highly evaluated their performance (7.1 out of 10). The
ability to find a less optimal but safer path remains in demand for the task of
navigating in a cluttered and unstructured environment.","['Ildar Babataev', 'Aleksey Fedoseev', 'Nipun Weerakkodi', 'Elena Nazarova', 'Dzmitry Tsetserukou']",2022-09-20T18:29:08Z,http://arxiv.org/abs/2209.09940v1,"['cs.RO', 'cs.HC']","Virtual Reality,Path Planning,Quadruped Robot,Cluttered Environment,Multi-Terrain,Global Path Planner,Local Path Planner,Human-Robot Cooperation,Computational Speed,Path Length"
Real-Time Locomotion on Soft Grounds With Dynamic Footprints,"When we move on snow, sand, or mud, the ground deforms under our feet,
immediately affecting our gait. We propose a physically based model for
computing such interactions in real time, from only the kinematic motion of a
virtual character. The force applied by each foot on the ground during contact
is estimated from the weight of the character, its current balance, the foot
speed at the time of contact, and the nature of the ground. We rely on a
standard stress-strain relationship to compute the dynamic deformation of the
soil under this force, where the amount of compression and lateral displacement
of material are, respectively, parameterized by the soil's Young modulus and
Poisson ratio. The resulting footprint is efficiently applied to the terrain
through procedural deformations of refined terrain patches, while the addition
of a simple controller on top of a kinematic character enables capturing the
effect of ground deformation on the character's gait. As our results show, the
resulting footprints greatly improve visual realism, while ground compression
results in consistent changes in the character's motion. Readily applicable to
any locomotion gait and soft soil material, our real-time model is ideal for
enhancing the visual realism of outdoor scenes in video games and virtual
reality applications.","['Eduardo Alvarado', 'Chloé Paliard', 'Damien Rohmer', 'Marie-Paule Cani']",2022-09-21T09:24:05Z,http://arxiv.org/abs/2209.10215v1,['cs.GR'],"Real-time,Locomotion,Soft Grounds,Dynamic Footprints,Kinematic Motion,Virtual Character,Ground Deformation,Soil,Young Modulus,Poisson Ratio"
"Improved Perception of AEC Construction Details via Immersive Teaching
  in Virtual Reality","This work proposes, implements and tests an immersive framework upon Virtual
Reality (VR) for comprehension, knowledge development and learning process
assisting an improved perception of complex spatial arrangements in AEC in
comparison to the traditional 2D projection drawing-based method. The research
focuses on the prototypical example of construction details as a traditionally
difficult teaching task for conveying geometric and semantic information to
students. Our mixed-methods study analyses test results of two test panel
groups upon different questions about geometric and functional aspects of the
construction detail as well as surveys and interviews of participating
lecturers, students and laypersons towards their experience using the VR tool.
The quantitative analysis of the test results prove that for participants with
little pre-existing knowledge (such as novice students), a significantly better
learning score for the test group is detected. Moreover, both groups rated the
VR experience as an enjoyable and engaging way of learning. Analysis of survey
results towards the VR experience reveals, that students, lecturers and
professionals alike enjoyed the VR experience more than traditional learning of
the construction detail. During the post-experiment qualitative evaluation in
the form of interviews, the panel expressed an improved understanding,
increased enthusiasm for the topic, and greater desire for other topics to be
presented using VR tools. The expressed better understanding of design concepts
after the VR experience by the students is statistically significant on average
in the exam results. The results support our core assumption, that the
presentation of contextual 3D models is a promising teaching approach to
illustrate content.","['Michael Kraus', 'Romana Rust', 'Maximilian Rietschel', 'Daniel Hall']",2022-09-21T19:22:25Z,http://arxiv.org/abs/2209.10617v1,['cs.HC'],"immersive framework,Virtual Reality (VR),AEC,spatial arrangements,construction details,geometric information,semantic information,mixed-methods study,VR tool,qualitative evaluation"
"Facilitating Self-monitored Physical Rehabilitation with Virtual Reality
  and Haptic feedback","Physical rehabilitation is essential to recovery from joint replacement
operations. As a representation, total knee arthroplasty (TKA) requires
patients to conduct intensive physical exercises to regain the knee's range of
motion and muscle strength. However, current joint replacement physical
rehabilitation methods rely highly on therapists for supervision, and existing
computer-assisted systems lack consideration for enabling self-monitoring,
making at-home physical rehabilitation difficult. In this paper, we
investigated design recommendations that would enable self-monitored
rehabilitation through clinical observations and focus group interviews with
doctors and therapists. With this knowledge, we further explored Virtual
Reality(VR)-based visual presentation and supplemental haptic motion guidance
features in our implementation VReHab, a self-monitored and multimodal physical
rehabilitation system with VR and vibrotactile and pneumatic feedback in a TKA
rehabilitation context. We found that the third point of view real-time
reconstructed motion on a virtual avatar overlaid with the target pose
effectively provides motion awareness and guidance while haptic feedback helps
enhance users' motion accuracy and stability. Finally, we implemented
\systemname to facilitate self-monitored post-operative exercises and validated
its effectiveness through a clinical study with 10 patients.","['Yu Jiang', 'Zhipeng Li', 'Ziyue Dang', 'Yuntao Wang', 'Yukang Yan', 'Y Zhang', 'Xinguang Wang', 'Yansong Li', 'Mouwang Zhou', 'Hua Tian', 'Yuanchun Shi']",2022-09-24T14:37:14Z,http://arxiv.org/abs/2209.12018v2,['cs.HC'],"physical rehabilitation,virtual reality,haptic feedback,joint replacement,total knee arthroplasty,self-monitoring,motion guidance,virtual avatar,clinical study"
"Virtual-Reality based Vestibular Ocular Motor Screening for Concussion
  Detection using Machine-Learning","Sport-related concussion (SRC) depends on sensory information from visual,
vestibular, and somatosensory systems. At the same time, the current clinical
administration of Vestibular/Ocular Motor Screening (VOMS) is subjective and
deviates among administrators. Therefore, for the assessment and management of
concussion detection, standardization is required to lower the risk of injury
and increase the validation among clinicians. With the advancement of
technology, virtual reality (VR) can be utilized to advance the standardization
of the VOMS, increasing the accuracy of testing administration and decreasing
overall false positive rates. In this paper, we experimented with multiple
machine learning methods to detect SRC on VR-generated data using VOMS. In our
observation, the data generated from VR for smooth pursuit (SP) and the Visual
Motion Sensitivity (VMS) tests are highly reliable for concussion detection.
Furthermore, we train and evaluate these models, both qualitatively and
quantitatively. Our findings show these models can reach high
true-positive-rates of around 99.9 percent of symptom provocation on the VR
stimuli-based VOMS vs. current clinical manual VOMS.","['Khondker Fariha Hossain', 'Sharif Amit Kamran', 'Prithul Sarker', 'Philip Pavilionis', 'Isayas Adhanom', 'Nicholas Murray', 'Alireza Tavakkoli']",2022-10-13T02:09:21Z,http://arxiv.org/abs/2210.09295v1,"['eess.IV', 'cs.CV']","virtual reality,vestibular ocular motor screening,concussion detection,machine learning"
"WiserVR: Semantic Communication Enabled Wireless Virtual Reality
  Delivery","Virtual reality (VR) over wireless is expected to be one of the killer
applications in next-generation communication networks. Nevertheless, the huge
data volume along with stringent requirements on latency and reliability under
limited bandwidth resources makes untethered wireless VR delivery increasingly
challenging. Such bottlenecks, therefore, motivate this work to seek the
potential of using semantic communication, a new paradigm that promises to
significantly ease the resource pressure, for efficient VR delivery. To this
end, we propose a novel framework, namely WIreless SEmantic deliveRy for VR
(WiserVR), for delivering consecutive 360{\deg} video frames to VR users.
Specifically, deep learning-based multiple modules are well-devised for the
transceiver in WiserVR to realize high-performance feature extraction and
semantic recovery. Among them, we dedicatedly develop a concept of semantic
location graph and leverage the joint-semantic-channel-coding method with
knowledge sharing to not only substantially reduce communication latency, but
also to guarantee adequate transmission reliability and resilience under
various channel states. Moreover, implementation of WiserVR is presented,
followed by corresponding initial simulations for performance evaluation
compared with benchmarks. Finally, we discuss several open issues and offer
feasible solutions to unlock the full potential of WiserVR.","['Le Xia', 'Yao Sun', 'Chengsi Liang', 'Daquan Feng', 'Runze Cheng', 'Yang Yang', 'Muhammad Ali Imran']",2022-11-02T16:22:41Z,http://arxiv.org/abs/2211.01241v4,"['eess.IV', 'cs.AI', 'eess.SP']","Wireless,Virtual Reality,Semantic Communication,Latency,Reliability,Bandwidth,Deep Learning,Channel Coding,Performance Evaluation,Resource Management"
Characterizing Virtual Reality Software Testing,"Virtual Reality (VR) is an emerging technique that provides a unique
real-time experience for users. VR technologies have provided revolutionary
user experiences in various scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). However,
testing VR applications is challenging due to their nature which necessitates
physical interactivity, and their reliance on hardware systems. Despite the
recent advancements in VR technology and its usage scenarios, we still know
little about VR application testing. To fill up this knowledge gap, we
performed an empirical study on 97 open-source VR applications including 28
industrial projects. Our analysis identified that 74.2% of the VR projects
evaluated did not have any tests, and for the VR projects that did, the median
functional-method to test-method ratio was low in comparison to other project
categories. Moreover, we uncovered tool support issues concerning the
measurement of VR code coverage, and the code coverage and assertion density
results we were able to generate were also relatively low, as they respectively
had averages of 15.63% and 17.69%. Finally, through manual analysis of 220 test
cases from four VR applications and 281 test cases from four non-VR
applications, we identified that VR applications require specific categories of
test cases to ensure VR application quality attributes. We believe that our
findings constitute a call to action for the VR development community to
improve testing aspects and provide directions for software engineering
researchers to develop advanced techniques for automatic test case generation
and test quality analysis for VR applications.","['Dhia Elhaq Rzig', 'Nafees Iqbal', 'Isabella Attisano', 'Xue Qin', 'Foyzul Hassan']",2022-11-03T16:59:01Z,http://arxiv.org/abs/2211.01992v1,['cs.SE'],"Virtual Reality,Software Testing,VR applications,Hardware systems,Empirical Study,Code Coverage,Assertion Density,Test Cases,Quality Attributes,Test Quality Analysis"
Analyzing Performance Issues of Virtual Reality Applications,"Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)
and Mixed Reality (MR). XR is an emerging technology that simulates a realistic
environment for users. XR techniques have provided revolutionary user
experiences in various application scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). Due to the
high computational cost of rendering real-time animation in limited-resource
devices and constant interaction with user activity, XR applications often face
performance bottlenecks, and these bottlenecks create a negative impact on the
user experience of XR software. Thus, performance optimization plays an
essential role in many industry-standard XR applications. Even though
identifying performance bottlenecks in traditional software (e.g., desktop
applications) is a widely explored topic, those approaches cannot be directly
applied within XR software due to the different nature of XR applications.
Moreover, XR applications developed in different frameworks such as Unity and
Unreal Engine show different performance bottleneck patterns and thus,
bottleneck patterns of Unity projects can't be applied for Unreal Engine
(UE)-based XR projects. To fill the knowledge gap for XR performance
optimizations of Unreal Engine-based XR projects, we present the first
empirical study on performance optimizations from seven UE XR projects, 78 UE
XR discussion issues and three sources of UE documentation. Our analysis
identified 14 types of performance bugs, including 12 types of bugs related to
UE settings issues and two types of CPP source code-related issues. To further
assist developers in detecting performance bugs based on the identified bug
patterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect
performance bugs in both configuration files and source code.","['Jason Hogan', 'Aaron Salo', 'Dhia Elhaq Rzig', 'Foyzul Hassan', 'Bruce Maxim']",2022-11-03T17:27:36Z,http://arxiv.org/abs/2211.02013v1,['cs.SE'],"Virtual Reality,Augmented Reality,Mixed Reality,Extended Reality,XR,performance issues,performance optimization,bottleneck patterns,Unreal Engine,UEPerfAnalyzer"
Assessment of Human Behavior in Virtual Reality by Eye Tracking,"Virtual reality (VR) is not a new technology but has been in development for
decades, driven by advances in computer technology. Currently, VR technology is
increasingly being used in applications to enable immersive, yet controlled
research settings. Education and entertainment are two important application
areas, where VR has been considered a key enabler of immersive experiences and
their further advancement. At the same time, the study of human behavior in
such innovative environments is expected to contribute to a better design of VR
applications. Therefore, modern VR devices are consistently equipped with
eye-tracking technology, enabling thus further studies of human behavior
through the collection of process data. In particular, eye-tracking technology
in combination with machine learning techniques and explainable models can
provide new insights for a deeper understanding of human behavior during
immersion in virtual environments.
  In this work, a systematic computational framework based on eye-tracking and
behavioral user data and state-of-the-art machine learning approaches is
proposed to understand human behavior and individual differences in VR
contexts. This computational framework is then employed in three user studies
across two different domains. In the educational domain, two different
immersive VR classrooms were created where students can learn and teachers can
train. In terms of VR entertainment, eye movements open a new avenue to
evaluate VR locomotion techniques from the perspective of user cognitive load
and user experience. This work paves the way for assessing human behavior in VR
scenarios and provides profound insights into the way of designing, evaluating,
and improving interactive VR systems. In particular, more effective and
customizable virtual environments can be created to provide users with tailored
experiences.",['Hong Gao'],2022-11-23T10:49:03Z,http://arxiv.org/abs/2211.12846v1,"['cs.HC', 'I.0']","Virtual reality,Eye tracking,Human behavior,Machine learning,Immersive experiences,User studies,Cognitive load,User experience,Interactive VR systems,Virtual environments"
Label Guidance based Object Locating in Virtual Reality,"Object locating in virtual reality (VR) has been widely used in many VR
applications, such as virtual assembly, virtual repair, virtual remote
coaching. However, when there are a large number of objects in the virtual
environment(VE), the user cannot locate the target object efficiently and
comfortably. In this paper, we propose a label guidance based object locating
method for locating the target object efficiently in VR. Firstly, we introduce
the label guidance based object locating pipeline to improve the efficiency of
the object locating. It arranges the labels of all objects on the same screen,
lets the user select the target labels first, and then uses the flying labels
to guide the user to the target object. Then we summarize five principles for
constructing the label layout for object locating and propose a two-level
hierarchical sorted and orientated label layout based on the five principles
for the user to select the candidate labels efficiently and comfortably. After
that, we propose the view and gaze based label guidance method for guiding the
user to locate the target object based on the selected candidate labels.It
generates specific flying trajectories for candidate labels, updates the flying
speed of candidate labels, keeps valid candidate labels , and removes the
invalid candidate labels in real time during object locating with the guidance
of the candidate labels. Compared with the traditional method, the user study
results show that our method significantly improves efficiency and reduces task
load for object locating.","['Xiaoheng Wei', 'Xuehuai Shi', 'Lili Wang']",2022-12-07T09:52:40Z,http://arxiv.org/abs/2212.03546v1,['cs.HC'],"Virtual reality,Object locating,Label guidance,Label layout,Hierarchical,Flying labels,User study,Efficiency,Task load"
"CoVRage: Millimeter-Wave Beamforming for Mobile Interactive Virtual
  Reality","Contemporary Virtual Reality (VR) setups often include an external source
delivering content to a Head-Mounted Display (HMD). ""Cutting the wire"" in such
setups and going truly wireless will require a wireless network capable of
delivering enormous amounts of video data at an extremely low latency. The
massive bandwidth of higher frequencies, such as the millimeter-wave (mmWave)
band, can meet these requirements. Due to high attenuation and path loss in the
mmWave frequencies, beamforming is essential. In wireless VR, where the antenna
is integrated into the HMD, any head rotation also changes the antenna's
orientation. As such, beamforming must adapt, in real-time, to the user's head
rotations. An HMD's built-in sensors providing accurate orientation estimates
may facilitate such rapid beamforming. In this work, we present coVRage, a
receive-side beamforming solution tailored for VR HMDs. Using built-in
orientation prediction present on modern HMDs, the algorithm estimates how the
Angle of Arrival (AoA) at the HMD will change in the near future, and covers
this AoA trajectory with a dynamically shaped oblong beam, synthesized using
sub-arrays. We show that this solution can cover these trajectories with
consistently high gain, even in light of temporally or spatially inaccurate
orientational data.","['Jakob Struye', 'Filip Lemic', 'Jeroen Famaey']",2022-12-12T13:04:28Z,http://arxiv.org/abs/2212.05865v1,"['cs.NI', 'eess.SP']","millimeter-wave,beamforming,virtual reality,Head-Mounted Display,wireless network,antenna,orientation,Angle of Arrival,sub-arrays,gain"
"Artificial intelligence-driven digital twin of a modern house
  demonstrated in virtual reality","A digital twin is a powerful tool that can help monitor and optimize physical
assets in real-time. Simply put, it is a virtual representation of a physical
asset, enabled through data and simulators, that can be used for a variety of
purposes such as prediction, monitoring, and decision-making. However, the
concept of digital twin can be vague and difficult to understand, which is why
a new concept called ""capability level"" has been introduced. This concept
categorizes digital twins based on their capability and defines a scale from
zero to five, with each level indicating an increasing level of functionality.
These levels are standalone, descriptive, diagnostic, predictive, prescriptive,
and autonomous. By understanding the capability level of a digital twin, we can
better understand its potential and limitations. To demonstrate the concepts,
we use a modern house as an example. The house is equipped with a range of
sensors that collect data about its internal state, which can then be used to
create digital twins of different capability levels. These digital twins can be
visualized in virtual reality, allowing users to interact with and manipulate
the virtual environment. The current work not only presents a blueprint for
developing digital twins but also suggests future research directions to
enhance this technology. Digital twins have the potential to transform the way
we monitor and optimize physical assets, and by understanding their
capabilities, we can unlock their full potential.","['Elias Mohammed Elfarri', 'Adil Rasheed', 'Omer San']",2022-12-14T08:48:37Z,http://arxiv.org/abs/2212.07102v2,"['cs.CV', 'cs.GR', 'cs.HC']","Artificial intelligence,Digital twin,Virtual reality,Physical asset,Data,Capability level,Sensors,Virtual environment,Prediction,Optimization"
"Assessment of user-interaction strategies for neurosurgical data
  navigation and annotation in virtual reality","While virtual-reality (VR) has shown great promise in radiological tasks,
effective user-interaction strategies that can improve efficiency and
ergonomics are still under-explored and systematic evaluations of VR
interaction techniques in the context of complex anatomical models are rare.
Therefore, our study aims to identify the most effective interaction techniques
for two common neurosurgical planning tasks in VR (point annotation and
note-taking) from the state-of-the-arts, and propose a novel technique for
efficient sub-volume selection necessary in neuroanatomical navigation. We
assessed seven user-interaction methods with multiple input modalities (gaze,
head motion, controller, and voice) for point placement and note-taking in the
context of annotating brain aneurysms for cerebrovascular surgery. Furthermore,
we proposed and evaluated a novel technique, called magnified selection diorama
(Maserama) for easy navigation and selection of complex 3D anatomies in VR.
Both quantitative and semi-quantitative (i.e., NASA Task Load Index) metrics
were employed through user studies to reveal the performance of each
interaction scheme in terms of accuracy, efficiency, and usability. Our
evaluations demonstrated that controller-based interaction is preferred over
eye-tracking-based methods for point placement while voice recording and
virtual keyboard typing are better than freehand writing for note-taking.
Furthermore, our new Maserama sub-volume selection technique was proven to be
highly efficient and easy-to-use. Our study is the first to provide a
systematic assessment of existing and new VR interaction schemes for
neurosurgical data navigation and annotation. It offers valuable insights and
tools to guide the design of future VR systems for radiological and surgical
applications.","['Owen Hellum', 'Marta Kersten-Oertel', 'Yiming Xiao']",2022-12-16T19:36:04Z,http://arxiv.org/abs/2212.08688v1,['cs.HC'],"neurosurgical data,virtual reality,user-interaction strategies,annotation,navigation,anatomical models,point placement,note-taking,sub-volume selection,3D anatomies"
SoK: Data Privacy in Virtual Reality,"The adoption of virtual reality (VR) technologies has rapidly gained momentum
in recent years as companies around the world begin to position the so-called
""metaverse"" as the next major medium for accessing and interacting with the
internet. While consumers have become accustomed to a degree of data harvesting
on the web, the real-time nature of data sharing in the metaverse indicates
that privacy concerns are likely to be even more prevalent in the new ""Web
3.0."" Research into VR privacy has demonstrated that a plethora of sensitive
personal information is observable by various would-be adversaries from just a
few minutes of telemetry data. On the other hand, we have yet to see VR
parallels for many privacy-preserving tools aimed at mitigating threats on
conventional platforms. This paper aims to systematize knowledge on the
landscape of VR privacy threats and countermeasures by proposing a
comprehensive taxonomy of data attributes, protections, and adversaries based
on the study of 68 collected publications. We complement our qualitative
discussion with a statistical analysis of the risk associated with various data
sources inherent to VR in consideration of the known attacks and defenses. By
focusing on highlighting the clear outstanding opportunities, we hope to
motivate and guide further research into this increasingly important field.","['Gonzalo Munilla Garrido', 'Vivek Nair', 'Dawn Song']",2023-01-14T16:02:40Z,http://arxiv.org/abs/2301.05940v2,"['cs.HC', 'cs.CR']","Data Privacy,Virtual Reality,Metaverse,Web 3.0,Telemetry Data,Privacy-Preserving Tools,Threats,Countermeasures,Taxonomy,Risk Analysis"
"Exploiting Out-of-band Motion Sensor Data to De-anonymize Virtual
  Reality Users","Virtual Reality (VR) is an exciting new consumer technology which offers an
immersive audio-visual experience to users through which they can navigate and
interact with a digitally represented 3D space (i.e., a virtual world) using a
headset device. By (visually) transporting users from the real or physical
world to exciting and realistic virtual spaces, VR systems can enable
true-to-life and more interactive versions of traditional applications such as
gaming, remote conferencing, social networking and virtual tourism. However, as
with any new consumer technology, VR applications also present significant
user-privacy challenges. This paper studies a new type of privacy attack
targeting VR users by connecting their activities visible in the virtual world
(enabled by some VR application/service) to their physical state sensed in the
real world. Specifically, this paper analyzes the feasibility of carrying out a
de-anonymization or identification attack on VR users by correlating visually
observed movements of users' avatars in the virtual world with some auxiliary
data (e.g., motion sensor data from mobile/wearable devices held by users)
representing their context/state in the physical world. To enable this attack,
this paper proposes a novel framework which first employs a learning-based
activity classification approach to translate the disparate visual movement
data and motion sensor data into an activity-vector to ease comparison,
followed by a filtering and identity ranking phase outputting an ordered list
of potential identities corresponding to the target visual movement data.
Extensive empirical evaluation of the proposed framework, under a comprehensive
set of experimental settings, demonstrates the feasibility of such a
de-anonymization attack.","['Mohd Sabra', 'Nisha Vinayaga Sureshkanth', 'Ari Sharma', 'Anindya Maiti', 'Murtuza Jadliwala']",2023-01-22T02:50:05Z,http://arxiv.org/abs/2301.09041v1,"['cs.CR', 'cs.HC']","Virtual Reality,Motion Sensor Data,De-anonymization,VR Users,Privacy Attack,Activity Classification,Learning-based,Identity Ranking,Framework,Empirical Evaluation"
"SONIA: an immersive customizable virtual reality system for the
  education and exploration of brain networks","While mastery of neuroanatomy is important for the investigation of the
brain, there is an increasing interest in exploring the neural pathways to
better understand the roles of neural circuitry in brain functions. To tackle
the limitations of traditional 2D-display-based neuronavigation software in
intuitively visualizing complex 3D anatomies, several virtual reality (VR) and
augmented reality (AR) solutions have been proposed to facilitate
neuroanatomical education. However, with the increasing knowledge on brain
connectivity and the functioning of the sub-systems, there is still a lack of
similar software solutions for the education and exploration of these topics,
which demand more elaborate visualization and interaction strategies. To
address this gap, we designed the immerSive custOmizable Neuro learnIng plAform
(SONIA), a novel user-friendly VR software system with a multi-scale
interaction paradigm that allows flexible customization of learning materials.
With both quantitative and qualitative evaluations through user studies, the
proposed system is shown to have high usability, attractive visual design, and
good educational value. As the first immersive system that integrates
customizable design and detailed narratives of the brain sub-systems for the
education of neuroanatomy and brain connectivity, SONIA showcases new potential
directions and provides valuable insights regarding medical learning and
exploration in VR.","['Owen Hellum', 'Christopher Steele', 'Yiming Xiao']",2023-01-24T01:04:15Z,http://arxiv.org/abs/2301.09772v2,"['cs.HC', 'cs.MM']","neuroanatomy,brain networks,virtual reality,neuronavigation,3D anatomies,brain connectivity,interaction strategies,user-friendly,educational value"
Minimizing the Motion-to-Photon-delay (MPD) in Virtual Reality Systems,"With the advent of low-power ultra-fast hardware and GPUs, virtual reality
(VR) has gained a lot of prominence in the last few years and is being used in
various areas such as education, entertainment, scientific visualization, and
computer-aided design. VR-based applications are highly interactive, and one of
the most important performance metrics for these applications is the
motion-to-photon-delay (MPD). MPD is the delay from the users head movement to
the time at which the image gets updated on the VR screen. Since the human
visual system can even detect an error of a few pixels (very spatially
sensitive), the MPD should be as small as possible. Popular VR vendors use the
GPU-accelerated Asynchronous Time Warp (ATW) algorithm to reduce the MPD. ATW
reduces the MPD if and only if the warping operation finishes just before the
display refreshes. However, due to the competition between applications for the
shared GPU, the GPU-accelerated ATW algorithm suffers from an unpredictable ATW
latency, making it challenging to find the ideal time instance for starting the
time warp and ensuring that it completes with the least amount of lag relative
to the screen refresh. Hence, the state-of-the-art is to use a separate
hardware unit for the time warping operation. Our approach, PredATW, uses an
ML-based predictor to predict the ATW latency for a VR application, and then
schedule it as late as possible. This is the first work to do so. Our predictor
achieves an error of 0.77 ms across several popular VR applications for
predicting the ATW latency. As compared to the baseline architecture, we reduce
deadline misses by 73.1%.","['Akanksha Dixit', 'Smruti R. Sarangi']",2023-01-25T05:10:04Z,http://arxiv.org/abs/2301.10408v1,['cs.AR'],"Virtual reality,Motion-to-photon-delay,GPU,Asynchronous Time Warp,ATW algorithm,Latency,ML-based predictor,Deadline misses,Hardware unit"
"User-centric Heterogeneous-action Deep Reinforcement Learning for
  Virtual Reality in the Metaverse over Wireless Networks","The Metaverse is emerging as maturing technologies are empowering the
different facets. Virtual Reality (VR) technologies serve as the backbone of
the virtual universe within the Metaverse to offer a highly immersive user
experience. As mobility is emphasized in the Metaverse context, VR devices
reduce their weights at the sacrifice of local computation abilities. In this
paper, for a system consisting of a Metaverse server and multiple VR users, we
consider two cases of (i) the server generating frames and transmitting them to
users, and (ii) users generating frames locally and thus consuming device
energy. Moreover, in our multi-user VR scenario for the Metaverse, users have
different characteristics and demands for Frames Per Second (FPS). Then the
channel access arrangement (including the decisions on frame generation
location), and transmission powers for the downlink communications from the
server to the users are jointly optimized to improve the utilities of users.
This joint optimization is addressed by deep reinforcement learning (DRL) with
heterogeneous actions. Our proposed user-centric DRL algorithm is called
User-centric Critic with Heterogenous Actors (UCHA). Extensive experiments
demonstrate that our UCHA algorithm leads to remarkable results under various
requirements and constraints.","['Wenhan Yu', 'Terence Jie Chua', 'Jun Zhao']",2023-02-03T00:12:12Z,http://arxiv.org/abs/2302.01471v2,"['cs.NI', 'cs.AI', 'cs.LG']","Virtual Reality,Metaverse,Wireless Networks,Deep Reinforcement Learning,User-centric,Frames Per Second,Channel Access,Transmission Powers,Heterogeneous Actions"
Virtual Reality for medical education and training of Diabetic Foot,"Diabetic Foot is one of the most common complications of Diabetes Mellitus
and the leading non-traumatic cause of lower-limb amputations worldwide. These
complications are preventable with early diagnosis and timely care. However,
even in the context of expanding primary health care, this problem continues to
increase, which suggests a gap in the training of primary health care
professionals regarding the diagnosis and treatment of Diabetic Foot. This
project proposes the development of a Virtual Reality simulator for training
students and professionals in primary health care, aiming to collaborate in
filling this gap. The application features gamification elements to increase
user engagement. The context of medical care in primary care will be simulated
with various clinical cases, including several virtual patients with different
stages related to the Diabetic Foot, seeking to provide credible and distinct
experiences to potential users. In addition, we aim to verify usability,
effectiveness, and any side effects (cybersickness). Finally, we plan to
conduct field studies with qualified students and professionals to identify the
main benefits and obstacles to applying the technology.","['Gabriel Riva', 'Wellington Dores', 'Artur Damasio', 'Daniel Guimarães Cacione', 'Joaquim Jorge', 'Ezequiel Zorzal']",2023-02-06T17:14:17Z,http://arxiv.org/abs/2302.02939v1,['cs.HC'],"Virtual Reality,medical education,training,Diabetic Foot,Diabetes Mellitus,lower-limb amputations,primary health care,Virtual Reality simulator,gamification,usability"
"Evaluation of Virtual Reality Interaction Techniques: the case of 3D
  Graph","The virtual reality (VR) and human-computer interaction (HCI) combination has
radically changed the way users approach a virtual environment, increasing the
feeling of VR immersion, and improving the user experience and usability. The
evolution of these two technologies led to the focus on VR locomotion and
interaction. Locomotion is generally controller-based, but today hand gesture
recognition methods were also used for this purpose. However, hand gestures can
be stressful for the user who has to keep the gesture activation for a long
time to ensure locomotion, especially continuously. Likewise, in Head Mounted
Display (HMD)-based virtual environment or Spherical-based system, the use of
classic controllers for the 3D scene interaction could be unnatural for the
user compared to using hand gestures such \eg pinching to grab 3D objects. To
address these issues, we propose a user study comparing the use of the classic
controllers (six-degree-of-freedom (6-DOF) or trackballs) in HMD and
spherical-based systems, and the hand tracking and gestures in both VR
immersive modes. In particular, we focused on the possible differences between
spherical-based systems and HMD in terms of the level of immersion perceived by
the user, the mode of user interaction (controller and hands), on the reaction
of users concerning usefulness, easiness, and behavioral intention to use.","['Nicola Capece', 'Ugo Erra', 'Delfina Malandrino', 'Max M. North', 'Monica Gruosso']",2023-02-11T11:35:19Z,http://arxiv.org/abs/2302.05660v1,"['cs.HC', 'cs.GR']","virtual reality,interaction techniques,3D graph,locomotion,hand gesture recognition,Head Mounted Display,user study,6-DOF,trackballs,immersive modes"
"Use of immersive virtual reality-based experiments to study tactical
  decision-making during emergency evacuation","Humans make their evacuation decisions first at strategic/tactical levels,
deciding their exit and route choice and then at operational level, navigating
to a way-point, avoiding collisions. What influences an individuals at tactical
level is of importance, for modelers to design a high fidelity simulation or
for safety engineers to create efficient designs/codes. Does an unlit exit sign
dissuades individual(s) to avoid a particular exit/route and vice versa? What
effect does the crowd's choices have on individual's decision making? To answer
these questions, we studied the effect of exit signage (unlit/lit), different
proportions of crowd movement towards the exits, and the combined
(reinforcing/conflicting) effect of the sign and the crowd treatment on
reaction times and exit choices of participants in an immersive virtual
reality(VR) evacuation experiment. We found that there is tolerance for queuing
when different sources of information, exit signage and crowd movement
reinforced one another. The effect of unlit exit signage on dissuading
individuals from using a particular exit/route was significant. The virtual
crowd was ineffective at encouraging utilization of a particular exit/route but
had a slight repulsive effect. Additionally, we found some similarities between
previous studies based on screen-based evacuation experiments and our VR-based
experiment.","['Laura M. Harris', 'Subhadeep Chakraborty', 'Aravinda Ramakrishnan Srinivasan']",2023-02-20T22:04:28Z,http://arxiv.org/abs/2302.10339v1,['cs.HC'],"immersive virtual reality,tactical decision-making,emergency evacuation,exit sign,route choice,crowd movement,reaction times,virtual reality experiment,exit choice,queuing"
"Using Virtual Reality to Shape Humanity's Return to the Moon: Key
  Takeaways from a Design Study","Revived interest in lunar exploration is heralding a new generation of design
solutions in support of human operations on the Moon. While space system design
has traditionally been guided by prototype deployments in analogue studies, the
resource-intensive nature of this approach has largely precluded application of
proficient user-centered design (UCD) methods from human-computer interaction
(HCI). This paper explores possible use of Virtual Reality (VR) to simulate
analogue studies in lab settings and thereby bring to bear UCD in this
otherwise engineering-dominated field. Drawing on the ongoing development of
the European Large Logistics Lander, we have recreated a prospective lunar
operational scenario in VR and evaluated it with a group of astronauts and
space experts (n=20). Our qualitative findings demonstrate the efficacy of VR
in facilitating UCD, enabling efficient contextual inquiries and improving
project team coordination. We conclude by proposing future directions to
further exploit VR in lunar systems design.","['Tommy Nilsson', 'Flavie Rometsch', 'Leonie Becker', 'Florian Dufresne', 'Paul de Medeiros', 'Enrico Guerra', 'Andrea E. M. Casini', 'Anna Vock', 'Florian Gaeremynck', 'Aidan Cowley']",2023-03-01T17:19:48Z,http://arxiv.org/abs/2303.00678v1,"['cs.HC', 'cs.CY', '93B51, 97M50', 'H.1.2; I.3.8; J.4; J.m; K.8.2; J.6']","Virtual Reality,Lunar exploration,Design study,User-centered design,Human-computer interaction,Analogue studies,Resource-intensive,European Large Logistics Lander,Project team coordination"
"User Preferences of Spatio-Temporal Referencing Approaches For Immersive
  3D Radar Charts","The use of head-mounted display technologies for virtual reality experiences
is inherently single-user-centred, allowing for the visual immersion of its
user in the computer-generated environment. This isolates them from their
physical surroundings, effectively preventing external visual information cues,
such as the pointing and referral to an artifact by another user. However, such
input is important and desired in collaborative scenarios when exploring and
analyzing data in virtual environments together with a peer. In this article,
we investigate different designs for making spatio-temporal references, i.e.,
visually highlighting virtual data artifacts, within the context of
Collaborative Immersive Analytics. The ability to make references to data is
foundational for collaboration, affecting aspects such as awareness, attention,
and common ground. Based on three design options, we implemented a variety of
approaches to make spatial and temporal references in an immersive virtual
reality environment that featured abstract visualization of spatio-temporal
data as 3D Radar Charts. We conducted a user study (n=12) to empirically
evaluate aspects such as aesthetic appeal, legibility, and general user
preference. The results indicate a unified favour for the presented location
approach as a spatial reference while revealing trends towards a preference of
mixed temporal reference approaches dependent on the task configuration:
pointer for elementary, and outline for synoptic references. Based on immersive
data visualization complexity as well as task reference configuration, we argue
that it can be beneficial to explore multiple reference approaches as
collaborative information cues, as opposed to following a rather uniform user
interface design.","['Nico Reski', 'Aris Alissandrakis', 'Andreas Kerren']",2023-03-14T13:38:20Z,http://arxiv.org/abs/2303.07899v1,"['cs.HC', 'H.5.2']","head-mounted display,virtual reality,immersive analytics,spatio-temporal references,3D Radar Charts,user study,aesthetic appeal,collaborative scenarios,data visualization"
"Designing a 3D Gestural Interface to Support User Interaction with
  Time-Oriented Data as Immersive 3D Radar Chart","The design of intuitive three-dimensional user interfaces is vital for
interaction in virtual reality, allowing to effectively close the loop between
a human user and the virtual environment. The utilization of 3D gestural input
allows for useful hand interaction with virtual content by directly grasping
visible objects, or through invisible gestural commands that are associated
with corresponding features in the immersive 3D space. The design of such
interfaces remains complex and challenging. In this article, we present a
design approach for a three-dimensional user interface using 3D gestural input
with the aim to facilitate user interaction within the context of Immersive
Analytics. Based on a scenario of exploring time-oriented data in immersive
virtual reality using 3D Radar Charts, we implemented a rich set of features
that is closely aligned with relevant 3D interaction techniques, data analysis
tasks, and aspects of hand posture comfort. We conducted an empirical
evaluation (n=12), featuring a series of representative tasks to evaluate the
developed user interface design prototype. The results, based on
questionnaires, observations, and interviews, indicate good usability and an
engaging user experience. We are able to reflect on the implemented hand-based
grasping and gestural command techniques, identifying aspects for improvement
in regard to hand detection and precision as well as emphasizing a prototype's
ability to infer user intent for better prevention of unintentional gestures.","['Nico Reski', 'Aris Alissandrakis', 'Andreas Kerren']",2023-03-14T15:48:14Z,http://arxiv.org/abs/2303.07995v2,"['cs.HC', 'H.5.2']","3D Gestural Interface,User Interaction,Time-Oriented Data,Immersive 3D Radar Chart,Virtual Reality,3D Gestural Input,Immersive Analytics,Data Analysis Tasks,Hand Posture Comfort,User Interface Design"
"How Interactions Influence Users' Security Perception of Virtual Reality
  Authentication?","Users readily embrace the rapid advancements in virtual reality (VR)
technology within various everyday contexts, such as gaming, social
interactions, shopping, and commerce. In order to facilitate transactions and
payments, VR systems require access to sensitive user data and assets, which
consequently necessitates user authentication. However, there exists a limited
understanding regarding how users' unique experiences in VR contribute to their
perception of security. In our study, we adopt a research approach known as
``technology probe'' to investigate this question. Specifically, we have
designed probes that explore the authentication process in VR, aiming to elicit
responses from participants from multiple perspectives. These probes were
seamlessly integrated into the routine payment system of a VR game, thereby
establishing an organic study environment. Through qualitative analysis, we
uncover the interplay between participants' interaction experiences and their
security perception. Remarkably, despite encountering unique challenges in
usability during VR interactions, our participants found the intuitive
virtualized authentication process beneficial and thoroughly enjoyed the
immersive nature of VR. Furthermore, we observe how these interaction
experiences influence participants' ability to transfer their pre-existing
understanding of authentication into VR, resulting in a discrepancy in
perceived security. Moreover, we identify users' conflicting expectations,
encompassing their desire for an enjoyable VR experience alongside the
assurance of secure VR authentication. Building upon our findings, we propose
recommendations aimed at addressing these expectations and alleviating
potential conflicts.","['Jingjie Li', 'Sunpreet Singh Arora', 'Kassem Fawaz', 'Younghyun Kim', 'Can Liu', 'Sebastian Meiser', 'Mohsen Minaei', 'Maliheh Shirvanian', 'Kim Wagner']",2023-03-21T03:47:54Z,http://arxiv.org/abs/2303.11575v3,"['cs.CR', 'cs.HC']","Virtual reality,Security perception,Authentication,Technology probe,Interaction experiences,Usability,Immersive nature,User data,Payment system,Qualitative analysis"
"UndoPort: Exploring the Influence of Undo-Actions for Locomotion in
  Virtual Reality on the Efficiency, Spatial Understanding and User Experience","When we get lost in Virtual Reality (VR) or want to return to a previous
location, we use the same methods of locomotion for the way back as for the way
forward. This is time-consuming and requires additional physical orientation
changes, increasing the risk of getting tangled in the headsets' cables. In
this paper, we propose the use of undo actions to revert locomotion steps in
VR. We explore eight different variations of undo actions as extensions of
point\&teleport, based on the possibility to undo position and orientation
changes together with two different visualizations of the undo step (discrete
and continuous). We contribute the results of a controlled experiment with 24
participants investigating the efficiency and orientation of the undo
techniques in a radial maze task. We found that the combination of position and
orientation undo together with a discrete visualization resulted in the highest
efficiency without increasing orientation errors.","['Florian Müller', 'Arantxa Ye', 'Dominik Schön', 'Julian Rasch']",2023-03-28T08:09:37Z,http://arxiv.org/abs/2303.15800v2,['cs.HC'],"VR,Locomotion,Undo actions,Spatial understanding,User experience,Point\&teleport,Controlled experiment,Orientation errors,Virtual Reality"
"Predictive Context-Awareness for Full-Immersive Multiuser Virtual
  Reality with Redirected Walking","The advancement of Virtual Reality (VR) technology is focused on improving
its immersiveness, supporting multiuser Virtual Experiences (VEs), and enabling
users to move freely within their VEs while remaining confined to specialized
VR setups through Redirected Walking (RDW). To meet their extreme data-rate and
latency requirements, future VR systems will require supporting wireless
networking infrastructures operating in millimeter Wave (mmWave) frequencies
that leverage highly directional communication in both transmission and
reception through beamforming and beamsteering. We propose the use of
predictive context-awareness to optimize transmitter and receiver-side
beamforming and beamsteering. By predicting users' short-term lateral movements
in multiuser VR setups with Redirected Walking (RDW), transmitter-side
beamforming and beamsteering can be optimized through Line-of-Sight (LoS)
""tracking"" in the users' directions. At the same time, predictions of
short-term orientational movements can be utilized for receiver-side
beamforming for coverage flexibility enhancements. We target two open problems
in predicting these two context information instances: i) predicting lateral
movements in multiuser VR settings with RDW, and ii) generating synthetic head
rotation datasets for training orientational movements predictors. Our
experimental results demonstrate that Long Short-Term Memory (LSTM) networks
feature promising accuracy in predicting lateral movements, and
context-awareness stemming from VEs further enhances this accuracy.
Additionally, we show that a TimeGAN-based approach for orientational data
generation can create synthetic samples that closely match experimentally
obtained ones.","['Filip Lemic', 'Jakob Struye', 'Thomas Van Onsem', 'Jeroen Famaey', 'Xavier Costa Perez']",2023-03-31T09:09:17Z,http://arxiv.org/abs/2303.17907v4,"['cs.NI', 'cs.LG']","Virtual Reality,Multiuser,Redirected Walking,Predictive Context-Awareness,Beamforming,Beamsteering,Millimeter Wave,Latency,Long Short-Term Memory (LSTM),TimeGAN"
"Virtual Reality Training of Social Skills in Autism Spectrum Disorder:
  An Examination of Acceptability, Usability, User Experience, Social Skills,
  and Executive Functions","Poor social skills in autism spectrum disorder (ASD) are associated with
reduced independence in daily life. Current interventions for improving the
social skills of individuals with ASD fail to represent the complexity of
real-life social settings and situations. Virtual reality (VR) may facilitate
social skills training in social environments and situations proximal to real
life, however, more research is needed for elucidating aspects such as the
acceptability, usability, and user experience of VR systems in ASD. Twenty-five
participants with ASD attended a neuropsychological evaluation and three
sessions of VR social skills training, incorporating five (5) social scenarios
with three difficulty levels for each. Participants reported high
acceptability, system usability, and user experience. Significant correlations
were observed between performance in social scenarios, self-reports, and
executive functions. Working memory and planning ability were significant
predictors of functionality level in ASD and the VR system's perceived
usability respectively. Yet, performance in social scenarios was the best
predictor of usability, acceptability, and functionality level in ASD. Planning
ability substantially predicted performance in social scenarios, postulating an
implication in social skills. Immersive VR social skills training appears
effective in individuals with ASD, yet an error-less approach, which is
adaptive to the individual's needs, should be preferred.","['Panagiotis Kourtesis', 'Evangelia-Chrysanthi Kouklari', 'Petros Roussos', 'Vasileios Mantas', 'Katerina Papanikolaou', 'Christos Skaloumbakas', 'Artemios Pehlivanidis']",2023-04-15T07:54:37Z,http://arxiv.org/abs/2304.07498v1,"['cs.HC', 'cs.CY', 'cs.MM', 'B.8; C.4; D.0; J.4']","autism spectrum disorder,virtual reality,social skills,executive functions,acceptability,usability,user experience,neuropsychological evaluation,immersive VR,social scenarios"
"Immersive Virtual Reality and Robotics for Upper Extremity
  Rehabilitation","Stroke patients often experience upper limb impairments that restrict their
mobility and daily activities. Physical therapy (PT) is the most effective
method to improve impairments, but low patient adherence and participation in
PT exercises pose significant challenges. To overcome these barriers, a
combination of virtual reality (VR) and robotics in PT is promising. However,
few systems effectively integrate VR with robotics, especially for upper limb
rehabilitation. This work introduces a new virtual rehabilitation solution that
combines VR with robotics and a wearable sensor to analyze elbow joint
movements. The framework also enhances the capabilities of a traditional
robotic device (KinArm) used for motor dysfunction assessment and
rehabilitation. A pilot user study (n = 16) was conducted to evaluate the
effectiveness and usability of the proposed VR framework. We used a two-way
repeated measures experimental design where participants performed two tasks
(Circle and Diamond) with two conditions (VR and VR KinArm). We observed no
significant differences in the main effect of conditions for task completion
time. However, there were significant differences in both the normalized number
of mistakes and recorded elbow joint angles (captured as resistance change
values from the wearable sleeve sensor) between the Circle and Diamond tasks.
Additionally, we report the system usability, task load, and presence in the
proposed VR framework. This system demonstrates the potential advantages of an
immersive, multi-sensory approach and provides future avenues for research in
developing more cost-effective, tailored, and personalized upper limb solutions
for home therapy applications.","['Vuthea Chheang', 'Rakshith Lokesh', 'Amit Chaudhari', 'Qile Wang', 'Lauren Baron', 'Behdokht Kiafar', 'Sagar Doshi', 'Erik Thostenson', 'Joshua Cashaback', 'Roghayeh Leila Barmaki']",2023-04-21T16:28:31Z,http://arxiv.org/abs/2304.11110v2,"['cs.HC', 'cs.RO']","Immersive virtual reality,Robotics,Upper extremity rehabilitation,Virtual reality (VR),Wearable sensor,Elbow joint movements,Motor dysfunction assessment,Pilot user study,Usability,Task load"
"Remote Monitoring and Teleoperation of Autonomous Vehicles $-$ Is
  Virtual Reality an Option?","While the promise of autonomous vehicles has led to significant scientific
and industrial progress, fully automated, SAE level 5 conform cars will likely
not see mass adoption anytime soon. Instead, in many applications, human
supervision, such as remote monitoring and teleoperation, will be required for
the foreseeable future. While Virtual Reality (VR) has been proposed as one
potential interface for teleoperation, its benefits and drawbacks over physical
monitoring and teleoperation solutions have not been thoroughly investigated.
To this end, we contribute three user studies, comparing and quantifying the
performance of and subjective feedback for a VR-based system with an existing
monitoring and teleoperation system, which is in industrial use today. Through
these three user studies, we contribute to a better understanding of future
virtual monitoring and teleoperation solutions for autonomous vehicles. The
results of our first user study (n=16) indicate that a VR interface replicating
the physical interface does not outperform the physical interface. It also
quantifies the negative effects that combined monitoring and teleoperating
tasks have on users irrespective of the interface being used. The results of
the second user study (n=24) indicate that the perceptual and ergonomic issues
caused by VR outweigh its benefits, like better concentration through
isolation. The third follow-up user study (n=24) specifically targeted the
perceptual and ergonomic issues of VR; the subjective feedback of this study
indicates that newer-generation VR headsets have the potential to catch up with
the current physical displays.","['Snehanjali Kalamkar', 'Verena Biener', 'Fabian Beck', 'Jens Grubert']",2023-04-21T19:29:42Z,http://arxiv.org/abs/2304.11228v2,['cs.HC'],"Remote Monitoring,Teleoperation,Autonomous Vehicles,Virtual Reality,User Studies,Performance,Subjective Feedback,Physical Interface,VR Interface"
"A New Technique of the Virtual Reality Visualization of Complex Volume
  Images from the Computer Tomography and Magnetic Resonance Imaging","This paper presents a new technique for the virtual reality (VR)
visu-alization of complex volume images obtained from computer tomography (CT)
and Magnetic Resonance Imaging (MRI) by combining three-dimensional (3D) mesh
processing and software coding within the gaming engine. The method operates on
real representations of human organs avoiding any structural ap-proximations of
the real physiological shape. In order to obtain realistic repre-sentation of
the mesh model, geometrical and topological corrections are per-formed on the
mesh surface with preserving real shape and geometric structure. Using
mathematical intervention on the 3D model and mesh triangulation the second
part of our algorithm ensures an automatic construction of new two-dimensional
(2D) shapes that represent vector slices along any user chosen di-rection. The
final result of our algorithm is developed software application that allows to
user complete visual experience and perceptual exploration of real human organs
through spatial manipulation of their 3D models. Thus our pro-posed method
achieves a threefold effect: i) high definition VR representation of real
models of human organs, ii) the real time generated slices of such a model
along any directions, and iii) almost unlimited amount of training data for
machine learning that is very useful in process of diagnosis. In addition, our
developed application also offers significant benefits to educational process
by ensuring interactive features and quality perceptual user experience.","['Iva Vasic', 'Roberto Pierdicca', 'Emanuele Frontoni', 'Bata Vasic']",2023-04-28T22:27:33Z,http://arxiv.org/abs/2305.00116v1,['cs.MM'],"Virtual Reality,Visualization,Complex Volume Images,Computer Tomography,Magnetic Resonance Imaging,3D Mesh Processing,Software Coding,Gaming Engine,Mesh Model,Geometrical Corrections"
"Attention-based QoE-aware Digital Twin Empowered Edge Computing for
  Immersive Virtual Reality","Metaverse applications such as virtual reality (VR) content streaming,
require optimal resource allocation strategies for mobile edge computing (MEC)
to ensure a high-quality user experience. In contrast to online reinforcement
learning (RL) algorithms, which can incur substantial communication overheads
and longer delays, the majority of existing works employ offline-trained RL
algorithms for resource allocation decisions in MEC systems. However, they
neglect the impact of desynchronization between the physical and digital worlds
on the effectiveness of the allocation strategy. In this paper, we tackle this
desynchronization using a continual RL framework that facilitates the resource
allocation dynamically for MEC-enabled VR content streaming. We first design a
digital twin-empowered edge computing (DTEC) system and formulate a quality of
experience (QoE) maximization problem based on attention-based resolution
perception. This problem optimizes the allocation of computing and bandwidth
resources while adapting the attention-based resolution of the VR content. The
continual RL framework in DTEC enables adaptive online execution in a
time-varying environment. The reward function is defined based on the QoE and
horizon-fairness QoE (hfQoE) constraints. Furthermore, we propose freshness
prioritized experience replay - continual deep deterministic policy gradient
(FPER-CDDPG) to enhance the performance of continual learning in the presence
of time-varying DT updates. We test FPER-CDDPG using extensive experiments and
evaluation. FPER-CDDPG outperforms the benchmarks in terms of average latency,
QoE, and successful delivery rate as well as meeting the hfQoE requirements and
performance over long-term execution while ensuring system scalability with the
increasing number of users.","['Jiadong Yu', 'Ahmad Alhilal', 'Tailin Zhou', 'Pan Hui', 'Danny H. K. Tsang']",2023-05-15T11:54:46Z,http://arxiv.org/abs/2305.08569v2,"['eess.SY', 'cs.SY']","Quality of Experience,Digital Twin,Edge Computing,Virtual Reality,Resource Allocation,Reinforcement Learning,Resolution Perception,Continual Learning,Deep Deterministic Policy Gradient,Horizon-fairness"
"Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and
  Privacy Challenges","Latest developments in computer hardware, sensor technologies, and artificial
intelligence can make virtual reality (VR) and virtual spaces an important part
of human everyday life. Eye tracking offers not only a hands-free way of
interaction but also the possibility of a deeper understanding of human visual
attention and cognitive processes in VR. Despite these possibilities,
eye-tracking data also reveal privacy-sensitive attributes of users when it is
combined with the information about the presented stimulus. To address these
possibilities and potential privacy issues, in this survey, we first cover
major works in eye tracking, VR, and privacy areas between the years 2012 and
2022. While eye tracking in the VR part covers the complete pipeline of
eye-tracking methodology from pupil detection and gaze estimation to offline
use and analyses, as for privacy and security, we focus on eye-based
authentication as well as computational methods to preserve the privacy of
individuals and their eye-tracking data in VR. Later, taking all into
consideration, we draw three main directions for the research community by
mainly focusing on privacy challenges. In summary, this survey provides an
extensive literature review of the utmost possibilities with eye tracking in VR
and the privacy implications of those possibilities.","['Efe Bozkir', 'Süleyman Özdel', 'Mengdi Wang', 'Brendan David-John', 'Hong Gao', 'Kevin Butler', 'Eakta Jain', 'Enkelejda Kasneci']",2023-05-23T14:02:38Z,http://arxiv.org/abs/2305.14080v1,"['cs.HC', 'cs.AI', 'cs.CR', 'cs.GR', 'cs.LG']","virtual reality,eye tracking,privacy challenges,sensor technologies,artificial intelligence,pupil detection,gaze estimation,eye-based authentication,computational methods"
"Inferring Private Personal Attributes of Virtual Reality Users from Head
  and Hand Motion Data","Motion tracking ""telemetry"" data lies at the core of nearly all modern
virtual reality (VR) and metaverse experiences. While generally presumed
innocuous, recent studies have demonstrated that motion data actually has the
potential to uniquely identify VR users. In this study, we go a step further,
showing that a variety of private user information can be inferred just by
analyzing motion data recorded from VR devices. We conducted a large-scale
survey of VR users (N=1,006) with dozens of questions ranging from background
and demographics to behavioral patterns and health information. We then
obtained VR motion samples of each user playing the game ""Beat Saber,"" and
attempted to infer their survey responses using just their head and hand motion
patterns. Using simple machine learning models, over 40 personal attributes
could be accurately and consistently inferred from VR motion data alone.
Despite this significant observed leakage, there remains limited awareness of
the privacy implications of VR motion data, highlighting the pressing need for
privacy-preserving mechanisms in multi-user VR applications.","['Vivek Nair', 'Christian Rack', 'Wenbo Guo', 'Rui Wang', 'Shuixian Li', 'Brandon Huang', 'Atticus Cull', ""James F. O'Brien"", 'Marc Latoschik', 'Louis Rosenberg', 'Dawn Song']",2023-05-30T16:44:40Z,http://arxiv.org/abs/2305.19198v3,"['cs.HC', 'cs.CR']","virtual reality,motion tracking,telemetry data,metaverse,private user information,behavioral patterns,health information,machine learning models,privacy implications,VR applications"
"Towards Conducting Effective Locomotion Through Hardware Transformation
  in Head-Mounted-Device -- A Review Study","Immersiveness is the main characteristic of Virtual Reality(VR) applications.
Precise integration between hardware design and software are necessary for
providing a seamless virtual experience. Allowing the user to navigate the VR
scene using locomotion techniques is crucial for making such experiences
`immersive'. Locomotion in VR acts as a motion tracking unit for the user and
simulates their movement in the virtual scene. These movements are commonly
rotational, axial or translational based on the Degree-of-Freedom (DOF) of the
application. To support effective locomotion, one of the primary challenges for
VR practitioners is to transform their hardware from 3-DOF to 6-DOF or vice
versa. We conducted a systematic review on different motion tracking methods
employed in the Head-Mounted-Devices (HMD) to understand such hardware
transformation. Our review discusses the fundamental aspects of the
hardware-based transformation of HMDs to conduct virtual locomotion. Our
observations led us to formulate a taxonomy of the tracking methods based on
system design, which can eventually be used for the hardware transformation of
HMDs. Our study also captures different metrics that VR practitioners use to
evaluate the hardware based on the context, performance, and significance of
its usage.","['Y Pawan Kumar Gururaj', 'Raghav Mittal', 'Sai Anirudh Karre', 'Y. Raghu Reddy', 'Syed Azeemuddin']",2023-06-25T11:19:57Z,http://arxiv.org/abs/2306.14210v1,"['cs.HC', 'cs.AR']","Virtual Reality,Hardware Transformation,Locomotion Techniques,Motion Tracking,Degree-of-Freedom (DOF),System Design,Tracking Methods,Head-Mounted-Devices (HMD),Taxonomy,Metrics."
"Towards Anatomy Education with Generative AI-based Virtual Assistants in
  Immersive Virtual Reality Environments","Virtual reality (VR) and interactive 3D visualization systems have enhanced
educational experiences and environments, particularly in complicated subjects
such as anatomy education. VR-based systems surpass the potential limitations
of traditional training approaches in facilitating interactive engagement among
students. However, research on embodied virtual assistants that leverage
generative artificial intelligence (AI) and verbal communication in the anatomy
education context is underrepresented. In this work, we introduce a VR
environment with a generative AI-embodied virtual assistant to support
participants in responding to varying cognitive complexity anatomy questions
and enable verbal communication. We assessed the technical efficacy and
usability of the proposed environment in a pilot user study with 16
participants. We conducted a within-subject design for virtual assistant
configuration (avatar- and screen-based), with two levels of cognitive
complexity (knowledge- and analysis-based). The results reveal a significant
difference in the scores obtained from knowledge- and analysis-based questions
in relation to avatar configuration. Moreover, results provide insights into
usability, cognitive task load, and the sense of presence in the proposed
virtual assistant configurations. Our environment and results of the pilot
study offer potential benefits and future research directions beyond medical
education, using generative AI and embodied virtual agents as customized
virtual conversational assistants.","['Vuthea Chheang', 'Shayla Sharmin', 'Rommy Marquez-Hernandez', 'Megha Patel', 'Danush Rajasekaran', 'Gavin Caulfield', 'Behdokht Kiafar', 'Jicheng Li', 'Pinar Kullu', 'Roghayeh Leila Barmaki']",2023-06-29T19:39:28Z,http://arxiv.org/abs/2306.17278v2,['cs.HC'],"Virtual reality,3D visualization,Anatomy education,Generative artificial intelligence,Virtual assistant,Cognitive complexity,User study,Avatar configuration,Usability,Embodied virtual agents"
"2D, 2.5D, or 3D? An Exploratory Study on Multilayer Network
  Visualisations in Virtual Reality","Relational information between different types of entities is often modelled
by a multilayer network (MLN) -- a network with subnetworks represented by
layers. The layers of an MLN can be arranged in different ways in a visual
representation, however, the impact of the arrangement on the readability of
the network is an open question. Therefore, we studied this impact for several
commonly occurring tasks related to MLN analysis. Additionally, layer
arrangements with a dimensionality beyond 2D, which are common in this
scenario, motivate the use of stereoscopic displays. We ran a human subject
study utilising a Virtual Reality headset to evaluate 2D, 2.5D, and 3D layer
arrangements. The study employs six analysis tasks that cover the spectrum of
an MLN task taxonomy, from path finding and pattern identification to
comparisons between and across layers. We found no clear overall winner.
However, we explore the task-to-arrangement space and derive empirical-based
recommendations on the effective use of 2D, 2.5D, and 3D layer arrangements for
MLNs.","['Stefan P. Feyer', 'Bruno Pinaud', 'Stephen G. Kobourov', 'Nicolas Brich', 'Michael Krone', 'Andreas Kerren', 'Michael Behrisch', 'Falk Schreiber', 'Karsten Klein']",2023-07-20T07:55:20Z,http://arxiv.org/abs/2307.10674v2,['cs.HC'],"2D,2.5D,3D,multilayer network,virtual reality,stereoscopic displays,human subject study,network analysis,task taxonomy,layer arrangements"
"Effect of eHMI on pedestrian road crossing behavior in shared space with
  Automated Vehicles-A Virtual Reality study","A shared space area is a low-speed urban area in which pedestrians, cyclists,
and vehicles share the road, often relying on informal interaction rules and
greatly expanding freedom of movement for pedestrians and cyclists. While
shared space has the potential to improve pedestrian priority in urban areas,
it presents unique challenges for pedestrian-AV interaction due to the absence
of a clear right of way. The current study applied Virtual Reality (VR)
experiments to investigate pedestrian-AV interaction in a shared space, with a
particular focus on the impact of external human-machine interfaces (eHMIs) on
pedestrian crossing behavior. Fifty-three participants took part in the VR
experiment and three eHMI conditions were investigated: no eHMI, eHMI with a
pedestrian sign on the windshield, and eHMI with a projected zebra crossing on
the road. Data collected via VR and questionnaires were used for objective and
subjective measures to understand pedestrian-AV interaction. The study revealed
that the presence of eHMI had an impact on participants' gazing behavior but
not on their crossing decisions. Additionally, participants had a positive user
experience with the current VR setting and expressed a high level of trust and
perceived safety during their interaction with the AV. These findings highlight
the potential of utilizing VR to explore and understand pedestrian-AV
interactions.","['Yan Feng', 'Haneen Farah', 'Bart van Arem']",2023-08-10T15:51:02Z,http://arxiv.org/abs/2308.05654v2,['cs.HC'],"shared space,Automated Vehicles,Virtual Reality,external human-machine interfaces,pedestrian crossing behavior,pedestrian-AV interaction,gazing behavior,user experience,perceived safety,trust"
"Feel the Breeze: Promoting Relaxation in Virtual Reality using Mid-Air
  Haptics","Mid-air haptic interfaces employ focused ultrasound waves to generate
touchless haptic sensations on the skin. Prior studies have demonstrated the
potential positive impact of mid-air haptic feedback on virtual experiences,
enhancing aspects such as enjoyment, immersion, and sense of agency. As a
highly immersive environment, Virtual Reality (VR) is being explored as a tool
for stress management and relaxation in current research. However, the impact
of incorporating mid-air haptic stimuli into relaxing experiences in VR has not
been studied thus far. In this paper, for the first time, we design a mid-air
haptic stimulation that is congruent with a relaxing scene in VR, and conduct a
user study investigating the effectiveness of this experience. Our user study
encompasses three different conditions: a control group with no relaxation
intervention, a VR-only relaxation experience, and a VR+Haptics relaxation
experience that includes the mid-air haptic feedback. While we did not find any
significant differences between the conditions, a trend suggesting that the
VR+Haptics condition might be associated with greater pleasure emerged,
requiring further validation with a larger sample size. These initial findings
set the foundation for future investigations into leveraging multimodal
interventions in VR, utilising mid-air haptics to potentially enhance
relaxation experiences.","['Naga Sai Surya Vamsy Malladi', 'Viktorija Paneva', 'Jörg Müller']",2023-08-18T09:45:42Z,http://arxiv.org/abs/2308.09424v1,['cs.HC'],"mid-air haptics,virtual reality,relaxation,haptic feedback,user study,immersive environment,stress management,ultrasound waves,user experience,multimodal interventions"
"Comparing Spatial Navigation and Human Environment Interaction in
  Virtual Reality vs. Identical Real Environments across the Adult Lifespan","Virtual reality (VR) is increasingly being used as a research platform for
investigating human responses to environmental variables. While VR provides
tremendous advantages in terms of variable isolation and manipulation, and ease
of data-collection, some researchers have expressed concerns about the
ecological validity of VR-based findings. In the current study we replicated a
real-world, multi-level educational facility in VR, and compared data collected
in the VR and real-world environments as participants (n=36) completed
identical wayfinding tasks. We found significant differences in all of the
measures used, including distance covered, number of mistakes made, time for
task completion, spatial memory, extent of backtracking, observation of
directional signs, perceived uncertainty levels, perceived cognitive workload,
and perceived task difficulty. We also analyzed potential age-related effects
to look for heightened VR/real response discrepancies among older adult
participants (>55 years) compared to younger adults. This analysis yielded no
significant effects of age. Finally, we examined the spatial distribution of
self-reported wayfinding uncertainty across the building floorplan, finding
that areas in which uncertainty was most pronounced were similar between the
real-world and VR settings. Thus, participants appeared to be responding to the
same environmental features in the real and VR conditions, but the extent of
these responses was significantly different. Overall, the findings suggest that
when VR is used to contrast varying environmental design conditions the
resulting data should be interpreted cautiously and should not be generalized
into real-world conclusions without further validation.","['Saleh Kalantari', 'Bill Tong Xu', 'Armin Mostafavi', 'Anne Seoyoung Lee', 'Qi Yang']",2023-08-30T05:51:09Z,http://arxiv.org/abs/2308.15774v1,['cs.HC'],"Spatial navigation,Human environment interaction,Virtual reality,Real environments,Adult lifespan,Wayfinding tasks,Spatial memory,Cognitive workload,Ecological validity,Age-related effects"
"Immersive Virtual Reality Platform for Robot-Assisted Antenatal
  Ultrasound Scanning","Maternal health remains a pervasive challenge in developing and
underdeveloped countries. Inadequate access to basic antenatal Ultrasound (US)
examinations, limited resources such as primary health services and
infrastructure, and lack of skilled healthcare professionals are the major
concerns. To improve the quality of maternal care, robot-assisted antenatal US
systems with teleoperable and autonomous capabilities were introduced. However,
the existing teleoperation systems rely on standard video stream-based
approaches that are constrained by limited immersion and scene awareness. Also,
there is no prior work on autonomous antenatal robotic US systems that automate
standardized scanning protocols. To that end, this paper introduces a novel
Virtual Reality (VR) platform for robotic antenatal ultrasound, which enables
sonologists to control a robotic arm over a wired network. The effectiveness of
the system is enhanced by providing a reconstructed 3D view of the environment
and immersing the user in a VR space. Also, the system facilitates a better
understanding of the anatomical surfaces to perform pragmatic scans using 3D
models. Further, the proposed robotic system also has autonomous capabilities;
under the supervision of the sonologist, it can perform the standard six-step
approach for obstetric US scanning recommended by the ISUOG. Using a 23-week
fetal phantom, the proposed system was demonstrated to technology and academia
experts at MEDICA 2022 as a part of the KUKA Innovation Award. The positive
feedback from them supports the feasibility of the system. It also gave an
insight into the improvisations to be carried out to make it a clinically
viable system.","['Shyam A', 'Aparna Purayath', 'Keerthivasan S', 'Akash S M', 'Aswathaman Govindaraju', 'Manojkumar Lakshmanan', 'Mohanasankar Sivaprakasam']",2023-09-07T14:12:04Z,http://arxiv.org/abs/2309.03725v1,['cs.RO'],"Immersive Virtual Reality Platform,Robot-Assisted,Antenatal Ultrasound Scanning,Teleoperable,Autonomous Capabilities,Virtual Reality (VR) Platform,Robotic Arm,Wired Network,3D View,Anatomical Surfaces,Obstetric US scanning,Fetal Phantom,KUKA Innovation Award."
"Differentiating Workload using Pilot's Stick Input in a Virtual Reality
  Flight Task","High-risk operational tasks such as those in aviation require training
environments that are realistic and capable of inducing high levels of
workload. Virtual Reality (VR) offers a simulated 3D environment for immersive,
safe and valid training of pilots. An added advantage of such training
environments is that they can be personalized to enhance learning, e.g., by
adapting the simulation to the user's workload in real-time. The question
remains how to reliably and robustly measure a pilot's workload during the
training. In this study, six novice military pilots (average of 34.33 flight
hours) conducted a speed change maneuver in a VR flight simulator. In half of
the runs an auditory 2-back task was added as a secondary task. This led to
trials of low and high workload which we compared using the pilot's control
input in longitudinal (i.e., pitch) and lateral (i.e., roll) directions. We
extracted Pilot Inceptor Workload (PIW) from the stick data and conducted a
binary logistic regression to determine whether PIW is predictive of
task-induced workload. The results show that inputs on the stick along its
longitudinal direction were predictive of workload (low vs. high) when
performing a speed change maneuver. Given that PIW may be a task-specific
measure, future work may consider (neuro)physiological predictors. Nonetheless,
the current paper provides evidence that measuring PIW in a VR flight simulator
yields real-time and non-invasive means to determine workload.","['Evy van Weelden', 'Carl W. E. van Beek', 'Maryam Alimardani', 'Travis J. Wiltshire', 'Wietse D. Ledegang', 'Eric L. Groen', 'Max M. Louwerse']",2023-09-18T09:43:45Z,http://arxiv.org/abs/2309.09619v2,['cs.HC'],"Virtual reality,Workload,Pilot,Flight task,Pilot's stick input,Flight simulator,Task-induced workload,Logistic regression,Neurophysiological predictors,Immersive training"
"Exploring Users' Pointing Performance on Virtual and Physical Large
  Curved Displays","Large curved displays have emerged as a powerful platform for collaboration,
data visualization, and entertainment. These displays provide highly immersive
experiences, a wider field of view, and higher satisfaction levels. Yet, large
curved displays are not commonly available due to their high costs. With the
recent advancement of Head Mounted Displays (HMDs), large curved displays can
be simulated in Virtual Reality (VR) with minimal cost and space requirements.
However, to consider the virtual display as an alternative to the physical
display, it is necessary to uncover user performance differences (e.g.,
pointing speed and accuracy) between these two platforms. In this paper, we
explored users' pointing performance on both physical and virtual large curved
displays. Specifically, with two studies, we investigate users' performance
between the two platforms for standard pointing factors such as target width,
target amplitude as well as users' position relative to the screen. Results
from user studies reveal no significant difference in pointing performance
between the two platforms when users are located at the same position relative
to the screen. In addition, we observe users' pointing performance improves
when they are located at the center of a semi-circular display compared to
off-centered positions. We conclude by outlining design implications for
pointing on large curved virtual displays. These findings show that large
curved virtual displays are a viable alternative to physical displays for
pointing tasks.","['A K M Amanat Ullah', 'William Delamare', 'Khalad Hasan']",2023-10-10T04:48:39Z,http://arxiv.org/abs/2310.06307v1,['cs.HC'],"large curved displays,pointing performance,virtual reality,physical displays,user performance,target width,target amplitude,user studies,design implications"
Training for Open-Ended Drilling through a Virtual Reality Simulation,"Virtual Reality (VR) can support effective and scalable training of
psychomotor skills in manufacturing. However, many industry training modules
offer experiences that are close-ended and do not allow for human error. We aim
to address this gap in VR training tools for psychomotor skills training by
exploring an open-ended approach to the system design. We designed a VR
training simulation prototype to perform open-ended practice of drilling using
a 3-axis milling machine. The simulation employs near ""end-to-end"" instruction
through a safety module, a setup and drilling tutorial, open-ended practice
complete with warnings of mistakes and failures, and a function to assess the
geometries and locations of drilled holes against an engineering drawing. We
developed and conducted a user study within an undergraduate-level introductory
fabrication course to investigate the impact of open-ended VR practice on
learning outcomes. Study results reveal positive trends, with the VR group
successfully completing the machining task of drilling at a higher rate (75% vs
64%), with fewer mistakes (1.75 vs 2.14 score), and in less time (17.67 mins vs
21.57 mins) compared to the control group. We discuss our findings and
limitations and implications for the design of open-ended VR training systems
for learning psychomotor skills.","['Hing Lie', 'Kachina Studer', 'Zhen Zhao', 'Ben Thomson', 'Dishita G Turakhia', 'John Liu']",2023-10-26T14:22:30Z,http://arxiv.org/abs/2310.17417v1,['cs.HC'],"Virtual Reality,Training,Psychomotor skills,Open-ended,Simulation,Drilling,3-axis milling machine,User study,Learning outcomes,VR training systems"
"A Virtual Reality Training System for Automotive Engines Assembly and
  Disassembly","Automotive engine assembly and disassembly are common and crucial programs in
the automotive industry. Traditional education trains students to learn
automotive engine assembly and disassembly in lecture courses and then to
operate with physical engines, which are generally low effectiveness and high
cost. In this work, we developed a multi-layer structured Virtual Reality (VR)
system to provide students with training in automotive engine (Buick Verano)
assembly and disassembly. We designed the VR training system with The VR
training system is designed to have several major features, including
replaceable engine parts and reusable tools, friendly user interfaces and
guidance, and bottom-up designed multi-layer architecture, which can be
extended to various engine models. The VR system is evaluated with controlled
experiments of two groups of students. The results demonstrate that our VR
training system provides remarkable usability in terms of effectiveness and
efficiency. Currently, our VR system has been demonstrated and employed in the
courses of Chinese colleges to train students in automotive engine assembly and
disassembly. A free-to-use executable file (Microsoft Windows) and open-source
code are available at https://github.com/LadissonLai/SUSTech_VREngine for
facilitating the development of VR systems in the automotive industry. Finally,
a video describing the operations in our VR training system is available at
https://www.youtube.com/watch?v=yZe4YTwwAC4","['Gongjin Lan', 'Qiangqiang Lai', 'Bing Bai', 'Zirui Zhao', 'Qi Hao']",2023-11-02T13:37:46Z,http://arxiv.org/abs/2311.02108v1,"['cs.HC', 'cs.AI']","Virtual Reality,Training System,Automotive Engines,Assembly,Disassembly,User Interfaces,Multi-layer Architecture,Controlled Experiments,Usability"
"User Dynamics-Aware Edge Caching and Computing for Mobile Virtual
  Reality","In this paper, we present a novel content caching and delivery approach for
mobile virtual reality (VR) video streaming. The proposed approach aims to
maximize VR video streaming performance, i.e., minimizing video frame missing
rate, by proactively caching popular VR video chunks and adaptively scheduling
computing resources at an edge server based on user and network dynamics.
First, we design a scalable content placement scheme for deciding which video
chunks to cache at the edge server based on tradeoffs between computing and
caching resource consumption. Second, we propose a machine learning-assisted VR
video delivery scheme, which allocates computing resources at the edge server
to satisfy video delivery requests from multiple VR headsets. A Whittle
index-based method is adopted to reduce the video frame missing rate by
identifying network and user dynamics with low signaling overhead. Simulation
results demonstrate that the proposed approach can significantly improve VR
video streaming performance over conventional caching and computing resource
scheduling strategies.","['Mushu Li', 'Jie Gao', 'Conghao Zhou', 'Xuemin Shen', 'Weihua Zhuang']",2023-11-17T17:01:09Z,http://arxiv.org/abs/2311.10645v1,"['eess.SP', 'cs.MM', 'cs.SY', 'eess.SY']","virtual reality,edge caching,edge computing,user dynamics,content caching,video streaming,computing resources,machine learning,Whittle index,simulation"
Collaborative software design and modeling in virtual reality,"Context: Software engineering is becoming more and more distributed.
Developers and other stakeholders are often located in different locations,
departments, and countries and operating within different time zones. Most
online software design and modeling tools are not adequate for distributed
collaboration since they do not support awareness and lack features for
effective communication.
  Objective: The aim of our research is to support distributed software design
activities in Virtual Reality (VR).
  Method: Using design science research methodology, we design and evaluate a
tool for collaborative design in VR. We evaluate the collaboration efficiency
and recall of design information when using the VR software design environment
compared to a non-VR software design environment. Moreover, we collect the
perceptions and preferences of users to explore the opportunities and
challenges that were incurred by using the VR software design environment.
  Results: We find that there is no significant difference in the efficiency
and recall of design information when using the VR compared to the non-VR
environment. Furthermore, we find that developers are more satisfied with
collaboration in VR.
  Conclusion: The results of our research and similar studies show that working
in VR is not yet faster or more efficient than working on standard desktops. It
is very important to improve the interface in VR (gestures with haptics,
keyboard and voice input), as confirmed by the difference in results between
the first and second evaluation.","['Martin Stancek', 'Ivan Polasek', 'Tibor Zalabai', 'Juraj Vincur', 'Rodi Jolak', 'Michel Chaudron']",2023-11-29T16:34:15Z,http://arxiv.org/abs/2311.17787v1,"['cs.SE', 'cs.HC']","Collaborative software design,modeling,virtual reality,distributed collaboration,design science research methodology,VR software design environment,efficiency,recall,interface,haptics"
"Free energy along drug-protein binding pathways interactively sampled in
  virtual reality","We describe a two-step approach for combining interactive molecular dynamics
in virtual reality (iMD-VR) with free energy (FE) calculation to explore the
dynamics of biological processes at the molecular level. We refer to this
combined approach as iMD-VR-FE. Stage one involves using a state-of-the-art
iMD-VR framework to generate a diverse range of protein-ligand unbinding
pathways, benefitting from the sophistication of human spatial and chemical
intuition. Stage two involves using the iMD-VR-sampled pathways as initial
guesses for defining a path-based reaction coordinate from which we can obtain
a corresponding free energy profile using FE methods. To investigate the
performance of the method, we apply iMD-VR-FE to investigate the unbinding of a
benzamidine ligand from a trypsin protein. The binding free energy calculated
using iMD-VR-FE is similar for each pathway, indicating internal consistency.
Moreover, the resulting free energy profiles can distinguish energetic
differences between pathways corresponding to various protein-ligand
conformations (e.g., helping to identify pathways that are more favourable) and
enable identification of metastable states along the pathways. The two-step
iMD-VR-FE approach offers an intuitive way for researchers to test hypotheses
for candidate pathways in biomolecular systems, quickly obtaining both
qualitative and quantitative insight.","['Helen M. Deeks', 'Kirill Zinovjev', 'Jonathan Barnoud', 'Adrian J. Mulholland', 'Marc W. van der Kamp', 'David R. Glowacki']",2023-11-21T13:45:08Z,http://arxiv.org/abs/2311.17925v1,"['physics.chem-ph', 'physics.bio-ph']","molecular dynamics,virtual reality,free energy,protein-ligand binding,reaction coordinate"
"Perceptual Dimensions of Physical Properties of Handheld Objects Induced
  by Impedance Changes","Haptics in virtual reality is the emerging dimension after audiovisual
experiences. Researchers designed several handheld VR controllers to simulate
haptic experiences in virtual reality environments. Some of these devices,
equipped to deliver active force, can dynamically alter the timing and
intensity of force feedback, potentially offering a wide array of haptic
sensations. Past research primarily used a single index to evaluate how users
perceive physical property parameters, potentially limiting the assessment to
the designer's intended scope and neglecting other potential perceptual
experiences.
  Therefore, this study evaluates not how much but how humans feel a physical
property when stimuli are changed. We conducted interviews to investigate how
people feel when a haptic device changes motion impedance. We used thematic
analysis to abstract the results of the interviews and gain an understanding of
how humans attribute force feedback to a phenomenon. We also generated a
vocabulary from the themes obtained from the interviews and asked users to
evaluate force feedback using the semantic difference method. A factor analysis
was used to investigate how changing the basic elements of motion, such as
inertia, viscosity, and stiffness of the motion system, affects haptic
perception. As a result, we obtained four critical factors: size, viscosity,
weight, and flexibility factor, and clarified the correspondence between these
factors and the change of impedance.","['Takeru Hashimoto', 'Shigeo Yoshida', 'Takuji Narumi']",2023-12-04T07:51:23Z,http://arxiv.org/abs/2312.01707v1,['cs.HC'],"Haptics,VR controllers,force feedback,physical properties,impedance changes,thematic analysis,factor analysis"
"VR for Acupuncture? Exploring Needs and Opportunities for Acupuncture
  Training and Treatment in Virtual Reality","Acupuncture is a form of medicine that involves inserting needles into
targeted areas of the body and requires knowledge of both Traditional Chinese
Medicine (TCM) and Evidence-Based Medicine (EBM). The process of acquiring such
knowledge and using it for practical treatment is challenging due to the need
for a deep understanding of human anatomy and the ability to apply both TCM and
EBM approaches. Visual aids have been introduced to aid in understanding the
alignment of acupuncture points with key elements of the human body, and are
indispensable tools for both learners and expert acupuncturists. However, they
are often not enough to enable effective practice and fail to fully support the
learning process. Novel approaches based on immersive visualization and Virtual
Reality (VR) have shown promise in many healthcare settings due to their unique
advantages in terms of realism and interactions, but it is still unknown
whether and how VR can possibly be beneficial to acupuncture training and
treatment. Following participatory design protocols such as observations and
semi-structured interviews with eight doctors and nine students, we explore the
needs and pain points of current acupuncture workflows at the intersection of
EBM and TCM in China and the United States. We highlight opportunities for
introducing VR in today's acupuncture training and treatment workflows, and
discuss two design approaches that build on 11 specific challenges spanning
education, diagnosis, treatment, and communication.","['Menghe Zhang', 'Chen Chen', 'Matin Yarmand', 'Nadir Weibel']",2023-12-12T22:31:28Z,http://arxiv.org/abs/2312.07772v1,"['cs.HC', 'K.3.m; J.3; I.3.8; D.2.10']","Acupuncture,Traditional Chinese Medicine (TCM),Evidence-Based Medicine (EBM),human anatomy,acupuncture points,visual aids,Virtual Reality (VR),immersive visualization,participatory design,healthcare settings"
"ASL Champ!: A Virtual Reality Game with Deep-Learning Driven Sign
  Recognition","We developed an American Sign Language (ASL) learning platform in a Virtual
Reality (VR) environment to facilitate immersive interaction and real-time
feedback for ASL learners. We describe the first game to use an interactive
teaching style in which users learn from a fluent signing avatar and the first
implementation of ASL sign recognition using deep learning within the VR
environment. Advanced motion-capture technology powers an expressive ASL
teaching avatar within an immersive three-dimensional environment. The teacher
demonstrates an ASL sign for an object, prompting the user to copy the sign.
Upon the user's signing, a third-party plugin executes the sign recognition
process alongside a deep learning model. Depending on the accuracy of a user's
sign production, the avatar repeats the sign or introduces a new one. We
gathered a 3D VR ASL dataset from fifteen diverse participants to power the
sign recognition model. The proposed deep learning model's training,
validation, and test accuracy are 90.12%, 89.37%, and 86.66%, respectively. The
functional prototype can teach sign language vocabulary and be successfully
adapted as an interactive ASL learning platform in VR.","['Md Shahinur Alam', 'Jason Lamberton', 'Jianye Wang', 'Carly Leannah', 'Sarah Miller', 'Joseph Palagano', 'Myles de Bastion', 'Heather L. Smith', 'Melissa Malzkuhn', 'Lorna C. Quandt']",2023-12-30T17:55:30Z,http://arxiv.org/abs/2401.00289v1,['cs.HC'],"Virtual Reality,Deep Learning,Sign Recognition,American Sign Language,Motion-Capture Technology,Avatar,Interactive Teaching,Deep Learning Model,ASL Dataset,Immersive Environment"
"Effects of Virtual Hand Representation on the Typing Performance, Upper
  Extremity Angle, and Neck Muscle Activity during Virtual Reality Typing","This study evaluated the effect of virtual hand representation on the typing
performance, upper extremity angle, neck muscle activity, and usability during
virtual reality (VR) typing. A total of 15 participants (7 females and 8 males)
performed a typing task with and without virtual hand representations. The
optical motion capture data was utilized to capture the upper extremity angles,
and electromyography device was used to collect the neck muscle activities
(left and right splenius capitis). The results showed that the typing
performance, upper extremity angle, neck muscle activity, and usability were
significantly different with and without virtual hand representations. With the
virtual hand representation, net typing speed (WPM) and usability increased
significantly by 171.4% and 25% compared to the without hand representation.
Without the virtual hand representation, participants showed increased wrist
extension, and decreased right shoulder abduction angles. The variability of
the neck rotation was increased while typing without the virtual hand
representation. The neck muscle activities were increased with the virtual hand
representation. The results suggest that typing with the virtual hand
representation could positively affect the typing performance and usability and
reduce the risk of the musculoskeletal disorders of the upper extremity. Future
study could explore the effect of the virtual hand representation for users
with varying levels of typing skills.","['Mobasshira Zaman', 'Jaejin Hwang']",2024-01-16T00:09:45Z,http://arxiv.org/abs/2401.08018v1,['cs.HC'],"virtual reality,typing performance,upper extremity angle,neck muscle activity,usability,motion capture,electromyography,wrist extension,shoulder abduction,musculoskeletal disorders"
"Training program on sign language: social inclusion through Virtual
  Reality in ISENSE project","Structured hand gestures that incorporate visual motions and signs are used
in sign language. Sign language is a valuable means of daily communication for
individuals who are deaf or have speech impairments, but it is still rare among
hearing people, and fewer are capable of understand it. Within the academic
context, parents and teachers play a crucial role in supporting deaf students
from childhood by facilitating their learning of sign language. In the last
years, among all the teaching tools useful for learning sign language, the use
of Virtual Reality (VR) has increased, as it has been demonstrated to improve
retention, memory and attention during the learning process. The ISENSE project
has been created to assist students with deafness during their academic life by
proposing different technological tools for teaching sign language to the
hearing community in the academic context. As part of the ISENSE project, this
work aims to develop an application for Spanish and Italian sign language
recognition that exploits the VR environment to quickly and easily create a
comprehensive database of signs and an Artificial Intelligence (AI)-based
software to accurately classify and recognize static and dynamic signs: from
letters to sentences.","['Alessia Bisio', 'Enrique Yeguas-Bolívar', 'Pilar Aparicio-Martínez', 'María Dolores Redel-Macías', 'Sara Pinzi', 'Stefano Rossi', 'Juri Taborri']",2024-01-15T20:40:46Z,http://arxiv.org/abs/2401.08714v1,"['cs.HC', 'cs.AI', 'cs.CV', 'cs.GR']","sign language,Virtual Reality,ISENSE project,hand gestures,deaf students,teaching tools,retention,memory,attention,Artificial Intelligence"
"Using Motion Forecasting for Behavior-Based Virtual Reality (VR)
  Authentication","Task-based behavioral biometric authentication of users interacting in
virtual reality (VR) environments enables seamless continuous authentication by
using only the motion trajectories of the person's body as a unique signature.
Deep learning-based approaches for behavioral biometrics show high accuracy
when using complete or near complete portions of the user trajectory, but show
lower performance when using smaller segments from the start of the task. Thus,
any systems designed with existing techniques are vulnerable while waiting for
future segments of motion trajectories to become available. In this work, we
present the first approach that predicts future user behavior using
Transformer-based forecasting and using the forecasted trajectory to perform
user authentication. Our work leverages the notion that given the current
trajectory of a user in a task-based environment we can predict the future
trajectory of the user as they are unlikely to dramatically shift their
behavior since it would preclude the user from successfully completing their
task goal. Using the publicly available 41-subject ball throwing dataset of
Miller et al. we show improvement in user authentication when using forecasted
data. When compared to no forecasting, our approach reduces the authentication
equal error rate (EER) by an average of 23.85% and a maximum reduction of
36.14%.","['Mingjun Li', 'Natasha Kholgade Banerjee', 'Sean Banerjee']",2024-01-30T00:43:41Z,http://arxiv.org/abs/2401.16649v1,"['cs.LG', 'cs.CR']","Motion forecasting,Behavior-based,Virtual Reality,Authentication,Deep learning,Transformer-based forecasting,User authentication,Trajectory,Task-based behavioral biometric authentication,User behavior"
Anything in Any Scene: Photorealistic Video Object Insertion,"Realistic video simulation has shown significant potential across diverse
applications, from virtual reality to film production. This is particularly
true for scenarios where capturing videos in real-world settings is either
impractical or expensive. Existing approaches in video simulation often fail to
accurately model the lighting environment, represent the object geometry, or
achieve high levels of photorealism. In this paper, we propose Anything in Any
Scene, a novel and generic framework for realistic video simulation that
seamlessly inserts any object into an existing dynamic video with a strong
emphasis on physical realism. Our proposed general framework encompasses three
key processes: 1) integrating a realistic object into a given scene video with
proper placement to ensure geometric realism; 2) estimating the sky and
environmental lighting distribution and simulating realistic shadows to enhance
the light realism; 3) employing a style transfer network that refines the final
video output to maximize photorealism. We experimentally demonstrate that
Anything in Any Scene framework produces simulated videos of great geometric
realism, lighting realism, and photorealism. By significantly mitigating the
challenges associated with video data generation, our framework offers an
efficient and cost-effective solution for acquiring high-quality videos.
Furthermore, its applications extend well beyond video data augmentation,
showing promising potential in virtual reality, video editing, and various
other video-centric applications. Please check our project website
https://anythinginanyscene.github.io for access to our project code and more
high-resolution video results.","['Chen Bai', 'Zeman Shao', 'Guoxiang Zhang', 'Di Liang', 'Jie Yang', 'Zhuorui Zhang', 'Yujian Guo', 'Chengzhang Zhong', 'Yiqiao Qiu', 'Zhendong Wang', 'Yichen Guan', 'Xiaoyin Zheng', 'Tao Wang', 'Cheng Lu']",2024-01-30T23:54:43Z,http://arxiv.org/abs/2401.17509v1,['cs.CV'],"video simulation,photorealistic,object insertion,lighting distribution,style transfer network,geometric realism,physical realism,virtual reality,video editing,high-resolution"
"Experimental Evaluation of Interactive Edge/Cloud Virtual Reality Gaming
  over Wi-Fi using Unity Render Streaming","Virtual Reality (VR) streaming enables end-users to seamlessly immerse
themselves in interactive virtual environments using even low-end devices.
However, the quality of the VR experience heavily relies on Wi-Fi performance,
since it serves as the last hop in the network chain. Our study delves into the
intricate interplay between Wi-Fi and VR traffic, drawing upon empirical data
and leveraging a simulator tailored to VR traffic patterns. In this work we
further evaluate Wi-Fi's suitability for VR streaming in terms of the quality
of service it provides. In particular, we employ Unity Render Streaming to
remotely stream real-time VR gaming content over Wi-Fi 6 using WebRTC,
leveraging a server physically located at the network's edge, near the end
user. Our findings demonstrate the system's sustained network performance,
showcasing minimal round-trip time and jitter at 60 and 90 fps. In addition, we
uncover the characteristics and patterns of the generated traffic streams,
unveiling a surprising video transmission approach inherent to WebRTC-based
services. This approach involves the fragmentation of video frames into
discrete batches of packets, transmitted at regular intervals regardless of the
targeted frame rate.This segmentation mechanism maintains consistent video
packet delays across video frame rates but leads to increased Wi-Fi airtime
consumption at higher frame rates. The presented results demonstrate that
shortening the interval between batches is advantageous as it improves Wi-Fi
efficiency and reduces delays in delivering complete frames.","['Miguel Casasnovas', 'Costas Michaelides', 'Marc Carrascosa-Zamacois', 'Boris Bellalta']",2024-02-01T12:06:41Z,http://arxiv.org/abs/2402.00540v1,['cs.NI'],"Interactive,Virtual Reality,Wi-Fi,Unity Render Streaming,VR traffic,Quality of Service,WebRTC,Edge,Network performance"
"Determining the Difficulties of Students With Dyslexia via Virtual
  Reality and Artificial Intelligence: An Exploratory Analysis","Learning disorders are neurological conditions that affect the brain's
ability to interconnect communication areas. Dyslexic students experience
problems with reading, memorizing, and exposing concepts; however the magnitude
of these can be mitigated through both therapies and the creation of
compensatory mechanisms. Several efforts have been made to mitigate these
issues, leading to the creation of digital resources for students with specific
learning disorders attending primary and secondary education levels.
Conversely, a standard approach is still missed in higher education. The
VRAIlexia project has been created to tackle this issue by proposing two
different tools: a mobile application integrating virtual reality (VR) to
collect data quickly and easily, and an artificial intelligencebased software
(AI) to analyze the collected data for customizing the supporting methodology
for each student. The first one has been created and is being distributed among
dyslexic students in Higher Education Institutions, for the conduction of
specific psychological and psychometric tests. The second tool applies specific
artificial intelligence algorithms to the data gathered via the application and
other surveys. These AI techniques have allowed us to identify the most
relevant difficulties faced by the students' cohort. Our different models have
obtained around 90\% mean accuracy for predicting the support tools and
learning strategies.","['Enrique Yeguas-Bolívar', 'José M. Alcalde-Llergo', 'Pilar Aparicio-Martínez', 'Juri Taborri', 'Andrea Zingoni', 'Sara Pinzi']",2024-01-15T20:26:09Z,http://arxiv.org/abs/2402.01668v1,"['cs.CY', 'cs.AI', 'cs.CV', 'cs.GR', 'cs.HC']","Virtual reality,Artificial intelligence,Dyslexia,Learning disorders,Neurological conditions,Compensatory mechanisms,Digital resources,Higher education,VRAIlexia project"
"Human-guided Swarms: Impedance Control-inspired Influence in Virtual
  Reality Environments","Prior works in human-swarm interaction (HSI) have sought to guide swarm
behavior towards established objectives, but may be unable to handle specific
scenarios that require finer human supervision, variable autonomy, or
application to large-scale swarms. In this paper, we present an approach that
enables human supervisors to tune the level of swarm control, and guide a large
swarm using an assistive control mechanism that does not significantly restrict
emergent swarm behaviors. We develop this approach in a virtual reality (VR)
environment, using the HTC Vive and Unreal Engine 4 with AirSim plugin. The
novel combination of an impedance control-inspired influence mechanism and a VR
test bed enables and facilitates the rapid design and test iterations to
examine trade-offs between swarming behavior and macroscopic-scale human
influence, while circumventing flight duration limitations associated with
battery-powered small unmanned aerial system (sUAS) systems. The impedance
control-inspired mechanism was tested by a human supervisor to guide a virtual
swarm consisting of 16 sUAS agents. Each test involved moving the swarm's
center of mass through narrow canyons, which were not feasible for a swarm to
traverse autonomously. Results demonstrate that integration of the influence
mechanism enabled the successful manipulation of the macro-scale behavior of
the swarm towards task completion, while maintaining the innate swarming
behavior.","['Spencer Barclay', 'Kshitij Jerath']",2024-02-06T22:41:29Z,http://arxiv.org/abs/2402.04451v1,"['cs.RO', 'cs.MA']","human-swarm interaction,impedance control,virtual reality,HTC Vive,Unreal Engine 4,AirSim plugin,large-scale swarms,autonomous systems,unmanned aerial systems,swarm behavior"
"Comparative Analysis of Kinect-Based and Oculus-Based Gaze Region
  Estimation Methods in a Driving Simulator","Driver's gaze information can be crucial in driving research because of its
relation to driver attention. Particularly, the inclusion of gaze data in
driving simulators broadens the scope of research studies as they can relate
drivers' gaze patterns to their features and performance. In this paper, we
present two gaze region estimation modules integrated in a driving simulator.
One uses the 3D Kinect device and another uses the virtual reality Oculus Rift
device. The modules are able to detect the region, out of seven in which the
driving scene was divided, where a driver is gazing at in every route processed
frame. Four methods were implemented and compared for gaze estimation, which
learn the relation between gaze displacement and head movement. Two are simpler
and based on points that try to capture this relation and two are based on
classifiers such as MLP and SVM. Experiments were carried out with 12 users
that drove on the same scenario twice, each one with a different visualization
display, first with a big screen and later with Oculus Rift. On the whole,
Oculus Rift outperformed Kinect as the best hardware for gaze estimation. The
Oculus-based gaze region estimation method with the highest performance
achieved an accuracy of 97.94%. The information provided by the Oculus Rift
module enriches the driving simulator data and makes it possible a multimodal
driving performance analysis apart from the immersion and realism obtained with
the virtual reality experience provided by Oculus.","['David González-Ortega', 'Francisco Javier Díaz-Perna', 'Mario Martínez-Zarzuela', 'Míriam Antón-Rodríguez']",2024-02-04T18:02:58Z,http://arxiv.org/abs/2402.05248v1,['cs.CV'],"Kinect-based,Oculus-based,gaze region estimation,driving simulator,3D Kinect device,virtual reality,gaze displacement,head movement,MLP,SVM,Oculus Rift."
"Privacy-Preserving Gaze Data Streaming in Immersive Interactive Virtual
  Reality: Robustness and User Experience","Eye tracking is routinely being incorporated into virtual reality (VR)
systems. Prior research has shown that eye tracking data, if exposed, can be
used for re-identification attacks. The state of our knowledge about currently
existing privacy mechanisms is limited to privacy-utility trade-off curves
based on data-centric metrics of utility, such as prediction error, and
black-box threat models. We propose that for interactive VR applications, it is
essential to consider user-centric notions of utility and a variety of threat
models. We develop a methodology to evaluate real-time privacy mechanisms for
interactive VR applications that incorporate subjective user experience and
task performance metrics. We evaluate selected privacy mechanisms using this
methodology and find that re-identification accuracy can be decreased to as low
as 14% while maintaining a high usability score and reasonable task
performance. Finally, we elucidate three threat scenarios (black-box, black-box
with exemplars, and white-box) and assess how well the different privacy
mechanisms hold up to these adversarial scenarios.
  This work advances the state of the art in VR privacy by providing a
methodology for end-to-end assessment of the risk of re-identification attacks
and potential mitigating solutions.","['Ethan Wilson', 'Azim Ibragimov', 'Michael J. Proulx', 'Sai Deep Tetali', 'Kevin Butler', 'Eakta Jain']",2024-02-12T14:53:12Z,http://arxiv.org/abs/2402.07687v2,"['cs.HC', 'cs.CR']","Privacy-Preserving,Gaze Data Streaming,Virtual Reality,Eye Tracking,Re-Identification Attacks,Privacy Mechanisms,Threat Models,User Experience,Task Performance,Privacy Mechanisms"
"Applied User Research in Virtual Reality: Tools, Methods, and Challenges","This chapter explores the practice of conducting user research studies and
design assessments in virtual reality (VR). An overview of key VR hardware and
software tools is provided, including game engines, such as Unity and Unreal
Engine. Qualitative and quantitative research methods, along with their various
synergies with VR, are likewise discussed, and some of the challenges
associated with VR, such as limited sensory stimulation, are reflected upon. VR
is proving particularly useful in the context of space systems development,
where its utilisation offers a cost-effective and secure method for simulating
extraterrestrial environments, allowing for rapid prototyping and evaluation of
innovative concepts under representative operational conditions. To illustrate
this, we present a case study detailing the application of VR to aid aerospace
engineers testing their ideas with end-users and stakeholders during early
design stages of the European Space Agency's (ESA) prospective Argonaut lunar
lander. This case study demonstrates the effectiveness of VR simulations in
gathering important feedback concerning the operability of the Argonaut lander
in poor lighting conditions as well as surfacing relevant ergonomics
considerations and constraints. The chapter concludes by discussing the
strengths and weaknesses associated with VR-based user studies and proposes
future research directions, emphasising the necessity for novel VR interfaces
to overcome existing technical limitations.","['Leonie Bensch', 'Andrea Casini', 'Aidan Cowley', 'Florian Dufresne', 'Enrico Guerra', 'Paul de Medeiros', 'Tommy Nilsson', 'Flavie Rometsch', 'Andreas Treuer', 'Anna Vock']",2024-02-24T02:55:31Z,http://arxiv.org/abs/2402.15695v1,"['cs.HC', 'cs.MM', '93B51, 97M50', 'H.1.2; I.3.8; J.4; J.m; K.8.2; J.6']","user research,virtual reality,tools,methods,challenges,VR hardware,software,game engines,qualitative research,quantitative research"
"Virtual Reality for Understanding Artificial-Intelligence-driven
  Scientific Discovery with an Application in Quantum Optics","Generative Artificial Intelligence (AI) models can propose solutions to
scientific problems beyond human capability. To truly make conceptual
contributions, researchers need to be capable of understanding the AI-generated
structures and extracting the underlying concepts and ideas. When algorithms
provide little explanatory reasoning alongside the output, scientists have to
reverse-engineer the fundamental insights behind proposals based solely on
examples. This task can be challenging as the output is often highly complex
and thus not immediately accessible to humans. In this work we show how
transferring part of the analysis process into an immersive Virtual Reality
(VR) environment can assist researchers in developing an understanding of
AI-generated solutions. We demonstrate the usefulness of VR in finding
interpretable configurations of abstract graphs, representing Quantum Optics
experiments. Thereby, we can manually discover new generalizations of
AI-discoveries as well as new understanding in experimental quantum optics.
Furthermore, it allows us to customize the search space in an informed way - as
a human-in-the-loop - to achieve significantly faster subsequent discovery
iterations. As concrete examples, with this technology, we discover a new
resource-efficient 3-dimensional entanglement swapping scheme, as well as a
3-dimensional 4-particle Greenberger-Horne-Zeilinger-state analyzer. Our
results show the potential of VR for increasing a human researcher's ability to
derive knowledge from graph-based generative AI that, which is a common
abstract data representation used in diverse fields of science.","['Philipp Schmidt', 'Sören Arlt', 'Carlos Ruiz-Gonzalez', 'Xuemei Gu', 'Carla Rodríguez', 'Mario Krenn']",2024-02-20T17:48:01Z,http://arxiv.org/abs/2403.00834v1,"['cs.HC', 'cs.AI', 'cs.GR', 'quant-ph']","Virtual Reality,Artificial Intelligence,Scientific Discovery,Quantum Optics,Generative Models,Graphs,Entanglement Swapping,Greenberger-Horne-Zeilinger-state,Data Representation"
"LightSword: A Customized Virtual Reality Exergame for Long-Term
  Cognitive Inhibition Training in Older Adults","The decline of cognitive inhibition significantly impacts older adults'
quality of life and well-being, making it a vital public health problem in
today's aging society. Previous research has demonstrated that Virtual reality
(VR) exergames have great potential to enhance cognitive inhibition among older
adults. However, existing commercial VR exergames were unsuitable for older
adults' long-term cognitive training due to the inappropriate cognitive
activation paradigm, unnecessary complexity, and unbefitting difficulty levels.
To bridge these gaps, we developed a customized VR cognitive training exergame
(LightSword) based on Dual-task and Stroop paradigms for long-term cognitive
inhibition training among healthy older adults. Subsequently, we conducted an
eight-month longitudinal user study with 12 older adults aged 60 years and
above to demonstrate the effectiveness of LightSword in improving cognitive
inhibition. After the training, the cognitive inhibition abilities of older
adults were significantly enhanced, with benefits persisting for 6 months. This
result indicated that LightSword has both short-term and long-term effects in
enhancing cognitive inhibition. Furthermore, qualitative feedback revealed that
older adults exhibited a positive attitude toward long-term training with
LightSword, which enhanced their motivation and compliance.","['Qiuxin Du', 'Zhen Song', 'Haiyan Jiang', 'Xiaoying Wei', 'Dongdong Weng', 'Mingming Fan']",2024-03-08T04:23:08Z,http://arxiv.org/abs/2403.05031v1,['cs.HC'],"Virtual reality,Exergame,Cognitive inhibition,Older adults,Long-term training,Dual-task paradigm,Stroop paradigm,User study,Cognitive enhancement,Motivation"
"The Value of Extended Reality Techniques to Improve Remote Collaborative
  Maintenance Operations: A User Study","In the Architecture, Engineering and Construction (AEC) sector, data
extracted from building information modelling (BIM) can be used to create a
digital twin (DT). The algorithms of a BIM-based DT can facilitate the
retrieval of information, which can then be used to improve building operation
and maintenance procedures. However, with the increased complexity and
automation of the building, maintenance operations are likely to become more
complex and may require expert intervention. Collaboration and interaction
between the operator and the expert may be limited as the latter may not be on
site or within the company. Recently, extended reality (XR) technologies have
proven to be effective in improving collaboration during maintenance
operations,through data display and shared interactions. This paper presents a
new collaborative solution using these technologies to enhance collaboration
during remote maintenance operations. The proposed approach consists of a mixed
reality (MR) set-up for the operator, a virtual reality (VR) set-up for the
remote expert and a shared Digital Model of a heat exchanger. The MR set-up is
used for tracking and displaying specific information, provided by the VR
module. A user study was carried out to compare the efficiency of our solution
with a standard audio-video collaboration. Our approach demonstrated
substantial enhancements in collaborative inspection, resulting in a
significative reduction in both the overall completion time of the inspection
and the frequency of errors committed by the operators.","['Corentin Coupry', 'Paul Richard', 'David Bigaud', 'Sylvain Noblecourt', 'David Baudry']",2024-02-29T12:28:40Z,http://arxiv.org/abs/2403.05580v1,"['cs.HC', 'cs.GR']","Extended Reality Techniques,Remote Collaborative Maintenance Operations,Building Information Modelling,Digital Twin,Maintenance Procedures,Collaboration,Interaction,Extended Reality Technologies,Mixed Reality,Virtual Reality"
"Understanding Parents' Perceptions and Practices Toward Children's
  Security and Privacy in Virtual Reality","Recent years have seen a sharp increase in the number of underage users in
virtual reality (VR), where security and privacy (S\&P) risks such as data
surveillance and self-disclosure in social interaction have been increasingly
prominent. Prior work shows children largely rely on parents to mitigate S\&P
risks in their technology use. Therefore, understanding parents' S\&P
knowledge, perceptions, and practices is critical for identifying the gaps for
parents, technology designers, and policymakers to enhance children's S\&P.
While such empirical knowledge is substantial in other consumer technologies,
it remains largely unknown in the context of VR. To address the gap, we
conducted in-depth semi-structured interviews with 20 parents of children under
the age of 18 who use VR at home. Our findings highlight parents generally lack
S\&P awareness due to the perception that VR is still in its infancy. To
protect their children's interactions with VR, parents currently primarily rely
on active strategies such as verbal education about S\&P. Passive strategies
such as using parental controls in VR are not commonly used among our
interviewees, mainly due to their perceived technical constraints. Parents also
highlight that a multi-stakeholder ecosystem must be established towards more
S\&P support for children in VR. Based on the findings, we propose actionable
S\&P recommendations for critical stakeholders, including parents, educators,
VR companies, and governments.","['Jiaxun Cao', 'Abhinaya S. B.', 'Anupam Das', 'Pardis Emami-Naeini']",2024-03-10T10:54:44Z,http://arxiv.org/abs/2403.06172v2,['cs.HC'],"underage users,virtual reality,security and privacy,data surveillance,social interaction,parental controls,technology designers,policymakers,parental education,multi-stakeholder ecosystem"
"Ergonomic Optimization in Worker-Robot Bimanual Object Handover:
  Implementing REBA Using Reinforcement Learning in Virtual Reality","Robots can serve as safety catalysts on construction job sites by taking over
hazardous and repetitive tasks while alleviating the risks associated with
existing manual workflows. Research on the safety of physical human-robot
interaction (pHRI) is traditionally focused on addressing the risks associated
with potential collisions. However, it is equally important to ensure that the
workflows involving a collaborative robot are inherently safe, even though they
may not result in an accident. For example, pHRI may require the human
counterpart to use non-ergonomic body postures to conform to the robot hardware
and physical configurations. Frequent and long-term exposure to such situations
may result in chronic health issues. Safety and ergonomics assessment measures
can be understood by robots if they are presented in algorithmic fashions so
optimization for body postures is attainable. While frameworks such as Rapid
Entire Body Assessment (REBA) have been an industry standard for many decades,
they lack a rigorous mathematical structure which poses challenges in using
them immediately for pHRI safety optimization purposes. Furthermore, learnable
approaches have limited robustness outside of their training data, reducing
generalizability. In this paper, we propose a novel framework that approaches
optimization through Reinforcement Learning, ensuring precise, online ergonomic
scores as compared to approximations, while being able to generalize and tune
the regiment to any human and any task. To ensure practicality, the training is
done in virtual reality utilizing Inverse Kinematics to simulate human movement
mechanics. Experimental findings are compared to ergonomically naive object
handover heuristics and indicate promising results where the developed
framework can find the optimal object handover coordinates in pHRI contexts for
manual material handling exemplary situations.","['Mani Amani', 'Reza Akhavian']",2024-03-18T18:03:08Z,http://arxiv.org/abs/2403.12149v1,['cs.RO'],"Ergonomics,Worker-Robot Interaction,REBA,Reinforcement Learning,Virtual Reality,Safety,Human-Robot Interaction,Inverse Kinematics,Optimization,Object Handover"
"Effects of Multisensory Feedback on the Perception and Performance of
  Virtual Reality Hand-Retargeted Interaction","Retargeting methods that modify the visual representation of real movements
have been widely used to expand the interaction space and create engaging
virtual reality experiences. For optimal user experience and performance, it is
essential to specify the perception of retargeting and utilize the appropriate
range of modification parameters. However, previous studies mostly concentrated
on whether users perceived the target sense or not and rarely examined the
perceptual accuracy and sensitivity to retargeting. Moreover, it is unknown how
the perception and performance in hand-retargeted interactions are influenced
by multisensory feedback. In this study, we used rigorous psychophysical
methods to specify users' perceptual accuracy and sensitivity to
hand-retargeting and provide acceptable ranges of retargeting parameters. We
also presented different multisensory feedback simultaneously with the
retargeting to probe its effect on users' perception and task performance. The
experimental results showed that providing continuous multisensory feedback,
proportionate to the distance between the virtual hand and the targeted
destination, heightened the accuracy of users' perception of hand retargeting
without altering their perceptual sensitivity. Furthermore, the utilization of
multisensory feedback considerably improved the precision of task performance,
particularly at lower gain factors. Based on these findings, we propose design
guidelines and potential applications of VR hand-retargeted interactions and
multisensory feedback for optimal user experience and performance.","['Hyunyoung Jang', 'Jinwook Kim', 'Jeongmi Lee']",2024-04-05T05:44:11Z,http://arxiv.org/abs/2404.03899v1,['cs.HC'],"Virtual Reality,Multisensory Feedback,Hand-Retargeted Interaction,Perception,Performance,Psychophysical Methods,Retargeting Parameters,Multisensory Integration,Task Performance,Design Guidelines"
"Exploring Physiological Responses in Virtual Reality-based Interventions
  for Autism Spectrum Disorder: A Data-Driven Investigation","Virtual Reality (VR) has emerged as a promising tool for enhancing social
skills and emotional well-being in individuals with Autism Spectrum Disorder
(ASD). Through a technical exploration, this study employs a multiplayer
serious gaming environment within VR, engaging 34 individuals diagnosed with
ASD and employing high-precision biosensors for a comprehensive view of the
participants' arousal and responses during the VR sessions. Participants were
subjected to a series of 3 virtual scenarios designed in collaboration with
stakeholders and clinical experts to promote socio-cognitive skills and
emotional regulation in a controlled and structured virtual environment. We
combined the framework with wearable non-invasive sensors for bio-signal
acquisition, focusing on the collection of heart rate variability, and
respiratory patterns to monitor participants behaviors. Further, behavioral
assessments were conducted using observation and semi-structured interviews,
with the data analyzed in conjunction with physiological measures to identify
correlations and explore digital-intervention efficacy. Preliminary analysis
revealed significant correlations between physiological responses and
behavioral outcomes, indicating the potential of physiological feedback to
enhance VR-based interventions for ASD. The study demonstrated the feasibility
of using real-time data to adapt virtual scenarios, suggesting a promising
avenue to support personalized therapy. The integration of quantitative
physiological feedback into digital platforms represents a forward step in the
personalized intervention for ASD. By leveraging real-time data to adjust
therapeutic content, this approach promises to enhance the efficacy and
engagement of digital-based therapies.","['Gianpaolo Alvari', 'Ersilia Vallefuoco', 'Melanie Cristofolini', 'Elio Salvadori', 'Marco Dianti', 'Alessia Moltani', 'Davide Dal Castello', 'Paola Venuti', 'Cesare Furlanello']",2024-04-10T16:50:07Z,http://arxiv.org/abs/2404.07159v1,"['cs.HC', 'cs.LG', '92C30 (Primary) 92C55, 68T99 (Secondary)']","Virtual Reality,Autism Spectrum Disorder,Physiological Responses,Data-Driven Investigation,Biosensors,Heart Rate Variability,Respiratory Patterns,Behavioral Assessments,Physiological Feedback,Digital Interventions"
"Comparison of On-Orbit Manual Attitude Control Methods for Non-Docking
  Spacecraft Through Virtual Reality Simulation","On-orbit manual attitude control of manned spacecraft is accomplished using
external visual references and some method of three axis attitude control. All
past, present, and developmental spacecraft feature the capability to manually
control attitude for deorbit. National Aeronautics and Space Administration
(NASA) spacecraft permit an aircraft windshield type front view, wherein an arc
of the Earths horizon is visible to the crew in deorbit attitude. Russian and
Chinese spacecraft permit the crew a bottom view wherein the entire circular
Earth horizon disk is visible to the crew in deorbit attitude. Our study
compared these two types of external views for efficiency in achievement of
deorbit attitude. We used a Unity Virtual Reality (VR) spacecraft simulator
that we built in house. The task was to accurately achieve deorbit attitude
while in a 400 km circular orbit. Six military test pilots and six civilians
with gaming experience flew the task using two methods of visual reference.
Comparison was based on time taken, fuel consumed, cognitive workload
assessment and user preference. We used ocular parameters, EEG, NASA TLX and
IBM SUS to quantify our results. Our study found that the bottom view was
easier to operate for manual deorbit task. Additionally, we realized that a VR
based system can work as a training simulator for manual on-orbit flight path
control tasks by pilots and non pilots. Results from our study can be used for
design of manual on orbit attitude control of present and future spacecrafts.","['Ajit Krishnan', 'Himanshu Vishwakarma', 'Maharudra Kharsade', 'Pradipta Biswas']",2024-04-22T07:19:27Z,http://arxiv.org/abs/2404.13933v1,"['cs.HC', 'H.5.2']","manual attitude control,spacecraft,virtual reality simulation,visual references,deorbit attitude,Unity Virtual Reality (VR),military test pilots,cognitive workload assessment,user preference,on-orbit flight path control"
"Shared Boundary Interfaces: can one fit all? A controlled study on
  virtual reality vs touch-screen interfaces on persons with Neurodevelopmental
  Disorders","Technology presents a significant educational opportunity, particularly in
enhancing emotional engagement and expanding learning and educational prospects
for individuals with Neurodevelopmental Disorders (NDD). Virtual reality
emerges as a promising tool for addressing such disorders, complemented by
numerous touchscreen applications that have shown efficacy in fostering
education and learning abilities. VR and touchscreen technologies represent
diverse interface modalities. This study primarily investigates which
interface, VR or touchscreen, more effectively facilitates food education for
individuals with NDD. We compared learning outcomes via pre- and post-exposure
questionnaires. To this end, we developed GEA, a dual-interface, user-friendly
web application for Food Education, adaptable for either immersive use in a
head-mounted display (HMD) or non-immersive use on a tablet. A controlled study
was conducted to determine which interface better promotes learning. Over three
sessions, the experimental group engaged with all GEA games in VR (condition
A), while the control group interacted with the same games on a tablet
(condition B). Results indicated a significant increase in post-questionnaire
scores across subjects, averaging a 46% improvement. This enhancement was
notably consistent between groups, with VR and Tablet groups showing 42% and
41% improvements, respectively.","['Francesco Vona', 'Eleonora Beccaluva', 'Marco Mores', 'Franca Garzotto']",2024-04-24T16:43:54Z,http://arxiv.org/abs/2404.15970v1,['cs.HC'],"Shared Boundary Interfaces,Virtual reality,Touch-screen interfaces,Neurodevelopmental Disorders,Technology,Educational,Learning,GEA,Food Education"
"Self-Avatar Animation in Virtual Reality: Impact of Motion Signals
  Artifacts on the Full-Body Pose Reconstruction","Virtual Reality (VR) applications have revolutionized user experiences by
immersing individuals in interactive 3D environments. These environments find
applications in numerous fields, including healthcare, education, or
architecture. A significant aspect of VR is the inclusion of self-avatars,
representing users within the virtual world, which enhances interaction and
embodiment. However, generating lifelike full-body self-avatar animations
remains challenging, particularly in consumer-grade VR systems, where
lower-body tracking is often absent. One method to tackle this problem is by
providing an external source of motion information that includes lower body
information such as full Cartesian positions estimated from RGB(D) cameras.
Nevertheless, the limitations of these systems are multiples: the
desynchronization between the two motion sources and occlusions are examples of
significant issues that hinder the implementations of such systems. In this
paper, we aim to measure the impact on the reconstruction of the articulated
self-avatar's full-body pose of (1) the latency between the VR motion features
and estimated positions, (2) the data acquisition rate, (3) occlusions, and (4)
the inaccuracy of the position estimation algorithm. In addition, we analyze
the motion reconstruction errors using ground truth and 3D Cartesian
coordinates estimated from \textit{YOLOv8} pose estimation. These analyzes show
that the studied methods are significantly sensitive to any degradation tested,
especially regarding the velocity reconstruction error.","['Antoine Maiorca', 'Seyed Abolfazl Ghasemzadeh', 'Thierry Ravet', 'François Cresson', 'Thierry Dutoit', 'Christophe De Vleeschouwer']",2024-04-29T12:02:06Z,http://arxiv.org/abs/2404.18628v1,['cs.CV'],"Virtual Reality,Self-avatar Animation,Motion Signals,Full-body Pose Reconstruction,Lower-body Tracking,Motion Information,Cartesian Positions,Occlusions,Latency,Pose Estimation"
"Learning High-Quality Navigation and Zooming on Omnidirectional Images
  in Virtual Reality","Viewing omnidirectional images (ODIs) in virtual reality (VR) represents a
novel form of media that provides immersive experiences for users to navigate
and interact with digital content. Nonetheless, this sense of immersion can be
greatly compromised by a blur effect that masks details and hampers the user's
ability to engage with objects of interest. In this paper, we present a novel
system, called OmniVR, designed to enhance visual clarity during VR navigation.
Our system enables users to effortlessly locate and zoom in on the objects of
interest in VR. It captures user commands for navigation and zoom, converting
these inputs into parameters for the Mobius transformation matrix. Leveraging
these parameters, the ODI is refined using a learning-based algorithm. The
resultant ODI is presented within the VR media, effectively reducing blur and
increasing user engagement. To verify the effectiveness of our system, we first
evaluate our algorithm with state-of-the-art methods on public datasets, which
achieves the best performance. Furthermore, we undertake a comprehensive user
study to evaluate viewer experiences across diverse scenarios and to gather
their qualitative feedback from multiple perspectives. The outcomes reveal that
our system enhances user engagement by improving the viewers' recognition,
reducing discomfort, and improving the overall immersive experience. Our system
makes the navigation and zoom more user-friendly.","['Zidong Cao', 'Zhan Wang', 'Yexin Liu', 'Yan-Pei Cao', 'Ying Shan', 'Wei Zeng', 'Lin Wang']",2024-05-01T07:08:24Z,http://arxiv.org/abs/2405.00351v1,"['cs.HC', 'cs.AI', 'cs.CV', 'cs.MM']","omnidirectional images,virtual reality,navigation,zooming,immersive experiences,Mobius transformation matrix,learning-based algorithm,user study,user engagement"
"Fostering Inclusive Virtual Reality Environments: Discussing Strategies
  for Promoting Group Dynamics and Mitigating Harassment","The rapid evolution of social Virtual Reality (VR) platforms has
significantly enhanced the way users interact and socialize in digital spaces,
offering immersive experiences that closely mimic real-world interactions [1].
However, this technological advancement has brought new challenges,
particularly in ensuring safety and preventing harassment [11]. Unlike
traditional social media platforms, the immersive nature of social VR
applications can intensify the impact of harassment, affecting users' emotions,
experiences, and reactions at both mental and physical levels [2, 9].
  Group dynamics can play a pivotal role in both preventing and mitigating
harassment [9]. Literature in group dynamics has provided insights into
fostering and nurturing communities, with special emphasis on how enabling
leadership, coordination, and cohesion among individuals can enable inclusive
and safer social spaces [4]. For this workshop, we propose to discuss
group-centric approaches to address harassment in social VR. We will discuss
group strategies such as matching, interactions, and reporting mechanisms aimed
at promoting safer and more supportive social VR spaces. By leveraging the
social structures among users, we aim to empower users and communities to
collectively counteract harassment and ensure a positive social experience for
all users.","['Niloofar Sayadi', 'Diego Gómez-Zará']",2024-04-23T17:45:44Z,http://arxiv.org/abs/2405.05917v1,['cs.HC'],"Virtual Reality,Group Dynamics,Harassment,Social Spaces,Immersive Experiences,Social VR,Safety,Communities,Leadership,Reporting Mechanisms"
"Exploring Proactive Interventions toward Harmful Behavior in Embodied
  Virtual Spaces","Technological advancements have undoubtedly revolutionized various aspects of
human life, altering the ways we perceive the world, engage with others, build
relationships, and conduct our daily work routines. Among the recent
advancements, the proliferation of virtual and mixed reality technologies
stands out as a significant leap forward, promising to elevate our experiences
and interactions to unprecedented levels. However, alongside the benefits,
these emerging technologies also introduce novel avenues for harm and misuse,
particularly in virtual and embodied spaces such as Zoom and virtual reality
(VR) environments.
  The immersive nature of virtual reality environments raises unique challenges
regarding psychological and emotional well-being. While VR can offer
captivating and immersive experiences, prolonged exposure to virtual
environments may lead to phenomena like cybersickness, disorientation, and even
psychological distress in susceptible individuals. Additionally, the blurring
of boundaries between virtual and real-world interactions in VR raises ethical
concerns regarding consent, harassment, and the potential for virtual
experiences to influence real-life behavior. Additionally, the increasing
integration of artificial intelligence (AI) and machine learning algorithms in
virtual spaces introduces risks related to algorithmic bias, discrimination,
and manipulation. In VR environments, AI-driven systems may inadvertently
perpetuate stereotypes, amplify inequalities, or manipulate user behavior
through personalized content recommendations and targeted advertising, posing
ethical dilemmas and societal risks.",['Ruchi Panchanadikar'],2024-04-23T17:38:27Z,http://arxiv.org/abs/2405.05920v1,['cs.HC'],"proactive interventions,harmful behavior,embodied,virtual spaces,virtual reality,mixed reality,immersive experiences,psychological well-being,artificial intelligence,machine learning algorithms"
"Enabling Additive Manufacturing Part Inspection of Digital Twins via
  Collaborative Virtual Reality","Digital twins (DTs) are an emerging capability in additive manufacturing
(AM), set to revolutionize design optimization, inspection, in situ monitoring,
and root cause analysis. AM DTs typically incorporate multimodal data streams,
ranging from machine toolpaths and in-process imaging to X-ray CT scans and
performance metrics. Despite the evolution of DT platforms, challenges remain
in effectively inspecting them for actionable insights, either individually or
in a multidisciplinary team setting. Quality assurance, manufacturing
departments, pilot labs, and plant operations must collaborate closely to
reliably produce parts at scale. This is particularly crucial in AM where
complex structures require a collaborative and multidisciplinary approach.
Additionally, the large-scale data originating from different modalities and
their inherent 3D nature pose significant hurdles for traditional 2D
desktop-based inspection methods. To address these challenges and increase the
value proposition of DTs, we introduce a novel virtual reality (VR) framework
to facilitate collaborative and real-time inspection of DTs in AM. This
framework includes advanced features for intuitive alignment and visualization
of multimodal data, visual occlusion management, streaming large-scale
volumetric data, and collaborative tools, substantially improving the
inspection of AM components and processes to fully exploit the potential of DTs
in AM.","['Vuthea Chheang', 'Saurabh Narain', 'Garrett Hooten', 'Robert Cerda', 'Brian Au', 'Brian Weston', 'Brian Giera', 'Peer-Timo Bremer', 'Haichao Miao']",2024-05-21T16:59:21Z,http://arxiv.org/abs/2405.12931v1,['cs.HC'],"additive manufacturing,part inspection,digital twins,collaborative virtual reality,multimodal data,X-ray CT scans,performance metrics,quality assurance,virtual reality framework"
"A Machine Learning Approach for Predicting Upper Limb Motion Intentions
  with Multimodal Data in Virtual Reality","Over the last decade, there has been significant progress in the field of
interactive virtual rehabilitation. Physical therapy (PT) stands as a highly
effective approach for enhancing physical impairments. However, patient
motivation and progress tracking in rehabilitation outcomes remain a challenge.
This work addresses the gap through a machine learning-based approach to
objectively measure outcomes of the upper limb virtual therapy system in a user
study with non-clinical participants. In this study, we use virtual reality to
perform several tracing tasks while collecting motion and movement data using a
KinArm robot and a custom-made wearable sleeve sensor. We introduce a two-step
machine learning architecture to predict the motion intention of participants.
The first step predicts reaching task segments to which the participant-marked
points belonged using gaze, while the second step employs a Long Short-Term
Memory (LSTM) model to predict directional movements based on resistance change
values from the wearable sensor and the KinArm. We specifically propose to
transpose our raw resistance data to the time-domain which significantly
improves the accuracy of the models by 34.6%. To evaluate the effectiveness of
our model, we compared different classification techniques with various data
configurations. The results show that our proposed computational method is
exceptional at predicting participant's actions with accuracy values of 96.72%
for diamond reaching task, and 97.44% for circle reaching task, which
demonstrates the great promise of using multimodal data, including eye-tracking
and resistance change, to objectively measure the performance and intention in
virtual rehabilitation settings.","['Pavan Uttej Ravva', 'Pinar Kullu', 'Mohammad Fahim Abrar', 'Roghayeh Leila Barmaki']",2024-05-15T15:08:14Z,http://arxiv.org/abs/2405.13023v1,['cs.HC'],"machine learning,upper limb,motion intentions,multimodal data,virtual reality,physical therapy,rehabilitation outcomes,Long Short-Term Memory (LSTM),resistance data,classification techniques"
"Narrative Review of Support for Emotional Expressions in Virtual
  Reality: Psychophysiology of speech-to-text interfaces","This narrative review on emotional expression in Speech-to-Text (STT)
interfaces with Virtual Reality (VR) aims to identify advancements,
limitations, and research gaps in incorporating emotional expression into
transcribed text generated by STT systems. Using a rigorous search strategy,
relevant articles published between 2020 and 2024 are extracted and categorized
into themes such as communication enhancement technologies, innovations in
captioning, emotion recognition in AR and VR, and empathic machines. The
findings reveal the evolution of tools and techniques to meet the needs of
individuals with hearing impairments, showcasing innovations in live
transcription, closed captioning, AR, VR, and emotion recognition technologies.
Despite improvements in accessibility, the absence of emotional nuance in
transcribed text remains a significant communication challenge. The study
underscores the urgency for innovations in STT technology to capture emotional
expressions. The research discusses integrating emotional expression into text
through strategies like animated text captions, emojilization tools, and models
associating emotions with animation properties. Extending these efforts into AR
and VR environments opens new possibilities for immersive and emotionally
resonant experiences, especially in educational contexts. The study also
explores empathic applications in healthcare, education, and human-robot
interactions, highlighting the potential for personalized and effective
interactions. The multidisciplinary nature of the literature underscores the
potential for collaborative and interdisciplinary research.","['Sunday David Ubur', 'Denis Gracanin']",2024-05-22T18:53:27Z,http://arxiv.org/abs/2405.13924v1,['cs.HC'],"emotional expression,virtual reality,psychophysiology,speech-to-text interfaces,AR,VR,emotion recognition,captioning,empathy,interdisciplinary research"
"Assessing 3D scan quality in Virtual Reality through paired-comparisons
  psychophysics test","Consumer 3D scanners and depth cameras are increasingly being used to
generate content and avatars for Virtual Reality (VR) environments and avoid
the inconveniences of hand modeling; however, it is sometimes difficult to
evaluate quantitatively the mesh quality at which 3D scans should be exported,
and whether the object perception might be affected by its shading. We propose
using a paired-comparisons test based on psychophysics of perception to do that
evaluation. As psychophysics is not subject to opinion, skill level, mental
state, or economic situation it can be considered a quantitative way to measure
how people perceive the mesh quality. In particular, we propose using the
psychophysical measure for the comparison of four different levels of mesh
quality (1K, 5K, 10K and 20K triangles). We present two studies within
subjects: in one we investigate the quality perception variations of seeing an
object in a regular screen monitor against an stereoscopic Head Mounted Display
(HMD); while in the second experiment we aim at detecting the effects of
shading into quality perception. At each iteration of the pair-test comparisons
participants pick the mesh that they think had higher quality; by the end of
the experiment we compile a preference matrix. The matrix evidences the
correlation between real quality and assessed quality. Regarding the shading
mode, we find an interaction with quality and shading when the model has high
definition. Furthermore, we assess the subjective realism of the most/least
preferred scans using an Immersive Augmented Reality (IAR) video-see-through
setup. Results show higher levels of realism were perceived through the HMD
than when using a monitor, although the quality was similarly perceived in both
systems.","['Jacob Thorn', 'Rodrigo Pizarro', 'Bernhard Spanlang', 'Pablo Bermell-Garcia', 'Mar Gonzalez-Franco']",2016-01-31T12:43:34Z,http://arxiv.org/abs/1602.00238v2,"['cs.HC', 'cs.MM']","3D scan,Virtual Reality,psychophysics,mesh quality,perception,shading,Head Mounted Display,Immersive Augmented Reality,triangles"
"Using Virtual Reality in Electrostatics Instruction: The Impact of
  Training","Recent years have seen a resurgence of interest in using Virtual Reality (VR)
technology to benefit instruction, especially in physics and related subjects.
As VR devices improve and become more widely available, there remains a number
of unanswered questions regarding the impact of VR on student learning and how
best to use this technology in the classroom. On the topic of electrostatics,
for example, a large, controlled, randomized study performed by Smith et al.
2017\cite{smith17}, found that VR-based instruction had an overall negligible
impact on student learning compared to videos or images. However, they did find
a strong trend for students who reported frequent video game play to learn
better from VR than other media. One possible interpretation of this result is
that extended videogame play provides a kind of ""training"" that enables a
student to learn more comfortably in the virtual environment. In the present
work we consider if a VR training activity that is unrelated to electrostatics
can help prepare students to learn electrostatics from subsequent VR
instruction. We find that preliminary VR training leads to a small but
statistically significant improvement in student performance on our
electrostatics assessment. We also find that student reported game play is
still correlated with higher scores on this metric.","['Chris D. Porter', 'Joseph R. H. Smith', 'Eric M. Stagar', 'Amber Simmons', 'Megan Nieberding', 'Jonathan R. Brown', 'Abigail Ayers', 'Chris M. Orban']",2020-01-22T20:14:29Z,http://arxiv.org/abs/2001.08257v4,['physics.ed-ph'],"Virtual Reality,Electrostatics,Training,Instruction,Physics,Impact,Student Learning,Classroom,Videogame Play"
"Towards the Systematic Testing of Virtual Reality Programs (extended
  version)","Software testing is a critical activity to ensure that software complies with
its specification. However, current software testing activities tend not to be
completely effective when applied in specific software domains in Virtual
Reality (VR) that has several new types of features such as images, sounds,
videos, and differentiated interaction, which can become sources of new kinds
of faults. This paper presents an overview of the main VR characteristics that
can have an impact on verification, validation, and testing (VV&T).
Furthermore, it analyzes some of the most successful VR open-source projects to
draw a picture concerning the danger of the lack of software testing
activities. We compared the current state of software testing practice in
open-source VR projects and evaluate how the lack of testing can be damaging to
the development of a product. We assessed the incidence of code smells and
verified how such projects behave concerning the tendency to present faults.
The results showed that the practice of software testing is not yet widespread
in the development of VR applications. It was also found that there is a high
incidence of code smells in VR projects. Regarding fault-proneness the results
showed that about 12.2% of the classes analyzed in VR projects are fault-prone.
Regarding the application of software testing techniques on VR projects, it was
observed that only a small number of projects are concerned about developing
test cases for VR projects, perhaps because we still do not have the necessary
tools to help in this direction. Concerning smells, we concluded that there is
a high incidence in VR projects, especially regarding implementing smells and
this high incidence can have a significant influence on faults. Finally, the
study related to fault proneness pointed out that the lack of software testing
activity is a significant risk to the success of the projects.","['Stevao A. Andrade', 'Fatima L. S. Nunes', 'Marcio E. Delamaro']",2020-09-18T16:54:09Z,http://arxiv.org/abs/2009.08930v1,['cs.SE'],"systematic testing,virtual reality programs,software testing,VR characteristics,code smells,fault-proneness,verification,validation,testing,open-source projects,test cases"
Improving Big Data Visual Analytics with Interactive Virtual Reality,"For decades, the growth and volume of digital data collection has made it
challenging to digest large volumes of information and extract underlying
structure. Coined 'Big Data', massive amounts of information has quite often
been gathered inconsistently (e.g from many sources, of various forms, at
different rates, etc.). These factors impede the practices of not only
processing data, but also analyzing and displaying it in an efficient manner to
the user. Many efforts have been completed in the data mining and visual
analytics community to create effective ways to further improve analysis and
achieve the knowledge desired for better understanding. Our approach for
improved big data visual analytics is two-fold, focusing on both visualization
and interaction. Given geo-tagged information, we are exploring the benefits of
visualizing datasets in the original geospatial domain by utilizing a virtual
reality platform. After running proven analytics on the data, we intend to
represent the information in a more realistic 3D setting, where analysts can
achieve an enhanced situational awareness and rely on familiar perceptions to
draw in-depth conclusions on the dataset. In addition, developing a
human-computer interface that responds to natural user actions and inputs
creates a more intuitive environment. Tasks can be performed to manipulate the
dataset and allow users to dive deeper upon request, adhering to desired
demands and intentions. Due to the volume and popularity of social media, we
developed a 3D tool visualizing Twitter on MIT's campus for analysis. Utilizing
emerging technologies of today to create a fully immersive tool that promotes
visualization and interaction can help ease the process of understanding and
representing big data.","['Andrew Moran', 'Vijay Gadepally', 'Matthew Hubbell', 'Jeremy Kepner']",2015-06-29T17:50:20Z,http://arxiv.org/abs/1506.08754v2,"['cs.HC', 'cs.CY']","Big Data,Visual Analytics,Interactive,Virtual Reality,Data Mining,Geo-tagged information,3D setting,Human-computer interface,Social media,Immersive tool"
IMHOTEP - Virtual Reality Framework for Surgical Applications,"Purpose: The data which is available to surgeons before, during and after
surgery is steadily increasing in quantity as well as diversity. When planning
a patient's treatment, this large amount of information can be difficult to
interpret. To aid in processing the information, new methods need to be found
to present multi-modal patient data, ideally combining textual, imagery,
temporal and 3D data in a holistic and context-aware system. Methods: We
present an open-source framework which allows handling of patient data in a
virtual reality (VR) environment. By using VR technology, the workspace
available to the surgeon is maximized and 3D patient data is rendered in
stereo, which increases depth perception. The framework organizes the data into
workspaces and contains tools which allow users to control, manipulate and
enhance the data. Due to the framework's modular design, it can easily be
adapted and extended for various clinical applications. Results: The framework
was evaluated by clinical personnel (77 participants). The majority of the
group stated that a complex surgical situation is easier to comprehend by using
the framework, and that it is very well suited for education. Furthermore, the
application to various clinical scenarios - including the simulation of
excitation-propagation in the human atrium - demonstrated the framework's
adaptability. As a feasibility study, the framework was used during the
planning phase of the surgical removal of a large central carcinoma from a
patient's liver. Conclusion: The clinical evaluation showed a large potential
and high acceptance for the VR environment in a medical context. The various
applications confirmed that the framework is easily extended and can be used in
real-time simulation as well as for the manipulation of complex anatomical
structures.","['Micha Pfeiffer', 'Hannes Kenngott', 'Anas Preukschas', 'Matthias Huber', 'Lisa Bettscheider', 'Beat Müller-Stich', 'Stefanie Speidel']",2018-03-22T08:38:44Z,http://arxiv.org/abs/1803.08264v1,['cs.HC'],"virtual reality,framework,surgical applications,patient data,3D data,VR technology,clinical applications,feasibility study,simulation,anatomical structures"
"Interactive molecular dynamics in virtual reality from quantum chemistry
  to drug binding: An open-source multi-person framework","As molecular scientists have made progress in their ability to engineer
nano-scale molecular structure, we are facing new challenges in our ability to
engineer molecular dynamics (MD) and flexibility. Dynamics at the molecular
scale differs from the familiar mechanics of everyday objects, because it
involves a complicated, highly correlated, and three-dimensional many-body
dynamical choreography which is often non-intuitive even for highly trained
researchers. We recently described how interactive molecular dynamics in
virtual reality (iMD-VR) can help to meet this challenge, enabling researchers
to manipulate real-time MD simulations of flexible structures in 3D. In this
article, we outline efforts to extend immersive technologies to the molecular
sciences, and we introduce 'Narupa', a flexible, open-source, multi-person
iMD-VR software framework which enables groups of researchers to simultaneously
cohabit real-time simulation environments to interactively visualize and
manipulate the dynamics of molecular structures with atomic-level precision. We
outline several application domains where iMD-VR is facilitating research,
communication, and creative approaches within the molecular sciences, including
training machines to learn reactive potential energy surfaces (PESs),
biomolecular conformational sampling, protein-ligand binding, reaction
discovery using 'on-the-fly' quantum chemistry, and transport dynamics in
materials. We touch on iMD-VR's various cognitive and perceptual affordances,
and how these provide research insight for molecular systems. By
synergistically combining human spatial reasoning and design insight with
computational automation, technologies like iMD-VR have the potential to
improve our ability to understand, engineer, and communicate microscopic
dynamical behavior, offering the potential to usher in a new paradigm for
engineering molecules and nano-architectures.","[""Michael O'Connor"", 'Simon J. Bennie', 'Helen M. Deeks', 'Alexander Jamieson-Binnie', 'Alex J. Jones', 'Robin J. Shannon', 'Rebecca Walters', 'Thomas J. Mitchell', 'Adrian J. Mulholland', 'David R. Glowacki']",2019-02-05T17:57:58Z,http://arxiv.org/abs/1902.01827v3,"['physics.chem-ph', 'cs.HC', 'physics.bio-ph', 'physics.ed-ph']","quantum chemistry,drug binding,molecular dynamics,virtual reality,open-source,multi-person framework,nano-scale,molecular structure,immersive technologies,reactive potential energy surfaces"
"Data Correlation-Aware Resource Management in Wireless Virtual Reality
  (VR): An Echo State Transfer Learning Approach","In this paper, the problem of wireless virtual reality (VR) resource
management is investigated for a wireless VR network in which VR contents are
sent by a cloud to cellular small base stations (SBSs). The SBSs will collect
tracking data from the VR users, over the uplink, in order to generate the VR
content and transmit it to the end-users using downlink cellular links. For
this model, the data requested or transmitted by the users can exhibit
correlation, since the VR users may engage in the same immersive virtual
environment with different locations and orientations. As such, the proposed
resource management framework can factor in such spatial data correlation, to
better manage uplink and downlink traffic. This potential spatial data
correlation can be factored into the resource allocation problem to reduce the
traffic load in both uplink and downlink. In the downlink, the cloud can
transmit 360 contents or specific visible contents that are extracted from the
original 360 contents to the users according to the users' data correlation to
reduce the backhaul traffic load. For uplink, each SBS can associate with the
users that have similar tracking information so as to reduce the tracking data
size. This data correlation-aware resource management problem is formulated as
an optimization problem whose goal is to maximize the users' successful
transmission probability, defined as the probability that the content
transmission delay of each user satisfies an instantaneous VR delay target. To
solve this problem, an echo state networks (ESNs) based transfer learning is
introduced. By smartly transferring information on the SBS's utility, the
proposed transfer-based ESN algorithm can quickly cope with changes in the
wireless networking environment.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin', 'Mérouane Debbah']",2019-02-14T01:44:10Z,http://arxiv.org/abs/1902.05181v1,"['cs.IT', 'math.IT']","wireless virtual reality,resource management,data correlation,cloud,small base stations,uplink,downlink,spatial data correlation,traffic load,transfer learning"
"A Transferable Adaptive Domain Adversarial Neural Network for Virtual
  Reality Augmented EMG-Based Gesture Recognition","Within the field of electromyography-based (EMG) gesture recognition,
disparities exist between the offline accuracy reported in the literature and
the real-time usability of a classifier. This gap mainly stems from two
factors: 1) The absence of a controller, making the data collected dissimilar
to actual control. 2) The difficulty of including the four main dynamic factors
(gesture intensity, limb position, electrode shift, and transient changes in
the signal), as including their permutations drastically increases the amount
of data to be recorded. Contrarily, online datasets are limited to the exact
EMG-based controller used to record them, necessitating the recording of a new
dataset for each control method or variant to be tested. Consequently, this
paper proposes a new type of dataset to serve as an intermediate between
offline and online datasets, by recording the data using a real-time
experimental protocol. The protocol, performed in virtual reality, includes the
four main dynamic factors and uses an EMG-independent controller to guide
movements. This EMG-independent feedback ensures that the user is in-the-loop
during recording, while enabling the resulting dynamic dataset to be used as an
EMG-based benchmark. The dataset is comprised of 20 able-bodied participants
completing three to four sessions over a period of 14 to 21 days. The ability
of the dynamic dataset to serve as a benchmark is leveraged to evaluate the
impact of different recalibration techniques for long-term (across-day) gesture
recognition, including a novel algorithm, named TADANN. TADANN consistently and
significantly (p<0.05) outperforms using fine-tuning as the recalibration
technique.","['Ulysse Côté-Allard', 'Gabriel Gagnon-Turcotte', 'Angkoon Phinyomark', 'Kyrre Glette', 'Erik Scheme', 'François Laviolette', 'Benoit Gosselin']",2019-12-16T18:41:56Z,http://arxiv.org/abs/1912.09380v2,"['cs.LG', 'cs.CV', 'cs.HC', 'stat.ML']","Adaptive neural network,Domain adversarial,EMG-based gesture recognition,Virtual reality,Transferable,Dynamic factors,Recalibration techniques,Offline accuracy,Real-time usability"
"Self-paced brain-computer interface control of ambulation in a virtual
  reality environment","Objective: Spinal cord injury (SCI) often leaves affected individuals unable
to ambulate. Electroencephalogramme (EEG) based brain-computer interface (BCI)
controlled lower extremity prostheses may restore intuitive and able-body-like
ambulation after SCI. To test its feasibility, the authors developed and tested
a novel EEG-based, data-driven BCI system for intuitive and self-paced control
of the ambulation of an avatar within a virtual reality environment (VRE).
  Approach: Eight able-bodied subjects and one with SCI underwent the following
10-min training session: subjects alternated between idling and walking
kinaesthetic motor imageries (KMI) while their EEG were recorded and analysed
to generate subject-specific decoding models. Subjects then performed a
goal-oriented online task, repeated over 5 sessions, in which they utilised the
KMI to control the linear ambulation of an avatar and make 10 sequential stops
at designated points within the VRE.
  Main results: The average offline training performance across subjects was
77.2 +/- 9.5%, ranging from 64.3% (p = 0.00176) to 94.5% (p = 6.26*10^-23),
with chance performance being 50%. The average online performance was 8.4 +/-
1.0 (out of 10) successful stops and 303 +/- 53 sec completion time (perfect =
211 sec). All subjects achieved performances significantly different than those
of random walk (p < 0.05) in 44 of the 45 online sessions.
  Significance: By using a data-driven machine learning approach to decode
users' KMI, this BCIVRE system enabled intuitive and purposeful self-paced
control of ambulation after only a 10-minute training. The ability to achieve
such BCI control with minimal training indicates that the implementation of
future BCI-lower extremity prosthesis systems may be feasible.","['Po T. Wang', 'Christine E. King', 'Luis A. Chui', 'An H. Do', 'Zoran Nenadic']",2012-08-30T00:58:22Z,http://arxiv.org/abs/1208.6057v1,['cs.HC'],"Brain-computer interface,Ambulation,Virtual reality environment,Electroencephalogram,Spinal cord injury,Lower extremity prosthesis,Motor imagery,Decoding models,Machine learning,Self-paced control"
"Immersive and Collaborative Data Visualization Using Virtual Reality
  Platforms","Effective data visualization is a key part of the discovery process in the
era of big data. It is the bridge between the quantitative content of the data
and human intuition, and thus an essential component of the scientific path
from data into knowledge and understanding. Visualization is also essential in
the data mining process, directing the choice of the applicable algorithms, and
in helping to identify and remove bad data from the analysis. However, a high
complexity or a high dimensionality of modern data sets represents a critical
obstacle. How do we visualize interesting structures and patterns that may
exist in hyper-dimensional data spaces? A better understanding of how we can
perceive and interact with multi dimensional information poses some deep
questions in the field of cognition technology and human computer interaction.
To this effect, we are exploring the use of immersive virtual reality platforms
for scientific data visualization, both as software and inexpensive commodity
hardware. These potentially powerful and innovative tools for multi dimensional
data visualization can also provide an easy and natural path to a collaborative
data visualization and exploration, where scientists can interact with their
data and their colleagues in the same visual space. Immersion provides benefits
beyond the traditional desktop visualization tools: it leads to a demonstrably
better perception of a datascape geometry, more intuitive data understanding,
and a better retention of the perceived relationships in the data.","['Ciro Donalek', 'S. G. Djorgovski', 'Scott Davidoff', 'Alex Cioc', 'Anwell Wang', 'Giuseppe Longo', 'Jeffrey S. Norris', 'Jerry Zhang', 'Elizabeth Lawler', 'Stacy Yeh', 'Ashish Mahabal', 'Matthew Graham', 'Andrew Drake']",2014-10-28T15:46:44Z,http://arxiv.org/abs/1410.7670v1,"['cs.HC', 'astro-ph.IM']","data visualization,virtual reality,collaborative visualization,immersive visualization,high dimensionality,multi dimensional,cognition technology,human computer interaction,scientific data visualization,data mining."
"Virtual Reality over Wireless Networks: Quality-of-Service Model and
  Learning-Based Resource Management","In this paper, the problem of resource management is studied for a network of
wireless virtual reality (VR) users communicating over small cell networks
(SCNs). In order to capture the VR users' quality-of-service (QoS) in SCNs, a
novel VR model, based on multi-attribute utility theory, is proposed. This
model jointly accounts for VR metrics such as tracking accuracy, processing
delay, and transmission delay. In this model, the small base stations (SBSs)
act as the VR control centers that collect the tracking information from VR
users over the cellular uplink. Once this information is collected, the SBSs
will then send the three dimensional images and accompanying surround stereo
audio to the VR users over the downlink. Therefore, the resource allocation
problem in VR wireless networks must jointly consider both the uplink and
downlink. This problem is then formulated as a noncooperative game and a
distributed algorithm based on the machine learning framework of echo state
networks (ESNs) is proposed to find the solution of this game. The use of the
proposed ESN algorithm enables the SBSs to predict the VR QoS of each SBS and
guarantees the convergence to a mixed-strategy Nash equilibrium. The analytical
result shows that each user's VR QoS jointly depends on both VR tracking
accuracy and wireless resource allocation. Simulation results show that the
proposed algorithm yields significant gains, in terms of total utility value of
VR QoS, that reach up to 22.2% and 37.5%, respectively, compared to Q-learning
and a baseline proportional fair algorithm. The results also show that the
proposed algorithm has a faster convergence time than Q-learning and can
guarantee low delays for VR services.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin']",2017-03-13T00:43:39Z,http://arxiv.org/abs/1703.04209v2,"['cs.IT', 'math.IT']","1. Virtual Reality,2. Wireless Networks,3. Quality-of-Service,4. Resource Management,5. Small Cell Networks,6. Multi-attribute Utility Theory,7. Machine Learning,8. Echo State Networks,9. Nash Equilibrium,10. Simulation Results."
"ReHabgame A non-immersive virtual reality rehabilitation system with
  applications in neuroscience","This paper proposes the use of a non-immersive virtual reality rehabilitation
system ReHabgame developed using Microsoft Kinect and the Thalmic Labs Myo
gesture control armband. The ReHabgame was developed based on two third-person
video games that provide a feasible possibility of assessing postural control
and functional reach tests. It accurately quantifies specific postural control
mechanisms including timed standing balance, functional reach tests using
real-time anatomical landmark orientation, joint velocity, and acceleration
while end trajectories were calculated using an inverse kinematics algorithm.
The game was designed to help patients with neurological impairment to be
subjected to physiotherapy activity and practice postures of daily activities.
The subjective experience of the ReHabgame was studied through the development
of an Engagement Questionnaire (EQ) for qualitative, quantitative and Rasch
model. The Monte-Carlo Tree Search (MCTS) and Random object (ROG) generator
algorithms were used to adapt the physical and gameplay intensity in the
ReHabgame based on the Motor Assessment Scale (MAS) and Hierarchical Scoring
System (HSS). Rasch analysis was conducted to assess the psychometric
characteristics of the ReHabgame and to identify if these are any misfitting
items in the game. Rasch rating scale model (RSM) was used to assess the
engagement of players in the ReHabgame and evaluate the effectiveness and
attractiveness of the game. The results showed that the scales assessing the
rehabilitation process met Rasch expectations of reliability, and
unidimensionality. Infit and outfit mean squares values are in the range of
(0.68 1.52) for all considered 16 items. The Root Mean Square Residual (RMSR)
and the person separation reliability were acceptable. The item/person map
showed that the persons and items were clustered symmetrically.","['Shabnam Sadeghi Esfahlani', 'Tommy Thompson', 'Ali D. Parsa', 'Ian Brown', 'Silvia Cirstea']",2018-04-27T07:51:32Z,http://arxiv.org/abs/1804.11247v1,['cs.HC'],"virtual reality,rehabilitation system,neuroscience,Microsoft Kinect,Thalmic Labs Myo,gesture control armband,postural control,functional reach tests,engagement questionnaire,Rasch analysis"
"Personal space of autonomous car's passengers sitting in the driver's
  seat","This article deals with the specific context of an autonomous car navigating
in an urban center within a shared space between pedestrians and cars. The
driver delegates the control to the autonomous system while remaining seated in
the driver's seat. The proposed study aims at giving a first insight into the
definition of human perception of space applied to vehicles by testing the
existence of a personal space around the car.It aims at measuring proxemic
information about the driver's comfort zone in such conditions.Proxemics, or
human perception of space, has been largely explored when applied to humans or
to robots, leading to the concept of personal space, but poorly when applied to
vehicles. In this article, we highlight the existence and the characteristics
of a zone of comfort around the car which is not correlated to the risk of a
collision between the car and other road users. Our experiment includes 19
volunteers using a virtual reality headset to look at 30 scenarios filmed in
360{\textdegree} from the point of view of a passenger sitting in the driver's
seat of an autonomous car.They were asked to say ""stop"" when they felt
discomfort visualizing the scenarios.As said, the scenarios voluntarily avoid
collision effect as we do not want to measure fear but discomfort.The scenarios
involve one or three pedestrians walking past the car at different distances
from the wings of the car, relative to the direction of motion of the car, on
both sides. The car is either static or moving straight forward at different
speeds.The results indicate the existence of a comfort zone around the car in
which intrusion causes discomfort.The size of the comfort zone is sensitive
neither to the side of the car where the pedestrian passes nor to the number of
pedestrians. In contrast, the feeling of discomfort is relative to the car's
motion (static or moving).Another outcome from this study is an illustration of
the usage of first person 360{\textdegree} video and a virtual reality headset
to evaluate feelings of a passenger within an autonomous car.","['Eleonore Ferrier-Barbut', 'Dominique Vaufreydaz', 'Jean-Alix David', 'Jérôme Lussereau', 'Anne Spalanzani']",2018-05-09T14:46:40Z,http://arxiv.org/abs/1805.03563v1,"['cs.HC', 'cs.RO']","autonomous car,personal space,passengers,driver's seat,proxemics,human perception,comfort zone,pedestrians,virtual reality,collision"
Movie Editing and Cognitive Event Segmentation in Virtual Reality Video,"Traditional cinematography has relied for over a century on a
well-established set of editing rules, called continuity editing, to create a
sense of situational continuity. Despite massive changes in visual content
across cuts, viewers in general experience no trouble perceiving the
discontinuous flow of information as a coherent set of events. However, Virtual
Reality (VR) movies are intrinsically different from traditional movies in that
the viewer controls the camera orientation at all times. As a consequence,
common editing techniques that rely on camera orientations, zooms, etc., cannot
be used. In this paper we investigate key relevant questions to understand how
well traditional movie editing carries over to VR. To do so, we rely on recent
cognition studies and the event segmentation theory, which states that our
brains segment continuous actions into a series of discrete, meaningful events.
We first replicate one of these studies to assess whether the predictions of
such theory can be applied to VR. We next gather gaze data from viewers
watching VR videos containing different edits with varying parameters, and
provide the first systematic analysis of viewers' behavior and the perception
of continuity in VR. From this analysis we make a series of relevant findings;
for instance, our data suggests that predictions from the cognitive event
segmentation theory are useful guides for VR editing; that different types of
edits are equally well understood in terms of continuity; and that spatial
misalignments between regions of interest at the edit boundaries favor a more
exploratory behavior even after viewers have fixated on a new region of
interest. In addition, we propose a number of metrics to describe viewers'
attentional behavior in VR. We believe the insights derived from our work can
be useful as guidelines for VR content creation.","['Ana Serrano', 'Vincent Sitzmann', 'Jaime Ruiz-Borau', 'Gordon Wetzstein', 'Diego Gutierrez', 'Belen Masia']",2018-06-13T10:08:48Z,http://arxiv.org/abs/1806.04924v1,['cs.GR'],"movie editing,cognitive event segmentation,virtual reality video,continuity editing,camera orientation,editing techniques,event segmentation theory,gaze data,perception of continuity,attentional behavior"
"Federated Echo State Learning for Minimizing Breaks in Presence in
  Wireless Virtual Reality Networks","In this paper, the problem of enhancing the virtual reality (VR) experience
for wireless users is investigated by minimizing the occurrence of breaks in
presence (BIP) that can detach the users from their virtual world. To measure
the BIP for wireless VR users, a novel model that jointly considers the VR
application type, transmission delay, VR video quality, and users' awareness of
the virtual environment is proposed. In the developed model, the base stations
(BSs) transmit VR videos to the wireless VR users using directional
transmission links so as to provide high data rates for the VR users, thus,
reducing the number of BIP for each user. Since the body movements of a VR user
may result in a blockage of its wireless link, the location and orientation of
VR users must also be considered when minimizing BIP. The BIP minimization
problem is formulated as an optimization problem which jointly considers the
predictions of users' locations, orientations, and their BS association. To
predict the orientation and locations of VR users, a distributed learning
algorithm based on the machine learning framework of deep (ESNs) is proposed.
The proposed algorithm uses concept from federated learning to enable multiple
BSs to locally train their deep ESNs using their collected data and
cooperatively build a learning model to predict the entire users' locations and
orientations. Using these predictions, the user association policy that
minimizes BIP is derived. Simulation results demonstrate that the developed
algorithm reduces the users' BIP by up to 16% and 26%, respectively, compared
to centralized ESN and deep learning algorithms.","['Mingzhe Chen', 'Omid Semiari', 'Walid Saad', 'Xuanlin Liu', 'Changchuan Yin']",2018-12-04T03:44:31Z,http://arxiv.org/abs/1812.01202v2,"['cs.IT', 'math.IT']","wireless virtual reality networks,breaks in presence,VR application,transmission delay,VR video quality,base stations,directional transmission links,VR users,body movements,wireless link,optimization problem,machine learning framework"
"Training neural nets to learn reactive potential energy surfaces using
  interactive quantum chemistry in virtual reality","Whilst the primary bottleneck to a number of computational workflows was not
so long ago limited by processing power, the rise of machine learning
technologies has resulted in a paradigm shift which places increasing value on
issues related to data curation - i.e., data size, quality, bias, format, and
coverage. Increasingly, data-related issues are equally as important as the
algorithmic methods used to process and learn from the data. Here we introduce
an open source GPU-accelerated neural network (NN) framework for learning
reactive potential energy surfaces (PESs), and investigate the use of real-time
interactive ab initio molecular dynamics in virtual reality (iMD-VR) as a new
strategy for rapidly sampling geometries along reaction pathways which can be
used to train NNs to learn reactive PESs. Focussing on hydrogen abstraction
reactions of CN radical with isopentane, we compare the performance of NNs
trained using iMD-VR data versus NNs trained using a more traditional method,
namely molecular dynamics (MD) constrained to sample a predefined grid of
points along hydrogen abstraction reaction coordinates. Both the NN trained
using iMD-VR data and the NN trained using the constrained MD data reproduce
important qualitative features of the reactive PESs, such as a low and early
barrier to abstraction. Quantitatively, learning is sensitive to the training
dataset. Our results show that user-sampled structures obtained with the
quantum chemical iMD-VR machinery enable better sampling in the vicinity of the
minimum energy path (MEP). As a result, the NN trained on the iMD-VR data does
very well predicting energies in the vicinity of the MEP, but less well
predicting energies for 'off-path' structures. The NN trained on the
constrained MD data does better in predicting energies for 'off-path'
structures, given that it included a number of such structures in its training
set.","['Silvia Amabilino', 'Lars A. Bratholm', 'Simon J. Bennie', 'Alain C. Vaucher', 'Markus Reiher', 'David R. Glowacki']",2019-01-16T18:09:10Z,http://arxiv.org/abs/1901.05417v2,"['physics.chem-ph', 'cs.ET', 'physics.comp-ph']","neural network,potential energy surface,quantum chemistry,virtual reality,data curation,machine learning,molecular dynamics,ab initio,reactive,training."
"The G332 molecular cloud ring: I. Morphology and physical
  characteristics","We present a morphological and physical analysis of a Giant Molecular Cloud
(GMC) using the carbon monoxide isotopologues ($^{12}$CO, $^{13}$CO, C$^{18}$O
$^{3}P_{2}\rightarrow$ $^{3}P_{1}$) survey of the Galactic Plane (Mopra CO
Southern Galactic Plane Survey), supplemented with neutral carbon maps from the
HEAT telescope in Antarctica. The giant molecular cloud structure (hereinafter
the ring) covers the sky region $332^\circ$ < $\ell$ < $333^\circ$ and
$\mathit{b}$ = $\pm 0.5^\circ$ (hereinafter the G332 region). The mass of the
ring and its distance are determined to be respectively
~2$\times10^{5}\mathrm{M_{\odot}}$ and ~3.7 kpc from Sun. The dark molecular
gas fraction, estimated from the $^{13}$CO and [CI] lines, is $\sim17\%$ for a
CO T$_{\mathrm{ex}}$ between [10,20 K]. Comparing the [CI] integrated intensity
and N(H$_{2}$) traced by $^{13}$CO and $^{12}$CO, we define an
X$\mathrm{_{CI}^{809}}$ factor, analogous to the usual X$_{\mathrm{co}}$,
through the [CI] line. X$\mathrm{_{CI}^{809}}$ ranges between
[1.8,2.0]$\times10^{21}\mathrm{cm}^{-2}\mathrm{K}^{-1}\mathrm{km}^{-1}\mathrm{s}$.
We examined local variation in X$_{\mathrm{co}}$ and T$_{\mathrm{ex}}$ across
the cloud, and find in regions where the star formation activity is not in an
advanced state, an increase in the mean and dispersion of the X$_{\mathrm{co}}$
factor as the excitation temperature decreases. We present a catalogue of
C$^{18}$O clumps within the cloud. The star formation (SF) activity ongoing in
the cloud shows a correlation with T$_{\mathrm{ex}}$, [CI] and CO emissions,
and anti-correlation with X$_{\mathrm{co}}$, suggesting a North-South spatial
gradient in the SF activity. We propose a method to disentangle dust emission
across the Galaxy, using HI and $^{13}$CO data. We describe Virtual Reality
(VR) and Augmented Reality (AR) data visualisation techniques for the analysis
of radio astronomy data.","['Domenico Romano', 'Michael G. Burton', 'Michael C. B. Ashley', 'Sergio Molinari', 'David Rebolledo', 'Catherine Braiding', 'Eugenio Schisano']",2019-01-17T16:03:42Z,http://arxiv.org/abs/1901.05961v1,['astro-ph.GA'],"Giant Molecular Cloud,Carbon Monoxide,Isotopologues,Dark Molecular Gas,Star Formation,Excitation Temperature,Clumps,Virtual Reality,Augmented Reality,Radio Astronomy"
Interactive Multi-User 3D Visual Analytics in Augmented Reality,"This publication reports on a research project in which we set out to explore
the advantages and disadvantages augmented reality (AR) technology has for
visual data analytics. We developed a prototype of an AR data analytics
application, which provides users with an interactive 3D interface, hand
gesture-based controls and multi-user support for a shared experience, enabling
multiple people to collaboratively visualize, analyze and manipulate data with
high dimensional features in 3D space. Our software prototype, called DataCube,
runs on the Microsoft HoloLens - one of the first true stand-alone AR headsets,
through which users can see computer-generated images overlaid onto real-world
objects in the user's physical environment. Using hand gestures, the users can
select menu options, control the 3D data visualization with various filtering
and visualization functions, and freely arrange the various menus and virtual
displays in their environment. The shared multi-user experience allows all
participating users to see and interact with the virtual environment, changes
one user makes will become visible to the other users instantly. As users
engage together they are not restricted from observing the physical world
simultaneously and therefore they can also see non-verbal cues such as
gesturing or facial reactions of other users in the physical environment. The
main objective of this research project was to find out if AR interfaces and
collaborative analysis can provide an effective solution for data analysis
tasks, and our experience with our prototype system confirms this.","['Wanze Xie', 'Yining Liang', 'Janet Johnson', 'Andrea Mower', 'Samuel Burns', 'Colleen Chelini', 'Paul D Alessandro', 'Nadir Weibel', 'Jürgen P. Schulze']",2020-02-13T01:35:56Z,http://arxiv.org/abs/2002.05305v1,"['cs.HC', 'cs.MM', 'H.5.1; I.3.8; J.1']","3D Visual Analytics,Augmented Reality,Interactive,Multi-User,Data Visualization,Hand Gesture-Based Controls,Microsoft HoloLens,Collaborative Analysis,Data Analytics Application,High Dimensional Features"
"An In-Depth Exploration of the Effect of 2D/3D Views and Controller
  Types on First Person Shooter Games in Virtual Reality","The amount of interest in Virtual Reality (VR) research has significantly
increased over the past few years, both in academia and industry. The release
of commercial VR Head-Mounted Displays (HMDs) has been a major contributing
factor. However, there is still much to be learned, especially how views and
input techniques, as well as their interaction, affect the VR experience. There
is little work done on First-Person Shooter (FPS) games in VR, and those few
studies have focused on a single aspect of VR FPS. They either focused on the
view, e.g., comparing VR to a typical 2D display or on the controller types. To
the best of our knowledge, there are no studies investigating variations of
2D/3D views in HMDs, controller types, and their interactions. As such, it is
challenging to distinguish findings related to the controller type from those
related to the view. If a study does not control for the input method and finds
that 2D displays lead to higher performance than VR, we cannot generalize the
results because of the confounding variables. To understand their interaction,
we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the
way it is controlled that gives the platforms their respective advantages. To
study the effects of the 2D/3D views, we created a 2D visual technique,
PlaneFrame, that was applied inside the VR headset. Our results show that the
controller type can have a significant positive impact on performance,
immersion, and simulator sickness when associated with a 2D view. They further
our understanding of the interactions that controllers and views have and
demonstrate that comparisons are highly dependent on how both factors go
together. Further, through a series of three experiments, we developed a
technique that can lead to a substantial performance, a good level of
immersion, and can minimize the level of simulator sickness.","['Diego Monteiro', 'Hai-Ning Liang', 'Jialin Wang', 'Hao Chen', 'Nilufar Baghaei']",2020-10-07T08:17:07Z,http://arxiv.org/abs/2010.03256v1,['cs.HC'],"Virtual Reality,2D views,3D views,Controller Types,First-Person Shooter,HMDs,Input Techniques,Interaction,Immersion,Simulator Sickness"
"Validation of the Virtual Reality Neuroscience Questionnaire: Maximum
  Duration of Immersive Virtual Reality Sessions Without the Presence of
  Pertinent Adverse Symptomatology","Research suggests that the duration of a VR session modulates the presence
and intensity of VRISE, but there are no suggestions regarding the appropriate
maximum duration of VR sessions. The implementation of high-end VR HMDs in
conjunction with ergonomic VR software seems to mitigate the presence of VRISE
substantially. However, a brief tool does not currently exist to appraise and
report both the quality of software features and VRISE intensity
quantitatively. The VRNQ was developed to assess the quality of VR software in
terms of user experience, game mechanics, in-game assistance, and VRISE. Forty
participants aged between 28 and 43 years were recruited (18 gamers and 22
non-gamers) for the study. They participated in 3 different VR sessions until
they felt weary or discomfort and subsequently filled in the VRNQ. Our results
demonstrated that VRNQ is a valid tool for assessing VR software as it has good
convergent, discriminant, and construct validity. The maximum duration of VR
sessions should be between 55-70 minutes when the VR software meets or exceeds
the parsimonious cut-offs of the VRNQ and the users are familiarized with the
VR system. Also. the gaming experience does not seem to affect how long VR
sessions should last. Also, while the quality of VR software substantially
modulates the maximum duration of VR sessions, age and education do not.
Finally, deeper immersion, better quality of graphics and sound, and more
helpful in-game instructions and prompts were found to reduce VRISE intensity.
The VRNQ facilitates the brief assessment and reporting of the quality of VR
software features and/or the intensity of VRISE, while its minimum and
parsimonious cut-offs may appraise the suitability of VR software. The findings
of this study contribute to the establishment of rigorous VR methods that are
crucial for the viability of immersive VR as a research and clinical tool.","['Panagiotis Kourtesis', 'Simona Collina', 'Leonidas A. A. Doumas', 'Sarah E. MacPherson']",2021-01-20T14:10:44Z,http://arxiv.org/abs/2101.08146v1,"['cs.HC', 'cs.CY', 'cs.MM', 'B.8; C.4; D.0; J.4']","Virtual Reality Neuroscience Questionnaire,Maximum Duration,Immersive Virtual Reality Sessions,VRISE,VR HMDs,VR Software,User Experience,Game Mechanics,In-game Assistance,Convergent Validity,Construct Validity,Immersion"
FoV-NeRF: Foveated Neural Radiance Fields for Virtual Reality,"Virtual Reality (VR) is becoming ubiquitous with the rise of consumer
displays and commercial VR platforms. Such displays require low latency and
high quality rendering of synthetic imagery with reduced compute overheads.
Recent advances in neural rendering showed promise of unlocking new
possibilities in 3D computer graphics via image-based representations of
virtual or physical environments. Specifically, the neural radiance fields
(NeRF) demonstrated that photo-realistic quality and continuous view changes of
3D scenes can be achieved without loss of view-dependent effects. While NeRF
can significantly benefit rendering for VR applications, it faces unique
challenges posed by high field-of-view, high resolution, and
stereoscopic/egocentric viewing, typically causing low quality and high latency
of the rendered images. In VR, this not only harms the interaction experience
but may also cause sickness. To tackle these problems toward
six-degrees-of-freedom, egocentric, and stereo NeRF in VR, we present the first
gaze-contingent 3D neural representation and view synthesis method. We
incorporate the human psychophysics of visual- and stereo-acuity into an
egocentric neural representation of 3D scenery. We then jointly optimize the
latency/performance and visual quality while mutually bridging human perception
and neural scene synthesis to achieve perceptually high-quality immersive
interaction. We conducted both objective analysis and subjective studies to
evaluate the effectiveness of our approach. We find that our method
significantly reduces latency (up to 99% time reduction compared with NeRF)
without loss of high-fidelity rendering (perceptually identical to
full-resolution ground truth). The presented approach may serve as the first
step toward future VR/AR systems that capture, teleport, and visualize remote
environments in real-time.","['Nianchen Deng', 'Zhenyi He', 'Jiannan Ye', 'Budmonde Duinkharjav', 'Praneeth Chakravarthula', 'Xubo Yang', 'Qi Sun']",2021-03-30T14:05:47Z,http://arxiv.org/abs/2103.16365v2,"['cs.GR', 'cs.CV', 'I.3']","neural radiance fields,virtual reality,foveated rendering,3D computer graphics,egocentric,stereoscopic,gaze-contingent,neural representation,view synthesis,latency"
"Robust Egocentric Photo-realistic Facial Expression Transfer for Virtual
  Reality","Social presence, the feeling of being there with a real person, will fuel the
next generation of communication systems driven by digital humans in virtual
reality (VR). The best 3D video-realistic VR avatars that minimize the uncanny
effect rely on person-specific (PS) models. However, these PS models are
time-consuming to build and are typically trained with limited data
variability, which results in poor generalization and robustness. Major sources
of variability that affects the accuracy of facial expression transfer
algorithms include using different VR headsets (e.g., camera configuration,
slop of the headset), facial appearance changes over time (e.g., beard,
make-up), and environmental factors (e.g., lighting, backgrounds). This is a
major drawback for the scalability of these models in VR. This paper makes
progress in overcoming these limitations by proposing an end-to-end
multi-identity architecture (MIA) trained with specialized augmentation
strategies. MIA drives the shape component of the avatar from three cameras in
the VR headset (two eyes, one mouth), in untrained subjects, using minimal
personalized information (i.e., neutral 3D mesh shape). Similarly, if the PS
texture decoder is available, MIA is able to drive the full avatar
(shape+texture) robustly outperforming PS models in challenging scenarios. Our
key contribution to improve robustness and generalization, is that our method
implicitly decouples, in an unsupervised manner, the facial expression from
nuisance factors (e.g., headset, environment, facial appearance). We
demonstrate the superior performance and robustness of the proposed method
versus state-of-the-art PS approaches in a variety of experiments.","['Amin Jourabloo', 'Baris Gecer', 'Fernando De la Torre', 'Jason Saragih', 'Shih-En Wei', 'Te-Li Wang', 'Stephen Lombardi', 'Danielle Belko', 'Autumn Trimble', 'Hernan Badino']",2021-04-10T15:48:53Z,http://arxiv.org/abs/2104.04794v2,['cs.CV'],"facial expression transfer,virtual reality,3D video-realistic avatars,person-specific models,VR headsets,facial appearance changes,environmental factors,multi-identity architecture,texture decoder,robustness,generalization"
"BurstLink: Techniques for Energy-Efficient Conventional and Virtual
  Reality Video Display","Conventional planar video streaming is the most popular application in mobile
systems and the rapid growth of 360 video content and virtual reality (VR)
devices are accelerating the adoption of VR video streaming. Unfortunately,
video streaming consumes significant system energy due to the high power
consumption of the system components (e.g., DRAM, display interfaces, and
display panel) involved in this process.
  We propose BurstLink, a novel system-level technique that improves the energy
efficiency of planar and VR video streaming. BurstLink is based on two key
ideas. First, BurstLink directly transfers a decoded video frame from the host
system to the display panel, bypassing the host DRAM. To this end, we extend
the display panel with a double remote frame buffer (DRFB), instead of the
DRAM's double frame buffer, so that the system can directly update the DRFB
with a new frame while updating the panel's pixels with the current frame
stored in the DRFB. Second, BurstLink transfers a complete decoded frame to the
display panel in a single burst, using the maximum bandwidth of modern display
interfaces. Unlike conventional systems where the frame transfer rate is
limited by the pixel-update throughput of the display panel, BurstLink can
always take full advantage of the high bandwidth of modern display interfaces
by decoupling the frame transfer from the pixel update as enabled by the DRFB.
This direct and burst frame transfer of BurstLink significantly reduces energy
consumption in video display by reducing access to the host DRAM and increasing
the system's residency at idle power states.
  We evaluate BurstLink using an analytical power model that we rigorously
validate on a real modern mobile system. Our evaluation shows that BurstLink
reduces system energy consumption for 4K planar and VR video streaming by 41%
and 33%, respectively.","['Jawad Haj-Yahya', 'Jisung Park', 'Rahul Bera', 'Juan Gómez Luna', 'Efraim Rotem', 'Taha Shahroodi', 'Jeremie Kim', 'Onur Mutlu']",2021-04-11T22:03:49Z,http://arxiv.org/abs/2104.05119v4,['cs.AR'],"Energy-efficient,Video Display,VR,BurstLink,System-level technique,Frame transfer,DRAM,Display panel,Display interface,Power consumption"
"Comfort and Sickness while Virtually Aboard an Autonomous Telepresence
  Robot","In this paper, we analyze how different path aspects affect a user's
experience, mainly VR sickness and overall comfort, while immersed in an
autonomously moving telepresence robot through a virtual reality headset. In
particular, we focus on how the robot turns and the distance it keeps from
objects, with the goal of planning suitable trajectories for an autonomously
moving immersive telepresence robot in mind; rotational acceleration is known
for causing the majority of VR sickness, and distance to objects modulates the
optical flow. We ran a within-subjects user study (n = 36, women = 18) in which
the participants watched three panoramic videos recorded in a virtual museum
while aboard an autonomously moving telepresence robot taking three different
paths varying in aspects such as turns, speeds, or distances to walls and
objects. We found a moderate correlation between the users' sickness as
measured by the SSQ and comfort on a 6-point Likert scale across all paths.
However, we detected no association between sickness and the choice of the most
comfortable path, showing that sickness is not the only factor affecting the
comfort of the user. The subjective experience of turn speed did not correlate
with either the SSQ scores or comfort, even though people often mentioned
turning speed as a source of discomfort in the open-ended questions. Through
exploring the open-ended answers more carefully, a possible reason is that the
length and lack of predictability also play a large role in making people
observe turns as uncomfortable. A larger subjective distance from walls and
objects increased comfort and decreased sickness both in quantitative and
qualitative data. Finally, the SSQ subscales and total weighted scores showed
differences by age group and by gender.","['Markku Suomalainen', 'Katherine J. Mimnaugh', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-09-09T11:30:17Z,http://arxiv.org/abs/2109.04177v1,"['cs.HC', 'cs.MM', 'cs.RO']","virtual reality,VR sickness,comfort,autonomously moving telepresence robot,rotational acceleration,distance to objects,trajectories,user study,SSQ scores,open-ended questions"
"Optimizing Performance and Satisfaction in Matching and Movement Tasks
  in Virtual Reality with Interventions Using the Data Visualization Literacy
  Framework","Virtual reality (VR) has seen increased use for training and instruction.
Designers can enable VR users to gain insights into their own performance by
visualizing telemetry data from their actions in VR. Our ability to detect
patterns and trends visually suggests the use of data visualization as a tool
for users to identify strategies for improved performance. Typical tasks in VR
training scenarios are manipulation of 3D objects (e.g., for learning how to
maintain a jet engine) and navigation (e.g., to learn the geography of a
building or landscape before traveling on-site). In this paper, we present the
results of the RUI VR (84 subjects) and Luddy VR studies (68 subjects), where
participants were divided into experiment and control cohorts. All subjects
performed a series of tasks: 44 cube-matching tasks in RUI VR and 48 navigation
tasks through a virtual building in Luddy VR (all divided into two sets). All
Luddy VR subjects used VR gear; RUI VR subjects were divided across three
setups: 2D Desktop (with laptop and mouse), VR Tabletop (in VR, sitting at a
table), and VR Standup (in VR, standing). In an intervention called ""Reflective
phase,"" the experiment cohorts were presented with data visualizations,
designed with the Data Visualization Literacy Framework (DVL-FW), of the data
they generated during the first set of tasks before continuing to the second
part of the study. For Luddy VR, we found that experiment users had
significantly faster completion times in their second trial (p = 0.014) while
scoring higher in a mid-questionnaire about the virtual building (p = 0.009).
For RUI VR, we found no significant differences for completion time and
accuracy between the two cohorts in the VR setups; however, 2D Desktop subjects
in the experiment cohort had significantly higher rotation accuracy as well as
satisfaction (p(rotation) = 0.031, p(satisfaction) = 0.040).","['Andreas Bueckle', 'Kilian Buehling', 'Patrick C. Shih', 'Katy Borner']",2021-12-03T21:06:07Z,http://arxiv.org/abs/2112.02159v1,['cs.HC'],"Virtual reality,Data visualization,Training,Telemetry data,Performance,Satisfaction,Manipulation,Navigation,Data visualization literacy,Framework"
"Structural Health Monitoring of a Foot Bridge in Virtual Reality
  Environment","Ageing civil infrastructure systems require imminent attention before any
failure mechanism becomes critical. Structural Health Monitoring (SHM) is
employed to track inputs and/or responses of structural systems for decision
support. Inspections and structural health monitoring require field visits, and
subsequently expert assessment of critical elements at site, which may be both
time-consuming and costly. Also, fieldwork including visits and inspections may
pose danger, require personal protective equipment and structure closures
during the fieldwork. To address some of these issues, a Virtual Reality (VR)
collaborative application is developed to bring the structure and SHM data from
the field to the office such that many experts from different places can
simultaneously virtually visit the bridge structure for final assessment. In
this work, we present an SHM system in a VR environment that includes the
technical and visual information necessary for the engineers to make decisions
for a footbridge on the campus of the University of Central Florida. In this VR
application, for the visualization stage, UAV (Unmanned Air Vehicle)
photogrammetry and LiDAR (Light Detection and Ranging) methods are used to
capture the bridge. For the technical assessment stage, Finite Element Analysis
(FEA) and Operational Modal Analysis (OMA) from vibration data as part of SHM
are analyzed. To better visualize the dynamic response of the structure, the
operational behaviour from the FEA is reflected on the LiDAR point cloud model
for immersive. The multi-user feature allowing teams to collaborate
simultaneously is essential for decision-making activities. In conclusion, the
proposed VR environment offers the potential to provide beneficial features
with further automated and real-time improvements along with the SHM and FEA
models.","['Furkan Luleci', 'Liangding Li', 'Jiapeng Chi', 'Dirk Reiners', 'Carolina Cruz-Neira', 'F. Necati Catbas']",2021-12-07T03:38:41Z,http://arxiv.org/abs/2112.03470v2,"['cs.HC', 'cs.GR']","Structural Health Monitoring,Virtual Reality,UAV,LiDAR,Finite Element Analysis,Operational Modal Analysis,Vibration Data,Multi-user,Decision-making,Footbridge"
"Improving Emergency Training for Earthquakes Through Immersive Virtual
  Environments and Anxiety Tests: A Case Study","Because of the occurrence of severe and large magnitude earthquakes each
year, earthquake-prone countries suffer considerable financial damage and loss
of life. Teaching essential safety measures will lead to a generation that can
perform basic procedures during an earthquake, which is an important and
effective solution in preventing the loss of life in this natural disaster. In
recent years, virtual reality technology is a tool that has been used to
educate people on safety matters. This paper evaluates the effect of education
and premonition on the incorrect decision-making of residents under the
stressful conditions of an earthquake. For this purpose, a virtual model has
been designed and built from a proposed classroom in a school of the city of
Tehran. Accordingly, two educational scenarios, presented in reality and the
virtual model respectively, were conducted on a statistical sample of 20
students within the range of 20 to 25 years of age. Within the mentioned
sample, the first group of 10 students were taught safety measures in a
traditional classroom. The second group of 10 students participated in a
virtual classroom. Evaluation tests on safety measures against earthquakes were
distributed after two weeks. Furthermore, two self-reporting tests of
Depression, anxiety, stress test (DASS), and Beck Anxiety Inventory (BAI) were
assigned to the second group to evaluate the effect of foresight under two
different scenarios. The results show that educating through virtual reality
technology yields a higher performance level relative to the traditional
approach to education. Additionally, the ability to detect earthquakes ahead of
time is an influential factor in controlling stress and determining the right
decisions should the event occur.","['Mohammad Sadra Rajabi', 'Hosein Taghaddos', 'Mehdi Zahrai']",2022-05-10T15:58:56Z,http://arxiv.org/abs/2205.04993v2,"['cs.HC', 'cs.CY']","emergency training,earthquakes,immersive virtual environments,anxiety tests,safety measures,virtual reality technology,education,decision-making,stress,foresight"
The Body Scaling Effect and Its Impact on Physics Plausibility,"In this study we investigated the effect of body ownership illusion-based
body scaling on physics plausibility in Virtual Reality (VR). Our interest was
in examining whether body ownership illusion-based body scaling could affect
the plausibility of rigid body dynamics similarly to altering VR users' scale
by manipulating their virtual interpupillary distance and viewpoint height. The
procedure involved the conceptual replication of two previous studies. We
investigated physics plausibility with 40 participants under two conditions. In
our synchronous condition, we used visuo-tactile stimuli to elicit a body
ownership illusion of inhabiting an invisible doll-sized body on participants
reclining on an exam table. Our asynchronous condition was otherwise similar,
but the visuo-tactile stimuli were provided asynchronously to prevent the onset
of the body ownership illusion. We were interested in whether the correct
approximation of physics (true physics) or physics that are incorrect and
appearing as if the environment is five times larger instead (movie physics)
appear more realistic to participants as a function of body scale. We found
that movie physics did appear more realistic to participants under the body
ownership illusion condition. However, our hypothesis that true physics would
appear more realistic in the asynchronous condition was unsupported. Our
exploratory analyses revealed that movie physics were perceived as plausible
under both conditions. Moreover, we were not able to replicate previous
findings from literature concerning object size estimations while inhabiting a
small invisible body. However, we found a significant opposite effect regarding
size estimations; the object sizes were on average underestimated during the
synchronous visuo-tactile condition when compared to the asynchronous
condition.","['Matti Pouke', 'Evan G. Center', 'Alexis P. Chambers', 'Sakaria Pouke', 'Timo Ojala', 'Steven M. LaValle']",2022-06-30T12:04:43Z,http://arxiv.org/abs/2206.15218v1,"['cs.MM', 'cs.HC']","body scaling,physics plausibility,virtual reality,body ownership illusion,rigid body dynamics,visuo-tactile stimuli,virtual interpupillary distance,viewpoint height,object size estimations"
"Action-Specific Perception & Performance on a Fitts's Law Task in
  Virtual Reality: The Role of Haptic Feedback","While user's perception & performance are predominantly examined
independently in virtual reality, the Action-Specific Perception (ASP) theory
postulates that the performance of an individual on a task modulates this
individual's spatial & time perception pertinent to the task's components &
procedures. This paper examines the association between performance &
perception & the potential effects that tactile feedback modalities could
generate. This paper reports a user study (N=24), in which participants
performed a Fitts's law target acquisition task by using three feedback
modalities: visual, visuo-electrotactile, & visuo-vibrotactile. The users
completed 3 Target Sizes X 2 Distances X 3 feedback modalities = 18 trials. The
size perception, distance perception, & (movement) time perception were
assessed at the end of each trial. Performance-wise, the results showed that
electrotactile feedback facilitates a significantly better accuracy compared to
vibrotactile & visual feedback, while vibrotactile provided the worst accuracy.
Electrotactile & visual feedback enabled a comparable reaction time, while the
vibrotactile offered a substantially slower reaction time than visual feedback.
Although amongst feedback types the pattern of differences in perceptual
aspects were comparable to performance differences, none of them was
statistically significant. However, performance indeed modulated perception.
Significant action-specific effects on spatial & time perception were detected.
Changes in accuracy modulate both size perception & time perception, while
changes in movement speed modulate distance perception. Also, the index of
difficulty was found to modulate perception. These outcomes highlighted the
importance of haptic feedback on performance, & importantly the significance of
action-specific effects on spatial & time perception in VR, which should be
considered in future VR studies.","['Panagiotis Kourtesis', 'Sebastian Vizcay', 'Maud Marchal', 'Claudio Pacchierotti', 'Ferran Argelaguet']",2022-07-15T11:07:15Z,http://arxiv.org/abs/2207.07400v2,"['cs.HC', 'cs.CY', 'I.3.7; H.5.2; J.3; J.7']","Action-Specific Perception,Performance,Fitts's Law Task,Virtual Reality,Haptic Feedback,Tactile Feedback,Visuo-electrotactile,Visuo-vibrotactile,Target Acquisition Task,Reaction Time"
"Virtual Reality via Object Pose Estimation and Active Learning:
  Realizing Telepresence Robots with Aerial Manipulation Capabilities","This article presents a novel telepresence system for advancing aerial
manipulation in dynamic and unstructured environments. The proposed system not
only features a haptic device, but also a virtual reality (VR) interface that
provides real-time 3D displays of the robot's workspace as well as a haptic
guidance to its remotely located operator. To realize this, multiple sensors
namely a LiDAR, cameras and IMUs are utilized. For processing of the acquired
sensory data, pose estimation pipelines are devised for industrial objects of
both known and unknown geometries. We further propose an active learning
pipeline in order to increase the sample efficiency of a pipeline component
that relies on Deep Neural Networks (DNNs) based object detection. All these
algorithms jointly address various challenges encountered during the execution
of perception tasks in industrial scenarios. In the experiments, exhaustive
ablation studies are provided to validate the proposed pipelines.
Methodologically, these results commonly suggest how an awareness of the
algorithms' own failures and uncertainty (`introspection') can be used tackle
the encountered problems. Moreover, outdoor experiments are conducted to
evaluate the effectiveness of the overall system in enhancing aerial
manipulation capabilities. In particular, with flight campaigns over days and
nights, from spring to winter, and with different users and locations, we
demonstrate over 70 robust executions of pick-and-place, force application and
peg-in-hole tasks with the DLR cable-Suspended Aerial Manipulator (SAM). As a
result, we show the viability of the proposed system in future industrial
applications.","['Jongseok Lee', 'Ribin Balachandran', 'Konstantin Kondak', 'Andre Coelho', 'Marco De Stefano', 'Matthias Humt', 'Jianxiang Feng', 'Tamim Asfour', 'Rudolph Triebel']",2022-10-18T08:42:30Z,http://arxiv.org/abs/2210.09678v2,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.HC', 'cs.LG']","Object Pose Estimation,Active Learning,Telepresence System,Aerial Manipulation,Virtual Reality,Haptic Device,LiDAR,IMUs,Deep Neural Networks,Perception Tasks"
"Investigating the Feasibility of Virtual Reality for Emotion Regulation
  with Youth","The ability to regulate and cope with strong emotions is essential for
maintaining our mental health and well-being. However, learning how to
emotionally regulate can be a bit of a mystery since it is largely an invisible
process and it can be difficult to conjure up strong emotions to practice
regulating them. This is where virtual reality (or VR) comes in. VR is a
computer-generated 3D environment where the user experiences a simulated world
through 360 visuals, stereo audio, and 3D interaction with tracking sensors. VR
is a very visceral experience that feels 'real' even though you know it isn't.
If a virtual ball came flying at your head, you would duck! My past research
shows that we can provide VR experiences that elicit strong emotional reactions
so that people can practice coping and regulating their emotional responses.
For example, I helped create a VR experience of being in nature and then going
into space to see the Earth; it created the emotional reaction of awe and
wonder that led to a deeper connection with our planet. This shows that
emotional VR experiences can impact our emotions and behaviour both in VR and
beyond. My research proposal is to investigate the feasibility of emotion
regulation skills development in VR with teenagers. The idea is to simulate
emotional experiences (like the 1st day of high school) as a means to develop
emotion regulation skills so that they will be able to better cope with their
emotions. I will lead the design, development, and evaluation of this VR
experience and work directly with youth to meet their needs. This proof of
concept prototype is the first step in developing a VR platform that provides
youth with an effective way to regulate their emotions and improve their mental
health from their own homes, which will lead to improvements in education,
socio-emotional, and economic outcomes for youth in Canada and globally.",['Alexandra Kitson'],2022-09-14T16:07:00Z,http://arxiv.org/abs/2212.00002v1,['cs.HC'],"Virtual Reality,Emotion Regulation,3D environment,Mental Health,Well-being,Tracking Sensors,Emotional Reactions,Teenagers,Prototype,VR Platform"
"Virtual Reality Photo-based Tours for Teaching Filipino Vocabulary in an
  Online Class in Japan: Transitioning into the New Normal","When educational institutions worldwide scrambled for ways to continue their
classes during lockdowns caused by the COVID-19 pandemic, the use of
information and communication technology (ICT) for remote teaching has become
widely considered to be a potential solution. As universities raced to
implement emergency remote teaching (ERT) strategies in Japan, some have
explored innovative interventions other than webinar platforms and learning
management systems to bridge the gap caused by restricted mobility among
teachers and learners. One such innovation is virtual reality (VR). VR has been
changing the landscape of higher education because of its ability to ""teleport""
learners to various places by simulating real-world environments in the virtual
world. Some teachers, including the authors of this paper, explored integrating
VR into their activities to address issues caused by geographical limitations
brought about by the heightened restrictions in 2020. Results were largely
encouraging. However, rules started relaxing in the succeeding years as more
people got vaccinated. Thus, some fully online classes in Japan shifted to
blended learning as they moved toward fully returning to in-person classes
prompting educators to modify how they implemented their VR-based
interventions. This paper describes how a class of university students in Japan
who were taking a Filipino language course experienced a VR-based intervention
in blended mode, which was originally prototyped during the peak of the ERT
era. Moreover, adjustments and comparisons regarding methodological
idiosyncrasies and findings between the fully online iteration and the recently
implemented blended one are reported in detail.","['Roberto Bacani Figueroa Jr.', 'Florinda Amparo Palma Gil', 'Hiroshi Taniguchi', 'Joshze Rica Esguerra']",2023-01-05T04:45:35Z,http://arxiv.org/abs/2301.01908v1,['cs.CY'],"virtual reality,photo-based tours,Filipino vocabulary,online class,Japan,blended learning,remote teaching,higher education,ICT,COVID-19 pandemic"
"Real-time simulation of viscoelastic tissue behavior with physics-guided
  deep learning","Finite element methods (FEM) are popular approaches for simulation of soft
tissues with elastic or viscoelastic behavior. However, their usage in
real-time applications, such as in virtual reality surgical training, is
limited by computational cost. In this application scenario, which typically
involves transportable simulators, the computing hardware severely constrains
the size or the level of details of the simulated scene. To address this
limitation, data-driven approaches have been suggested to simulate mechanical
deformations by learning the mapping rules from FEM generated datasets. Herein,
we propose a deep learning method for predicting displacement fields of soft
tissues with viscoelastic properties. The main contribution of this work is the
use of a physics-guided loss function for the optimization of the deep learning
model parameters. The proposed deep learning model is based on convolutional
(CNN) and recurrent layers (LSTM) to predict spatiotemporal variations. It is
augmented with a mass conservation law in the lost function to prevent the
generation of physically inconsistent results. The deep learning model is
trained on a set of FEM datasets that are generated from a commercially
available state-of-the-art numerical neurosurgery simulator. The use of the
physics-guided loss function in a deep learning model has led to a better
generalization in the prediction of deformations in unseen simulation cases.
Moreover, the proposed method achieves a better accuracy over the conventional
CNN models, where improvements were observed in unseen tissue from 8% to 30%
depending on the magnitude of external forces. It is hoped that the present
investigation will help in filling the gap in applying deep learning in virtual
reality simulators, hence improving their computational performance (compared
to FEM simulations) and ultimately their usefulness.","['Mohammad Karami', 'Hervé Lombaert', 'David Rivest-Hénault']",2023-01-11T18:17:10Z,http://arxiv.org/abs/2301.04614v1,"['cs.LG', 'cond-mat.soft', 'physics.comp-ph', 'q-bio.TO']","deep learning,viscoelastic,finite element methods,physics-guided,soft tissues,virtual reality,CNN,LSTM,mass conservation,neurosurgery"
"VR-LENS: Super Learning-based Cybersickness Detection and Explainable
  AI-Guided Deployment in Virtual Reality","A plethora of recent research has proposed several automated methods based on
machine learning (ML) and deep learning (DL) to detect cybersickness in Virtual
reality (VR). However, these detection methods are perceived as computationally
intensive and black-box methods. Thus, those techniques are neither trustworthy
nor practical for deploying on standalone VR head-mounted displays (HMDs). This
work presents an explainable artificial intelligence (XAI)-based framework
VR-LENS for developing cybersickness detection ML models, explaining them,
reducing their size, and deploying them in a Qualcomm Snapdragon 750G
processor-based Samsung A52 device. Specifically, we first develop a novel
super learning-based ensemble ML model for cybersickness detection. Next, we
employ a post-hoc explanation method, such as SHapley Additive exPlanations
(SHAP), Morris Sensitivity Analysis (MSA), Local Interpretable Model-Agnostic
Explanations (LIME), and Partial Dependence Plot (PDP) to explain the expected
results and identify the most dominant features. The super learner
cybersickness model is then retrained using the identified dominant features.
Our proposed method identified eye tracking, player position, and galvanic
skin/heart rate response as the most dominant features for the integrated
sensor, gameplay, and bio-physiological datasets. We also show that the
proposed XAI-guided feature reduction significantly reduces the model training
and inference time by 1.91X and 2.15X while maintaining baseline accuracy. For
instance, using the integrated sensor dataset, our reduced super learner model
outperforms the state-of-the-art works by classifying cybersickness into 4
classes (none, low, medium, and high) with an accuracy of 96% and regressing
(FMS 1-10) with a Root Mean Square Error (RMSE) of 0.03.","['Ripan Kumar Kundu', 'Osama Yahia Elsaid', 'Prasad Calyam', 'Khaza Anuarul Hoque']",2023-02-03T20:15:51Z,http://arxiv.org/abs/2302.01985v1,"['cs.LG', 'cs.AI', 'cs.HC']","machine learning,deep learning,cybersickness detection,explainable artificial intelligence,ensemble model,eye tracking,player position,galvanic skin response,heart rate response,feature reduction"
"Real-Time Recognition of In-Place Body Actions and Head Gestures using
  Only a Head-Mounted Display","Body actions and head gestures are natural interfaces for interaction in
virtual environments. Existing methods for in-place body action recognition
often require hardware more than a head-mounted display (HMD), making body
action interfaces difficult to be introduced to ordinary virtual reality (VR)
users as they usually only possess an HMD. In addition, there lacks a unified
solution to recognize in-place body actions and head gestures. This potentially
hinders the exploration of the use of in-place body actions and head gestures
for novel interaction experiences in virtual environments. We present a unified
two-stream 1-D convolutional neural network (CNN) for recognition of body
actions when a user performs walking-in-place (WIP) and for recognition of head
gestures when a user stands still wearing only an HMD. Compared to previous
approaches, our method does not require specialized hardware and/or additional
tracking devices other than an HMD and can recognize a significantly larger
number of body actions and head gestures than other existing methods. In total,
ten in-place body actions and eight head gestures can be recognized with the
proposed method, which makes this method a readily available body action
interface (head gestures included) for interaction with virtual environments.
We demonstrate one utility of the interface through a virtual locomotion task.
Results show that the present body action interface is reliable in detecting
body actions for the VR locomotion task but is physically demanding compared to
a touch controller interface. The present body action interface is promising
for new VR experiences and applications, especially for VR fitness applications
where workouts are intended.","['Jingbo Zhao', 'Mingjun Shao', 'Yaojun Wang', 'Ruolin Xu']",2023-02-25T14:58:26Z,http://arxiv.org/abs/2302.13096v1,['cs.HC'],"body actions,head gestures,head-mounted display,virtual reality,convolutional neural network,walking-in-place,in-place body actions,virtual environments,locomotion task,VR fitness applications"
Virtual Reality Sickness Reduces Attention During Immersive Experiences,"In this paper, we show that Virtual Reality (VR) sickness is associated with
a reduction in attention, which was detected with the P3b Event-Related
Potential (ERP) component from electroencephalography (EEG) measurements
collected in a dual-task paradigm. We hypothesized that sickness symptoms such
as nausea, eyestrain, and fatigue would reduce the users' capacity to pay
attention to tasks completed in a virtual environment, and that this reduction
in attention would be dynamically reflected in a decrease of the P3b amplitude
while VR sickness was experienced. In a user study, participants were taken on
a tour through a museum in VR along paths with varying amounts of rotation,
shown previously to cause different levels of VR sickness. While paying
attention to the virtual museum (the primary task), participants were asked to
silently count tones of a different frequency (the secondary task). Control
measurements for comparison against the VR sickness conditions were taken when
the users were not wearing the Head-Mounted Display (HMD) and while they were
immersed in VR but not moving through the environment. This exploratory study
shows, across multiple analyses, that the effect mean amplitude of the P3b
collected during the task is associated with both sickness severity measured
after the task with a questionnaire (SSQ) and with the number of counting
errors on the secondary task. Thus, VR sickness may impair attention and task
performance, and these changes in attention can be tracked with ERP measures as
they happen, without asking participants to assess their sickness symptoms in
the moment.","['Katherine J. Mimnaugh', 'Evan G. Center', 'Markku Suomalainen', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Timo Ojala', 'Steven M. LaValle', 'Kara D. Federmeier']",2023-06-23T14:06:13Z,http://arxiv.org/abs/2306.13505v2,"['cs.HC', 'q-bio.NC']","Virtual Reality Sickness,Attention,P3b Event-Related Potential,Electroencephalography,Dual-Task Paradigm,Nausea,Eyestrain,Fatigue,Head-Mounted Display"
"Towards Modeling Software Quality of Virtual Reality Applications from
  Users' Perspectives","Virtual Reality (VR) technology has become increasingly popular in recent
years as a key enabler of the Metaverse. VR applications have unique
characteristics, including the revolutionized human-computer interaction
mechanisms, that distinguish them from traditional software. Hence, user
expectations for the software quality of VR applications diverge from those for
traditional software. Investigating these quality expectations is crucial for
the effective development and maintenance of VR applications, which remains an
under-explored area in prior research.
  To bridge the gap, we conduct the first large-scale empirical study to model
the software quality of VR applications from users' perspectives. To this end,
we analyze 1,132,056 user reviews of 14,150 VR applications across seven app
stores through a semiautomatic review mining approach. We construct a taxonomy
of 12 software quality attributes that are of major concern to VR users. Our
analysis reveals that the VR-specific quality attributes are of utmost
importance to users, which are closely related to the most unique properties of
VR applications like revolutionized interaction mechanisms and immersive
experiences. Our examination of relevant user complaints reveals the major
factors impacting user satisfaction with VR-specific quality attributes. We
identify that poor design or implementation of the movement mechanisms, control
mechanisms, multimedia systems, and physics, can significantly degrade the user
experience. Moreover, we discuss the implications of VR quality assurance for
both developers and researchers to shed light on future work. For instance, we
suggest developers implement sufficient accessibility and comfort options for
users with mobility limitations, sensory impairments, and other specific needs
to customize the interaction mechanisms. Our datasets and results will be
released to facilitate follow-up studies.","['Shuqing Li', 'Lili Wei', 'Yepang Liu', 'Cuiyun Gao', 'Shing-Chi Cheung', 'Michael R. Lyu']",2023-08-13T14:42:47Z,http://arxiv.org/abs/2308.06783v1,"['cs.SE', 'cs.HC', 'D.2.9; H.5.1']","software quality,virtual reality,users' perspectives,Metaverse,human-computer interaction,user expectations,VR applications,user reviews,review mining,software quality attributes,interaction mechanisms"
MetaVRadar: Measuring Metaverse Virtual Reality Network Activity,"The ""metaverse"", wherein users can enter virtual worlds to work, study, play,
shop, socialize, and entertain, is fast becoming a reality, attracting billions
of dollars in investment from companies such as Meta, Microsoft, and Clipo
Labs. Further, virtual reality (VR) headsets from entities like Oculus, HTC,
and Microsoft are rapidly maturing to provide fully immersive experiences to
metaverse users. However, little is known about the network dynamics of
metaverse VR applications in terms of service domains, flow counts, traffic
rates and volumes, content location and latency, etc., which are needed to make
telecommunications network infrastructure ""metaverse ready"". This paper is an
empirical measurement study of metaverse VR network behavior aimed at helping
telecommunications network operators better provision and manage the network to
ensure good user experience. Using illustrative hour-long network traces of
metaverse sessions on the Oculus VR headset, we first develop a categorization
of user activity into distinct states ranging from login home to streetwalking
and event attendance to asset trading, and undertake a detailed analysis of
network traffic per state, identifying unique service domains, protocols, flow
profiles, and volumetric patterns, thereby highlighting the vastly more complex
nature of a metaverse session compared to streaming video or gaming. Armed with
the network behavioral profiles, our second contribution develops a real-time
method MetaVRadar to detect metaverse session and classify the user activity
state leveraging formalized flow signatures and volumetric attributes. Our
third contribution practically implements MetaVRadar, evaluates its accuracy in
our lab environment, and demonstrates its usability in a large university
network so operators can better monitor and plan resources to support requisite
metaverse user experience.","['Minzhao Lyu', 'Rahul Dev Tripathi', 'Vijay Sivaraman']",2024-02-13T08:32:21Z,http://arxiv.org/abs/2402.08286v1,['cs.NI'],"Metaverse,Virtual Reality,Network Activity,Network Dynamics,Telecommunications,Network Infrastructure,Network Traffic,User Activity,Flow Profiles,Volumetric Patterns"
"An Empirical Study on Oculus Virtual Reality Applications: Security and
  Privacy Perspectives","Although Virtual Reality (VR) has accelerated its prevalent adoption in
emerging metaverse applications, it is not a fundamentally new technology. On
one hand, most VR operating systems (OS) are based on off-the-shelf mobile OS.
As a result, VR apps also inherit privacy and security deficiencies from
conventional mobile apps. On the other hand, in contrast to conventional mobile
apps, VR apps can achieve immersive experience via diverse VR devices, such as
head-mounted displays, body sensors, and controllers though achieving this
requires the extensive collection of privacy-sensitive human biometrics.
Moreover, VR apps have been typically implemented by 3D gaming engines (e.g.,
Unity), which also contain intrinsic security vulnerabilities. Inappropriate
use of these technologies may incur privacy leaks and security vulnerabilities
although these issues have not received significant attention compared to the
proliferation of diverse VR apps. In this paper, we develop a security and
privacy assessment tool, namely the VR-SP detector for VR apps. The VR-SP
detector has integrated program static analysis tools and privacy-policy
analysis methods. Using the VR-SP detector, we conduct a comprehensive
empirical study on 500 popular VR apps. We obtain the original apps from the
popular Oculus and SideQuest app stores and extract APK files via the Meta
Oculus Quest 2 device. We evaluate security vulnerabilities and privacy data
leaks of these VR apps by VR app analysis, taint analysis, and privacy-policy
analysis. We find that a number of security vulnerabilities and privacy leaks
widely exist in VR apps. Moreover, our results also reveal conflicting
representations in the privacy policies of these apps and inconsistencies of
the actual data collection with the privacy-policy statements of the apps.
Based on these findings, we make suggestions for the future development of VR
apps.","['Hanyang Guo', 'Hong-Ning Dai', 'Xiapu Luo', 'Zibin Zheng', 'Gengyang Xu', 'Fengliang He']",2024-02-21T13:53:25Z,http://arxiv.org/abs/2402.13815v1,"['cs.SE', 'cs.CR']","Virtual Reality,Security,Privacy,Metaverse,Operating Systems,Biometrics,Gaming Engines,Privacy Policy,Security Vulnerabilities,Data Leaks"
Inception Attacks: Immersive Hijacking in Virtual Reality Systems,"Recent advances in virtual reality (VR) system provide fully immersive
interactions that connect users with online resources, applications, and each
other. Yet these immersive interfaces can make it easier for users to fall prey
to a new type of security attacks. We introduce the inception attack, where an
attacker controls and manipulates a user's interaction with their VR
environment and applications, by trapping them inside a malicious VR
application that masquerades as the full VR system. Once trapped in an
""inception VR layer"", all of the user's interactions with remote servers,
network applications, and other VR users can be recorded or modified without
their knowledge. This enables traditional attacks (recording passwords and
modifying user actions in flight), as well as VR interaction attacks, where
(with generative AI tools) two VR users interacting can experience two
dramatically different conversations.
  In this paper, we introduce inception attacks and their design, and describe
our implementation that works on all Meta Quest VR headsets. Our implementation
of inception attacks includes a cloned version of the Meta Quest browser that
can modify data as it's displayed to the user, and alter user input en route to
the server (e.g. modify amount of $ transferred in a banking session). Our
implementation also includes a cloned VRChat app, where an attacker can
eavesdrop and modify live audio between two VR users. We then conduct a study
on users with a range of VR experiences, execute the inception attack during
their session, and debrief them about their experiences. Only 37% of users
noticed the momentary visual ""glitch"" when the inception attack began, and all
but 1 user attributed it to imperfections in the VR platform. Finally, we
consider and discuss efficacy and tradeoffs for a wide range of potential
inception defenses.","['Zhuolin Yang', 'Cathy Yuanchen Li', 'Arman Bhalla', 'Ben Y. Zhao', 'Haitao Zheng']",2024-03-08T23:22:16Z,http://arxiv.org/abs/2403.05721v1,['cs.CR'],"immersive interactions,security attacks,inception attack,VR environment,malicious VR application,generative AI tools,Meta Quest VR headsets,VRChat app,inception defense"
"Evaluating the efficacy of haptic feedback, 360°
  treadmill-integrated Virtual Reality framework and longitudinal training on
  decision-making performance in a complex search-and-shoot simulation","Virtual Reality (VR) has made significant strides, offering users a multitude
of ways to interact with virtual environments. Each sensory modality in VR
provides distinct inputs and interactions, enhancing the user's immersion and
presence. However, the potential of additional sensory modalities, such as
haptic feedback and 360{\deg} locomotion, to improve decision-making
performance has not been thoroughly investigated. This study addresses this gap
by evaluating the impact of a haptic feedback, 360{\deg} locomotion-integrated
VR framework and longitudinal, heterogeneous training on decision-making
performance in a complex search-and-shoot simulation. The study involved 32
participants from a defence simulation base in India, who were randomly divided
into two groups: experimental (haptic feedback, 360{\deg} locomotion-integrated
VR framework with longitudinal, heterogeneous training) and placebo control
(longitudinal, heterogeneous VR training without extrasensory modalities). The
experiment lasted 10 days. On Day 1, all subjects executed a search-and-shoot
simulation closely replicating the elements/situations in the real world. From
Day 2 to Day 9, the subjects underwent heterogeneous training, imparted by the
design of various complexity levels in the simulation using changes in
behavioral attributes/artificial intelligence of the enemies. On Day 10, they
repeated the search-and-shoot simulation executed on Day 1. The results showed
that the experimental group experienced a gradual increase in presence,
immersion, and engagement compared to the placebo control group. However, there
was no significant difference in decision-making performance between the two
groups on day 10. We intend to use these findings to design multisensory VR
training frameworks that enhance engagement levels and decision-making
performance.","['Akash K Rao', 'Arnav Bhavsar', 'Shubhajit Roy Chowdhury', 'Sushil Chandra', 'Ramsingh Negi', 'Prakash Duraisamy', 'Varun Dutt']",2024-04-14T05:33:26Z,http://arxiv.org/abs/2404.09147v1,['cs.HC'],"haptic feedback,360°,Virtual Reality,decision-making performance,simulation,longitudinal training,multisensory,immersive,presence,engagement"
"AstroMD. A multi-dimensional data analysis tool for astrophysical
  simulations","Over the past few years, the role of visualization for scientific purpose has
grown up enormously. Astronomy makes an extended use of visualization
techniques to analyze data, and scientific visualization has became a
fundamental part of modern researches in Astronomy. With the evolution of high
performance computers, numerical simulations have assumed a great role in the
scientific investigation, allowing the user to run simulation with higher and
higher resolution. Data produced in these simulations are often
multi-dimensional arrays with several physical quantities. These data are very
hard to manage and to analyze efficiently. Consequently the data analysis and
visualization tools must follow the new requirements of the research. AstroMD
is a tool for data analysis and visualization of astrophysical data and can
manage different physical quantities and multi-dimensional data sets. The tool
uses virtual reality techniques by which the user has the impression of
travelling through a computer-based multi-dimensional model. AstroMD is a
freely available tool for the whole astronomical community
(http://www.cineca.it/astromd/).","['U. Becciani', 'V. Antonuccio-Delogu', 'C. Gheller', 'L. Calori', 'F. Buonomo', 'S. Imboden']",2000-06-28T08:56:33Z,http://arxiv.org/abs/astro-ph/0006402v1,['astro-ph'],"data analysis,astrophysical simulations,visualization,astronomy,scientific visualization,high performance computers,numerical simulations,multi-dimensional arrays,physical quantities,virtual reality"
Immersive 4D Interactive Visualization of Large-Scale Simulations,"In dense clusters a bewildering variety of interactions between stars can be
observed, ranging from simple encounters to collisions and other mass-transfer
encounters. With faster and special-purpose computers like GRAPE, the amount of
data per simulation is now exceeding 1TB. Visualization of such data has now
become a complex 4D data-mining problem, combining space and time, and finding
interesting events in these large datasets. We have recently starting using the
virtual reality simulator, installed in the Hayden Planetarium in the American
Museum for Natural History, to tackle some of these problem. This work
(http://www.astro.umd.edu/nemo/amnh/) reports on our first ``observations'',
modifications needed for our specific experiments, and perhaps field ideas for
other fields in science which can benefit from such immersion. We also discuss
how our normal analysis programs can be interfaced with this kind of
visualization.","['Peter Teuben', 'Piet Hut', 'Stuart Levy', 'Jun Makino', 'Steve McMillan', 'Simon Portegies Zwart', 'Mike Shara', 'Carter Emmart']",2001-01-18T17:13:11Z,http://arxiv.org/abs/astro-ph/0101334v1,['astro-ph'],"immersive visualization,4D,large-scale simulations,interactions,collisions,data mining,virtual reality simulator,Hayden Planetarium,data analysis"
The Starlab Environment for Dense Stellar Systems,"Traditionally, a simulation of a dense stellar system required choosing an
initial model, running an integrator, and analyzing the output. Almost all of
the effort went into writing a clever integrator that could handle binaries,
triples and encounters between various multiple systems efficiently. Recently,
the scope and complexity of these simulations has increased dramatically, for
three reasons: 1) the sheer size of the data sets, measured in Terabytes, make
traditional `awking and grepping' of a single output file impractical; 2) the
addition of stellar evolution data brings qualitatively new challenges to the
data reduction; 3) increased realism of the simulations invites realistic forms
of `SOS': Simulations of Observations of Simulations, to be compared directly
with observations. We are now witnessing a shift toward the construction of
archives as well as tailored forms of visualization including the use of
virtual reality simulators and planetarium domes, and a coupling of both with
budding efforts in constructing virtual observatories. This review describes
these new trends, presenting Starlab as the first example of a full software
environment for realistic large-scale simulations of dense stellar systems.",['Piet Hut'],2002-04-25T16:53:23Z,http://arxiv.org/abs/astro-ph/0204431v1,['astro-ph'],"simulation,dense stellar systems,integrator,binaries,triples,encounters,data sets,stellar evolution,data reduction,virtual reality"
"Seeing the Forest in the Tree: Applying VRML to Mathematical Problems in
  Number Theory","We show how VRML (Virtual Reality Modeling Language) can provide potentially
powerful insight into the 3x + 1 problem via the introduction of a unique
geometrical object, called the 'G-cell', akin to a fractal generator. We
present an example of a VRML world developed programmatically with the G-cell.
The role of VRML as a tool for furthering the understanding the 3x+1 problem is
potentially significant for several reasons: a) VRML permits the observer to
zoom into the geometric structure at all scales (up to limitations of the
computing platform). b) VRML enables rotation to alter comparative visual
perspective (similar to Tukey's data-spinning concept). c) VRML facilitates the
demonstration of interesting tree features between collaborators on the
internet who might otherwise have difficulty conveying their ideas
unambiguously. d) VRML promises to reveal any dimensional dependencies among
3x+1 sequences.",['Neil J. Gunther'],1999-12-31T18:36:38Z,http://arxiv.org/abs/cs/9912021v2,"['cs.MS', 'cs.CE', 'G.2.2;G.4;H.5.1;I.3.2;I.3.7;I.6.8;I.7.2;J.2;K.3.1']","VRML,Mathematical Problems,Number Theory,3x + 1 problem,G-cell,Geometrical Object,Fractal Generator,Computing Platform,Rotation,Dimensional Dependencies"
Design and Evaluation of Mechanisms for a Multicomputer Object Store,"Multicomputers have traditionally been viewed as powerful compute engines. It
is from this perspective that they have been applied to various problems in
order to achieve significant performance gains. There are many applications for
which this compute intensive approach is only a partial solution. CAD, virtual
reality, simulation, document management and analysis all require timely access
to large amounts of data. This thesis investigates the use of the object store
paradigm to harness the large distributed memories found on multicomputers. The
design, implementation, and evaluation of a distributed object server on the
Fujitsu AP1000 is described. The performance of the distributed object server
under example applications, mainly physical simulation problems, is used to
evaluate solutions to the problems of client space recovery, object migration,
and coherence maintenance.
  The distributed object server follows the client-server model, allows object
replication, and uses binary semaphores as a concurrency control measure.
Instrumentation of the server under these applications supports several
conclusions: client space recovery should be dynamically controlled by the
application, predictively prefetching object replicas yields benefits in
restricted circumstances, object migration by storage unit (segment) is not
generally suitable where there are many objects per storage unit, and binary
semaphores are an expensive concurrency control measure in this environment.",['Lex Weaver'],2000-04-18T17:29:06Z,http://arxiv.org/abs/cs/0004010v1,"['cs.DC', 'cs.DB', 'H.3.4; H.2.4']","Multicomputer,Object Store,CAD,Virtual Reality,Simulation,Document Management,Object Server,Fujitsu AP1000,Binary Semaphores,Concurrency Control"
Verbal Interactions in Virtual Worlds,"We first discuss respective advantages of language interaction in virtual
worlds and of using 3D images in dialogue systems. Then, we describe an example
of a verbal interaction system in virtual reality: Ulysse. Ulysse is a
conversational agent that helps a user navigate in virtual worlds. It has been
designed to be embedded in the representation of a participant of a virtual
conference and it responds positively to motion orders. Ulysse navigates the
user's viewpoint on his/her behalf in the virtual world. On tests we carried
out, we discovered that users, novices as well as experienced ones have
difficulties moving in a 3D environment. Agents such as Ulysse enable a user to
carry out navigation motions that would have been impossible with classical
interaction devices. From the whole Ulysse system, we have stripped off a
skeleton architecture that we have ported to VRML, Java, and Prolog. We hope
this skeleton helps the design of language applications in virtual worlds.",['Pierre Nugues'],2000-06-13T09:50:03Z,http://arxiv.org/abs/cs/0006027v1,"['cs.CL', 'cs.HC', 'H.5.2; I.2.7']","virtual worlds,language interaction,3D images,dialogue systems,verbal interaction system,conversational agent,virtual reality,Ulysse,navigation motions,classical interaction devices"
Data sonification and sound visualization,"This article describes a collaborative project between researchers in the
Mathematics and Computer Science Division at Argonne National Laboratory and
the Computer Music Project of the University of Illinois at Urbana-Champaign.
The project focuses on the use of sound for the exploration and analysis of
complex data sets in scientific computing. The article addresses digital sound
synthesis in the context of DIASS (Digital Instrument for Additive Sound
Synthesis) and sound visualization in a virtual-reality environment by means of
M4CAVE. It describes the procedures and preliminary results of some experiments
in scientific sonification and sound visualization.","['Hans G. Kaper', 'Sever Tipei', 'Elizabeth Wiebel']",2000-07-05T21:26:48Z,http://arxiv.org/abs/cs/0007007v1,"['cs.SD', 'cs.HC', 'cs.MM', 'H.5.5']","data sonification,sound visualization,sound synthesis,scientific computing,digital sound synthesis,DIASS,virtual-reality,M4CAVE,scientific sonification,preliminary results"
"Generating Multilingual Personalized Descriptions of Museum Exhibits -
  The M-PIRO Project","This paper provides an overall presentation of the M-PIRO project. M-PIRO is
developing technology that will allow museums to generate automatically textual
or spoken descriptions of exhibits for collections available over the Web or in
virtual reality environments. The descriptions are generated in several
languages from information in a language-independent database and small
fragments of text, and they can be tailored according to the backgrounds of the
users, their ages, and their previous interaction with the system. An authoring
tool allows museum curators to update the system's database and to control the
language and content of the resulting descriptions. Although the project is
still in progress, a Web-based demonstrator that supports English, Greek and
Italian is already available, and it is used throughout the paper to highlight
the capabilities of the emerging technology.","['Ion Androutsopoulos', 'Vassiliki Kokkinaki', 'Aggeliki Dimitromanolaki', 'Jo Calder', 'Jon Oberlander', 'Elena Not']",2001-10-29T16:55:32Z,http://arxiv.org/abs/cs/0110057v1,"['cs.CL', 'cs.AI', 'I.2.7; H.5.2; H.5.4; I.7.4']","multilingual,personalized,descriptions,museum exhibits,M-PIRO project,technology,language-independent database,authoring tool,virtual reality environments"
Subjective Evaluation of Forms in an Immersive Environment,"User's perception of product, by essence subjective, is a major topic in
marketing and industrial design. Many methods, based on users' tests, are used
so as to characterise this perception. We are interested in three main methods:
multidimensional scaling, semantic differential method, and preference mapping.
These methods are used to built a perceptual space, in order to position the
new product, to specify requirements by the study of user's preferences, to
evaluate some product attributes, related in particular to style (aesthetic).
These early stages of the design are primordial for a good orientation of the
project. In parallel, virtual reality tools and interfaces are more and more
efficient for suggesting to the user complex feelings, and creating in this way
various levels of perceptions. In this article, we present on an example the
use of multidimensional scaling, semantic differential method and preference
mapping for the subjective assessment of virtual products. These products,
which geometrical form is variable, are defined with a CAD model and are
proposed to the user with a spacemouse and stereoscopic glasses. Advantages and
limitations of such evaluation is next discussed..","['Jean-François Petiot', 'Damien Chablat']",2007-05-10T06:54:11Z,http://arxiv.org/abs/0705.1395v1,"['cs.HC', 'cs.RO']","subjective evaluation,immersive environment,multidimensional scaling,semantic differential method,preference mapping,perceptual space,virtual reality,CAD model,spacemouse,stereoscopic glasses"
"A distributed Approach for Access and Visibility Task under Ergonomic
  Constraints with a Manikin in a Virtual Reality Environment","This paper presents a new method, based on a multi-agent system and on
digital mock-up technology, to assess an efficient path planner for a manikin
for access and visibility task under ergonomic constraints. In order to solve
this problem, the human operator is integrated in the process optimization to
contribute to a global perception of the environment. This operator cooperates,
in real-time, with several automatic local elementary agents. The result of
this work validates solutions brought by digital mock-up and that can be
applied to simulate maintenance task.","['Florence Bidault', 'Damien Chablat', 'Patrick Chedmail', 'Laurent Pino']",2007-07-13T15:34:22Z,http://arxiv.org/abs/0707.2042v1,['cs.RO'],"distributed approach,access task,visibility task,ergonomic constraints,manikin,virtual reality environment,multi-agent system,digital mock-up technology,path planner,maintenance task"
Prototyping Bio-Nanorobots using Molecular Dynamics Simulation,"This paper presents a molecular mechanics study using a molecular dynamics
software (NAMD) coupled to virtual reality (VR) techniques for intuitive
Bio-NanoRobotic prototyping. Using simulated Bio-Nano environments in VR, the
operator can design and characterize through physical simulation and 3-D
visualization the behavior of Bio-NanoRobotic components and structures. The
main novelty of the proposed simulations is based on the characterization of
stiffness performances of passive joints-based deca-alanine protein molecule
and active joints-based viral protein motor (VPL) in their native environment.
Their use as elementary Bio-NanoRobotic components (1 dof platform) are also
simulated and the results discussed.","['Mustapha Hamdi', 'Gaurav Sharma', 'A. Ferreira', 'Constantinos Mavroidis']",2007-08-14T09:05:40Z,http://arxiv.org/abs/0708.1840v1,['physics.bio-ph'],"molecular dynamics simulation,molecular mechanics,virtual reality,Bio-NanoRobotic prototyping,Bio-Nano environments,passive joints,active joints,deca-alanine protein molecule,viral protein motor,stiffness performances"
"Gaze as a Supplementary Modality for Interacting with Ambient
  Intelligence Environments","We present our current research on the implementation of gaze as an efficient
and usable pointing modality supplementary to speech, for interacting with
augmented objects in our daily environment or large displays, especially
immersive virtual reality environments, such as reality centres and caves. We
are also addressing issues relating to the use of gaze as the main interaction
input modality. We have designed and developed two operational user interfaces:
one for providing motor-disabled users with easy gaze-based access to map
applications and graphical software; the other for iteratively testing and
improving the usability of gaze-contingent displays.","['Daniel Gepner', 'Jérôme Simonin', 'Noëlle Carbonell']",2007-08-26T18:53:41Z,http://arxiv.org/abs/0708.3505v1,['cs.HC'],"gaze,supplementary modality,ambient intelligence environments,pointing modality,augmented objects,virtual reality,reality centres,caves,user interfaces,gaze-contingent displays"
Low Dimensional Embedding of fMRI datasets,"We propose a novel method to embed a functional magnetic resonance imaging
(fMRI) dataset in a low-dimensional space. The embedding optimally preserves
the local functional coupling between fMRI time series and provides a
low-dimensional coordinate system for detecting activated voxels. To compute
the embedding, we build a graph of functionally connected voxels. We use the
commute time, instead of the geodesic distance, to measure functional distances
on the graph. Because the commute time can be computed directly from the
eigenvectors of (a symmetric version) the graph probability transition matrix,
we use these eigenvectors to embed the dataset in low dimensions. After
clustering the datasets in low dimensions, coherent structures emerge that can
be easily interpreted. We performed an extensive evaluation of our method
comparing it to linear and nonlinear techniques using synthetic datasets and in
vivo datasets. We analyzed datasets from the EBC competition obtained with
subjects interacting in an urban virtual reality environment. Our exploratory
approach is able to detect independently visual areas (V1/V2, V5/MT), auditory
areas, and language areas. Our method can be used to analyze fMRI collected
during ``natural stimuli''.","['Xilin Shen', 'François G. Meyer']",2007-09-19T22:56:15Z,http://arxiv.org/abs/0709.3121v2,"['stat.ML', 'q-bio.NC', 'stat.AP']","functional magnetic resonance imaging,fMRI,low-dimensional embedding,functional coupling,voxels,commute time,eigenvectors,clustering,dataset analysis,natural stimuli"
Implementation of perception and action at nanoscale,"Real time combination of nanosensors and nanoactuators with virtual reality
environment and multisensorial interfaces enable us to efficiently act and
perceive at nanoscale. Advanced manipulation of nanoobjects and new strategies
for scientific education are the key motivations. We have no existing intuitive
representation of the nanoworld ruled by laws foreign to our experience. A
central challenge is then the construction of nanoworld simulacrum that we can
start to visit and to explore. In this nanoworld simulacrum, object
identifications will be based on probed entity physical and chemical intrinsic
properties, on their interactions with sensors and on the final choices made in
building a multisensorial interface so that these objects become coherent
elements of the human sphere of action and perception. Here we describe a 1D
virtual nanomanipulator, part of the Cit\'e des Sciences EXPO NANO in Paris,
that is the first realization based on this program.","['Sylvain Marlière', 'Jean Loup Florens', 'Florence Marchi', 'Annie Luciani', 'Joel Chevrier']",2008-01-04T13:38:39Z,http://arxiv.org/abs/0801.0678v1,"['cs.RO', 'cs.HC']","nanosensors,nanoactuators,virtual reality,multisensorial interfaces,nanoobjects,scientific education,nanoworld,simulacrum,physical properties,chemical properties"
"Haptic sensing for MEMS with application for cantilever and Casimir
  effect","This paper presents an implementation of the Cosserat theory into haptic
sensing technologies for real-time simulation of microstructures. Cosserat
theory is chosen instead of the classical theory of elasticity for a better
representation of stress, especially in the nonlinear regime. The use of
Cosserat theory leads to a reduction of the complexity of the modelling and
thus increases its capability for real time simulation which is indispensable
for haptic technologies. The incorporation of Cosserat theory into haptic
sensing technology enables the designer to simulate in real-time the components
in a virtual reality environment (VRE) which can enable virtual manufacturing
and prototyping. The software tool created as a result of this methodology
demonstrates the feasibility of the proposed model. As test demonstrators, a
cantilever microbeam and microbridge undergoing bending in VRE are presented.","['M. Calis', 'Marc Desmulliez']",2008-05-07T09:43:45Z,http://arxiv.org/abs/0805.0929v1,['cs.OH'],"haptic sensing,MEMS,Cosserat theory,real-time simulation,microstructures,virtual reality environment,VRE,cantilever,Casimir effect"
"BiopSym: a simulator for enhanced learning of ultrasound-guided prostate
  biopsy","This paper describes a simulator of ultrasound-guided prostate biopsies for
cancer diagnosis. When performing biopsy series, the clinician has to move the
ultrasound probe and to mentally integrate the real-time bi-dimensional images
into a three-dimensional (3D) representation of the anatomical environment.
Such a 3D representation is necessary to sample regularly the prostate in order
to maximize the probability of detecting a cancer if any. To make the training
of young physicians easier and faster we developed a simulator that combines
images computed from three-dimensional ultrasound recorded data to haptic
feedback. The paper presents the first version of this simulator.","['Stefano Sclaverano', 'Grégoire Chevreau', 'Lucile Vadcard', 'Pierre Mozer', 'Jocelyne Troccaz']",2008-12-17T09:22:47Z,http://arxiv.org/abs/0812.3226v1,['cs.RO'],"simulator,ultrasound-guided,prostate biopsy,cancer diagnosis,ultrasound probe,bi-dimensional images,three-dimensional representation,anatomical environment,haptic feedback"
"A Distributed Software Architecture for Collaborative Teleoperation
  based on a VR Platform and Web Application Interoperability","Augmented Reality and Virtual Reality can provide to a Human Operator (HO) a
real help to complete complex tasks, such as robot teleoperation and
cooperative teleassistance. Using appropriate augmentations, the HO can
interact faster, safer and easier with the remote real world. In this paper, we
present an extension of an existing distributed software and network
architecture for collaborative teleoperation based on networked human-scaled
mixed reality and mobile platform. The first teleoperation system was composed
by a VR application and a Web application. However the 2 systems cannot be used
together and it is impossible to control a distant robot simultaneously. Our
goal is to update the teleoperation system to permit a heterogeneous
collaborative teleoperation between the 2 platforms. An important feature of
this interface is based on different Mobile platforms to control one or many
robots.","['Christophe Domingues', 'Samir Otmane', 'Frédéric Davesne', 'Malik Mallem']",2009-04-14T11:21:47Z,http://arxiv.org/abs/0904.2096v1,"['cs.HC', 'cs.GR', 'cs.MM', 'cs.RO']","Distributed software architecture,Collaborative teleoperation,VR platform,Web application interoperability,Augmented Reality,Virtual Reality,Human Operator,Teleassistance,Mixed reality,Mobile platform"
"Extended Range Telepresence for Evacuation Training in Pedestrian
  Simulations","In this contribution, we propose a new framework to evaluate pedestrian
simula-tions by using Extended Range Telepresence. Telepresence is used as a
virtual reality walking simulator, which provides the user with a realistic
impression of being present and walking in a virtual environment that is much
larger than the real physical environment, in which the user actually walks.
The validation of the simulation is performed by comparing motion data of the
telepresent user with simulated data at some points of the simulation. The use
of haptic feedback from the simulation makes the framework suitable for
training in emergency situations.","['Antonia Perez Arias', 'Uwe D. Hanebeck', 'Peter Ehrhardt', 'Stefan Hengst', 'Tobias Kretz', 'Peter Vortisch']",2010-02-19T16:22:04Z,http://arxiv.org/abs/1002.3770v1,"['cs.HC', 'cs.MA']","Extended Range Telepresence,Evacuation Training,Pedestrian Simulations,Virtual Reality,Walking Simulator,Haptic Feedback,Motion Data,Simulation Validation,Emergency Situations"
Fundamentals of Mathematical Theory of Emotional Robots,"In this book we introduce a mathematically formalized concept of emotion,
robot's education and other psychological parameters of intelligent robots. We
also introduce unitless coefficients characterizing an emotional memory of a
robot. Besides, the effect of a robot's memory upon its emotional behavior is
studied, and theorems defining fellowship and conflicts in groups of robots are
proved. Also unitless parameters describing emotional states of those groups
are introduced, and a rule of making alternative (binary) decisions based on
emotional selection is given. We introduce a concept of equivalent educational
process for robots and a concept of efficiency coefficient of an educational
process, and suggest an algorithm of emotional contacts within a group of
robots. And generally, we present and describe a model of a virtual reality
with emotional robots. The book is meant for mathematical modeling specialists
and emotional robot software developers.","['Oleg Pensky', 'Kirill Chernikov']",2010-10-17T12:36:48Z,http://arxiv.org/abs/1011.1841v1,"['cs.RO', 'cs.AI', '68T40', 'I.6.5']","mathematical theory,emotional robots,emotional memory,robot's education,psychological parameters,unitless coefficients,emotional behavior,fellowship,conflicts,emotional states"
"Biopsym : a learning environment for transrectal ultrasound guided
  prostate biopsies","This paper describes a learning environment for image-guided prostate
biopsies in cancer diagnosis; it is based on an ultrasound probe simulator
virtually exploring real datasets obtained from patients. The aim is to make
the training of young physicians easier and faster with a tool that combines
lectures, biopsy simulations and recommended exercises to master this medical
gesture. It will particularly help acquiring the three-dimensional
representation of the prostate needed for practicing biopsy sequences. The
simulator uses a haptic feedback to compute the position of the virtual probe
from three-dimensional (3D) ultrasound recorded data. This paper presents the
current version of this learning environment.","['Janssoone Thomas', 'Grégoire Chevreau', 'Lucile Vadcard', 'Pierre Mozer', 'Jocelyne Troccaz']",2010-11-08T15:54:10Z,http://arxiv.org/abs/1011.2107v1,['cs.OH'],"transrectal ultrasound,guided biopsy,prostate,learning environment,simulator,haptic feedback,3D ultrasound,cancer diagnosis,medical gesture"
"A distributed Approach for Access and Visibility Task with a Manikin and
  a Robot in a Virtual Reality Environment","This paper presents a new method, based on a multi-agent system and on a
digital mock-up technology, to assess an efficient path planner for a manikin
or a robot for access and visibility task taking into account ergonomic
constraints or joint and mechanical limits. In order to solve this problem, the
human operator is integrated in the process optimization to contribute to a
global perception of the environment. This operator cooperates, in real-time,
with several automatic local elementary agents. The result of this work
validates solutions through the digital mock-up; it can be applied to simulate
maintenability and mountability tasks.","['Patrick Chedmail', 'Damien Chablat', 'Christophe Le Roy']",2011-04-05T09:16:34Z,http://arxiv.org/abs/1104.0780v1,['cs.RO'],"distributed approach,visibility task,manikin,robot,virtual reality environment,multi-agent system,digital mock-up technology,path planner,ergonomic constraints,mechanical limits"
"Can virtual reality predict body part discomfort and performance of
  people in realistic world for assembling tasks?","This paper presents our work on relationship of evaluation results between
virtual environment (VE) and realistic environment (RE) for assembling tasks.
Evaluation results consist of subjective results (BPD and RPE) and objective
results (posture and physical performance). Same tasks were performed with same
experimental configurations and evaluation results were measured in RE and VE
respectively. Then these evaluation results were compared. Slight difference of
posture between VE and RE was found but not great difference of effect on
people according to conventional ergonomics posture assessment method.
Correlation of BPD and performance results between VE and RE are found by
linear regression method. Moreover, results of BPD, physical performance, and
RPE in VE are higher than that in RE with significant difference. Furthermore,
these results indicates that subjects feel more discomfort and fatigue in VE
than RE because of additional effort required in VE.","['Bo Hu', 'Liang Ma', 'Wei Zhang', 'Gaverial Salvendy', 'Damien Chablat', 'Fouad Bennis']",2011-04-06T20:40:38Z,http://arxiv.org/abs/1104.1191v1,['cs.RO'],"virtual reality,body part discomfort,performance,assembling tasks,evaluation results,subjective results,objective results,posture,physical performance,correlation,linear regression"
Open-loop multi-channel inversion of room impulse response,"This paper considers methods for audio display in a CAVE-type virtual reality
theater, a 3 m cube with displays covering all six rigid faces. Headphones are
possible since the user's headgear continuously measures ear positions, but
loudspeakers are preferable since they enhance the sense of total immersion.
The proposed solution consists of open-loop acoustic point control. The
transfer function, a matrix of room frequency responses from the loudspeakers
to the ears of the user, is inverted using multi-channel inversion methods, to
create exactly the desired sound field at the user's ears. The inverse transfer
function is constructed from impulse responses simulated by the image source
method. This technique is validated by measuring a 2x2 matrix transfer
function, simulating a transfer function with the same geometry, and filtering
the measured transfer function through the inverse of the simulation. Since
accuracy of the image source method decreases with time, inversion performance
is improved by windowing the simulated response prior to inversion. Parameters
of the simulation and inversion are adjusted to minimize residual reverberant
energy; the best-case dereverberation ratio is 10 dB.","['Bowon Lee', 'Camille Goudeseune', 'Mark A. Hasegawa-Johnson']",2011-06-06T21:03:31Z,http://arxiv.org/abs/1106.1199v1,"['cs.SD', '76Q05', 'H.5.5; G.2.3; I.5.4']","audio display,CAVE-type virtual reality theater,acoustic point control,transfer function,multi-channel inversion,impulse responses,image source method,dereverberation ratio,room impulse response,loudspeakers"
Updatable Queue Protocol Based On TCP For Virtual Reality Environment,"The variance in number and types of tasks required to be implemented within
Distributed Virtual Environments (DVE) highlights the needs for communication
protocols can achieve consistency. In addition, these applications have to
handle an increasing number of participants and deal with the difficult problem
of scalability. Moreover, the real-time requirements of these applications make
the scalability problem more difficult to solve. In this paper, we have
implemented Updatable Queue Abstraction protocol (UQA) on TCP (TCP-UQA) and
compared it with original TCP, UDP, and Updatable Queue Abstraction based on
UDP (UDP-UQA) protocols. Results showed that TCP-UQA was the best in queue
management.","[""Ala'a Z. Al-Howaide"", 'Mohammed I. Khaleel', 'Ayad M. Salhieh']",2011-11-07T16:12:31Z,http://arxiv.org/abs/1111.1628v1,"['cs.NI', 'cs.PF']","TCP,Virtual Reality,Communication Protocols,Distributed Virtual Environments,Scalability,Real-time Requirements,Updatable Queue Abstraction,UDP"
"Simulation Techniques and Prosthetic Approach Towards Biologically
  Efficient Artificial Sense Organs- An Overview","An overview of the applications of control theory to prosthetic sense organs
including the senses of vision, taste and odor is being presented in this
paper. Simulation aspect nowadays has been the centre of research in the field
of prosthesis. There have been various successful applications of prosthetic
organs, in case of natural biological organs dis-functioning patients.
Simulation aspects and control modeling are indispensible for knowing system
performance, and to generate an original approach of artificial organs. This
overview focuses mainly on control techniques, by far a theoretical overview
and fusion of artificial sense organs trying to mimic the efficacies of
biologically active sensory organs. Keywords: virtual reality, prosthetic
vision, artificial","['Biswarup Neogi', 'Soumya Ghosal', 'Soumyajit Mukherjee', 'Achintya Das', 'D. N. Tibarewala']",2011-11-07T19:16:16Z,http://arxiv.org/abs/1111.1684v1,"['cs.RO', 'cs.SY']","control theory,prosthetic sense organs,simulation,prosthesis,artificial organs,biological organs,artificial sense organs,control techniques,sensory organs"
In Vivo Quantification of Clot Formation in Extracorporeal Circuits,"Clot formation is a common complication in extracorporeal circuits. In this
paper we describe a novel method for clot formation analysis using image
processing. We assembled a closed extracorporeal circuit and circulated blood
at varying speeds. Blood filters were placed in downstream of the flow, and
clotting agents were added to the circuit. Digital images of the filter were
subsequently taken, and image analysis was applied to calculate the density of
the clot. Our results show a significant correlation between the cumulative
size of the clots, the density measure of the clot based on image analysis, and
flow duration in the system.","['Omid David', 'Rabin Gerrah']",2012-12-21T14:20:05Z,http://arxiv.org/abs/1212.5454v1,"['cs.CV', 'physics.med-ph']","clot formation,extracorporeal circuits,image processing,blood filters,clotting agents,digital images,image analysis,density measure,flow duration"
"A Review into eHealth Services and Therapies: Potential for Virtual
  Therapeutic Communities - Supporting People with Severe Personality Disorder","eHealth has expanded hugely over the last fifteen years and continues to
evolve, providing greater benefits for patients, health care professionals and
providers alike. The technologies that support these systems have become
increasingly more sophisticated and have progressed significantly from standard
databases, used for patient records, to highly advanced Virtual Reality (VR)
systems for the treatment of complex mental health illnesses. The scope of this
paper is to initially explore e-Health, particularly in relation to
technologies supporting the treatment and management of wellbeing in mental
health. It then provides a case study of how technology in e-Health can lend
itself to an application that could support and maintain the wellbeing of
people with a severe mental illness. The case study uses Borderline Personality
Disorder as an example, but could be applicable in many other areas, including
depression, anxiety, addiction and PTSD. This type of application demonstrates
how e-Health can empower the individuals using it but also potentially reducing
the impact upon health care providers and services.","['Alice Good', 'Arunasalam Sambhanthan']",2013-02-22T07:11:45Z,http://arxiv.org/abs/1302.5499v1,['cs.CY'],"eHealth services,therapies,virtual therapeutic communities,severe personality disorder,mental health,Virtual Reality (VR) systems,Borderline Personality Disorder,depression,anxiety,addiction,PTSD"
"Intelligent Approaches to interact with Machines using Hand Gesture
  Recognition in Natural way: A Survey","Hand gestures recognition (HGR) is one of the main areas of research for the
engineers, scientists and bioinformatics. HGR is the natural way of Human
Machine interaction and today many researchers in the academia and industry are
working on different application to make interactions more easy, natural and
convenient without wearing any extra device. HGR can be applied from games
control to vision enabled robot control, from virtual reality to smart home
systems. In this paper we are discussing work done in the area of hand gesture
recognition where focus is on the intelligent approaches including soft
computing based methods like artificial neural network, fuzzy logic, genetic
algorithms etc. The methods in the preprocessing of image for segmentation and
hand image construction also taken into study. Most researchers used fingertips
for hand detection in appearance based modeling. Finally the comparison of
results given by different researchers is also presented.","['Ankit Chaudhary', 'J. L. Raheja', 'Karen Das', 'Sonia Raheja']",2013-03-10T08:29:48Z,http://arxiv.org/abs/1303.2292v1,"['cs.HC', 'cs.CV']","hand gesture recognition,human machine interaction,soft computing,artificial neural network,fuzzy logic,genetic algorithms,image preprocessing,segmentation,appearance based modeling"
Analyzing Web Services Networks: a WS-NEXT Application,"Web services represent a system with a huge number of units and many various
and complex interactions. Complex networks as a tool for modelling and
analyzing natural environments seem to be well adapted to such a complex
system. To describe a set of Web services we propose three Web services network
models based on the notions of dependency, interaction and similarity. Using
the WS-NEXT extractor we instantiate the models with a collection of Web
services descriptions. We take advantage of complex network properties to
provide an analyzis of the Web services networks. Those networks and the
knowledge of their toplogical properties can be exploited for the discovery and
composition processes.","['Chantal Cherifi', 'Jean-François Santucci']",2013-05-01T14:51:46Z,http://arxiv.org/abs/1305.0190v1,['cs.SE'],"Web services,networks,modeling,interactions,dependencies,similarity,WS-NEXT,extractor,topology,composition"
"Speech: A Challenge to Digital Signal Processing Technology for
  Human-to-Computer Interaction","This software project based paper is for a vision of the near future in which
computer interaction is characterized by natural face-to-face conversations
with lifelike characters that speak, emote, and gesture. The first step is
speech. The dream of a true virtual reality, a complete human-computer
interaction system will not come true unless we try to give some perception to
machine and make it perceive the outside world as humans communicate with each
other. This software project is under development for listening and replying
machine (Computer) through speech. The Speech interface is developed to convert
speech input into some parametric form (Speech-to-Text) for further processing
and the results, text output to speech synthesis (Text-to-Speech)","['Urmila Shrawankar', 'Anjali Mahajan']",2013-05-08T05:55:50Z,http://arxiv.org/abs/1305.1925v1,"['cs.HC', 'cs.CL']","Speech,Digital Signal Processing,Human-to-Computer Interaction,Virtual reality,Natural language processing,Speech synthesis,Text-to-Speech,Speech-to-Text,Machine perception,Face-to-face conversations"
Hand Pointing Detection Using Live Histogram Template of Forehead Skin,"Hand pointing detection has multiple applications in many fields such as
virtual reality and control devices in smart homes. In this paper, we proposed
a novel approach to detect pointing vector in 2D space of a room. After
background subtraction, face and forehead is detected. In the second step,
forehead skin H-S plane histograms in HSV space is calculated. By using these
histogram templates of users skin, and back projection method, skin areas are
detected. The contours of hand are extracted using Freeman chain code
algorithm. Next step is finding fingertips. Points in hand contour which are
candidates for the fingertip can be found in convex defects of convex hull and
contour. We introduced a novel method for finding the fingertip based on the
special points on the contour and their relationships. Our approach detects
hand-pointing vectors in live video from a common webcam with 94%TP and 85%TN.","['Ghassem Tofighi', 'Nasser Ali Afarin', 'Kamraan Raahemifar', 'Anastasios N. Venetsanopoulos']",2014-07-18T07:10:03Z,http://arxiv.org/abs/1407.4898v1,['cs.CV'],"hand pointing detection,forehead skin,histogram template,background subtraction,HSV space,back projection,skin areas,Freeman chain code algorithm,convex defects,convex hull"
"Real-Time and Robust Method for Hand Gesture Recognition System Based on
  Cross-Correlation Coefficient","Hand gesture recognition possesses extensive applications in virtual reality,
sign language recognition, and computer games. The direct interface of hand
gestures provides us a new way for communicating with the virtual environment.
In this paper a novel and real-time approach for hand gesture recognition
system is presented. In the suggested method, first, the hand gesture is
extracted from the main image by the image segmentation and morphological
operation and then is sent to feature extraction stage. In feature extraction
stage the Cross-correlation coefficient is applied on the gesture to recognize
it. In the result part, the proposed approach is applied on American Sign
Language (ASL) database and the accuracy rate obtained 98.34%.","['Reza Azad', 'Babak Azad', 'Iman Tavakoli Kazerooni']",2014-08-08T04:52:59Z,http://arxiv.org/abs/1408.1759v1,['cs.CV'],"Hand gesture recognition,real-time,robust,cross-correlation coefficient,image segmentation,morphological operation,feature extraction,American Sign Language (ASL) database,accuracy rate"
Preprint Traffic Management and Forecasting System Based on 3D GIS,"This is the preprint version of our paper on 2015 15th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing (CCGrid). This paper takes
Shenzhen Futian comprehensive transportation junction as the case, and makes
use of continuous multiple real-time dynamic traffic information to carry out
monitoring and analysis on spatial and temporal distribution of passenger flow
under different means of transportation and service capacity of junction from
multi-dimensional space-time perspectives such as different period and special
period. Virtual reality geographic information system is employed to present
the forecasting result.","['Xiaoming Li', 'Zhihan Lv', 'Jinxing Hu', 'Baoyun Zhang', 'Ling Yin', 'Chen Zhong', 'Weixi Wang', 'Shengzhong Feng']",2015-04-04T22:08:29Z,http://arxiv.org/abs/1504.01375v3,"['cs.OH', 'I.3.7']","preprint,traffic management,forecasting system,3D GIS,transportation junction,dynamic traffic information,passenger flow,service capacity,spatial distribution,temporal distribution"
Hypernom: Mapping VR Headset Orientation to S^3,"Hypernom is a virtual reality game. The cells of a regular 4D polytope are
radially projected to S^3, the sphere in 4D space, then stereographically
projected to 3D space where they are viewed in the headset. The orientation of
the headset is given by an element of the group SO(3), which is also a space
that is double covered by S^3. In fact, the headset outputs a point of this
double cover: a unit quaternion. The positions of the cells are multiplied by
this quaternion before projection to 3D space, which moves the player through
S^3. When the player is sufficiently close to a cell, they eat it. The aim of
the game is to eat all of the cells of the polytope, which, roughly speaking,
is achieved by moving one's head through all possible orientations, twice.","['Vi Hart', 'Andrea Hawksley', 'Henry Segerman', 'Marc ten Bosch']",2015-07-21T04:42:48Z,http://arxiv.org/abs/1507.05707v1,"['math.HO', 'math.GT', '00A66 (Primary), 57M60 (Secondary)']","virtual reality,4D polytope,S^3,stereographically projected,headset orientation,SO(3),unit quaternion,player position,spatial orientation,game mechanics"
Preprint WebVRGIS Based Traffic Analysis and Visualization System,"This is the preprint version of our paper on Advances in Engineering
Software. With several characteristics, such as large scale, diverse
predictability and timeliness, the city traffic data falls in the range of
definition of Big Data. A Virtual Reality GIS based traffic analysis and
visualization system is proposed as a promising and inspiring approach to
manage and develop traffic big data. In addition to the basic GIS interaction
functions, the proposed system also includes some intelligent visual analysis
and forecasting functions. The passenger flow forecasting algorithm is
introduced in detail.","['Xiaoming Li', 'Zhihan Lv', 'Weixi Wang', 'Baoyun Zhang', 'Jinxing Hu', 'Ling Yin', 'Shengzhong Feng']",2015-11-19T19:16:17Z,http://arxiv.org/abs/1511.06313v1,['cs.OH'],"Preprint,WebVRGIS,Traffic Analysis,Visualization System,Big Data,Virtual Reality GIS,Intelligent Visual Analysis,Forecasting,Passenger Flow Forecasting Algorithm"
Breaking the Barriers to True Augmented Reality,"In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained
considerable commercial traction, with Facebook acquiring Oculus VR for \$2
billion, Magic Leap attracting more than \$500 million of funding, and
Microsoft announcing their HoloLens head-worn computer. Where is humanity
headed: a brave new dystopia-or a paradise come true?
  In this article, we present discussions, which started at the symposium
""Making Augmented Reality Real"", held at Nara Institute of Science and
Technology in August 2014. Ten scientists were invited to this three-day event,
which started with a full day of public presentations and panel discussions
(video recordings are available at the event web page), followed by two days of
roundtable discussions addressing the future of AR and VR.","['Christian Sandor', 'Martin Fuchs', 'Alvaro Cassinelli', 'Hao Li', 'Richard Newcombe', 'Goshiro Yamamoto', 'Steven Feiner']",2015-12-17T05:57:06Z,http://arxiv.org/abs/1512.05471v1,['cs.HC'],"Augmented Reality,Virtual Reality,Oculus VR,Magic Leap,Microsoft HoloLens,symposium,Nara Institute of Science and Technology,panel discussions,roundtable discussions,future of AR"
Comment Diffusons-nous sur les Réseaux Sociaux ?,"The emergence of new communication media such as blogs, online newspapers and
social networks allow us to go further in the understanding of human behavior.
Indeed, these public exchange spaces are now firmly planted in our modern
society and appear to be powerful sensors of social behavior and opinion
movements. In this paper, we focus on information spreading and attempt to
understand what are the conditions in which a person decides to speak on a
subject. For this purpose, we propose a set of measures that aim to
characterize the diffusion behavior. Our measures have been used on messages
related to two events that took place in January 2015: presentation by
Microsoft of a new virtual reality headset and the election of a political
party of radical left in Greece.",['Erick Stattner'],2016-01-31T14:31:20Z,http://arxiv.org/abs/1602.00252v1,['cs.SI'],"communication media,blogs,online newspapers,social networks,human behavior,social behavior,opinion movements,information spreading,diffusion behavior"
"A Novel Human Computer Interaction Platform based College Mathematical
  Education Methodology","This article proposes the analysis on novel human computer interaction (HCI)
platform based college mathematical education methodology. Above for the
application of virtual reality technology in teaching the problems in the
study, only through the organization focus on the professional and technical
personnel, and constantly improve researchers in development process of
professional knowledge, close to the actual needs of the teaching can we
achieve the satisfactory result. To obtain better education output, we combine
the Kinect to form the HCI based teaching environment. We firstly review the
latest HCI technique and principles of college math courses, then we introduce
basic components of the Kinect including the gesture segmentation, systematic
implementation and the primary characteristics of the platform. As the further
step, we implement the system with the re-write of script code to build up the
personalized HCI assisted education scenario. The verification and simulation
proves the feasibility of our method.",['Zhiyan Li'],2016-02-02T06:26:37Z,http://arxiv.org/abs/1602.00801v2,['cs.HC'],"Human Computer Interaction,College Mathematical Education,Virtual Reality Technology,Professional Knowledge,Kinect,Gesture Segmentation,Systematic Implementation,HCI Technique,HCI Principles,Education Output"
"Multi-agent evolutionary systems for the generation of complex virtual
  worlds","Modern films, games and virtual reality applications are dependent on
convincing computer graphics. Highly complex models are a requirement for the
successful delivery of many scenes and environments. While workflows such as
rendering, compositing and animation have been streamlined to accommodate
increasing demands, modelling complex models is still a laborious task. This
paper introduces the computational benefits of an Interactive Genetic Algorithm
(IGA) to computer graphics modelling while compensating the effects of user
fatigue, a common issue with Interactive Evolutionary Computation. An
intelligent agent is used in conjunction with an IGA that offers the potential
to reduce the effects of user fatigue by learning from the choices made by the
human designer and directing the search accordingly. This workflow accelerates
the layout and distribution of basic elements to form complex models. It
captures the designer's intent through interaction, and encourages playful
discovery.","['Jan Kruse', 'Andy M. Connor']",2016-04-20T02:43:47Z,http://arxiv.org/abs/1604.05792v1,['cs.NE'],"Multi-agent systems,Evolutionary systems,Virtual worlds,Computer graphics,Interactive Genetic Algorithm,Modeling,User fatigue,Intelligent agent,Layout,Distribution"
Fine-To-Coarse Global Registration of RGB-D Scans,"RGB-D scanning of indoor environments is important for many applications,
including real estate, interior design, and virtual reality. However, it is
still challenging to register RGB-D images from a hand-held camera over a long
video sequence into a globally consistent 3D model. Current methods often can
lose tracking or drift and thus fail to reconstruct salient structures in large
environments (e.g., parallel walls in different rooms). To address this
problem, we propose a ""fine-to-coarse"" global registration algorithm that
leverages robust registrations at finer scales to seed detection and
enforcement of new correspondence and structural constraints at coarser scales.
To test global registration algorithms, we provide a benchmark with 10,401
manually-clicked point correspondences in 25 scenes from the SUN3D dataset.
During experiments with this benchmark, we find that our fine-to-coarse
algorithm registers long RGB-D sequences better than previous methods.","['Maciej Halber', 'Thomas Funkhouser']",2016-07-28T17:19:46Z,http://arxiv.org/abs/1607.08539v3,['cs.CV'],"RGB-D scanning,global registration,hand-held camera,3D model,tracking,drift,3D reconstruction,structural constraints,benchmark,point correspondences"
3D visualization of astronomy data cubes using immersive displays,"We report on an exploratory project aimed at performing immersive 3D
visualization of astronomical data, starting with spectral-line radio data
cubes from galaxies. This work is done as a collaboration between the
Department of Physics and Astronomy and the Department of Computer Science at
the University of Manitoba. We are building our prototype using the 3D engine
Unity, because of its ease of use for integration with advanced displays such
as a CAVE environment, a zSpace tabletop, or virtual reality headsets. We
address general issues regarding 3D visualization, such as: load and convert
astronomy data, perform volume rendering on the GPU, and produce physically
meaningful visualizations using principles of visual literacy. We discuss some
challenges to be met when designing a user interface that allows us to take
advantage of this new way of exploring data. We hope to lay the foundations for
an innovative framework useful for all astronomers who use spectral line data
cubes, and encourage interested parties to join our efforts. This pilot project
addresses the challenges presented by frontier astronomy experiments, such as
the Square Kilometre Array and its precursors.","['Gilles Ferrand', 'Jayanne English', 'Pourang Irani']",2016-07-29T17:39:31Z,http://arxiv.org/abs/1607.08874v1,"['astro-ph.IM', 'cs.GR']","3D visualization,astronomy data cubes,immersive displays,spectral-line radio data,3D engine Unity,CAVE environment,volume rendering,GPU,visual literacy,user interface"
"Using cognitive agent-based simulation for the evaluation of indoor
  wayfinding systems","This paper presents a novel approach to simulate human wayfinding behaviour
incorporating visual cognition into a software agent for a computer aided
evaluation of wayfinding systems in large infrastructures. The proposed
approach follows the Sense-Plan-Act paradigm comprised of a model for visual
attention, navigation behaviour and pedestrian movement. Stochastic features of
perception are incorporated to enhance generality and diversity of the
developed wayfinding simulation to reflect a variety of behaviours. The
validity of the proposed approach was evaluated based on empirical data
collected through wayfinding experiments with 20 participants in an immersive
virtual reality environment using a life-sized 3D replica of Vienna's new
central railway station. The results show that the developed cognitive
agent-based simulation provides a further contribution to the simulation of
human wayfinding and subsequently a further step to an effective evaluation
tool for the planning of wayfinding and signage.","['Helmut Schrom-Feiertag', 'Martin Stubenschrott', 'Georg Regal', 'Johann Schrammel', 'Volker Settgast']",2016-11-08T10:04:44Z,http://arxiv.org/abs/1611.02459v1,['cs.HC'],"cognitive agent-based simulation,wayfinding systems,visual cognition,Sense-Plan-Act paradigm,pedestrian movement,stochastic features,perception,wayfinding simulation,virtual reality,signage"
"Dissecting the End-to-end Latency of Interactive Mobile Video
  Applications","In this paper we measure the step-wise latency in the pipeline of three kinds
of interactive mobile video applications that are rapidly gaining popularity,
namely Remote Graphics Rendering (RGR) of which we focus on mobile cloud
gaming, Mobile Augmented Reality (MAR), and Mobile Virtual Reality (MVR). The
applications differ from each other by the way in which the user interacts with
the application, i.e., video I/O and user controls, but they all share in
common the fact that their user experience is highly sensitive to end-to-end
latency. Long latency between a user control event and display update renders
the application unusable. Hence, understanding the nature and origins of
latency of these applications is of paramount importance. We show through
extensive measurements that control input and display buffering have a
substantial effect on the overall delay. Our results shed light on the latency
bottlenecks and the maturity of technology for seamless user experience with
these applications.","['Teemu Kämäräinen', 'Matti Siekkinen', 'Antti Ylä-Jääski', 'Wenxiao Zhang', 'Pan Hui']",2016-11-25T17:11:57Z,http://arxiv.org/abs/1611.08520v1,['cs.HC'],"End-to-end latency,Interactive mobile video,Remote Graphics Rendering,Mobile Augmented Reality,Mobile Virtual Reality,User experience,Latency bottlenecks,Control input,Display buffering"
Navigable videos for presenting scientific data on head-mounted displays,"Immersive, stereoscopic viewing enables scientists to better analyze the
spatial structures of visualized physical phenomena. However, their findings
cannot be properly presented in traditional media, which lack these core
attributes. Creating a presentation tool that captures this environment poses
unique challenges, namely related to poor viewing accessibility. Immersive
scientific renderings often require high-end equipment, which can be
impractical to obtain. We address these challenges with our authoring tool and
navigational interface, which is designed for affordable head-mounted displays.
With the authoring tool, scientists can show salient data features as connected
360{\deg} video paths, resulting in a ""choose-your-own-adventure"" experience.
Our navigational interface features bidirectional video playback for added
viewing control when users traverse the tailor-made content. We evaluate our
system's benefits by authoring case studies on several data sets and conducting
a usability study on the navigational interface's design. In summary, our
approach provides scientists an immersive medium to visually present their
research to the intended audience--spanning from students to colleagues--on
affordable virtual reality headsets.","['Jacqueline Chu', 'Leonardo Ferrer', 'Min Shih', 'Kwan-Liu Ma']",2016-11-28T01:03:57Z,http://arxiv.org/abs/1611.08947v1,['cs.GR'],"navigable videos,scientific data,head-mounted displays,immersive viewing,spatial structures,presentation tool,authoring tool,navigational interface,360-degree video paths,virtual reality"
How do people explore virtual environments?,"Understanding how people explore immersive virtual environments is crucial
for many applications, such as designing virtual reality (VR) content,
developing new compression algorithms, or learning computational models of
saliency or visual attention. Whereas a body of recent work has focused on
modeling saliency in desktop viewing conditions, VR is very different from
these conditions in that viewing behavior is governed by stereoscopic vision
and by the complex interaction of head orientation, gaze, and other kinematic
constraints. To further our understanding of viewing behavior and saliency in
VR, we capture and analyze gaze and head orientation data of 169 users
exploring stereoscopic, static omni-directional panoramas, for a total of 1980
head and gaze trajectories for three different viewing conditions. We provide a
thorough analysis of our data, which leads to several important insights, such
as the existence of a particular fixation bias, which we then use to adapt
existing saliency predictors to immersive VR conditions. In addition, we
explore other applications of our data and analysis, including automatic
alignment of VR video cuts, panorama thumbnails, panorama video synopsis, and
saliency-based compression.","['Vincent Sitzmann', 'Ana Serrano', 'Amy Pavel', 'Maneesh Agrawala', 'Diego Gutierrez', 'Belen Masia', 'Gordon Wetzstein']",2016-12-13T20:01:18Z,http://arxiv.org/abs/1612.04335v2,['cs.CV'],"immersive virtual environments,virtual reality,compression algorithms,computational models,saliency,visual attention,gaze data,head orientation,stereoscopic vision,kinematic constraints"
"EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras
  (Extended Abstract)","Marker-based and marker-less optical skeletal motion-capture methods use an
outside-in arrangement of cameras placed around a scene, with viewpoints
converging on the center. They often create discomfort by possibly needed
marker suits, and their recording volume is severely restricted and often
constrained to indoor scenes with controlled backgrounds. We therefore propose
a new method for real-time, marker-less and egocentric motion capture which
estimates the full-body skeleton pose from a lightweight stereo pair of fisheye
cameras that are attached to a helmet or virtual-reality headset. It combines
the strength of a new generative pose estimation framework for fisheye views
with a ConvNet-based body-part detector trained on a new automatically
annotated and augmented dataset. Our inside-in method captures full-body motion
in general indoor and outdoor scenes, and also crowded scenes.","['Helge Rhodin', 'Christian Richardt', 'Dan Casas', 'Eldar Insafutdinov', 'Mohammad Shafiei', 'Hans-Peter Seidel', 'Bernt Schiele', 'Christian Theobalt']",2016-12-31T16:49:39Z,http://arxiv.org/abs/1701.00142v1,['cs.CV'],"Egocentric,Marker-less,Motion Capture,Fisheye Cameras,Pose Estimation,ConvNet,Body-part Detector,Indoor Scenes,Outdoor Scenes,Crowded Scenes"
Embedded Systems Architecture for SLAM Applications,"In recent years, we have observed a clear trend in the rapid rise of
autonomous vehicles, robotics, virtual reality, and augmented reality. The core
technology enabling these applications, Simultaneous Localization And Mapping
(SLAM), imposes two main challenges: first, these workloads are computationally
intensive and they often have real-time requirements; second, these workloads
run on battery-powered mobile devices with limited energy budget. In short, the
essence of these challenges is that performance should be improved while
simultaneously reducing energy consumption, two rather contradicting goals by
conventional wisdom. In this paper, we take a close look at state-of-the-art
Simultaneous Localization And Mapping (SLAM) workloads, especially how these
workloads behave on mobile devices. Based on the results, we propose a mobile
architecture to improve SLAM performance on mobile devices.","['Jie Tang', 'Shaoshan Liu', 'Jean-Luc Gaudiot']",2017-02-04T14:37:38Z,http://arxiv.org/abs/1702.01295v1,['cs.AR'],"Embedded Systems,Architecture,SLAM Applications,Autonomous Vehicles,Robotics,Virtual Reality,Augmented Reality,Computationally Intensive,Real-time Requirements"
"Towards Developing an Easy-To-Use Scripting Environment for Animating
  Virtual Characters","This paper presents the three scripting commands and main functionalities of
a novel character animation environment called CHASE. CHASE was developed for
enabling inexperienced programmers, animators, artists, and students to animate
in meaningful ways virtual reality characters. This is achieved by scripting
simple commands within CHASE. The commands identified, which are associated
with simple parameters, are responsible for generating a number of predefined
motions and actions of a character. Hence, the virtual character is able to
animate within a virtual environment and to interact with tasks located within
it. An additional functionality of CHASE is supplied. It provides the ability
to generate multiple tasks of a character, such as providing the user the
ability to generate scenario-related animated sequences. However, since
multiple characters may require simultaneous animation, the ability to script
actions of different characters at the same time is also provided.",['Christos Mousas'],2017-02-10T16:37:55Z,http://arxiv.org/abs/1702.03246v1,['cs.GR'],"scripting environment,animating,virtual characters,commands,functionalities,virtual reality,parameters,predefined motions,virtual environment,simultaneous animation"
"The Blank Stare: Retrieving Unique Eye Tracking Signatures Independent
  of Visual Stimuli","Using Low Cost Portable Eye Tracking for Biometric Identification Or
Verification: Eye tracking technologies have in recent years become available
outside of specialised labs, and are starting to become integrated in tablets
and virtual reality headsets. This offers new opportunities for use in common
office- and home environments, such as for biometric recognition
(identification or verification), alone or in combination with other
technologies. This paper exposes two fundamentally different approaches that
have been suggested, based on spatial and temporal signatures respectively.
While deploying different stimulation paradigms for recording, it also proposes
an alternative way to analyze spatial domain signatures using Fourier
transformation. Empirical data recorded from two subjects over two weeks, three
months apart, are found to support previous results. Further, variations and
stability of some of the proposed signatures are analyzed over the extended
timeframe and under slightly varying conditions.","['Per Bækgaard', 'Michael Kai Petersen', 'Jakob Eg Larsen']",2017-05-08T11:01:51Z,http://arxiv.org/abs/1705.02823v1,['cs.HC'],"eye tracking,biometric identification,spatial signatures,temporal signatures,stimulation paradigms,Fourier transformation,empirical data,stability analysis,biometric verification"
"Frequency Domain Singular Value Decomposition for Efficient Spatial
  Audio Coding","Advances in virtual reality have generated substantial interest in accurately
reproducing and storing spatial audio in the higher order ambisonics (HOA)
representation, given its rendering flexibility. Recent standardization for HOA
compression adopted a framework wherein HOA data are decomposed into principal
components that are then encoded by standard audio coding, i.e., frequency
domain quantization and entropy coding to exploit psychoacoustic redundancy. A
noted shortcoming of this approach is the occasional mismatch in principal
components across blocks, and the resulting suboptimal transitions in the data
fed to the audio coder. Instead, we propose a framework where singular value
decomposition (SVD) is performed after transformation to the frequency domain
via the modified discrete cosine transform (MDCT). This framework not only
ensures smooth transition across blocks, but also enables frequency dependent
SVD for better energy compaction. Moreover, we introduce a novel noise
substitution technique to compensate for suppressed ambient energy in discarded
higher order ambisonics channels, which significantly enhances the perceptual
quality of the reconstructed HOA signal. Objective and subjective evaluation
results provide evidence for the effectiveness of the proposed framework in
terms of both higher compression gains and better perceptual quality, compared
to existing methods.","['Sina Zamani', 'Tejaswi Nanjundaswamy', 'Kenneth Rose']",2017-05-10T17:58:53Z,http://arxiv.org/abs/1705.03877v2,['cs.SD'],"Spatial audio coding,Frequency domain,Singular value decomposition,Higher order ambisonics,Standard audio coding,Frequency domain quantization,Entropy coding,Modified discrete cosine transform,Energy compaction,Perceptual quality"
"HuGaDB: Human Gait Database for Activity Recognition from Wearable
  Inertial Sensor Networks","This paper presents a human gait data collection for analysis and activity
recognition consisting of continues recordings of combined activities, such as
walking, running, taking stairs up and down, sitting down, and so on; and the
data recorded are segmented and annotated. Data were collected from a body
sensor network consisting of six wearable inertial sensors (accelerometer and
gyroscope) located on the right and left thighs, shins, and feet. Additionally,
two electromyography sensors were used on the quadriceps (front thigh) to
measure muscle activity. This database can be used not only for activity
recognition but also for studying how activities are performed and how the
parts of the legs move relative to each other. Therefore, the data can be used
(a) to perform health-care-related studies, such as in walking rehabilitation
or Parkinson's disease recognition, (b) in virtual reality and gaming for
simulating humanoid motion, or (c) for humanoid robotics to model humanoid
walking. This dataset is the first of its kind which provides data about human
gait in great detail. The database is available free of charge
https://github.com/romanchereshnev/HuGaDB.","['Roman Chereshnev', 'Attila Kertesz-Farkas']",2017-05-10T13:36:38Z,http://arxiv.org/abs/1705.08506v2,['cs.CY'],"Human Gait Database,Activity Recognition,Wearable Inertial Sensor Networks,Body Sensor Network,Accelerometer,Gyroscope,Electromyography sensors,Walking Rehabilitation,Parkinson's Disease Recognition,Humanoid Robotics"
BodyDigitizer: An Open Source Photogrammetry-based 3D Body Scanner,"With the rising popularity of Augmented and Virtual Reality, there is a need
for representing humans as virtual avatars in various application domains
ranging from remote telepresence, games to medical applications. Besides
explicitly modelling 3D avatars, sensing approaches that create person-specific
avatars are becoming popular. However, affordable solutions typically suffer
from a low visual quality and professional solution are often too expensive to
be deployed in nonprofit projects.
  We present an open-source project, BodyDigitizer, which aims at providing
both build instructions and configuration software for a high-resolution
photogrammetry-based 3D body scanner. Our system encompasses up to 96 Rasperry
PI cameras, active LED lighting, a sturdy frame construction and open-source
configuration software. %We demonstrate the applicability of the body scanner
in a nonprofit Mixed Reality health project. The detailed build instruction and
software are available at http://www.bodydigitizer.org.","['Travis Gesslein', 'Daniel Scherer', 'Jens Grubert']",2017-10-03T20:10:10Z,http://arxiv.org/abs/1710.01370v2,"['cs.CV', 'cs.HC']","Photogrammetry,3D body scanner,Open source,Augmented Reality,Virtual Reality,Avatar,Rasperry PI cameras,LED lighting,Mixed Reality"
"Combining absolute and relative pointing for fast and accurate distant
  interaction","Traditional relative pointing devices such as mice and trackpads are
unsuitable for pointing at distant displays, because they encumber the users by
requiring either a flat surface to operate on or being held by two hands. Past
research has examined many new pointing methods, but few could surpass the
speed and accuracy of mice and trackpads. This paper introduces a new pointing
system that is developed based on HTC Vive, a relatively low-cost virtual
reality system, and proposes two methods of combining absolute and relative
pointing. The proposed methods were compared against single-mode pointing
methods (i.e., pure absolute pointing and pure relative pointing) in a Fitts'
law study. The results show that with only a short period of practice, one
hybrid pointing technique enabled faster and more accurate pointing than both
single-mode pointing techniques, which included a trackpad.",['Yunfeng Zhang'],2017-10-04T19:42:07Z,http://arxiv.org/abs/1710.01778v1,['cs.HC'],"absolute pointing,relative pointing,distant interaction,pointing devices,HTC Vive,virtual reality,Fitts' law,hybrid pointing technique,trackpad"
VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,"Accurate detection of objects in 3D point clouds is a central problem in many
applications, such as autonomous navigation, housekeeping robots, and
augmented/virtual reality. To interface a highly sparse LiDAR point cloud with
a region proposal network (RPN), most existing efforts have focused on
hand-crafted feature representations, for example, a bird's eye view
projection. In this work, we remove the need of manual feature engineering for
3D point clouds and propose VoxelNet, a generic 3D detection network that
unifies feature extraction and bounding box prediction into a single stage,
end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud
into equally spaced 3D voxels and transforms a group of points within each
voxel into a unified feature representation through the newly introduced voxel
feature encoding (VFE) layer. In this way, the point cloud is encoded as a
descriptive volumetric representation, which is then connected to a RPN to
generate detections. Experiments on the KITTI car detection benchmark show that
VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a
large margin. Furthermore, our network learns an effective discriminative
representation of objects with various geometries, leading to encouraging
results in 3D detection of pedestrians and cyclists, based on only LiDAR.","['Yin Zhou', 'Oncel Tuzel']",2017-11-17T04:25:24Z,http://arxiv.org/abs/1711.06396v1,['cs.CV'],"point cloud,3D object detection,deep network,LiDAR,voxel,feature representation,bounding box prediction,VFE layer,RPN,KITTI benchmark"
"Visible Light Communication for Next Generation Untethered Virtual
  Reality Systems","Virtual and augmented reality (VR/AR) systems are emerging technologies
requiring data rates of multiple Gbps. Existing high quality VR headsets
require connections through HDMI cables to a computer rendering rich graphic
contents to meet the extremely high data transfer rate requirement. Such a
cable connection limits the VR user's mobility and interferes with the VR
experience. Current wireless technologies such as WiFi cannot support the
multi-Gbps graphics data transfer. Instead, we propose to use visible light
communication (VLC) for establishing high speed wireless links between a
rendering computer and a VR headset. But, VLC transceivers are highly
directional with narrow beams and require constant maintenance of line-of-sight
(LOS) alignment between the transmitter and the receiver. Thus, we present a
novel multi-detector hemispherical VR headset design to tackle the beam
misalignment problem caused by the VR user's random head orientation. We
provide detailed analysis on how the number of detectors on the headset can be
minimized while maintaining the required beam alignment and providing high
quality VR experience.","['Mahmudur Khan', 'Jacob Chakareski']",2019-04-07T20:24:55Z,http://arxiv.org/abs/1904.03735v1,['cs.NI'],"Visible Light Communication,Virtual Reality,Augmented Reality,Gbps data rates,HDMI cables,Wireless communication,Multi-Gbps graphics data transfer,VLC transceivers,Line-of-sight alignment,VR headset design"
Quasi-Direct Drive for Low-Cost Compliant Robotic Manipulation,"Robots must cost less and be force-controlled to enable widespread, safe
deployment in unconstrained human environments. We propose Quasi-Direct Drive
actuation as a capable paradigm for robotic force-controlled manipulation in
human environments at low-cost. Our prototype - Blue - is a human scale 7
Degree of Freedom arm with 2kg payload. Blue can cost less than $5000. We show
that Blue has dynamic properties that meet or exceed the needs of human
operators: the robot has a nominal position-control bandwidth of 7.5Hz and
repeatability within 4mm. We demonstrate a Virtual Reality based interface that
can be used as a method for telepresence and collecting robot training
demonstrations. Manufacturability, scaling, and potential use-cases for the
Blue system are also addressed. Videos and additional information can be found
online at berkeleyopenarms.github.io","['David V. Gealy', 'Stephen McKinley', 'Brent Yi', 'Philipp Wu', 'Phillip R. Downey', 'Greg Balke', 'Allan Zhao', 'Menglong Guo', 'Rachel Thomasson', 'Anthony Sinclair', 'Peter Cuellar', 'Zoe McCarthy', 'Pieter Abbeel']",2019-04-08T03:21:23Z,http://arxiv.org/abs/1904.03815v2,['cs.RO'],"Quasi-Direct Drive,Robotic Manipulation,Force-Controlled,Low-Cost,Actuation,Degree of Freedom,Payload,Dynamic Properties,Virtual Reality,Telepresence"
"Affordance Analysis of Virtual and Augmented Reality Mediated
  Communication","Virtual and augmented reality communication platforms are seen as promising
modalities for next-generation remote face-to-face interactions. Our study
attempts to explore non-verbal communication features in relation to their
conversation context for virtual and augmented reality mediated communication
settings. We perform a series of user experiments, triggering nine conversation
tasks in 4 settings, each containing corresponding non-verbal communication
features. Our results indicate that conversation types which involve less
emotional engagement are more likely to be acceptable in virtual reality and
augmented reality settings with low-fidelity avatar representation, compared to
scenarios that involve high emotional engagement or intellectually difficult
discussions. We further systematically analyze and rank the impact of
low-fidelity representation of micro-expressions, body scale, head pose, and
hand gesture in affecting the user experience in one-on-one conversations, and
validate that preserving micro-expression cues plays the most effective role in
improving bi-directional conversations in future virtual and augmented reality
settings.","['Mohammad Keshavarzi', 'Michael Wu', 'Michael N. Chin', 'Robert N. Chin', 'Allen Y. Yang']",2019-04-09T15:07:51Z,http://arxiv.org/abs/1904.04723v1,"['cs.HC', 'cs.MM']","Affordance Analysis,Virtual reality,Augmented reality,Communication,Non-verbal communication,Conversation context,User experiments,Avatar representation,Micro-expressions,Body scale"
Image Quality Assessment for Omnidirectional Cross-reference Stitching,"Along with the development of virtual reality (VR), omnidirectional images
play an important role in producing multimedia content with immersive
experience. However, despite various existing approaches for omnidirectional
image stitching, how to quantitatively assess the quality of stitched images is
still insufficiently explored. To address this problem, we establish a novel
omnidirectional image dataset containing stitched images as well as
dual-fisheye images captured from standard quarters of 0$^\circ$, 90$^\circ$,
180$^\circ$ and 270$^\circ$. In this manner, when evaluating the quality of an
image stitched from a pair of fisheye images (e.g., 0$^\circ$ and 180$^\circ$),
the other pair of fisheye images (e.g., 90$^\circ$ and 270$^\circ$) can be used
as the cross-reference to provide ground-truth observations of the stitching
regions. Based on this dataset, we further benchmark six widely used stitching
models with seven evaluation metrics for IQA. To the best of our knowledge, it
is the first dataset that focuses on assessing the stitching quality of
omnidirectional images.","['Kaiwen Yu', 'Jia Li', 'Yu Zhang', 'Yifan Zhao', 'Long Xu']",2019-04-10T01:11:07Z,http://arxiv.org/abs/1904.04960v2,['cs.CV'],"omnidirectional images,image quality assessment,stitching,dataset,dual-fisheye images,evaluation metrics"
"Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic
  Point Clouds","Accurate detection of 3D objects is a fundamental problem in computer vision
and has an enormous impact on autonomous cars, augmented/virtual reality and
many applications in robotics. In this work we present a novel fusion of neural
network based state-of-the-art 3D detector and visual semantic segmentation in
the context of autonomous driving. Additionally, we introduce
Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable
evaluation metric for comparison of object detections, which speeds up our
inference time up to 20\% and halves training time. On top, we apply
state-of-the-art online multi target feature tracking on the object
measurements to further increase accuracy and robustness utilizing temporal
information. Our experiments on KITTI show that we achieve same results as
state-of-the-art in all related categories, while maintaining the performance
and accuracy trade-off and still run in real-time. Furthermore, our model is
the first one that fuses visual semantic with 3D object detection.","['Martin Simon', 'Karl Amende', 'Andrea Kraus', 'Jens Honer', 'Timo Sämann', 'Hauke Kaulbersch', 'Stefan Milz', 'Horst Michael Gross']",2019-04-16T08:49:06Z,http://arxiv.org/abs/1904.07537v1,['cs.CV'],"3D object detection,tracking,semantic point clouds,neural network,visual semantic segmentation,evaluation metric,feature tracking,autonomous driving,real-time"
Multi-Objective Autonomous Braking System using Naturalistic Dataset,"A deep reinforcement learning based multi-objective autonomous braking system
is presented. The design of the system is formulated in a continuous action
space and seeks to maximize both pedestrian safety and perception as well as
passenger comfort. The vehicle agent is trained against a large naturalistic
dataset containing pedestrian road-crossing trials in which respondents walked
across a road under various traffic conditions within an interactive virtual
reality environment. The policy for brake control is learned through computer
simulation using two reinforcement learning methods i.e. Proximal Policy
Optimization and Deep Deterministic Policy Gradient and the efficiency of each
are compared. Results show that the system is able to reduce the negative
influence on passenger comfort by half while maintaining safe braking
operation.","['Rafael Vasquez', 'Bilal Farooq']",2019-04-15T16:07:45Z,http://arxiv.org/abs/1904.07705v2,"['cs.RO', 'cs.AI', 'cs.LG']","deep reinforcement learning,multi-objective,autonomous braking system,naturalistic dataset,pedestrian safety,perception,passenger comfort,virtual reality environment,Proximal Policy Optimization,Deep Deterministic Policy Gradient"
"DeepWait: Pedestrian Wait Time Estimation in Mixed Traffic Conditions
  Using Deep Survival Analysis","Pedestrian's road crossing behaviour is one of the important aspects of urban
dynamics that will be affected by the introduction of autonomous vehicles. In
this study we introduce DeepSurvival, a novel framework for estimating
pedestrian's waiting time at unsignalized mid-block crosswalks in mixed traffic
conditions. We exploit the strengths of deep learning in capturing the
nonlinearities in the data and develop a cox proportional hazard model with a
deep neural network as the log-risk function. An embedded feature selection
algorithm for reducing data dimensionality and enhancing the interpretability
of the network is also developed. We test our framework on a dataset collected
from 160 participants using an immersive virtual reality environment.
Validation results showed that with a C-index of 0.64 our proposed framework
outperformed the standard cox proportional hazard-based model with a C-index of
0.58.","['Arash Kalatian', 'Bilal Farooq']",2019-04-16T00:04:11Z,http://arxiv.org/abs/1904.11008v3,"['cs.HC', 'cs.CV']","Pedestrian,Wait Time Estimation,Mixed Traffic Conditions,Deep Survival Analysis,Deep Learning,Cox Proportional Hazard Model,Feature Selection Algorithm,Data Dimensionality,C-index"
Hypergraph Spectral Analysis and Processing in 3D Point Cloud,"Along with increasingly popular virtual reality applications, the
three-dimensional (3D) point cloud has become a fundamental data structure to
characterize 3D objects and surroundings. To process 3D point clouds
efficiently, a suitable model for the underlying structure and outlier noises
is always critical. In this work, we propose a hypergraph-based new point cloud
model that is amenable to efficient analysis and processing. We introduce
tensor-based methods to estimate hypergraph spectrum components and frequency
coefficients of point clouds in both ideal and noisy settings. We establish an
analytical connection between hypergraph frequencies and structural features.
We further evaluate the efficacy of hypergraph spectrum estimation in two
common point cloud applications of sampling and denoising for which also we
elaborate specific hypergraph filter design and spectral properties. The
empirical performance demonstrates the strength of hypergraph signal processing
as a tool in 3D point clouds and the underlying properties.","['Songyang Zhang', 'Shuguang Cui', 'Zhi Ding']",2020-01-08T05:30:16Z,http://arxiv.org/abs/2001.02384v1,"['eess.SP', 'cs.CV']","hypergraph spectral analysis,3D point cloud,virtual reality applications,data structure,outlier noises,hypergraph model,tensor-based methods,spectrum estimation,hypergraph filter design,spectral properties"
"TanGi: Tangible Proxies for Embodied Object Exploration and Manipulation
  in Virtual Reality","Exploring and manipulating complex virtual objects is challenging due to
limitations of conventional controllers and free-hand interaction techniques.
We present the TanGi toolkit which enables novices to rapidly build physical
proxy objects using Composable Shape Primitives. TanGi also provides
Manipulators allowing users to build objects including movable parts, making
them suitable for rich object exploration and manipulation in VR. With a set of
different use cases and applications we show the capabilities of the TanGi
toolkit, and evaluate its use. In a study with 16 participants, we demonstrate
that novices can quickly build physical proxy objects using the Composable
Shape Primitives, and explore how different levels of object embodiment affect
virtual object exploration. In a second study with 12 participants we evaluate
TanGi's Manipulators, and investigate the effectiveness of embodied
interaction. Findings from this study show that TanGi's proxies outperform
traditional controllers, and were generally favored by participants.","['Martin Feick', 'Scott Bateman', 'Anthony Tang', 'André Miede', 'Nicolai Marquardt']",2020-01-09T14:36:30Z,http://arxiv.org/abs/2001.03021v1,"['cs.HC', 'H.5.2']","Tangible Proxies,Embodied Object Exploration,Manipulation,Virtual Reality,Composable Shape Primitives,Manipulators,Object Embodiment,Embodied Interaction,Traditional Controllers"
"Exploratory Study on User's Dynamic Visual Acuity and Quality Perception
  of Impaired Images","In this paper we assess the impact of head movement on user's visual acuity
and their quality perception of impaired images. There are physical limitations
on the amount of visual information a person can perceive and physical
limitations regarding the speed at which our body, and as a consequence our
head, can explore a scene. In these limitations lie fundamental solutions for
the communication of multimedia systems. As such, subjects were asked to
evaluate the perceptual quality of static images presented on a TV screen while
their head was in a dynamic (moving) state. The idea is potentially applicable
to virtual reality applications and therefore, we also measured the image
quality perception of each subject on a head mounted display. Experiments show
the significant decrease in visual acuity and quality perception when the
user's head is not static, and give an indication on how much the quality can
be reduced without the user noticing any impairments.","['Jolien De Letter', 'Anissa All', 'Lieven De Marez', 'Vasileios Avramelos', 'Peter Lambert', 'Glenn Van Wallendael']",2020-01-10T16:15:06Z,http://arxiv.org/abs/2001.03542v1,"['cs.MM', 'cs.HC']","dynamic visual acuity,quality perception,impaired images,head movement,visual information,multimedia systems,static images,TV screen,virtual reality applications,head mounted display"
"Variable Curvature Displays: Optical Designs and Applications for
  VR/AR/MR Headsets","In the present paper, we discuss the design of a projection system with
curved display and its enhancement by variably adjusting the curvature. We
demonstrate that the focal surface curvature varies significantly with a change
of the object position and that it can easily be computed with the Seidel
aberration theory. Using this analytically derived curvature value as the
starting point, we optimise a refocusable projection system with 90 degrees
field of view and F/#=6.2 . It is demonstrated that such a system can provide
stable image quality and illumination when refocusing from infinity to 1.5 m.
The gain in spatial resolution is as high as 1.54 times with respect to a flat
focal surface. Furthermore, we prove that a silicon die can be curved to the
required shape with a safety factor of 4.3 in terms of the mechanical stress.
Finally, it is shown that the developed system can be used in a virtual reality
headset providing high resolution, low distortion and a flexible focusing mode.","['Eduard Muslimov', 'Thibault Behaghel', 'Emmanuel Hugot', 'Kelly Joaquina', 'Ilya Guskov']",2020-01-20T15:02:53Z,http://arxiv.org/abs/2001.07132v1,['physics.optics'],"Variable curvature display,Optical design,Projection system,Curved display,Seidel aberration theory,Refocusable,Field of view,Silicon die,Mechanical stress,Virtual reality headset."
"A Fixation-based 360° Benchmark Dataset for Salient Object
  Detection","Fixation prediction (FP) in panoramic contents has been widely investigated
along with the booming trend of virtual reality (VR) applications. However,
another issue within the field of visual saliency, salient object detection
(SOD), has been seldom explored in 360{\deg} (or omnidirectional) images due to
the lack of datasets representative of real scenes with pixel-level
annotations. Toward this end, we collect 107 equirectangular panoramas with
challenging scenes and multiple object classes. Based on the consistency
between FP and explicit saliency judgements, we further manually annotate 1,165
salient objects over the collected images with precise masks under the guidance
of real human eye fixation maps. Six state-of-the-art SOD models are then
benchmarked on the proposed fixation-based 360{\deg} image dataset (F-360iSOD),
by applying a multiple cubic projection-based fine-tuning method. Experimental
results show a limitation of the current methods when used for SOD in panoramic
images, which indicates the proposed dataset is challenging. Key issues for
360{\deg} SOD is also discussed. The proposed dataset is available at
https://github.com/PanoAsh/F-360iSOD.","['Yi Zhang', 'Lu Zhang', 'Wassim Hamidouche', 'Olivier Deforges']",2020-01-22T11:16:39Z,http://arxiv.org/abs/2001.07960v2,['cs.CV'],"Fixation prediction,360° images,Salient object detection,Benchmark dataset,Equirectangular panoramas,Saliency judgements,SOD models,Fine-tuning method,Panoramic images,Dataset availability"
Flightmare: A Flexible Quadrotor Simulator,"State-of-the-art quadrotor simulators have a rigid and highly-specialized
structure: either are they really fast, physically accurate, or
photo-realistic. In this work, we propose a novel quadrotor simulator:
Flightmare. Flightmare is composed of two main components: a configurable
rendering engine built on Unity and a flexible physics engine for dynamics
simulation. Those two components are totally decoupled and can run
independently of each other. This makes our simulator extremely fast: rendering
achieves speeds of up to 230 Hz, while physics simulation of up to 200,000 Hz
on a laptop. In addition, Flightmare comes with several desirable features: (i)
a large multi-modal sensor suite, including an interface to extract the 3D
point-cloud of the scene; (ii) an API for reinforcement learning which can
simulate hundreds of quadrotors in parallel; and (iii) integration with a
virtual-reality headset for interaction with the simulated environment. We
demonstrate the flexibility of Flightmare by using it for two different robotic
tasks: quadrotor control using deep reinforcement learning and collision-free
path planning in a complex 3D environment.","['Yunlong Song', 'Selim Naji', 'Elia Kaufmann', 'Antonio Loquercio', 'Davide Scaramuzza']",2020-09-01T16:50:45Z,http://arxiv.org/abs/2009.00563v2,"['cs.RO', 'cs.AI']","Quadrotor simulator,Rendering engine,Physics engine,Dynamics simulation,Multi-modal sensor suite,Reinforcement learning,Virtual reality headset,Deep reinforcement learning,Path planning,3D environment"
Towards a Practical Virtual Office for Mobile Knowledge Workers,"As more people work from home or during travel, new opportunities and
challenges arise around mobile office work. On one hand, people may work at
flexible hours, independent of traffic limitations, but on the other hand, they
may need to work at makeshift spaces, with less than optimal working conditions
and decoupled from co-workers. Virtual Reality (VR) has the potential to change
the way information workers work: it enables personal bespoke working
environments even on the go and allows new collaboration approaches that can
help mitigate the effects of physical distance. In this paper, we investigate
opportunities and challenges for realizing a mobile VR offices environments and
discuss implications from recent findings of mixing standard off-the-shelf
equipment, such as tablets, laptops or desktops, with VR to enable effective,
efficient, ergonomic, and rewarding mobile knowledge work. Further, we
investigate the role of conceptual and physical spaces in a mobile VR office.","['Eyal Ofek', 'Jens Grubert', 'Michel Pahud', 'Mark Phillips', 'Per Ola Kristensson']",2020-09-07T08:53:04Z,http://arxiv.org/abs/2009.02947v1,"['cs.HC', 'H.5.2']","mobile knowledge workers,virtual reality,mobile office work,collaboration approaches,off-the-shelf equipment,ergonomic,conceptual spaces"
"Artificial Intelligence for 5G Wireless Systems: Opportunities,
  Challenges, and Future Research Directions","The advent of the wireless communications systems augurs new cutting-edge
technologies, including self-driving vehicles, unmanned aerial systems,
autonomous robots, Internet-of-Things, and virtual reality. These technologies
require high data rates, ultra-low latency, and high reliability, all of which
are promised by the fifth generation of wireless communication systems (5G).
Many research groups state that 5G cannot meet its demands without artificial
intelligence (AI) integration as 5G wireless networks are expected to generate
unprecedented traffic giving wireless research designers access to big data
that can help in predicting the demands and adjust cell designs to meet the
users requirements. Subsequently, many researchers applied AI in many aspects
of 5G wireless communication design including radio resource allocation,
network management, and cyber-security. In this paper, we provide an in-depth
review of AI for 5G wireless communication systems. In this respect, the aim of
this paper is to survey AI in 5G wireless communication systems by discussing
many case studies and the associated challenges, and shedding new light on
future research directions for leveraging AI in 5G wireless communications.","['Youness Arjoune', 'Saleh Faruque']",2020-09-10T15:42:00Z,http://arxiv.org/abs/2009.04943v1,['eess.SP'],"Artificial Intelligence,5G Wireless Systems,Self-driving vehicles,Unmanned aerial systems,Internet-of-Things,Virtual reality,Radio resource allocation,Network management,Cyber-security"
Segmented Pairwise Distance for Time Series with Large Discontinuities,"Time series with large discontinuities are common in many scenarios. However,
existing distance-based algorithms (e.g., DTW and its derivative algorithms)
may perform poorly in measuring distances between these time series pairs. In
this paper, we propose the segmented pairwise distance (SPD) algorithm to
measure distances between time series with large discontinuities. SPD is
orthogonal to distance-based algorithms and can be embedded in them. We
validate advantages of SPD-embedded algorithms over corresponding
distance-based ones on both open datasets and a proprietary dataset of surgical
time series (of surgeons performing a temporal bone surgery in a virtual
reality surgery simulator). Experimental results demonstrate that SPD-embedded
algorithms outperform corresponding distance-based ones in distance measurement
between time series with large discontinuities, measured by the Silhouette
index (SI).","['Jiabo He', 'Sarah Erfani', 'Sudanthi Wijewickrema', ""Stephen O'Leary"", 'Kotagiri Ramamohanarao']",2020-09-23T09:17:57Z,http://arxiv.org/abs/2009.11013v1,['cs.DB'],"segmented pairwise distance,time series,large discontinuities,distance-based algorithms,DTW,distance measurement,virtual reality,Silhouette index"
"From decision to action : intentionality, a guide for the specification
  of intelligent agents' behaviour","This article introduces a reflexion about behavioural specification for
interactive and participative agent-based simulation in virtual reality. Within
this context, it is neces sary to reach a high level of expressivness in order
to enforce interactions between the designer and the behavioural model during
the in-line prototyping. This requires to consider the need of semantic very
early in the design process. The Intentional agent model is here exposed as a
possible answer. It relies on a mixed imperative and declarative approach which
focuses on the link between decision and action. The design of a tool able to
simulate virtual environment implying agents based on this model is discuss","['Pierre De Loor', 'Favier Pierre-Alexandre']",2011-07-17T13:02:15Z,http://arxiv.org/abs/1107.3298v1,"['cs.AI', 'cs.MA']","intentionality,specification,intelligent agents,behavioural,simulation,virtual reality,expressiveness,semantic,intentional agent model"
Avatar-independent scripting for real-time gesture animation,"When animation of a humanoid figure is to be generated at run-time, instead
of by replaying pre-composed motion clips, some method is required of
specifying the avatar's movements in a form from which the required motion data
can be automatically generated. This form must be of a more abstract nature
than raw motion data: ideally, it should be independent of the particular
avatar's proportions, and both writable by hand and suitable for automatic
generation from higher-level descriptions of the required actions.
  We describe here the development and implementation of such a scripting
language for the particular area of sign languages of the deaf, called SiGML
(Signing Gesture Markup Language), based on the existing HamNoSys notation for
sign languages.
  We conclude by suggesting how this work may be extended to more general
animation for interactive virtual reality applications.",['Richard Kennaway'],2015-02-10T16:03:37Z,http://arxiv.org/abs/1502.02961v1,"['cs.GR', 'I.3.7']","real-time,gesture animation,humanoid figure,avatar-independent,scripting,motion data,abstract nature,sign languages,SiGML,HamNoSys"
The Ultimate Display,"Astronomical images and datasets are increasingly high-resolution and
multi-dimensional. The vast majority of astronomers perform all of their
visualisation and analysis tasks on low-resolution, two-dimensional desktop
monitors. If there were no technological barriers to designing the ultimate
stereoscopic display for astronomy, what would it look like? What capabilities
would we require of our compute hardware to drive it? And are existing
technologies even close to providing a true 3D experience that is compatible
with the depth resolution of human stereoscopic vision? We consider the CAVE2
(an 80 Megapixel, hybrid 2D and 3D virtual reality environment directly
integrated with a 100 Tflop/s GPU-powered supercomputer) and the Oculus Rift (a
low- cost, head-mounted display) as examples at opposite financial ends of the
immersive display spectrum.","['C. J. Fluke', 'D. G. Barnes']",2016-01-14T01:18:51Z,http://arxiv.org/abs/1601.03459v1,['astro-ph.IM'],"high-resolution,multi-dimensional,stereoscopic display,astronomy,compute hardware,3D experience,depth resolution,virtual reality,GPU,head-mounted display"
Heat as an inertial force: A quantum equivalence principle,"The firewall was introduced into black hole evaporation scenarios as a deus
ex machina designed to break entanglements and preserve unitarity (Almheiri
et.al., 2013). Here we show that the firewall actually exists and does break
entanglements, but only in the context of a virtual reality for observers
stationed near the horizon, who are following the long-term evolution of the
hole. These observers are heated by acceleration radiation at the Unruh
temperature and see pair creation at the horizon as a high-energy phenomenon.
The objective reality is very different. We argue that Hawking pair creation is
entirely a low-energy process in which entanglements never arise. The Hawking
particles materialize as low-energy excitations with typical wavelength
considerably larger than the black hole radius. They thus emerge into a very
non-uniform environment inimical to entanglement-formation.","['K. Thanjavur', 'W. Israel']",2016-01-17T18:07:56Z,http://arxiv.org/abs/1601.04319v2,"['gr-qc', 'hep-th']","inertial force,quantum equivalence principle,firewall,black hole evaporation,entanglements,unitarity,acceleration radiation,Unruh temperature,pair creation,Hawking particles,entanglement-formation"
A Comparative Study of Algorithms for Realtime Panoramic Video Blending,"Unlike image blending algorithms, video blending algorithms have been little
studied. In this paper, we investigate 6 popular blending algorithms---feather
blending, multi-band blending, modified Poisson blending, mean value coordinate
blending, multi-spline blending and convolution pyramid blending. We consider
in particular realtime panoramic video blending, a key problem in various
virtual reality tasks. To evaluate the performance of the 6 algorithms on this
problem, we have created a video benchmark of several videos captured under
various conditions. We analyze the time and memory needed by the above 6
algorithms, for both CPU and GPU implementations (where readily
parallelizable). The visual quality provided by these algorithms is also
evaluated both objectively and subjectively. The video benchmark and algorithm
implementations are publicly available.","['Zhe Zhu', 'Jiaming Lu', 'Minxuan Wang', 'Songhai Zhang', 'Ralph Martin', 'Hantao Liu', 'Shimin Hu']",2016-06-01T03:26:43Z,http://arxiv.org/abs/1606.00103v2,['cs.CV'],"algorithms,realtime,panoramic,video blending,virtual reality,CPU,GPU,benchmark,parallelizable"
"A computer program for simulating time travel and a possible 'solution'
  for the grandfather paradox","While the possibility of time travel in physics is still debated, the
explosive growth of virtual-reality simulations opens up new possibilities to
rigorously explore such time travel and its consequences in the digital domain.
Here we provide a computational model of time travel and a computer program
that allows exploring digital time travel. In order to explain our method we
formalize a simplified version of the famous grandfather paradox, show how the
system can allow the participant to go back in time, try to kill their
ancestors before they were born, and experience the consequences. The system
has even come up with scenarios that can be considered consistent ""solutions""
of the grandfather paradox. We discuss the conditions for digital time travel,
which indicate that it has a large number of practical applications.",['Doron Friedman'],2016-09-26T15:09:29Z,http://arxiv.org/abs/1609.08470v1,['cs.AI'],"time travel,computer program,simulation,grandfather paradox,computational model,virtual-reality,digital domain,digital time travel,system,participant"
"FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in Virtual
  Reality","We propose FaceVR, a novel image-based method that enables video
teleconferencing in VR based on self-reenactment. State-of-the-art face
tracking methods in the VR context are focused on the animation of rigged 3d
avatars. While they achieve good tracking performance the results look
cartoonish and not real. In contrast to these model-based approaches, FaceVR
enables VR teleconferencing using an image-based technique that results in
nearly photo-realistic outputs. The key component of FaceVR is a robust
algorithm to perform real-time facial motion capture of an actor who is wearing
a head-mounted display (HMD), as well as a new data-driven approach for eye
tracking from monocular videos. Based on reenactment of a prerecorded stereo
video of the person without the HMD, FaceVR incorporates photo-realistic
re-rendering in real time, thus allowing artificial modifications of face and
eye appearances. For instance, we can alter facial expressions or change gaze
directions in the prerecorded target video. In a live setup, we apply these
newly-introduced algorithmic components.","['Justus Thies', 'Michael Zollhöfer', 'Marc Stamminger', 'Christian Theobalt', 'Matthias Nießner']",2016-10-11T01:35:56Z,http://arxiv.org/abs/1610.03151v2,['cs.CV'],"FaceVR,facial reenactment,eye gaze control,virtual reality,image-based method,facial motion capture,head-mounted display (HMD),data-driven approach,eye tracking,photo-realistic outputs"
Image Based Camera Localization: an Overview,"Recently, virtual reality, augmented reality, robotics, autonomous driving et
al attract much attention of both academic and industrial community, in which
image based camera localization is a key task. However, there has not been a
complete review on image-based camera localization. It is urgent to map this
topic to help people enter the field quickly. In this paper, an overview of
image based camera localization is presented. A new and complete kind of
classifications for image based camera localization is provided and the related
techniques are introduced. Trends for the future development are also
discussed. It will be useful to not only researchers but also engineers and
other people interested.","['Yihong Wu', 'Fulin Tang', 'Heping Li']",2016-10-12T10:19:36Z,http://arxiv.org/abs/1610.03660v4,['cs.CV'],"image-based camera localization,virtual reality,augmented reality,robotics,autonomous driving,classifications,techniques,future development,researchers,engineers"
How efficiently can one untangle a double-twist? Waving is believing!,"It has long been known to mathematicians and physicists that while a full
rotation in three-dimensional Euclidean space causes tangling, two rotations
can be untangled. Formally, an untangling is a based nullhomotopy of the
double-twist loop in the special orthogonal group of rotations. We study a
particularly simple, geometrically defined untangling procedure, leading to new
conclusions regarding the minimum possible complexity of untanglings. We
animate and analyze how our untangling operates on frames in 3-space, and teach
readers in a video how to wave the nullhomotopy with their hands.","['David Pengelley', 'Daniel Ramras']",2016-10-15T01:48:02Z,http://arxiv.org/abs/1610.04680v2,"['math.GT', 'math.AT', '57S05 (Primary), 57S15, 57M99, 55P99, 55Q99 (Secondary)']","double-twist,untangling,rotation,nullhomotopy,special orthogonal group,complexity,frames,3-space,video,waving"
MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System,"The basis for most vision based applications like robotics, self-driving cars
and potentially augmented and virtual reality is a robust, continuous
estimation of the position and orientation of a camera system w.r.t the
observed environment (scene). In recent years many vision based systems that
perform simultaneous localization and mapping (SLAM) have been presented and
released as open source. In this paper, we extend and improve upon a
state-of-the-art SLAM to make it applicable to arbitrary, rigidly coupled
multi-camera systems (MCS) using the MultiCol model. In addition, we include a
performance evaluation on accurate ground truth and compare the robustness of
the proposed method to a single camera version of the SLAM system. An open
source implementation of the proposed multi-fisheye camera SLAM system can be
found on-line https://github.com/urbste/MultiCol-SLAM.","['Steffen Urban', 'Stefan Hinz']",2016-10-24T09:27:47Z,http://arxiv.org/abs/1610.07336v1,['cs.CV'],"vision-based applications,robotics,self-driving cars,augmented reality,virtual reality,simultaneous localization and mapping,SLAM,multi-camera systems,MultiCol model"
"A low cost non-wearable gaze detection system based on infrared image
  processing","Human eye gaze detection plays an important role in various fields, including
human-computer interaction, virtual reality and cognitive science. Although
different relatively accurate systems of eye tracking and gaze detection exist,
they are usually either too expensive to be bought for low cost applications or
too complex to be implemented easily. In this article, we propose a
non-wearable system for eye tracking and gaze detection with low complexity and
cost. The proposed system provides a medium accuracy which makes it suitable
for general applications in which low cost and easy implementation is more
important than achieving very precise gaze detection. The proposed method
includes pupil and marker detection using infrared image processing, and gaze
evaluation using an interpolation-based strategy. The interpolation-based
strategy exploits the positions of the detected pupils and markers in a target
captured image and also in some previously captured training images for
estimating the position of a point that the user is gazing at. The proposed
system has been evaluated by three users in two different lighting conditions.
The experimental results show that the accuracy of this low cost system can be
between 90% and 100% for finding major gazing directions.","['Ehsan Arbabi', 'Mohammad Shabani', 'Ali Yarigholi']",2017-09-12T07:46:05Z,http://arxiv.org/abs/1709.03717v1,"['cs.HC', 'cs.CV']","infrared image processing,gaze detection,eye tracking,pupil detection,marker detection,interpolation-based strategy,low cost system,accuracy,lighting conditions"
Visual to Sound: Generating Natural Sound for Videos in the Wild,"As two of the five traditional human senses (sight, hearing, taste, smell,
and touch), vision and sound are basic sources through which humans understand
the world. Often correlated during natural events, these two modalities combine
to jointly affect human perception. In this paper, we pose the task of
generating sound given visual input. Such capabilities could help enable
applications in virtual reality (generating sound for virtual scenes
automatically) or provide additional accessibility to images or videos for
people with visual impairments. As a first step in this direction, we apply
learning-based methods to generate raw waveform samples given input video
frames. We evaluate our models on a dataset of videos containing a variety of
sounds (such as ambient sounds and sounds from people/animals). Our experiments
show that the generated sounds are fairly realistic and have good temporal
synchronization with the visual inputs.","['Yipin Zhou', 'Zhaowen Wang', 'Chen Fang', 'Trung Bui', 'Tamara L. Berg']",2017-12-04T22:24:29Z,http://arxiv.org/abs/1712.01393v2,['cs.CV'],"visual input,sound generation,virtual reality,raw waveform samples,learning-based methods,dataset,ambient sounds,temporal synchronization,visual impairments"
"Concave/convex switchable lens using active phase-change material
  Ge3Sb2Te6","Normally, the focal length of a conventional lens is fixed. Scientists have
made much effort in modulating it into bifocal, which is very important for
virtual reality (VR) and argument reality (AR) 3D display. It is even much more
difficult for a lens to realize both convex and concave functions with one
geometric structure, that is, a concave/convex switchable lens, which can tune
3D real-images and virtual-images in AR and VR, corresponding to long depth of
view in 3D display. Based on the tunable refractive indexes of phase-change
materials, here we propose a series of concave/convex switchable lenses. When
the phase-change material is in different states, one switchable lens is able
to perform a negative or positive focal length, or perform negative and
positive focal lengths simultaneously. The lenses can be either cylindrical,
spherical or other types. For the superior characteristics, these switchable
lenses can be employed in various optical fields.","['Xueliang Shi', 'Juan Liu', 'Gaolei Xue', 'Weiting Peng', 'Bin Hu', 'Yongtian Wang']",2018-03-05T08:45:16Z,http://arxiv.org/abs/1803.01561v1,['physics.optics'],"phase-change material,concave,convex,switchable lens,focal length,refractive indexes,3D display,virtual reality,augmented reality"
"MIS-SLAM: Real-time Large Scale Dense Deformable SLAM System in Minimal
  Invasive Surgery Based on Heterogeneous Computing","Real-time simultaneously localization and dense mapping is very helpful for
providing Virtual Reality and Augmented Reality for surgeons or even surgical
robots. In this paper, we propose MIS-SLAM: a complete real-time large scale
dense deformable SLAM system with stereoscope in Minimal Invasive Surgery based
on heterogeneous computing by making full use of CPU and GPU. Idled CPU is used
to perform ORB- SLAM for providing robust global pose. Strategies are taken to
integrate modules from CPU and GPU. We solved the key problem raised in
previous work, that is, fast movement of scope and blurry images make the scope
tracking fail. Benefiting from improved localization, MIS-SLAM can achieve
large scale scope localizing and dense mapping in real-time. It transforms and
deforms current model and incrementally fuses new observation while keeping
vivid texture. In-vivo experiments conducted on publicly available datasets
presented in the form of videos demonstrate the feasibility and practicality of
MIS-SLAM for potential clinical purpose.","['Jingwei Song', 'Jun Wang', 'Liang Zhao', 'Shoudong Huang', 'Gamini Dissanayake']",2018-03-06T04:19:24Z,http://arxiv.org/abs/1803.02009v2,['cs.CV'],"Real-time,Dense mapping,SLAM system,Heterogeneous computing,CPU,GPU,ORB-SLAM,Stereoscope,Scope tracking,Deformable SLAM"
"QoE-Oriented Resource Allocation for 360-degree Video Transmission over
  Heterogeneous Networks","Immersive media streaming, especially virtual reality (VR)/360-degree video
streaming which is very bandwidth demanding, has become more and more popular
due to the rapid growth of the multimedia and networking deployments. To better
explore the usage of resource and achieve better quality of experience (QoE)
perceived by users, this paper develops an application-layer scheme to jointly
exploit the available bandwidth from the LTE and Wi-Fi networks in 360-degree
video streaming. This newly proposed scheme and the corresponding solution
algorithms utilize the saliency of video, prediction of users' view and the
status information of users to obtain an optimal association of the users with
different Wi-Fi access points (APs) for maximizing the system's utility.
Besides, a novel buffer strategy is proposed to mitigate the influence of
short-time prediction problem for transmitting 360-degree videos in
time-varying networks. The promising performance and low complexity of the
proposed scheme and algorithms are validated in simulations with various
360-degree videos.","['Wei Huang', 'Lianghui Ding', 'Hung-Yu Wei', 'Jenq-Neng Hwang', 'Yiling Xu', 'Wenjun Zhang']",2018-03-21T08:07:12Z,http://arxiv.org/abs/1803.07789v1,"['cs.MM', 'cs.NI']","QoE,Resource Allocation,360-degree Video,Heterogeneous Networks,Bandwidth,LTE,Wi-Fi,Saliency,Buffer Strategy,Simulation"
Multi-Task Trust Transfer for Human-Robot Interaction,"Trust is essential in shaping human interactions with one another and with
robots. This paper discusses how human trust in robot capabilities transfers
across multiple tasks. We first present a human-subject study of two distinct
task domains: a Fetch robot performing household tasks and a virtual reality
simulation of an autonomous vehicle performing driving and parking maneuvers.
The findings expand our understanding of trust and inspire new differentiable
models of trust evolution and transfer via latent task representations: (i) a
rational Bayes model, (ii) a data-driven neural network model, and (iii) a
hybrid model that combines the two. Experiments show that the proposed models
outperform prevailing models when predicting trust over unseen tasks and users.
These results suggest that (i) task-dependent functional trust models capture
human trust in robot capabilities more accurately, and (ii) trust transfer
across tasks can be inferred to a good degree. The latter enables
trust-mediated robot decision-making for fluent human-robot interaction in
multi-task settings.","['Harold Soh', 'Yaqi Xie', 'Min Chen', 'David Hsu']",2018-07-05T06:58:13Z,http://arxiv.org/abs/1807.01866v3,"['cs.RO', 'cs.HC']","Trust,Human-Robot Interaction,Multi-Task,Transfer,Bayesian model,Neural network model,Latent task representations,Trust evolution,Robot capabilities"
BRIEF: Backward Reduction of CNNs with Information Flow Analysis,"This paper proposes BRIEF, a backward reduction algorithm that explores
compact CNN-model designs from the information flow perspective. This algorithm
can remove substantial non-zero weighting parameters (redundant neural
channels) of a network by considering its dynamic behavior, which traditional
model-compaction techniques cannot achieve. With the aid of our proposed
algorithm, we achieve significant model reduction on ResNet-34 in the ImageNet
scale (32.3% reduction), which is 3X better than the previous result (10.8%).
Even for highly optimized models such as SqueezeNet and MobileNet, we can
achieve additional 10.81% and 37.56% reduction, respectively, with negligible
performance degradation.","['Yu-Hsun Lin', 'Chun-Nan Chou', 'Edward Y. Chang']",2018-07-16T08:32:54Z,http://arxiv.org/abs/1807.05726v3,"['cs.LG', 'cs.CV', 'stat.ML']","Backward reduction,CNNs,Information flow analysis,Compact CNN-model designs,Non-zero weighting parameters,Model-compaction techniques,ResNet-34,ImageNet scale,SqueezeNet,MobileNet"
Peeking Behind Objects: Layered Depth Prediction from a Single Image,"While conventional depth estimation can infer the geometry of a scene from a
single RGB image, it fails to estimate scene regions that are occluded by
foreground objects. This limits the use of depth prediction in augmented and
virtual reality applications, that aim at scene exploration by synthesizing the
scene from a different vantage point, or at diminished reality. To address this
issue, we shift the focus from conventional depth map prediction to the
regression of a specific data representation called Layered Depth Image (LDI),
which contains information about the occluded regions in the reference frame
and can fill in occlusion gaps in case of small view changes. We propose a
novel approach based on Convolutional Neural Networks (CNNs) to jointly predict
depth maps and foreground separation masks used to condition Generative
Adversarial Networks (GANs) for hallucinating plausible color and depths in the
initially occluded areas. We demonstrate the effectiveness of our approach for
novel scene view synthesis from a single image.","['Helisa Dhamo', 'Keisuke Tateno', 'Iro Laina', 'Nassir Navab', 'Federico Tombari']",2018-07-23T18:23:53Z,http://arxiv.org/abs/1807.08776v1,['cs.CV'],"depth estimation,RGB image,occluded regions,Layered Depth Image (LDI),convolutional neural networks (CNNs),foreground separation masks,Generative Adversarial Networks (GANs),scene view synthesis,augmented reality,virtual reality"
"Theology and Metaphysics in Sombre, Scientific Times","In view of the sobering findings of science, theology and to a lesser degree
metaphysics is confronted with a humiliating loss, and a need for
reinterpretation, of allegories and narratives which have served as guidance to
the perplexed for millennia. Future revolutions of world perception might
include the emergence of consciousness and superhuman artificial intelligence
from universal computation, extensive virtual reality simulations, the
persistence of claims of irreducible chance in the Universe, as well as
contacts with alien species and the abundance of inhabited planets. As tragic
and as discomforting as this might be perceived for the religious orthodoxy and
by individual believers, a theology guided by science may lead us to a better
and more adequate understanding of our existence. The post factum theological
options are plentiful. These include dualistic scenarios, as well as (to quote
Kelly James Clark), a curling or bowling deity, that is, creatio continua, or
ex nihilo. These might be grounded in, or corroborated by the metaphysical
enigma of existence, which appears to be immune and robust with respect to the
aforementioned challenges of science.",['Karl Svozil'],2018-09-14T10:19:02Z,http://arxiv.org/abs/1809.05339v2,['physics.hist-ph'],"theology,metaphysics,science,consciousness,artificial intelligence,virtual reality,universe,alien species,existence,metaphysical"
Optimal Multicast of Tiled 360 VR Video,"In this letter, we study optimal multicast of tiled 360 virtual reality (VR)
video from one server (base station or access point) to multiple users. We
consider random viewing directions and random channel conditions, and adopt
time division multiple access (TDMA). For given video quality, we optimize the
transmission time and power allocation to minimize the average transmission
energy. For given transmission energy budget, we optimize the transmission time
and power allocation as well as the encoding rate of each tile to maximize the
received video quality. These two optimization problems are challenging
non-convex problems. We obtain globally optimal closed-form solutions of the
two non-convex problems, which reveal important design insights for multicast
of tiled 360 VR video. Finally, numerical results demonstrate the advantage of
the proposed solutions.","['Chengjun Guo', 'Ying Cui', 'Zhi Liu']",2018-09-24T05:36:22Z,http://arxiv.org/abs/1809.08767v1,"['cs.IT', 'math.IT']","optimal multicast,tiled 360 VR video,random viewing directions,random channel conditions,time division multiple access (TDMA),transmission energy,encoding rate,closed-form solutions"
C4Synth: Cross-Caption Cycle-Consistent Text-to-Image Synthesis,"Generating an image from its description is a challenging task worth solving
because of its numerous practical applications ranging from image editing to
virtual reality. All existing methods use one single caption to generate a
plausible image. A single caption by itself, can be limited, and may not be
able to capture the variety of concepts and behavior that may be present in the
image. We propose two deep generative models that generate an image by making
use of multiple captions describing it. This is achieved by ensuring
'Cross-Caption Cycle Consistency' between the multiple captions and the
generated image(s). We report quantitative and qualitative results on the
standard Caltech-UCSD Birds (CUB) and Oxford-102 Flowers datasets to validate
the efficacy of the proposed approach.","['K J Joseph', 'Arghya Pal', 'Sailaja Rajanala', 'Vineeth N Balasubramanian']",2018-09-20T07:18:57Z,http://arxiv.org/abs/1809.10238v1,"['cs.CV', 'cs.AI', 'cs.LG', 'stat.ML']","text-to-image synthesis,deep generative models,cross-caption cycle consistency,image generation,multiple captions,Caltech-UCSD Birds dataset,Oxford-102 Flowers dataset,virtual reality"
Optimal Multicast of Tiled 360 VR Video in OFDMA Systems,"In this letter, we study optimal multicast of tiled 360 virtual reality (VR)
video from one server (base station or access point) to multiple users in an
orthogonal frequency division multiple access (OFDMA) system. For given video
quality, we optimize the subcarrier, transmission power and transmission rate
allocation to minimize the total transmission power. For given transmission
power budget, we optimize the subcarrier, transmission power and transmission
rate allocation to maximize the received video quality. These two optimization
problems are non-convex problems. We obtain a globally optimal closed-form
solution and a near optimal solution of the two problems, separately, both
revealing important design insights for multicast of tiled 360 VR video in
OFDMA systems.","['Chengjun Guo', 'Ying Cui', 'Zhi Liu']",2018-09-27T07:29:08Z,http://arxiv.org/abs/1809.10677v1,"['cs.IT', 'math.IT']","Multicast,Tiled 360 VR Video,OFDMA Systems,Subcarrier,Transmission Power,Transmission Rate Allocation,Total Transmission Power,Received Video Quality,Non-Convex Problems"
Internet Congestion Control via Deep Reinforcement Learning,"We present and investigate a novel and timely application domain for deep
reinforcement learning (RL): Internet congestion control. Congestion control is
the core networking task of modulating traffic sources' data-transmission rates
to efficiently utilize network capacity, and is the subject of extensive
attention in light of the advent of Internet services such as live video,
virtual reality, Internet-of-Things, and more. We show that casting congestion
control as RL enables training deep network policies that capture intricate
patterns in data traffic and network conditions, and leverage this to
outperform the state-of-the-art. We also highlight significant challenges
facing real-world adoption of RL-based congestion control, including fairness,
safety, and generalization, which are not trivial to address within
conventional RL formalism. To facilitate further research and reproducibility
of our results, we present a test suite for RL-guided congestion control based
on the OpenAI Gym interface.","['Nathan Jay', 'Noga H. Rotman', 'P. Brighten Godfrey', 'Michael Schapira', 'Aviv Tamar']",2018-10-08T03:43:25Z,http://arxiv.org/abs/1810.03259v3,['cs.NI'],"deep reinforcement learning,internet congestion control,networking,traffic sources,data transmission,network capacity,data traffic,network conditions,state-of-the-art,OpenAI Gym"
"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality","The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it.",['Nicole M. Radziwill'],2018-10-17T23:06:06Z,http://arxiv.org/abs/1810.07829v1,['cs.CY'],"cloud computing,big data,virtual reality,augmented reality,blockchain,additive manufacturing,artificial intelligence,machine learning,Internet of Things,cyber-physical systems"
"Immercity: a curation content application in Virtual and Augmented
  reality","When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']",2018-10-24T06:23:46Z,http://arxiv.org/abs/1810.10206v1,"['cs.GR', 'cs.HC']","curation,content application,Virtual Reality,Mixed Reality,Augmented Reality,technology definitions,interaction techniques,artificial intelligence integration"
Very Long Term Field of View Prediction for 360-degree Video Streaming,"360-degree videos have gained increasing popularity in recent years with the
developments and advances in Virtual Reality (VR) and Augmented Reality (AR)
technologies. In such applications, a user only watches a video scene within a
field of view (FoV) centered in a certain direction. Predicting the future FoV
in a long time horizon (more than seconds ahead) can help save bandwidth
resources in on-demand video streaming while minimizing video freezing in
networks with significant bandwidth variations. In this work, we treat the FoV
prediction as a sequence learning problem, and propose to predict the target
user's future FoV not only based on the user's own past FoV center trajectory
but also other users' future FoV locations. We propose multiple prediction
models based on two different FoV representations: one using FoV center
trajectories and another using equirectangular heatmaps that represent the FoV
center distributions. Extensive evaluations with two public datasets
demonstrate that the proposed models can significantly outperform benchmark
models, and other users' FoVs are very helpful for improving long-term
predictions.","['Chenge Li', 'Weixi Zhang', 'Yong Liu', 'Yao Wang']",2019-02-04T19:43:40Z,http://arxiv.org/abs/1902.01439v1,['cs.CV'],"Field of View,360-degree video,Streaming,Virtual Reality,Augmented Reality,Bandwidth,Sequence learning,Equirectangular heatmaps,Prediction models"
"wavEMS: Improving Signal Variation Freedom of Electrical Muscle
  Stimulation","There has been a long history in electrical muscle stimulation (EMS), which
has been used for medical and interaction purposes. Human-computer interaction
(HCI) researchers are now working on various applications, including virtual
reality (VR), notification, and learning. For the electric signals applied to
the human body, various types of waveforms have been considered and tested. In
typical applications, pulses with short duration are applied, however, many
perspectives are required to be considered. In addition to the duration and
polarity of the pulse/waves, the wave shapes can also be an essential factor to
consider. A problem of conventional EMS toolkits and systems are that they have
a limitation to the variety of signals that it can produce. For example, some
may be limited to monophonic pulses. Furthermore, they are usually limited to
rectangular pulses and a limited range of frequencies, and other waveforms
cannot be produced. These kinds of limitations make us challenging to consider
variations of EMS signals in HCI research and applications. The purpose of
""{\it wavEMS}"" is to encourage testing of a variety of waveforms for EMS, which
can be manipulated through audio output. We believe that this can help improve
HCI applications, and to open up new application areas.","['Michinari Kono', 'Jun Rekimoto']",2019-02-08T16:46:43Z,http://arxiv.org/abs/1902.03184v1,['cs.HC'],"electrical muscle stimulation,waveforms,pulses,wave shapes,frequencies,EMS signals,HCI research,wavEMS,human-computer interaction"
"Virtual Manipulation in an Immersive Virtual Environment: Simulation of
  Virtual Assembly","To fill the lack of research efforts in virtual assembly of modules and
training, this paper presents a virtual manipulation of building objects in an
Immersive Virtual Environment (IVE). A worker wearing a Virtual Reality (VR)
head-mounted device (HMD) virtually perform an assembly of multiple modules
while identifying any issues. Hand motions of the worker are tracked by a
motion sensor mounted on the HMD. The worker can be graded based on his/her
overall performance and speed during this VR simulation. The developed VR
simulation can ultimately enable workers to identify unforeseen issues (e.g.,
not enough clearance for an object to be installed). The presented method can
solve current deficiencies in discrepancy detection in 3D scanned models of
elements. The developed VR platform can also be used for interactive training
and simulation sessions that can potentially improve efficiency and help
achieve better work performance for assemblies of complex systems.","['Mojtaba Noghabaei', 'Khashayar Asadi', 'Kevin Han']",2019-01-30T20:12:09Z,http://arxiv.org/abs/1902.05099v1,['cs.CY'],"Virtual manipulation,Immersive Virtual Environment,Virtual Reality,assembly,modules,simulation,motion sensor,discrepancy detection,3D scanned models,interactive training"
Virtual Environments for Rehabilitation of Postural Control Dysfunction,"We developed a novel virtual reality [VR] platform with 3-dimensional sounds
to help improve sensory integration and visuomotor processing for postural
control and fall prevention in individuals with balance problems related to
sensory deficits, such as vestibular dysfunction (disease of the inner ear).
The system has scenes that simulate scenario-based environments. We can adjust
the intensity of the visual and audio stimuli in the virtual scenes by
controlling the user interface (UI) settings. A VR headset (HTC Vive or Oculus
Rift) delivers stereo display while providing real-time position and
orientation of the participants' head. The 3D game-like scenes make
participants feel immersed and gradually exposes them to situations that may
induce dizziness, anxiety or imbalance in their daily-living.","['Zhu Wang', 'Anat Lubetzky', 'Marta Gospodarek', 'Makan TaghaviDilamani', 'Ken Perlin']",2019-02-08T20:16:42Z,http://arxiv.org/abs/1902.10223v1,['cs.HC'],"Virtual reality,Postural control,Rehabilitation,Sensory integration,Visuomotor processing,Balance problems,Vestibular dysfunction,User interface (UI),VR headset,3D sounds"
"Co-Designing in Social VR. Process awareness and suitable
  representations to empower user participation","To allow non-designers' involvement in design projects new methods are
needed. Co-design gives the same opportunity to all the multidisciplinary
participants to co-create ideas simultaneously. Nevertheless, current co-design
processes involving such users tend to limit their contribution to the proposal
of basic design ideas only through brainstorming. The co-design approach needs
to be enhanced by a properly suited representational ecosystem supporting
active participation and by conscious use of structured verbal exchanges giving
awareness of the creative process. In this respect, we developed two social
virtual reality co-design systems, and a co-design verbal exchange methodology
to favour participants' awareness of the co-creative process. By using such
representations and verbal exchanges, participants could co-create with more
ease by benefiting from being informed of the process and from the collective
immersion, empowering their participation. This paper presents the rationale
behind this approach of using Social VR in co-design and the feedback of three
co-design workshops.","['Tomás Dorta', 'Stéphane Safin', 'Sana Boudhraâ', 'Emmanuel Beaudry Marchand']",2019-06-26T12:02:20Z,http://arxiv.org/abs/1906.11004v1,['cs.HC'],"Co-design,Social VR,Process awareness,Representations,User participation,Virtual reality,Verbal exchange,Creative process,Co-creative process,Workshop feedback"
Action Prediction in Humans and Robots,"Efficient action prediction is of central importance for the fluent workflow
between humans and equally so for human-robot interaction. To achieve
prediction, actions can be encoded by a series of events, where every event
corresponds to a change in a (static or dynamic) relation between some of the
objects in a scene. Manipulation actions and others can be uniquely encoded
this way and only, on average, less than 60% of the time series has to pass
until an action can be predicted. Using a virtual reality setup and testing ten
different manipulation actions, here we show that in most cases humans predict
actions at the same event as the algorithm. In addition, we perform an in-depth
analysis about the temporal gain resulting from such predictions when chaining
actions and show in some robotic experiments that the percentage gain for
humans and robots is approximately equal. Thus, if robots use this algorithm
then their prediction-moments will be compatible to those of their human
interaction partners, which should much benefit natural human-robot
collaboration.","['Florentin Wörgötter', 'Fatemeh Ziaeetabar', 'Stefan Pfeiffer', 'Osman Kaya', 'Tomas Kulvicius', 'Minija Tamosiunaite']",2019-07-03T13:23:03Z,http://arxiv.org/abs/1907.01932v1,"['cs.RO', 'cs.AI']","action prediction,humans,robots,events,manipulation actions,virtual reality,temporal gain,algorithm,robotic experiments,human-robot collaboration"
BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs,"We present BlazeFace, a lightweight and well-performing face detector
tailored for mobile GPU inference. It runs at a speed of 200-1000+ FPS on
flagship devices. This super-realtime performance enables it to be applied to
any augmented reality pipeline that requires an accurate facial region of
interest as an input for task-specific models, such as 2D/3D facial keypoint or
geometry estimation, facial features or expression classification, and face
region segmentation. Our contributions include a lightweight feature extraction
network inspired by, but distinct from MobileNetV1/V2, a GPU-friendly anchor
scheme modified from Single Shot MultiBox Detector (SSD), and an improved tie
resolution strategy alternative to non-maximum suppression.","['Valentin Bazarevsky', 'Yury Kartynnik', 'Andrey Vakunov', 'Karthik Raveendran', 'Matthias Grundmann']",2019-07-11T08:40:08Z,http://arxiv.org/abs/1907.05047v2,['cs.CV'],"BlazeFace,Neural Face Detection,Mobile GPUs,GPU inference,augmented reality pipeline,facial keypoint,facial features,expression classification,face region segmentation,SSD."
Real-time Hair Segmentation and Recoloring on Mobile GPUs,"We present a novel approach for neural network-based hair segmentation from a
single camera input specifically designed for real-time, mobile application.
Our relatively small neural network produces a high-quality hair segmentation
mask that is well suited for AR effects, e.g. virtual hair recoloring. The
proposed model achieves real-time inference speed on mobile GPUs (30-100+ FPS,
depending on the device) with high accuracy. We also propose a very realistic
hair recoloring scheme. Our method has been deployed in major AR application
and is used by millions of users.","['Andrei Tkachenka', 'Gregory Karpiak', 'Andrey Vakunov', 'Yury Kartynnik', 'Artsiom Ablavatski', 'Valentin Bazarevsky', 'Siargey Pisarchyk']",2019-07-15T20:39:15Z,http://arxiv.org/abs/1907.06740v1,['cs.CV'],"neural network,hair segmentation,real-time,mobile GPUs,AR effects,inference speed,recoloring,deployment,AR application"
Towards Markerless Grasp Capture,"Humans excel at grasping objects and manipulating them. Capturing human
grasps is important for understanding grasping behavior and reconstructing it
realistically in Virtual Reality (VR). However, grasp capture - capturing the
pose of a hand grasping an object, and orienting it w.r.t. the object - is
difficult because of the complexity and diversity of the human hand, and
occlusion. Reflective markers and magnetic trackers traditionally used to
mitigate this difficulty introduce undesirable artifacts in images and can
interfere with natural grasping behavior. We present preliminary work on a
completely marker-less algorithm for grasp capture from a video depicting a
grasp. We show how recent advances in 2D hand pose estimation can be used with
well-established optimization techniques. Uniquely, our algorithm can also
capture hand-object contact in detail and integrate it in the grasp capture
process. This is work in progress, find more details at https://contactdb.
cc.gatech.edu/grasp_capture.html.","['Samarth Brahmbhatt', 'Charles C. Kemp', 'James Hays']",2019-07-17T08:41:21Z,http://arxiv.org/abs/1907.07388v1,['cs.CV'],"grasp capture,markerless,virtual reality,hand pose estimation,optimization techniques,hand-object contact"
Temporally Coherent General Dynamic Scene Reconstruction,"Existing techniques for dynamic scene reconstruction from multiple
wide-baseline cameras primarily focus on reconstruction in controlled
environments, with fixed calibrated cameras and strong prior constraints. This
paper introduces a general approach to obtain a 4D representation of complex
dynamic scenes from multi-view wide-baseline static or moving cameras without
prior knowledge of the scene structure, appearance, or illumination.
Contributions of the work are: An automatic method for initial coarse
reconstruction to initialize joint estimation; Sparse-to-dense temporal
correspondence integrated with joint multi-view segmentation and reconstruction
to introduce temporal coherence; and a general robust approach for joint
segmentation refinement and dense reconstruction of dynamic scenes by
introducing shape constraint. Comparison with state-of-the-art approaches on a
variety of complex indoor and outdoor scenes, demonstrates improved accuracy in
both multi-view segmentation and dense reconstruction. This paper demonstrates
unsupervised reconstruction of complete temporally coherent 4D scene models
with improved non-rigid object segmentation and shape reconstruction and its
application to free-viewpoint rendering and virtual reality.","['Armin Mustafa', 'Marco Volino', 'Hansung Kim', 'Jean-Yves Guillemaut', 'Adrian Hilton']",2019-07-18T12:33:25Z,http://arxiv.org/abs/1907.08195v2,['cs.CV'],"dynamic scene reconstruction,wide-baseline cameras,multi-view,4D representation,temporal coherence,joint estimation,segmentation,dense reconstruction,shape constraint,non-rigid object segmentation"
"Hand-Gesture-Recognition Based Text Input Method for AR/VR Wearable
  Devices","Static and dynamic hand movements are basic way for human-machine
interactions. To recognize and classify these movements, first these movements
are captured by the cameras mounted on the augmented reality (AR) or virtual
reality (VR) wearable devices. The hand is segmented using segmentation method
and its gestures are passed to hand gesture recognition algorithm, which
depends on depth-wise separable convolutional neural network for training,
testing and finally running smoothly on mobile AR/VR devices, while maintaining
the accuracy and balancing the load. A number of gestures are processed for
identification of right gesture and to classify the gesture and ignore the all
intermittent gestures. With proposed method, a user can write letters and
numbers in air by just moving his/her hand in air. Gesture based operations are
performed, and trajectory of hand is recorded as handwritten text. Finally,
that handwritten text is processed for the text recognition.","['Nizamuddin Maitlo', 'Yanbo Wang', 'Chao Ping Chen', 'Lantian Mi', 'Wenbo Zhang']",2019-07-29T02:53:21Z,http://arxiv.org/abs/1907.12188v2,['cs.HC'],"hand gesture recognition,text input method,AR,VR,wearable devices,segmentation method,convolutional neural network,mobile devices,gesture classification,text recognition"
Video Stitching for Linear Camera Arrays,"Despite the long history of image and video stitching research, existing
academic and commercial solutions still produce strong artifacts. In this work,
we propose a wide-baseline video stitching algorithm for linear camera arrays
that is temporally stable and tolerant to strong parallax. Our key insight is
that stitching can be cast as a problem of learning a smooth spatial
interpolation between the input videos. To solve this problem, inspired by
pushbroom cameras, we introduce a fast pushbroom interpolation layer and
propose a novel pushbroom stitching network, which learns a dense flow field to
smoothly align the multiple input videos for spatial interpolation. Our
approach outperforms the state-of-the-art by a significant margin, as we show
with a user study, and has immediate applications in many areas such as virtual
reality, immersive telepresence, autonomous driving, and video surveillance.","['Wei-Sheng Lai', 'Orazio Gallo', 'Jinwei Gu', 'Deqing Sun', 'Ming-Hsuan Yang', 'Jan Kautz']",2019-07-31T17:42:38Z,http://arxiv.org/abs/1907.13622v1,['cs.CV'],"video stitching,linear camera arrays,pushbroom interpolation layer,pushbroom stitching network,dense flow field,spatial interpolation,virtual reality,immersive telepresence,autonomous driving,video surveillance"
"A Decentralized Time- and Energy-Optimal Control Framework for Connected
  Automated Vehicles: From Simulation to Field Test","The implementation of connected and automated vehicle (CAV) technologies
enables a novel computational framework for real-time control aimed at
optimizing energy consumption with associated benefits. In this paper, we
implement an optimal control framework, developed previously, in an Audi A3
etron plug-in hybrid electric vehicle, and demonstrate that we can improve the
vehicle's efficiency and travel time in a corridor including an on-ramp
merging, a speed reduction zone, and a roundabout. Our exposition includes the
development, integration, implementation and validation of the proposed
framework in (1) simulation, (2) hardware-in-the-loop (HIL) testing, (3)
connectivity enabled virtual reality based bench-test, and (4) field test in
Mcity. We show that by adopting such inexpensive, yet effective process, we can
efficiently integrate and test the controller framework, ensure proper
connectivity and data transmission between different modules of the system, and
reduce uncertainty. We evaluate the performance and effectiveness of the
control framework and observe significant improvement in terms of energy and
travel time compared to the baseline scenario.","['A M Ishtiaque Mahbub', 'Vasanthi Karri', 'Darshil Parikh', 'Shyam Jade', 'Andreas A. Malikopoulos']",2019-11-04T18:09:02Z,http://arxiv.org/abs/1911.01380v2,"['math.OC', 'eess.SP']","decentralized,optimal control framework,connected automated vehicles,energy consumption,simulation,field test"
"TouchVR: a Wearable Haptic Interface for VR Aimed at Delivering
  Multi-modal Stimuli at the User's Palm","TouchVR is a novel wearable haptic interface which can deliver multimodal
tactile stimuli on the palm by DeltaTouch haptic display and vibrotactile
feedback on the fingertips by vibration motors for the Virtual Reality (VR)
user. DeltaTouch display is capable of generating 3D force vector at the
contact point and presenting multimodal tactile sensation of weight, slippage,
encounter, softness, and texture. The VR system consists of HTC Vive Pro base
stations and head-mounted display (HMD), and Leap Motion controller for
tracking the user's hands motion in VR. The MatrixTouch, BallFeel, and RoboX
applications have been developed to demonstrate the capabilities of the
proposed technology. A novel haptic interface can potentially bring a new level
of immersion of the user in VR and make it more interactive and tangible.","['Daria Trinitatova', 'Dzmitry Tsetserukou']",2019-11-11T17:06:18Z,http://arxiv.org/abs/1911.04395v1,"['cs.RO', 'cs.HC']","Wearable,Haptic Interface,Virtual Reality,Tactile Stimuli,DeltaTouch,Vibrotactile Feedback,HTC Vive Pro,Leap Motion,Immersion"
"SlingDrone: Mixed Reality System for Pointing and Interaction Using a
  Single Drone","We propose SlingDrone, a novel Mixed Reality interaction paradigm that
utilizes a micro-quadrotor as both pointing controller and interactive robot
with a slingshot motion type. The drone attempts to hover at a given position
while the human pulls it in desired direction using a hand grip and a leash.
Based on the displacement, a virtual trajectory is defined. To allow for
intuitive and simple control, we use virtual reality (VR) technology to trace
the path of the drone based on the displacement input. The user receives force
feedback propagated through the leash. Force feedback from SlingDrone coupled
with visualized trajectory in VR creates an intuitive and user friendly
pointing device. When the drone is released, it follows the trajectory that was
shown in VR. Onboard payload (e.g. magnetic gripper) can perform various
scenarios for real interaction with the surroundings, e.g. manipulation or
sensing. Unlike HTC Vive controller, SlingDrone does not require handheld
devices, thus it can be used as a standalone pointing technology in VR.","['Evgeny Tsykunov', 'Roman Ibrahimov', 'Derek Vasquez', 'Dzmitry Tsetserukou']",2019-11-12T05:30:24Z,http://arxiv.org/abs/1911.04680v1,"['cs.RO', 'cs.HC']","Mixed Reality,Pointing Controller,Interactive Robot,Micro-quadrotor,Virtual Reality,Force Feedback,Onboard Payload,Magnetic Gripper,HTC Vive Controller,VR Technology."
"Recognizing Facial Expressions of Occluded Faces using Convolutional
  Neural Networks","In this paper, we present an approach based on convolutional neural networks
(CNNs) for facial expression recognition in a difficult setting with severe
occlusions. More specifically, our task is to recognize the facial expression
of a person wearing a virtual reality (VR) headset which essentially occludes
the upper part of the face. In order to accurately train neural networks for
this setting, in which faces are severely occluded, we modify the training
examples by intentionally occluding the upper half of the face. This forces the
neural networks to focus on the lower part of the face and to obtain better
accuracy rates than models trained on the entire faces. Our empirical results
on two benchmark data sets, FER+ and AffectNet, show that our CNN models'
predictions on lower-half faces are up to 13% higher than the baseline CNN
models trained on entire faces, proving their suitability for the VR setting.
Furthermore, our models' predictions on lower-half faces are no more than 10%
under the baseline models' predictions on full faces, proving that there are
enough clues in the lower part of the face to accurately predict facial
expressions.","['Mariana-Iuliana Georgescu', 'Radu Tudor Ionescu']",2019-11-12T13:53:56Z,http://arxiv.org/abs/1911.04852v1,"['cs.CV', 'cs.LG']","facial expression recognition,occluded faces,convolutional neural networks,virtual reality,VR headset,benchmark data sets,lower-half faces,training examples,accuracy rates"
Quality Assessment of DIBR-synthesized views: An Overview,"The Depth-Image-Based-Rendering (DIBR) is one of the main fundamental
technique to generate new views in 3D video applications, such as Multi-View
Videos (MVV), Free-Viewpoint Videos (FVV) and Virtual Reality (VR). However,
the quality assessment of DIBR-synthesized views is quite different from the
traditional 2D images/videos. In recent years, several efforts have been made
towards this topic, but there {is a lack of} detailed survey in {the}
literature. In this paper, we provide a comprehensive survey on various current
approaches for DIBR-synthesized views. The current accessible datasets of
DIBR-synthesized views are firstly reviewed{, followed} by a summary analysis
of the representative state-of-the-art objective metrics. Then, the
performances of different objective metrics are evaluated and discussed on all
available datasets. Finally, we discuss the potential challenges and suggest
possible directions for future research.","['Shishun Tian', 'Lu Zhang', 'Wenbin Zou', 'Xia Li', 'Ting Su', 'Luce Morin', 'Olivier Deforges']",2019-11-16T14:13:56Z,http://arxiv.org/abs/1911.07036v2,"['eess.IV', 'cs.CV', 'cs.MM']","Quality Assessment,DIBR,Synthesized Views,3D Video Applications,Multi-View Videos,Free-Viewpoint Videos,Virtual Reality,Objective Metrics,Accessible Datasets,Performance Evaluation"
Construction of a Validated Virtual Embodiment Questionnaire,"User embodiment is important for many virtual reality (VR) applications, for
example, in the context of social interaction, therapy, training, or
entertainment. However, there is no validated instrument to empirically measure
the perception of embodiment, necessary to reliably evaluate this important
quality of user experience (UX). To assess components of virtual embodiment in
a valid, reliable, and consistent fashion, we develped a Virtual Embodiment
Questionnaire (VEQ). We reviewed previous literature to identify applicable
constructs and items, and performed a confirmatory factor analysis (CFA) on the
data from three experiments (N = 196). Each experiment modified a distinct
simulation property, namely, the level of immersion, the level of
personalization, and the level of behavioral realism. The analysis confirmed
three factors: (1) ownership of a virtual body, (2) agency over a virtual body,
and (3) change in the perceived body schema. A fourth study (N = 22) further
confirmed the reliability and validity of the scale and investigated the
impacts of latency jitter of avatar movements presented in the simulation
compared to linear latencies and a baseline. We present the final scale and
further insights from the studies regarding related constructs.","['Daniel Roth', 'Marc Erich Latoschik']",2019-11-22T18:23:19Z,http://arxiv.org/abs/1911.10176v2,['cs.HC'],"virtual embodiment,questionnaire,user experience,confirmatory factor analysis,simulation property,immersion,personalization,behavioral realism,body schema,latency"
"Inattentional Blindness for Redirected Walking Using Dynamic Foveated
  Rendering","Redirected walking is a Virtual Reality(VR) locomotion technique which
enables users to navigate virtual environments (VEs) that are spatially larger
than the available physical tracked space. In this work we present a novel
technique for redirected walking in VR based on the psychological phenomenon of
inattentional blindness. Based on the user's visual fixation points we divide
the user's view into zones. Spatially-varying rotations are applied according
to the zone's importance and are rendered using foveated rendering. Our
technique is real-time and applicable to small and large physical spaces.
Furthermore, the proposed technique does not require the use of stimulated
saccades but rather takes advantage of naturally occurring saccades and blinks
for a complete refresh of the framebuffer. We performed extensive testing and
present the analysis of the results of three user studies conducted for the
evaluation.","['Yashas Joshi', 'Charalambos Poullis']",2019-11-27T18:08:21Z,http://arxiv.org/abs/1911.12327v1,['cs.GR'],"Redirected walking,Dynamic foveated rendering,Inattentional blindness,Virtual reality,Locomotion technique,User's visual fixation points,Spatially-varying rotations,Foveated rendering,Stimulated saccades,User studies."
"A New Terrain in HCI: Emotion Recognition Interface using Biometric Data
  for an Immersive VR Experience","Emotion recognition technology is crucial in providing a personalized user
experience. It is especially important in virtual reality(VR) to assess the
user's emotions to enhance their sense of immersion. We propose an emotion
recognition interface that incorporates the user's biometric data with machine
learning technology for increasing user engagement in VR. Our key technologies
include brainwave sensors and eye-tracking cameras embedded in a VR headset,
which seamlessly acquire physiological signals, and secondly, an attractiveness
recognition algorithm that uses bio-signals to predict the user's attraction on
visual stimuli. We conducted experiments to test the performance of the system,
and also interviewed experts and participants to acquire opinions on the
system. This study demonstrated the technical feasibility of our system with
high accuracy and usability. Interviewees expected that the interface will be
actively used in the context of various applications. Our proposed interface
could contribute to an immersive VR experience design.","['Jaehyun Nam', 'Hyesun Chung', 'Young ah Seong', 'Honggu Lee']",2019-12-03T03:23:25Z,http://arxiv.org/abs/1912.01177v1,"['cs.HC', 'eess.SP']","Emotion recognition,Interface,Biometric data,Immersive VR experience,Machine learning,Brainwave sensors,Eye-tracking cameras,Attractiveness recognition algorithm,Bio-signals,User engagement"
"Overcoming the Channel Estimation Barrier in Massive MIMO Communication
  Systems","A new wave of wireless services, including virtual reality, autonomous
driving and internet of things, is driving the design of new generations of
wireless systems to deliver ultra-high data rates, massive number of connected
devices and ultra low latency. Massive multiple-input multiple-output (MIMO) is
one of the critical underlying technologies that allow future wireless networks
to meet these service needs. This article discusses the application of deep
learning (DL) for massive MIMO channel estimation in wireless networks by
integrating the underlying characteristics of channels in future high-speed
cellular deployment. We develop important insights derived from the physical
radio frequency (RF) channel properties and present a comprehensive overview on
the application of DL for accurately estimating channel state information (CSI)
with low overhead. We provide examples of successful DL application in CSI
estimation for massive MIMO wireless systems and highlight several promising
directions for future research.","['Zhenyu Liu', 'Lin Zhang', 'Zhi Ding']",2019-12-23T01:11:33Z,http://arxiv.org/abs/1912.10573v1,"['cs.IT', 'eess.SP', 'math.IT']","massive MIMO,channel estimation,deep learning,wireless networks,channel state information (CSI),radio frequency (RF),high-speed cellular deployment,connected devices,ultra low latency"
SLOAM: Semantic Lidar Odometry and Mapping for Forest Inventory,"This paper describes an end-to-end pipeline for tree diameter estimation
based on semantic segmentation and lidar odometry and mapping. Accurate mapping
of this type of environment is challenging since the ground and the trees are
surrounded by leaves, thorns and vines, and the sensor typically experiences
extreme motion. We propose a semantic feature based pose optimization that
simultaneously refines the tree models while estimating the robot pose. The
pipeline utilizes a custom virtual reality tool for labeling 3D scans that is
used to train a semantic segmentation network. The masked point cloud is used
to compute a trellis graph that identifies individual instances and extracts
relevant features that are used by the SLAM module. We show that traditional
lidar and image based methods fail in the forest environment on both Unmanned
Aerial Vehicle (UAV) and hand-carry systems, while our method is more robust,
scalable, and automatically generates tree diameter estimations.","['Steven W. Chen', 'Guilherme V. Nardari', 'Elijah S. Lee', 'Chao Qu', 'Xu Liu', 'Roseli A. F. Romero', 'Vijay Kumar']",2019-12-29T20:38:32Z,http://arxiv.org/abs/1912.12726v1,"['cs.RO', 'cs.CV', 'cs.LG']","Semantic segmentation,Lidar odometry,Mapping,Tree diameter estimation,Pose optimization,Semantic feature,Point cloud,Trellis graph,SLAM,UAV"
Study of Gesture Recognition methods and augmented reality,"With the growing technology, we humans always need something that stands out
from the other thing. Gestures are most desirable source to Communicate with
the Machines. Human Computer Interaction finds its importance when it comes to
working with the Human gestures to control the computer applications. Usually
we control the applications using mouse, keyboard, laser pointers etc. but,
with the recent advent in the technology it has even left behind their usage by
introducing more efficient techniques to control applications. There are many
Gesture Recognition techniques that have been implemented using image
processing in the past.
  However recognizing the gestures in the noisy background has always been a
difficult task to achieve. In the proposed system, we are going to use one such
technique called Augmentation in Image processing to control Media Player. We
will recognize Gestures using which we are going to control the operations on
Media player. Augmentation usually is one step ahead when it comes to virtual
reality. It has no restrictions on the background. Moreover it also does not
rely on certain things like gloves, color pointers etc. for recognizing the
gesture. This system mainly appeals to those users who always looks out for a
better option that makes their interaction with computer more simpler or
easier.","['Sandeep Vasave', 'Amol Plave']",2014-11-19T07:52:04Z,http://arxiv.org/abs/1411.5137v1,['cs.HC'],"Gesture Recognition,Augmented Reality,Image Processing,Human Computer Interaction,Media Player,Virtual Reality,Recognition Techniques,Noise Background,Control Applications,Augmentation"
Preprint: Bigdata Oriented Multimedia Mobile Health Applications,"This is the preprint version of our paper on JOMS. In this paper, two mHealth
applications are introduced, which can be employed as the terminals of bigdata
based health service to collect information for electronic medical records
(EMRs). The first one is a hybrid system for improving the user experience in
the hyperbaric oxygen chamber by 3D stereoscopic virtual reality glasses and
immersive perception. Several HMDs have been tested and compared. The second
application is a voice interactive serious game as a likely solution for
providing assistive rehabilitation tool for therapists. The recorder of the
voice of patients could be analysed to evaluate the long-time rehabilitation
results and further to predict the rehabilitation process.","['Zhihan Lv', 'Javier Chirivella', 'Pablo Gagliardo']",2016-03-20T17:31:00Z,http://arxiv.org/abs/1603.06243v1,['cs.HC'],"Bigdata,Multimedia,Mobile Health,Applications,mHealth,Electronic Medical Records (EMRs),3D stereoscopic,Virtual reality,Voice interactive,Serious game"
A Survey of Visual Analysis of Human Motion and Its Applications,"This paper summarizes the recent progress in human motion analysis and its
applications. In the beginning, we reviewed the motion capture systems and the
representation model of human's motion data. Next, we sketched the advanced
human motion data processing technologies, including motion data filtering,
temporal alignment, and segmentation. The following parts overview the
state-of-the-art approaches of action recognition and dynamics measuring since
these two are the most active research areas in human motion analysis. The last
part discusses some emerging applications of the human motion analysis in
healthcare, human robot interaction, security surveillance, virtual reality and
animation. The promising research topics of human motion analysis in the future
is also summarized in the last part.",['Qifei Wang'],2016-08-02T05:50:11Z,http://arxiv.org/abs/1608.00700v2,"['cs.CV', 'cs.AI']","Visual analysis,Human motion,Motion capture systems,Representation model,Motion data processing,Action recognition,Dynamics measuring,Healthcare,Human robot interaction,Virtual reality"
On testing the simulation theory,"Can the theory that reality is a simulation be tested? We investigate this
question based on the assumption that if the system performing the simulation
is finite (i.e. has limited resources), then to achieve low computational
complexity, such a system would, as in a video game, render content (reality)
only at the moment that information becomes available for observation by a
player and not at the moment of detection by a machine (that would be part of
the simulation and whose detection would also be part of the internal
computation performed by the Virtual Reality server before rendering content to
the player). Guided by this principle we describe conceptual wave/particle
duality experiments aimed at testing the simulation theory.","['Tom Campbell', 'Houman Owhadi', 'Joe Sauvageau', 'David Watkinson']",2017-02-28T21:17:29Z,http://arxiv.org/abs/1703.00058v2,['quant-ph'],"simulation theory,reality,computational complexity,video game,rendering,Virtual Reality,wave/particle duality,experiments,testing"
"Detailed, accurate, human shape estimation from clothed 3D scan
  sequences","We address the problem of estimating human pose and body shape from 3D scans
over time. Reliable estimation of 3D body shape is necessary for many
applications including virtual try-on, health monitoring, and avatar creation
for virtual reality. Scanning bodies in minimal clothing, however, presents a
practical barrier to these applications. We address this problem by estimating
body shape under clothing from a sequence of 3D scans. Previous methods that
have exploited body models produce smooth shapes lacking personalized details.
We contribute a new approach to recover a personalized shape of the person. The
estimated shape deviates from a parametric model to fit the 3D scans. We
demonstrate the method using high quality 4D data as well as sequences of
visual hulls extracted from multi-view images. We also make available BUFF, a
new 4D dataset that enables quantitative evaluation
(http://buff.is.tue.mpg.de). Our method outperforms the state of the art in
both pose estimation and shape estimation, qualitatively and quantitatively.","['Chao Zhang', 'Sergi Pujades', 'Michael Black', 'Gerard Pons-Moll']",2017-03-13T15:41:36Z,http://arxiv.org/abs/1703.04454v2,['cs.CV'],"human pose estimation,body shape estimation,3D scans,virtual try-on,avatar creation,virtual reality,parametric model,visual hulls,pose estimation,shape estimation"
"Google Cardboard Dates Augmented Reality : Issues, Challenges and Future
  Opportunities","The Google's frugal Cardboard solution for immersive Virtual Reality
experiences has come a long way in the VR market. The Google Cardboard VR
applications will support us in the fields such as education, virtual tourism,
entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has
introduced support for developing mixed reality applications for Google
Cardboard which can combine Virtual and Augmented Reality to develop exciting
and immersive experiences. In this work, we present a comprehensive review of
Google Cardboard for AR and also highlight its technical and subjective
limitations by conducting a feasibility study through the inspection of a
Desktop computer use-case. Additionally, we recommend the future avenues for
the Google Cardboard in AR. This work also serves as a guide for Android/iOS
developers as there are no published scholarly articles or well documented
studies exclusively on Google Cardboard with both user and developer's
experience captured at one place.","['Ramakrishna Perla', 'Ramya Hebbalaguppe']",2017-06-05T06:26:25Z,http://arxiv.org/abs/1706.03851v1,['cs.HC'],"Google Cardboard,Augmented Reality,Virtual Reality,Qualcomm Vuforia SDK,mixed reality,immersive experiences,feasibility study,Android/iOS developers"
"""Synchronize"" to VR Body: Full Body Illusion in VR Space","Virtual Reality (VR) becomes accessible to mimic a ""real-like"" world now.
People who have a VR experience usually can be impressed by the immersive
feeling, they might consider themselves are actually existed in the VR space.
Self-consciousness is important for people to identify their own characters in
VR space, and illusory ownership can help people to ""build"" their ""bodies"". The
rubber hand illusion can convince us a fake hand made by rubber is a part of
our bodies under certain circumstances. Researches about autoscopic phenomena
extend this illusory to the so-called full body illusion. We conducted 3 type
of experiments to study the illusory ownership in VR space as it shows in
Figure 1, and we learned: Human body must receive the synchronized visual
signal and somatosensory stimulus at the same time; The visual signal must be
the first person perceptive; the subject and the virtual body needs to be the
same height as much as possible. All these illusory ownerships accompanied by
the body temperature decreases, where the body is stimulated.","['Peikun Xiong', 'Chen Sun', 'Dongsheng Cai']",2017-06-20T07:56:43Z,http://arxiv.org/abs/1706.06579v1,"['cs.HC', 'H.5.1']","Virtual Reality,Full Body Illusion,Illusory Ownership,Autoscopic Phenomena,Synchronized Visual Signal,Somatosensory Stimulus,First Person Perceptive,Virtual Body,Body Temperature"
"Providing Effective Real-time Feedback in Simulation-based Surgical
  Training","Virtual reality simulation is becoming popular as a training platform in
surgical education. However, one important aspect of simulation-based surgical
training that has not received much attention is the provision of automated
real-time performance feedback to support the learning process. Performance
feedback is actionable advice that improves novice behaviour. In simulation,
automated feedback is typically extracted from prediction models trained using
data mining techniques. Existing techniques suffer from either low
effectiveness or low efficiency resulting in their inability to be used in
real-time. In this paper, we propose a random forest based method that finds a
balance between effectiveness and efficiency. Experimental results in a
temporal bone surgery simulation show that the proposed method is able to
extract highly effective feedback at a high level of efficiency.","['Xingjun Ma', 'Sudanthi Wijewickrema', 'Yun Zhou', 'Shuo Zhou', ""Stephen O'Leary"", 'James Bailey']",2017-06-30T06:36:14Z,http://arxiv.org/abs/1706.10036v1,"['cs.AI', 'cs.LG']","virtual reality simulation,surgical training,performance feedback,automated feedback,prediction models,data mining techniques,random forest,temporal bone surgery,simulation-based training,real-time feedback"
Large-scale Multiview 3D Hand Pose Dataset,"Accurate hand pose estimation at joint level has several uses on human-robot
interaction, user interfacing and virtual reality applications. Yet, it
currently is not a solved problem. The novel deep learning techniques could
make a great improvement on this matter but they need a huge amount of
annotated data. The hand pose datasets released so far present some issues that
make them impossible to use on deep learning methods such as the few number of
samples, high-level abstraction annotations or samples consisting in depth
maps. In this work, we introduce a multiview hand pose dataset in which we
provide color images of hands and different kind of annotations for each, i.e
the bounding box and the 2D and 3D location on the joints in the hand. Besides,
we introduce a simple yet accurate deep learning architecture for real-time
robust 2D hand pose estimation.","['Francisco Gomez-Donoso', 'Sergio Orts-Escolano', 'Miguel Cazorla']",2017-07-12T14:39:49Z,http://arxiv.org/abs/1707.03742v3,"['cs.HC', 'cs.CV']","large-scale,multiview,3D hand pose,dataset,deep learning,annotated data,color images,bounding box,joints,2D hand pose"
"360-degree videos: a new visualization technique for astrophysical
  simulations","360-degree videos are a new type of movie that renders over all 4$\pi$
steradian. Video sharing sites such as YouTube now allow this unique content to
be shared via virtual reality (VR) goggles, hand-held smartphones/tablets, and
computers. Creating 360$^\circ$ videos from astrophysical simulations is not
only a new way to view these simulations as you are immersed in them, but is
also a way to create engaging content for outreach to the public. We present
what we believe is the first 360$^\circ$ video of an astrophysical simulation:
a hydrodynamics calculation of the central parsec of the Galactic centre. We
also describe how to create such movies, and briefly comment on what new
science can be extracted from astrophysical simulations using 360$^\circ$
videos.",['Christopher M. P. Russell'],2017-07-21T16:04:42Z,http://arxiv.org/abs/1707.06954v1,['astro-ph.IM'],"360-degree videos,visualization technique,astrophysical simulations,movie rendering,4π steradian,virtual reality,VR goggles,smartphones,tablets,outreach."
Graph-Based Classification of Omnidirectional Images,"Omnidirectional cameras are widely used in such areas as robotics and virtual
reality as they provide a wide field of view. Their images are often processed
with classical methods, which might unfortunately lead to non-optimal solutions
as these methods are designed for planar images that have different geometrical
properties than omnidirectional ones. In this paper we study image
classification task by taking into account the specific geometry of
omnidirectional cameras with graph-based representations. In particular, we
extend deep learning architectures to data on graphs; we propose a principled
way of graph construction such that convolutional filters respond similarly for
the same pattern on different positions of the image regardless of lens
distortions. Our experiments show that the proposed method outperforms current
techniques for the omnidirectional image classification problem.","['Renata Khasanova', 'Pascal Frossard']",2017-07-26T06:39:45Z,http://arxiv.org/abs/1707.08301v1,"['cs.CV', 'cs.LG']","omnidirectional images,graph-based classification,deep learning architectures,image classification,omnidirectional cameras,convolutional filters,graph construction,lens distortions,virtual reality,robotics"
Blind Demixing for Low-Latency Communication,"In the next generation wireless networks, lowlatency communication is
critical to support emerging diversified applications, e.g., Tactile Internet
and Virtual Reality. In this paper, a novel blind demixing approach is
developed to reduce the channel signaling overhead, thereby supporting
low-latency communication. Specifically, we develop a low-rank approach to
recover the original information only based on a single observed vector without
any channel estimation. Unfortunately, this problem turns out to be a highly
intractable non-convex optimization problem due to the multiple non-convex
rankone constraints. To address the unique challenges, the quotient manifold
geometry of product of complex asymmetric rankone matrices is exploited by
equivalently reformulating original complex asymmetric matrices to the
Hermitian positive semidefinite matrices. We further generalize the geometric
concepts of the complex product manifolds via element-wise extension of the
geometric concepts of the individual manifolds. A scalable Riemannian
trust-region algorithm is then developed to solve the blind demixing problem
efficiently with fast convergence rates and low iteration cost. Numerical
results will demonstrate the algorithmic advantages and admirable performance
of the proposed algorithm compared with the state-of-art methods.","['Jialin Dong', 'Kai Yang', 'Yuanming Shi']",2018-01-07T08:28:05Z,http://arxiv.org/abs/1801.02158v2,"['cs.IT', 'math.IT']","low-latency communication,blind demixing,channel signaling overhead,non-convex optimization,rank-one constraints,quotient manifold geometry,Hermitian positive semidefinite matrices,Riemannian trust-region algorithm,convergence rates,iteration cost"
"Edge Computing Meets Millimeter-wave Enabled VR: Paving the Way to
  Cutting the Cord","In this paper, a novel proactive computing and mmWave communication for
ultra-reliable and low latency wireless virtual reality (VR is proposed. By
leveraging information about users' poses, proactive computing and caching are
used to pre-compute and store users' HD video frames to minimize the computing
latency. Furthermore, multi-connectivity is exploited to ensure reliable mmWave
links to deliver users' requested HD frames. The performance of the proposed
approach is validated on a VR network serving an interactive gaming arcade,
where dynamic and real-time rendering of HD video frames is needed and impulse
actions of different players impact the content to be shown. Simulation results
show significant gains of up to $30\%$ reduction in end-to-end delay and $50\%$
in the $90^{\textrm{th}}$ percentile communication delay.","['Mohammed S. Elbamby', 'Cristina Perfecto', 'Mehdi Bennis', 'Klaus Doppler']",2018-01-23T15:20:43Z,http://arxiv.org/abs/1801.07614v3,"['cs.IT', 'cs.NI', 'math.IT']","Edge computing,Millimeter-wave communication,Virtual reality,Proactive computing,Low latency,Multi-connectivity,HD video frames,End-to-end delay,Communication delay,Interactive gaming."
"Interactive Generative Adversarial Networks for Facial Expression
  Generation in Dyadic Interactions","A social interaction is a social exchange between two or more
individuals,where individuals modify and adjust their behaviors in response to
their interaction partners. Our social interactions are one of most fundamental
aspects of our lives and can profoundly affect our mood, both positively and
negatively. With growing interest in virtual reality and avatar-mediated
interactions,it is desirable to make these interactions natural and human like
to promote positive effect in the interactions and applications such as
intelligent tutoring systems, automated interview systems and e-learning. In
this paper, we propose a method to generate facial behaviors for an agent.
These behaviors include facial expressions and head pose and they are generated
considering the users affective state. Our models learn semantically meaningful
representations of the face and generate appropriate and temporally smooth
facial behaviors in dyadic interactions.","['Behnaz Nojavanasghari', 'Yuchi Huang', 'Saad Khan']",2018-01-27T14:01:17Z,http://arxiv.org/abs/1801.09092v2,['cs.CV'],"Generative Adversarial Networks,Facial Expression Generation,Dyadic Interactions,Virtual Reality,Avatar-Mediated Interactions,Affective State,Facial Behaviors,Intelligent Tutoring Systems"
"AAS WorldWide Telescope: Seamless, Cross-Platform Data Visualization
  Engine for Astronomy Research, Education, and Democratizing Data","The American Astronomical Society's WorldWide Telescope (WWT) project enables
terabytes of astronomical images, data, and stories to be viewed and shared
among researchers, exhibited in science museums, projected into full-dome
immersive planetariums and virtual reality headsets, and taught in classrooms
from middle school to college levels. We review the WWT ecosystem, how WWT has
been used in the astronomical community, and comment on future directions.","['Philip Rosenfield', 'Jonathan Fay', 'Ronald K Gilchrist', 'Chenzhou Cui', 'A. David Weigel', 'Thomas Robitaille', 'Oderah Justin Otor', 'Alyssa Goodman']",2018-01-27T18:29:15Z,http://arxiv.org/abs/1801.09119v1,['astro-ph.IM'],"WorldWide Telescope,Data Visualization,Cross-Platform,Astronomy Research,Education,Democratizing Data,Terabytes,Immersive Planetariums,Virtual Reality,WWT Ecosystem"
"Switchable Virtual, Augmented, and Mixed Reality through Optical
  Cloaking","A switchable virtual reality (VR), augmented reality (AR), and mixed reality
(MR) system is proposed using digital optical cloaking. Optical cloaking allows
completely opaque VR devices to be ""cloaked,"" switching to AR or MR while
providing correct three-dimensional (3D) parallax and perspective of the real
world, without the need for transparent optics. On the other hand, 3D capture
and display devices with non-zero thicknesses, require optical cloaking to
properly display captured reality. A simplified stereoscopic system with two
cameras and existing VR systems can be an approximation for limited VR, AR, or
MR. To provide true 3D visual effects, multiple input cameras, a 3D display,
and a simple linear calculation amounting to cloaking can be used. Since the
display size requirements for VR, AR, and MR are usually small, with increasing
computing power and pixel densities, the framework presented here can provide a
widely deployable VR, AR, MR design.",['Joseph S. Choi'],2018-02-06T07:18:51Z,http://arxiv.org/abs/1802.01826v1,['physics.optics'],"virtual reality,augmented reality,mixed reality,optical cloaking,3D parallax,perspective,stereoscopic system,3D display,computing power"
"Minimizing Latency to Support VR Social Interactions over Wireless
  Cellular Systems via Bandwidth Allocation","Immersive social interactions of mobile users are soon to be enabled within a
virtual space, by means of virtual reality (VR) technologies and wireless
cellular systems. In a VR mobile social network, the states of all interacting
users should be updated synchronously and with low latency via two-way
communications with edge computing servers. The resulting end-to-end latency
depends on the relationship between the virtual and physical locations of the
wireless VR users and of the edge servers. In this work, the problem of
analyzing and optimizing the end-to-end latency is investigated for a simple
network topology, yielding important insights into the interplay between
physical and virtual geometries.","['Jihong Park', 'Petar Popovski', 'Osvaldo Simeone']",2018-02-09T21:26:06Z,http://arxiv.org/abs/1802.03450v2,"['cs.IT', 'cs.NI', 'math.IT']","virtual reality,social interactions,wireless cellular systems,bandwidth allocation,latency,edge computing servers,network topology,physical geometries,virtual geometries"
Novel View Synthesis for Large-scale Scene using Adversarial Loss,"Novel view synthesis aims to synthesize new images from different viewpoints
of given images. Most of previous works focus on generating novel views of
certain objects with a fixed background. However, for some applications, such
as virtual reality or robotic manipulations, large changes in background may
occur due to the egomotion of the camera. Generated images of a large-scale
environment from novel views may be distorted if the structure of the
environment is not considered. In this work, we propose a novel fully
convolutional network, that can take advantage of the structural information
explicitly by incorporating the inverse depth features. The inverse depth
features are obtained from CNNs trained with sparse labeled depth values. This
framework can easily fuse multiple images from different viewpoints. To fill
the missing textures in the generated image, adversarial loss is applied, which
can also improve the overall image quality. Our method is evaluated on the
KITTI dataset. The results show that our method can generate novel views of
large-scale scene without distortion. The effectiveness of our approach is
demonstrated through qualitative and quantitative evaluation.","['Xiaochuan Yin', 'Henglai Wei', 'Penghong lin', 'Xiangwei Wang', 'Qijun Chen']",2018-02-20T11:21:11Z,http://arxiv.org/abs/1802.07064v1,['cs.CV'],"Novel view synthesis,Adversarial loss,Large-scale scene,Fully convolutional network,Inverse depth features,CNNs,KITTI dataset,Virtual reality,Robotic manipulations"
Equalizer 2.0 - Convergence of a Parallel Rendering Framework,"Developing complex, real world graphics applications which leverage multiple
GPUs and computers for interactive 3D rendering tasks is a complex task. It
requires expertise in distributed systems and parallel rendering in addition to
the application domain itself. We present a mature parallel rendering framework
which provides a large set of features, algorithms and system integration for a
wide range of real-world research and industry applications. Using the
Equalizer parallel rendering framework, we show how a wide set of generic
algorithms can be integrated in the framework to help application scalability
and development in many different domains, highlighting how concrete
applications benefit from the diverse aspects and use cases of Equalizer. We
present novel parallel rendering algorithms, powerful abstractions for large
visualization setups and virtual reality, as well as new experimental results
for parallel rendering and data distribution.","['Stefan Eilemann', 'David Steiner', 'Renato Pajarola']",2018-02-22T13:04:42Z,http://arxiv.org/abs/1802.08022v1,"['cs.GR', 'I.3.2.a; I.3.7.g; I.3.8; I.3.6.a; I.6.9.f']","parallel rendering,framework,GPUs,distributed systems,algorithms,system integration,scalability,virtual reality,data distribution"
Web-Based VR Experiments Powered by the Crowd,"We build on the increasing availability of Virtual Reality (VR) devices and
Web technologies to conduct behavioral experiments in VR using crowdsourcing
techniques. A new recruiting and validation method allows us to create a panel
of eligible experiment participants recruited from Amazon Mechanical Turk.
Using this panel, we ran three different crowdsourced VR experiments, each
reproducing one of three VR illusions: place illusion, embodiment illusion, and
plausibility illusion. Our experience and worker feedback on these experiments
show that conducting Web-based VR experiments using crowdsourcing is already
feasible, though some challenges---including scale---remain. Such crowdsourced
VR experiments on the Web have the potential to finally support replicable VR
experiments with diverse populations at a low cost.","['Xiao Ma', 'Megan Cackett', 'Leslie Park', 'Eric Chien', 'Mor Naaman']",2018-02-22T23:13:54Z,http://arxiv.org/abs/1802.08345v2,['cs.HC'],"Virtual Reality,Web technologies,behavioral experiments,crowdsourcing,recruiting,validation method,Amazon Mechanical Turk,VR illusions,replicable experiments"
DeepIM: Deep Iterative Matching for 6D Pose Estimation,"Estimating the 6D pose of objects from images is an important problem in
various applications such as robot manipulation and virtual reality. While
direct regression of images to object poses has limited accuracy, matching
rendered images of an object against the observed image can produce accurate
results. In this work, we propose a novel deep neural network for 6D pose
matching named DeepIM. Given an initial pose estimation, our network is able to
iteratively refine the pose by matching the rendered image against the observed
image. The network is trained to predict a relative pose transformation using
an untangled representation of 3D location and 3D orientation and an iterative
training process. Experiments on two commonly used benchmarks for 6D pose
estimation demonstrate that DeepIM achieves large improvements over
state-of-the-art methods. We furthermore show that DeepIM is able to match
previously unseen objects.","['Yi Li', 'Gu Wang', 'Xiangyang Ji', 'Yu Xiang', 'Dieter Fox']",2018-03-31T14:02:25Z,http://arxiv.org/abs/1804.00175v4,"['cs.CV', 'cs.RO']","DeepIM,6D pose estimation,deep neural network,pose matching,relative pose transformation,untangled representation,iterative training,benchmarks,state-of-the-art methods"
A Review of Augmented Reality Applications for Building Evacuation,"Evacuation is one of the main disaster management solutions to reduce the
impact of man-made and natural threats on building occupants. To date, several
modern technologies and gamification concepts, e.g. immersive virtual reality
and serious games, have been used to enhance building evacuation preparedness
and effectiveness. Those tools have been used both to investigate human
behavior during building emergencies and to train building occupants on how to
cope with building evacuations.
  Augmented Reality (AR) is novel technology that can enhance this process
providing building occupants with virtual contents to improve their evacuation
performance. This work aims at reviewing existing AR applications developed for
building evacuation. This review identifies the disasters and types of building
those tools have been applied for. Moreover, the application goals, hardware
and evacuation stages affected by AR are also investigated in the review.
Finally, this review aims at identifying the challenges to face for further
development of AR evacuation tools.",['Lovreglio Ruggiero'],2018-04-10T07:50:14Z,http://arxiv.org/abs/1804.04186v1,"['cs.OH', 'cs.CY']","augmented reality,building evacuation,disaster management,immersive virtual reality,serious games,human behavior,training,virtual contents,hardware,evacuation stages"
High Dimensional Time Series Generators,"Multidimensional time series are sequences of real valued vectors. They occur
in different areas, for example handwritten characters, GPS tracking, and
gestures of modern virtual reality motion controllers. Within these areas, a
common task is to search for similar time series. Dynamic Time Warping (DTW) is
a common distance function to compare two time series. The Edit Distance with
Real Penalty (ERP) and the Dog Keeper Distance (DK) are two more distance
functions on time series. Their behaviour has been analyzed on 1-dimensional
time series. However, it is not easy to evaluate their behaviour in relation to
growing dimensionality. For this reason we propose two new data synthesizers
generating multidimensional time series. The first synthesizer extends the well
known cylinder-bell-funnel (CBF) dataset to multidimensional time series. Here,
each time series has an arbitrary type (cylinder, bell, or funnel) in each
dimension, thus for $d$-dimensional time series there are $3^{d}$ different
classes. The second synthesizer (RAM) creates time series with ideas adapted
from Brownian motions which is a common model of movement in physics. Finally,
we evaluate the applicability of a 1-nearest neighbor classifier using DTW on
datasets generated by our synthesizers.","['Jörg P. Bachmann', 'Johann-Christoph Freytag']",2018-04-17T16:24:14Z,http://arxiv.org/abs/1804.06352v3,"['cs.LG', 'stat.ML']","high-dimensional time series,generators,multidimensional time series,dynamic time warping,edit distance with real penalty,dog keeper distance,data synthesizers,cylinder-bell-funnel,RAM,1-nearest neighbor classifier"
"Open Tactile - An open, modular hardware system for controlling tactile
  displays","Tactile displays have a wide potential field of applications, ranging from
enhancing Virtual-Reality scenarios up to aiding telesurgery as well as in
fundamental psychological and neurophysiological research. In this paper, we
describe an open source hardware and software architecture that is designed to
drive a variety of different tactile displays. For demonstration purposes, a
tactile computer mouse featuring a simple tactile display, based on lateral
piezoelectric (PZT) actuators, is presented. Even though we will focus on
driving mechanical actuators in this paper, the system can be extended to
different working principles. The suggested architecture is supplied with a
custom, easy to use, software stack allowing a simple definition of tactile
scenarios as well as user studies while being especially tailored to
non-computer scientists. By releasing the OpenTactile system under MIT license
we hope to ease the burden of controlling tactile displays as well as designing
and reproducing the related experiments.","['Andreas Tarnowsky', 'Jan Jamaszyk', 'Daniel Brandes', 'Franz-Erich Wolter']",2018-04-24T08:33:59Z,http://arxiv.org/abs/1804.08895v1,['cs.HC'],"Tactile displays,Hardware system,Modular,Control,Open source,Software architecture,Piezoelectric actuators,User studies,MIT license"
"Communication, Computing and Caching for Mobile VR Delivery: Modeling
  and Trade-off","Mobile virtual reality (VR) delivery is gaining increasing attention from
both industry and academia due to its ability to provide an immersive
experience. However, achieving mobile VR delivery requires ultra-high
transmission rate, deemed as a first killer application for 5G wireless
networks. In this paper, in order to alleviate the traffic burden over wireless
networks, we develop an implementation framework for mobile VR delivery by
utilizing caching and computing capabilities of mobile VR device. We then
jointly optimize the caching and computation offloading policy for minimizing
the required average transmission rate under the latency and local average
energy consumption constraints. In a symmetric scenario, we obtain the optimal
joint policy and the closed-form expression of the minimum average transmission
rate. Accordingly, we analyze the tradeoff among communication, computing and
caching, and then reveal analytically the fact that the communication overhead
can be traded by the computing and caching capabilities of mobile VR device,
and also what conditions must be met for it to happen. Finally, we discuss the
optimization problem in a heterogeneous scenario, and propose an efficient
suboptimal algorithm with low computation complexity, which is shown to achieve
good performance in the numerical results.","['Yaping Sun', 'Zhiyong Chen', 'Meixia Tao', 'Hui Liu']",2018-04-27T04:13:55Z,http://arxiv.org/abs/1804.10335v1,"['cs.IT', 'math.IT']","Communication,Computing,Caching,Mobile VR Delivery,Modeling,Trade-off,Transmission Rate,5G Wireless Networks,Computation Offloading,Optimization"
"URLLC-eMBB Slicing to Support VR Multimodal Perceptions over Wireless
  Cellular Systems","Virtual reality (VR) enables mobile wireless users to experience multimodal
perceptions in a virtual space. In this paper we investigate the problem of
concurrent support of visual and haptic perceptions over wireless cellular
networks, with a focus on the downlink transmission phase. While the visual
perception requires moderate reliability and maximized rate, the haptic
perception requires fixed rate and high reliability. Hence, the visuo-haptic VR
traffic necessitates the use of two different network slices: enhanced mobile
broadband (eMBB) for visual perception and ultra-reliable and low latency
communication (URLLC) for haptic perception. We investigate two methods by
which these two slices share the downlink resources orthogonally and
non-orthogonally, respectively. We compare these methods in terms of the
just-noticeable difference (JND), an established measure in psychophysics, and
show that non-orthogonal slicing becomes preferable under a higher target
integrated-perceptual resolution and/or a higher target rate for haptic
perceptions.","['Jihong Park', 'Mehdi Bennis']",2018-05-01T00:40:19Z,http://arxiv.org/abs/1805.00142v2,"['cs.IT', 'cs.NI', 'eess.SP', 'math.IT']","URLLC,eMBB,slicing,VR,multimodal perceptions,wireless cellular systems,haptic perception,visual perception,network slices,orthogonally"
Virtual Reality Wireless Mobile Walkthrough Framework,"The last years have witnessed a dramatic growth in the number as well as the
variety of graphics intensive mobile applications, which allow users to
interact and navigate through large scenes such as ancient places, museums and
even virtual cities. These applications support many clients and impose a heavy
requirement on network resources and computational resources. One key issue in
the design of cost efficient mobile walkthrough applications is the data
transmission between servers and mobile client devices. In this paper, we
propose an effective progressive mesh transmission framework that stores and
divide scene objects into different resolutions. This way, each mobile
progressively receives and process the only object's details that match with
its display resolution, and hence improving the user's perception and overall
system's response time. A fine grained cache mechanism is used to keep the most
frequently requested objects' details in the device memory and consequently
reduce the network traffics. Lastly, practical experiments are carried out to
illustrate the effectiveness of the proposed framework under various setting of
the virtual scene and mobile devices configuration/types.",['Ghada Mohammed Fathy'],2018-04-28T12:26:20Z,http://arxiv.org/abs/1805.01763v1,['cs.NI'],"virtual reality,wireless,mobile,walkthrough,framework,progressive mesh transmission,network resources,computational resources,cache mechanism"
Object Detection in Equirectangular Panorama,"We introduced a high-resolution equirectangular panorama (360-degree, virtual
reality) dataset for object detection and propose a multi-projection variant of
YOLO detector. The main challenge with equirectangular panorama image are i)
the lack of annotated training data, ii) high-resolution imagery and iii)
severe geometric distortions of objects near the panorama projection poles. In
this work, we solve the challenges by i) using training examples available in
the ""conventional datasets"" (ImageNet and COCO), ii) employing only
low-resolution images that require only moderate GPU computing power and
memory, and iii) our multi-projection YOLO handles projection distortions by
making multiple stereographic sub-projections. In our experiments, YOLO
outperforms the other state-of-art detector, Faster RCNN and our
multi-projection YOLO achieves the best accuracy with low-resolution input.","['Wenyan Yang', 'Yanlin Qian', 'Francesco Cricri', 'Lixin Fan', 'Joni-Kristian Kamarainen']",2018-05-21T12:11:38Z,http://arxiv.org/abs/1805.08009v1,['cs.CV'],"Object Detection,Equirectangular Panorama,Dataset,YOLO Detector,Geometric Distortions,Training Data,High-Resolution Imagery,GPU Computing Power,Multi-Projection,Stereographic Sub-Projections"
"Automated Performance Assessment in Transoesophageal Echocardiography
  with Convolutional Neural Networks","Transoesophageal echocardiography (TEE) is a valuable diagnostic and
monitoring imaging modality. Proper image acquisition is essential for
diagnosis, yet current assessment techniques are solely based on manual expert
review. This paper presents a supervised deep learn ing framework for
automatically evaluating and grading the quality of TEE images. To obtain the
necessary dataset, 38 participants of varied experience performed TEE exams
with a high-fidelity virtual reality (VR) platform. Two Convolutional Neural
Network (CNN) architectures, AlexNet and VGG, structured to perform regression,
were finetuned and validated on manually graded images from three evaluators.
Two different scoring strategies, a criteria-based percentage and an overall
general impression, were used. The developed CNN models estimate the average
score with a root mean square accuracy ranging between 84%-93%, indicating the
ability to replicate expert valuation. Proposed strategies for automated TEE
assessment can have a significant impact on the training process of new TEE
operators, providing direct feedback and facilitating the development of the
necessary dexterous skills.","['Evangelos B. Mazomenos', 'Kamakshi Bansal', 'Bruce Martin', 'Andrew Smith', 'Susan Wright', 'Danail Stoyanov']",2018-06-13T17:29:29Z,http://arxiv.org/abs/1806.05154v1,['cs.CV'],"Transoesophageal echocardiography,Convolutional Neural Networks,deep learning framework,TEE images,dataset,CNN architectures,AlexNet,VGG,regression,supervised learning."
Captain Einstein: a VR experience of relativity,"Captain Einstein is a virtual reality (VR) movie that takes you on a boat
trip in a world with a slow speed of light. This allows for a direct experience
of the theory of special relativity, much in the same spirit as in the Mr.
Tompkins adventure by George Gamow (1939). In this paper we go through the
different relativistic effects (e.g. length contraction, time dilation, Doppler
shift, light aberration) that show up during the boat trip and we explain how
these effects were implemented in the 360 video production process. We also
provide exercise questions that can be used - in combination with the VR movie
- to gain insight and sharpen the intuition on the basic concepts of special
relativity.","['Karel Van Acoleyen', 'Jos Van Doorsselaere']",2018-06-28T16:58:11Z,http://arxiv.org/abs/1806.11085v1,"['physics.pop-ph', 'physics.ed-ph']","virtual reality,special relativity,relativity,VR experience,theory of special relativity,relativistic effects,length contraction,time dilation,Doppler shift,360 video production"
"Heter-Sim: Heterogeneous multi-agent systems simulation by interactive
  data-driven optimization","Interactive multi-agent simulation algorithms are used to compute the
trajectories and behaviors of different entities in virtual reality scenarios.
However, current methods involve considerable parameter tweaking to generate
plausible behaviors. We introduce a novel approach (Heter-Sim) that combines
physics-based simulation methods with data-driven techniques using an
optimization-based formulation. Our approach is general and can simulate
heterogeneous agents corresponding to human crowds, traffic, vehicles, or
combinations of different agents with varying dynamics. We estimate motion
states from real-world datasets that include information about position,
velocity, and control direction. Our optimization algorithm considers several
constraints, including velocity continuity, collision avoidance, attraction,
and direction control. To accelerate the computations, we reduce the search
space for both collision avoidance and optimal solution computation. Heter-Sim
can simulate tens or hundreds of agents at interactive rates and we compare its
accuracy with real-world datasets and prior algorithms. We also perform user
studies that evaluate the plausible behaviors generated by our algorithm and a
user study that evaluates the plausibility of our algorithm via VR.","['Jiaping Ren', 'Wei Xiang', 'Yangxi Xiao', 'Ruigang Yang', 'Dinesh Manocha', 'Xiaogang Jin']",2018-12-02T02:36:00Z,http://arxiv.org/abs/1812.00307v1,['cs.GR'],"multi-agent simulation,data-driven optimization,physics-based simulation,heterogeneous agents,real-world datasets,collision avoidance,motion states,interactive rates,user studies,virtual reality"
PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image,"This paper proposes a deep neural architecture, PlaneRCNN, that detects and
reconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN
employs a variant of Mask R-CNN to detect planes with their plane parameters
and segmentation masks. PlaneRCNN then jointly refines all the segmentation
masks with a novel loss enforcing the consistency with a nearby view during
training. The paper also presents a new benchmark with more fine-grained plane
segmentations in the ground-truth, in which, PlaneRCNN outperforms existing
state-of-the-art methods with significant margins in the plane detection,
segmentation, and reconstruction metrics. PlaneRCNN makes an important step
towards robust plane extraction, which would have an immediate impact on a wide
range of applications including Robotics, Augmented Reality, and Virtual
Reality.","['Chen Liu', 'Kihwan Kim', 'Jinwei Gu', 'Yasutaka Furukawa', 'Jan Kautz']",2018-12-10T20:35:55Z,http://arxiv.org/abs/1812.04072v2,['cs.CV'],"deep neural architecture,PlaneRCNN,3D plane detection,reconstruction,single image,Mask R-CNN,segmentation masks,plane parameters,benchmark,fine-grained plane segmentations"
"An Experimental Information Gathering and Utilization Systems (IGUS)
  Robot to Demonstrate the Physics of Now","The past, present and future are not fundamental properties of Minkowski
spacetime. It has been suggested that they are properties of a class of
information gathering and utilizing systems (IGUSs).The past, present and
future are psychologically created phenomena not actually properties of
spacetime. A human is a model IGUS robot. We develop a way to establish that
the past, present, and future do not follow from the laws of physics by
constructing robots that process information differently and therefore
experience different nows (presents). We construct a customized virtual reality
(VR) system which allows an observer to switch between present and past. This
robot (human with VR system) can experience immersion in the immediate past ad
libitum. Being able to actually construct an IGUS that has the same present at
two different coordinates along the worldline lends support to the IGUS
hypothesis.","['Ronald P. Gruber', 'Ryan P. Smith']",2018-12-10T17:20:55Z,http://arxiv.org/abs/1812.06147v1,['cs.HC'],"Information Gathering,Utilization Systems,IGUS,Physics,Robot,Minkowski Spacetime,Virtual Reality,Observer,Worldline"
"Continuous Trajectory Planning Based on Learning Optimization in High
  Dimensional Input Space for Serial Manipulators","To continuously generate trajectories for serial manipulators with high
dimensional degrees of freedom (DOF) in the dynamic environment, a real-time
optimal trajectory generation method based on machine learning aiming at high
dimensional inputs is presented in this paper. First, a learning optimization
(LO) framework is established, and implementations with different sub-methods
are discussed. Additionally, multiple criteria are defined to evaluate the
performance of LO models. Furthermore, aiming at high dimensional inputs, a
database generation method based on input space dimension-reducing mapping is
proposed. At last, this method is validated on motion planning for haptic
feedback manipulators (HFM) in virtual reality systems. Results show that the
input space dimension-reducing method can significantly elevate the efficiency
and quality of database generation and consequently improve the performance of
the LO. Moreover, using this LO method, real-time trajectory generation with
high dimensional inputs can be achieved, which lays a foundation for continuous
trajectory planning for high-DOF-robots in complex environments.","['Shiyu Zhang', 'Shuling Dai']",2018-12-18T08:11:00Z,http://arxiv.org/abs/1812.07221v1,"['cs.RO', 'cs.LG']","trajectory planning,learning optimization,high dimensional input space,serial manipulators"
Perceptual deep depth super-resolution,"RGBD images, combining high-resolution color and lower-resolution depth from
various types of depth sensors, are increasingly common. One can significantly
improve the resolution of depth maps by taking advantage of color information;
deep learning methods make combining color and depth information particularly
easy. However, fusing these two sources of data may lead to a variety of
artifacts. If depth maps are used to reconstruct 3D shapes, e.g., for virtual
reality applications, the visual quality of upsampled images is particularly
important. The main idea of our approach is to measure the quality of depth map
upsampling using renderings of resulting 3D surfaces. We demonstrate that a
simple visual appearance-based loss, when used with either a trained CNN or
simply a deep prior, yields significantly improved 3D shapes, as measured by a
number of existing perceptual metrics. We compare this approach with a number
of existing optimization and learning-based techniques.","['Oleg Voynov', 'Alexey Artemov', 'Vage Egiazarian', 'Alexander Notchenko', 'Gleb Bobrovskikh', 'Denis Zorin', 'Evgeny Burnaev']",2018-12-24T09:43:25Z,http://arxiv.org/abs/1812.09874v3,"['cs.CV', 'cs.GR', 'cs.LG']","perceptual deep depth super-resolution,RGBD images,depth sensors,deep learning methods,artifacts,3D shapes,virtual reality applications,visual quality,upsampling,CNN"
Gamified Automation in Immersive Media for Education and Research,"The potential of using video games as well as gaming engines for educational
and research purposes is promising, especially with the current progress of
Industry 4.0 technologies such as augmented and virtual reality devices.
However, it is important to be aware of the barriers of these technologies.
Integrating additional software libraries into current developer environments
would not only increase accessibility for the general public, but would gamify
the development process through multiplayer functionality. In this paper we
will briefly discuss a couple case studies demonstrating the usefulness of
using Unreal Engine 4 for collaborative research purposes. In addition, we
present several ideas on how to extend such development environments to further
assist researchers and students to easily create prototypes for the purpose of
gamified scientific creativity and inquiry.","['Janelle Resch', 'Ireneusz', 'Ocelewski', 'Judy Ehrentraut', 'Michael Barnett-Cowan']",2018-12-11T22:26:21Z,http://arxiv.org/abs/1901.00729v1,['physics.ed-ph'],"gamified automation,immersive media,education,research,Industry 4.0 technologies,augmented reality,virtual reality,software libraries,Unreal Engine 4,collaborative research"
"Unsupervised Learning of Depth and Ego-Motion from Cylindrical Panoramic
  Video","We introduce a convolutional neural network model for unsupervised learning
of depth and ego-motion from cylindrical panoramic video. Panoramic depth
estimation is an important technology for applications such as virtual reality,
3D modeling, and autonomous robotic navigation. In contrast to previous
approaches for applying convolutional neural networks to panoramic imagery, we
use the cylindrical panoramic projection which allows for the use of the
traditional CNN layers such as convolutional filters and max pooling without
modification. Our evaluation of synthetic and real data shows that unsupervised
learning of depth and ego-motion on cylindrical panoramic images can produce
high-quality depth maps and that an increased field-of-view improves ego-motion
estimation accuracy. We also introduce Headcam, a novel dataset of panoramic
video collected from a helmet-mounted camera while biking in an urban setting.","['Alisha Sharma', 'Jonathan Ventura']",2019-01-04T04:09:28Z,http://arxiv.org/abs/1901.00979v2,"['cs.CV', 'cs.LG', 'cs.RO']","unsupervised learning,depth,ego-motion,cylindrical panoramic,convolutional neural network,virtual reality,3D modeling,autonomous robotic navigation,convolutional filters,max pooling"
"OMG-VR: Open-source Mudra Gloves for Manipulating Molecular Simulations
  in VR","As VR finds increasing application in scientific research domains like
nanotechnology and biochemistry, we are beginning to better understand the
domains in which it brings the most benefit, as well as the gestures and form
factors that are most useful for specific applications. Here we describe
Open-source Mudra Gloves for Virtual Reality (OMG-VR): etextile gloves designed
to facilitate research scientists and students carrying out detailed and
complex manipulation of simulated 3d molecular objects in VR. The OMG-VR is
designed to sense when a user pinches together their thumb and index finger, or
thumb and middle finger, forming a ""mudra"" position. Tests show that they
provide good positional tracking of the point at which a pinch takes place,
require no calibration, and are sufficiently accurate and robust to enable
scientists to accomplish a range of tasks that involve complex spatial
manipulation of molecules. The open source design offers a promising
alternative to existing controllers and more costly commercial VR data gloves.","['Rachel Freire', 'Becca Rose Glowacki', 'Rhoslyn Roebuck Williams', 'Mark Wonnacott', 'Alexander Jamieson-Binnie', 'David R. Glowacki']",2019-01-11T10:02:27Z,http://arxiv.org/abs/1901.03532v2,"['cs.HC', 'physics.bio-ph', 'physics.chem-ph']","open-source,Mudra Gloves,molecular simulations,VR,etextile gloves,positional tracking,calibration,spatial manipulation,molecules,data gloves"
"Virtual Immersive Reality based Analysis of Behavioral Responses in
  Connected and Autonomous Vehicle Environment","Recently, we developed a dynamic distributed end-to-end vehicle routing
system (E2ECAV) using a network of intelligent intersections and level 5 CAVs
(Djavadian & Farooq, 2018). The case study of the downtown Toronto Network
showed that E2ECAV has the ability to maximize throughput and reduce travel
time up to 40%. However, the efficiency of these new technologies relies on the
acceptance of users in adapting to them and their willingness to give control
fully or partially to CAVs. In this study a stated preference laboratory
experiment is designed employing Virtual Reality Immersive Environment (VIRE)
driving simulator to evaluate the behavioral response of drivers to E2ECAV. The
aim is to investigate under what conditions drivers are more willing to adapt.
The results show that factors such as locus of control, congestion level and
ability to multi-task have significant impact.","['Shadi Djavadian', 'Bilal Farooq', 'Rafael Vasquez', 'Grace Yip']",2019-01-22T02:13:10Z,http://arxiv.org/abs/1901.07151v1,['cs.HC'],"Virtual Immersive Reality,Analysis,Behavioral Responses,Connected and Autonomous Vehicle,End-to-End Vehicle Routing System,Level 5 CAVs,Stated Preference Laboratory Experiment,Virtual Reality Immersive Environment,Behavioral Response"
X-Section: Cross-Section Prediction for Enhanced RGBD Fusion,"Detailed 3D reconstruction is an important challenge with application to
robotics, augmented and virtual reality, which has seen impressive progress
throughout the past years. Advancements were driven by the availability of
depth cameras (RGB-D), as well as increased compute power, e.g.\ in the form of
GPUs -- but also thanks to inclusion of machine learning in the process. Here,
we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep
learning to make object-level predictions about thicknesses that can be readily
integrated into a volumetric multi-view fusion process, where we propose an
extension to the popular KinectFusion approach. In essence, our method allows
to complete shape in general indoor scenes behind what is sensed by the RGB-D
camera, which may be crucial e.g.\ for robotic manipulation tasks or efficient
scene exploration. Predicting object thicknesses rather than volumes allows us
to work with comparably high spatial resolution without exploding memory and
training data requirements on the employed Convolutional Neural Networks. In a
series of qualitative and quantitative evaluations, we demonstrate how we
accurately predict object thickness and reconstruct general 3D scenes
containing multiple objects.","['Andrea Nicastro', 'Ronald Clark', 'Stefan Leutenegger']",2019-03-03T20:58:04Z,http://arxiv.org/abs/1903.00987v3,['cs.CV'],"3D reconstruction,RGB-D fusion,cross-section prediction,deep learning,object-level predictions,volumetric multi-view fusion,KinectFusion,indoor scenes,robotic manipulation,Convolutional Neural Networks"
"Liver Pathology Simulation: Algorithm for Haptic Rendering and Force
  Maps for Palpation Assessment","Preoperative gestures include tactile sampling of the mechanical properties
of biological tissue for both histological and pathological considerations.
Tactile properties used in conjunction with visual cues can provide useful
feedback to the surgeon. Development of novel cost effective haptic-based
simulators and their introduction in the minimally invasive surgery learning
cycle can absorb the learning curve for your residents. Receiving pre-training
in a core set of surgical skills can reduce skill acquisition time and risks.
We present the integration of a real-time surface stiffness adjustment
algorithm and a novel paradigm -- force maps -- in a visuo-haptic simulator
module designed to train internal organs disease diagnostics through palpation.","['Felix G. Hamza-Lup', 'Adrian Seitan', 'Dorin M. Popovici', 'Crenguta M. Bogdan']",2019-03-01T18:16:20Z,http://arxiv.org/abs/1903.01249v1,"['cs.GR', 'cs.HC']","Liver pathology,Simulation,Algorithm,Haptic rendering,Force maps,Palpation assessment,Tactile properties,Minimally invasive surgery,Surgical skills,Internal organs disease diagnostics"
"Learning multimodal representations for sample-efficient recognition of
  human actions","Humans interact in rich and diverse ways with the environment. However, the
representation of such behavior by artificial agents is often limited. In this
work we present \textit{motion concepts}, a novel multimodal representation of
human actions in a household environment. A motion concept encompasses a
probabilistic description of the kinematics of the action along with its
contextual background, namely the location and the objects held during the
performance. Furthermore, we present Online Motion Concept Learning (OMCL), a
new algorithm which learns novel motion concepts from action demonstrations and
recognizes previously learned motion concepts. The algorithm is evaluated on a
virtual-reality household environment with the presence of a human avatar. OMCL
outperforms standard motion recognition algorithms on an one-shot recognition
task, attesting to its potential for sample-efficient recognition of human
actions.","['Miguel Vasco', 'Francisco S. Melo', 'David Martins de Matos', 'Ana Paiva', 'Tetsunari Inamura']",2019-03-06T17:37:21Z,http://arxiv.org/abs/1903.02511v1,"['cs.CV', 'cs.AI', 'cs.LG']","multimodal representations,human actions,motion concepts,kinematics,contextual background,Online Motion Concept Learning,algorithm,recognition,virtual-reality"
"Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative
  Information Analysis","Immersive environments have gradually become standard for visualizing and
analyzing large or complex datasets that would otherwise be cumbersome, if not
impossible, to explore through smaller scale computing devices. However, this
type of workspace often proves to possess limitations in terms of interaction,
flexibility, cost and scalability.
  In this paper we introduce a novel immersive environment called Dataspace,
which features a new combination of heterogeneous technologies and methods of
interaction towards creating a better team workspace. Dataspace provides 15
high-resolution displays that can be dynamically reconfigured in space through
robotic arms, a central table where information can be projected, and a unique
integration with augmented reality (AR) and virtual reality (VR) headsets and
other mobile devices. In particular, we contribute novel interaction
methodologies to couple the physical environment with AR and VR technologies,
enabling visualization of complex types of data and mitigating the scalability
issues of existing immersive environments.
  We demonstrate through four use cases how this environment can be effectively
used across different domains and reconfigured based on user requirements.
  Finally, we compare Dataspace with existing technologies, summarizing the
trade-offs that should be considered when attempting to build better
collaborative workspaces for the future.","['Marco Cavallo', 'Mishal Dholakia', 'Matous Havlena', 'Kenneth Ocheltree', 'Mark Podlaseck']",2019-03-08T23:53:10Z,http://arxiv.org/abs/1903.03700v1,['cs.HC'],"Dataspace,Hybrid Reality Environment,Collaborative Information Analysis,Immersive Environments,Heterogeneous Technologies,Augmented Reality,Virtual Reality,Interaction Methodologies,Scalability Issues,Team Workspace."
"VRKitchen: an Interactive 3D Virtual Environment for Task-oriented
  Learning","One of the main challenges of advancing task-oriented learning such as visual
task planning and reinforcement learning is the lack of realistic and
standardized environments for training and testing AI agents. Previously,
researchers often relied on ad-hoc lab environments. There have been recent
advances in virtual systems built with 3D physics engines and photo-realistic
rendering for indoor and outdoor environments, but the embodied agents in those
systems can only conduct simple interactions with the world (e.g., walking
around, moving objects, etc.). Most of the existing systems also do not allow
human participation in their simulated environments. In this work, we design
and implement a virtual reality (VR) system, VRKitchen, with integrated
functions which i) enable embodied agents powered by modern AI methods (e.g.,
planning, reinforcement learning, etc.) to perform complex tasks involving a
wide range of fine-grained object manipulations in a realistic environment, and
ii) allow human teachers to perform demonstrations to train agents (i.e.,
learning from demonstration). We also provide standardized evaluation
benchmarks and data collection tools to facilitate a broad use in research on
task-oriented learning and beyond.","['Xiaofeng Gao', 'Ran Gong', 'Tianmin Shu', 'Xu Xie', 'Shu Wang', 'Song-Chun Zhu']",2019-03-13T23:31:21Z,http://arxiv.org/abs/1903.05757v1,"['cs.HC', 'cs.AI', 'cs.CV']","Interactive 3D Virtual Environment,Task-oriented Learning,VR system,Embodied agents,AI methods,Reinforcement Learning,Object Manipulations,Learning from Demonstration,Evaluation Benchmarks,Data Collection Tools"
"InGaN/GaN μLED SPICE modelling with size dependent ABC model
  integration","The need of high brightness micro-displays in portable applications dedicated
to mixed and/or virtual reality has drawn an important research wave on
InGaN/GaN based micro-sized light emitting diodes ({\mu}LEDs). We propose to
use a SPICE modelling technique to describe and simulate the electro-optical
behavior of the {\mu}LED. A sub-circuit portrayal of the whole device will be
used to describe current-voltage behavior and the optical power performance of
the device based on the ABC model. We suggest an innovative method to derive
instantaneously the carrier concentration from the simulated electrical current
in order to determine the {\mu}LED quantum efficiency. In a second step, a
statistical approach is also added into the SPICE model in order to apprehend
the spread on experimental data. This {\mu}LED SPICE modelling approach is very
important to allow the design of robust pixel driving circuits.","['Anis Daami', 'François Olivier']",2019-03-18T16:25:08Z,http://arxiv.org/abs/1903.07538v1,['physics.app-ph'],"InGaN,GaN,μLED,SPICE modelling,ABC model,electro-optical behavior,carrier concentration,quantum efficiency,pixel driving circuits"
"From the Hands of an Early Adopter's Avatar to Virtual Junkyards:
  Analysis of Virtual Goods' Lifetime Survival","One of the major questions in the study of economics, logistics, and business
forecasting is the measurement and prediction of value creation, distribution,
and lifetime in the form of goods. In ""real"" economies, a perfect model for the
circulation of goods is impossible. However, virtual realities and economies
pose a new frontier for the broad study of economics, since every good and
transaction can be accurately tracked. Therefore, models that predict goods'
circulation can be tested and confirmed before their introduction to ""real
life"" and other scenarios. The present study is focused on the characteristics
of early-stage adopters for virtual goods, and how they predict the lifespan of
the goods. We employ machine learning and decision trees as the basis of our
prediction models. Results provide evidence that the prediction of the lifespan
of virtual objects is possible based just on data from early holders of those
objects. Overall, communication and social activity are the main drivers for
the effective propagation of virtual goods, and they are the most expected
characteristics of early adopters.","['Kamil Bortko', 'Patryk Pazura', 'Juho Hamari', 'Piotr Bartków', 'Jarosław Jankowski']",2019-03-27T15:56:40Z,http://arxiv.org/abs/1903.11506v1,['cs.SI'],"economics,logistics,business forecasting,virtual realities,virtual economies,goods circulation,prediction models,machine learning,decision trees,social activity"
Haptics-Augmented Physics Simulation: Coriolis Effect,"The teaching of abstract physics concepts can be enhanced by incorporating
visual and haptic sensory modalities in the classroom, using the correct
perspectives. We have developed virtual reality simulations to assist students
in learning the Coriolis effect, an apparent deflection on an object in motion
when observed from within a rotating frame of reference. Twenty four
undergraduate physics students participated in this study. Students were able
to feel the forces through feedback on a Novint Falcon device. The assessment
results show an improvement in the learning experience and better content
retention as compared with traditional instruction methods. We prove that large
scale deployment of visuo-haptic reconfigurable applications is now possible
and feasible in a science laboratory setup.","['Felix G. Hamza-Lup', 'Benjamin Page']",2019-03-07T02:04:56Z,http://arxiv.org/abs/1903.11567v1,"['cs.HC', 'physics.ed-ph']","Haptics,Physics,Simulation,Coriolis Effect,Virtual reality,Sensory modalities,Feedback,Novint Falcon,Content retention,Science laboratory"
"Analysis of distracted pedestrians' waiting time: Head-Mounted Immersive
  Virtual Reality application","This paper analyzes the distracted pedestrians' waiting time before crossing
the road in three conditions: 1) not distracted, 2) distracted with a
smartphone and 3) distracted with a smartphone in the presence of virtual
flashing LED lights on the crosswalk as a safety measure. For the means of data
collection, we adapted an in-house developed virtual immersive reality
environment (VIRE). A total of 42 volunteers participated in the experiment.
Participants' positions and head movements were recorded and used to calculate
walking speeds, acceleration and deceleration rates, surrogate safety measures,
time spent playing smartphone game, etc. After a descriptive analysis on the
data, the effects of these variables on pedestrians' waiting time are analyzed
by employing a cox proportional hazard model. Several factors were identified
as having impact on waiting time. The results show that an increase in initial
walk speed, percentage of time the head was oriented toward smartphone during
crossing, bigger minimum missed gaps and unsafe crossings resulted in shorter
waiting times. On the other hand, an increase in the percentage of time the
head was oriented toward smartphone during waiting time, crossing time and maze
solving time, means longer waiting times for participants.","['Arash Kalatian', 'Anae Sobhani', 'Bilal Farooq']",2019-03-28T07:23:10Z,http://arxiv.org/abs/1903.11812v1,['cs.HC'],"distracted pedestrians,waiting time,head-mounted,immersive,virtual reality,smartphone,flashing LED lights,data collection,walking speeds,safety measures"
"Lightweight Monocular Depth Estimation Model by Joint End-to-End Filter
  pruning","Convolutional neural networks (CNNs) have emerged as the state-of-the-art in
multiple vision tasks including depth estimation. However, memory and computing
power requirements remain as challenges to be tackled in these models.
Monocular depth estimation has significant use in robotics and virtual reality
that requires deployment on low-end devices. Training a small model from
scratch results in a significant drop in accuracy and it does not benefit from
pre-trained large models. Motivated by the literature of model pruning, we
propose a lightweight monocular depth model obtained from a large trained
model. This is achieved by removing the least important features with a novel
joint end-to-end filter pruning. We propose to learn a binary mask for each
filter to decide whether to drop the filter or not. These masks are trained
jointly to exploit relations between filters at different layers as well as
redundancy within the same layer. We show that we can achieve around 5x
compression rate with small drop in accuracy on the KITTI driving dataset. We
also show that masking can improve accuracy over the baseline with fewer
parameters, even without enforcing compression loss.","['Sara Elkerdawy', 'Hong Zhang', 'Nilanjan Ray']",2019-05-13T18:01:01Z,http://arxiv.org/abs/1905.05212v1,['cs.CV'],"Convolutional neural networks,Depth estimation,Model pruning,Lightweight model,End-to-end filter pruning,Robotics,Virtual reality,Training,Compression rate,KITTI dataset"
3D Virtual Garment Modeling from RGB Images,"We present a novel approach that constructs 3D virtual garment models from
photos. Unlike previous methods that require photos of a garment on a human
model or a mannequin, our approach can work with various states of the garment:
on a model, on a mannequin, or on a flat surface. To construct a complete 3D
virtual model, our approach only requires two images as input, one front view
and one back view. We first apply a multi-task learning network called JFNet
that jointly predicts fashion landmarks and parses a garment image into
semantic parts. The predicted landmarks are used for estimating sizing
information of the garment. Then, a template garment mesh is deformed based on
the sizing information to generate the final 3D model. The semantic parts are
utilized for extracting color textures from input images. The results of our
approach can be used in various Virtual Reality and Mixed Reality applications.","['Yi Xu', 'Shanglin Yang', 'Wei Sun', 'Li Tan', 'Kefeng Li', 'Hui Zhou']",2019-07-31T21:47:52Z,http://arxiv.org/abs/1908.00114v1,['cs.CV'],"3D,Virtual,Garment,Modeling,RGB Images,Multi-task Learning Network,Fashion Landmarks,Semantic Parts,Template Garment Mesh,Virtual Reality"
Super-resolution of Omnidirectional Images Using Adversarial Learning,"An omnidirectional image (ODI) enables viewers to look in every direction
from a fixed point through a head-mounted display providing an immersive
experience compared to that of a standard image. Designing immersive virtual
reality systems with ODIs is challenging as they require high resolution
content. In this paper, we study super-resolution for ODIs and propose an
improved generative adversarial network based model which is optimized to
handle the artifacts obtained in the spherical observational space.
Specifically, we propose to use a fast PatchGAN discriminator, as it needs
fewer parameters and improves the super-resolution at a fine scale. We also
explore the generative models with adversarial learning by introducing a
spherical-content specific loss function, called 360-SS. To train and test the
performance of our proposed model we prepare a dataset of 4500 ODIs. Our
results demonstrate the efficacy of the proposed method and identify new
challenges in ODI super-resolution for future investigations.","['Cagri Ozcinar', 'Aakanksha Rana', 'Aljosa Smolic']",2019-08-12T16:05:59Z,http://arxiv.org/abs/1908.04297v1,"['cs.CV', 'cs.LG', 'cs.MM', 'eess.IV']","Omnidirectional image,Super-resolution,Adversarial learning,Generative adversarial network,PatchGAN discriminator,Spherical observational space,Dataset,Virtual reality,Immersive experience"
stdgpu: Efficient STL-like Data Structures on the GPU,"Tremendous advances in parallel computing and graphics hardware opened up
several novel real-time GPU applications in the fields of computer vision,
computer graphics as well as augmented reality (AR) and virtual reality (VR).
Although these applications built upon established open-source frameworks that
provide highly optimized algorithms, they often come with custom self-written
data structures to manage the underlying data. In this work, we present stdgpu,
an open-source library which defines several generic GPU data structures for
fast and reliable data management. Rather than abandoning previous established
frameworks, our library aims to extend them, therefore bridging the gap between
CPU and GPU computing. This way, it provides clean and familiar interfaces and
integrates seamlessly into new as well as existing projects. We hope to foster
further developments towards unified CPU and GPU computing and welcome
contributions from the community.",['Patrick Stotko'],2019-08-16T11:37:42Z,http://arxiv.org/abs/1908.05936v1,"['cs.DC', 'cs.GR']","parallel computing,GPU,data structures,open-source,algorithms,CPU,library,frameworks,real-time,graphics"
"Impacts of Retina-related Zones on Quality Perception of Omnidirectional
  Image","Virtual Reality (VR), which brings immersive experiences to viewers, has been
gaining popularity in recent years. A key feature in VR systems is the use of
omnidirectional content, which provides 360-degree views of scenes. In this
work, we study the human quality perception of omnidirectional images, focusing
on different zones surrounding the foveation point. For that purpose, an
extensive subjective experiment is carried out to assess the perceptual quality
of omnidirectional images with non-uniform quality. Through experimental
results, the impacts of different zones are analyzed. Moreover, nineteen
objective quality metrics, including foveal quality metrics, are evaluated
using our database. It is quantitatively shown that the zones corresponding to
the fovea and parafovea of human eyes are extremely important for quality
perception, while the impacts of the other zones corresponding to the perifovea
and periphery are small. Besides, the investigated metrics are found to be not
effective enough to reflect the quality perceived by viewers.","['Huyen T. T. Tran', 'Duc V. Nguyen', 'Nam Pham Ngoc', 'Trang H. Hoang', 'Truong Thu Huong', 'Truong Cong Thang']",2019-08-17T04:43:18Z,http://arxiv.org/abs/1908.06239v1,"['eess.IV', 'cs.MM']","Retina-related,Quality Perception,Omnidirectional Image,Virtual Reality,Fovea,Parafovea,Perifovea,Periphery,Subjective Experiment,Objective Quality Metrics"
"Efficient 2.5D Hand Pose Estimation via Auxiliary Multi-Task Training
  for Embedded Devices","2D Key-point estimation is an important precursor to 3D pose estimation
problems for human body and hands. In this work, we discuss the data,
architecture, and training procedure necessary to deploy extremely efficient
2.5D hand pose estimation on embedded devices with highly constrained memory
and compute envelope, such as AR/VR wearables. Our 2.5D hand pose estimation
consists of 2D key-point estimation of joint positions on an egocentric image,
captured by a depth sensor, and lifted to 2.5D using the corresponding depth
values. Our contributions are two fold: (a) We discuss data labeling and
augmentation strategies, the modules in the network architecture that
collectively lead to $3\%$ the flop count and $2\%$ the number of parameters
when compared to the state of the art MobileNetV2 architecture. (b) We propose
an auxiliary multi-task training strategy needed to compensate for the small
capacity of the network while achieving comparable performance to MobileNetV2.
Our 32-bit trained model has a memory footprint of less than 300 Kilobytes,
operates at more than 50 Hz with less than 35 MFLOPs.","['Prajwal Chidananda', 'Ayan Sinha', 'Adithya Rao', 'Douglas Lee', 'Andrew Rabinovich']",2019-09-12T18:33:05Z,http://arxiv.org/abs/1909.05897v1,"['cs.CV', 'cs.HC']","2.5D hand pose estimation,Multi-task training,Embedded devices,Key-point estimation,Data labeling,Depth sensor,MobileNetV2,Augmentation strategies,Network architecture,Memory footprint"
Learning an Action-Conditional Model for Haptic Texture Generation,"Rich haptic sensory feedback in response to user interactions is desirable
for an effective, immersive virtual reality or teleoperation system. However,
this feedback depends on material properties and user interactions in a
complex, non-linear manner. Therefore, it is challenging to model the mapping
from material and user interactions to haptic feedback in a way that
generalizes over many variations of the user's input. Current methodologies are
typically conditioned on user interactions, but require a separate model for
each material. In this paper, we present a learned action-conditional model
that uses data from a vision-based tactile sensor (GelSight) and user's action
as input. This model predicts an induced acceleration that could be used to
provide haptic vibration feedback to a user. We trained our proposed model on a
publicly available dataset (Penn Haptic Texture Toolkit) that we augmented with
GelSight measurements of the different materials. We show that a unified model
over all materials outperforms previous methods and generalizes to new actions
and new instances of the material categories in the dataset.","['Negin Heravi', 'Wenzhen Yuan', 'Allison M. Okamura', 'Jeannette Bohg']",2019-09-28T04:45:42Z,http://arxiv.org/abs/1909.13025v2,"['cs.RO', 'cs.LG']","haptic texture generation,action-conditional model,virtual reality,teleoperation system,material properties,user interactions,tactile sensor,GelSight,haptic feedback,dataset"
"A Mobile Manipulation System for One-Shot Teaching of Complex Tasks in
  Homes","We describe a mobile manipulation hardware and software system capable of
autonomously performing complex human-level tasks in real homes, after being
taught the task with a single demonstration from a person in virtual reality.
This is enabled by a highly capable mobile manipulation robot, whole-body task
space hybrid position/force control, teaching of parameterized primitives
linked to a robust learned dense visual embeddings representation of the scene,
and a task graph of the taught behaviors. We demonstrate the robustness of the
approach by presenting results for performing a variety of tasks, under
different environmental conditions, in multiple real homes. Our approach
achieves 85% overall success rate on three tasks that consist of an average of
45 behaviors each.","['Max Bajracharya', 'James Borders', 'Dan Helmick', 'Thomas Kollar', 'Michael Laskey', 'John Leichty', 'Jeremy Ma', 'Umashankar Nagarajan', 'Akiyoshi Ochiai', 'Josh Petersen', 'Krishna Shankar', 'Kevin Stone', 'Yutaka Takaoka']",2019-09-30T22:03:07Z,http://arxiv.org/abs/1910.00127v3,"['cs.RO', 'cs.CV']","mobile manipulation system,complex tasks,one-shot teaching,homes,autonomous tasks,real homes,virtual reality,task space control,dense visual embeddings,task graph"
"Eyenet: Attention based Convolutional Encoder-Decoder Network for Eye
  Region Segmentation","With the immersive development in the field of augmented and virtual reality,
accurate and speedy eye-tracking is required. Facebook Research has organized a
challenge, named OpenEDS Semantic Segmentation challenge for per-pixel
segmentation of the key eye regions: the sclera, the iris, the pupil, and
everything else (background). There are two constraints set for the
participants viz MIOU and the computational complexity of the model. More
recently, researchers have achieved quite a good result using the convolutional
neural networks (CNN) in segmenting eyeregions. However, the environmental
challenges involved in this task such as low resolution, blur, unusual glint
and, illumination, off-angles, off-axis, use of glasses and different color of
iris region hinder the accuracy of segmentation. To address the challenges in
eye segmentation, the present work proposes a robust and computationally
efficient attention-based convolutional encoder-decoder network for segmenting
all the eye regions. Our model, named EyeNet, includes modified residual units
as the backbone, two types of attention blocks and multi-scale supervision for
segmenting the aforesaid four eye regions. Our proposed model achieved a total
score of 0.974(EDS Evaluation metric) on test data, which demonstrates superior
results compared to the baseline methods.","['Priya Kansal', 'Sabari Nathan']",2019-10-08T08:43:27Z,http://arxiv.org/abs/1910.03274v1,['cs.CV'],"eye region segmentation,attention-based,convolutional encoder-decoder network,CNN,segmentation,sclera,iris,pupil,environmental challenges,EyeNet"
"Optimization and Manipulation of Contextual Mutual Spaces for Multi-User
  Virtual and Augmented Reality Interaction","Spatial computing experiences are physically constrained by the geometry and
semantics of the local user environment. This limitation is elevated in remote
multi-user interaction scenarios, where finding a common virtual ground
physically accessible for all participants becomes challenging. Locating a
common accessible virtual ground is difficult for the users themselves,
particularly if they are not aware of the spatial properties of other
participants. In this paper, we introduce a framework to generate an optimal
mutual virtual space for a multi-user interaction setting where remote users'
room spaces can have different layout and sizes. The framework further
recommends movement of surrounding furniture objects that expand the size of
the mutual space with minimal physical effort. Finally, we demonstrate the
performance of our solution on real-world datasets and also a real HoloLens
application. Results show the proposed algorithm can effectively discover
optimal shareable space for multi-user virtual interaction and hence facilitate
remote spatial computing communication in various collaborative workflows.","['Mohammad Keshavarzi', 'Allen Y. Yang', 'Woojin Ko', 'Luisa Caldas']",2019-10-14T09:10:54Z,http://arxiv.org/abs/1910.05998v2,"['cs.HC', 'cs.CV', 'cs.GR']","Optimization,Manipulation,Contextual Mutual Spaces,Multi-User,Virtual Reality,Augmented Reality,Interaction,Spatial Computing,Framework,HoloLens"
"Occurence of A Cyber Security Eco-System: A Nature Oriented Project and
  Evaluation of An Indirect Social Experiment","Because of todays technological developments and the influence of digital
systems into every aspect of our lives, importance of cyber security improves
more and more day-by-day. Projects, educational processes and seminars realized
for this aim create and improve awareness among individuals and provide useful
tools for growing equipped generations. The aim of this study is to focus on a
cyber security eco-system, which was self-occurred within the interactive
educational environment designed under the scope of TUBITAK 4004 Nature
Education and Science Schools Projects (with the name of A Cyber Security
Adventure) with the use of important technologies such as virtual reality,
augmented reality, and artificial intelligence. The eco-system occurred within
the interactive educational process where high school students took place
caused both students and the project team to experience an indirect social
experiment environment. In this sense, it is thought that the findings and
comments presented in the study will give important ideas to everyone involved
in cyber security education, life-long learning processes, and the technology
use in software oriented educational tools.",['Utku Kose'],2019-09-29T22:04:19Z,http://arxiv.org/abs/1910.07083v1,"['cs.HC', 'cs.CY']","Cyber security,Eco-system,Nature oriented project,Evaluation,Social experiment,Virtual reality,Augmented reality,Artificial intelligence,TUBITAK 4004,Educational environment"
AeroVR: Immersive Visualization System for Aerospace Design,"One of today's most propitious immersive technologies is virtual reality
(VR). This term is colloquially associated with headsets that transport users
to a bespoke, built-for-purpose immersive 3D virtual environment. It has given
rise to the field of immersive analytics---a new field of research that aims to
use immersive technologies for enhancing and empowering data analytics.
However, in developing such a new set of tools, one has to ask whether the move
from standard hardware setup to a fully immersive 3D environment is
justified---both in terms of efficiency and development costs. To this end, in
this paper, we present the AeroVR--an immersive aerospace design environment
with the objective of aiding the component aerodynamic design process by
interactively visualizing performance and geometry. We decompose the design of
such an environment into function structures, identify the primary and
secondary tasks, present an implementation of the system, and verify the
interface in terms of usability and expressiveness. We deploy AeroVR on a
prototypical design study of a compressor blade for an engine.","['Slawomir Konrad Tadeja', 'Pranay Seshadri', 'Per Ola Kristensson']",2019-10-22T07:22:29Z,http://arxiv.org/abs/1910.09800v1,['cs.HC'],"immersive visualization,aerospace design,virtual reality,immersive technologies,3D environment,immersive analytics,aerodynamic design,component design,interface,usability."
"Immersive Insights: A Hybrid Analytics System for Collaborative
  Exploratory Data Analysis","In the past few years, augmented reality (AR) and virtual reality (VR)
technologies have experienced terrific improvements in both accessibility and
hardware capabilities, encouraging the application of these devices across
various domains. While researchers have demonstrated the possible advantages of
AR and VR for certain data science tasks, it is still unclear how these
technologies would perform in the context of exploratory data analysis (EDA) at
large. In particular, we believe it is important to better understand which
level of immersion EDA would concretely benefit from, and to quantify the
contribution of AR and VR with respect to standard analysis workflows.
  In this work, we leverage a Dataspace reconfigurable hybrid reality
environment to study how data scientists might perform EDA in a co-located,
collaborative context. Specifically, we propose the design and implementation
of Immersive Insights, a hybrid analytics system combining high-resolution
displays, table projections, and augmented reality (AR) visualizations of the
data.
  We conducted a two-part user study with twelve data scientists, in which we
evaluated how different levels of data immersion affect the EDA process and
compared the performance of Immersive Insights with a state-of-the-art,
non-immersive data analysis system.","['Marco Cavallo', 'Mishal Dholakia', 'Matous Havlena', 'Kenneth Ocheltree', 'Mark Podlaseck']",2019-10-27T06:44:30Z,http://arxiv.org/abs/1910.12193v1,['cs.HC'],"augmented reality,virtual reality,hybrid analytics system,collaborative,exploratory data analysis,immersion,data scientists"
A Survey on Human Machine Interaction in Industry 4.0,"Industry 4.0 or Industrial IoT both describe new paradigms for seamless
interaction between humans and machines. Both concepts rely on intelligent,
inter-connected cyber-physical production systems that are able to control the
process flow of industrial production. As those machines take many decisions
autonomously and further interact with production and manufacturing planning
systems, the integration of human users requires new paradigms. In this paper,
we provide an analysis of the current state-of-the-art in human-machine
interaction in the Industry 4.0 domain.We focus on new paradigms that integrate
the application of augmented and virtual reality technology. Based on our
analysis, we further provide a discussion of research challenges.","['Christian Krupitzer', 'Sebastian Müller', 'Veronika Lesch', 'Marwin Züfle', 'Janick Edinger', 'Alexander Lemken', 'Dominik Schäfer', 'Samuel Kounev', 'Christian Becker']",2020-02-03T21:53:48Z,http://arxiv.org/abs/2002.01025v1,['cs.HC'],"Industry 4.0,Human-machine interaction,Industrial IoT,Cyber-physical production systems,Autonomous decision-making,Manufacturing planning systems,Augmented reality,Virtual reality,Research challenges"
"An overview of physical layer design for Ultra-Reliable Low-Latency
  Communications in 3GPP Release 15 and Release 16","For the fifth generation (5G) wireless network technology, the most important
design goals were improved metrics of reliability and latency, in addition to
network resilience and flexibility so as to best adapt the network operation
for new applications such as augmented virtual reality, industrial automation
and autonomous vehicles. That led to design efforts conducted under the subject
of ultra-reliable low-latency communication (URLLC). The technologies
integrated in Release 15 of the 3rd Generation Partnership Project (3GPP)
finalized in December 2017 provided the foundation of physical layer design for
URLLC. New features such as numerologies, new transmission schemes with
sub-slot interval, configured grant resources, etc., were standardized to
improve mainly the latency aspects.
  5G evolution in Release 16, PHY version finalized in December 2019, allows
achieving improved metrics for latency and reliability by standardizing a
number of new features. This article provides a detailed overview of the URLLC
features from 5G Release 15 and Release 16, describing how these features
permit URLLC operation in 5G networks.","['Trung-Kien Le', 'Umer Salim', 'Florian Kaltenberger']",2020-02-10T13:18:19Z,http://arxiv.org/abs/2002.03713v1,['eess.SP'],"physical layer design,Ultra-Reliable Low-Latency Communications,3GPP Release 15,Release 16,reliability,latency,network resilience,flexibility,numerologies,transmission schemes"
"Computation Resource Allocation for Heterogeneous Time-Critical IoT
  Services in MEC","Mobile edge computing (MEC) is one of the promising solutions to process
computational-intensive tasks within short latency for emerging
Internet-of-Things (IoT) use cases, e.g., virtual reality (VR), augmented
reality (AR), autonomous vehicle. Due to the coexistence of heterogeneous
services in MEC system, the task arrival interval and required execution time
can vary depending on services. It is challenging to schedule computation
resource for the services with stochastic arrivals and runtime at an edge
server (ES). In this paper, we propose a flexible computation offloading
framework among users and ESs. Based on the framework, we propose a
Lyapunov-based algorithm to dynamically allocate computation resource for
heterogeneous time-critical services at the ES. The proposed algorithm
minimizes the average timeout probability without any prior knowledge on task
arrival process and required runtime. The numerical results show that, compared
with the standard queuing models used at ES, the proposed algorithm achieves at
least 35% reduction of the timeout probability, and approximated utilization
efficiency of computation resource to non-cause queuing model under various
scenarios.","['Jianhui Liu', 'Qi Zhang']",2020-02-12T09:00:55Z,http://arxiv.org/abs/2002.04851v1,['cs.NI'],"Mobile edge computing,Internet-of-Things,computation resource allocation,time-critical services,heterogeneous services,edge server,Lyapunov-based algorithm,average timeout probability,queuing models,utilization efficiency"
Presence in VR experiences -- an empirical cost-benefit-analysis,"Virtual reality (VR) is on the edge of getting a mainstream platform for
gaming, education and product design. The feeling of being present in the
virtual world is influenced by many factors and even more intriguing a single
negative influence can destroy the illusion that was created with a lot of
effort by other measures. Therefore, it is crucial to have a balance between
the influencing factors, know the importance of the factors and have a good
estimation of how much effort it takes to bring each factor to a certain level
of fidelity. This paper collects influencing factors discussed in literature,
analyses the immersion of current off-the-shelf VR-solutions and presents
results from an empirical study on efforts and benefits from certain aspects
influencing presence in VR experiences. It turns out, that sometimes delivering
high fidelity is easier to achieve than medium fidelity and for other aspects
it is worthwhile investing more effort to achieve higher fidelity to improve
presence a lot.","['René Peinl', 'Tobias Wirth']",2020-02-17T15:17:17Z,http://arxiv.org/abs/2002.07576v1,"['cs.HC', 'I.6.3']","Virtual reality,Presence,Empirical study,Gaming,Education,Product design,Influencing factors,Immersion,Fidelity"
3D Augmented Reality Tangible User Interface using Commodity Hardware,"During the last years, the emerging field of Augmented and Virtual Reality
(AR-VR) has seen tremendous growth. An interface that has also become very
popular for the AR systems is the tangible interface or passive-haptic
interface. Specifically, an interface where users can manipulate digital
information with input devices that are physical objects. This work presents a
low cost Augmented Reality system with a tangible interface that offers
interaction between the real and the virtual world. The system estimates in
real-time the 3D position of a small colored ball (input device), it maps it to
the 3D virtual world and then uses it to control the AR application that runs
in a mobile device. Using the 3D position of our ""input"" device, it allows us
to implement more complicated interactivity compared to a 2D input device.
Finally, we present a simple, fast and robust algorithm that can estimate the
corners of a convex quadrangle. The proposed algorithm is suitable for the fast
registration of markers and significantly improves performance compared to the
state of the art.","['Dimitrios Chamzas', 'Konstantinos Moustakas']",2020-03-02T18:29:58Z,http://arxiv.org/abs/2003.01092v1,['cs.HC'],"Augmented Reality,Tangible User Interface,Commodity Hardware,3D position,Virtual Reality,Passive-haptic interface,Input device,2D input device,Convex quadrangle,Algorithm"
Gaze-Sensing LEDs for Head Mounted Displays,"We introduce a new gaze tracker for Head Mounted Displays (HMDs). We modify
two off-the-shelf HMDs to be gaze-aware using Light Emitting Diodes (LEDs). Our
key contribution is to exploit the sensing capability of LEDs to create
low-power gaze tracker for virtual reality (VR) applications. This yields a
simple approach using minimal hardware to achieve good accuracy and low latency
using light-weight supervised Gaussian Process Regression (GPR) running on a
mobile device. With our hardware, we show that Minkowski distance measure based
GPR implementation outperforms the commonly used radial basis function-based
support vector regression (SVR) without the need to precisely determine free
parameters. We show that our gaze estimation method does not require complex
dimension reduction techniques, feature extraction, or distortion corrections
due to off-axis optical paths. We demonstrate two complete HMD prototypes with
a sample eye-tracked application, and report on a series of subjective tests
using our prototypes.","['Kaan Akşit', 'Jan Kautz', 'David Luebke']",2020-03-18T23:03:06Z,http://arxiv.org/abs/2003.08499v1,"['cs.CV', 'cs.HC', 'cs.LG']","gaze tracker,Head Mounted Displays,Light Emitting Diodes,virtual reality,Gaussian Process Regression,Minkowski distance measure,support vector regression,eye-tracked application,subjective tests"
"Hazard recognition in an immersive virtual environment: Framework for
  the simultaneous analysis of visual search and EEG patterns","Unmanaged hazards in dangerous construction environments proved to be one of
the main sources of injuries and accidents. Hazard recognition is crucial to
achieve effective safety management and reduce injuries and fatalities in
hazardous job sites. Still, there has been lack of effort that can efficiently
assist workers in improving their hazard recognition skills. This study
presents virtual safety training in an Immersive Virtual Environment (IVE) to
enhance worker's hazard recognition skills. A worker wearing a Virtual Reality
(VR) device, that is equipped with an eye-tracker, virtually recognizes hazards
on simulated construction sites while a brainwave-sensing device records brain
activities. This platform can analyze the overall performance of the workers in
a visual hazard recognition task and identify hazards that need additional
intervention for each worker. This study provides novel insights on how a
worker's brain and eye act simultaneously during a visual hazard recognition
process. The presented method can take current safety training programs into
another level by providing personalized feedback to the workers.","['Mojtaba Noghabaei', 'Kevin Han']",2020-03-14T20:12:38Z,http://arxiv.org/abs/2003.09494v1,"['cs.HC', 'cs.CY', 'eess.SP']","Hazard recognition,Immersive Virtual Environment,Visual search,EEG patterns,Virtual Reality,Eye-tracker,Brainwave-sensing device,Safety training,Hazard analysis,Personalized feedback"
LayoutMP3D: Layout Annotation of Matterport3D,"Inferring the information of 3D layout from a single equirectangular panorama
is crucial for numerous applications of virtual reality or robotics (e.g.,
scene understanding and navigation). To achieve this, several datasets are
collected for the task of 360 layout estimation. To facilitate the learning
algorithms for autonomous systems in indoor scenarios, we consider the
Matterport3D dataset with their originally provided depth map ground truths and
further release our annotations for layout ground truths from a subset of
Matterport3D. As Matterport3D contains accurate depth ground truths from
time-of-flight (ToF) sensors, our dataset provides both the layout and depth
information, which enables the opportunity to explore the environment by
integrating both cues.","['Fu-En Wang', 'Yu-Hsuan Yeh', 'Min Sun', 'Wei-Chen Chiu', 'Yi-Hsuan Tsai']",2020-03-30T14:40:56Z,http://arxiv.org/abs/2003.13516v1,['cs.CV'],"3D layout,annotation,Matterport3D,equirectangular panorama,virtual reality,robotics,scene understanding,navigation,depth map,autonomous systems"
"Multi-agent Reinforcement Learning for Resource Allocation in IoT
  networks with Edge Computing","To support popular Internet of Things (IoT) applications such as virtual
reality, mobile games and wearable devices, edge computing provides a front-end
distributed computing archetype of centralized cloud computing with low
latency. However, it's challenging for end users to offload computation due to
their massive requirements on spectrum and computation resources and frequent
requests on Radio Access Technology (RAT). In this paper, we investigate
computation offloading mechanism with resource allocation in IoT edge computing
networks by formulating it as a stochastic game. Here, each end user is a
learning agent observing its local environment to learn optimal decisions on
either local computing or edge computing with the goal of minimizing long term
system cost by choosing its transmit power level, RAT and sub-channel without
knowing any information of the other end users. Therefore, a multi-agent
reinforcement learning framework is developed to solve the stochastic game with
a proposed independent learners based multi-agent Q-learning (IL-based MA-Q)
algorithm. Simulations demonstrate that the proposed IL-based MA-Q algorithm is
feasible to solve the formulated problem and is more energy efficient without
extra cost on channel estimation at the centralized gateway compared to the
other two benchmark algorithms.","['Xiaolan Liu', 'Jiadong Yu', 'Yue Gao']",2020-04-05T20:59:20Z,http://arxiv.org/abs/2004.02315v1,"['eess.SP', 'cs.LG']","Multi-agent reinforcement learning,Resource allocation,IoT,Edge computing,Stochastic game,Computation offloading,Radio Access Technology (RAT),Sub-channel,Independent learners,Q-learning"
MAGES 3.0: Tying the knot of medical VR,"In this work, we present MAGES 3.0, a novel Virtual Reality (VR)-based
authoring SDK platform for accelerated surgical training and assessment. The
MAGES Software Development Kit (SDK) allows code-free prototyping of any VR
psychomotor simulation of medical operations by medical professionals, who
urgently need a tool to solve the issue of outdated medical training. Our
platform encapsulates the following novel algorithmic techniques: a)
collaborative networking layer with Geometric Algebra (GA) interpolation engine
b) supervised machine learning analytics module for real-time recommendations
and user profiling c) GA deformable cutting and tearing algorithm d) on-the-go
configurable soft body simulation for deformable surfaces.","['George Papagiannakis', 'Paul Zikas', 'Nick Lydatakis', 'Steve Kateros', 'Mike Kentros', 'Efstratios Geronikolakis', 'Manos Kamarianakis', 'Ioanna Kartsonaki', 'Giannis Evangelou']",2020-05-03T20:27:10Z,http://arxiv.org/abs/2005.01180v2,"['cs.HC', '97R60, 97U50, 97Q70, 97Q60', 'K.3.1; I.3.6; I.6.8; I.2.6']","Virtual Reality,SDK,surgical training,medical operations,psychomotor simulation,collaborative networking,Geometric Algebra,machine learning analytics,soft body simulation,deformable surfaces"
"A Survey Study to Understand Industry Vision for Virtual and Augmented
  Reality Applications in Design and Construction","With advances in Building Information Modeling (BIM), Virtual Reality (VR)
and Augmented Reality (AR) technologies have many potential applications in the
Architecture, Engineering, and Construction (AEC) industry. However, the AEC
industry, relative to other industries, has been slow in adopting AR/VR
technologies, partly due to lack of feasibility studies examining the actual
cost of implementation versus an increase in profit. The main objectives of
this paper are to understand the industry trends in adopting AR/VR technologies
and identifying gaps between AEC research and industry practices. The
identified gaps can lead to opportunities for developing new tools and finding
new use cases. To achieve these goals, two rounds of a survey at two different
time periods (a year apart) were conducted. Responses from 158 industry experts
and researchers were analyzed to assess the current state, growth, and saving
opportunities for AR/VR technologies for the AEC industry. The authors used
t-test for hypothesis testing. The findings show a significant increase in
AR/VR utilization in the AEC industry over the past year from 2017 to 2018. The
industry experts also anticipate strong growth in the use of AR/VR technologies
over the next 5 to 10 years.","['Mojtaba Noghabaei', 'Arsalan Heydarian', 'Vahid Balali', 'Kevin Han']",2020-05-06T13:16:05Z,http://arxiv.org/abs/2005.02795v1,['cs.CY'],"Building Information Modeling,Virtual Reality,Augmented Reality,Architecture,Engineering,Construction,Feasibility Studies,Industry Trends,Use Cases"
The Hyper360 toolset for enriched 360$^\circ$ video,"360$^\circ$ video is a novel media format, rapidly becoming adopted in media
production and consumption as part of todays ongoing virtual reality
revolution. Due to its novelty, there is a lack of tools for producing highly
engaging 360$^\circ$ video for consumption on a multitude of platforms. In this
work, we describe the work done so far in the Hyper360 project on tools for
360$^\circ$ video. Furthermore, the first pilots which have been produced with
the Hyper360 tools are presented.","['Hannes Fassold', 'Antonis Karakottas', 'Dorothea Tsatsou', 'Dimitrios Zarpalas', 'Barnabas Takacs', 'Christian Fuhrhop', 'Angelo Manfredi', 'Nicolas Patz', 'Simona Tonoli', 'Iana Dulskaia']",2020-04-19T13:50:30Z,http://arxiv.org/abs/2005.06944v1,['cs.MM'],"360-degree video,media format,virtual reality,Hyper360,tools,consumption,platforms,project,pilots"
Mobile Edge Computing Network Control: Tradeoff Between Delay and Cost,"As mobile edge computing (MEC) finds widespread use for relieving the
computational burden of compute- and interaction-intensive applications on end
user devices, understanding the resulting delay and cost performance is drawing
significant attention. While most existing works focus on singletask offloading
in single-hop MEC networks, next generation applications (e.g., industrial
automation, augmented/virtual reality) require advance models and algorithms
for dynamic configuration of multi-task services over multi-hop MEC networks.
In this work, we leverage recent advances in dynamic cloud network control to
provide a comprehensive study of the performance of multi-hop MEC networks,
addressing the key problems of multi-task offloading, timely packet scheduling,
and joint computation and communication resource allocation. We present a fully
distributed algorithm based on Lyapunov control theory that achieves
throughput-optimal performance with delay and cost guarantees. Simulation
results validate our theoretical analysis and provide insightful guidelines on
the interplay between communication and computation resources in MEC networks.","['Yang Cai', 'Jaime Llorca', 'Antonia M. Tulino', 'Andreas F. Molisch']",2020-05-16T03:21:47Z,http://arxiv.org/abs/2005.07854v3,"['cs.NI', 'cs.SY', 'eess.SY']","Mobile Edge Computing,Network Control,Delay,Cost,Multi-hop,Multi-task Offloading,Packet Scheduling,Computation Resource Allocation,Communication Resource Allocation,Lyapunov Control Theory"
"Gentlemen on the Road: Understanding How Pedestrians Interpret Yielding
  Behavior of Autonomous Vehicles using Machine Learning","Autonomous vehicles (AVs) can prevent collisions by understanding pedestrian
intention. We conducted a virtual reality experiment with 39 participants and
measured crossing times (seconds) and head orientation (yaw degrees). We
manipulated AV yielding behavior (no-yield, slow-yield, and fast-yield) and the
AV size (small, medium, and large). Using machine learning approach, we
classified head orientation change of pedestrians by time into 6 clusters of
patterns. Results indicate that pedestrian head orientation change was
influenced by AV yielding behavior as well as the size of the AV. Participants
fixated on the front most of the time even when the car approached near.
Participants changed head orientation most frequently when a large size AV did
not yield (no-yield). In post-experiment interviews, participants reported that
yielding behavior and size affected their decision to cross and perceived
safety. For autonomous vehicles to be perceived more safe and trustful,
vehicle-specific factors such as size and yielding behavior should be
considered in the designing process.","['Yoon Kyung Lee', 'Yong-Eun Rhee', 'Jeh-Kwang Ryu', 'Sowon Hahn']",2020-05-16T04:54:37Z,http://arxiv.org/abs/2005.07872v2,"['cs.HC', 'cs.RO']","autonomous vehicles,pedestrians,yielding behavior,machine learning,crossing times,head orientation,AV size,virtual reality experiment,clusters,perception"
Benefits of temporal information for appearance-based gaze estimation,"State-of-the-art appearance-based gaze estimation methods, usually based on
deep learning techniques, mainly rely on static features. However, temporal
trace of eye gaze contains useful information for estimating a given gaze
point. For example, approaches leveraging sequential eye gaze information when
applied to remote or low-resolution image scenarios with off-the-shelf cameras
are showing promising results. The magnitude of contribution from temporal gaze
trace is yet unclear for higher resolution/frame rate imaging systems, in which
more detailed information about an eye is captured. In this paper, we
investigate whether temporal sequences of eye images, captured using a
high-resolution, high-frame rate head-mounted virtual reality system, can be
leveraged to enhance the accuracy of an end-to-end appearance-based
deep-learning model for gaze estimation. Performance is compared against a
static-only version of the model. Results demonstrate statistically-significant
benefits of temporal information, particularly for the vertical component of
gaze.","['Cristina Palmero', 'Oleg V. Komogortsev', 'Sachin S. Talathi']",2020-05-24T07:19:53Z,http://arxiv.org/abs/2005.11670v1,"['cs.CV', 'cs.LG']","deep learning,gaze estimation,temporal information,appearance-based,eye gaze,high-resolution,high-frame rate,virtual reality,end-to-end model,static-only"
Towards Mesh Saliency Detection in 6 Degrees of Freedom,"Traditional 3D mesh saliency detection algorithms and corresponding databases
were proposed under several constraints such as providing limited viewing
directions and not taking the subject's movement into consideration. In this
work, a novel 6DoF mesh saliency database is developed which provides both the
subject's 6DoF data and eye-movement data. Different from traditional
databases, subjects in the experiment are allowed to move freely to observe 3D
meshes in a virtual reality environment. Based on the database, we first
analyze the inter-observer variation and the influence of viewing direction
towards subject's visual attention, then we provide further investigations
about the subject's visual attention bias during observation. Furthermore, we
propose a 6DoF mesh saliency detection algorithm based on the uniqueness
measure and the bias preference. To evaluate the proposed approach, we also
design an evaluation metric accordingly which takes the 6DoF information into
consideration, and extend some state-of-the-art 3D saliency detection methods
to make comparisons. The experimental results demonstrate the superior
performance of our approach for 6DoF mesh saliency detection, in addition to
providing benchmarks for the presented 6DoF mesh saliency database. The
database and the corresponding algorithms will be made publicly available for
research purposes.","['Xiaoying Ding', 'Zhenzhong Chen']",2020-05-27T02:04:33Z,http://arxiv.org/abs/2005.13127v2,"['cs.CV', 'eess.IV']","Mesh saliency detection,6 degrees of freedom,database,eye-movement data,virtual reality,visual attention,uniqueness measure,bias preference,evaluation metric,benchmarks."
A Virtual Obstacle Course within Diverse Sensory Environments,"We developed a novel assessment platform with untethered virtual reality,
3-dimensional sounds, and pressure sensing floor mat to help assess the walking
balance and negotiation of obstacles given diverse sensory load and/or
cognitive load. The platform provides an immersive 3D city-like scene with
anticipated/unanticipated virtual obstacles. Participants negotiate the
obstacles with perturbations of: auditory load by spatial audio, cognitive load
by a memory task, and visual flow by generated by avatars movements at various
amounts and speeds. A VR headset displays the scenes while providing real-time
position and orientation of the participant's head. A pressure-sensing walkway
senses foot pressure and visualizes it in a heatmap. The system helps to assess
walking balance via pressure dynamics per foot, success rate of crossing
obstacles, available response time as well as head kinematics in response to
obstacles and multitasking. Based on the assessment, specific balance training
and fall prevention program can be prescribed.","['Zhu Wang', 'Anat Lubetzky', 'Charles Hendee', 'Marta Gospodarek', 'Ken Perlin']",2020-05-31T02:03:45Z,http://arxiv.org/abs/2006.00410v1,['cs.HC'],"Virtual reality,3D sounds,pressure sensing floor mat,sensory load,cognitive load,auditory load,visual flow,VR headset,pressure dynamics,balance training"
"MusicID: A Brainwave-based User Authentication System for Internet of
  Things","We propose MusicID, an authentication solution for smart devices that uses
music-induced brainwave patterns as a behavioral biometric modality. We
experimentally evaluate MusicID using data collected from real users whilst
they are listening to two forms of music; a popular English song and
individual's favorite song. We show that an accuracy over 98% for user
identification and an accuracy over 97% for user verification can be achieved
by using data collected from a 4-electrode commodity brainwave headset. We
further show that a single electrode is able to provide an accuracy of
approximately 85% and the use of two electrodes provides an accuracy of
approximately 95%. As already shown by commodity brain-sensing headsets for
meditation applications, we believe including dry EEG electrodes in
smart-headsets is feasible and MusicID has the potential of providing an entry
point and continuous authentication framework for upcoming surge of
smart-devices mainly driven by Augmented Reality (AR)/Virtual Reality (VR)
applications.","['Jinani Sooriyaarachchi', 'Suranga Seneviratne', 'Kanchana Thilakarathna', 'Albert Y. Zomaya']",2020-06-02T16:23:49Z,http://arxiv.org/abs/2006.01751v1,"['cs.CR', 'eess.SP']","MusicID,Brainwave-based,User Authentication System,Internet of Things,Behavioral Biometric Modality,Data Collection,Electrode,User Identification,User Verification,Smart Devices"
Learning Neural Light Transport,"In recent years, deep generative models have gained significance due to their
ability to synthesize natural-looking images with applications ranging from
virtual reality to data augmentation for training computer vision models. While
existing models are able to faithfully learn the image distribution of the
training set, they often lack controllability as they operate in 2D pixel space
and do not model the physical image formation process. In this work, we
investigate the importance of 3D reasoning for photorealistic rendering. We
present an approach for learning light transport in static and dynamic 3D
scenes using a neural network with the goal of predicting photorealistic
images. In contrast to existing approaches that operate in the 2D image domain,
our approach reasons in both 3D and 2D space, thus enabling global illumination
effects and manipulation of 3D scene geometry. Experimentally, we find that our
model is able to produce photorealistic renderings of static and dynamic
scenes. Moreover, it compares favorably to baselines which combine path tracing
and image denoising at the same computational budget.","['Paul Sanzenbacher', 'Lars Mescheder', 'Andreas Geiger']",2020-06-05T13:26:05Z,http://arxiv.org/abs/2006.03427v1,['cs.CV'],"deep generative models,neural network,photorealistic rendering,3D scenes,global illumination,image formation,path tracing,image denoising,computational budget"
"Ergodic Specifications for Flexible Swarm Control: From User Commands to
  Persistent Adaptation","This paper presents a formulation for swarm control and high-level task
planning that is dynamically responsive to user commands and adaptable to
environmental changes. We design an end-to-end pipeline from a tactile tablet
interface for user commands to onboard control of robotic agents based on
decentralized ergodic coverage. Our approach demonstrates reliable and dynamic
control of a swarm collective through the use of ergodic specifications for
planning and executing agent trajectories as well as responding to user and
external inputs. We validate our approach in a virtual reality simulation
environment and in real-world experiments at the DARPA OFFSET Urban Swarm
Challenge FX3 field tests with a robotic swarm where user-based control of the
swarm and mission-based tasks require a dynamic and flexible response to
changing conditions and objectives in real-time.","['Ahalya Prabhakar', 'Ian Abraham', 'Annalisa Taylor', 'Millicent Schlafly', 'Katarina Popovic', 'Giovani Diniz', 'Brendan Teich', 'Borislava Simidchieva', 'Shane Clark', 'Todd Murphey']",2020-06-10T21:51:18Z,http://arxiv.org/abs/2006.06081v1,['cs.RO'],"Swarm control,Task planning,User commands,Adaptation,Ergodic specifications,Robotic agents,Decentralized coverage,Trajectories,Real-world experiments,Dynamic response"
The 4th Industrial Revolution Effect on the Enterprise Cyber Strategy,"The Fourth (4th) Industrial Revolution represents the profound advancement of
technology that will likely transform the boundaries between the digital and
physical worlds in modern society. The impact of advance technology will
disrupt almost every aspect of business and government communities alike. In
the past few years, the advancement of information technologies has opened the
door to artificial intelligence (AI), block chain technologies, robotics,
virtual reality and the possibility of quantum computing being released in the
commercial sector. The use of these innovative technologies will likely impact
society by leveraging modern technological platforms such as cloud computing
and AI. This also includes the release of 5G network technologies by Internet
Service Providers (ISP) beginning in 2019. Networks that rely upon 5G
technologies in combination with cloud computing platforms will open the door
allow greater innovations and change the nature of how work is performed in the
4th Industrial Revolution.",['Christopher Gorham'],2020-06-12T22:04:11Z,http://arxiv.org/abs/2006.07488v1,['cs.CY'],"4th Industrial Revolution,Enterprise Cyber Strategy,technology advancement,artificial intelligence,blockchain technologies,robotics,virtual reality,quantum computing,cloud computing,5G network technologies"
BlazePose: On-device Real-time Body Pose tracking,"We present BlazePose, a lightweight convolutional neural network architecture
for human pose estimation that is tailored for real-time inference on mobile
devices. During inference, the network produces 33 body keypoints for a single
person and runs at over 30 frames per second on a Pixel 2 phone. This makes it
particularly suited to real-time use cases like fitness tracking and sign
language recognition. Our main contributions include a novel body pose tracking
solution and a lightweight body pose estimation neural network that uses both
heatmaps and regression to keypoint coordinates.","['Valentin Bazarevsky', 'Ivan Grishchenko', 'Karthik Raveendran', 'Tyler Zhu', 'Fan Zhang', 'Matthias Grundmann']",2020-06-17T23:52:46Z,http://arxiv.org/abs/2006.10204v1,['cs.CV'],"BlazePose,convolutional neural network,human pose estimation,real-time inference,mobile devices,body keypoints,frames per second,fitness tracking,sign language recognition,regression"
A flexible spiraling-metasurface as a versatile haptic interface,"Haptic feedback is the most significant sensory interface following visual
cues. Developing thin, flexible surfaces that function as haptic interfaces is
important for augmenting virtual reality, wearable devices, robotics and
prostheses. For example, adding a haptic feedback interface to prosthesis could
improve their acceptance among amputees. State of the art programmable
interfaces targeting the skin feel-of-touch through mechano-receptors are
limited by inadequate sensory feedback, cumbersome mechanisms or narrow
frequency of operation. Here, we present a flexible metasurface as a generic
haptic interface capable of producing complex tactile patterns on the human
skin at wide range of frequencies. The metasurface is composed of multiple
""pixels"" that can locally amplify both input displacements and forces. Each of
these pixels encodes various deformation patterns capable of producing
different sensations on contact. The metasurface can transform a harmonic
signal containing multiple frequencies into a complex preprogrammed tactile
pattern. Our findings, corroborated by user studies conducted on human
candidates, can open new avenues for wearable and robotic interfaces.","['Osama R. Bilal', 'Vincenzo Costanza', 'Ali Israr', 'Antonio Palermo', 'Paolo Celli', 'Frances Lau', 'Chiara Daraio']",2020-06-18T17:48:16Z,http://arxiv.org/abs/2006.10717v1,"['physics.app-ph', 'cond-mat.soft', 'cs.RO']","flexible,spiraling-metasurface,haptic interface,virtual reality,wearable devices,robotics,prostheses,metasurface,tactile patterns"
"Tactile Perception of Objects by the User's Palm for the Development of
  Multi-contact Wearable Tactile Displays","The user's palm plays an important role in object detection and manipulation.
The design of a robust multi-contact tactile display must consider the
sensation and perception of of the stimulated area aiming to deliver the right
stimuli at the correct location. To the best of our knowledge, there is no
study to obtain the human palm data for this purpose. The objective of this
work is to introduce the method to investigate the user's palm sensations
during the interaction with objects. An array of fifteen Force Sensitive
Resistors (FSRs) was located at the user's palm to get the area of interaction,
and the normal force delivered to four different convex surfaces. Experimental
results showed the active areas at the palm during the interaction with each of
the surfaces at different forces. The obtained results can be applied in the
development of multi-contact wearable tactile and haptic displays for the palm,
and in training a machine-learning algorithm to predict stimuli aiming to
achieve a highly immersive experience in Virtual Reality.","['Miguel Altamirano Cabrera', 'Juan Heredia', 'Dzmitry Tsetserukou']",2020-06-22T15:47:01Z,http://arxiv.org/abs/2006.12349v1,['cs.HC'],"Tactile perception,Objects,User's palm,Multi-contact,Wearable tactile displays,Force Sensitive Resistors (FSRs),Interaction,Convex surfaces,Experimental results,Machine-learning algorithm"
"Scoring and Assessment in Medical VR Training Simulators with Dynamic
  Time Series Classification","This research proposes and evaluates scoring and assessment methods for
Virtual Reality (VR) training simulators. VR simulators capture detailed
n-dimensional human motion data which is useful for performance analysis.
Custom made medical haptic VR training simulators were developed and used to
record data from 271 trainees of multiple clinical experience levels. DTW
Multivariate Prototyping (DTW-MP) is proposed. VR data was classified as
Novice, Intermediate or Expert. Accuracy of algorithms applied for time-series
classification were: dynamic time warping 1-nearest neighbor (DTW-1NN) 60%,
nearest centroid SoftDTW classification 77.5%, Deep Learning: ResNet 85%, FCN
75%, CNN 72.5% and MCDCNN 28.5%. Expert VR data recordings can be used for
guidance of novices. Assessment feedback can help trainees to improve skills
and consistency. Motion analysis can identify different techniques used by
individuals. Mistakes can be detected dynamically in real-time, raising alarms
to prevent injuries.","['Neil Vaughan', 'Bogdan Gabrys']",2020-06-11T15:46:25Z,http://arxiv.org/abs/2006.12366v1,"['eess.SP', 'cs.AI', 'I.2.0']","Scoring,Assessment,Medical VR,Training Simulators,Dynamic Time Series Classification,Haptic,DTW Multivariate Prototyping,Deep Learning,ResNet,FCN"
"TeslaMirror: Multistimulus Encounter-Type Haptic Display for Shape and
  Texture Rendering in VR","This paper proposes a novel concept of a hybrid tactile display with
multistimulus feedback, allowing the real-time experience of the position,
shape, and texture of the virtual object. The key technology of the TeslaMirror
is that we can deliver the sensation of object parameters (pressure, vibration,
and electrotactile feedback) without any wearable haptic devices. We developed
the full digital twin of the 6 DOF UR robot in the virtual reality (VR)
environment, allowing the adaptive surface simulation and control of the hybrid
display in real-time. The preliminary user study was conducted to evaluate the
ability of TeslaMirror to reproduce shape sensations with the under-actuated
end-effector. The results revealed that potentially this approach can be used
in the virtual systems for rendering versatile VR shapes with high fidelity
haptic experience.","['Aleksey Fedoseev', 'Akerke Tleugazy', 'Luiza Labazanova', 'Dzmitry Tsetserukou']",2020-06-23T14:33:17Z,http://arxiv.org/abs/2006.13660v3,"['cs.HC', 'cs.RO']","haptic display,multistimulus feedback,shape rendering,texture rendering,virtual reality,TeslaMirror,electrotactile feedback,wearable haptic devices,digital twin,UR robot"
"Tilt Map: Interactive Transitions Between Choropleth Map, Prism Map and
  Bar Chart in Immersive Environments","We introduce Tilt Map, a novel interaction technique for intuitively
transitioning between 2D and 3D map visualisations in immersive environments.
Our focus is visualising data associated with areal features on maps, for
example, population density by state. Tilt Map transitions from 2D choropleth
maps to 3D prism maps to 2D bar charts to overcome the limitations of each. Our
paper includes two user studies. The first study compares subjects' task
performance interpreting population density data using 2D choropleth maps and
3D prism maps in virtual reality (VR). We observed greater task accuracy with
prism maps, but faster response times with choropleth maps. The complementarity
of these views inspired our hybrid Tilt Map design. Our second study compares
Tilt Map to: a side-by-side arrangement of the various views; and interactive
toggling between views. The results indicate benefits for Tilt Map in user
preference; and accuracy (versus side-by-side) and time (versus toggle).","['Yalong Yang', 'Tim Dwyer', 'Kim Marriott', 'Bernhard Jenny', 'Sarah Goodwin']",2020-06-25T00:52:57Z,http://arxiv.org/abs/2006.14120v1,"['cs.HC', 'cs.GR']","Tilt Map,Interactive Transitions,Choropleth Map,Prism Map,Bar Chart,Immersive Environments,Virtual Reality,User Studies,Population Density,Areal Features."
Democratizing the Edge: A Pervasive Edge Computing Framework,"The needs of emerging applications, such as augmented and virtual reality,
federated machine learning, and autonomous driving, have motivated edge
computing--the push of computation capabilities to the edge. Various edge
computing architectures have emerged, including multi-access edge computing and
edge-cloud, all with the premise of reducing communication latency and
augmenting privacy. However, these architectures rely on static and
pre-deployed infrastructure, falling short in harnessing the abundant resources
at the network's edge. In this paper, we discuss the design of Pervasive Edge
Computing (PEC)--a democratized edge computing framework, which enables
end-user devices (e.g., smartphones, IoT devices, and vehicles) to dynamically
participate in a large-scale computing ecosystem. Our vision of the
democratized edge involves the real-time composition of services using
available edge resources like data, software, and compute-hardware from
multiple stakeholders. We discuss how the novel Named-Data Networking
architecture can facilitate service deployment, discovery, invocation, and
migration. We also discuss the economic models critical to the adoption of PEC
and the outstanding challenges for its full realization.","['Reza Tourani', 'Srikathyayani Srikanteswara', 'Satyajayant Misra', 'Richard Chow', 'Lily Yang', 'Xiruo Liu', 'Yi Zhang']",2020-07-01T17:46:08Z,http://arxiv.org/abs/2007.00641v1,['cs.NI'],"edge computing,pervasive edge computing framework,augmented reality,virtual reality,federated machine learning,autonomous driving,multi-access edge computing,edge-cloud,Named-Data Networking architecture"
An Efficient Data Imputation Technique for Human Activity Recognition,"The tremendous applications of human activity recognition are surging its
span from health monitoring systems to virtual reality applications. Thus, the
automatic recognition of daily life activities has become significant for
numerous applications. In recent years, many datasets have been proposed to
train the machine learning models for efficient monitoring and recognition of
human daily living activities. However, the performance of machine learning
models in activity recognition is crucially affected when there are incomplete
activities in a dataset, i.e., having missing samples in dataset captures.
Therefore, in this work, we propose a methodology for extrapolating the missing
samples of a dataset to better recognize the human daily living activities. The
proposed method efficiently pre-processes the data captures and utilizes the
k-Nearest Neighbors (KNN) imputation technique to extrapolate the missing
samples in dataset captures. The proposed methodology elegantly extrapolated a
similar pattern of activities as they were in the real dataset.","['Ivan Miguel Pires', 'Faisal Hussain', 'Nuno M. Garcia', 'Eftim Zdravevski']",2020-07-08T22:05:38Z,http://arxiv.org/abs/2007.04456v1,"['eess.SP', 'cs.AI', 'cs.CY', 'cs.LG']","data imputation,human activity recognition,machine learning models,missing samples,dataset,k-Nearest Neighbors (KNN),extrapolation,pre-processing,virtual reality applications"
Robust Vision Using Retro Reflective Markers for Remote Handling in ITER,"ITER's working environment is characterized by extreme conditions, that deem
maintenance and inspection tasks to be carried out through remote handling. 3D
Node is a hardware/software module that extracts critical information from the
remote environment during fine alignment tasks using an eye-in-hand camera
system and updates the models behind the virtual reality-based remote handling
platform. In this work we develop a retro-reflective marker-based version of 3D
Node that estimates the pose of a planar target, the knuckle of the cassette
locking system, using the markers attached to its surface. We demonstrate a
pin-tool insertion task using these methods. Results show that our approach
works reliably with a single low-resolution camera and outperforms the
previously researched stereo depth estimation based approaches. We conclude
that retro-reflective marker-based tracking has the potential to be a key
enabler for remote handling operations in ITER.","['Laura Goncalves Ribeiro', 'Olli J. Suominen', 'Sari Peltonen', 'Emilio Ruiz Morales', 'Atanas Gotchev']",2020-07-24T13:16:02Z,http://arxiv.org/abs/2007.12514v3,['eess.IV'],"remote handling,retro-reflective markers,3D Node,pose estimation,virtual reality,camera system,pin-tool insertion,stereo depth estimation,tracking,ITER"
"Subjective Quality Database and Objective Study of Compressed Point
  Clouds With 6DoF Head-Mounted Display","In this paper, we focus on subjective and objective Point Cloud Quality
Assessment (PCQA) in an immersive environment and study the effect of geometry
and texture attributes in compression distortion. Using a Head-Mounted Display
(HMD) with six degrees of freedom, we establish a subjective PCQA database,
named SIAT Point Cloud Quality Database (SIAT-PCQD). Our database consists of
340 distorted point clouds compressed by the MPEG point cloud encoder with the
combination of 20 sequences and 17 pairs of geometry and texture quantization
parameters. The impact of distorted geometry and texture attributes is further
discussed in this paper. Then, we propose two projection-based objective
quality evaluation methods, i.e., a weighted view projection based model and a
patch projection based model. Our subjective database and findings can be used
in point cloud processing, transmission, and coding, especially for virtual
reality applications. The subjective dataset has been released in the public
repository.","['Xinju Wu', 'Yun Zhang', 'Chunling Fan', 'Junhui Hou', 'Sam Kwong']",2020-08-06T07:54:29Z,http://arxiv.org/abs/2008.02501v2,['eess.IV'],"Point Cloud Quality Assessment,Compression Distortion,Head-Mounted Display,Six Degrees of Freedom,Point Cloud Encoder,Geometry Attributes,Texture Attributes,Objective Quality Evaluation,Virtual Reality Applications,Public Repository."
"MatryODShka: Real-time 6DoF Video View Synthesis using Multi-Sphere
  Images","We introduce a method to convert stereo 360{\deg} (omnidirectional stereo)
imagery into a layered, multi-sphere image representation for six
degree-of-freedom (6DoF) rendering. Stereo 360{\deg} imagery can be captured
from multi-camera systems for virtual reality (VR), but lacks motion parallax
and correct-in-all-directions disparity cues. Together, these can quickly lead
to VR sickness when viewing content. One solution is to try and generate a
format suitable for 6DoF rendering, such as by estimating depth. However, this
raises questions as to how to handle disoccluded regions in dynamic scenes. Our
approach is to simultaneously learn depth and disocclusions via a multi-sphere
image representation, which can be rendered with correct 6DoF disparity and
motion parallax in VR. This significantly improves comfort for the viewer, and
can be inferred and rendered in real time on modern GPU hardware. Together,
these move towards making VR video a more comfortable immersive medium.","['Benjamin Attal', 'Selena Ling', 'Aaron Gokaslan', 'Christian Richardt', 'James Tompkin']",2020-08-14T18:33:05Z,http://arxiv.org/abs/2008.06534v1,['cs.CV'],"6DoF,Video View Synthesis,Multi-Sphere Images,Stereo 360-degree,Omnidirectional Stereo,Virtual Reality,Depth Estimation,Disocclusions,Motion Parallax,GPU Hardware"
"RoomShift: Room-scale Dynamic Haptics for VR with Furniture-moving Swarm
  Robots","RoomShift is a room-scale dynamic haptic environment for virtual reality,
using a small swarm of robots that can move furniture. RoomShift consists of
nine shape-changing robots: Roombas with mechanical scissor lifts. These robots
drive beneath a piece of furniture to lift, move and place it. By augmenting
virtual scenes with physical objects, users can sit on, lean against, place and
otherwise interact with furniture with their whole body; just as in the real
world. When the virtual scene changes or users navigate within it, the swarm of
robots dynamically reconfigures the physical environment to match the virtual
content. We describe the hardware and software implementation, applications in
virtual tours and architectural design and interaction techniques.","['Ryo Suzuki', 'Hooman Hedayati', 'Clement Zheng', 'James Bohn', 'Daniel Szafir', 'Ellen Yi-Luen Do', 'Mark D. Gross', 'Daniel Leithinger']",2020-08-19T22:47:50Z,http://arxiv.org/abs/2008.08695v1,"['cs.RO', 'cs.HC']","room-scale dynamic haptics,virtual reality,swarm robots,furniture-moving,shape-changing robots,Roombas,mechanical scissor lifts,physical objects,interaction techniques"
"Multi-Dimension Fusion Network for Light Field Spatial Super-Resolution
  using Dynamic Filters","Light field cameras have been proved to be powerful tools for 3D
reconstruction and virtual reality applications. However, the limited
resolution of light field images brings a lot of difficulties for further
information display and extraction. In this paper, we introduce a novel
learning-based framework to improve the spatial resolution of light fields.
First, features from different dimensions are parallelly extracted and fused
together in our multi-dimension fusion architecture. These features are then
used to generate dynamic filters, which extract subpixel information from
micro-lens images and also implicitly consider the disparity information.
Finally, more high-frequency details learned in the residual branch are added
to the upsampled images and the final super-resolved light fields are obtained.
Experimental results show that the proposed method uses fewer parameters but
achieves better performances than other state-of-the-art methods in various
kinds of datasets. Our reconstructed images also show sharp details and
distinct lines in both sub-aperture images and epipolar plane images.","['Qingyan Sun', 'Shuo Zhang', 'Song Chang', 'Lixi Zhu', 'Youfang Lin']",2020-08-26T09:05:07Z,http://arxiv.org/abs/2008.11449v1,"['eess.IV', 'cs.CV']","Light field cameras,3D reconstruction,virtual reality,spatial super-resolution,dynamic filters,multi-dimension fusion network,micro-lens images,disparity information,high-frequency details,residual branch"
Light Field Compression by Residual CNN Assisted JPEG,"Light field (LF) imaging has gained significant attention due to its recent
success in 3-dimensional (3D) displaying and rendering as well as augmented and
virtual reality usage. Nonetheless, because of the two extra dimensions, LFs
are much larger than conventional images. We develop a JPEG-assisted
learning-based technique to reconstruct an LF from a JPEG bitstream with a bit
per pixel ratio of 0.0047 on average. For compression, we keep the LF's center
view and use JPEG compression with 50% quality. Our reconstruction pipeline
consists of a small JPEG enhancement network (JPEG-Hance), a depth estimation
network (Depth-Net), followed by view synthesizing by warping the enhanced
center view. Our pipeline is significantly faster than using video compression
on pseudo-sequences extracted from an LF, both in compression and
decompression, while maintaining effective performance. We show that with a 1%
compression time cost and 18x speedup for decompression, our methods
reconstructed LFs have better structural similarity index metric (SSIM) and
comparable peak signal-to-noise ratio (PSNR) compared to the state-of-the-art
video compression techniques used to compress LFs.","['Eisa Hedayati', 'Timothy C. Havens', 'Jeremy P. Bos']",2020-09-30T19:09:37Z,http://arxiv.org/abs/2010.00062v2,"['eess.IV', 'cs.LG']","Light field,Compression,Residual CNN,JPEG,3-dimensional,Rendering,Augmented reality,Virtual reality,SSIM,PSNR"
"Learning Acoustic Scattering Fields for Dynamic Interactive Sound
  Propagation","We present a novel hybrid sound propagation algorithm for interactive
applications. Our approach is designed for dynamic scenes and uses a neural
network-based learned scattered field representation along with ray tracing to
generate specular, diffuse, diffraction, and occlusion effects efficiently. We
use geometric deep learning to approximate the acoustic scattering field using
spherical harmonics. We use a large 3D dataset for training, and compare its
accuracy with the ground truth generated using an accurate wave-based solver.
The additional overhead of computing the learned scattered field at runtime is
small and we demonstrate its interactive performance by generating plausible
sound effects in dynamic scenes with diffraction and occlusion effects. We
demonstrate the perceptual benefits of our approach based on an audio-visual
user study.","['Zhenyu Tang', 'Hsien-Yu Meng', 'Dinesh Manocha']",2020-10-10T01:43:50Z,http://arxiv.org/abs/2010.04865v2,"['cs.SD', 'cs.GR', 'eess.AS']","acoustic scattering fields,dynamic scenes,neural network-based,ray tracing,specular effects,diffuse effects,diffraction effects,occlusion effects,geometric deep learning,spherical harmonics"
Omni-Directional Image Generation from Single Snapshot Image,"An omni-directional image (ODI) is the image that has a field of view
covering the entire sphere around the camera. The ODIs have begun to be used in
a wide range of fields such as virtual reality (VR), robotics, and social
network services. Although the contents using ODI have increased, the available
images and videos are still limited, compared with widespread snapshot images.
A large number of ODIs are desired not only for the VR contents, but also for
training deep learning models for ODI. For these purposes, a novel computer
vision task to generate ODI from a single snapshot image is proposed in this
paper. To tackle this problem, the conditional generative adversarial network
was applied in combination with class-conditioned convolution layers. With this
novel task, VR images and videos will be easily created even with a smartphone
camera.","['Keisuke Okubo', 'Takao Yamanaka']",2020-10-12T11:12:04Z,http://arxiv.org/abs/2010.05600v1,['cs.CV'],"omni-directional image,single snapshot image,virtual reality,robotics,social network services,deep learning models,generative adversarial network,convolution layers,smartphone camera"
DynaSLAM II: Tightly-Coupled Multi-Object Tracking and SLAM,"The assumption of scene rigidity is common in visual SLAM algorithms.
However, it limits their applicability in populated real-world environments.
Furthermore, most scenarios including autonomous driving, multi-robot
collaboration and augmented/virtual reality, require explicit motion
information of the surroundings to help with decision making and scene
understanding. We present in this paper DynaSLAM II, a visual SLAM system for
stereo and RGB-D configurations that tightly integrates the multi-object
tracking capability.
  DynaSLAM II makes use of instance semantic segmentation and of ORB features
to track dynamic objects. The structure of the static scene and of the dynamic
objects is optimized jointly with the trajectories of both the camera and the
moving agents within a novel bundle adjustment proposal. The 3D bounding boxes
of the objects are also estimated and loosely optimized within a fixed temporal
window. We demonstrate that tracking dynamic objects does not only provide rich
clues for scene understanding but is also beneficial for camera tracking.
  The project code will be released upon acceptance.","['Berta Bescos', 'Carlos Campos', 'Juan D. Tardós', 'José Neira']",2020-10-15T15:25:30Z,http://arxiv.org/abs/2010.07820v1,"['cs.RO', 'cs.CV']","SLAM,Multi-Object Tracking,Instance Semantic Segmentation,ORB Features,Bundle Adjustment,3D Bounding Boxes,Camera Tracking,Autonomous Driving"
"Connections between Relational Event Model and Inverse Reinforcement
  Learning for Characterizing Group Interaction Sequences","In this paper we explore previously unidentified connections between
relational event model (REM) from the field of network science and inverse
reinforcement learning (IRL) from the field of machine learning with respect to
their ability to characterize sequences of directed social interaction events
in group settings. REM is a conventional approach to tackle such a problem
whereas the application of IRL is a largely unbeaten path. We begin by
examining the mathematical components of both REM and IRL and find
straightforward analogies between the two methods as well as unique
characteristics of the IRL approach. We demonstrate the special utility of IRL
in characterizing group social interactions with an empirical experiment, in
which we use IRL to infer individual behavioral preferences based on a sequence
of directed communication events from a group of virtual-reality game players
interacting and cooperating to accomplish a shared goal. Our comparison and
experiment introduce fresh perspectives for social behavior analytics and help
inspire new research opportunities at the nexus of social network analysis and
machine learning.",['Congyu Wu'],2020-10-19T19:40:29Z,http://arxiv.org/abs/2010.09810v1,"['cs.LG', 'cs.SI']","Relational Event Model,Inverse Reinforcement Learning,Group Interaction Sequences,Network Science,Machine Learning,Social Interaction Events,Behavioral Preferences,Virtual-Reality Game Players,Social Behavior Analytics"
"XR-Ed Framework: Designing Instruction-driven andLearner-centered
  Extended Reality Systems for Education","Recently, the HCI community has seen an increased interest in applying
Virtual Reality (VR), AugmentedReality (AR) and Mixed Reality (MR) into
educational settings. Despite many literature reviews, there stilllacks a clear
framework that reveals the different design dimensions in educational Extended
Reality (XR)systems. Addressing this gap, we synthesize a broad range of
educational XR to propose the XR-Ed framework,which reveals design space in six
dimensions (Physical Accessibility, Scenario, Social Interactivity,
Agency,Virtuality Degree, Assessment). Within each dimension, we contextualize
the framework using existing designcases. Based on the XR-Ed Design framework,
we incorporated instructional design approaches to proposeXR-Ins, an
instruction-oriented, step-by-step guideline in educational XR instruction
design. Jointly, they aimto support practitioners by revealing implicit design
choices, offering design inspirations as well as guide themto design
instructional activities for XR technologies in a more instruction-oriented and
learner-centered way.","['Kexin Yang', 'Xiaofei Zhou', 'Iulian Radu']",2020-10-24T03:18:05Z,http://arxiv.org/abs/2010.13779v1,['cs.HC'],"Extended Reality,XR-Ed framework,Virtual Reality,Augmented Reality,Mixed Reality,Instruction-driven,Learner-centered,educational settings,design dimensions,XR-Ins,instructional design"
A Visuospatial Dataset for Naturalistic Verb Learning,"We introduce a new dataset for training and evaluating grounded language
models. Our data is collected within a virtual reality environment and is
designed to emulate the quality of language data to which a pre-verbal child is
likely to have access: That is, naturalistic, spontaneous speech paired with
richly grounded visuospatial context. We use the collected data to compare
several distributional semantics models for verb learning. We evaluate neural
models based on 2D (pixel) features as well as feature-engineered models based
on 3D (symbolic, spatial) features, and show that neither modeling approach
achieves satisfactory performance. Our results are consistent with evidence
from child language acquisition that emphasizes the difficulty of learning
verbs from naive distributional data. We discuss avenues for future work on
cognitively-inspired grounded language learning, and release our corpus with
the intent of facilitating research on the topic.","['Dylan Ebert', 'Ellie Pavlick']",2020-10-28T20:47:13Z,http://arxiv.org/abs/2010.15225v1,"['cs.CL', 'I.2.7']","visuospatial dataset,verb learning,grounded language models,virtual reality environment,distributional semantics models,neural models,feature-engineered models,child language acquisition,language learning,corpus"
Correspondence Matrices are Underrated,"Point-cloud registration (PCR) is an important task in various applications
such as robotic manipulation, augmented and virtual reality, SLAM, etc. PCR is
an optimization problem involving minimization over two different types of
interdependent variables: transformation parameters and point-to-point
correspondences. Recent developments in deep-learning have produced
computationally fast approaches for PCR. The loss functions that are optimized
in these networks are based on the error in the transformation parameters. We
hypothesize that these methods would perform significantly better if they
calculated their loss function using correspondence error instead of only using
error in transformation parameters. We define correspondence error as a metric
based on incorrectly matched point pairs. We provide a fundamental explanation
for why this is the case and test our hypothesis by modifying existing methods
to use correspondence-based loss instead of transformation-based loss. These
experiments show that the modified networks converge faster and register more
accurately even at larger misalignment when compared to the original networks.","['Tejas Zodage', 'Rahul Chakwate', 'Vinit Sarode', 'Rangaprasad Arun Srivatsan', 'Howie Choset']",2020-10-30T05:31:50Z,http://arxiv.org/abs/2010.16085v1,['cs.CV'],"point-cloud registration,robotic manipulation,augmented reality,virtual reality,SLAM,optimization problem,transformation parameters,correspondences,deep-learning,loss function"
"Exploring Severe Occlusion: Multi-Person 3D Pose Estimation with Gated
  Convolution","3D human pose estimation (HPE) is crucial in many fields, such as human
behavior analysis, augmented reality/virtual reality (AR/VR) applications, and
self-driving industry. Videos that contain multiple potentially occluded people
captured from freely moving monocular cameras are very common in real-world
scenarios, while 3D HPE for such scenarios is quite challenging, partially
because there is a lack of such data with accurate 3D ground truth labels in
existing datasets. In this paper, we propose a temporal regression network with
a gated convolution module to transform 2D joints to 3D and recover the missing
occluded joints in the meantime. A simple yet effective localization approach
is further conducted to transform the normalized pose to the global trajectory.
To verify the effectiveness of our approach, we also collect a new moving
camera multi-human (MMHuman) dataset that includes multiple people with heavy
occlusion captured by moving cameras. The 3D ground truth joints are provided
by accurate motion capture (MoCap) system. From the experiments on
static-camera based Human3.6M data and our own collected moving-camera based
data, we show that our proposed method outperforms most state-of-the-art
2D-to-3D pose estimation methods, especially for the scenarios with heavy
occlusions.","['Renshu Gu', 'Gaoang Wang', 'Jenq-Neng Hwang']",2020-10-31T04:35:24Z,http://arxiv.org/abs/2011.00184v1,['cs.CV'],"3D human pose estimation,Gated Convolution,occlusion,multi-person,2D joints,3D ground truth labels,temporal regression network,localization,moving camera dataset,motion capture"
"HAVEN: A Unity-based Virtual Robot Environment to Showcase HRI-based
  Augmented Reality","Due to the COVID-19 pandemic, conducting Human-Robot Interaction (HRI)
studies in person is not permissible due to social distancing practices to
limit the spread of the virus. Therefore, a virtual reality (VR) simulation
with a virtual robot may offer an alternative to real-life HRI studies. Like a
real intelligent robot, a virtual robot can utilize the same advanced
algorithms to behave autonomously. This paper introduces HAVEN (HRI-based
Augmentation in a Virtual robot Environment using uNity), a VR simulation that
enables users to interact with a virtual robot. The goal of this system design
is to enable researchers to conduct HRI Augmented Reality studies using a
virtual robot without being in a real environment. This framework also
introduces two common HRI experiment designs: a hallway passing scenario and
human-robot team object retrieval scenario. Both reflect HAVEN's potential as a
tool for future AR-based HRI studies.","['Andre Cleaver', 'Darren Tang', 'Victoria Chen', 'Jivko Sinapov']",2020-11-06T16:34:17Z,http://arxiv.org/abs/2011.03464v1,['cs.RO'],"Human-Robot Interaction,Virtual Reality,Virtual Robot,Unity,Augmented Reality,Algorithms,Simulation,Framework,Experiment Designs,HRI Studies"
"Sound Synthesis, Propagation, and Rendering: A Survey","Sound, as a crucial sensory channel, plays a vital role in improving the
reality and immersiveness of a virtual environment, following only vision in
importance. Sound can provide important clues such as sound directionality and
spatial size. This paper gives a broad overview of research works on sound
simulation in virtual reality, games, multimedia, computer-aided design. We
first survey various sound synthesis methods, including harmonic synthesis,
texture synthesis, spectral analysis, and physics-based synthesis. Then, we
summarize popular sound propagation techniques, namely wave-based methods,
geometric-based methods, and hybrid methods. Next, the sound rendering methods
are reviewed. We further demonstrate the latest deep learning based sound
simulation approaches. Finally, we point to some future directions of this
field. To the best of our knowledge, this is the first attempt to provide a
comprehensive summary of sound research in the field of computer graphics.","['Shiguang Liu', 'Dinesh Manocha']",2020-11-11T04:08:38Z,http://arxiv.org/abs/2011.05538v5,"['cs.SD', 'cs.GR']","Sound synthesis,sound propagation,sound rendering,virtual reality,games,multimedia,computer-aided design,harmonic synthesis,spectral analysis,deep learning"
"Combining Gesture and Voice Control for Mid-Air Manipulation of CAD
  Models in VR Environments","Modeling 3D objects in domains like Computer Aided Design (CAD) is
time-consuming and comes with a steep learning curve needed to master the
design process as well as tool complexities. In order to simplify the modeling
process, we designed and implemented a prototypical system that leverages the
strengths of Virtual Reality (VR) hand gesture recognition in combination with
the expressiveness of a voice-based interface for the task of 3D modeling.
Furthermore, we use the Constructive Solid Geometry (CSG) tree representation
for 3D models within the VR environment to let the user manipulate objects from
the ground up, giving an intuitive understanding of how the underlying basic
shapes connect. The system uses standard mid-air 3D object manipulation
techniques and adds a set of voice commands to help mitigate the deficiencies
of current hand gesture recognition techniques. A user study was conducted to
evaluate the proposed prototype. The combination of our hybrid input paradigm
shows to be a promising step towards easier to use CAD modeling.","['Markus Friedrich', 'Stefan Langer', 'Fabian Frey']",2020-11-18T07:26:29Z,http://arxiv.org/abs/2011.09138v1,"['cs.HC', 'cs.CG']","Gesture control,Voice control,Mid-air manipulation,CAD models,Virtual Reality,Constructive Solid Geometry (CSG),3D object manipulation,User study,Hybrid input paradigm"
UNOC: Understanding Occlusion for Embodied Presence in Virtual Reality,"Tracking body and hand motions in the 3D space is essential for social and
self-presence in augmented and virtual environments. Unlike the popular 3D pose
estimation setting, the problem is often formulated as inside-out tracking
based on embodied perception (e.g., egocentric cameras, handheld sensors). In
this paper, we propose a new data-driven framework for inside-out body
tracking, targeting challenges of omnipresent occlusions in optimization-based
methods (e.g., inverse kinematics solvers). We first collect a large-scale
motion capture dataset with both body and finger motions using optical markers
and inertial sensors. This dataset focuses on social scenarios and captures
ground truth poses under self-occlusions and body-hand interactions. We then
simulate the occlusion patterns in head-mounted camera views on the captured
ground truth using a ray casting algorithm and learn a deep neural network to
infer the occluded body parts. In the experiments, we show that our method is
able to generate high-fidelity embodied poses by applying the proposed method
on the task of real-time inside-out body tracking, finger motion synthesis, and
3-point inverse kinematics.","['Mathias Parger', 'Chengcheng Tang', 'Yuanlu Xu', 'Christopher Twigg', 'Lingling Tao', 'Yijing Li', 'Robert Wang', 'Markus Steinberger']",2020-11-12T09:31:09Z,http://arxiv.org/abs/2012.03680v1,"['cs.CV', 'cs.LG']","3D pose estimation,inside-out tracking,embodied perception,occlusions,optimization-based methods,motion capture dataset,ground truth poses,ray casting algorithm,deep neural network,real-time tracking"
"Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive
  Sense of Touch","Noninvasive brain-computer interface (BCI) is widely used to recognize users'
intentions. Especially, BCI related to tactile and sensation decoding could
provide various effects on many industrial fields such as manufacturing
advanced touch displays, controlling robotic devices, and more immersive
virtual reality or augmented reality. In this paper, we introduce haptic and
sensory perception-based BCI systems called neurohaptics. It is a preliminary
study for a variety of scenarios using actual touch and touch imagery
paradigms. We designed a novel experimental environment and a device that could
acquire brain signals under touching designated materials to generate natural
touch and texture sensations. Through the experiment, we collected the
electroencephalogram (EEG) signals with respect to four different texture
objects. Seven subjects were recruited for the experiment and evaluated
classification performances using machine learning and deep learning
approaches. Hence, we could confirm the feasibility of decoding actual touch
and touch imagery on EEG signals to develop practical neurohaptics.","['Jeong-Hyun Cho', 'Ji-Hoon Jeong', 'Myoung-Ki Kim', 'Seong-Whan Lee']",2020-12-12T08:08:47Z,http://arxiv.org/abs/2012.06753v2,['cs.HC'],"Brain-computer interface,Neurohaptics,Sense of touch,Tactile sensation,EEG signals,Machine learning,Deep learning,Haptic perception,Sensory perception"
"iDaVIE-v: immersive Data Visualisation Interactive Explorer for
  volumetric rendering","We present the beta release of iDaVIE-v, a new Virtual Reality software for
data cube exploration. The beta release of iDaVIE-v (immersive Data
Visualisation Interactive Explorer for volumetric rendering) is planned for
release in early 2021. iDaVIE-v has been developed through the Unity game
engine using the SteamVR plugin and is compatible with all commercial headsets.
It allows the visualization, exploration and interaction of data for scientific
analysis. Originally developed to serve the HI Radio Astronomy community for HI
source identification, the software has now completed the alpha testing phase
and is already showing capabilities that will serve the broader astronomical
community and more. iDaVIE-v has been developed at the IDIA Visualisation Lab
(IVL) based at the University of Cape Town in collaboration with the Italian
National Institute for Astrophysics (INAF) in Catania.","['Lucia Marchetti', 'Thomas H. Jarrett', 'Angus Comrie', 'Alexander K. Sivitilli', 'Fabio Vitello', 'Ugo Becciani', 'A. R. Taylor']",2020-12-21T18:32:35Z,http://arxiv.org/abs/2012.11553v1,"['astro-ph.IM', 'astro-ph.GA']","Virtual Reality,software,data cube,exploration,immersive,volumetric rendering,Unity game engine,SteamVR plugin,scientific analysis"
The Challenges in Modeling Human Performance in 3D Space with Fitts' Law,"With the rapid growth in virtual reality technologies, object interaction is
becoming increasingly more immersive, elucidating human perception and leading
to promising directions towards evaluating human performance under different
settings. This spike in technological growth exponentially increased the need
for a human performance metric in 3D space. Fitts' law is perhaps the most
widely used human prediction model in HCI history attempting to capture human
movement in lower dimensions. Despite the collective effort towards deriving an
advanced extension of a 3D human performance model based on Fitts' law, a
standardized metric is still missing. Moreover, most of the extensions to date
assume or limit their findings to certain settings, effectively disregarding
important variables that are fundamental to 3D object interaction. In this
review, we investigate and analyze the most prominent extensions of Fitts' law
and compare their characteristics pinpointing to potentially important aspects
for deriving a higher-dimensional performance model. Lastly, we mention the
complexities, frontiers as well as potential challenges that may lay ahead.","['Eleftherios Triantafyllidis', 'Zhibin Li']",2021-01-01T16:03:45Z,http://arxiv.org/abs/2101.00260v1,"['cs.HC', 'cs.RO']","modeling,human performance,3D space,Fitts' Law,virtual reality,object interaction,human perception,HCI,human prediction model"
Duration-Squeezing-Aware Communication and Computing for Proactive VR,"Proactive tile-based virtual reality video streaming computes and delivers
the predicted tiles to be requested before playback. All existing works
overlook the important fact that computing and communication (CC) tasks for a
segment may squeeze the time for the tasks for the next segment, which will
cause less and less available time for the latter segments. In this paper, we
jointly optimize the durations for CC tasks to maximize the completion rate of
CC tasks under the task duration-squeezing-aware constraint. To ensure the
latter segments remain enough time for the tasks, the CC tasks for a segment
are not allowed to squeeze the time for computing and delivering the subsequent
segment. We find the closed-form optimal solution, from which we find a
minimum-resource-limited, an unconditional and a conditional resource-tradeoff
regions, which are determined by the total time for proactive CC tasks and the
playback duration of a segment. Owing to the duration-squeezing-prohibited
constraints, the increase of the configured resources may not be always useful
for improving the completion rate of CC tasks. Numerical results validate the
impact of the duration-squeezing-prohibited constraints and illustrate the
three regions.","['Xing Wei', 'Chenyang Yang', 'Shengqian Han']",2021-01-03T11:57:04Z,http://arxiv.org/abs/2101.00611v1,['cs.MM'],"Proactive VR,Virtual reality,Video streaming,Computing,Communication,Tasks,Optimization,Completion rate,Resource tradeoff,Constraints."
"Covert Embodied Choice: Decision-Making and the Limits of Privacy Under
  Biometric Surveillance","Algorithms engineered to leverage rich behavioral and biometric data to
predict individual attributes and actions continue to permeate public and
private life. A fundamental risk may emerge from misconceptions about the
sensitivity of such data, as well as the agency of individuals to protect their
privacy when fine-grained (and possibly involuntary) behavior is tracked. In
this work, we examine how individuals adjust their behavior when incentivized
to avoid the algorithmic prediction of their intent. We present results from a
virtual reality task in which gaze, movement, and other physiological signals
are tracked. Participants are asked to decide which card to select without an
algorithmic adversary anticipating their choice. We find that while
participants use a variety of strategies, data collected remains highly
predictive of choice (80% accuracy). Additionally, a significant portion of
participants became more predictable despite efforts to obfuscate, possibly
indicating mistaken priors about the dynamics of algorithmic prediction.","['Jeremy Gordon', 'Max Curran', 'John Chuang', 'Coye Cheshire']",2021-01-04T04:45:22Z,http://arxiv.org/abs/2101.00771v1,"['cs.HC', 'cs.CY']","covert,embodied choice,decision-making,limits of privacy,biometric surveillance,algorithms,behavioral data,biometric data,privacy protection,algorithmic prediction"
Analysing ocular parameters for web browsing and graph visualization,"This paper proposes a set of techniques to investigate eye gaze and fixation
patterns while users interact with electronic user interfaces. In particular,
two case studies are presented - one on analysing eye gaze while interacting
with deceptive materials in web pages and another on analysing graphs in
standard computer monitor and virtual reality displays. We analysed spatial and
temporal distributions of eye gaze fixations and sequence of eye gaze
movements. We used this information to propose new design guidelines to avoid
deceptive materials in web and user-friendly representation of data in 2D
graphs. In 2D graph study we identified that area graph has lowest number of
clusters for user's gaze fixations and lowest average response time. The
results of 2D graph study were implemented in virtual and mixed reality
environment. Along with this, it was ob-served that the duration while
interacting with deceptive materials in web pages is independent of the number
of fixations. Furthermore, web-based data visualization tool for analysing eye
tracking data from single and multiple users was developed.","['Somnath Arjun', 'KamalPreet Singh Saluja', 'Pradipta Biswas']",2021-01-04T06:20:02Z,http://arxiv.org/abs/2101.00794v1,"['cs.HC', 'D.2.2; H.1.2; I.3.6']","ocular parameters,eye gaze,fixation patterns,web browsing,graph visualization,spatial distribution,temporal distribution,design guidelines,2D graphs,eye tracking data"
"All Factors Should Matter! Reference Checklist for Describing Research
  Conditions in Pursuit of Comparable IVR Experiments","A significant problem with immersive virtual reality (IVR) experiments is the
ability to compare research conditions. VR kits and IVR environments are
complex and diverse but researchers from different fields, e.g. ICT,
psychology, or marketing, often neglect to describe them with a level of detail
sufficient to situate their research on the IVR landscape. Careful reporting of
these conditions may increase the applicability of research results and their
impact on the shared body of knowledge on HCI and IVR. Based on literature
review, our experience, practice and a synthesis of key IVR factors, in this
article we present a reference checklist for describing research conditions of
IVR experiments. Including these in publications will contribute to the
comparability of IVR research and help other researchers decide to what extent
reported results are relevant to their own research goals. The compiled
checklist is a ready-to-use reference tool and takes into account key hardware,
software and human factors as well as diverse factors connected to visual,
audio, tactile, and other aspects of interaction.","['Kinga Skorupska', 'Daniel Cnotkowski', 'Julia Paluch', 'RafaŁEMasłyk', 'Anna Jaskulska', 'Monika Kornacka', 'Wiesław KopeāE]",2021-01-04T23:45:52Z,http://arxiv.org/abs/2101.01285v2,"['cs.HC', 'cs.CY', 'cs.MM', 'H.5.2; H.5.m']","immersive virtual reality,research conditions,VR kits,IVR environments,literature review,reference checklist,hardware,software,human factors,interaction"
"Unified Learning Approach for Egocentric Hand Gesture Recognition and
  Fingertip Detection","Head-mounted device-based human-computer interaction often requires
egocentric recognition of hand gestures and fingertips detection. In this
paper, a unified approach of egocentric hand gesture recognition and fingertip
detection is introduced. The proposed algorithm uses a single convolutional
neural network to predict the probabilities of finger class and positions of
fingertips in one forward propagation. Instead of directly regressing the
positions of fingertips from the fully connected layer, the ensemble of the
position of fingertips is regressed from the fully convolutional network.
Subsequently, the ensemble average is taken to regress the final position of
fingertips. Since the whole pipeline uses a single network, it is significantly
fast in computation. Experimental results show that the proposed method
outperforms the existing fingertip detection approaches including the Direct
Regression and the Heatmap-based framework. The effectiveness of the proposed
method is also shown in-the-wild scenario as well as in a use-case of virtual
reality.","['Mohammad Mahmudul Alam', 'Mohammad Tariqul Islam', 'S. M. Mahbubur Rahman']",2021-01-06T14:05:13Z,http://arxiv.org/abs/2101.02047v3,['cs.CV'],"Egocentric hand gesture recognition,Fingertip detection,Convolutional neural network,Finger class,Fully convolutional network,Ensemble,Regression,Computation,Experimental results,Virtual reality"
Virtual Data Cosmos -- Information Design in Modern Astronomy,"Where do cosmic X-rays come from? Every new unidentified X-ray source could
potentially revolutionize our understanding of the universe. The international
collaborative astronomy project EXTraS aimed at automatically classifying new
sources of X-ray emission (e.g., stars or galaxies) in the large observation
database of the X-ray satellite XMM-Newton. Because data archives have reached
dimensions of big data astronomers used different machine-learning (ML) random
forest decision tree algorithms that performed the classification process. In
this bachelor thesis in information design, I was interested in the challenge
to visualize these big data sets and the results of the ML algorithms in an
interactive and intuitive way to facilitate the visual exploration of its
internal structures and relationships. The VIRTUAL DATA COSMOS is an
interactive data visualization tool in virtual reality (VR) for scientists to
explore multidimensional data sets.",['Annika Kreikenbohm'],2021-01-08T14:10:19Z,http://arxiv.org/abs/2101.03019v1,['astro-ph.IM'],"Virtual Data Cosmos,Information Design,Modern Astronomy,X-rays,X-ray source,EXTraS,XMM-Newton,machine-learning,random forest decision tree algorithms,data visualization"
"Streaming VR Games to the Broad Audience: A Comparison of the
  First-Person and Third-Person Perspectives","The spectatorship experience for virtual reality (VR) games differs strongly
from its non-VR precursor. When watching non-VR games on platforms such as
Twitch, spectators just see what the player sees, as the physical interaction
is mostly unimportant for the overall impression. In VR, the immersive
full-body interaction is a crucial part of the player experience. Hence,
content creators, such as streamers, often rely on green screens or similar
solutions to offer a mixed-reality third-person view to disclose their
full-body actions. Our work compares the most popular realizations of the
first-person and the third-person perspective in an online survey (N=217) with
three different VR games. Contrary to the current trend to stream in
third-person, our key result is that most viewers prefer the first-person
version, which they attribute mostly to the better focus on in-game actions and
higher involvement. Based on the study insights, we provide design
recommendations for both perspectives.","['Katharina Emmerich', 'Andrey Krekhov', 'Sebastian Cmentowski', 'Jens Krueger']",2021-01-12T12:46:04Z,http://arxiv.org/abs/2101.04449v1,['cs.HC'],"virtual reality,VR games,spectatorship experience,Twitch,immersive interaction,full-body actions,mixed-reality,online survey,first-person perspective,third-person perspective"
"Proxemics and Social Interactions in an Instrumented Virtual Reality
  Workshop","Virtual environments (VEs) can create collaborative and social spaces, which
are increasingly important in the face of remote work and travel reduction.
Recent advances, such as more open and widely available platforms, create new
possibilities to observe and analyse interaction in VEs. Using a custom
instrumented build of Mozilla Hubs to measure position and orientation, we
conducted an academic workshop to facilitate a range of typical workshop
activities. We analysed social interactions during a keynote, small group
breakouts, and informal networking/hallway conversations. Our mixed-methods
approach combined environment logging, observations, and semi-structured
interviews. The results demonstrate how small and large spaces influenced group
formation, shared attention, and personal space, where smaller rooms
facilitated more cohesive groups while larger rooms made small group formation
challenging but personal space more flexible. Beyond our findings, we show how
the combination of data and insights can fuel collaborative spaces' design and
deliver more effective virtual workshops.","['Julie Williamson', 'Jie Li', 'Vinoba Vinayagamoorthy', 'David A. Shamma', 'Pablo Cesar']",2021-01-13T19:00:57Z,http://arxiv.org/abs/2101.05300v1,"['cs.HC', 'H.5.m; J.4']","virtual environments,social interactions,instrumented build,Mozilla Hubs,workshop activities,mixed-methods approach,environment logging,observations,semi-structured interviews,collaborative spaces"
"S3: Neural Shape, Skeleton, and Skinning Fields for 3D Human Modeling","Constructing and animating humans is an important component for building
virtual worlds in a wide variety of applications such as virtual reality or
robotics testing in simulation. As there are exponentially many variations of
humans with different shape, pose and clothing, it is critical to develop
methods that can automatically reconstruct and animate humans at scale from
real world data. Towards this goal, we represent the pedestrian's shape, pose
and skinning weights as neural implicit functions that are directly learned
from data. This representation enables us to handle a wide variety of different
pedestrian shapes and poses without explicitly fitting a human parametric body
model, allowing us to handle a wider range of human geometries and topologies.
We demonstrate the effectiveness of our approach on various datasets and show
that our reconstructions outperform existing state-of-the-art methods.
Furthermore, our re-animation experiments show that we can generate 3D human
animations at scale from a single RGB image (and/or an optional LiDAR sweep) as
input.","['Ze Yang', 'Shenlong Wang', 'Sivabalan Manivasagam', 'Zeng Huang', 'Wei-Chiu Ma', 'Xinchen Yan', 'Ersin Yumer', 'Raquel Urtasun']",2021-01-17T02:16:56Z,http://arxiv.org/abs/2101.06571v1,['cs.CV'],"Neural Shape,Skeleton,Skinning Fields,3D Human Modeling,Implicit Functions,Parametric Body Model,Human Geometries,Topologies,Reconstruction,Re-animation"
Multimodality in VR: A survey,"Virtual reality (VR) is rapidly growing, with the potential to change the way
we create and consume content. In VR, users integrate multimodal sensory
information they receive, to create a unified perception of the virtual world.
In this survey, we review the body of work addressing multimodality in VR, and
its role and benefits in user experience, together with different applications
that leverage multimodality in many disciplines. These works thus encompass
several fields of research, and demonstrate that multimodality plays a
fundamental role in VR; enhancing the experience, improving overall
performance, and yielding unprecedented abilities in skill and knowledge
transfer.","['Daniel Martin', 'Sandra Malpica', 'Diego Gutierrez', 'Belen Masia', 'Ana Serrano']",2021-01-20T00:29:23Z,http://arxiv.org/abs/2101.07906v3,"['cs.HC', 'cs.GR']","multimodality,VR,sensory information,virtual world,user experience,applications,disciplines,research,performance,knowledge transfer"
"TaNTIN: Terrestrial and Non-Terrestrial Integrated Networks-A
  collaborative technologies perspective for beyond 5G and 6G","The world is moving toward globalization rapidly. Everybody has easy access
to information with the spread of Internet technology. Businesses are growing
beyond national borders. Internationalization affects every aspect of life. In
this scenario, by dispersing functions and tasks across organizational borders,
time and space, global organizations have higher requirements for
collaboration. In order to allow decision-makers and knowledge workers,
situated at different times and spaces, to work more efficiently, collaborative
technologies are needed. In this paper, we give an overview of potential
collaborative technologies, their benefits, risks and challenges, types, and
elements. Based on the conceptualization of terrestrial and non-terrestrial
integrated networks (TaNTIN), we highlight artificial intelligence (AI),
blockchains, tactile Internet, mobile edge computing (MEC)/fog computing,
augmented reality and virtual reality, and so forth as the key features to
ensure quality-of-service (QoS) guarantee of futuristic collaborative services
such as telemedicine, e-education, online gaming, online businesses, the
entertainment industry. We also discuss how these technologies will impact
human life in the near future.","['Muhammad Waseem Akhtar', 'Syed Ali Hassan']",2021-01-20T17:17:05Z,http://arxiv.org/abs/2101.08221v1,['cs.NI'],"Terrestrial,Non-Terrestrial,Integrated Networks,Collaborative Technologies,Beyond 5G,6G,Artificial Intelligence,Blockchains,Tactile Internet,Mobile Edge Computing,Fog Computing"
Digital Transformations of Classrooms in Virtual Reality,"With rapid developments in consumer-level head-mounted displays and computer
graphics, immersive VR has the potential to take online and remote learning
closer to real-world settings. However, the effects of such digital
transformations on learners, particularly for VR, have not been evaluated in
depth. This work investigates the interaction-related effects of sitting
positions of learners, visualization styles of peer-learners and teachers, and
hand-raising behaviors of virtual peer-learners on learners in an immersive VR
classroom, using eye tracking data. Our results indicate that learners sitting
in the back of the virtual classroom may have difficulties extracting
information. Additionally, we find indications that learners engage with
lectures more efficiently if virtual avatars are visualized with realistic
styles. Lastly, we find different eye movement behaviors towards different
performance levels of virtual peer-learners, which should be investigated
further. Our findings present an important baseline for design decisions for VR
classrooms.","['Hong Gao', 'Efe Bozkir', 'Lisa Hasenbein', 'Jens-Uwe Hahn', 'Richard Göllner', 'Enkelejda Kasneci']",2021-01-23T20:15:17Z,http://arxiv.org/abs/2101.09576v2,['cs.HC'],"immersive VR,online learning,remote learning,virtual classroom,visualization styles,hand-raising behaviors,eye tracking data,virtual avatars,eye movement behaviors"
"Evolution of Small Cell from 4G to 6G: Past, Present, and Future","To boost the capacity of the cellular system, the operators have started to
reuse the same licensed spectrum by deploying 4G LTE small cells (Femto Cells)
in the past. But in time, these small cell licensed spectrum is not sufficient
to satisfy future applications like augmented reality (AR)and virtual reality
(VR). Hence, cellular operators look for alternate unlicensed spectrum in Wi-Fi
5 GHz band, later 3GPP named as LTE Licensed Assisted Access (LAA). The recent
and current rollout of LAA deployments (in developed nations like the US)
provides an opportunity to understand coexistence profound ground truth. This
paper discusses a high-level overview of my past, present, and future research
works in the direction of small cell benefits. In the future, we shift the
focus onto the latest unlicensed band: 6 GHz, where the latest Wi-Fi version,
802.11ax, will coexist with the latest cellular technology, 5G New Radio(NR) in
unlicensed",['Vanlin Sathya'],2020-12-29T17:28:08Z,http://arxiv.org/abs/2101.10451v1,"['cs.NI', 'cs.LG']","small cell,4G,6G,licensed spectrum,unlicensed spectrum,Wi-Fi,LTE Licensed Assisted Access (LAA),augmented reality (AR),virtual reality (VR),5G New Radio(NR)"
"An Overview of Enhancing Distance Learning Through Augmented and Virtual
  Reality Technologies","Although distance learning presents a number of interesting educational
advantages as compared to in-person instruction, it is not without its
downsides. We first assess the educational challenges presented by distance
learning as a whole and identify 4 main challenges that distance learning
currently presents as compared to in-person instruction: the lack of social
interaction, reduced student engagement and focus, reduced comprehension and
information retention, and the lack of flexible and customizable instructor
resources. After assessing each of these challenges in-depth, we examine how
AR/VR technologies might serve to address each challenge along with their
current shortcomings, and finally outline the further research that is required
to fully understand the potential of AR/VR technologies as they apply to
distance learning.","['Elizabeth Childs', 'Ferzam Mohammad', 'Logan Stevens', 'Hugo Burbelo', 'Amanuel Awoke', 'Nicholas Rewkowski', 'Dinesh Manocha']",2021-01-26T22:56:25Z,http://arxiv.org/abs/2101.11000v3,['cs.HC'],"Distance learning,Augmented reality,Virtual reality,Social interaction,Student engagement,Information retention,Instructor resources,Educational advantages,Challenges,Research"
A privacy-preserving approach to streaming eye-tracking data,"Eye-tracking technology is being increasingly integrated into mixed reality
devices. Although critical applications are being enabled, there are
significant possibilities for violating user privacy expectations. We show that
there is an appreciable risk of unique user identification even under natural
viewing conditions in virtual reality. This identification would allow an app
to connect a user's personal ID with their work ID without needing their
consent, for example. To mitigate such risks we propose a framework that
incorporates gatekeeping via the design of the application programming
interface and via software-implemented privacy mechanisms. Our results indicate
that these mechanisms can reduce the rate of identification from as much as 85%
to as low as 30%. The impact of introducing these mechanisms is less than
1.5$^\circ$ error in gaze position for gaze prediction. Gaze data streams can
thus be made private while still allowing for gaze prediction, for example,
during foveated rendering. Our approach is the first to support
privacy-by-design in the flow of eye-tracking data within mixed reality use
cases.","['Brendan David-John', 'Diane Hosfelt', 'Kevin Butler', 'Eakta Jain']",2021-02-02T21:43:01Z,http://arxiv.org/abs/2102.01770v2,['cs.HC'],"privacy-preserving,streaming,eye-tracking data,mixed reality,user identification,application programming interface,privacy mechanisms,gaze prediction,foveated rendering,privacy-by-design"
"I2UV-HandNet: Image-to-UV Prediction Network for Accurate and
  High-fidelity 3D Hand Mesh Modeling","Reconstructing a high-precision and high-fidelity 3D human hand from a color
image plays a central role in replicating a realistic virtual hand in
human-computer interaction and virtual reality applications. The results of
current methods are lacking in accuracy and fidelity due to various hand poses
and severe occlusions. In this study, we propose an I2UV-HandNet model for
accurate hand pose and shape estimation as well as 3D hand super-resolution
reconstruction. Specifically, we present the first UV-based 3D hand shape
representation. To recover a 3D hand mesh from an RGB image, we design an
AffineNet to predict a UV position map from the input in an image-to-image
translation fashion. To obtain a higher fidelity shape, we exploit an
additional SRNet to transform the low-resolution UV map outputted by AffineNet
into a high-resolution one. For the first time, we demonstrate the
characterization capability of the UV-based hand shape representation. Our
experiments show that the proposed method achieves state-of-the-art performance
on several challenging benchmarks.","['Ping Chen', 'Yujin Chen', 'Dong Yang', 'Fangyin Wu', 'Qin Li', 'Qingpei Xia', 'Yong Tan']",2021-02-07T05:49:50Z,http://arxiv.org/abs/2102.03725v2,['cs.CV'],"Image-to-UV Prediction,3D Hand Mesh Modeling,Hand Pose Estimation,Shape Estimation,3D Reconstruction,AffineNet,UV Representation,Image-to-Image Translation,Super-Resolution,Characterization"
"OV$^{2}$SLAM : A Fully Online and Versatile Visual SLAM for Real-Time
  Applications","Many applications of Visual SLAM, such as augmented reality, virtual reality,
robotics or autonomous driving, require versatile, robust and precise
solutions, most often with real-time capability. In this work, we describe
OV$^{2}$SLAM, a fully online algorithm, handling both monocular and stereo
camera setups, various map scales and frame-rates ranging from a few Hertz up
to several hundreds. It combines numerous recent contributions in visual
localization within an efficient multi-threaded architecture. Extensive
comparisons with competing algorithms shows the state-of-the-art accuracy and
real-time performance of the resulting algorithm. For the benefit of the
community, we release the source code:
\url{https://github.com/ov2slam/ov2slam}.","['Maxime Ferrera', 'Alexandre Eudes', 'Julien Moras', 'Martial Sanfourche', 'Guy Le Besnerais']",2021-02-08T08:39:23Z,http://arxiv.org/abs/2102.04060v1,"['cs.CV', 'cs.RO']","Visual SLAM,augmented reality,virtual reality,robotics,autonomous driving,online algorithm,monocular camera,stereo camera,multi-threaded architecture,real-time performance"
"A Survey on 360-Degree Video: Coding, Quality of Experience and
  Streaming","The commercialization of Virtual Reality (VR) headsets has made immersive and
360-degree video streaming the subject of intense interest in the industry and
research communities. While the basic principles of video streaming are the
same, immersive video presents a set of specific challenges that need to be
addressed. In this survey, we present the latest developments in the relevant
literature on four of the most important ones: (i) omnidirectional video coding
and compression, (ii) subjective and objective Quality of Experience (QoE) and
the factors that can affect it, (iii) saliency measurement and Field of View
(FoV) prediction, and (iv) the adaptive streaming of immersive 360-degree
videos. The final objective of the survey is to provide an overview of the
research on all the elements of an immersive video streaming system, giving the
reader an understanding of their interplay and performance.",['Federico Chiariotti'],2021-02-16T14:39:59Z,http://arxiv.org/abs/2102.08192v1,"['cs.MM', 'cs.NI', 'eess.IV']","360-degree video,coding,Quality of Experience,streaming,omnidirectional video,compression,saliency measurement,Field of View,adaptive streaming"
Analysis of User Preferences for Robot Motions in Immersive Telepresence,"This paper considers how the motions of a telepresence robot moving
autonomously affect a person immersed in the robot through a head-mounted
display. In particular, we explore the preference, comfort, and naturalness of
elements of piecewise linear paths compared to the same elements on a smooth
path. In a user study, thirty-six subjects watched panoramic videos of three
different paths through a simulated museum in virtual reality and responded to
questionnaires regarding each path. Preference for a particular path was
influenced the most by comfort, forward speed, and characteristics of the
turns. Preference was also strongly associated with the users' perceived
naturalness, which was primarily determined by the ability to see salient
objects, the distance to the walls and objects, as well as the turns.
Participants favored the paths that had a one meter per second forward speed
and rated the path with the least amount of turns as the most comfortable","['Katherine J. Mimnaugh', 'Markku Suomalainen', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-03-05T06:50:48Z,http://arxiv.org/abs/2103.03496v2,"['cs.RO', 'cs.MM']","robot motions,immersive telepresence,user preferences,comfort,naturalness,piecewise linear paths,smooth path,virtual reality,user study,forward speed"
"Learning to compose 6-DoF omnidirectional videos using multi-sphere
  images","Omnidirectional video is an essential component of Virtual Reality. Although
various methods have been proposed to generate content that can be viewed with
six degrees of freedom (6-DoF), existing systems usually involve complex depth
estimation, image in-painting or stitching pre-processing. In this paper, we
propose a system that uses a 3D ConvNet to generate a multi-sphere images (MSI)
representation that can be experienced in 6-DoF VR. The system utilizes
conventional omnidirectional VR camera footage directly without the need for a
depth map or segmentation mask, thereby significantly simplifying the overall
complexity of the 6-DoF omnidirectional video composition. By using a newly
designed weighted sphere sweep volume (WSSV) fusing technique, our approach is
compatible with most panoramic VR camera setups. A ground truth generation
approach for high-quality artifact-free 6-DoF contents is proposed and can be
used by the research and development community for 6-DoF content generation.","['Jisheng Li', 'Yuze He', 'Yubin Hu', 'Yuxing Han', 'Jiangtao Wen']",2021-03-10T03:09:55Z,http://arxiv.org/abs/2103.05842v1,"['cs.CV', 'cs.MM']","omnidirectional video,Virtual Reality,6-DoF,multi-sphere images,3D ConvNet,VR camera,depth map,segmentation mask,sphere sweep volume,ground truth generation"
"Immersive Operation of a Semi-Autonomous Aerial Platform for Detecting
  and Mapping Radiation","Recent advancements in radiation detection and computer vision have enabled
small unmanned aerial systems (sUASs) to produce 3D nuclear radiation maps in
real-time. Currently these state-of-the-art systems still require two
operators: one to pilot the sUAS and another operator to monitor the detected
radiation. In this work we present a system that integrates real-time 3D
radiation visualization with semi-autonomous sUAS control. Our Virtual Reality
interface enables a single operator to define trajectories using waypoints to
abstract complex flight control and utilize the semi-autonomous maneuvering
capabilities of the sUAS. The interface also displays a fused radiation
visualization and environment map, thereby enabling simultaneous remote
operation and radiation monitoring by a single operator. This serves as the
basis for development of a single system that deploys and autonomously controls
fleets of sUASs.","['P. Dayani', 'N. Orr', 'A. Thomopoulos', 'V. Saran', 'S. Krishnaswamy', 'E. Zhang', 'N. Hu', 'D. McPherson', 'J. Menke', 'A. Yang', 'K. Vetter']",2021-03-18T07:38:37Z,http://arxiv.org/abs/2103.10057v1,"['cs.RO', 'cs.HC']","immersive operation,semi-autonomous aerial platform,radiation detection,mapping,computer vision,unmanned aerial systems,3D radiation maps,real-time visualization,virtual reality interface,semi-autonomous control"
Neural Networks for Semantic Gaze Analysis in XR Settings,"Virtual-reality (VR) and augmented-reality (AR) technology is increasingly
combined with eye-tracking. This combination broadens both fields and opens up
new areas of application, in which visual perception and related cognitive
processes can be studied in interactive but still well controlled settings.
However, performing a semantic gaze analysis of eye-tracking data from
interactive three-dimensional scenes is a resource-intense task, which so far
has been an obstacle to economic use. In this paper we present a novel approach
which minimizes time and information necessary to annotate volumes of interest
(VOIs) by using techniques from object recognition. To do so, we train
convolutional neural networks (CNNs) on synthetic data sets derived from
virtual models using image augmentation techniques. We evaluate our method in
real and virtual environments, showing that the method can compete with
state-of-the-art approaches, while not relying on additional markers or
preexisting databases but instead offering cross-platform use.","['Lena Stubbemann', 'Dominik Dürrschnabel', 'Robert Refflinghaus']",2021-03-18T18:05:01Z,http://arxiv.org/abs/2103.10451v1,"['cs.CV', 'cs.HC', 'cs.LG', '68T45', 'I.4.8; I.4.9; I.2.10']","Neural Networks,Semantic Gaze Analysis,XR Settings,Eye-Tracking,Virtual Reality,Augmented Reality,Convolutional Neural Networks,Object Recognition,Volumes of Interest (VOIs),Image Augmentation"
Learning 6DoF Grasping Using Reward-Consistent Demonstration,"As the number of the robot's degrees of freedom increases, the implementation
of robot motion becomes more complex and difficult. In this study, we focus on
learning 6DOF-grasping motion and consider dividing the grasping motion into
multiple tasks. We propose to combine imitation and reinforcement learning in
order to facilitate a more efficient learning of the desired motion. In order
to collect demonstration data as teacher data for the imitation learning, we
created a virtual reality (VR) interface that allows humans to operate the
robot intuitively. Moreover, by dividing the motion into simpler tasks, we
simplify the design of reward functions for reinforcement learning and show in
our experiments a reduction in the steps required to learn the grasping motion.","['Daichi Kawakami', 'Ryoichi Ishikawa', 'Menandro Roxas', 'Yoshihiro Sato', 'Takeshi Oishi']",2021-03-23T05:33:59Z,http://arxiv.org/abs/2103.12321v1,"['cs.RO', 'cs.LG']","6DoF Grasping,Reward-Consistent Demonstration,Degrees of Freedom,Robot Motion,Imitation Learning,Reinforcement Learning,Demonstration Data,Virtual Reality Interface,Reward Functions,Grasping Motion."
"ScanGAN360: A Generative Model of Realistic Scanpaths for 360$^{\circ}$
  Images","Understanding and modeling the dynamics of human gaze behavior in 360$^\circ$
environments is a key challenge in computer vision and virtual reality.
Generative adversarial approaches could alleviate this challenge by generating
a large number of possible scanpaths for unseen images. Existing methods for
scanpath generation, however, do not adequately predict realistic scanpaths for
360$^\circ$ images. We present ScanGAN360, a new generative adversarial
approach to address this challenging problem. Our network generator is tailored
to the specifics of 360$^\circ$ images representing immersive environments.
Specifically, we accomplish this by leveraging the use of a spherical
adaptation of dynamic-time warping as a loss function and proposing a novel
parameterization of 360$^\circ$ scanpaths. The quality of our scanpaths
outperforms competing approaches by a large margin and is almost on par with
the human baseline. ScanGAN360 thus allows fast simulation of large numbers of
virtual observers, whose behavior mimics real users, enabling a better
understanding of gaze behavior and novel applications in virtual scene design.","['Daniel Martin', 'Ana Serrano', 'Alexander W. Bergman', 'Gordon Wetzstein', 'Belen Masia']",2021-03-25T15:34:18Z,http://arxiv.org/abs/2103.13922v1,"['cs.CV', 'cs.GR']","generative model,scanpaths,360$^{\circ}$ images,computer vision,virtual reality,generative adversarial approach,dynamic-time warping,parameterization,simulation,gaze behavior."
Contextual Scene Augmentation and Synthesis via GSACNet,"Indoor scene augmentation has become an emerging topic in the field of
computer vision and graphics with applications in augmented and virtual
reality. However, current state-of-the-art systems using deep neural networks
require large datasets for training. In this paper we introduce GSACNet, a
contextual scene augmentation system that can be trained with limited scene
priors. GSACNet utilizes a novel parametric data augmentation method combined
with a Graph Attention and Siamese network architecture followed by an
Autoencoder network to facilitate training with small datasets. We show the
effectiveness of our proposed system by conducting ablation and comparative
studies with alternative systems on the Matterport3D dataset. Our results
indicate that our scene augmentation outperforms prior art in scene synthesis
with limited scene priors available.","['Mohammad Keshavarzi', 'Flaviano Christian Reyes', 'Ritika Shrivastava', 'Oladapo Afolabi', 'Luisa Caldas', 'Allen Y. Yang']",2021-03-29T06:47:01Z,http://arxiv.org/abs/2103.15369v1,"['cs.CV', 'cs.AI', 'cs.GR']","Indoor scene augmentation,Computer vision,Graphics,Deep neural networks,Data augmentation,Graph Attention network,Siamese network,Autoencoder network,Matterport3D dataset,Scene synthesis"
Egocentric Pose Estimation from Human Vision Span,"Estimating camera wearer's body pose from an egocentric view (egopose) is a
vital task in augmented and virtual reality. Existing approaches either use a
narrow field of view front facing camera that barely captures the wearer, or an
extruded head-mounted top-down camera for maximal wearer visibility. In this
paper, we tackle the egopose estimation from a more natural human vision span,
where camera wearer can be seen in the peripheral view and depending on the
head pose the wearer may become invisible or has a limited partial view. This
is a realistic visual field for user-centric wearable devices like glasses
which have front facing wide angle cameras. Existing solutions are not
appropriate for this setting, and so, we propose a novel deep learning system
taking advantage of both the dynamic features from camera SLAM and the body
shape imagery. We compute 3D head pose, 3D body pose, the figure/ground
separation, all at the same time while explicitly enforcing a certain geometric
consistency across pose attributes. We further show that this system can be
trained robustly with lots of existing mocap data so we do not have to collect
and annotate large new datasets. Lastly, our system estimates egopose in real
time and on the fly while maintaining high accuracy.","['Hao Jiang', 'Vamsi Krishna Ithapu']",2021-04-12T02:38:22Z,http://arxiv.org/abs/2104.05167v1,['cs.CV'],"Egocentric pose estimation,Human vision span,Augmented reality,Virtual reality,Field of view,Camera SLAM,Body pose,Geometric consistency,Deep learning,Mocap data"
"Some Lessons Learned Running Virtual Reality Experiments Out of the
  Laboratory","In the past twelve months, our team has had to move rapidly from conducting
most of our user experiments in a laboratory setting, to running experiments in
the wild away from the laboratory and without direct synchronous oversight from
an experimenter. This has challenged us to think about what types of experiment
we can run, and to improve our tools and methods to allow us to reliably
capture the necessary data. It has also offered us an opportunity to engage
with a more diverse population than we would normally engage with in the
laboratory. In this position paper we elaborate on the challenges and
opportunities, and give some lessons learned from our own experience.","['Anthony Steed', 'Daniel Archer', 'Ben Congdon', 'Sebastian Friston', 'David Swapp', 'Felix J. Thiel']",2021-04-12T11:23:38Z,http://arxiv.org/abs/2104.05359v1,['cs.HC'],"virtual reality,experiments,laboratory,wild,synchronous oversight,experimenter,tools,data,diverse population,challenges"
"Trimanipulation: Evaluation of human performance in a 3-handed
  coordination task","Many teleoperation tasks require three or more tools working together, which
need the cooperation of multiple operators. The effectiveness of such schemes
may be limited by communication. Trimanipulation by a single operator using an
artificial third arm controlled together with their natural arms is a promising
solution to this issue. Foot-controlled interfaces have previously shown the
capability to be used for the continuous control of robot arms. However, the
use of such interfaces for controlling a supernumerary robotic limb (SRLs) in
coordination with the natural limbs, is not well understood. In this paper, a
teleoperation task imitating physically coupled hands in a virtual reality
scene was conducted with 14 subjects to evaluate human performance during
tri-manipulation. The participants were required to move three limbs together
in a coordinated way mimicking three arms holding a shared physical object. It
was found that after a short practice session, the three-hand tri-manipulation
using a single subject's hands and foot was still slower than dyad operation,
however, they displayed similar performance in success rate and higher motion
efficiency than two person's cooperation.","['Yanpei Huang', 'Jonathan Eden', 'Ekaterina Ivanova', 'Soo Jay Phee', 'Etienne Burdet']",2021-04-13T10:25:33Z,http://arxiv.org/abs/2104.06080v1,"['cs.RO', 'cs.HC']","Trimanipulation,Human performance,3-handed coordination task,Teleoperation,Artificial third arm,Foot-controlled interfaces,Robot arms,Supernumerary robotic limb (SRLs),Virtual reality,Human performance evaluation"
"Optimal Transmission of Multi-Quality Tiled 360 VR Video in MIMO-OFDMA
  Systems","In this paper, we study the optimal transmission of a multi-quality tiled 360
virtual reality (VR) video from a multi-antenna server (e.g., access point or
base station) to multiple single-antenna users in a multiple-input
multiple-output (MIMO)-orthogonal frequency division multiple access (OFDMA)
system. We minimize the total transmission power with respect to the subcarrier
allocation constraints, rate allocation constraints, and successful
transmission constraints, by optimizing the beamforming vector and subcarrier,
transmission power and rate allocation. The formulated resource allocation
problem is a challenging mixed discrete-continuous optimization problem. We
obtain an asymptotically optimal solution in the case of a large antenna array,
and a suboptimal solution in the general case. As far as we know, this is the
first work providing optimization-based design for 360 VR video transmission in
MIMO-OFDMA systems. Finally, by numerical results, we show that the proposed
solutions achieve significant improvement in performance compared to the
existing solutions.","['Chengjun Guo', 'Ying Cui', 'Zhi Liu', 'Derrick Wing Kwan Ng']",2021-04-13T13:34:09Z,http://arxiv.org/abs/2104.06183v1,['cs.MM'],"transmission,multi-quality,tiled,360 VR video,MIMO,OFDMA,beamforming,subcarrier allocation,rate allocation"
"Spatial Knowledge Acquisition in Virtual and Physical Reality: A
  Comparative Evaluation","Virtual Reality (VR) head-mounted displays (HMDs) have been studied widely as
tools for the most diverse kinds of training activities. One special kind that
is the basis for many real-world applications is spatial knowledge acquisition
and navigation. For example, knowing well by heart escape routes can be an
important factor for firefighters and soldiers. Prior research on how well
knowledge acquired in virtual worlds translates to real, physical one has had
mixed results, with some suggesting spatial learning in VR is akin to using a
regular 2D display. However, VR HMDs have evolved drastically in the last
decade, and little is known about how spatial training skills in a simulated
environment using up-to-date VR HMDs compares to training in the real world. In
this paper, we aim to investigate how people trained in a VR maze compare
against those trained in a physical maze in terms of recall of the position of
items inside the environment. While our results did not find significant
differences in time performance for people who experienced the physical and
those who trained in VR, other behavioural factors were different.","['Diego Monteiro', 'Xian Wang', 'Hai-Ning Liang', 'Yiyu Cai']",2021-04-15T17:40:52Z,http://arxiv.org/abs/2104.07624v1,['cs.HC'],"Spatial knowledge acquisition,Virtual reality,Physical reality,Comparative evaluation,Head-mounted displays,Navigation,Training activities,Spatial learning,2D display"
"Lighthouse Positioning System: Dataset, Accuracy, and Precision for UAV
  Research","The Lighthouse system was originally developed as tracking system for virtual
reality applications. Due to its affordable price, it has also found attractive
use-cases in robotics in the past. However, existing works frequently rely on
the centralized official tracking software, which make the solution less
attractive for UAV swarms. In this work, we consider an open-source tracking
software that can run onboard small Unmanned Aerial Vehicles (UAVs) in
real-time and enable distributed swarming algorithms. We provide a dataset
specifically for the use cases i) flight; and ii) as ground truth for other
commonly-used distributed swarming localization systems such as ultra-wideband.
We then use this dataset to analyze both accuracy and precision of the
Lighthouse system in different use-cases. To our knowledge, we are the first to
compare two different Lighthouse hardware versions with a motion capture system
and the first to analyze the accuracy using tracking software that runs onboard
a microcontroller.","['Arnaud Taffanel', 'Barbara Rousselot', 'Jonas Danielsson', 'Kimberly McGuire', 'Kristoffer Richardsson', 'Marcus Eliasson', 'Tobias Antonsson', 'Wolfgang Hönig']",2021-04-23T10:18:05Z,http://arxiv.org/abs/2104.11523v1,['cs.RO'],"Lighthouse Positioning System,Dataset,Accuracy,Precision,UAV,Tracking Software,Swarming Algorithms,Ground Truth,Ultra-wideband,Motion Capture System"
Thinking Outside the Lab: VR Size & Depth Perception in the Wild,"Size and distance perception in Virtual Reality (VR) have been widely
studied, albeit in a controlled laboratory setting with a small number of
participants. We describe a fully remote perceptual study with a gamified
protocol to encourage participant engagement, which allowed us to quickly
collect high-quality data from a large, diverse participant pool (N=60). Our
study aims to understand medium-field size and egocentric distance perception
in real-world usage of consumer VR devices. We utilized two perceptual matching
tasks -- distance bisection and size matching -- at the same target distances
of 1--9 metres. While the bisection protocol indicated a near-universal trend
of nonlinear distance compression, the size matching estimates were more
equivocal. Varying eye-height from the floor plane showed no significant effect
on the judgements. We also discuss the pros and cons of a fully remote
perceptual study in VR, the impact of hardware variation, and measures needed
to ensure high-quality data.","['Rahul Arora', 'Jiannan Li', 'Gongyi Shi', 'Karan Singh']",2021-05-03T00:20:44Z,http://arxiv.org/abs/2105.00584v1,"['cs.HC', 'H.5.1; H.1.2']","Virtual Reality,Size perception,Depth perception,Perceptual study,Distance bisection,Size matching,Consumer VR devices,Eye-height,Hardware variation,High-quality data"
Multi-scale Image Decomposition using a Local Statistical Edge Model,"We present a progressive image decomposition method based on a novel
non-linear filter named Sub-window Variance filter. Our method is specifically
designed for image detail enhancement purpose; this application requires
extraction of image details which are small in terms of both spatial and
variation scales. We propose a local statistical edge model which develops its
edge awareness using spatially defined image statistics. Our decomposition
method is controlled by two intuitive parameters which allow the users to
define what image details to suppress or enhance. By using the summed-area
table acceleration method, our decomposition pipeline is highly parallel. The
proposed filter is gradient preserving and this allows our enhancement results
free from the gradient-reversal artefact. In our evaluations, we compare our
method in various multi-scale image detail manipulation applications with other
mainstream solutions.",['Kin-Ming Wong'],2021-05-05T09:38:07Z,http://arxiv.org/abs/2105.01951v1,['cs.CV'],"image decomposition,non-linear filter,Sub-window Variance filter,image detail enhancement,local statistical edge model,spatial statistics,summed-area table acceleration method,gradient preserving,multi-scale image manipulation"
"Mixing Modalities of 3D Sketching and Speech for Interactive Model
  Retrieval in Virtual Reality","Sketch and speech are intuitive interaction methods that convey complementary
information and have been independently used for 3D model retrieval in virtual
environments. While sketch has been shown to be an effective retrieval method,
not all collections are easily navigable using this modality alone. We design a
new challenging database for sketch comprised of 3D chairs where each of the
components (arms, legs, seat, back) are independently colored. To overcome
this, we implement a multimodal interface for querying 3D model databases
within a virtual environment. We base the sketch on the state-of-the-art for 3D
Sketch Retrieval, and use a Wizard-of-Oz style experiment to process the voice
input. In this way, we avoid the complexities of natural language processing
which frequently requires fine-tuning to be robust. We conduct two user studies
and show that hybrid search strategies emerge from the combination of
interactions, fostering the advantages provided by both modalities.","['Daniele Giunchi', 'Alejandro Sztrajman', 'Stuart James', 'Anthony Steed']",2021-05-05T15:44:20Z,http://arxiv.org/abs/2105.02139v1,['cs.HC'],"3D sketching,speech,interactive model retrieval,virtual reality,multimodal interface,3D Sketch Retrieval,Wizard-of-Oz,natural language processing,user studies,hybrid search strategies"
"Procedural animations in interactive art experiences -- A state of the
  art review","The state of the art review broadly oversees the use of novel research
utilized in the creation of virtual environments applied in interactive art
experiences, with a specific focus on the application of procedural animation
in spatially augmented reality (SAR) exhibitions. These art exhibitions
frequently combine sensory displays that appeal, replace, and augment the
visual, auditory and touch or haptic senses. We analyze and break down
art-technology related innovations in the last three years, and thoroughly
identify the most recent and vibrant applications of interactive art
experiences in the review of numerous installation applications, studies, and
events. Display mediums such as virtual reality, augmented reality, mixed
reality, and robotics are overviewed in the context of art experiences such as
visual art museums, park or historic site tours, live concerts, and theatre. We
explore research and extrapolate how recent innovations can lead to different
applications that will be seen in the future.",['C. Tollola'],2021-05-16T05:14:56Z,http://arxiv.org/abs/2105.09153v1,"['cs.HC', 'cs.MM']","Procedural animations,Interactive art experiences,Virtual environments,Spatially augmented reality,SAR,Sensory displays,Haptic senses,Art-technology innovations,Mixed reality,Robotics."
Designing Limitless Path in Virtual Reality Environment,"Walking in a Virtual Environment is a bounded task. It is challenging for a
subject to navigate a large virtual environment designed in a limited physical
space. External hardware support may be required to achieve such an act in a
concise physical area without compromising navigation and virtual scene
rendering quality. This paper proposes an algorithmic approach to let a subject
navigate a limitless virtual environment within a limited physical space with
no additional external hardware support apart from the regular
Head-Mounted-Device (HMD) itself. As part of our work, we developed a Virtual
Art Gallery as a use-case to validate our algorithm. We conducted a simple
user-study to gather feedback from the participants to evaluate the ease of
locomotion of the application. The results showed that our algorithm could
generate limitless paths of our use-case under predefined conditions and can be
extended to other use-cases.","['Raghav Mittal', 'Sai Anirudh Karre', 'Y. Raghu Reddy']",2021-05-22T13:09:02Z,http://arxiv.org/abs/2105.10720v1,['cs.HC'],"Virtual reality,Limitless path,Virtual environment,Navigation,External hardware support,Algorithmic approach,Head-Mounted-Device (HMD),Virtual Art Gallery,User study,Locomotion"
DPLM: A Deep Perceptual Spatial-Audio Localization Metric,"Subjective evaluations are critical for assessing the perceptual realism of
sounds in audio-synthesis driven technologies like augmented and virtual
reality. However, they are challenging to set up, fatiguing for users, and
expensive. In this work, we tackle the problem of capturing the perceptual
characteristics of localizing sounds. Specifically, we propose a framework for
building a general purpose quality metric to assess spatial localization
differences between two binaural recordings. We model localization similarity
by utilizing activation-level distances from deep networks trained for
direction of arrival (DOA) estimation. Our proposed metric (DPLM) outperforms
baseline metrics on correlation with subjective ratings on a diverse set of
datasets, even without the benefit of any human-labeled training data.","['Pranay Manocha', 'Anurag Kumar', 'Buye Xu', 'Anjali Menon', 'Israel D. Gebru', 'Vamsi K. Ithapu', 'Paul Calamia']",2021-05-29T02:38:31Z,http://arxiv.org/abs/2105.14180v1,"['eess.AS', 'cs.SD']","Deep Learning,Spatial Audio,Localization,Metric,Perceptual,Binaural Recordings,Direction of Arrival,Deep Networks,Quality Metric"
"Domain Adaptation for Facial Expression Classifier via Domain
  Discrimination and Gradient Reversal","Bringing empathy to a computerized system could significantly improve the
quality of human-computer communications, as soon as machines would be able to
understand customer intentions and better serve their needs. According to
different studies (Literature Review), visual information is one of the most
important channels of human interaction and contains significant behavioral
signals, that may be captured from facial expressions. Therefore, it is
consistent and natural that the research in the field of Facial Expression
Recognition (FER) has acquired increased interest over the past decade due to
having diverse application area including health-care, sociology, psychology,
driver-safety, virtual reality, cognitive sciences, security, entertainment,
marketing, etc. We propose a new architecture for the task of FER and examine
the impact of domain discrimination loss regularization on the learning
process. With regard to observations, including both classical training
conditions and unsupervised domain adaptation scenarios, important aspects of
the considered domain adaptation approach integration are traced. The results
may serve as a foundation for further research in the field.",['Kamil Akhmetov'],2021-06-02T20:58:24Z,http://arxiv.org/abs/2106.01467v1,"['cs.CV', 'cs.LG']","Domain Adaptation,Facial Expression Classifier,Domain Discrimination,Gradient Reversal,Human-Computer Communication,Facial Expression Recognition,Behavioral Signals,Unsupervised Domain Adaptation,Learning Process,Integration Approach"
Deep Learning for Multi-View Stereo via Plane Sweep: A Survey,"3D reconstruction has lately attracted increasing attention due to its wide
application in many areas, such as autonomous driving, robotics and virtual
reality. As a dominant technique in artificial intelligence, deep learning has
been successfully adopted to solve various computer vision problems. However,
deep learning for 3D reconstruction is still at its infancy due to its unique
challenges and varying pipelines. To stimulate future research, this paper
presents a review of recent progress in deep learning methods for Multi-view
Stereo (MVS), which is considered as a crucial task of image-based 3D
reconstruction. It also presents comparative results on several publicly
available datasets, with insightful observations and inspiring future research
directions.","['Qingtian Zhu', 'Chen Min', 'Zizhuang Wei', 'Yisong Chen', 'Guoping Wang']",2021-06-18T14:10:44Z,http://arxiv.org/abs/2106.15328v2,['cs.CV'],"Deep Learning,Multi-View Stereo,Plane Sweep,3D Reconstruction,Artificial Intelligence,Computer Vision,Pipelines,Research Directions"
"eXtended Reality for Autism Interventions: The importance of Mediation
  and Sensory-Based Approaches","eXtended Reality (XR) autism research, ranging from Augmented Reality to
Virtual Reality, focuses on socio-emotional abilities and high-functioning
autism. However common autism interventions address the entire spectrum over
social, sensory and mediation issues. To bridge the gap between autism research
and real interventions, we compared existing literature on XR and autism with
stakeholders' needs obtained by interviewing 34 skateholders, mainly
practitioners. It allow us first to suggest XR use cases that could better
support practitioners' interventions, and second to derive design guidelines
accordingly. Findings demonstrate that collaborative XR sensory-based and
mediation approaches would benefit the entire spectrum, and encourage to
consider the overall intervention context when designing XR protocols.","['Valentin Bauer', 'Tifanie Bouchara', 'Patrick Bourdot']",2021-06-30T11:11:23Z,http://arxiv.org/abs/2106.15983v1,"['cs.HC', 'H.5.1; J.3; J.4']","eXtended Reality,Autism Interventions,Mediation,Sensory-Based Approaches,Augmented Reality,Virtual Reality,Socio-emotional abilities,High-functioning autism,Stakeholders,Design guidelines"
UrbanVR: An immersive analytics system for context-aware urban design,"Urban design is a highly visual discipline that requires visualization for
informed decision making. However, traditional urban design tools are mostly
limited to representations on 2D displays that lack intuitive awareness. The
popularity of head-mounted displays (HMDs) promotes a promising alternative
with consumer-grade 3D displays. We introduce UrbanVR, an immersive analytics
system with effective visualization and interaction techniques, to enable
architects to assess designs in a virtual reality (VR) environment.
Specifically, UrbanVR incorporates 1) a customized parallel coordinates plot
(PCP) design to facilitate quantitative assessment of high-dimensional design
metrics, 2) a series of egocentric interactions, including gesture interactions
and handle-bar metaphors, to facilitate user interactions, and 3) a viewpoint
optimization algorithm to help users explore both the PCP for quantitative
analysis, and objects of interest for context awareness. Effectiveness and
feasibility of the system are validated through quantitative user studies and
qualitative expert feedbacks.","['Chi Zhang', 'Wei Zeng', 'Ligang Liu']",2021-07-01T05:49:19Z,http://arxiv.org/abs/2107.00227v2,"['cs.HC', 'cs.GR']","Urban design,Visualization,Immersive analytics,Context-aware,Virtual reality (VR),Head-mounted displays (HMDs),3D displays,Parallel coordinates plot (PCP),Egocentric interactions,Viewpoint optimization algorithm"
"VREUD -- An End-User Development Tool to Simplify the Creation of
  Interactive VR Scenes","Recent advances in Virtual Reality (VR) technology and the increased
availability of VR-equipped devices enable a wide range of consumer-oriented
applications. For novice developers, however, creating interactive scenes for
VR applications is a complex and cumbersome task that requires high technical
knowledge which is often missing. This hinders the potential of enabling
novices to create, modify, and execute their own interactive VR scenes.
Although recent authoring tools for interactive VR scenes are promising, most
of them focus on expert professionals as the target group and neglect the
novices with low programming knowledge. To lower the entry barrier, we provide
an open-source web-based End-User Development (EUD) tool, called VREUD, that
supports the rapid construction and execution of interactive VR scenes.
Concerning construction, VREUD enables the specification of the VR scene
including interactions and tasks. Furthermore, VREUD supports the execution and
immersive experience of the created interactive VR scenes on VR head-mounted
displays. Based on a user study, we have analyzed the effectiveness,
efficiency, and user satisfaction of VREUD which shows promising results to
empower novices in creating their interactive VR scenes.","['Enes Yigitbas', 'Jonas Klauke', 'Sebastian Gottschalk', 'Gregor Engels']",2021-07-01T11:30:42Z,http://arxiv.org/abs/2107.00377v1,['cs.HC'],"End-User Development,Virtual Reality,VR Scenes,Interactive,Authoring Tools,Novices,Programming Knowledge,Web-based,Immersive Experience"
"An Overview of Low latency for Wireless Communications: an Evolutionary
  Perspective","Ultra-low latency supported by the fifth generation (5G) give impetus to the
prosperity of many wireless network applications, such as autonomous driving,
robotics, telepresence, virtual reality and so on. Ultra-low latency is not
achieved in a moment, but requires long-term evolution of network structure and
key enabling communication technologies. In this paper, we provide an
evolutionary overview of low latency in mobile communication systems, including
two different evolutionary perspectives: 1) network architecture; 2) physical
layer air interface technologies. We firstly describe in detail the evolution
of communication network architecture from the second generation (2G) to 5G,
highlighting the key points reducing latency. Moreover, we review the evolution
of key enabling technologies in the physical layer from 2G to 5G, which is also
aimed at reducing latency. We also discussed the challenges and future research
directions for low latency in network architecture and physical layer.","['Xin Fan', 'Yan Huo']",2021-07-07T21:15:47Z,http://arxiv.org/abs/2107.03484v1,"['cs.NI', 'cs.PF']","Low latency,Wireless communications,Evolutionary perspective,5G,Network architecture,Physical layer,Communication technologies,Mobile communication systems,Second generation (2G),Future research."
"Investigating the Sense of Presence Between Handcrafted and Panorama
  Based Virtual Environments","Virtual Reality applications are becoming increasingly mature. The
requirements and complexity of such systems is steadily increasing. Realistic
and detailed environments are often omitted in order to concentrate on the
interaction possibilities within the application. Creating an accurate and
realistic virtual environment is not a task for laypeople, but for experts in
3D design and modeling. To save costs and avoid hiring experts, panorama images
are often used to create realistic looking virtual environments. These images
can be captured and provided by non-experts. Panorama images are an alternative
to handcrafted 3D models in many cases because they offer immersion and a scene
can be captured in great detail with the touch of a button. This work
investigates whether it is advisable to recreate an environment in detail by
hand or whether it is recommended to use panorama images for virtual
environments in certain scenarios. For this purpose, an interactive virtual
environment was created in which a handmade 3D environment is almost
indistinguishable from an environment created with panorama images. Interactive
elements were added and a user study was conducted to investigate the effect of
both environments to the user. The study conducted indicates that panorama
images can be a useful substitute for 3D modeled environments.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2021-07-08T13:09:54Z,http://arxiv.org/abs/2107.03823v1,['cs.HC'],"Virtual Reality,Virtual Environment,Panorama Images,3D Design,Modeling,Immersion,Interactive Elements,User Study,Handcrafted,Presence"
"Monoscopic vs. Stereoscopic Views and Display Types in the Teleoperation
  of Unmanned Ground Vehicles for Object Avoidance","Virtual reality (VR) head-mounted displays (HMD) have recently been used to
provide an immersive, first-person vision/view in real-time for manipulating
remotely-controlled unmanned ground vehicles (UGV). The teleoperation of UGV
can be challenging for operators when it is done in real time. One big
challenge is for operators to perceive quickly and rapidly the distance of
objects that are around the UGV while it is moving. In this research, we
explore the use of monoscopic and stereoscopic views and display types
(immersive and non-immersive VR) for operating vehicles remotely. We conducted
two user studies to explore their feasibility and advantages. Results show a
significantly better performance when using an immersive display with
stereoscopic view for dynamic, real-time navigation tasks that require avoiding
both moving and static obstacles. The use of stereoscopic view in an immersive
display in particular improved user performance and led to better usability.","['Yiming Luo', 'Jialin Wang', 'Hai-Ning Liang', 'Shan Luo', 'Eng Gee Lim']",2021-07-12T04:53:37Z,http://arxiv.org/abs/2107.05194v1,['cs.HC'],"Teleoperation,Unmanned Ground Vehicles,Object Avoidance,Monoscopic Views,Stereoscopic Views,Display Types,Virtual Reality,HMD,User Studies,Immersive Display"
"""I Packed My Bag and in It I Put..."": A Taxonomy of Inventory Systems
  for Virtual Reality Games","On a journey, a backpack is a perfect place to store and organize the
necessary provisions and tools. Similarly, carrying and managing items is a
central part of most digital games, providing significant prospects for the
player experience. Even though VR games are gradually becoming more mature,
most of them still avoid this essential feature. Some of the reasons for this
deficit are the additional requirements and challenges that VR imposes on
developers to achieve a compelling user experience. We structure the ample
design space of VR inventories by analyzing popular VR games and developing a
structural taxonomy. We combine our insights with feedback from game developers
to identify the essential building blocks and design choices. Finally, we
propose meaningful design implications and demonstrate the practical use of our
work in action.","['Sebastian Cmentowski', 'Andrey Krekhov', 'Jens Krüger']",2021-07-18T13:02:06Z,http://arxiv.org/abs/2107.08434v2,['cs.HC'],"Inventory Systems,Virtual Reality Games,Design Space,VR Inventories,Design Choices"
"One-Class Classification for Wafer Map using Adversarial Autoencoder
  with DSVDD Prior","Recently, semiconductors' demand has exploded in virtual reality,
smartphones, wearable devices, the internet of things, robotics, and
automobiles. Semiconductor manufacturers want to make semiconductors with high
yields. To do this, manufacturers conduct many quality assurance activities.
Wafer map pattern classification is a typical way of quality assurance. The
defect pattern on the wafer map can tell us which process has a problem. Most
of the existing wafer map classification methods are based on supervised
methods. The supervised methods tend to have high performance, but they require
extensive labor and expert knowledge to produce labeled datasets with a
balanced distribution in mind. In the semiconductor manufacturing process, it
is challenging to get defect data with balanced distribution. In this paper, we
propose a one-class classification method using an Adversarial Autoencoder
(AAE) with Deep Support Vector Data Description (DSVDD) prior, which generates
random vectors within the hypersphere of DSVDD. We use the WM-811k dataset,
which consists of a real-world wafer map. We compare the F1 score performance
of our model with DSVDD and AAE.","['Ha Young Jo', 'Seong-Whan Lee']",2021-07-15T05:45:27Z,http://arxiv.org/abs/2107.08823v1,"['cs.LG', 'cs.AI']","One-Class Classification,Wafer Map,Adversarial Autoencoder,DSVDD Prior,Semiconductor,Quality Assurance,Defect Pattern,Supervised Methods,Balanced Distribution,F1 Score"
"""Deep Cut"": An all-in-one Geometric Algorithm for Unconstrained Cut,
  Tear and Drill of Soft-bodies in Mobile VR","In this work, we present an integrated geometric framework: ""deep- cut"" that
enables for the first time a user to geometrically and algorithmically cut,
tear and drill the surface of a skinned model without prior constraints,
layered on top of a custom soft body mesh deformation algorithm. Both layered
algorithms in this frame- work yield real-time results and are amenable for
mobile Virtual Reality, in order to be utilized in a variety of interactive
application scenarios. Our framework dramatically improves real-time user
experience and task performance in VR, without pre-calculated or artificially
designed cuts, tears, drills or surface deformations via predefined rigged
animations, which is the current state-of-the-art in mobile VR. Thus our
framework improves user experience on one hand, on the other hand saves both
time and costs from expensive, manual, labour-intensive design pre-calculation
stages.","['Manos Kamarianakis', 'Nick Lydatakis', 'Antonis Protopsaltis', 'John Petropoulos', 'Michail Tamiolakis', 'Paul Zikas', 'George Papagiannakis']",2021-08-11T15:29:13Z,http://arxiv.org/abs/2108.05281v1,"['cs.GR', 'cs.HC']","Geometric Algorithm,Soft-bodies,Mobile VR,Cut,Tear,Drill,Skinned Model,Mesh Deformation,Real-time,Virtual Reality"
Retrieval and Localization with Observation Constraints,"Accurate visual re-localization is very critical to many artificial
intelligence applications, such as augmented reality, virtual reality, robotics
and autonomous driving. To accomplish this task, we propose an integrated
visual re-localization method called RLOCS by combining image retrieval,
semantic consistency and geometry verification to achieve accurate estimations.
The localization pipeline is designed as a coarse-to-fine paradigm. In the
retrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ
DBSCAN followed by spatial verification to obtain a better initial coarse pose.
We design a module called observation constraints, which combines geometry
information and semantic consistency for filtering outliers. Comprehensive
experiments are conducted on open datasets, including retrieval on R-Oxford5k
and R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen
Day-Night and InLoc. By creatively modifying separate modules in the total
pipeline, our method achieves many performance improvements on the challenging
localization benchmarks.","['Yuhao Zhou', 'Huanhuan Fan', 'Shuang Gao', 'Yuchen Yang', 'Xudong Zhang', 'Jijunnan Li', 'Yandong Guo']",2021-08-19T06:14:33Z,http://arxiv.org/abs/2108.08516v1,"['cs.CV', 'cs.RO']","re-localization,image retrieval,semantic consistency,geometry verification,ResNet101,DBSCAN,spatial verification,observation constraints,semantic segmentation,localization"
"HapticBots: Distributed Encountered-type Haptics for VR with Multiple
  Shape-changing Mobile Robots","HapticBots introduces a novel encountered-type haptic approach for Virtual
Reality (VR) based on multiple tabletop-size shape-changing robots. These
robots move on a tabletop and change their height and orientation to haptically
render various surfaces and objects on-demand. Compared to previous
encountered-type haptic approaches like shape displays or robotic arms, our
proposed approach has an advantage in deployability, scalability, and
generalizability -- these robots can be easily deployed due to their compact
form factor. They can support multiple concurrent touch points in a large area
thanks to the distributed nature of the robots. We propose and evaluate a novel
set of interactions enabled by these robots which include: 1) rendering haptics
for VR objects by providing just-in-time touch-points on the user's hand, 2)
simulating continuous surfaces with the concurrent height and position change,
and 3) enabling the user to pick up and move VR objects through graspable proxy
objects. Finally, we demonstrate HapticBots with various applications,
including remote collaboration, education and training, design and 3D modeling,
and gaming and entertainment.","['Ryo Suzuki', 'Eyal Ofek', 'Mike Sinclair', 'Daneil Leithinger', 'Mar Gonzalez-Franco']",2021-08-24T16:26:22Z,http://arxiv.org/abs/2108.10829v1,"['cs.RO', 'cs.HC']","HapticBots,Encountered-type haptics,Virtual Reality,Shape-changing robots,Distributed,Scalability,Generalizability,Touch points,Interaction,Graspable proxy objects"
RealisticHands: A Hybrid Model for 3D Hand Reconstruction,"Estimating 3D hand meshes from RGB images robustly is a highly desirable
task, made challenging due to the numerous degrees of freedom, and issues such
as self similarity and occlusions. Previous methods generally either use
parametric 3D hand models or follow a model-free approach. While the former can
be considered more robust, e.g. to occlusions, they are less expressive. We
propose a hybrid approach, utilizing a deep neural network and differential
rendering based optimization to demonstrably achieve the best of both worlds.
In addition, we explore Virtual Reality (VR) as an application. Most VR
headsets are nowadays equipped with multiple cameras, which we can leverage by
extending our method to the egocentric stereo domain. This extension proves to
be more resilient to the above mentioned issues. Finally, as a use-case, we
show that the improved image-model alignment can be used to acquire the user's
hand texture, which leads to a more realistic virtual hand representation.","['Michael Seeber', 'Roi Poranne', 'Marc Polleyfeys', 'Martin R. Oswald']",2021-08-31T17:40:49Z,http://arxiv.org/abs/2108.13995v2,['cs.CV'],"3D hand reconstruction,hand meshes,RGB images,parametric 3D hand models,model-free approach,deep neural network,differential rendering,Virtual Reality (VR),egocentric stereo,image-model alignment"
"Exploratory Design of a Hands-free Video Game Controller for a
  Quadriplegic Individual","From colored pixels to hyper-realistic 3D landscapes of virtual reality,
video games have evolved immensely over the last few decades. However, video
game input still requires two-handed dexterous finger manipulations for
simultaneous joystick and trigger or mouse and keyboard presses. In this work,
we explore the design of a hands-free game control method using realtime facial
expression recognition for individuals with neurological and neuromuscular
diseases who are unable to use traditional game controllers. Similar to other
Assistive Technologies (AT), our facial input technique is also designed and
tested in collaboration with a graduate student who has Spinal Muscular
Atrophy. Our preliminary evaluation shows the potential of facial expression
recognition for augmenting the lives of quadriplegic individuals by enabling
them to accomplish things like walking, running, flying or other adventures
that may not be so attainable otherwise.","['Atieh Taheri', 'Ziv Weissman', 'Misha Sra']",2021-09-02T19:29:47Z,http://arxiv.org/abs/2109.01186v1,['cs.HC'],"exploratory design,hands-free,video game controller,quadriplegic individual,facial expression recognition,neurological diseases,neuromuscular diseases,Assistive Technologies,Spinal Muscular Atrophy"
ECO: Edge-Cloud Optimization of 5G applications,"Centralized cloud computing with 100+ milliseconds network latencies cannot
meet the tens of milliseconds to sub-millisecond response times required for
emerging 5G applications like autonomous driving, smart manufacturing, tactile
internet, and augmented or virtual reality. We describe a new, dynamic runtime
that enables such applications to make effective use of a 5G network, computing
at the edge of this network, and resources in the centralized cloud, at all
times. Our runtime continuously monitors the interaction among the
microservices, estimates the data produced and exchanged among the
microservices, and uses a novel graph min-cut algorithm to dynamically map the
microservices to the edge or the cloud to satisfy application-specific response
times. Our runtime also handles temporary network partitions, and maintains
data consistency across the distributed fabric by using microservice proxies to
reduce WAN bandwidth by an order of magnitude, all in an {\it
application-specific manner} by leveraging knowledge about the application's
functions, latency-critical pipelines and intermediate data. We illustrate the
use of our runtime by successfully mapping two complex, representative
real-world video analytics applications to the AWS/Verizon Wavelength
edge-cloud architecture, and improving application response times by 2x when
compared with a static edge-cloud implementation.","['Kunal Rao', 'Giuseppe Coviello', 'Wang-Pin Hsiung', 'Srimat Chakradhar']",2021-09-02T20:29:32Z,http://arxiv.org/abs/2109.01201v1,['cs.DC'],"Edge computing,Cloud computing,5G applications,Runtime,Microservices,Graph min-cut algorithm,Edge-cloud optimization,Data consistency,Network latency"
"Horizontal and Vertical Collaboration for VR Delivery in MEC-Enabled
  Small-Cell Networks","Due to the large bandwidth, low latency and computationally intensive
features of virtual reality (VR) video applications, the current
resource-constrained wireless and edge networks cannot meet the requirements of
on-demand VR delivery. In this letter, we propose a joint horizontal and
vertical collaboration architecture in mobile edge computing (MEC)-enabled
small-cell networks for VR delivery. In the proposed architecture, multiple MEC
servers can jointly provide VR head-mounted devices (HMDs) with edge caching
and viewpoint computation services, while the computation tasks can also be
performed at HMDs or on the cloud. Power allocation at base stations (BSs) is
considered in coordination with horizontal collaboration (HC) and vertical
collaboration (VC) of MEC servers to obtain lower end-to-end latency of VR
delivery. A joint caching, power allocation and task offloading problem is then
formulated, and a discrete branch-reduce-and-bound (DBRB) algorithm inspired by
monotone optimization is proposed to effectively solve the problem. Simulation
results demonstrate the advantage of the proposed architecture and algorithm in
terms of existing ones.","['Zhuojia Gu', 'Hancheng Lu', 'Chenkai Zou']",2021-09-05T02:36:47Z,http://arxiv.org/abs/2109.01971v1,"['cs.NI', 'eess.SP']","virtual reality,MEC,small-cell networks,edge computing,caching,computation,power allocation,task offloading,latency,optimization"
"VRMenuDesigner: A toolkit for automatically generating and modifying VR
  menus","With the rapid development of Virtual Reality (VR) technology, the research
of User Interface (UI), especially menus, in the VR environment has attracted
more and more attention. However, it is very tedious for researchers to develop
UI from scratch or modify existing functions and there are no easy-to-use tools
for efficient development. This paper aims to present VRMenuDesigner, a
flexible and modular toolkit for automatically generating/modifying VR menus.
This toolkit is provided as open-source library and easy to extend to adapt to
various requirements. The main contribution of this work is to organize the
menus and functions with object-oriented thinking, which makes the system very
understandable and extensible. VRMenuDesigner includes two key tools: Creator
and Modifier for quickly generating and modifying elements. Moreover, we
developed several built-in menus and discussed their usability. After a brief
review and taxonomy of 3D menus, the architecture and implementation of the
toolbox are introduced.","['Shengzhe Hou', 'Bruce H. Thomas']",2021-09-21T13:39:15Z,http://arxiv.org/abs/2109.10172v1,['cs.HC'],"Virtual Reality,User Interface,VR menus,Toolkit,Automatic generation,Modification,Object-oriented,Open-source,Library,Extensibility"
"A 4-DoF Parallel Origami Haptic Device for Normal, Shear, and Torsion
  Feedback","We present a mesoscale finger-mounted 4-degree-of-freedom (DoF) haptic device
that is created using origami fabrication techniques. The 4-DoF device is a
parallel kinematic mechanism capable of delivering normal, shear, and torsional
haptic feedback to the fingertip. Traditional methods of robot fabrication are
not well suited for designing small robotic devices because it is challenging
and expensive to manufacture small, low-friction joints. Our device uses
origami manufacturing principles to reduce complexity and the device footprint.
We characterize the bandwidth, workspace, and force output of the device. The
capabilities of the torsion-DoF are demonstrated in a virtual reality scenario.
Our results show that the device can deliver haptic feedback in 4-DoFs with an
effective operational workspace of 0.64cm$^3$ with $\pm 30 ^ \circ$ rotation at
every location. The maximum forces and torques the device can apply in the x-,
y-, z-, and $\theta$-directions, are $\pm$1.5N, $\pm$1.5N, 2N, and 5N$\cdot$mm,
respectively, and the device has an operating bandwidth of 9Hz.","['Sophia R. Williams', 'Jacob M. Suchoski', 'Zonghe Chua', 'Allison M. Okamura']",2021-09-24T18:02:11Z,http://arxiv.org/abs/2109.12134v1,"['cs.RO', 'cs.HC']","origami fabrication,haptic device,parallel kinematic mechanism,normal feedback,shear feedback,torsional feedback,mesoscale,force output,operational workspace,bandwidth"
Cybersickness-aware Tile-based Adaptive 360° Video Streaming,"In contrast to traditional videos, the imaging in virtual reality (VR) is
360{\deg}, and it consumes larger bandwidth to transmit video contents. To
reduce bandwidth consumption, tile-based streaming has been proposed to deliver
the focused part of the video, instead of the whole one. On the other hand, the
techniques to alleviate cybersickness, which is akin to motion sickness and
happens when using digital displays, have not been jointly explored with the
tile selection in VR. In this paper, we investigate Tile Selection with
Cybersickness Control (TSCC) in an adaptive 360{\deg} video streaming system
with cybersickness alleviation. We propose an m-competitive online algorithm
with Cybersickness Indicator (CI) and Video Loss Indicator (VLI) to evaluate
instant cybersickness and the total loss of video quality. Moreover, the
algorithm exploits Sickness Migration Indicator (SMI) to evaluate the
cybersickness accumulated over time and the increase of optical flow to improve
the tile quality assignment. Simulations with a real network dataset show that
our algorithm outperforms the baselines regarding video quality and
cybersickness accumulation.","['Chiao-Wen Lin', 'Chih-Hang Wang', 'De-Nian Yang', 'Wanjiun Liao']",2021-10-04T08:44:59Z,http://arxiv.org/abs/2110.01252v1,"['cs.NI', 'cs.MM']","cybersickness,tile-based streaming,360° video,adaptive streaming,bandwidth consumption,virtual reality,motion sickness,digital displays,online algorithm,optical flow"
"How You Move Your Head Tells What You Do: Self-supervised Video
  Representation Learning with Egocentric Cameras and IMU Sensors","Understanding users' activities from head-mounted cameras is a fundamental
task for Augmented and Virtual Reality (AR/VR) applications. A typical approach
is to train a classifier in a supervised manner using data labeled by humans.
This approach has limitations due to the expensive annotation cost and the
closed coverage of activity labels. A potential way to address these
limitations is to use self-supervised learning (SSL). Instead of relying on
human annotations, SSL leverages intrinsic properties of data to learn
representations. We are particularly interested in learning egocentric video
representations benefiting from the head-motion generated by users' daily
activities, which can be easily obtained from IMU sensors embedded in AR/VR
devices. Towards this goal, we propose a simple but effective approach to learn
video representation by learning to tell the corresponding pairs of video clip
and head-motion. We demonstrate the effectiveness of our learned representation
for recognizing egocentric activities of people and dogs.","['Satoshi Tsutsui', 'Ruta Desai', 'Karl Ridgeway']",2021-10-04T19:25:15Z,http://arxiv.org/abs/2110.01680v1,['cs.CV'],"self-supervised learning,video representation learning,egocentric cameras,IMU sensors,augmented reality,virtual reality,head-mounted cameras,activity recognition,video clip,head-motion"
"ViewfinderVR: Configurable Viewfinder for Selection of Distant Objects
  in VR","Selection is one of the fundamental user interactions in virtual reality (VR)
and 3D user interaction, and raycasting has been one of the most popular object
selection techniques in VR. However, the selection of small or distant objects
through raycasting has been known to be difficult. To overcome this limitation,
this study proposed a new technique called ViewfinderVR for improved selection
of distant objects in VR, utilizing a virtual viewfinder panel with a modern
adaptation of the through-the-lens metaphor. ViewfinderVR enables faster and
more accurate target selection by allowing customization of the interaction
space projected onto a virtual panel within reach, and users can select objects
reflected on the panel with either ray-based or touch interaction. Experimental
results of Fitts' law-based tests with 20 participants showed that ViewfinderVR
outperformed traditional raycasting in terms of task performance (movement
time, error rate, and throughput) and perceived workload (NASA-TLX ratings),
where touch interaction was superior to ray-based interaction. The associated
user behavior was also recorded and analyzed to understand the underlying
reasons for the improved task performance and reduced workload. The proposed
technique can be used in VR applications to enhance the selection of distant
objects.","['Woojoo Kim', 'Shuping Xiong']",2021-10-06T05:35:04Z,http://arxiv.org/abs/2110.02514v1,"['cs.HC', 'H.5.1; H.5.2']","virtual reality,3D user interaction,raycasting,object selection,ViewfinderVR,through-the-lens metaphor,interaction space,touch interaction,Fitts' law,task performance"
Proactive Scheduling and Caching for Wireless VR Viewport Streaming,"Virtual Reality (VR) applications require high data rate for a high-quality
immersive experience, in addition to low latency to avoid dizziness and motion
sickness. One of the key wireless VR challenges is providing seamless
connectivity and meeting the stringent latency and bandwidth requirements. This
work proposes a proactive wireless VR system that utilizes information about
the user's future orientation for proactive scheduling and caching. This is
achieved by leveraging deep neural networks to predict users' orientation
trained on a real dataset. The 360{\deg} scene is then partitioned using an
overlapping viewports scheme so that only portions of the scene covered by the
users' perceptive field-of-view are streamed. Furthermore, to minimize the
backhaul latency, popular viewports are cached at the edge cloud based on
spatial popularity profiles. Through extensive simulations, we show that the
proposed system provides significant latency and throughput performance
improvement, especially in fluctuating channels and heavy load conditions. The
proactive scheduling enabled by the combination of machine learning prediction
and the proposed viewport scheme reduces the mean latency by more than 80%
while achieving successful delivery rate close to 100%.","['Mostafa Abdelrahman', 'Mohammed Elbamby', 'Vilho Räisänen']",2021-10-06T11:08:50Z,http://arxiv.org/abs/2110.02653v1,['cs.NI'],"Wireless VR,Proactive Scheduling,Caching,Viewport Streaming,Latency,Bandwidth,Deep Neural Networks,Edge Cloud,Machine Learning,Immersive Experience."
A Survey on Deep Learning for Skeleton-Based Human Animation,"Human character animation is often critical in entertainment content
production, including video games, virtual reality or fiction films. To this
end, deep neural networks drive most recent advances through deep learning and
deep reinforcement learning. In this article, we propose a comprehensive survey
on the state-of-the-art approaches based on either deep learning or deep
reinforcement learning in skeleton-based human character animation. First, we
introduce motion data representations, most common human motion datasets and
how basic deep models can be enhanced to foster learning of spatial and
temporal patterns in motion data. Second, we cover state-of-the-art approaches
divided into three large families of applications in human animation pipelines:
motion synthesis, character control and motion editing. Finally, we discuss the
limitations of the current state-of-the-art methods based on deep learning
and/or deep reinforcement learning in skeletal human character animation and
possible directions of future research to alleviate current limitations and
meet animators' needs.","['L. Mourot', 'L. Hoyet', 'F. Le Clerc', 'François Schnitzler', 'Pierre Hellier']",2021-10-13T17:29:50Z,http://arxiv.org/abs/2110.06901v2,['cs.GR'],"deep learning,skeleton-based,human animation,neural networks,motion data,spatial patterns,temporal patterns,motion synthesis,character control,motion editing"
Smooth head tracking for virtual reality applications,"In this work, we propose a new head-tracking solution for human-machine
real-time interaction with virtual 3D environments. This solution leverages
RGBD data to compute virtual camera pose according to the movements of the
user's head. The process starts with the extraction of a set of facial features
from the images delivered by the sensor. Such features are matched against
their respective counterparts in a reference image for the computation of the
current head pose. Afterwards, a prediction approach is used to guess the most
likely next head move (final pose). Pythagorean Hodograph interpolation is then
adapted to determine the path and local frames taken between the two poses. The
result is a smooth head trajectory that serves as an input to set the camera in
virtual scenes according to the user's gaze. The resulting motion model has the
advantage of being: continuous in time, it adapts to any frame rate of
rendering; it is ergonomic, as it frees the user from wearing tracking markers;
it is smooth and free from rendering jerks; and it is also torsion and
curvature minimizing as it produces a path with minimum bending energy.",['Abdenour Amamra'],2021-10-27T05:47:21Z,http://arxiv.org/abs/2110.14193v1,"['cs.CV', 'cs.GR']","head-tracking,virtual reality,RGBD data,camera pose,facial features,prediction approach,Pythagorean Hodograph interpolation,motion model,rendering,bending energy"
On-device Real-time Hand Gesture Recognition,"We present an on-device real-time hand gesture recognition (HGR) system,
which detects a set of predefined static gestures from a single RGB camera. The
system consists of two parts: a hand skeleton tracker and a gesture classifier.
We use MediaPipe Hands as the basis of the hand skeleton tracker, improve the
keypoint accuracy, and add the estimation of 3D keypoints in a world metric
space. We create two different gesture classifiers, one based on heuristics and
the other using neural networks (NN).","['George Sung', 'Kanstantsin Sokal', 'Esha Uboweja', 'Valentin Bazarevsky', 'Jonathan Baccash', 'Eduard Gabriel Bazavan', 'Chuo-Ling Chang', 'Matthias Grundmann']",2021-10-29T18:33:25Z,http://arxiv.org/abs/2111.00038v1,['cs.CV'],"on-device,real-time,hand gesture recognition,RGB camera,MediaPipe Hands,skeleton tracker,gesture classifier,3D keypoints,neural networks"
"On Efficient Uncertainty Estimation for Resource-Constrained Mobile
  Applications","Deep neural networks have shown great success in prediction quality while
reliable and robust uncertainty estimation remains a challenge. Predictive
uncertainty supplements model predictions and enables improved functionality of
downstream tasks including embedded and mobile applications, such as virtual
reality, augmented reality, sensor fusion, and perception. These applications
often require a compromise in complexity to obtain uncertainty estimates due to
very limited memory and compute resources. We tackle this problem by building
upon Monte Carlo Dropout (MCDO) models using the Axolotl framework;
specifically, we diversify sampled subnetworks, leverage dropout patterns, and
use a branching technique to improve predictive performance while maintaining
fast computations. We conduct experiments on (1) a multi-class classification
task using the CIFAR10 dataset, and (2) a more complex human body segmentation
task. Our results show the effectiveness of our approach by reaching close to
Deep Ensemble prediction quality and uncertainty estimation, while still
achieving faster inference on resource-limited mobile platforms.","['Johanna Rock', 'Tiago Azevedo', 'René de Jong', 'Daniel Ruiz-Muñoz', 'Partha Maji']",2021-11-11T22:24:15Z,http://arxiv.org/abs/2111.09838v2,['cs.LG'],"Uncertainty Estimation,Resource-Constrained,Mobile Applications,Deep Neural Networks,Monte Carlo Dropout,Axolotl Framework,Subnetworks,Dropout Patterns,Branching Technique,Inference速"
"Dataset of Spatial Room Impulse Responses in a Variable Acoustics Room
  for Six Degrees-of-Freedom Rendering and Analysis","Room acoustics measurements are used in many areas of audio research, from
physical acoustics modelling and speech enhancement to virtual reality
applications. This paper documents the technical specifications and choices
made in the measurement of a dataset of spatial room impulse responses (SRIRs)
in a variable acoustics room. Two spherical microphone arrays are used: the mh
Acoustics Eigenmike em32 and the Zylia ZM-1, capable of up to fourth- and
third-order Ambisonic capture, respectively. The dataset consists of three
source and seven receiver positions, repeated with five configurations of the
room's acoustics with varying levels of reverberation. Possible applications of
the dataset include six degrees-of-freedom (6DoF) analysis and rendering, SRIR
interpolation methods, and spatial dereverberation techniques.","['Thomas McKenzie', 'Leo McCormack', 'Christoph Hold']",2021-11-23T13:51:17Z,http://arxiv.org/abs/2111.11882v1,"['eess.AS', 'cs.SD', 'eess.SP']","Spatial room impulse responses,Variable acoustics room,Six degrees-of-freedom rendering,Acoustics modelling,Speech enhancement,Virtual reality applications,Microphone arrays,Ambisonic capture,Reverberation,Dereverberation techniques"
Distortion Reduction for Off-Center Perspective Projection of Panoramas,"A single Panorama can be drawn perspectively without distortions in arbitrary
viewing directions and field-of-views when the camera position is at the
origin. This is a key advantage in VR and virtual tour applications because it
enables the user to freely""look around"" in a virtual world with just a single
panorama, albeit at a fixed position. However, when the camera moves away from
the center, barrel distortions appear and realism breaks. We propose
modifications to the equirectangular-to-perspective(E2P) projection that
significantly reduce distortions when the camera position is away from the
origin. This enables users to not only ""look around"" but also ""walk around""
virtually in a single panorama with more convincing renderings. We compare with
other techniques that aim to augment panoramas with 3D information, including:
1) panoramas with depth information and 2) panoramas augmented with room
layouts, and show that our approach provides more visually convincing results","['Chi-Han Peng', 'Jiayao Zhang']",2021-11-23T17:29:26Z,http://arxiv.org/abs/2111.12018v1,['cs.GR'],"Distortion reduction,Off-center,Perspective projection,Panoramas,Virtual reality,Equirectangular,Barrel distortions,3D information,Room layouts,Renderings"
360MonoDepth: High-Resolution 360° Monocular Depth Estimation,"360{\deg} cameras can capture complete environments in a single shot, which
makes 360{\deg} imagery alluring in many computer vision tasks. However,
monocular depth estimation remains a challenge for 360{\deg} data, particularly
for high resolutions like 2K (2048x1024) and beyond that are important for
novel-view synthesis and virtual reality applications. Current CNN-based
methods do not support such high resolutions due to limited GPU memory. In this
work, we propose a flexible framework for monocular depth estimation from
high-resolution 360{\deg} images using tangent images. We project the 360{\deg}
input image onto a set of tangent planes that produce perspective views, which
are suitable for the latest, most accurate state-of-the-art perspective
monocular depth estimators. To achieve globally consistent disparity estimates,
we recombine the individual depth estimates using deformable multi-scale
alignment followed by gradient-domain blending. The result is a dense,
high-resolution 360{\deg} depth map with a high level of detail, also for
outdoor scenes which are not supported by existing methods. Our source code and
data are available at https://manurare.github.io/360monodepth/.","['Manuel Rey-Area', 'Mingze Yuan', 'Christian Richardt']",2021-11-30T18:57:29Z,http://arxiv.org/abs/2111.15669v2,['cs.CV'],"360° cameras,monocular depth estimation,high resolution,tangent images,CNN-based methods,GPU memory,perspective views,state-of-the-art,deformable multi-scale alignment,gradient-domain blending"
Digital Twinning Remote Laboratories for Online Practical Learning,"The COVID19 pandemic has demonstrated a need for remote learning and virtual
learning applications such as virtual reality (VR) and tablet-based solutions.
Creating complex learning scenarios by developers is highly time-consuming and
can take over a year. It is also costly to employ teams of system analysts,
developers and 3D artists. There is a requirement to provide a simple method to
enable lecturers to create their own content for their laboratory tutorials.
Research has been undertaken into developing generic models to enable the
semi-automatic creation of a virtual learning tools for subjects that require
practical interactions with the lab resources. In addition to the system for
creating digital twins, a case study describing the creation of a virtual
learning application for an electrical laboratory tutorial has been presented.","['Claire Palmer', 'Ben Roullier', 'Muhammad Aamir', 'Frank McQuade', 'Leonardo Stella', 'Ashiq Anjum']",2021-12-01T16:55:58Z,http://arxiv.org/abs/2112.00649v3,"['cs.HC', 'cs.AI']","Digital Twinning,Remote Laboratories,Online Practical Learning,Virtual Reality,Tablet-based Solutions,Virtual Learning,Virtual Learning Tools,Lab Resources,Electrical Laboratory Tutorial,Generic Models"
Attention based Occlusion Removal for Hybrid Telepresence Systems,"Traditionally, video conferencing is a widely adopted solution for
telecommunication, but a lack of immersiveness comes inherently due to the 2D
nature of facial representation. The integration of Virtual Reality (VR) in a
communication/telepresence system through Head Mounted Displays (HMDs) promises
to provide users a much better immersive experience. However, HMDs cause
hindrance by blocking the facial appearance and expressions of the user. To
overcome these issues, we propose a novel attention-enabled encoder-decoder
architecture for HMD de-occlusion. We also propose to train our person-specific
model using short videos (1-2 minutes) of the user, captured in varying
appearances, and demonstrated generalization to unseen poses and appearances of
the user. We report superior qualitative and quantitative results over
state-of-the-art methods. We also present applications of this approach to
hybrid video teleconferencing using existing animation and 3D face
reconstruction pipelines.","['Surabhi Gupta', 'Ashwath Shetty', 'Avinash Sharma']",2021-12-02T10:18:22Z,http://arxiv.org/abs/2112.01098v1,['cs.CV'],"telepresence systems,Attention,Occlusion Removal,Hybrid Systems,Virtual Reality (VR),Head Mounted Displays (HMDs),Encoder-decoder architecture,person-specific model,3D face reconstruction."
"Acoustic Sensing-based Hand Gesture Detection for Wearable Device
  Interaction","Hand gesture recognition attracts great attention for interaction since it is
intuitive and natural to perform. In this paper, we explore a novel method for
interaction by using bone-conducted sound generated by finger movements while
performing gestures. We design a set of gestures that generate unique sound
features, and capture the resulting sound from the wrist using a commodity
microphone. Next, we design a sound event detector and a recognition model to
classify the gestures. Our system achieves an overall accuracy of 90.13% in
quiet environments and 85.79% under noisy conditions. This promising technology
can be deployed on existing smartwatches as a low power service at no
additional cost, and can be used for interaction in augmented and virtual
reality applications.","['Bing Zhou', 'Matias Aiskovich', 'Sinem Guven']",2021-12-11T13:50:27Z,http://arxiv.org/abs/2112.05986v1,['cs.HC'],"Acoustic Sensing,Hand Gesture Detection,Wearable Device,Bone-conducted sound,Sound Event Detector,Recognition Model,Smartwatches,Augmented Reality,Virtual Reality"
"Watch It Move: Unsupervised Discovery of 3D Joints for Re-Posing of
  Articulated Objects","Rendering articulated objects while controlling their poses is critical to
applications such as virtual reality or animation for movies. Manipulating the
pose of an object, however, requires the understanding of its underlying
structure, that is, its joints and how they interact with each other.
Unfortunately, assuming the structure to be known, as existing methods do,
precludes the ability to work on new object categories. We propose to learn
both the appearance and the structure of previously unseen articulated objects
by observing them move from multiple views, with no joints annotation
supervision, or information about the structure. We observe that 3D points that
are static relative to one another should belong to the same part, and that
adjacent parts that move relative to each other must be connected by a joint.
To leverage this insight, we model the object parts in 3D as ellipsoids, which
allows us to identify joints. We combine this explicit representation with an
implicit one that compensates for the approximation introduced. We show that
our method works for different structures, from quadrupeds, to single-arm
robots, to humans.","['Atsuhiro Noguchi', 'Umar Iqbal', 'Jonathan Tremblay', 'Tatsuya Harada', 'Orazio Gallo']",2021-12-21T16:37:48Z,http://arxiv.org/abs/2112.11347v2,['cs.CV'],"3D joints,Unsupervised discovery,Articulated objects,Re-posing,Virtual reality,Animation,Structure learning,Object categories,Ellipsoids,Quadrupeds"
The Time Perception Control and Regulation in VR Environment,"To adapt to different environments, human circadian rhythms will be
constantly adjusted as the environment changes, which follows the principle of
survival of the fittest. According to this principle, objective factors (such
as circadian rhythms, and light intensity) can be utilized to control time
perception. The subjective judgment on the estimation of elapsed time is called
time perception. In the physical world, factors that can affect time
perception, represented by illumination, are called the Zeitgebers. In recent
years, with the development of Virtual Reality (VR) technology, effective
control of zeitgebers has become possible, which is difficult to achieve in the
physical world. Based on previous studies, this paper deeply explores the
actual performance in VR environment of four types of time zeitgebers (music,
color, cognitive load, and concentration) that have been proven to have a
certain impact on time perception in the physical world. It discusses the study
of the measurement of the difference between human time perception and
objective escaped time in the physical world.","['Zhitao Liu', 'Jinke Shi', 'Junhao He', 'Yu Wu', 'Ning Xie', 'Ke Xiong', 'Yutong Liu']",2021-12-22T07:49:52Z,http://arxiv.org/abs/2112.11714v1,['cs.HC'],"circadian rhythms,time perception,Virtual Reality (VR) technology,zeitgebers,illumination,concentration,cognitive load,music,color,measurement"
"Bottom-up approaches for multi-person pose estimation and it's
  applications: A brief review","Human Pose Estimation (HPE) is one of the fundamental problems in computer
vision. It has applications ranging from virtual reality, human behavior
analysis, video surveillance, anomaly detection, self-driving to medical
assistance. The main objective of HPE is to obtain the person's posture from
the given input. Among different paradigms for HPE, one paradigm is called
bottom-up multi-person pose estimation. In the bottom-up approach, initially,
all the key points of the targets are detected, and later in the optimization
stage, the detected key points are associated with the corresponding targets.
This review paper discussed the recent advancements in bottom-up approaches for
the HPE and listed the possible high-quality datasets used to train the models.
Additionally, a discussion of the prominent bottom-up approaches and their
quantitative results on the standard performance matrices are given. Finally,
the limitations of the existing methods are highlighted, and guidelines of the
future research directions are given.","['Milan KresoviāE, 'Thong Duy Nguyen']",2021-12-22T12:20:26Z,http://arxiv.org/abs/2112.11834v1,"['cs.CV', 'cs.AI']","multi-person pose estimation,bottom-up approaches,computer vision,human pose estimation,key points,targets,optimization,datasets,performance metrics,research directions"
"A Review of Deep Learning Techniques for Markerless Human Motion on
  Synthetic Datasets","Markerless motion capture has become an active field of research in computer
vision in recent years. Its extensive applications are known in a great variety
of fields, including computer animation, human motion analysis, biomedical
research, virtual reality, and sports science. Estimating human posture has
recently gained increasing attention in the computer vision community, but due
to the depth of uncertainty and the lack of the synthetic datasets, it is a
challenging task. Various approaches have recently been proposed to solve this
problem, many of which are based on deep learning. They are primarily focused
on improving the performance of existing benchmarks with significant advances,
especially 2D images. Based on powerful deep learning techniques and recently
collected real-world datasets, we explored a model that can predict the
skeleton of an animation based solely on 2D images. Frames generated from
different real-world datasets with synthesized poses using different body
shapes from simple to complex. The implementation process uses DeepLabCut on
its own dataset to perform many necessary steps, then use the input frames to
train the model. The output is an animated skeleton for human movement. The
composite dataset and other results are the ""ground truth"" of the deep model.","['Doan Duy Vo', 'Russell Butler']",2022-01-07T15:42:50Z,http://arxiv.org/abs/2201.02503v1,['cs.CV'],"deep learning,markerless motion capture,human motion analysis,computer vision,synthetic datasets,human posture,2D images,real-world datasets,DeepLabCut,animated skeleton"
"In-Device Feedback in Immersive Head-Mounted Displays for Distance
  Perception During Teleoperation of Unmanned Ground Vehicles","In recent years, Virtual Reality (VR) Head-Mounted Displays (HMD) have been
used to provide an immersive, first-person view in real-time for the
remote-control of Unmanned Ground Vehicles (UGV). One critical issue is that it
is challenging to perceive the distance of obstacles surrounding the vehicle
from 2D views in the HMD, which deteriorates the control of UGV. Conventional
distance indicators used in HMD take up screen space which leads clutter on the
display and can further reduce situation awareness of the physical environment.
To address the issue, in this paper we propose off-screen in-device feedback
using vibro-tactile and/or light-visual cues to provide real-time distance
information for the remote control of UGV. Results from a study show a
significantly better performance with either feedback type, reduced workload
and improved usability in a driving task that requires continuous perception of
the distance between the UGV and its environmental objects or obstacles. Our
findings show a solid case for in-device vibro-tactile and/or light-visual
feedback to support remote operation of UGVs that highly relies on distance
perception of objects.","['Yiming Luo', 'Jialin Wang', 'Rongkai Shi', 'Hai-Ning Liang', 'Shan Luo']",2022-01-09T15:26:50Z,http://arxiv.org/abs/2201.03036v1,['cs.HC'],"Immersive Head-Mounted Displays,Teleoperation,Unmanned Ground Vehicles,Distance Perception,In-Device Feedback"
VR Viewport Pose Model for Quantifying and Exploiting Frame Correlations,"The importance of the dynamics of the viewport pose, i.e., the location and
the orientation of users' points of view, for virtual reality (VR) experiences
calls for the development of VR viewport pose models. In this paper, informed
by our experimental measurements of viewport trajectories across 3 different
types of VR interfaces, we first develop a statistical model of viewport poses
in VR environments. Based on the developed model, we examine the correlations
between pixels in VR frames that correspond to different viewport poses, and
obtain an analytical expression for the visibility similarity (ViS) of the
pixels across different VR frames. We then propose a lightweight ViS-based
ALG-ViS algorithm that adaptively splits VR frames into the background and the
foreground, reusing the background across different frames. Our implementation
of ALG-ViS in two Oculus Quest 2 rendering systems demonstrates ALG-ViS running
in real time, supporting the full VR frame rate, and outperforming baselines on
measures of frame quality and bandwidth consumption.","['Ying Chen', 'Hojung Kwon', 'Hazer Inaltekin', 'Maria Gorlatova']",2022-01-11T17:05:00Z,http://arxiv.org/abs/2201.04060v2,"['eess.SY', 'cs.SY']","virtual reality,viewport pose,frame correlations,statistical model,VR frames,visibility similarity,ALG-ViS algorithm,Oculus Quest 2,frame rate"
"A Survey on Applications of Digital Human Avatars toward Virtual
  Co-presence","This paper investigates different approaches to build and use digital human
avatars toward interactive Virtual Co-presence (VCP) environments. We evaluate
the evolution of technologies for creating VCP environments and how the
advancement in Artificial Intelligence (AI) and Computer Graphics affect the
quality of VCP environments. We categorize different methods in the literature
based on their applications and methodology and compare various groups and
strategies based on their applications, contributions, and limitations. We also
have a brief discussion about the approaches that other forms of human
representation, rather than digital human avatars, have been utilized in VCP
environments. Our goal is to fill the gap in the research domain where there is
a lack of literature review investigating different approaches for creating
avatar-based VCP environments. We hope this study will be useful for future
research involving human representation in VCP or Virtual Reality (VR)
environments. To the best of our knowledge, it is the first survey research
that investigates avatar-based VCP environments. Specifically, the
categorization methodology suggested in this paper for avatar-based methods is
new.","['Matthew Korban', 'Xin Li']",2022-01-11T19:38:44Z,http://arxiv.org/abs/2201.04168v1,"['cs.HC', 'cs.AI']","digital human avatars,Virtual Co-presence,Artificial Intelligence,Computer Graphics,literature review,human representation,Virtual Reality,methodology,categorization,research domain"
A Review on Serious Games for Phobia,"Phobia is a widespread mental illness, and severe phobias can seriously
impact patients daily lives. One-session Exposure Treatment (OST) has been used
to treat phobias in the early days,but it has many disadvantages. As a new way
to treat a phobia, virtual reality exposure therapy(VRET) based on serious
games is introduced. There have been much researches in the field of serious
games for phobia therapy (SGPT), so this paper presents a detailed review of
SGPT from three perspectives. First, SGPT in different stages has different
forms with the update and iteration of technology. Therefore, we reviewed the
development history of SGPT from the perspective of equipment. Secondly, there
is no unified classification framework for a large number of SGPT. So we
classified and combed SGPT according to different types of phobias. Finally,
most articles on SGPT have studied the therapeutic effects of serious games
from a medical perspective, and few have studied serious games from a technical
perspective. Therefore, we conducted in-depth research on SGPT from a technical
perspective in order to provide technical guidance for the development of SGPT.
Accordingly, the challenges facing the existing technology has been explored
and listed.","['Sha Li', 'Peichen Yang', 'Rongyang Li', 'Fadi Farha', 'Jianguo Ding', 'Per Backlund', 'Huansheng Ning']",2022-01-15T02:26:18Z,http://arxiv.org/abs/2201.05738v1,['cs.HC'],"serious games,phobia,one-session exposure treatment,virtual reality exposure therapy,phobia therapy,equipment,classification framework,types of phobias,therapeutic effects,technical perspective"
Real-Time Gaze Tracking with Event-Driven Eye Segmentation,"Gaze tracking is increasingly becoming an essential component in Augmented
and Virtual Reality. Modern gaze tracking al gorithms are heavyweight; they
operate at most 5 Hz on mobile processors despite that near-eye cameras
comfortably operate at a r eal-time rate ($>$ 30 Hz). This paper presents a
real-time eye tracking algorithm that, on average, operates at 30 Hz on a
mobile processor, achieves \ang{0.1}--\ang{0.5} gaze accuracies, all the while
requiring only 30K parameters, one to two orders of magn itude smaller than
state-of-the-art eye tracking algorithms. The crux of our algorithm is an
Auto~ROI mode, which continuously pr edicts the Regions of Interest (ROIs) of
near-eye images and judiciously processes only the ROIs for gaze estimation. To
that end, we introduce a novel, lightweight ROI prediction algorithm by
emulating an event camera. We discuss how a software emulation of events
enables accurate ROI prediction without requiring special hardware. The code of
our paper is available at https://github.com/horizon-research/edgaze.","['Yu Feng', 'Nathan Goulding-Hotta', 'Asif Khan', 'Hans Reyserhove', 'Yuhao Zhu']",2022-01-19T00:46:16Z,http://arxiv.org/abs/2201.07367v1,['cs.HC'],"Real-Time Gaze Tracking,Event-Driven Eye Segmentation,Augmented Reality,Virtual Reality,Gaze Accuracies,Parameters,Regions of Interest (ROIs),Near-Eye Images,Event Camera."
Rate Splitting for General Multicast,"Immersive video, such as virtual reality (VR) and multi-view videos, is
growing in popularity. Its wireless streaming is an instance of general
multicast, extending conventional unicast and multicast, whose effective design
is still open. This paper investigates the optimization of general rate
splitting with linear beamforming for general multicast. Specifically, we
consider a multi-carrier single-cell wireless network where a multi-antenna
base station (BS) communicates to multiple single-antenna users via general
multicast. Linear beamforming is adopted at the BS, and joint decoding is
adopted at each user. We consider the maximization of the weighted sum rate,
which is a challenging nonconvex problem. Then, we propose an iterative
algorithm for the problem to obtain a KKT point using the concave-convex
procedure (CCCP). The proposed optimization framework generalizes the existing
ones for rate splitting for various types of services. Finally, we numerically
show substantial gains of the proposed solutions over existing schemes and
reveal the design insights of general rate splitting for general multicast.","['Lingzhi Zhao', 'Ying Cui', 'Sheng Yang', 'Shlomo Shamai', 'Yunbo Han', 'Yunfei Zhang']",2022-01-19T02:32:34Z,http://arxiv.org/abs/2201.07795v1,"['cs.IT', 'math.IT']","general multicast,rate splitting,linear beamforming,multi-carrier,single-cell,wireless network,multi-antenna,base station,joint decoding,weighted sum rate"
Naturalistic stimuli in touch research,"Neural mechanisms of touch are typically studied in laboratory settings using
robotic or other types of well-controlled devices. Such stimuli are very
different from highly complex naturalistic human-to-human touch interactions.
The lack of scientifically useful naturalistic stimuli hampers progress,
particularly in social touch research. Vision science, on the other hand, has
benefitted from inventions such as virtual reality systems that have provided
researchers with precision control of naturalistic stimuli. In the field of
touch research, producing and manipulating stimuli is particularly challenging
due to the complexity of skin mechanics. Here we review the history of touch
neuroscience focusing on the contrast between strictly controlled and
naturalistic stimuli and compare with vision science. We discuss new methods
that may overcome the obstacles with precision-controlled tactile stimuli, and
recent successes in naturalistic texture production. In social touch research,
precise tracking and measurement of naturalistic human-to-human touch
interactions offers exciting new possibilities.","['Anne Margarette S. Maallo', 'Basil Duvernoy', 'Håkan Olausson', 'Sarah McIntyre']",2022-01-26T14:01:51Z,http://arxiv.org/abs/2201.11868v2,['q-bio.NC'],"Naturalistic stimuli,Touch research,Neural mechanisms,Social touch,Vision science,Virtual reality systems,Skin mechanics,Touch neuroscience,Tactile stimuli,Human-to-human touch interactions"
"Will Metaverse be NextG Internet? Vision, Hype, and Reality","Metaverse, with the combination of the prefix ""meta"" (meaning transcending)
and the word ""universe"", has been deemed as the next-generation (NextG)
Internet. It aims to create a shared virtual space that connects all virtual
worlds via the Internet, where users, represented as digital avatars, can
communicate and collaborate as if they are in the physical world. Nevertheless,
there is still no unified definition of the Metaverse. This article first
presents our vision of what the key requirements of Metaverse should be and
reviews what has been heavily advocated by the industry and the positions of
various high-tech companies. It then briefly introduces existing social virtual
reality (VR) platforms that can be viewed as early prototypes of Metaverse and
conducts a reality check by diving into the network operation and performance
of two representative platforms, Workrooms from Meta and AltspaceVR from
Microsoft. Finally, it concludes by discussing several opportunities and future
directions for further innovation.","['Ruizhi Cheng', 'Nan Wu', 'Songqing Chen', 'Bo Han']",2022-01-30T19:04:43Z,http://arxiv.org/abs/2201.12894v2,"['cs.NI', 'cs.MM', 'cs.SI']","Metaverse,NextG Internet,virtual space,Internet,digital avatar,virtual reality,network operation,performance,social VR,innovation"
"PanoDepth: A Two-Stage Approach for Monocular Omnidirectional Depth
  Estimation","Omnidirectional 3D information is essential for a wide range of applications
such as Virtual Reality, Autonomous Driving, Robotics, etc. In this paper, we
propose a novel, model-agnostic, two-stage pipeline for omnidirectional
monocular depth estimation. Our proposed framework PanoDepth takes one 360
image as input, produces one or more synthesized views in the first stage, and
feeds the original image and the synthesized images into the subsequent stereo
matching stage. In the second stage, we propose a differentiable Spherical
Warping Layer to handle omnidirectional stereo geometry efficiently and
effectively. By utilizing the explicit stereo-based geometric constraints in
the stereo matching stage, PanoDepth can generate dense high-quality depth. We
conducted extensive experiments and ablation studies to evaluate PanoDepth with
both the full pipeline as well as the individual modules in each stage. Our
results show that PanoDepth outperforms the state-of-the-art approaches by a
large margin for 360 monocular depth estimation.","['Yuyan Li', 'Zhixin Yan', 'Ye Duan', 'Liu Ren']",2022-02-02T23:08:06Z,http://arxiv.org/abs/2202.01323v1,['cs.CV'],"Monocular,Omnidirectional,Depth Estimation,Two-Stage Approach,Stereo Matching,Spherical Warping Layer,Geometric Constraints,Virtual Reality,Autonomous Driving"
"Deep Impulse Responses: Estimating and Parameterizing Filters with Deep
  Networks","Impulse response estimation in high noise and in-the-wild settings, with
minimal control of the underlying data distributions, is a challenging problem.
We propose a novel framework for parameterizing and estimating impulse
responses based on recent advances in neural representation learning. Our
framework is driven by a carefully designed neural network that jointly
estimates the impulse response and the (apriori unknown) spectral noise
characteristics of an observed signal given the source signal. We demonstrate
robustness in estimation, even under low signal-to-noise ratios, and show
strong results when learning from spatio-temporal real-world speech data. Our
framework provides a natural way to interpolate impulse responses on a spatial
grid, while also allowing for efficiently compressing and storing them for
real-time rendering applications in augmented and virtual reality.","['Alexander Richard', 'Peter Dodds', 'Vamsi Krishna Ithapu']",2022-02-07T18:57:23Z,http://arxiv.org/abs/2202.03416v1,"['cs.SD', 'cs.LG', 'eess.AS']","Impulse response,Filtering,Deep networks,Neural representation learning,Signal processing,Signal-to-noise ratio,Spatial grid,Real-time rendering,Virtual reality"
Prototyping a Virtual Agent for Pre-school English Teaching,"This paper describes a case study and the insights gained from prototyping an
Intelligent Virtual Agent (IVA) for English vocabulary building for
Spanish-speaking preschool children. After an initial exploration to evaluate
the feasibility of developing an IVA, we followed a Human-Centered Design (HCD)
approach to create a prototype. We report on the multidisciplinary process used
that incorporated two well-known educative concepts: gamification and
story-telling as the main components for engagement. Our results suggest that a
multidisciplinary approach to developing an educational IVA is effective. We
report on the relevant aspects of the ideation and design processes that
informed the vision and mission of the project.","['Eduardo Benitez Sandoval', 'Diego Vazquez Rojas', 'Clarissa A. Parada Cereceres', 'Alvaro Anzueto Rios', 'Amit Barde', 'Mark Billinghurst']",2022-02-08T07:26:21Z,http://arxiv.org/abs/2202.06946v1,"['cs.HC', 'I.3.8; K.3.1']","Virtual agent,Prototyping,Pre-school,English teaching,Intelligent Virtual Agent (IVA),Human-Centered Design (HCD),Gamification,Story-telling,Educational,Ideation"
OKVIS2: Realtime Scalable Visual-Inertial SLAM with Loop Closure,"Robust and accurate state estimation remains a challenge in robotics,
Augmented, and Virtual Reality (AR/VR), even as Visual-Inertial Simultaneous
Localisation and Mapping (VI-SLAM) getting commoditised. Here, a full VI-SLAM
system is introduced that particularly addresses challenges around long as well
as repeated loop-closures. A series of experiments reveals that it achieves and
in part outperforms what state-of-the-art open-source systems achieve. At the
core of the algorithm sits the creation of pose-graph edges through
marginalisation of common observations, which can fluidly be turned back into
landmarks and observations upon loop-closure. The scheme contains a realtime
estimator optimising a bounded-size factor graph consisting of observations,
IMU pre-integral error terms, and pose-graph edges -- and it allows for
optimisation of larger loops re-using the same factor-graph asynchronously when
needed.",['Stefan Leutenegger'],2022-02-18T13:53:43Z,http://arxiv.org/abs/2202.09199v2,['eess.IV'],"Visual-Inertial SLAM,Loop closure,State estimation,Robotics,Augmented Reality,Virtual Reality,Pose-graph edges,Factor graph,IMU pre-integrated error terms,Realtime estimator"
"A Dynamic Model of a Skydiver With Validation in Wind Tunnel and Free
  Fall","An innovative approach of gaining insight into motor skills involved in human
body flight is proposed. The key idea is the creation of a model autonomous
system capable of virtually performing skydiving maneuvers. A dynamic skydiver
model and simulator is developed, comprising biomechanical, aerodynamic, and
kinematic models, dynamic equations of motion, and a virtual reality
environment. Limb relative orientations, and resulting inertial body angular
position and velocity are measured in skydiving experiments in a vertical wind
tunnel and in free fall. These experimental data are compared with
corresponding simulation data to tune and verify the model for basic skydiving
maneuvers. The model is further extended to reconstruct advanced aerial
maneuvers, such as transitions between stable equilibria. The experimental data
are used to estimate skydiver's conscious inputs as a function of time, via an
Unscented Kalman Filter modified for this purpose.","['Anna Clarke', 'Per-Olof Gutman']",2022-02-16T17:27:46Z,http://arxiv.org/abs/2202.10233v1,"['cs.RO', 'cs.SY', 'eess.SY']","dynamic model,skydiver,wind tunnel,free fall,biomechanical model,aerodynamic model,kinematic model,virtual reality,inertial body,Unscented Kalman Filter"
Evoking realistic affective touch experiences in virtual reality,"This study aims to better understand the emotional and physiological
correlates of being caressed in VR depending on the type of multisensory
feedback provided and the animate or inanimate nature of the virtual
representation that touches an embodied virtual body. We evaluated how
pleasure, arousal, embodiment, and the illusion of being touched in VR were
influenced by the inclusion of only visual feedback compared to visuotactile
stimulation conditions, where participants, in addition to seeing an avatar or
feather caressing their virtual bodies, also perceived congruent mid-air
ultrasonic tactile stimulation or real interpersonal touch. We found that
visuotactile feedback, either based on ultrasound or real interpersonal touch,
boosts the illusion of being affectively touched and embodied in a virtual body
compared to conditions only based on visual feedback. However, real
interpersonal touch led to the strongest behavioral and emotional responses
compared to the other conditions. Moreover, arousal and the desire to withdraw
the caressed hand was highest when being touched by a female avatar compared to
a virtual feather. Female participants reported a stronger illusion of being
caressed in VR compared to males. Overall, this study advances knowledge of the
emotional and physiological impact of affective touch in VR.","['Sofia Seinfeld', 'Ivette Schmidt', 'Jörg Müller']",2022-02-27T16:09:23Z,http://arxiv.org/abs/2202.13389v1,"['cs.HC', '68', 'J.4']","virtual reality,multisensory feedback,visuotactile stimulation,ultrasonic tactile stimulation,interpersonal touch,embodiment,illusion,emotional response,physiological impact,affective touch"
Improving X-ray Diagnostics through Eye-Tracking and XR,"There is a growing need to assist radiologists in performing X-ray readings
and diagnoses fast, comfortably, and effectively. As radiologists strive to
maximize productivity, it is essential to consider the impact of reading rooms
in interpreting complex examinations and ensure that higher volume and
reporting speeds do not compromise patient outcomes. Virtual Reality (VR) is a
disruptive technology for clinical practice in assessing X-ray images. We argue
that conjugating eye-tracking with VR devices and Machine Learning may overcome
obstacles posed by inadequate ergonomic postures and poor room conditions that
often cause erroneous diagnostics when professionals examine digital images.","['Catarina Moreira', 'Isabel Blanco Nobre', 'Sandra Costa Sousa', 'João Madeiras Pereira', 'Joaquim Jorge']",2022-03-03T11:04:41Z,http://arxiv.org/abs/2203.01643v1,"['cs.HC', 'cs.LG']","X-ray diagnostics,eye-tracking,XR,radiologists,reading rooms,Virtual Reality,Machine Learning,ergonomic postures,digital images"
Anatomy Studio II: A Cross-Reality Application for Teaching Anatomy,"Virtual Reality has become an important educational tool, due to the pandemic
and increasing globalization of education. This paper presents a framework for
teaching Virtual Anatomy at the university level. Virtual classes have become a
staple of today's curricula because of the isolation and quarantine
requirements and the increased international collaboration. Our work builds on
the Visible Human Projects for Virtual Dissection material and provides a
medium for groups of students to do collaborative anatomical dissections in
real-time using sketching and 3D visualizations and audio coupled with
interactive 2D tablets for precise drawing. We describe the system
architecture, compare requirements with those of previous development, and
discuss the preliminary results. Discussions with Anatomists show that this is
an effective tool. We introduce avenues for further research and discuss
collaboration challenges posed by this context.","['Joaquim Jorge', 'Pedro Belchior', 'Abel Gomes', 'Maurício Sousa', 'João Pereira', 'Jean-François Uhl']",2022-03-04T08:37:53Z,http://arxiv.org/abs/2203.02186v1,"['cs.HC', 'I.3; J.3']","Virtual Reality,Anatomy,Education,Visible Human Project,Virtual Dissection,3D Visualization,Interactive Tablets,System Architecture,Collaborative Dissections"
High Speed Emulation in a Vehicle-in-the-Loop Driving Simulator,"Rendering accurate multisensory feedback is critical to ensure natural user
behavior in driving simulators. In this work, we present a virtual reality
(VR)-based Vehicle-in-the-Loop (ViL) simulator that provides visual,
vestibular, and haptic feedback to drivers in high speed driving conditions.
Designing our simulator around a four-wheel steer-by-wire vehicle enables us to
emulate the dynamics of a vehicle traveling significantly faster than the test
vehicle and to transmit corresponding haptic steering feedback to the driver.
By scaling the speed of the test vehicle through a combination of VR visuals,
vehicle dynamics emulation, and steering wheel force feedback, we can safely
and immersively run experiments up to highway speeds within a limited driving
space. In double lane change and highway weaving experiments, our high speed
emulation method tracks yaw motion within human perception limits and provides
sensory feedback comparable to the same maneuvers driven manually.","['Elliot Weiss', 'J. Christian Gerdes']",2022-03-06T20:27:57Z,http://arxiv.org/abs/2203.03043v2,"['eess.SY', 'cs.RO', 'cs.SY']","High speed emulation,Vehicle-in-the-Loop,Driving simulator,Virtual reality,Haptic feedback,Dynamics emulation,Steering feedback,Yaw motion,Highway speeds,Vehicle dynamics"
L2CS-Net: Fine-Grained Gaze Estimation in Unconstrained Environments,"Human gaze is a crucial cue used in various applications such as human-robot
interaction and virtual reality. Recently, convolution neural network (CNN)
approaches have made notable progress in predicting gaze direction. However,
estimating gaze in-the-wild is still a challenging problem due to the
uniqueness of eye appearance, lightning conditions, and the diversity of head
pose and gaze directions. In this paper, we propose a robust CNN-based model
for predicting gaze in unconstrained settings. We propose to regress each gaze
angle separately to improve the per-angel prediction accuracy, which will
enhance the overall gaze performance. In addition, we use two identical losses,
one for each angle, to improve network learning and increase its
generalization. We evaluate our model with two popular datasets collected with
unconstrained settings. Our proposed model achieves state-of-the-art accuracy
of 3.92{\deg} and 10.41{\deg} on MPIIGaze and Gaze360 datasets, respectively.
We make our code open source at https://github.com/Ahmednull/L2CS-Net.","['Ahmed A. Abdelrahman', 'Thorsten Hempel', 'Aly Khalifa', 'Ayoub Al-Hamadi']",2022-03-07T12:35:39Z,http://arxiv.org/abs/2203.03339v1,"['cs.CV', 'cs.LG', 'cs.RO']","Fine-grained gaze estimation,L2CS-Net,Unconstrained environments,Convolutional neural network,Gaze direction,Gaze angle,Loss function,Generalization,MPIIGaze dataset,Gaze360 dataset"
Metaverse: Security and Privacy Concerns,"The term ""metaverse"", a three-dimensional virtual universe similar to the
real realm, has always been full of imagination since it was put forward in the
1990s. Recently, it is possible to realize the metaverse with the continuous
emergence and progress of various technologies, and thus it has attracted
extensive attention again. It may bring a lot of benefits to human society such
as reducing discrimination, eliminating individual differences, and
socializing. However, everything has security and privacy concerns, which is no
exception for the metaverse. In this article, we firstly analyze the concept of
the metaverse and propose that it is a super virtual-reality (VR) ecosystem
compared with other VR technologies. Then, we carefully analyze and elaborate
on possible security and privacy concerns from four perspectives: user
information, communication, scenario, and goods, and immediately, the potential
solutions are correspondingly put forward. Meanwhile, we propose the need to
take advantage of the new buckets effect to comprehensively address security
and privacy concerns from a philosophical perspective, which hopefully will
bring some progress to the metaverse community.","['Ruoyu Zhao', 'Yushu Zhang', 'Youwen Zhu', 'Rushi Lan', 'Zhongyun Hua']",2022-03-08T05:06:13Z,http://arxiv.org/abs/2203.03854v3,['cs.CY'],"Metaverse,Security,Privacy Concerns,Virtual reality,User information,Communication,Scenario,Goods,Solutions,New buckets effect"
A Survey on Reinforcement Learning Methods in Character Animation,"Reinforcement Learning is an area of Machine Learning focused on how agents
can be trained to make sequential decisions, and achieve a particular goal
within an arbitrary environment. While learning, they repeatedly take actions
based on their observation of the environment, and receive appropriate rewards
which define the objective. This experience is then used to progressively
improve the policy controlling the agent's behavior, typically represented by a
neural network. This trained module can then be reused for similar problems,
which makes this approach promising for the animation of autonomous, yet
reactive characters in simulators, video games or virtual reality environments.
This paper surveys the modern Deep Reinforcement Learning methods and discusses
their possible applications in Character Animation, from skeletal control of a
single, physically-based character to navigation controllers for individual
agents and virtual crowds. It also describes the practical side of training DRL
systems, comparing the different frameworks available to build such agents.","['Ariel Kwiatkowski', 'Eduardo Alvarado', 'Vicky Kalogeiton', 'C. Karen Liu', 'Julien Pettré', 'Michiel van de Panne', 'Marie-Paule Cani']",2022-03-07T23:39:00Z,http://arxiv.org/abs/2203.04735v1,"['cs.GR', 'cs.LG']","Reinforcement Learning,Machine Learning,Sequential decisions,Environment,Actions,Rewards,Policy,Neural network,Character Animation,Deep Reinforcement Learning"
"Triangular Character Animation Sampling with Motion, Emotion, and
  Relation","Dramatic progress has been made in animating individual characters. However,
we still lack automatic control over activities between characters, especially
those involving interactions. In this paper, we present a novel energy-based
framework to sample and synthesize animations by associating the characters'
body motions, facial expressions, and social relations. We propose a
Spatial-Temporal And-Or graph (ST-AOG), a stochastic grammar model, to encode
the contextual relationship between motion, emotion, and relation, forming a
triangle in a conditional random field. We train our model from a labeled
dataset of two-character interactions. Experiments demonstrate that our method
can recognize the social relation between two characters and sample new scenes
of vivid motion and emotion using Markov Chain Monte Carlo (MCMC) given the
social relation. Thus, our method can provide animators with an automatic way
to generate 3D character animations, help synthesize interactions between
Non-Player Characters (NPCs), and enhance machine emotion intelligence (EQ) in
virtual reality (VR).","['Yizhou Zhao', 'Liang Qiu', 'Wensi Ai', 'Pan Lu', 'Song-Chun Zhu']",2022-03-09T18:19:03Z,http://arxiv.org/abs/2203.04930v1,"['cs.GR', 'cs.CV']","character animation,sampling,motion,emotion,relation,Spatial-Temporal And-Or graph,stochastic grammar model,conditional random field,Markov Chain Monte Carlo"
Information-Theoretic Odometry Learning,"In this paper, we propose a unified information theoretic framework for
learning-motivated methods aimed at odometry estimation, a crucial component of
many robotics and vision tasks such as navigation and virtual reality where
relative camera poses are required in real time. We formulate this problem as
optimizing a variational information bottleneck objective function, which
eliminates pose-irrelevant information from the latent representation. The
proposed framework provides an elegant tool for performance evaluation and
understanding in information-theoretic language. Specifically, we bound the
generalization errors of the deep information bottleneck framework and the
predictability of the latent representation. These provide not only a
performance guarantee but also practical guidance for model design, sample
collection, and sensor selection. Furthermore, the stochastic latent
representation provides a natural uncertainty measure without the needs for
extra structures or computations. Experiments on two well-known odometry
datasets demonstrate the effectiveness of our method.","['Sen Zhang', 'Jing Zhang', 'Dacheng Tao']",2022-03-11T02:37:35Z,http://arxiv.org/abs/2203.05724v2,['cs.CV'],"information theoretic,odometry learning,robotics,vision tasks,navigation,virtual reality,variational information bottleneck,deep information bottleneck,latent representation"
Online Continual Learning for Embedded Devices,"Real-time on-device continual learning is needed for new applications such as
home robots, user personalization on smartphones, and augmented/virtual reality
headsets. However, this setting poses unique challenges: embedded devices have
limited memory and compute capacity and conventional machine learning models
suffer from catastrophic forgetting when updated on non-stationary data
streams. While several online continual learning models have been developed,
their effectiveness for embedded applications has not been rigorously studied.
In this paper, we first identify criteria that online continual learners must
meet to effectively perform real-time, on-device learning. We then study the
efficacy of several online continual learning methods when used with mobile
neural networks. We measure their performance, memory usage, compute
requirements, and ability to generalize to out-of-domain inputs.","['Tyler L. Hayes', 'Christopher Kanan']",2022-03-21T00:23:09Z,http://arxiv.org/abs/2203.10681v3,"['cs.LG', 'cs.AI']","Online Continual Learning,Embedded Devices,Real-time Learning,Memory Usage,Compute Capacity,Machine Learning Models,Catastrophic Forgetting,Data Streams,Neural Networks,Generalization."
Software Rasterization of 2 Billion Points in Real Time,"We propose a software rasterization pipeline for point clouds that is capable
of brute-force rendering up to two billion points in real time (60fps).
Improvements over the state of the art are achieved by batching points in a way
that a number of batch-level optimizations can be computed before rasterizing
the points within the same rendering pass. These optimizations include frustum
culling, level-of-detail rendering, and choosing the appropriate coordinate
precision for a given batch of points directly within a compute workgroup.
Adaptive coordinate precision, in conjunction with visibility buffers, reduces
the number of loaded bytes for the majority of points down to 4, thus making
our approach several times faster than the bandwidth-limited state of the art.
Furthermore, support for LOD rendering makes our software-rasterization
approach suitable for rendering arbitrarily large point clouds, and to meet the
increased performance demands of virtual reality rendering.","['Markus Schütz', 'Bernhard Kerbl', 'Michael Wimmer']",2022-04-04T07:41:43Z,http://arxiv.org/abs/2204.01287v1,['cs.GR'],"Software rasterization,Point clouds,Real time,Brute-force rendering,Batch-level optimizations,Frustum culling,Level-of-detail rendering,Coordinate precision,Visibility buffers,LOD rendering"
From PHY to QoE: A Parameterized Framework Design,"The rapid development of 5G communication technology has given birth to
various real-time broadband communication services, such as augmented reality
(AR), virtual reality (VR) and cloud games. Compared with traditional services,
consumers tend to focus more on their subjective experience when utilizing
these services. In the meantime, the problem of power consumption is
particularly prominent in 5G and beyond. The traditional design of physical
layer (PHY) receiver is based on maximizing spectrum efficiency or minimizing
error, but this will no longer be the best after considering energy efficiency
and these new-coming services. Therefore, this paper uses quality of experience
(QoE) as the optimization criterion of the PHY algorithm. In order to establish
the relationship between PHY and QoE, this paper models the end-to-end
transmission from UE perspective and proposes a five-layer framework based on
hierarchical analysis method, which includes system-level model, bitstream
model, packet model, service quality model and experience quality model. Real
data in 5G network is used to train the parameters of the involved models for
each type of services, respectively. The results show that the PHY algorithms
can be simplified in perspective of QoE.","['Hao Wang', 'Lei Ji', 'Zhenxing Gao']",2022-04-08T03:54:58Z,http://arxiv.org/abs/2204.03828v1,"['cs.IT', 'cs.MM', 'cs.PF', 'math.IT']","5G communication,broadband communication,augmented reality (AR),virtual reality (VR),cloud games,power consumption,physical layer (PHY),quality of experience (QoE),optimization criterion,hierarchical analysis."
"Information and Communication Technology in Migration: A Framework for
  Applications, Customization, and Research","This paper addresses the role of Information and Communication Technology
(ICT) in migration governance, support, and experience with particular
attention to emerging technologies such as artificial intelligence, social
media, and virtual reality. We propose a framework for technology use based on
user groups and process types. We provide examples of using emerging
technologies for migration-related tasks within the context of this framework.
We then identify how such technologies can be applied to migration-related
tasks, developed for customized use, and improved through research to add new
features that can help different migration stakeholders. We suggest a series of
possible directions for future research and development to take advantage of
specific affordances of those emerging technologies more effectively.","['Ali Arya', 'Luciara Nardon', 'Md Riyadh']",2022-04-13T19:02:42Z,http://arxiv.org/abs/2204.06611v1,['cs.CY'],"Information and Communication Technology,Migration Governance,Emerging Technologies,Artificial Intelligence,Social Media,Virtual Reality,User Groups,Process Types,Customized Use,Future Research"
Facing the Illusion and Reality of Safety in Social VR,"The ethical design of social Virtual Reality (VR) is not a new topic, but
""safety"" concerns of using social VR are escalated to a different level given
the heat of the Metaverse. For example, it was reported that nearly half of the
female-identifying VR participants have had at least one instance of virtual
sexual harassment. Feeling safe is a basic human right - in any place,
regardless in real or virtual spaces. In this paper, we are seeking to
understand the discrepancy between user concerns and designs in protecting user
safety in social VR applications. We study safety concerns on social VR
experience first by analyzing Twitter posts and then synthesize practices on
safety protection adopted by four mainstream social VR platforms. We argue that
future research and platforms should explore the design of social VR with
boundary-awareness.","['Qingxiao Zheng', 'Tue Ngoc Do', 'Lingqing Wang', 'Yun Huang']",2022-04-14T17:21:58Z,http://arxiv.org/abs/2204.07121v2,['cs.HC'],"Social VR,Safety,Virtual Reality,Metaverse,Virtual sexual harassment,User concerns,User safety,Twitter posts,Safety protection,Boundary-awareness"
"Bandwidth bounds for wide-field-of-view dispersion-engineered achromatic
  metalenses","Optical systems with wide field-of-views (FOV) are crucial for many
applications such as high performance imaging, optical projection,
augmented/virtual reality, and miniaturized medical imaging tools. Typically,
aberration-free imaging with a wide FOV is achieved by stacking multiple
refractive lenses (as in a ""fisheye"" lens), adding to the size and weight of
the optical system. Single metalenses designed to have a wide FOV have the
potential to replace these bulky imaging systems and, moreover, they may be
dispersion engineered for spectrally broadband operation. In this paper, we
derive a fundamental bound on the spectral bandwidth of dispersion-engineered
wide-FOV achromatic metalenses. We show that for metalenses with a relatively
large numerical aperture (NA), there is a tradeoff between the maximum
achievable bandwidth and the FOV; interestingly, however, the bandwidth
reduction saturates beyond a certain FOV that depends on the NA of the
metalens. These findings may provide important information and insights for the
design of future wide-FOV achromatic flat lenses.","['Kunal Shastri', 'Francesco Monticone']",2022-04-19T23:04:37Z,http://arxiv.org/abs/2204.09154v1,['physics.optics'],"wide-field-of-view,dispersion-engineered,achromatic,metalenses,aberration-free imaging,refractive lenses,spectrally broadband,numerical aperture,bandwidth,flat lenses"
Adversarial Attention for Human Motion Synthesis,"Analysing human motions is a core topic of interest for many disciplines,
from Human-Computer Interaction, to entertainment, Virtual Reality and
healthcare. Deep learning has achieved impressive results in capturing human
pose in real-time. On the other hand, due to high inter-subject variability,
human motion analysis models often suffer from not being able to generalise to
data from unseen subjects due to very limited specialised datasets available in
fields such as healthcare. However, acquiring human motion datasets is highly
time-consuming, challenging, and expensive. Hence, human motion synthesis is a
crucial research problem within deep learning and computer vision. We present a
novel method for controllable human motion synthesis by applying
attention-based probabilistic deep adversarial models with end-to-end training.
We show that we can generate synthetic human motion over both short- and
long-time horizons through the use of adversarial attention. Furthermore, we
show that we can improve the classification performance of deep learning models
in cases where there is inadequate real data, by supplementing existing
datasets with synthetic motions.","['Matthew Malek-Podjaski', 'Fani Deligianni']",2022-04-25T16:12:42Z,http://arxiv.org/abs/2204.11751v1,"['cs.CV', 'cs.LG']","Adversarial Attention,Human Motion Synthesis,Deep Learning,Human Pose,Inter-subject Variability,Specialized Datasets,End-to-End Training,Synthetic Motion,Deep Adversarial Models"
"Foveated Rendering: Motivation, Taxonomy, and Research Directions","With the recent interest in virtual reality and augmented reality, there is a
newfound demand for displays that can provide high resolution with a wide field
of view (FOV). However, such displays incur significantly higher costs for
rendering the larger number of pixels. This poses the challenge of rendering
realistic real-time images that have a wide FOV and high resolution using
limited computing resources. The human visual system does not need every pixel
to be rendered at a uniformly high quality. Foveated rendering methods provide
perceptually high-quality images while reducing computational workload and are
becoming a crucial component for large-scale rendering. In this paper, we
present key motivations, research directions, and challenges for leveraging the
limitations of the human visual system as they relate to foveated rendering. We
provide a taxonomy to compare and contrast various foveated techniques based on
key factors. We also review aliasing artifacts arising due to foveation methods
and discuss several approaches that attempt to mitigate such effects. Finally,
we present several open problems and possible future research directions that
can further reduce computational costs while generating perceptually
high-quality renderings.","['Susmija Jabbireddy', 'Xuetong Sun', 'Xiaoxu Meng', 'Amitabh Varshney']",2022-05-09T19:48:01Z,http://arxiv.org/abs/2205.04529v2,['cs.GR'],"Foveated Rendering,Virtual Reality,Augmented Reality,Field of View (FOV),Real-Time Images,Human Visual System,Computational Workload,Rendering Methods,Aliasing Artifacts,Open Problems"
"VRCockpit: Mitigating Simulator Sickness in VR Games Using Multiple
  Egocentric 2D View Frames","Virtual reality head-mounted displays (VR HMDs) have become a popular
platform for gaming. However, simulator sickness (SS) is still an impediment to
VR's wider adoption, particularly in gaming. It can induce strong discomfort
and impair players' immersion, performance, and enjoyment. Researchers have
explored techniques to mitigate SS. While these techniques have been shown to
help lessen SS, they may not be applicable to games because they cannot be
easily integrated into various types of games without impacting gameplay,
immersion, and performance. In this research, we introduce a new SS mitigation
technique, VRCockpit. VRCockpit is a visual technique that surrounds the player
with four 2D views, one for each cardinal direction, that show 2D copies of the
areas of the 3D environment around the player. To study its effectiveness, we
conducted two different experiments, one with a car racing game, followed by a
first-person shooter game. Our results show that VRCockpit has the potential to
mitigate SS and still allows players to have the same level of immersion and
gameplay performance.","['Hao Chen', 'Rongkai Shi', 'Diego Monteiro', 'Nilufar Baghaei', 'Hai-Ning Liang']",2022-05-14T11:36:11Z,http://arxiv.org/abs/2205.07041v2,['cs.HC'],"Virtual reality,simulator sickness,VRCockpit,2D view frames,egocentric,gaming,immersion,performance,gameplay,effectiveness"
"Design and implementation of brain surgery bipolar electrocautery
  simulator using haptic technology","Surgical simulators have been widely used in training and evaluation of
physicians and surgeons. Virtual reality augmented with haptic technology has
made it feasible to develop more realistic surgical simulators. In this
context, we set out to design and develop a brain surgery bipolar
electrocautery simulator using haptic technology. A 3D model of brain tissue
was generated based on a brain craniotomy image. Bipolar forceps were also
modeled to visually assimilate real forceps. An experiment was developed to
assess the learning process of the participants. In this experiment, the
volunteers were asked to cauterize a large blood vessel in the brain while
minimizing the damage done to the brain tissue. The experiment was performed on
20 volunteers, and statistical analysis was conducted on the learning process
and error reduction during the surgery. Next, the volunteers were divided into
gamer and non gamer groups. The analysis of the volunteers operation
demonstrated that, on average, there was a 5 percent reduction in the
percentage of applied force error. It was also shown that the results achieved
by the gamer and non gamer group has significant difference with a p value of
0.0001. So, playing computer games would increase hand control, focus, and
reflex and positively affect surgery skills.","['Reza Karimzadeh', 'Javad Sheikh', 'Hamed Azarnoush', 'Hossein Arabi']",2022-05-18T15:31:56Z,http://arxiv.org/abs/2205.08999v1,['physics.med-ph'],"brain surgery,bipolar electrocautery,simulator,haptic technology,3D model,forceps,surgical training,virtual reality,statistical analysis,hand control"
Deep Learning for Omnidirectional Vision: A Survey and New Perspectives,"Omnidirectional image (ODI) data is captured with a 360x180 field-of-view,
which is much wider than the pinhole cameras and contains richer spatial
information than the conventional planar images. Accordingly, omnidirectional
vision has attracted booming attention due to its more advantageous performance
in numerous applications, such as autonomous driving and virtual reality. In
recent years, the availability of customer-level 360 cameras has made
omnidirectional vision more popular, and the advance of deep learning (DL) has
significantly sparked its research and applications. This paper presents a
systematic and comprehensive review and analysis of the recent progress in DL
methods for omnidirectional vision. Our work covers four main contents: (i) An
introduction to the principle of omnidirectional imaging, the convolution
methods on the ODI, and datasets to highlight the differences and difficulties
compared with the 2D planar image data; (ii) A structural and hierarchical
taxonomy of the DL methods for omnidirectional vision; (iii) A summarization of
the latest novel learning strategies and applications; (iv) An insightful
discussion of the challenges and open problems by highlighting the potential
research directions to trigger more research in the community.","['Hao Ai', 'Zidong Cao', 'Jinjing Zhu', 'Haotian Bai', 'Yucheng Chen', 'Lin Wang']",2022-05-21T00:19:56Z,http://arxiv.org/abs/2205.10468v2,"['cs.CV', '68T45', 'I.4.0']","Omnidirectional vision,Deep learning,360 cameras,Field-of-view,Spatial information,Convolution methods,Datasets,Taxonomy,Learning strategies"
"sat2pc: Estimating Point Cloud of Building Roofs from 2D Satellite
  Images","Three-dimensional (3D) urban models have gained interest because of their
applications in many use-cases such as urban planning and virtual reality.
However, generating these 3D representations requires LiDAR data, which are not
always readily available. Thus, the applicability of automated 3D model
generation algorithms is limited to a few locations. In this paper, we propose
sat2pc, a deep learning architecture that predicts the point cloud of a
building roof from a single 2D satellite image. Our architecture combines
Chamfer distance and EMD loss, resulting in better 2D to 3D performance. We
extensively evaluate our model and perform ablation studies on a building roof
dataset. Our results show that sat2pc was able to outperform existing baselines
by at least 18.6%. Further, we show that the predicted point cloud captures
more detail and geometric characteristics than other baselines.","['Yoones Rezaei', 'Stephen Lee']",2022-05-25T03:24:40Z,http://arxiv.org/abs/2205.12464v1,"['cs.CV', 'cs.AI', 'cs.LG']","point cloud,building roofs,2D satellite images,deep learning,LiDAR data,3D model generation,Chamfer distance,EMD loss,ablation studies,geometric characteristics"
"Increasing Fault Tolerance and Throughput with Adaptive Control Plane in
  Smart Factories","Future smart factories are expected to deploy an emerging dynamic Virtual
Reality (VR) applications with high bandwidth wireless connections in the THz
communication bands, where a factory worker can follow activities through
360{\deg}video streams with high quality resolution. THz communications, while
promising as a high bandwidth wireless communication technology, are however
known for low fault tolerance, and are sensible to external factors. Since THz
channel states are in general hard to estimate, what is needed is a system that
can adaptively react to transceiver configurations in terms of coding and
modulation. To this end, we propose an adaptive control plane that can help us
configure the THz communication system. The control plane implements a workflow
algorithm designed to adaptively choose between various coding and modulation
schemes depending on THz channel states. The results show that an adaptive
control plane can improve throughput and signal resolution quality, with
theoretically zeroed bit error probability and a maximum achievable throughput
in the scenarios analayzed.","['Cao Vien Phung', 'Admela Jukan']",2022-05-25T21:35:08Z,http://arxiv.org/abs/2205.13057v1,['cs.NI'],"Fault tolerance,Throughput,Adaptive control plane,Smart factories,Virtual Reality applications,THz communications,Coding,Modulation,Channel states,Bit error probability."
COFS: Controllable Furniture layout Synthesis,"Scalable generation of furniture layouts is essential for many applications
in virtual reality, augmented reality, game development and synthetic data
generation. Many existing methods tackle this problem as a sequence generation
problem which imposes a specific ordering on the elements of the layout making
such methods impractical for interactive editing or scene completion.
Additionally, most methods focus on generating layouts unconditionally and
offer minimal control over the generated layouts. We propose COFS, an
architecture based on standard transformer architecture blocks from language
modeling. The proposed model is invariant to object order by design, removing
the unnatural requirement of specifying an object generation order.
Furthermore, the model allows for user interaction at multiple levels enabling
fine grained control over the generation process. Our model consistently
outperforms other methods which we verify by performing quantitative
evaluations. Our method is also faster to train and sample from, compared to
existing methods.","['Wamiq Reyaz Para', 'Paul Guerrero', 'Niloy Mitra', 'Peter Wonka']",2022-05-29T13:31:18Z,http://arxiv.org/abs/2205.14657v1,"['cs.CV', 'cs.GR', 'cs.LG']","furniture layout synthesis,virtual reality,augmented reality,game development,synthetic data generation,transformer architecture,user interaction,generation process,quantitative evaluations,training速"
"Sharing Construction Safety Inspection Experiences and Site-Specific
  Knowledge through XR-Augmented Visual Assistance","Early identification of on-site hazards is crucial for accident prevention in
the construction industry. Currently, the construction industry relies on
experienced safety advisors (SAs) to identify site hazards and generate
mitigation measures to guide field workers. However, more than half of the site
hazards remain unrecognized due to the lack of field experience or
site-specific knowledge of some SAs. To address these limitations, this study
proposed an Extended Reality (XR)-augmented visual assistance framework,
including Virtual Reality (VR) and Augmented Reality (AR), that enables
capturing and transferring subconscious inspection strategies between workers
or workers/machines for a construction safety inspection. The purpose is to
enhance SA's training and real-time situational awareness for identifying
on-site hazards while reducing their mental workloads.","['Pengkun Liu', 'Jinding Xing', 'Ruoxin Xiong', 'Pingbo Tang']",2022-05-31T14:37:22Z,http://arxiv.org/abs/2205.15833v1,['cs.HC'],"construction industry,safety inspection,XR,augmented visual assistance,Virtual Reality,Augmented Reality,site-specific knowledge,accident prevention,on-site hazards"
"Performance Study of Low Inertia Magnetorheological Actuators for
  Kinesthetic Haptic Devices","A challenge to high quality virtual reality (VR) simulations is the
development of high-fidelity haptic devices that can render a wide range of
impedances at both low and high frequencies. To this end, a thorough analytical
and experimental assessment of the performance of magnetorheological (MR)
actuators is performed and compared to electric motor (EM) actuation. A 2
degrees-of-freedom dynamic model of a kinesthetic haptic device is used to
conduct the analytical study comparing the rendering area, rendering bandwidth,
gearing and scaling of both technologies. Simulation predictions are
corroborated by experimental validation over a wide range of operating
conditions. Results show that, for a same output force, MR actuators can render
a bandwidth over 52.9% higher than electric motors due to their low inertia.
Unlike electric motors, the performance of MR actuators for use in haptic
devices are not limited by their output inertia but by their viscous damping,
which must be carefully addressed at the design stage.","['Louis-Philippe Lebel', 'Jean-Alexis Verreault', 'Jean-Philippe Lucking Bigué', 'Jean-Sébastien Plante', 'Alexandre Girard']",2022-06-01T16:23:39Z,http://arxiv.org/abs/2206.00607v1,['cs.RO'],"Low inertia,Magnetorheological actuators,Kinesthetic haptic devices,High fidelity,Virtual reality,Impedances,Analytical study,Experimental assessment,Electric motors,Viscous damping"
"Compilation and Optimizations for Efficient Machine Learning on Embedded
  Systems","Deep Neural Networks (DNNs) have achieved great success in a variety of
machine learning (ML) applications, delivering high-quality inferencing
solutions in computer vision, natural language processing, and virtual reality,
etc. However, DNN-based ML applications also bring much increased computational
and storage requirements, which are particularly challenging for embedded
systems with limited compute/storage resources, tight power budgets, and small
form factors. Challenges also come from the diverse application-specific
requirements, including real-time responses, high-throughput performance, and
reliable inference accuracy. To address these challenges, we introduce a series
of effective design methodologies, including efficient ML model designs,
customized hardware accelerator designs, and hardware/software co-design
strategies to enable efficient ML applications on embedded systems.","['Xiaofan Zhang', 'Yao Chen', 'Cong Hao', 'Sitao Huang', 'Yuhong Li', 'Deming Chen']",2022-06-06T02:54:05Z,http://arxiv.org/abs/2206.03326v2,"['cs.LG', 'cs.AR']","Compilation,Optimizations,Machine Learning,Embedded Systems,Deep Neural Networks,DNNs,Computational,Storage,Power Budgets,Hardware Accelerator"
"Interaction Design for VR Applications: Understanding Needs for
  University Curricula","As virtual reality (VR) is emerging in the tech sector, developers and
designers are under pressure to create immersive experiences for their
products. However, the current curricula from top institutions focus primarily
on technical considerations for building VR applications, missing out on
concerns and usability problems specific to VR interaction design. To better
understand current needs, we examined the status quo of existing university
pedagogies by carrying out a content analysis of undergraduate and graduate
courses about VR and related areas offered in the major citadels of learning
and conducting interviews with 7 industry experts. Our analysis reveals that
the current teaching practices underemphasize design thinking, prototyping, and
evaluation skills, while focusing on technical implementation. We recommend VR
curricula should emphasize design principles and guidelines, offer training in
prototyping and ideation, prioritize practical design exercises while providing
industry insights, and encourage students to solve VR design problems beyond
the classroom.","['Oloff C. Biermann', 'Daniel Ajisafe', 'Dongwook Yoon']",2022-06-09T09:55:59Z,http://arxiv.org/abs/2206.04386v1,"['cs.HC', 'cs.CY', 'cs.ET', 'K.3.2; H.5.1']","Interaction Design,VR Applications,University Curricula,Curricula Analysis,Design Thinking,Prototyping,Evaluation Skills,Design Principles,Guidelines,Ideation"
SparseFormer: Attention-based Depth Completion Network,"Most pipelines for Augmented and Virtual Reality estimate the ego-motion of
the camera by creating a map of sparse 3D landmarks. In this paper, we tackle
the problem of depth completion, that is, densifying this sparse 3D map using
RGB images as guidance. This remains a challenging problem due to the low
density, non-uniform and outlier-prone 3D landmarks produced by SfM and SLAM
pipelines. We introduce a transformer block, SparseFormer, that fuses 3D
landmarks with deep visual features to produce dense depth. The SparseFormer
has a global receptive field, making the module especially effective for depth
completion with low-density and non-uniform landmarks. To address the issue of
depth outliers among the 3D landmarks, we introduce a trainable refinement
module that filters outliers through attention between the sparse landmarks.","['Frederik Warburg', 'Michael Ramamonjisoa', 'Manuel López-Antequera']",2022-06-09T15:08:24Z,http://arxiv.org/abs/2206.04557v1,['cs.CV'],"attention,depth completion,network,3D landmarks,RGB images,transformer block,global receptive field,outliers,trainable refinement module"
"i-FlatCam: A 253 FPS, 91.49 $μ$J/Frame Ultra-Compact Intelligent
  Lensless Camera for Real-Time and Efficient Eye Tracking in VR/AR","We present a first-of-its-kind ultra-compact intelligent camera system,
dubbed i-FlatCam, including a lensless camera with a computational (Comp.)
chip. It highlights (1) a predict-then-focus eye tracking pipeline for boosted
efficiency without compromising the accuracy, (2) a unified compression scheme
for single-chip processing and improved frame rate per second (FPS), and (3)
dedicated intra-channel reuse design for depth-wise convolutional layers
(DW-CONV) to increase utilization. i-FlatCam demonstrates the first eye
tracking pipeline with a lensless camera and achieves 3.16 degrees of accuracy,
253 FPS, 91.49 $\mu$J/Frame, and 6.7mm x 8.9mm x 1.2mm camera form factor,
paving the way for next-generation Augmented Reality (AR) and Virtual Reality
(VR) devices.","['Yang Zhao', 'Ziyun Li', 'Yonggan Fu', 'Yongan Zhang', 'Chaojian Li', 'Cheng Wan', 'Haoran You', 'Shang Wu', 'Xu Ouyang', 'Vivek Boominathan', 'Ashok Veeraraghavan', 'Yingyan Lin']",2022-06-15T08:55:55Z,http://arxiv.org/abs/2206.08141v2,['cs.AR'],"i-FlatCam,lensless camera,computational chip,eye tracking,pipeline,frame rate per second (FPS),compression scheme,depth-wise convolutional layers (DW-CONV),accuracy,form factor"
"Method for detector description transformation to Unity and application
  in BESIII","Detector and event visualization are essential parts of the software used in
high-energy physics (HEP) experiments. Modern visualization techniques and
multimedia production platforms such as Unity provide impressive display
effects and professional extensions for visualization in HEP experiments. In
this study, a method for automatic detector description transformation is
presented, which can convert the complicated HEP detector geometry from GDML in
offline software to 3D modeling in Unity. The method was successfully applied
in the BESIII experiment and can be further developed into applications such as
event displays, data monitoring, or virtual reality. It has great potential in
detector design, offline software development, physics analysis, and outreach
for next-generation HEP experiments as well as applications in nuclear
techniques for the industry.","['Kai-Xuan Huang', 'Zhi-Jun Li', 'Zhen Qian', 'Jiang Zhu', 'Hao-Yuan Li', 'Yu-Mei Zhang', 'Sheng-Sen Sun', 'Zheng-Yun You']",2022-06-21T05:25:03Z,http://arxiv.org/abs/2206.10117v2,['physics.ins-det'],"detector description,transformation,Unity,BESIII,visualization,high-energy physics,GDML,3D modeling,event displays,virtual reality"
"BlazePose GHUM Holistic: Real-time 3D Human Landmarks and Pose
  Estimation","We present BlazePose GHUM Holistic, a lightweight neural network pipeline for
3D human body landmarks and pose estimation, specifically tailored to real-time
on-device inference. BlazePose GHUM Holistic enables motion capture from a
single RGB image including avatar control, fitness tracking and AR/VR effects.
Our main contributions include i) a novel method for 3D ground truth data
acquisition, ii) updated 3D body tracking with additional hand landmarks and
iii) full body pose estimation from a monocular image.","['Ivan Grishchenko', 'Valentin Bazarevsky', 'Andrei Zanfir', 'Eduard Gabriel Bazavan', 'Mihai Zanfir', 'Richard Yee', 'Karthik Raveendran', 'Matsvei Zhdanovich', 'Matthias Grundmann', 'Cristian Sminchisescu']",2022-06-23T13:09:58Z,http://arxiv.org/abs/2206.11678v1,['cs.CV'],"neural network,3D,human landmarks,pose estimation,real-time,on-device inference,motion capture,RGB image,hand landmarks,monocular image"
Feature Refinement to Improve High Resolution Image Inpainting,"In this paper, we address the problem of degradation in inpainting quality of
neural networks operating at high resolutions. Inpainting networks are often
unable to generate globally coherent structures at resolutions higher than
their training set. This is partially attributed to the receptive field
remaining static, despite an increase in image resolution. Although downscaling
the image prior to inpainting produces coherent structure, it inherently lacks
detail present at higher resolutions. To get the best of both worlds, we
optimize the intermediate featuremaps of a network by minimizing a multiscale
consistency loss at inference. This runtime optimization improves the
inpainting results and establishes a new state-of-the-art for high resolution
inpainting. Code is available at:
https://github.com/geomagical/lama-with-refiner/tree/refinement.","['Prakhar Kulshreshtha', 'Brian Pugh', 'Salma Jiddi']",2022-06-27T21:59:12Z,http://arxiv.org/abs/2206.13644v2,"['cs.CV', 'eess.IV']","high resolution,image inpainting,neural networks,degradation,receptive field,downscaling,featuremaps,multiscale consistency loss,state-of-the-art"
"GazBy: Gaze-Based BERT Model to Incorporate Human Attention in Neural
  Information Retrieval","This paper is interested in investigating whether human gaze signals can be
leveraged to improve state-of-the-art search engine performance and how to
incorporate this new input signal marked by human attention into existing
neural retrieval models. In this paper, we propose GazBy ({\bf Gaz}e-based {\bf
B}ert model for document relevanc{\bf y}), a light-weight joint model that
integrates human gaze fixation estimation into transformer models to predict
document relevance, incorporating more nuanced information about cognitive
processing into information retrieval (IR). We evaluate our model on the Text
Retrieval Conference (TREC) Deep Learning (DL) 2019 and 2020 Tracks. Our
experiments show encouraging results and illustrate the effective and
ineffective entry points for using human gaze to help with transformer-based
neural retrievers. With the rise of virtual reality (VR) and augmented reality
(AR), human gaze data will become more available. We hope this work serves as a
first step exploring using gaze signals in modern neural search engines.","['Sibo Dong', 'Justin Goldstein', 'Grace Hui Yang']",2022-07-04T18:50:48Z,http://arxiv.org/abs/2207.01674v1,['cs.IR'],"GazBy,Gaze-based,BERT model,neural information retrieval,human attention,gaze signals,transformer models,document relevance,cognitive processing,information retrieval"
NeuralPassthrough: Learned Real-Time View Synthesis for VR,"Virtual reality (VR) headsets provide an immersive, stereoscopic visual
experience, but at the cost of blocking users from directly observing their
physical environment. Passthrough techniques are intended to address this
limitation by leveraging outward-facing cameras to reconstruct the images that
would otherwise be seen by the user without the headset. This is inherently a
real-time view synthesis challenge, since passthrough cameras cannot be
physically co-located with the eyes. Existing passthrough techniques suffer
from distracting reconstruction artifacts, largely due to the lack of accurate
depth information (especially for near-field and disoccluded objects), and also
exhibit limited image quality (e.g., being low resolution and monochromatic).
In this paper, we propose the first learned passthrough method and assess its
performance using a custom VR headset that contains a stereo pair of RGB
cameras. Through both simulations and experiments, we demonstrate that our
learned passthrough method delivers superior image quality compared to
state-of-the-art methods, while meeting strict VR requirements for real-time,
perspective-correct stereoscopic view synthesis over a wide field of view for
desktop-connected headsets.","['Lei Xiao', 'Salah Nouri', 'Joel Hegland', 'Alberto Garcia Garcia', 'Douglas Lanman']",2022-07-05T17:39:22Z,http://arxiv.org/abs/2207.02186v1,"['cs.CV', 'cs.LG']","Real-Time View Synthesis,Virtual Reality,Passthrough Techniques,Neural Networks,Stereo Pair,Depth Information,Image Quality,VR Headset,Perspective-Correct"
Perceptual Quality Assessment of Omnidirectional Images,"Omnidirectional images and videos can provide immersive experience of
real-world scenes in Virtual Reality (VR) environment. We present a perceptual
omnidirectional image quality assessment (IQA) study in this paper since it is
extremely important to provide a good quality of experience under the VR
environment. We first establish an omnidirectional IQA (OIQA) database, which
includes 16 source images and 320 distorted images degraded by 4 commonly
encountered distortion types, namely JPEG compression, JPEG2000 compression,
Gaussian blur and Gaussian noise. Then a subjective quality evaluation study is
conducted on the OIQA database in the VR environment. Considering that humans
can only see a part of the scene at one movement in the VR environment, visual
attention becomes extremely important. Thus we also track head and eye movement
data during the quality rating experiments. The original and distorted
omnidirectional images, subjective quality ratings, and the head and eye
movement data together constitute the OIQA database. State-of-the-art
full-reference (FR) IQA measures are tested on the OIQA database, and some new
observations different from traditional IQA are made.","['Huiyu Duan', 'Guangtao Zhai', 'Xiongkuo Min', 'Yucheng Zhu', 'Yi Fang', 'Xiaokang Yang']",2022-07-06T13:40:38Z,http://arxiv.org/abs/2207.02674v1,['cs.CV'],"Perceptual Quality Assessment,Omnidirectional Images,Virtual Reality,IQA Study,OIQA Database,JPEG Compression,JPEG2000 Compression,Gaussian Blur,Gaussian Noise,Full-Reference IQA Measures"
Adaptive Virtual Neuroarchitecture,"Our surrounding environment impacts our cognitive-emotional processes on a
daily basis and shapes our physical, psychological and social wellbeing.
Although the effects of the built environment on our psycho-physiological
processes are well studied, virtual environment design with a potentially
similar impact on the user, has received limited attention. Based on the
influence of space design on a user and combining that with the dynamic
affordances of virtual spaces, we present the idea of adaptive virtual
neuroarchitecture (AVN), where virtual environments respond to the user and the
user's real world context while simultaneously influencing them both in
realtime. To show how AVN has been explored in current research, we present a
sampling of recent work that demonstrates reciprocal relationships using
physical affordances (space, objects), the user's state (physiological,
cognitive, emotional), and the virtual world used in the design of novel
virtual reality experiences. We believe AVN has the potential to help us learn
how to design spaces and environments that can enhance the wellbeing of their
inhabitants.","['Abhinandan Jain', 'Pattie Maes', 'Misha Sra']",2022-07-10T17:14:37Z,http://arxiv.org/abs/2207.04508v1,['cs.HC'],"adaptive,virtual neuroarchitecture,virtual environment design,psycho-physiological processes,virtual spaces,dynamic affordances,real world context,physical affordances,virtual reality,virtual environments"
"The Confluence of Blockchain and 6G Network: Scenarios Analysis and
  Performance Assessment","Emerging advanced applications, such as smart cities, healthcare, and virtual
reality, demand more challenging requirements on sixth-generation (6G) mobile
networks, including the need for improved secrecy, greater integrity,
non-repudiation, authentication, and access control. While blockchain, with its
intrinsic features, is generally regarded as one of the most disruptive
technological enablers for 6G functional standards, there is no comprehensive
study of whether, when, and how blockchain will be used in 6G scenarios.
Existing research lacks performance assessment methodology for the use of
blockchain in 6G scenarios. Therefore, we abstract seven fine-grained 6G
possibilities from the application layer and investigate the why, what, and
when issues for 6G scenarios in this work. Moreover, we provide a methodology
for evaluating the performance and scalability of blockchain-based 6G
scenarios. In conclusion, we undertake comprehensive experimental to assess the
performance of the Quorum blockchain and 6G scenarios. The experimental results
show that a consortium blockchain with the proper settings may satisfy the
performance and scalability requiremen","['Bo Li', 'Shuiguang Deng', 'Xueqiang Yan', 'Schahram Dustdar']",2022-07-11T10:05:07Z,http://arxiv.org/abs/2207.04744v1,['cs.DC'],"Blockchain,6G network,Scenarios analysis,Performance assessment,Smart cities,Healthcare,Virtual reality,Secrecy,Integrity,Authentication"
SLAM Backends with Objects in Motion: A Unifying Framework and Tutorial,"Simultaneous Localization and Mapping (SLAM) algorithms are frequently
deployed to support a wide range of robotics applications, such as autonomous
navigation in unknown environments, and scene mapping in virtual reality. Many
of these applications require autonomous agents to perform SLAM in highly
dynamic scenes. To this end, this tutorial extends a recently introduced,
unifying optimization-based SLAM backend framework to environments with moving
objects and features. Using this framework, we consider a rapprochement of
recent advances in dynamic SLAM. Moreover, we present dynamic EKF SLAM: a
novel, filtering-based dynamic SLAM algorithm generated from our framework, and
prove that it is mathematically equivalent to a direct extension of the
classical EKF SLAM algorithm to the dynamic environment setting. Empirical
results with simulated data indicate that dynamic EKF SLAM can achieve high
localization and mobile object pose estimation accuracy, as well as high map
precision, with high efficiency.",['Chih-Yuan Chiu'],2022-07-11T17:52:49Z,http://arxiv.org/abs/2207.05043v7,"['cs.RO', 'cs.SY', 'eess.SY']","SLAM,Robotics,Localization,Mapping,Optimization,Dynamic SLAM,EKF SLAM,Autonomous Navigation,Moving Objects"
Development of VR Teaching System for Engine Dis-assembly,"With the worldwide ravaging of the covid-19 epidemic, the traditional
face-to-face education systems have been interrupted frequently. It is demanded
to develop high-quality online education modalities. The webcasting based
online classroom is one of the popular education modalities but suffers from
poor teacher-student interactions and and low immersive learning experiences.
This thesis aims to improve the online education quality by using the virtual
reality (VR) technology. For the purpose of automobile engine education, we
develop a VR based engine maintenance learning system. The system includes many
teaching and learning components in VR enabled by the Unity engine. Users can
immersively experience the complete engine disassembly process through the
wearable VR display and interactive devices. The system is designed with an
interactive layer, a control layer, and a physical data layer. Such a system
architecture effectively separates the specific implementations of different
domains and improves the R&D efficiency. Once new object models and process
profiles are provided, the proposed system architecture requires no
modification of codes for changed learning objects and processes. The
efficiency and effictiveness of the proposed method are verfied by various
experiments. The developed techniques can be useful for many other
applications.",['Zhuochen Xiong'],2022-07-12T02:27:41Z,http://arxiv.org/abs/2207.05265v1,['cs.HC'],"VR,Teaching System,Engine Dis-assembly,Online Education,Virtual Reality,Unity engine,Immersive Learning,Interactive Devices,System Architecture,R&D Efficiency"
HoloLens 2 Technical Evaluation as Mixed Reality Guide,"Mixed Reality (MR) is an evolving technology lying in the continuum spanned
by related technologies such as Virtual Reality (VR) and Augmented Reality
(AR), and creates an exciting way of interacting with people and the
environment. This technology is fast becoming a tool used by many people,
potentially improving living environments and work efficiency. Microsoft
HoloLens has played an important role in the progress of MR, from the first
generation to the second generation. In this paper, we systematically evaluate
the functions of applicable functions in HoloLens 2. These evaluations can
serve as a performance benchmark that can help people who need to use this
instrument for research or applications in the future. The detailed tests and
the performance evaluation of the different functionalities show the usability
and possible limitations of each function. We mainly divide the experiment into
the existing functions of the HoloLens 1, the new functions of the HoloLens 2,
and the use of research mode. This research results will be useful for MR
researchers who want to use HoloLens 2 as a research tool to design their own
MR applications.","['Hung-Jui Guo', 'Balakrishnan Prabhakaran']",2022-07-19T21:19:23Z,http://arxiv.org/abs/2207.09554v1,['cs.HC'],"HoloLens 2,Mixed Reality,Virtual Reality,Augmented Reality,Microsoft,functions,performance evaluation,research mode,limitations,MR applications"
"A Pilot Study on The Impact of Stereoscopic Display Type on User
  Interactions Within A Immersive Analytics Environment","Immersive Analytics (IA) and consumer adoption of augmented reality (AR) and
virtual reality (VR) head-mounted displays (HMDs) are both rapidly growing.
When used in conjunction, stereoscopic IA environments can offer improved user
understanding and engagement; however, it is unclear how the choice of
stereoscopic display impacts user interactions within an IA environment. This
paper presents a pilot study that examines the impact of stereoscopic display
type on object manipulation and environmental navigation using
consumer-available AR and VR displays. This work finds that the display type
can impact how users manipulate virtual content, how they navigate the
environment, and how able they are to answer questions about the represented
data.","['Adam S. Williams', 'Xiaoyan Zhou', 'Michel Pahud', 'Francisco R. Ortega']",2022-07-25T22:23:53Z,http://arxiv.org/abs/2207.12558v1,['cs.HC'],"Immersive Analytics,Stereoscopic Display,User Interactions,Augmented Reality,Virtual Reality,Head-Mounted Displays,Object Manipulation,Environmental Navigation,Consumer Adoption"
Exploring the Privacy Risks of Adversarial VR Game Design,"Fifty study participants playtested an innocent-looking ""escape room"" game in
virtual reality (VR). Within just a few minutes, an adversarial program had
accurately inferred over 25 of their personal data attributes, from
anthropometrics like height and wingspan to demographics like age and gender.
As notoriously data-hungry companies become increasingly involved in VR
development, this experimental scenario may soon represent a typical VR user
experience. Since the Cambridge Analytica scandal of 2018, adversarially
designed gamified elements have been known to constitute a significant privacy
threat in conventional social platforms. In this work, we present a case study
of how metaverse environments can similarly be adversarially constructed to
covertly infer dozens of personal data attributes from seemingly anonymous
users. While existing VR privacy research largely focuses on passive
observation, we argue that because individuals subconsciously reveal personal
information via their motion in response to specific stimuli, active attacks
pose an outsized risk in VR environments.","['Vivek Nair', 'Gonzalo Munilla Garrido', 'Dawn Song', ""James F. O'Brien""]",2022-07-26T20:48:48Z,http://arxiv.org/abs/2207.13176v4,['cs.CR'],"privacy risks,adversarial,VR game design,personal data,anthropometrics,demographics,gamified elements,privacy threat,metaverse,passive observation"
Polarization-decoupled Flat Displays,"Many modern applications like entertainment displays, data encryption,
security, and virtual reality (VR) technology require asymmetric light
manipulation. Symmetric spin-orbit interactions (SOI) apply a limit in
achieving an asymmetrical metahologram. However, different reported asymmetric
SOI's based on propagation and geometric phase mergence techniques effectively
break this limit at the expense of design complexity and greater computation
cost. This work proposes a novel helicity multiplexing technique that breaks
all the aforementioned barriers in achieving on-axis dual side holograms.
Benefiting from the geometric phase modulation of anisotropic nano-resonating
antennas, we have employed a single unit cell to achieve helicity multiplexing.
A low extinction coefficient material a-Si:H is used for device analysis. Due
to the simple single unit cell-based designing technique, simulation and
fabrication complexities were significantly reduced. As a result, based on the
helicity and incidence direction of electromagnetic wave, we have achieved
highly transmissive dual holographic images in the visible band. Our simulated
efficiencies are 55%, 75%, and 80% for the blue ({\lambda} = 488 nm), green
({\lambda} = 532 nm), and red light ({\lambda} = 633 nm).","['Isma Javed', 'Muhammad Ashar Naveed', 'Muhammad Qasim Mehmood', 'Yehia Massoud']",2022-07-27T22:04:48Z,http://arxiv.org/abs/2207.13810v1,['physics.optics'],"Polarization-decoupled,Flat Displays,Spin-orbit interactions,Metahologram,Helicity multiplexing,Geometric phase,Nano-resonating antennas,Extinction coefficient,Simulation,Fabrication"
"IPDAE: Improved Patch-Based Deep Autoencoder for Lossy Point Cloud
  Geometry Compression","Point cloud is a crucial representation of 3D contents, which has been widely
used in many areas such as virtual reality, mixed reality, autonomous driving,
etc. With the boost of the number of points in the data, how to efficiently
compress point cloud becomes a challenging problem. In this paper, we propose a
set of significant improvements to patch-based point cloud compression, i.e., a
learnable context model for entropy coding, octree coding for sampling centroid
points, and an integrated compression and training process. In addition, we
propose an adversarial network to improve the uniformity of points during
reconstruction. Our experiments show that the improved patch-based autoencoder
outperforms the state-of-the-art in terms of rate-distortion performance, on
both sparse and large-scale point clouds. More importantly, our method can
maintain a short compression time while ensuring the reconstruction quality.","['Kang You', 'Pan Gao', 'Qing Li']",2022-08-04T08:12:35Z,http://arxiv.org/abs/2208.02519v1,"['cs.CV', 'cs.IT', 'cs.MM', 'eess.IV', 'math.IT']","point cloud,deep autoencoder,geometry compression,entropy coding,octree coding,adversarial network,rate-distortion performance,reconstruction quality,compression time"
"Intensity-Adjustable Non-contact Cold Sensation Presentation Based on
  the Vortex Effect","Cold sensations of varying intensities are perceived when human skin is
subject to diverse environments. The accurate presentation of temperature
changes is important to elicit immersive sensations in applications such as
virtual reality. We developed a method to elicit intensity-adjustable
non-contact cold sensations based on the vortex effect. We applied this effect
to generate cold air at approximately 0 {\deg}C and varied the skin temperature
over a wide range. The perception of different temperatures can be elicited by
adjusting the volume flow rate of the cold air. Additionally, we introduced a
cooling model to relate the changes in skin temperature to various parameters
such as the cold air volume flow rate and distance from the cold air outlet to
the skin. For validation, we conducted measurement experiments and found that
our model can estimate the change in skin temperature with a root mean-square
error of 0.16 {\deg}C. Furthermore, we evaluated the performance of a prototype
in psychophysical cold discrimination experiments based on the discrimination
threshold. Thus, cold sensations of varying intensities can be generated by
varying the parameters. These cold sensations can be combined with images,
sounds, and other stimuli to create an immersive and realistic artificial
environment.","['Jiayi Xu', 'Shunsuke Yoshimoto', 'Naoto Ienaga', 'Yoshihiro Kuroda']",2022-08-28T14:55:46Z,http://arxiv.org/abs/2208.13229v1,['cs.HC'],"non-contact,cold sensation,vortex effect,temperature changes,immersive sensations,intensity-adjustable,cold air,skin temperature,volume flow rate,cooling model"
"When Internet of Things meets Metaverse: Convergence of Physical and
  Cyber Worlds","In recent years, the Internet of Things (IoT) is studied in the context of
the Metaverse to provide users immersive cyber-virtual experiences in mixed
reality environments. This survey introduces six typical IoT applications in
the Metaverse, including collaborative healthcare, education, smart city,
entertainment, real estate, and socialization. In the IoT-inspired Metaverse,
we also comprehensively survey four pillar technologies that enable augmented
reality (AR) and virtual reality (VR), namely, responsible artificial
intelligence (AI), high-speed data communications, cost-effective mobile edge
computing (MEC), and digital twins. According to the physical-world demands, we
outline the current industrial efforts and seven key requirements for building
the IoT-inspired Metaverse: immersion, variety, economy, civility,
interactivity, authenticity, and independence. In addition, this survey
describes the open issues in the IoT-inspired Metaverse, which need to be
addressed to eventually achieve the convergence of physical and cyber worlds.","['Kai Li', 'Yingping Cui', 'Weicai Li', 'Tiejun Lv', 'Xin Yuan', 'Shenghong Li', 'Wei Ni', 'Meryem Simsek', 'Falko Dressler']",2022-08-29T11:17:54Z,http://arxiv.org/abs/2208.13501v1,['cs.NI'],"Internet of Things,Metaverse,immersive cyber-virtual experiences,mixed reality environments,IoT applications,augmented reality,virtual reality,responsible artificial intelligence,high-speed data communications,mobile edge computing"
"LinkGlide-S: A Wearable Multi-Contact Tactile Display Aimed at Rendering
  Object Softness at the Palm with Impedance Control in VR and Telemanipulation","LinkGlide-S is a novel wearable hand-worn tactile display to deliver
multi-contact and multi-modal stimuli at the user's palm.} The array of
inverted five-bar linkages generates three independent contact points to cover
the whole palm area. \textcolor{black} {The independent contact points generate
various tactile patterns at the user's hand, providing multi-contact tactile
feedback. An impedance control delivers the stiffness of objects according to
different parameters. Three experiments were performed to evaluate the
perception of patterns, investigate the realistic perception of object
interaction in Virtual Reality, and assess the users' softness perception by
the impedance control. The experimental results revealed a high recognition
rate for the generated patterns. These results confirm that the performance of
LinkGlide-S is adequate to detect and manipulate virtual objects with different
stiffness. This novel haptic device can potentially achieve a highly immersive
VR experience and more interactive applications during telemanipulation.","['Miguel Altamirano Cabrera', 'Jonathan Tirado', 'Juan Heredia', 'Dzmitry Tsetserukou']",2022-08-30T11:09:00Z,http://arxiv.org/abs/2208.14149v1,['cs.HC'],"Wearable,Tactile Display,Multi-Contact,Impedance Control,Virtual Reality,Telemanipulation,Haptic Device,Object Softness,Stiffness"
Being at Home in the Metaverse? Prospectus for a Social Imaginary,"The metaverse has seen growing corporate and popular interest over the past
few years. While visions vary, the metaverse is generally seen as an extension
of the internet that may be developed through advances in a number of digital
technologies, such as augmented and virtual reality, as well as new technical
infrastructure and standards. The metaverse constitutes an emerging social
imaginary, a way of both understanding and directing our shared existence. This
paper examines this emerging social imaginary through the phenomenological
concept of dwelling, or being at home in the world, as developed by Martin
Heidegger. To examine in depth one influential articulation of this social
imaginary, this paper focuses on the metaverse as envisioned by Mark
Zuckerberg, CEO of Meta (formerly Facebook). The paper presents a thematic
analysis of Zuckerberg's public statements regarding the metaverse to provide a
close reading of this particular vision. Then, through the lens of Heidegger's
philosophy of dwelling, this paper identifies numerous threats to dwelling
posed by the metaverse social imaginary. This paper explains these threats and
their prognoses, and it closes with some considerations for how the metaverse
could be designed to better facilitate human dwelling.",['Tim Gorichanaz'],2022-09-01T16:03:16Z,http://arxiv.org/abs/2209.00559v1,"['cs.CY', 'cs.HC']","metaverse,social imaginary,internet,digital technologies,augmented reality,virtual reality,technical infrastructure,phenomenological concept,dwelling,Mark Zuckerberg"
Impact of 4ir technology and its impact on the current deployment,"The Fourth Industrial Revolution represents a fundamental change in how we
live, work, and relate to one another. It is a new chapter in human development
with remarkable technological advancements comparable to those of the first,
second, and third industrial revolutions. These developments are fusing the
physical, digital, and biological worlds in ways that hold great promise as
well as the possibility of great danger. The way that modern people live and
work is changing as a result of disruptive technologies and trends including
the Internet of Things (IoT), robotics, virtual reality (VR), and artificial
intelligence (AI). This is known as the fourth industrial revolution. Industry
4.0 refers to the incorporation of these technologies into production
processes. In this article, we discussed the history of 4IR technology, its
impact of 4IR technology, and its impact on the current deployment.","['Bandar Alsulaimani', 'Amanul Islam']",2022-09-05T07:01:22Z,http://arxiv.org/abs/2209.01791v1,"['cs.CR', 'cs.GT']","4IR technology,Fourth Industrial Revolution,technological advancements,Internet of Things,robotics,virtual reality,artificial intelligence,Industry 4.0,production processes"
"Neural3Points: Learning to Generate Physically Realistic Full-body
  Motion for Virtual Reality Users","Animating an avatar that reflects a user's action in the VR world enables
natural interactions with the virtual environment. It has the potential to
allow remote users to communicate and collaborate in a way as if they met in
person. However, a typical VR system provides only a very sparse set of up to
three positional sensors, including a head-mounted display (HMD) and optionally
two hand-held controllers, making the estimation of the user's full-body
movement a difficult problem. In this work, we present a data-driven
physics-based method for predicting the realistic full-body movement of the
user according to the transformations of these VR trackers and simulating an
avatar character to mimic such user actions in the virtual world in real-time.
We train our system using reinforcement learning with carefully designed
pretraining processes to ensure the success of the training and the quality of
the simulation. We demonstrate the effectiveness of the method with an
extensive set of examples.","['Yongjing Ye', 'Libin Liu', 'Lei Hu', 'Shihong Xia']",2022-09-13T06:33:59Z,http://arxiv.org/abs/2209.05753v1,['cs.GR'],"avatar,virtual reality,motion,VR system,positional sensors,physics-based method,reinforcement learning,simulation,training,simulation-quality"
CU-Net: Real-Time High-Fidelity Color Upsampling for Point Clouds,"Point cloud upsampling is essential for high-quality augmented reality,
virtual reality, and telepresence applications, due to the capture, processing,
and communication limitations of existing technologies. Although geometry
upsampling to densify a point cloud's coordinates has been well studied, the
upsampling of the color attributes has been largely overlooked. In this paper,
we propose CU-Net, the first deep-learning point cloud color upsampling model
that enables low latency and high visual fidelity operation. CU-Net achieves
linear time and space complexity by leveraging a feature extractor based on
sparse convolution and a color prediction module based on neural implicit
function. Therefore, CU-Net is theoretically guaranteed to be more efficient
than most existing methods with quadratic complexity. Experimental results
demonstrate that CU-Net can colorize a photo-realistic point cloud with nearly
a million points in real time, while having notably better visual performance
than baselines. Besides, CU-Net can adapt to arbitrary upsampling ratios and
unseen objects without retraining. Our source code is available at
https://github.com/UMass-LIDS/cunet.","['Lingdong Wang', 'Mohammad Hajiesmaili', 'Jacob Chakareski', 'Ramesh K. Sitaraman']",2022-09-12T04:22:09Z,http://arxiv.org/abs/2209.06112v2,['cs.CV'],"point cloud,upsampling,color attributes,deep-learning,sparse convolution,neural implicit function,real time,visual fidelity,source code"
Self-supervised Multi-Modal Video Forgery Attack Detection,"Video forgery attack threatens the surveillance system by replacing the video
captures with unrealistic synthesis, which can be powered by the latest augment
reality and virtual reality technologies. From the machine perception aspect,
visual objects often have RF signatures that are naturally synchronized with
them during recording. In contrast to video captures, the RF signatures are
more difficult to attack given their concealed and ubiquitous nature. In this
work, we investigate multimodal video forgery attack detection methods using
both vision and wireless modalities. Since wireless signal-based human
perception is environmentally sensitive, we propose a self-supervised training
strategy to enable the system to work without external annotation and thus can
adapt to different environments. Our method achieves a perfect human detection
accuracy and a high forgery attack detection accuracy of 94.38% which is
comparable with supervised methods.","['Chenhui Zhao', 'Xiang Li', 'Rabih Younes']",2022-09-13T23:41:26Z,http://arxiv.org/abs/2209.06345v2,['cs.MM'],"self-supervised,multi-modal,video forgery,attack detection,RF signatures,wireless modalities,training strategy,human detection,forgery attack detection"
"End-to-End Multi-View Structure-from-Motion with Hypercorrelation
  Volumes","Image-based 3D reconstruction is one of the most important tasks in Computer
Vision with many solutions proposed over the last few decades. The objective is
to extract metric information i.e. the geometry of scene objects directly from
images. These can then be used in a wide range of applications such as film,
games, virtual reality, etc. Recently, deep learning techniques have been
proposed to tackle this problem. They rely on training on vast amounts of data
to learn to associate features between images through deep convolutional neural
networks and have been shown to outperform traditional procedural techniques.
In this paper, we improve on the state-of-the-art two-view
structure-from-motion(SfM) approach of [11] by incorporating 4D correlation
volume for more accurate feature matching and reconstruction. Furthermore, we
extend it to the general multi-view case and evaluate it on the complex
benchmark dataset DTU [4]. Quantitative evaluations and comparisons with
state-of-the-art multi-view 3D reconstruction methods demonstrate its
superiority in terms of the accuracy of reconstructions.","['Qiao Chen', 'Charalambos Poullis']",2022-09-14T20:58:44Z,http://arxiv.org/abs/2209.06926v1,['cs.CV'],"structure-from-motion,multi-view,hypercorrelation volumes,deep learning,3D reconstruction,metric information,convolutional neural networks,feature matching,benchmark dataset,accuracy"
"You Can't Hide Behind Your Headset: User Profiling in Augmented and
  Virtual Reality","Virtual and Augmented Reality (VR, AR) are increasingly gaining traction
thanks to their technical advancement and the need for remote connections,
recently accentuated by the pandemic. Remote surgery, telerobotics, and virtual
offices are only some examples of their successes. As users interact with
VR/AR, they generate extensive behavioral data usually leveraged for measuring
human behavior. However, little is known about how this data can be used for
other purposes.
  In this work, we demonstrate the feasibility of user profiling in two
different use-cases of virtual technologies: AR everyday application ($N=34$)
and VR robot teleoperation ($N=35$). Specifically, we leverage machine learning
to identify users and infer their individual attributes (i.e., age, gender). By
monitoring users' head, controller, and eye movements, we investigate the ease
of profiling on several tasks (e.g., walking, looking, typing) under different
mental loads. Our contribution gives significant insights into user profiling
in virtual environments.","['Pier Paolo Tricomi', 'Federica Nenna', 'Luca Pajola', 'Mauro Conti', 'Luciano Gamberini']",2022-09-22T08:29:32Z,http://arxiv.org/abs/2209.10849v1,"['cs.HC', 'cs.CR']","Augmented Reality,Virtual Reality,User Profiling,Machine Learning,Behavioral Data,Human Behavior,Remote Surgery,Telerobotics,Teleoperation,Virtual Environments"
"Minimizing Human Assistance: Augmenting a Single Demonstration for Deep
  Reinforcement Learning","The use of human demonstrations in reinforcement learning has proven to
significantly improve agent performance. However, any requirement for a human
to manually 'teach' the model is somewhat antithetical to the goals of
reinforcement learning. This paper attempts to minimize human involvement in
the learning process while retaining the performance advantages by using a
single human example collected through a simple-to-use virtual reality
simulation to assist with RL training. Our method augments a single
demonstration to generate numerous human-like demonstrations that, when
combined with Deep Deterministic Policy Gradients and Hindsight Experience
Replay (DDPG + HER) significantly improve training time on simple tasks and
allows the agent to solve a complex task (block stacking) that DDPG + HER alone
cannot solve. The model achieves this significant training advantage using a
single human example, requiring less than a minute of human input. Moreover,
despite learning from a human example, the agent is not constrained to
human-level performance, often learning a policy that is significantly
different from the human demonstration.","['Abraham George', 'Alison Bartsch', 'Amir Barati Farimani']",2022-09-22T19:04:43Z,http://arxiv.org/abs/2209.11275v2,"['cs.LG', 'cs.RO']","Deep Reinforcement Learning,Human Demonstrations,Agent Performance,Single Demonstration,Virtual Reality Simulation,RL Training,Deep Deterministic Policy Gradients,Hindsight Experience Replay,Training Time,Block Stacking"
Accelerating the Convex Hull Computation with a Parallel GPU Algorithm,"The convex hull is a fundamental geometrical structure for many applications
where groups of points must be enclosed or represented by a convex polygon.
Although efficient sequential convex hull algorithms exist, and are constantly
being used in applications, their computation time is often considered an issue
for time-sensitive tasks such as real-time collision detection, clustering or
image processing for virtual reality, among others, where fast response times
are required. In this work we propose a parallel GPU-based adaptation of
heaphull, which is a state of the art CPU algorithm that computes the convex
hull by first doing a efficient filtering stage followed by the actual convex
hull computation. More specifically, this work parallelizes the filtering
stage, adapting it to the GPU programming model as a series of parallel
reductions. Experimental evaluation shows that the proposed implementation
significantly improves the performance of the convex hull computation, reaching
up to $4\times$ of speedup over the sequential CPU-based heaphull and between
$3\times \sim 4\times$ over existing GPU based approaches.","['Alan Keith', 'Héctor Ferrada', 'Cristóbal A. Navarro']",2022-09-25T19:50:51Z,http://arxiv.org/abs/2209.12310v1,['cs.DC'],"Convex hull,Parallel GPU Algorithm,Geometrical structure,Sequential,Computation time,Collision detection,Clustering,Image processing,Virtual reality,Speedup."
"Mitigating Attacks on Artificial Intelligence-based Spectrum Sensing for
  Cellular Network Signals","Cellular networks (LTE, 5G, and beyond) are dramatically growing with high
demand from consumers and more promising than the other wireless networks with
advanced telecommunication technologies. The main goal of these networks is to
connect billions of devices, systems, and users with high-speed data
transmission, high cell capacity, and low latency, as well as to support a wide
range of new applications, such as virtual reality, metaverse, telehealth,
online education, autonomous and flying vehicles, advanced manufacturing, and
many more. To achieve these goals, spectrum sensing has been paid more
attention, along with new approaches using artificial intelligence (AI) methods
for spectrum management in cellular networks. This paper provides a
vulnerability analysis of spectrum sensing approaches using AI-based semantic
segmentation models for identifying cellular network signals under adversarial
attacks with and without defensive distillation methods. The results showed
that mitigation methods can significantly reduce the vulnerabilities of
AI-based spectrum sensing models against adversarial attacks.","['Ferhat Ozgur Catak', 'Murat Kuzlu', 'Salih Sarp', 'Evren Catak', 'Umit Cali']",2022-09-27T11:14:47Z,http://arxiv.org/abs/2209.13007v1,"['cs.NI', 'cs.AI', 'cs.CR']","Artificial intelligence,Spectrum sensing,Cellular networks,Adversarial attacks,Vulnerability analysis,Semantic segmentation,Defensive distillation,LTE,5G,Spectrum management"
"OmniNeRF: Hybriding Omnidirectional Distance and Radiance fields for
  Neural Surface Reconstruction","3D reconstruction from images has wide applications in Virtual Reality and
Automatic Driving, where the precision requirement is very high.
Ground-breaking research in the neural radiance field (NeRF) by utilizing
Multi-Layer Perceptions has dramatically improved the representation quality of
3D objects. Some later studies improved NeRF by building truncated signed
distance fields (TSDFs) but still suffer from the problem of blurred surfaces
in 3D reconstruction. In this work, this surface ambiguity is addressed by
proposing a novel way of 3D shape representation, OmniNeRF. It is based on
training a hybrid implicit field of Omni-directional Distance Field (ODF) and
neural radiance field, replacing the apparent density in NeRF with
omnidirectional information. Moreover, we introduce additional supervision on
the depth map to further improve reconstruction quality. The proposed method
has been proven to effectively deal with NeRF defects at the edges of the
surface reconstruction, providing higher quality 3D scene reconstruction
results.","['Jiaming Shen', 'Bolin Song', 'Zirui Wu', 'Yi Xu']",2022-09-27T14:39:23Z,http://arxiv.org/abs/2209.13433v1,['cs.CV'],"Neural Radiance Field,Multi-Layer Perceptions,Truncated Signed Distance Fields,Omni-directional Distance Field,Depth Map"
"Jubileo: An Open-Source Robot and Framework for Research in Human-Robot
  Social Interaction","Human-robot interaction (HRI) is essential to the widespread use of robots in
daily life. Robots will eventually be able to carry out a variety of duties in
human civilization through effective social interaction. Creating
straightforward and understandable interfaces to engage with robots as they
start to proliferate in the personal workspace is essential. Typically,
interactions with simulated robots are displayed on screens. A more appealing
alternative is virtual reality (VR), which gives visual cues more like those
seen in the real world. In this study, we introduce Jubileo, a robotic
animatronic face with various tools for research and application development in
human-robot social interaction field. Jubileo project offers more than just a
fully functional open-source physical robot; it also gives a comprehensive
framework to operate with a VR interface, enabling an immersive environment for
HRI application tests and noticeably better deployment speed.","['Jair A. Bottega', 'Victor A. Kich', 'Alisson H. Kolling', 'Jardel D. S. Dyonisio', 'Pedro L. Corçaque', 'Rodrigo da S. Guerra', 'Daniel F. T. Gamarra']",2022-09-27T16:24:39Z,http://arxiv.org/abs/2209.13509v1,['cs.RO'],"Open-source,Robot,Framework,Research,Human-robot interaction,Social interaction,Virtual reality,VR interface,Immersive environment,Deployment speed"
"Resource Allocation and Resolution Control in the Metaverse with Mobile
  Augmented Reality","With the development of blockchain and communication techniques, the
Metaverse is considered as a promising next-generation Internet paradigm, which
enables the connection between reality and the virtual world. The key to
rendering a virtual world is to provide users with immersive experiences and
virtual avatars, which is based on virtual reality (VR) technology and high
data transmission rate. However, current VR devices require intensive
computation and communication, and users suffer from high delay while using
wireless VR devices. To build the connection between reality and the virtual
world with current technologies, mobile augmented reality (MAR) is a feasible
alternative solution due to its cheaper communication and computation cost.
This paper proposes an MAR-based connection model for the Metaverse, and
proposes a communication resources allocation algorithm based on outer
approximation (OA) to achieve the best utility. Simulation results show that
our proposed algorithm is able to provide users with basic MAR services for the
Metaverse, and outperforms the benchmark greedy algorithm.","['Peiyuan Si', 'Jun Zhao', 'Huimei Han', 'Kwok-Yan Lam', 'Yang Liu']",2022-09-28T07:09:52Z,http://arxiv.org/abs/2209.13871v1,"['eess.SP', 'cs.SI']","Resource Allocation,Resolution Control,Metaverse,Mobile Augmented Reality,Virtual Reality,Communication Techniques,Virtual Avatars,Data Transmission Rate,Outer Approximation,Simulation Results"
"FoVR: Attention-based VR Streaming through Bandwidth-limited Wireless
  Networks","Consumer Virtual Reality (VR) has been widely used in various application
areas, such as entertainment and medicine. In spite of the superb immersion
experience, to enable high-quality VR on untethered mobile devices remains an
extremely challenging task. The high bandwidth demands of VR streaming
generally overburden a conventional wireless connection, which affects the user
experience and in turn limits the usability of VR in practice. In this paper,
we propose FoVR, attention-based hierarchical VR streaming through
bandwidth-limited wireless networks. The design of FoVR stems from the insight
that human's vision is hierarchical, so that different areas in the field of
view (FoV) can be served with VR content of different qualities. By exploiting
the gaze tracking capacity of the VR devices, FoVR is able to accurately
predict the user's attention so that the streaming of hierarchical VR can be
appropriately scheduled. In this way, FoVR significantly reduces the bandwidth
cost and computing cost while keeping high quality of user experience. We
implement FoVR on a commercial VR device and evaluate its performance in
various scenarios. The experiment results show that FoVR reduces the bandwidth
cost by 88.9% and 76.2%, respectively compared to the original VR streaming and
the state-of-the-art approach.","['Songzhou Yang', 'Yuan He', 'Xiaolong Zheng']",2022-09-30T02:58:08Z,http://arxiv.org/abs/2209.15198v1,"['cs.NI', 'cs.MM']","Attention-based,VR streaming,Bandwidth-limited,Wireless Networks,Gaze tracking,Hierarchical,User experience,Computing cost,Commercial VR device"
"NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review","Neural Radiance Field (NeRF) has recently become a significant development in
the field of Computer Vision, allowing for implicit, neural network-based scene
representation and novel view synthesis. NeRF models have found diverse
applications in robotics, urban mapping, autonomous navigation, virtual
reality/augmented reality, and more. Due to the growing popularity of NeRF and
its expanding research area, we present a comprehensive survey of NeRF papers
from the past two years. Our survey is organized into architecture and
application-based taxonomies and provides an introduction to the theory of NeRF
and its training via differentiable volume rendering. We also present a
benchmark comparison of the performance and speed of key NeRF models. By
creating this survey, we hope to introduce new researchers to NeRF, provide a
helpful reference for influential works in this field, as well as motivate
future research directions with our discussion section.","['Kyle Gao', 'Yina Gao', 'Hongjie He', 'Dening Lu', 'Linlin Xu', 'Jonathan Li']",2022-10-01T21:35:11Z,http://arxiv.org/abs/2210.00379v5,"['cs.CV', 'I.4']","NeRF,Neural Radiance Field,Computer Vision,Neural Network,Scene Representation,View Synthesis,Robotics,Urban Mapping,Virtual Reality,Augmented Reality"
"MOTSLAM: MOT-assisted monocular dynamic SLAM using single-view depth
  estimation","Visual SLAM systems targeting static scenes have been developed with
satisfactory accuracy and robustness. Dynamic 3D object tracking has then
become a significant capability in visual SLAM with the requirement of
understanding dynamic surroundings in various scenarios including autonomous
driving, augmented and virtual reality. However, performing dynamic SLAM solely
with monocular images remains a challenging problem due to the difficulty of
associating dynamic features and estimating their positions. In this paper, we
present MOTSLAM, a dynamic visual SLAM system with the monocular configuration
that tracks both poses and bounding boxes of dynamic objects. MOTSLAM first
performs multiple object tracking (MOT) with associated both 2D and 3D bounding
box detection to create initial 3D objects. Then, neural-network-based
monocular depth estimation is applied to fetch the depth of dynamic features.
Finally, camera poses, object poses, and both static, as well as dynamic map
points, are jointly optimized using a novel bundle adjustment. Our experiments
on the KITTI dataset demonstrate that our system has reached best performance
on both camera ego-motion and object tracking on monocular dynamic SLAM.","['Hanwei Zhang', 'Hideaki Uchiyama', 'Shintaro Ono', 'Hiroshi Kawasaki']",2022-10-05T06:07:10Z,http://arxiv.org/abs/2210.02038v1,['cs.CV'],"MOTSLAM,dynamic SLAM,monocular,depth estimation,object tracking,poses,bounding boxes,neural network,bundle adjustment,KITTI dataset"
Designing Virtual Environments for Social Engagement in Older Adults,"Virtual reality (VR) is increasingly used as a platform for social
interaction, including as a means for older adults to maintain engagement.
However, there has been limited research to examine the features of social VR
that are most relevant to older adults experiences. The current study was
conducted to qualitatively analyze the behavior of older adults in a
collaborative VR environment and evaluate aspects of design that affected their
engagement outcomes. We paired 36 participants over the age of 60, from three
diverse geographic locations, and asked them to interact in collaborative VR
modules. Video-based observation methods and thematic analyses were used to
study the resulting interactions. The results indicated a strong link between
perceived spatial presence in the VR and social engagement, while also
highlighting the importance of individual personality and compatibility. The
study provides new insights into design guidelines that could improve social VR
programs for older adults.","['Tong Bill Xu', 'Armin Mostafavi', 'Benjamin Kim', 'Angella Lee', 'Walter Boot', 'Sara Czaja', 'Saleh Kalantari']",2022-10-06T20:46:26Z,http://arxiv.org/abs/2210.03202v1,['cs.HC'],"Virtual reality,Social engagement,Older adults,Collaborative environment,Design,Spatial presence,Compatibility,Engagement outcomes,Thematic analysis,Design guidelines"
"The PerspectiveLiberator -- an upmixing 6DoF rendering plugin for
  single-perspective Ambisonic room impulse responses","Nowadays, virtual reality interfaces allow the user to change perspectives in
six degrees of freedom (6DoF) virtually, and consistently with the visual part,
the acoustic perspective needs to be updated interactively. Single-perspective
rendering with dynamic head rotation already works quite reliably with upmixed
first-order Ambisonic room impulse responses (ASDM, SIRR, etc.). This
contribution presents a plugin to free the virtual perspective from the
measured one by real-time perspective extrapolation: The PerspectiveLiberator.
The plugin permits selecting between two different algorithms for directional
resolution enhancement (ASDM, 4DE). And for its main task of convolution-based
6DoF rendering, the plugin detects and localizes prominent directional sound
events in the early Ambisonic room impulse response and re-encodes them with
direction, time of arrival, and level adapted to the variable perspective of
the virtual listener. The diffuse residual is enhanced in directional
resolution but remains unaffected by translatory movement to preserve as much
of the original room impression as possible.","['Kaspar Müller', 'Franz Zotter']",2022-10-07T07:11:01Z,http://arxiv.org/abs/2210.03360v2,"['cs.SD', 'eess.AS']","6DoF,upmixing,rendering plugin,Ambisonic,room impulse responses,virtual reality interfaces,dynamic head rotation,first-order Ambisonic,ASDM,SIRR"
"3D Reconstruction of Sculptures from Single Images via Unsupervised
  Domain Adaptation on Implicit Models","Acquiring the virtual equivalent of exhibits, such as sculptures, in virtual
reality (VR) museums, can be labour-intensive and sometimes infeasible. Deep
learning based 3D reconstruction approaches allow us to recover 3D shapes from
2D observations, among which single-view-based approaches can reduce the need
for human intervention and specialised equipment in acquiring 3D sculptures for
VR museums. However, there exist two challenges when attempting to use the
well-researched human reconstruction methods: limited data availability and
domain shift. Considering sculptures are usually related to humans, we propose
our unsupervised 3D domain adaptation method for adapting a single-view 3D
implicit reconstruction model from the source (real-world humans) to the target
(sculptures) domain. We have compared the generated shapes with other methods
and conducted ablation studies as well as a user study to demonstrate the
effectiveness of our adaptation method. We also deploy our results in a VR
application.","['Ziyi Chang', 'George Alex Koulieris', 'Hubert P. H. Shum']",2022-10-09T13:48:00Z,http://arxiv.org/abs/2210.04265v1,['cs.CV'],"3D Reconstruction,Sculptures,Single Images,Unsupervised Domain Adaptation,Implicit Models,Deep Learning,Virtual Reality,Domain Shift,Ablation Studies,VR Application"
"KP-RNN: A Deep Learning Pipeline for Human Motion Prediction and
  Synthesis of Performance Art","Digitally synthesizing human motion is an inherently complex process, which
can create obstacles in application areas such as virtual reality. We offer a
new approach for predicting human motion, KP-RNN, a neural network which can
integrate easily with existing image processing and generation pipelines. We
utilize a new human motion dataset of performance art, Take The Lead, as well
as the motion generation pipeline, the Everybody Dance Now system, to
demonstrate the effectiveness of KP-RNN's motion predictions. We have found
that our neural network can predict human dance movements effectively, which
serves as a baseline result for future works using the Take The Lead dataset.
Since KP-RNN can work alongside a system such as Everybody Dance Now, we argue
that our approach could inspire new methods for rendering human avatar
animation. This work also serves to benefit the visualization of performance
art in digital platforms by utilizing accessible neural networks.","['Patrick Perrine', 'Trevor Kirkby']",2022-10-09T22:46:55Z,http://arxiv.org/abs/2210.04366v3,"['cs.CV', 'cs.AI', 'cs.LG']","deep learning,human motion prediction,synthesis,performance art,KP-RNN,neural network,image processing,motion generation,dataset,Everybody Dance Now"
"Self-Supervised 3D Human Pose Estimation in Static Video Via Neural
  Rendering","Inferring 3D human pose from 2D images is a challenging and long-standing
problem in the field of computer vision with many applications including motion
capture, virtual reality, surveillance or gait analysis for sports and
medicine. We present preliminary results for a method to estimate 3D pose from
2D video containing a single person and a static background without the need
for any manual landmark annotations. We achieve this by formulating a simple
yet effective self-supervision task: our model is required to reconstruct a
random frame of a video given a frame from another timepoint and a rendered
image of a transformed human shape template. Crucially for optimisation, our
ray casting based rendering pipeline is fully differentiable, enabling end to
end training solely based on the reconstruction task.","['Luca Schmidtke', 'Benjamin Hou', 'Athanasios Vlontzos', 'Bernhard Kainz']",2022-10-10T09:24:07Z,http://arxiv.org/abs/2210.04514v1,"['cs.CV', 'cs.AI', 'cs.GR']","3D human pose estimation,self-supervised,static video,neural rendering,computer vision,motion capture,virtual reality,surveillance,gait analysis,ray casting."
A Perception-Driven Approach To Immersive Remote Telerobotics,"Virtual Reality (VR) interfaces are increasingly used as remote visualization
media in telerobotics. Remote environments captured through RGB-D cameras and
visualized using VR interfaces can enhance operators' situational awareness and
sense of presence. However, this approach has strict requirements for the
speed, throughput, and quality of the visualized 3D data.Further, telerobotics
requires operators to focus on their tasks fully, requiring high perceptual and
cognitive skills. This paper shows a work-in-progress framework to address
these challenges by taking the human visual system (HVS) as an inspiration.
Human eyes use attentional mechanisms to select and draw user engagement to a
specific place from the dynamic environment. Inspired by this, the framework
implements functionalities to draw users's engagement to a specific place while
simultaneously reducing latency and bandwidth requirements.","['Y. T. Tefera', 'D. Mazzanti', 'S. Anastasi', 'D. G. Caldwell', 'P. Fiorini', 'N. Deshpande']",2022-10-11T12:49:30Z,http://arxiv.org/abs/2210.05417v1,"['cs.HC', 'cs.RO']","Immersive,Remote Telerobotics,Virtual Reality,RGB-D cameras,Situational awareness,Sense of presence,3D data,Human visual system,Attentional mechanisms,Latency"
Size Does Matter: An Experimental Study of Anxiety in Virtual Reality,"The emotional response of users induced by VR scenarios has become a topic of
interest, however, whether changing the size of objects in VR scenes induces
different levels of anxiety remains a question to be studied. In this study, we
conducted an experiment to initially reveal how the size of a large object in a
VR environment affects changes in participants' (N = 38) anxiety level and
heart rate. To holistically quantify the size of large objects in the VR visual
field, we used the omnidirectional field of view occupancy (OFVO) criterion for
the first time to represent the dimension of the object in the participant's
entire field of view. The results showed that the participants' heartbeat and
anxiety while viewing the large objects were positively and significantly
correlated to OFVO. These study reveals that the increase of object size in VR
environments is accompanied by a higher degree of user's anxiety.","['Junyi Shen', 'Itaru Kitahara', 'Shinichi Koyama', 'Qiaoge Li']",2022-10-13T03:32:28Z,http://arxiv.org/abs/2210.06697v1,"['cs.MM', 'cs.HC']","virtual reality,anxiety,experiment,object size,VR scenarios,emotional response,heart rate,field of view,omnidirectional,anxiety level"
"Visual onoma-to-wave: environmental sound synthesis from visual
  onomatopoeias and sound-source images","We propose a method for synthesizing environmental sounds from visually
represented onomatopoeias and sound sources. An onomatopoeia is a word that
imitates a sound structure, i.e., the text representation of sound. From this
perspective, onoma-to-wave has been proposed to synthesize environmental sounds
from the desired onomatopoeia texts. Onomatopoeias have another representation:
visual-text representations of sounds in comics, advertisements, and virtual
reality. A visual onomatopoeia (visual text of onomatopoeia) contains rich
information that is not present in the text, such as a long-short duration of
the image, so the use of this representation is expected to synthesize diverse
sounds. Therefore, we propose visual onoma-to-wave for environmental sound
synthesis from visual onomatopoeia. The method can transfer visual concepts of
the visual text and sound-source image to the synthesized sound. We also
propose a data augmentation method focusing on the repetition of onomatopoeias
to enhance the performance of our method. An experimental evaluation shows that
the methods can synthesize diverse environmental sounds from visual text and
sound-source images.","['Hien Ohnaka', 'Shinnosuke Takamichi', 'Keisuke Imoto', 'Yuki Okamoto', 'Kazuki Fujii', 'Hiroshi Saruwatari']",2022-10-17T15:19:51Z,http://arxiv.org/abs/2210.09173v1,"['cs.SD', 'eess.AS']","environmental sound synthesis,onomatopoeias,sound sources,visual onomatopoeia,data augmentation,experimental evaluation"
"Review of Persuasive User Interface as Strategy for Technology Addiction
  in Virtual Environments","In the era of virtuality, the increasingly ubiquitous technology bears the
challenge of excessive user dependency, also known as user addiction. Augmented
reality (AR) and virtual reality (VR) have become increasingly integrated into
daily life. Although discussions about the drawbacks of these technologies are
abundant, their exploration for solutions is still rare. Thus, using the PRISMA
methodology, this paper reviewed the literature on technology addiction and
persuasive technology. After describing the key research trends, the paper
summed up nine persuasive elements of user interfaces (UIs) that AR and VR
developers could add to their apps to make them less addictive. Furthermore,
this review paper encourages more research into a persuasive strategy for
controlling user dependency in virtual-physical blended cyberspace.","['Fachrina Dewi Puspitasari', 'Lik-Hang Lee']",2022-10-18T06:54:06Z,http://arxiv.org/abs/2210.09628v1,['cs.HC'],"persuasive user interface,technology addiction,virtual environments,augmented reality,virtual reality,PRISMA methodology,persuasive technology,user interfaces,addictive,cyberspace"
"Analysis of Smooth Pursuit Assessment in Virtual Reality and Concussion
  Detection using BiLSTM","The sport-related concussion (SRC) battery relies heavily upon subjective
symptom reporting in order to determine the diagnosis of a concussion.
Unfortunately, athletes with SRC may return-to-play (RTP) too soon if they are
untruthful of their symptoms. It is critical to provide accurate assessments
that can overcome underreporting to prevent further injury. To lower the risk
of injury, a more robust and precise method for detecting concussion is needed
to produce reliable and objective results. In this paper, we propose a novel
approach to detect SRC using long short-term memory (LSTM) recurrent neural
network (RNN) architectures from oculomotor data. In particular, we propose a
new error metric that incorporates mean squared error in different proportions.
The experimental results on the smooth pursuit test of the VR-VOMS dataset
suggest that the proposed approach can predict concussion symptoms with higher
accuracy compared to symptom provocation on the vestibular ocular motor
screening (VOMS).","['Prithul Sarker', 'Khondker Fariha Hossain', 'Isayas Berhe Adhanom', 'Philip K Pavilionis', 'Nicholas G. Murray', 'Alireza Tavakkoli']",2022-10-12T16:52:31Z,http://arxiv.org/abs/2210.11238v1,"['physics.med-ph', 'cs.AI', 'eess.IV']","smooth pursuit,assessment,virtual reality,concussion detection,BiLSTM,long short-term memory (LSTM),recurrent neural network (RNN),oculomotor data,error metric"
Slippage-robust Gaze Tracking for Near-eye Display,"In recent years, head-mounted near-eye display devices have become the key
hardware foundation for virtual reality and augmented reality. Thus
head-mounted gaze tracking technology has received attention as an essential
part of human-computer interaction. However, unavoidable slippage of
head-mounted devices (HMD) often results higher gaze tracking errors and
hinders the practical usage of HMD. To tackle this problem, we propose a
slippage-robust gaze tracking for near-eye display method based on the aspheric
eyeball model and accurately compute the eyeball optical axis and rotation
center. We tested several methods on datasets with slippage and the
experimental results show that the proposed method significantly outperforms
the previous method (almost double the suboptimal method).","['Wei Zhang', 'Jiaxi Cao', 'Xiang Wang', 'Enqi Tian', 'Bin Li']",2022-10-20T23:47:56Z,http://arxiv.org/abs/2210.11637v2,"['cs.CV', 'cs.HC']","slippage-robust,gaze tracking,near-eye display,head-mounted devices,HMD,human-computer interaction,aspheric eyeball model,optical axis,rotation center"
"Deciphering Contact Interactions and Exploration Strategies Underlying
  Tactile Perception of Material Softness","Our sense of touch is essential and permeates in interactions involving
natural explorations and affective communications. For instance, we routinely
judge the ripeness of fruit at the grocery store, caress the arm of a spouse to
offer comfort, and stroke textiles to gauge their softness. Meanwhile,
interactive displays that provide tactile feedback are becoming normal and
ubiquitous in our daily lives, and are extending rich and immersive
interactions into augmented and virtual reality. To replicate touch sensation
and make virtual objects feel tangible, such feedback will need to relay a
sense of compliance, or softness, one of the key dimensions underlying haptic
perception. As our understanding of softness perception remains incomplete,
this study seeks to understand exploratory strategies and perceptual cues that
may optimally encode material softness. Specifically, we employ methods of
computational finite element modeling, biomechanical experimentation,
psychophysical evaluation, and data-driven analysis. Overall, this work may aid
in engineering the next-generation wearable haptic displays, which must be more
tangible, compatible, and perceptually naturalistic.",['Chang Xu'],2022-10-23T08:30:10Z,http://arxiv.org/abs/2210.12657v1,"['cs.HC', 'q-bio.QM']","contact interactions,exploration strategies,tactile perception,material softness,haptic perception,finite element modeling,biomechanical experimentation,psychophysical evaluation,data-driven analysis,wearable haptic displays"
Live Captions in Virtual Reality (VR),"Few VR applications and games implement captioning of speech and audio cues,
which either inhibits or prevents access of their application by deaf or hard
of hearing (DHH) users, new language learners, and other caption users.
Additionally, little to no guidelines exist on how to implement live captioning
on VR headsets and how it may differ from traditional television captioning. To
help fill the void of information behind user preferences of different VR
captioning styles, we conducted a study with eight DHH participants to test
three caption movement behaviors (headlocked, lag, and appear) while watching
live-captioned, single-speaker presentations in VR. Participants answered a
series of Likert scale and open-ended questions about their experience.
Participant preferences were split, but the majority of participants reported
feeling comfortable with using live captions in VR and enjoyed the experience.
When participants ranked the caption behaviors, there was almost an equal
divide between the three types tested. IPQ results indicated each behavior had
similar immersion ratings, however participants found headlocked and lag
captions more user-friendly than appear captions. We suggest that participants
may vary in caption preference depending on how they use captions, and that
providing opportunities for caption customization is best.","['Pranav Pidathala', 'Dawson Franz', 'James Waller', 'Raja Kushalnagar', 'Christian Vogler']",2022-10-26T22:57:00Z,http://arxiv.org/abs/2210.15072v1,['cs.HC'],"Virtual Reality,Live Captions,Captioning,Deaf and Hard of Hearing,Caption Styles,VR Headsets,User Preferences,Immersion,User-Friendly"
"Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit
  Representation","In this work, we present a dense tracking and mapping system named
Vox-Fusion, which seamlessly fuses neural implicit representations with
traditional volumetric fusion methods. Our approach is inspired by the recently
developed implicit mapping and positioning system and further extends the idea
so that it can be freely applied to practical scenarios. Specifically, we
leverage a voxel-based neural implicit surface representation to encode and
optimize the scene inside each voxel. Furthermore, we adopt an octree-based
structure to divide the scene and support dynamic expansion, enabling our
system to track and map arbitrary scenes without knowing the environment like
in previous works. Moreover, we proposed a high-performance multi-process
framework to speed up the method, thus supporting some applications that
require real-time performance. The evaluation results show that our methods can
achieve better accuracy and completeness than previous methods. We also show
that our Vox-Fusion can be used in augmented reality and virtual reality
applications. Our source code is publicly available at
https://github.com/zju3dv/Vox-Fusion.","['Xingrui Yang', 'Hai Li', 'Hongjia Zhai', 'Yuhang Ming', 'Yuqian Liu', 'Guofeng Zhang']",2022-10-28T02:56:47Z,http://arxiv.org/abs/2210.15858v3,"['cs.CV', 'cs.GR', 'cs.RO']","dense tracking,mapping,voxel-based,neural implicit representation,volumetric fusion methods,implicit mapping,positioning system,voxel-based neural implicit surface representation,octree-based structure,multi-process framework"
"CardsVR: A Two-Person VR Experience with Passive Haptic Feedback from a
  Deck of Playing Cards","Presence in virtual reality (VR) is meaningful for remotely connecting with
others and facilitating social interactions despite great distance while
providing a sense of ""being there."" This work presents CardsVR, a two-person VR
experience that allows remote participants to play a game of cards together. An
entire deck of tracked cards are used to recreate the sense of playing cards
in-person. Prior work in VR commonly provides passive haptic feedback either
through a single object or through static objects in the environment. CardsVR
is novel in providing passive haptic feedback through multiple cards that are
individually tracked and represented in the virtual environment. Participants
interact with the physical cards by picking them up, holding them, playing
them, or moving them on the physical table. Our participant study (N=23) shows
that passive haptic feedback provides significant improvement in three standard
measures of presence: Possibility to Act, Realism, and Haptics.","['Andrew Huard', 'Mengyu Chen', 'Misha Sra']",2022-10-30T09:27:37Z,http://arxiv.org/abs/2210.16785v1,['cs.HC'],"virtual reality,passive haptic feedback,playing cards,tracked cards,virtual environment,participant study"
"DeltaFinger: a 3-DoF Wearable Haptic Display Enabling High-Fidelity
  Force Vector Presentation at a User Finger","This paper presents a novel haptic device DeltaFinger designed to deliver the
force of interaction with virtual objects by guiding user's finger with
wearable delta mechanism. The developed interface is capable to deliver 3D
force vector to the fingertip of the index finger of the user, allowing complex
rendering of virtual reality (VR) environment. The developed device is able to
produce the kinesthetic feedback up to 1.8 N in vertical projection and 0.9 N
in horizontal projection without restricting the motion freedom of of the
remaining fingers. The experimental results showed a sufficient precision in
perception of force vector with DeltaFinger (mean force vector error of 0.6
rad). The proposed device potentially can be applied to VR communications,
medicine, and navigation of the people with vision problems.","['Artem Lykov', 'Aleksey Fedoseev', 'Dzmitry Tsetserukou']",2022-11-01T21:15:49Z,http://arxiv.org/abs/2211.00752v1,"['cs.HC', 'cs.RO']","3-DoF,Wearable Haptic Display,Force Vector,Delta Mechanism,Kinesthetic Feedback,Virtual Reality,Perception,Precision,Navigation,Vision Problems"
Binaural Rendering of Ambisonic Signals by Neural Networks,"Binaural rendering of ambisonic signals is of broad interest to virtual
reality and immersive media. Conventional methods often require manually
measured Head-Related Transfer Functions (HRTFs). To address this issue, we
collect a paired ambisonic-binaural dataset and propose a deep learning
framework in an end-to-end manner. Experimental results show that neural
networks outperform the conventional method in objective metrics and achieve
comparable subjective metrics. To validate the proposed framework, we
experimentally explore different settings of the input features, model
structures, output features, and loss functions. Our proposed system achieves
an SDR of 7.32 and MOSs of 3.83, 3.58, 3.87, 3.58 in quality, timbre,
localization, and immersion dimensions.","['Yin Zhu', 'Qiuqiang Kong', 'Junjie Shi', 'Shilei Liu', 'Xuzhou Ye', 'Ju-chiang Wang', 'Junping Zhang']",2022-11-04T07:57:37Z,http://arxiv.org/abs/2211.02301v1,"['cs.SD', 'cs.AI', 'eess.AS']","Binaural rendering,Ambisonic signals,Neural networks,Head-Related Transfer Functions (HRTFs),Deep learning framework,Experimental results,Objective metrics,Subjective metrics"
"Zero Touch Coordinated UAV Network Formation for 360° Views of a
  Moving Ground Target in Remote VR Applications","Unmanned aerial vehicles (UAVs) with on-board cameras are widely used for
remote surveillance and video capturing applications. In remote virtual reality
(VR) applications, multiple UAVs can be used to capture different partially
overlapping angles of the ground target, which can be stitched together to
provide 360{\deg} views. This requires coordinated formation of UAVs that is
adaptive to movements of the ground target. In this paper, we propose a joint
UAV formation and tracking framework to capture 360{\deg} angles of the target.
The proposed framework uses a zero touch approach for automated and adaptive
reconfiguration of multiple UAVs in a coordinated manner without the need for
human intervention. This is suited to both military and civilian applications.
Simulation results demonstrate the convergence and configuration of the UAVs
with arbitrary initial locations and orientations. The performance has been
tested for various number of UAVs and different mobility patterns of the ground
target.","['Yuhui Wang', 'Junaid Farooq']",2022-11-05T07:09:28Z,http://arxiv.org/abs/2211.02833v1,"['cs.RO', 'cs.SY', 'eess.SY']","UAVs,on-board cameras,remote VR applications,coordinated formation,ground target,zero touch,adaptive reconfiguration,simulation,mobility patterns"
VRDoc: Gaze-based Interactions for VR Reading Experience,"Virtual reality (VR) offers the promise of an infinite office and remote
collaboration, however, existing interactions in VR do not strongly support one
of the most essential tasks for most knowledge workers, reading. This paper
presents VRDoc, a set of gaze-based interaction methods designed to improve the
reading experience in VR. We introduce three key components: Gaze
Select-and-Snap for document selection, Gaze MagGlass for enhanced text
legibility, and Gaze Scroll for ease of document traversal. We implemented each
of these tools using a commodity VR headset with eye-tracking. In a series of
user studies with 13 participants, we show that VRDoc makes VR reading both
more efficient (p < 0.01 ) and less demanding (p < 0.01), and when given a
choice, users preferred to use our tools over the current VR reading methods.","['Geonsun Lee', 'Jennifer Healey', 'Dinesh Manocha']",2022-11-06T01:11:11Z,http://arxiv.org/abs/2211.03001v1,"['cs.HC', 'D.2.2; I.3.7']","Virtual reality,VRDoc,gaze-based interactions,reading experience,eye-tracking,document selection,text legibility,document traversal,user studies"
A Quantum-Powered Photorealistic Rendering,"Achieving photorealistic rendering of real-world scenes poses a significant
challenge with diverse applications, including mixed reality and virtual
reality. Neural networks, extensively explored in solving differential
equations, have previously been introduced as implicit representations for
photorealistic rendering. However, achieving realism through traditional
computing methods is arduous due to the time-consuming optical ray tracing, as
it necessitates extensive numerical integration of color, transparency, and
opacity values for each sampling point during the rendering process. In this
paper, we introduce Quantum Radiance Fields (QRF), which incorporate quantum
circuits, quantum activation functions, and quantum volume rendering to
represent scenes implicitly. Our results demonstrate that QRF effectively
confronts the computational challenges associated with extensive numerical
integration by harnessing the parallelism capabilities of quantum computing.
Furthermore, current neural networks struggle with capturing fine signal
details and accurately modeling high-frequency information and higher-order
derivatives. Quantum computing's higher order of nonlinearity provides a
distinct advantage in this context. Consequently, QRF leverages two key
strengths of quantum computing: highly non-linear processing and extensive
parallelism, making it a potent tool for achieving photorealistic rendering of
real-world scenes.","['YuanFu Yang', 'Min Sun']",2022-11-07T10:23:32Z,http://arxiv.org/abs/2211.03418v5,['cs.CV'],"photorealistic rendering,neural networks,quantum computing,quantum circuits,quantum activation functions,quantum volume rendering,numerical integration,parallelism,high-frequency information,nonlinearity"
"An Open, Multi-Platform Software Architecture for Online Education in
  the Metaverse","Use of online platforms for education is a vibrant and growing arena,
incorporating a variety of software platforms and technologies, including
various modalities of extended reality. We present our Enhanced Reality
Teaching Concierge, an open networking hub architected to enable efficient and
easy connectivity between a wide variety of services or applications to a wide
variety of clients, designed to showcase 3D for academic purposes across web
technologies, virtual reality, and even virtual worlds. The agnostic nature of
the system, paired with efficient architecture, and simple and open protocols
furnishes an ecosystem that can easily be tailored to maximize the innate
characteristics of each 3D display environment while sharing common data and
control systems with the ultimate goal of a seamless, expandable, nimble
education metaverse.","['S. Lombeyda', 'S. G. Djorgovski', 'A. Tran', 'J. Liu', 'A. Noyes', 'S. Fomina']",2022-11-09T21:13:24Z,http://arxiv.org/abs/2211.05199v1,"['cs.HC', 'physics.ed-ph', 'H.5; I.3.7; K.3']","online education,software architecture,metaverse,extended reality,networking hub,3D,web technologies,virtual reality,virtual worlds,education metaverse"
"Scaling Neural Face Synthesis to High FPS and Low Latency by Neural
  Caching","Recent neural rendering approaches greatly improve image quality, reaching
near photorealism. However, the underlying neural networks have high runtime,
precluding telepresence and virtual reality applications that require high
resolution at low latency. The sequential dependency of layers in deep networks
makes their optimization difficult. We break this dependency by caching
information from the previous frame to speed up the processing of the current
one with an implicit warp. The warping with a shallow network reduces latency
and the caching operations can further be parallelized to improve the frame
rate. In contrast to existing temporal neural networks, ours is tailored for
the task of rendering novel views of faces by conditioning on the change of the
underlying surface mesh. We test the approach on view-dependent rendering of 3D
portrait avatars, as needed for telepresence, on established benchmark
sequences. Warping reduces latency by 70$\%$ (from 49.4ms to 14.9ms on
commodity GPUs) and scales frame rates accordingly over multiple GPUs while
reducing image quality by only 1$\%$, making it suitable as part of end-to-end
view-dependent 3D teleconferencing applications. Our project page can be found
at: https://yu-frank.github.io/lowlatency/.","['Frank Yu', 'Sid Fels', 'Helge Rhodin']",2022-11-10T18:58:00Z,http://arxiv.org/abs/2211.05773v1,['cs.CV'],"Neural rendering,Image quality,Latency,Neural networks,Optimization,Shallow network,Parallelized,Frame rate,Telepresence,Virtual reality"
The Value Chain of Education Metaverse,"Since the end of 2021, the Metaverse has been booming. Many unknown
possibilities are gradually being realized, but many people only determined
that they use Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality
(MR) in the Metaverse. It is even considered that as long as the above
realities (VR, AR, MR) are used, it is equal to the Metaverse. However, this is
not true, for Reality-based display tools are only one of the presentation
methods of the Metaverse. If we cannot return to the three main characteristics
of the Metaverse: ""digital avatars,"" a decentralized ""consensus value system,""
and ""Immersive experience,"" the practice and imagination of the Metaverse will
become very narrow. Since 2022, the concept of Metaverse has also been widely
used in classroom teaching to integrate into teaching activities. Therefore, to
prevent teachers and students from understanding the Metaverse not only in the
""Using VR, AR, MR is equivalent to Metaverse"" but also pay more attention to
the other two characteristics of the Metaverse: ""digital avatars"" and a
decentralized ""consensus value system.""",['Yun-Cheng Tsai'],2022-11-07T15:28:06Z,http://arxiv.org/abs/2211.05833v2,"['cs.CY', 'cs.SI']","Education,Metaverse,Virtual Reality,Augmented Reality,Mixed Reality,Digital avatars,Consensus value system,Immersive experience,Classroom teaching,Presentation methods"
Parallel Downlink Data Distribution in Indoor Multi-hop THz Networks,"The emerging dynamic Virtual Reality (VR) applications are the best candidate
applications in high bandwidth indoor Terahertz (THz) wireless networks, with
the Reconfigurable Intelligent Surface (RIS) devices presenting a breakthrough
solution in extending the typically short THz communication range and
alleviating line-of-sight link blockages. In future smart factories, it is
envisioned that factory workers will use VR devices via VR application data
with high quality resolution, while transmitting over THz links and RIS
devices, enabled by the Mobile Edge Computing (MEC) capabilities. Since indoor
RIS placement is static, whereas VR users move and send multiple VR data
download requests simultaneously, there is a challenge of proper network load
balancing, which if unaddressed can result in poor resource utilization and low
throughput. To address this challenge, we propose a parallel downlink data
distribution system and develop multi-criteria optimization solutions that can
improve throughput, while transmitting each downlink data flow over a set of
possible paths between source and destination devices. The results show that
the proposed system can enhance the performance in terms of throughput benefit,
as compared to the system using one serial download link distribution.","['Cao Vien Phung', 'Andre Drummond', 'Admela Jukan']",2022-11-14T08:45:04Z,http://arxiv.org/abs/2211.07202v1,['cs.NI'],"indoor,multi-hop,Terahertz,wireless networks,Reconfigurable Intelligent Surface (RIS),Mobile Edge Computing (MEC),network load balancing,throughput,parallel data distribution,multi-criteria optimization"
Immersive Neural Graphics Primitives,"Neural radiance field (NeRF), in particular its extension by instant neural
graphics primitives, is a novel rendering method for view synthesis that uses
real-world images to build photo-realistic immersive virtual scenes. Despite
its potential, research on the combination of NeRF and virtual reality (VR)
remains sparse. Currently, there is no integration into typical VR systems
available, and the performance and suitability of NeRF implementations for VR
have not been evaluated, for instance, for different scene complexities or
screen resolutions. In this paper, we present and evaluate a NeRF-based
framework that is capable of rendering scenes in immersive VR allowing users to
freely move their heads to explore complex real-world scenes. We evaluate our
framework by benchmarking three different NeRF scenes concerning their
rendering performance at different scene complexities and resolutions.
Utilizing super-resolution, our approach can yield a frame rate of 30 frames
per second with a resolution of 1280x720 pixels per eye. We discuss potential
applications of our framework and provide an open source implementation online.","['Ke Li', 'Tim Rolff', 'Susanne Schmidt', 'Reinhard Bacher', 'Simone Frintrop', 'Wim Leemans', 'Frank Steinicke']",2022-11-24T09:33:38Z,http://arxiv.org/abs/2211.13494v1,"['cs.CV', 'cs.GR', 'cs.HC', 'cs.LG']","Neural radiance field,Neural graphics primitives,Rendering method,View synthesis,Virtual reality (VR),Performance evaluation,Scene complexities,Screen resolutions,Super-resolution,Frame rate"
"Accelerating Interactive Human-like Manipulation Learning with GPU-based
  Simulation and High-quality Demonstrations","Dexterous manipulation with anthropomorphic robot hands remains a challenging
problem in robotics because of the high-dimensional state and action spaces and
complex contacts. Nevertheless, skillful closed-loop manipulation is required
to enable humanoid robots to operate in unstructured real-world environments.
Reinforcement learning (RL) has traditionally imposed enormous interaction data
requirements for optimizing such complex control problems. We introduce a new
framework that leverages recent advances in GPU-based simulation along with the
strength of imitation learning in guiding policy search towards promising
behaviors to make RL training feasible in these domains. To this end, we
present an immersive virtual reality teleoperation interface designed for
interactive human-like manipulation on contact rich tasks and a suite of
manipulation environments inspired by tasks of daily living. Finally, we
demonstrate the complementary strengths of massively parallel RL and imitation
learning, yielding robust and natural behaviors. Videos of trained policies,
our source code, and the collected demonstration datasets are available at
https://maltemosbach.github.io/interactive_ human_like_manipulation/.","['Malte Mosbach', 'Kara Moraw', 'Sven Behnke']",2022-12-05T09:37:27Z,http://arxiv.org/abs/2212.02126v1,"['cs.RO', 'cs.AI', 'cs.LG']","GPU-based simulation,Reinforcement learning,Imitation learning,Anthropomorphic robot hands,Manipulation learning,Virtual reality teleoperation,Contact-rich tasks,Daily living tasks,Massively parallel RL,Interactive human-like manipulation"
Head Movement Modeling for Immersive Visualization in VR,"Virtual Reality, and Extended Reality in general, connect the physical body
with the virtual world. Movement of our body translates to interactions with
this virtual world. Only by moving our head will we see a different
perspective. By doing so, the physical restrictions of our body's movement
restrict our capabilities virtually. By modelling the capabilities of human
movement, render engines can get useful information to pre-cache visual texture
information or immersive light information. Such pre-caching becomes vital due
to ever increasing realism in virtual environments. This work is the first work
to predict the volume in which the head will be positioned in the future based
on a data-driven binned-ellipsoid technique. The proposed technique can reduce
a 1m3 volume to a size of 10cm3 with negligible accuracy loss. This volume then
provides the render engine with the necessary information to pre-cache visual
data.","['Glenn Van Wallendael', 'Lucas Liegeois', 'Julie Artois', 'Peter Lambert']",2022-12-08T15:58:04Z,http://arxiv.org/abs/2212.04363v1,"['cs.MM', 'cs.HC']","head movement,modeling,immersive visualization,VR,virtual reality,extended reality,render engines,pre-caching,binned-ellipsoid technique,data-driven"
"A systematic literature review on Virtual Reality and Augmented Reality
  in terms of privacy, authorization and data-leaks","In recent years, VR and AR has exploded into a multimillionaire market. As
this emerging technology has spread to a variety of businesses and is rapidly
increasing among users. It is critical to address potential privacy and
security concerns that these technologies might pose. In this study, we discuss
the current status of privacy and security in VR and AR. We analyse possible
problems and risks. Besides, we will look in detail at a few of the major
concerns issues and related security solutions for AR and VR. Additionally, as
VR and AR authentication is the most thoroughly studied aspect of the problem,
we concentrate on the research that has already been done in this area.","['Parth Dipakkumar Patel', 'Prem Trivedi']",2022-12-09T01:28:58Z,http://arxiv.org/abs/2212.04621v1,['cs.CR'],"Virtual Reality,Augmented Reality,privacy,authorization,data-leaks,security concerns,authentication,security solutions,privacy risks"
DopplerBAS: Binaural Audio Synthesis Addressing Doppler Effect,"Recently, binaural audio synthesis (BAS) has emerged as a promising research
field for its applications in augmented and virtual realities. Binaural audio
helps users orient themselves and establish immersion by providing the brain
with interaural time differences reflecting spatial information. However,
existing BAS methods are limited in terms of phase estimation, which is crucial
for spatial hearing. In this paper, we propose the \textbf{DopplerBAS} method
to explicitly address the Doppler effect of the moving sound source.
Specifically, we calculate the radial relative velocity of the moving speaker
in spherical coordinates, which further guides the synthesis of binaural audio.
This simple method introduces no additional hyper-parameters and does not
modify the loss functions, and is plug-and-play: it scales well to different
types of backbones. DopperBAS distinctly improves the representative WarpNet
and BinauralGrad backbones in the phase error metric and reaches a new state of
the art (SOTA): 0.780 (versus the current SOTA 0.807). Experiments and ablation
studies demonstrate the effectiveness of our method.","['Jinglin Liu', 'Zhenhui Ye', 'Qian Chen', 'Siqi Zheng', 'Wen Wang', 'Qinglin Zhang', 'Zhou Zhao']",2022-12-14T03:18:21Z,http://arxiv.org/abs/2212.07000v3,['eess.AS'],"Binaural audio synthesis,Doppler effect,Phase estimation,Spatial hearing,Radial relative velocity,Spherical coordinates,Loss functions,Backbones,State of the art,Ablation studies"
"Sense of Embodiment Inducement for People with Reduced Lower-body
  Mobility and Sensations with Partial-Visuomotor Stimulation","To induce the Sense of Embodiment~(SoE) on the virtual 3D avatar during a
Virtual Reality~(VR) walking scenario, VR interfaces have employed the
visuotactile or visuomotor approaches. However, people with reduced lower-body
mobility and sensation~(PRLMS) who are incapable of feeling or moving their
legs would find this task extremely challenging. Here, we propose an upper-body
motion tracking-based partial-visuomotor technique to induce SoE and positive
feedback for PRLMS patients. We design partial-visuomotor stimulation
consisting of two distinctive inputs~(\textit{Button Control} \& \textit{Upper
Motion tracking}) and outputs~(\textit{wheelchair motion} \& \textit{Gait
Motion}). The preliminary user study was conducted to explore subjective
preference with qualitative feedback. From the qualitative study result, we
observed the positive response on the partial-visuomotor regarding SoE in the
asynchronous VR experience for PRLMS.","['Hyuckjin Jang', 'Taehei Kim', 'Seo Young Oh', 'Jeongmi Lee', 'Sunghee Lee', 'Sang Ho Yoon']",2022-12-23T06:52:50Z,http://arxiv.org/abs/2212.12170v1,"['cs.HC', 'cs.GR']","Sense of Embodiment,Lower-body Mobility,Visuomotor Stimulation,Virtual Reality,Upper-body Motion Tracking,Partial-visuomotor Technique,Qualitative Feedback,Wheelchair Motion,Gait Motion,VR Experience"
"Distributed Machine Learning for UAV Swarms: Computing, Sensing, and
  Semantics","Unmanned aerial vehicle (UAV) swarms are considered as a promising technique
for next-generation communication networks due to their flexibility, mobility,
low cost, and the ability to collaboratively and autonomously provide services.
Distributed learning (DL) enables UAV swarms to intelligently provide
communication services, multi-directional remote surveillance, and target
tracking. In this survey, we first introduce several popular DL algorithms such
as federated learning (FL), multi-agent Reinforcement Learning (MARL),
distributed inference, and split learning, and present a comprehensive overview
of their applications for UAV swarms, such as trajectory design, power control,
wireless resource allocation, user assignment, perception, and satellite
communications. Then, we present several state-of-the-art applications of UAV
swarms in wireless communication systems, such us reconfigurable intelligent
surface (RIS), virtual reality (VR), semantic communications, and discuss the
problems and challenges that DL-enabled UAV swarms can solve in these
applications. Finally, we describe open problems of using DL in UAV swarms and
future research directions of DL enabled UAV swarms. In summary, this survey
provides a comprehensive survey of various DL applications for UAV swarms in
extensive scenarios.","['Yahao Ding', 'Zhaohui Yang', 'Quoc-Viet Pham', 'Zhaoyang Zhang', 'Mohammad Shikh-Bahaei']",2023-01-03T01:05:18Z,http://arxiv.org/abs/2301.00912v1,"['cs.LG', 'cs.AI']","Distributed Learning,UAV Swarms,Federated Learning,Multi-Agent Reinforcement Learning,Split Learning,Trajectory Design,Power Control,Wireless Resource Allocation,Semantic Communications"
School visits to a physics research laboratory using virtual reality,"School visits to research laboratories or facilities represent a unique way
to bring students closer to science and STEM (Science, Technology, Engineering
and Mathematics) careers. However, such visits can be very expensive for
students and teachers, in terms of both time and money. In this paper, we
present a possible alternative to on-site visits consisting inan activity
addressed to high school students that makes use of a VR application to make
them enter into a particle physics experiment. This proposal can represent a
valid way of guaranteeing a visit to a research centre for all schools,
regardless of their social or geographical origin. We describe the tests we
carried out with a focus group of teachers and their students, and the obtained
results.","['Ilaria De Angelis', 'Antonio Budano', 'Giacomo De Pietro', 'Alberto Martini', 'Adriana Postiglione']",2023-01-04T10:11:50Z,http://arxiv.org/abs/2301.01515v1,['physics.ed-ph'],"School visits,Physics research laboratory,Virtual reality,STEM careers,Research laboratories,High school students,VR application,Particle physics experiment,Focus group"
Bayesian modelling of visual discrimination learning in mice,"The brain constantly turns large flows of sensory information into selective
representations of the environment. It, therefore, needs to learn to process
those sensory inputs that are most relevant for behaviour. It is not well
understood how learning changes neural circuits in visual and decision-making
brain areas to adjust and improve its visually guided decision-making. To
address this question, head-fixed mice were trained to move through virtual
reality environments and learn visual discrimination while neural activity was
recorded with two-photon calcium imaging. Previously, descriptive models of
neuronal activity were fitted to the data, which was used to compare the
activity of excitatory and different inhibitory cell types. However, the
previous models did not take the internal representations and learning dynamics
into account. Here, I present a framework to infer a model of internal
representations that are used to generate the behaviour during the task. We
model the learning process from untrained mice to trained mice within the
normative framework of the ideal Bayesian observer and provide a Markov model
for generating the movement and licking. The framework provides a space of
models where a range of hypotheses about the internal representations could be
compared for a given data set.",['Pouya Baniasadi'],2022-11-15T16:59:20Z,http://arxiv.org/abs/2301.02659v1,['q-bio.NC'],"Bayesian modelling,visual discrimination,learning,mice,neural circuits,decision-making,two-photon calcium imaging,Markov model,internal representations,Bayesian observer"
"What you see is (not) what you get: A VR Framework for Correcting Robot
  Errors","Many solutions tailored for intuitive visualization or teleoperation of
virtual, augmented and mixed (VAM) reality systems are not robust to robot
failures, such as the inability to detect and recognize objects in the
environment or planning unsafe trajectories. In this paper, we present a novel
virtual reality (VR) framework where users can (i) recognize when the robot has
failed to detect a real-world object, (ii) correct the error in VR, (iii)
modify proposed object trajectories and, (iv) implement behaviors on a
real-world robot. Finally, we propose a user study aimed at testing the
efficacy of our framework. Project materials can be found in the OSF
repository.","['Maciej K. Wozniak', 'Rebecca Stower', 'Patric Jensfelt', 'Andre Pereira']",2023-01-12T10:27:30Z,http://arxiv.org/abs/2301.04919v2,['cs.RO'],"virtual reality,framework,robot errors,object recognition,trajectories,user study"
A Framework for Active Haptic Guidance Using Robotic Haptic Proxies,"Haptic feedback is an important component of creating an immersive mixed
reality experience. Traditionally, haptic forces are rendered in response to
the user's interactions with the virtual environment. In this work, we explore
the idea of rendering haptic forces in a proactive manner, with the explicit
intention to influence the user's behavior through compelling haptic forces. To
this end, we present a framework for active haptic guidance in mixed reality,
using one or more robotic haptic proxies to influence user behavior and deliver
a safer and more immersive virtual experience. We provide details on common
challenges that need to be overcome when implementing active haptic guidance,
and discuss example applications that show how active haptic guidance can be
used to influence the user's behavior. Finally, we apply active haptic guidance
to a virtual reality navigation problem, and conduct a user study that
demonstrates how active haptic guidance creates a safer and more immersive
experience for users.","['Niall L. Williams', 'Nicholas Rewkowski', 'Jiasheng Li', 'Ming C. Lin']",2023-01-12T21:43:34Z,http://arxiv.org/abs/2301.05311v2,"['cs.RO', 'cs.GR']","Haptic feedback,Mixed reality,Robotic haptic proxies,User behavior,Virtual environment,Active haptic guidance,Immersive experience,User study,Virtual reality,Navigation."
A Survey on Human Action Recognition,"Human Action Recognition (HAR), one of the most important tasks in computer
vision, has developed rapidly in the past decade and has a wide range of
applications in health monitoring, intelligent surveillance, virtual reality,
human computer interaction and so on. Human actions can be represented by a
wide variety of modalities, such as RGB-D cameras, audio, inertial sensors,etc.
Consequently, in addition to the mainstream single modality based HAR
approaches, more and more research is devoted to the multimodal domain due to
the complementary properties between multimodal data. In this paper, we present
a survey of HAR methods in recent years according to the different input
modalities. Meanwhile, considering that most of the recent surveys on HAR focus
on the third perspective, while this survey aims to provide a more
comprehensive introduction to HAR novices and researchers, we therefore also
investigate the actions recognition methods from the first perspective in
recent years. Finally, we give a brief introduction about the benchmark HAR
datasets and show the performance comparison of different methods on these
datasets.",['Zhou Shuchang'],2022-12-20T08:02:20Z,http://arxiv.org/abs/2301.06082v1,['cs.CV'],"Human Action Recognition,Computer Vision,Modalities,RGB-D cameras,Audio,Inertial Sensors,Multimodal,HAR Methods,Benchmark Datasets,Performance Comparison"
"Developing a Framework for Heterotopias as Discursive Playgrounds: A
  Comparative Analysis of Non-Immersive and Immersive Technologies","The discursive space represents the reordering of knowledge gained through
accumulation. In the digital age, multimedia has become the language of
information, and the space for archival practices is provided by non-immersive
technologies, resulting in the disappearance of several layers from discursive
activities. Heterotopias are unique, multilayered epistemic contexts that
connect other systems through the exchange of information. This paper describes
a process to create a framework for Virtual Reality, Mixed Reality, and
personal computer environments based on heterotopias to provide absent layers.
This study provides virtual museum space as an informational terrain that
contains a ""world within worlds"" and presents place production as a layer of
heterotopia and the subject of discourse. Automation for the individual
multimedia content is provided via various sorting and grouping algorithms, and
procedural content generation algorithms such as Binary Space Partitioning,
Cellular Automata, Growth Algorithm, and Procedural Room Generation. Versions
of the framework were comparatively evaluated through a user study involving 30
participants, considering factors such as usability, technology acceptance, and
presence. The results of the study show that the framework can serve diverse
contexts to construct multilayered digital habitats and is flexible for
integration into professional and daily life practices.","['Elif Hilal Korkut', 'Elif Surer']",2023-01-20T13:26:36Z,http://arxiv.org/abs/2301.08565v1,"['cs.HC', 'cs.MM']","heterotopias,discursive space,multimedia,archival practices,Virtual Reality,Mixed Reality,personal computer environments,algorithms,user study,digital habitats"
"A Modular Multi-stage Lightweight Graph Transformer Network for Human
  Pose and Shape Estimation from 2D Human Pose","In this research, we address the challenge faced by existing deep
learning-based human mesh reconstruction methods in balancing accuracy and
computational efficiency. These methods typically prioritize accuracy,
resulting in large network sizes and excessive computational complexity, which
may hinder their practical application in real-world scenarios, such as virtual
reality systems. To address this issue, we introduce a modular multi-stage
lightweight graph-based transformer network for human pose and shape estimation
from 2D human pose, a pose-based human mesh reconstruction approach that
prioritizes computational efficiency without sacrificing reconstruction
accuracy. Our method consists of a 2D-to-3D lifter module that utilizes graph
transformers to analyze structured and implicit joint correlations in 2D human
poses, and a mesh regression module that combines the extracted pose features
with a mesh template to produce the final human mesh parameters.","['Ayman Ali', 'Ekkasit Pinyoanuntapong', 'Pu Wang', 'Mohsen Dorodchi']",2023-01-31T04:42:47Z,http://arxiv.org/abs/2301.13403v1,"['cs.CV', 'cs.AI']","modular,multi-stage,lightweight,graph transformer network,human pose,shape estimation,2D human pose,lifter module,mesh regression module"
"GeneFace: Generalized and High-Fidelity Audio-Driven 3D Talking Face
  Synthesis","Generating photo-realistic video portrait with arbitrary speech audio is a
crucial problem in film-making and virtual reality. Recently, several works
explore the usage of neural radiance field in this task to improve 3D realness
and image fidelity. However, the generalizability of previous NeRF-based
methods to out-of-domain audio is limited by the small scale of training data.
In this work, we propose GeneFace, a generalized and high-fidelity NeRF-based
talking face generation method, which can generate natural results
corresponding to various out-of-domain audio. Specifically, we learn a
variaitional motion generator on a large lip-reading corpus, and introduce a
domain adaptative post-net to calibrate the result. Moreover, we learn a
NeRF-based renderer conditioned on the predicted facial motion. A head-aware
torso-NeRF is proposed to eliminate the head-torso separation problem.
Extensive experiments show that our method achieves more generalized and
high-fidelity talking face generation compared to previous methods.","['Zhenhui Ye', 'Ziyue Jiang', 'Yi Ren', 'Jinglin Liu', 'JinZheng He', 'Zhou Zhao']",2023-01-31T05:56:06Z,http://arxiv.org/abs/2301.13430v1,['cs.CV'],"neural radiance field,3D realness,image fidelity,lip-reading,domain adaptative post-net,NeRF-based renderer,head-aware torso-NeRF,generalized,high-fidelity,talking face generation"
A Vision-Based Algorithm for a Path Following Problem,"A novel prize-winner algorithm designed for a path following problem within
the Unmanned Aerial Vehicle (UAV) field is presented in this paper. The
proposed approach exploits the advantages offered by the pure pursuing
algorithm to set up an intuitive and simple control framework. A path fora
quad-rotor UAV is obtained by using downward facing camera images implementing
an Image-Based Visual Servoing (IBVS) approach. Numerical simulations in MATLAB
together with the MathWorks Virtual Reality (VR) toolbox demonstrate the
validity and the effectiveness of the proposed solution. The code is released
as open-source making it possible to go through any part of the system and to
replicate the obtained results.","['Mario Terlizzi', 'Giuseppe Silano', 'Luigi Russo', 'Muhammad Aatif', 'Amin Basiri', 'Valerio Mariani', 'Luigi Iannelli', 'Luigi Glielmo']",2023-02-09T16:21:00Z,http://arxiv.org/abs/2302.04742v1,['cs.RO'],"path following,algorithm,unmanned aerial vehicle (UAV),Image-Based Visual Servoing (IBVS),numerical simulations,MATLAB,Virtual Reality (VR) toolbox,open-source,quad-rotor UAV,control framework"
"Improvement of attention in subjects diagnosed with hyperkinetic
  syndrome using BIOVIT Simulator","This study aimed to stimulate the brain's executive function through a series
of tasks and rules based on dynamic perceptual stimuli using the Biotechnology
Virtual Immersion Simulator (BIOVIT) and thus evaluate its usefulness to
maintain and increase attention levels in subjects diagnosed with hyperkinetic
syndrome. With a quantitative methodology framed in a longitudinal trend
design, the cause of the exposure-outcome relationships was studied using the
BIOVIT simulator. Exploratory analysis of oscillatory brain activity was
measured using a graphical recording of brain electrical activity and attention
levels. Data consisted of 77,566 observations from n = 18 separately studied
participants. The findings established that the BIOVIT simulator maintained and
increased the attention levels of the participants by 77.8%. Furthermore, the
hypothesis was tested that virtual reality immersion technologies significantly
affect attention levels in participants aged 8 to 12. The evidence shows that
the BIOVIT simulator is an alternative to developing learning methodologies in
vulnerable populations. The low implementation costs and the diversity of
academic applications may allow schools in developing countries to solve this
problem that afflicts thousands of children with attention deficit and
hyperactivity disorder.",['Dr. Cesar R Salas-Guerra'],2023-02-11T01:25:43Z,http://arxiv.org/abs/2302.05562v1,"['cs.HC', 'cs.CY', 'q-bio.NC']","attention,hyperkinetic syndrome,Biotechnology Virtual Immersion Simulator (BIOVIT),executive function,dynamic perceptual stimuli,longitudinal trend design,oscillatory brain activity,virtual reality immersion technologies"
Towards Zero-trust Security for the Metaverse,"By focusing on immersive interaction among users, the burgeoning Metaverse
can be viewed as a natural extension of existing social media. Similar to
traditional online social networks, there are numerous security and privacy
issues in the Metaverse (e.g., attacks on user authentication and
impersonation). In this paper, we develop a holistic research agenda for
zero-trust user authentication in social virtual reality (VR), an early
prototype of the Metaverse. Our proposed research includes four concrete steps:
investigating biometrics-based authentication that is suitable for continuously
authenticating VR users, leveraging federated learning (FL) for protecting user
privacy in biometric data, improving the accuracy of continuous VR
authentication with multimodal data, and boosting the usability of zero-trust
security with adaptive VR authentication. Our preliminary study demonstrates
that conventional FL algorithms are not well suited for biometrics-based
authentication of VR users, leading to an accuracy of less than 10%. We discuss
the root cause of this problem, the associated open challenges, and several
future directions for realizing our research vision.","['Ruizhi Cheng', 'Songqing Chen', 'Bo Han']",2023-02-17T14:13:02Z,http://arxiv.org/abs/2302.08885v1,"['cs.CR', 'cs.LG', 'cs.NI']","zero-trust security,Metaverse,user authentication,social virtual reality,biometrics-based authentication,federated learning,multimodal data,continuous authentication,privacy,usability"
"AutoVis: Enabling Mixed-Immersive Analysis of Automotive User Interface
  Interaction Studies","Automotive user interface (AUI) evaluation becomes increasingly complex due
to novel interaction modalities, driving automation, heterogeneous data, and
dynamic environmental contexts. Immersive analytics may enable efficient
explorations of the resulting multilayered interplay between humans, vehicles,
and the environment. However, no such tool exists for the automotive domain.
With AutoVis, we address this gap by combining a non-immersive desktop with a
virtual reality view enabling mixed-immersive analysis of AUIs. We identify
design requirements based on an analysis of AUI research and domain expert
interviews (N=5). AutoVis supports analyzing passenger behavior, physiology,
spatial interaction, and events in a replicated study environment using
avatars, trajectories, and heatmaps. We apply context portals and driving-path
events as automotive-specific visualizations. To validate AutoVis against
real-world analysis tasks, we implemented a prototype, conducted heuristic
walkthroughs using authentic data from a case study and public datasets, and
leveraged a real vehicle in the analysis process.","['Pascal Jansen', 'Julian Britten', 'Alexander Häusele', 'Thilo Segschneider', 'Mark Colley', 'Enrico Rukzio']",2023-02-21T08:58:37Z,http://arxiv.org/abs/2302.10531v1,['cs.HC'],"Automotive user interface,Interaction studies,Immersive analytics,Virtual reality,Passenger behavior,Physiology,Spatial interaction,Avatars,Trajectories,Heatmaps"
"Unpaired Translation from Semantic Label Maps to Images by Leveraging
  Domain-Specific Simulations","Photorealistic image generation from simulated label maps are necessitated in
several contexts, such as for medical training in virtual reality. With
conventional deep learning methods, this task requires images that are paired
with semantic annotations, which typically are unavailable. We introduce a
contrastive learning framework for generating photorealistic images from
simulated label maps, by learning from unpaired sets of both. Due to
potentially large scene differences between real images and label maps,
existing unpaired image translation methods lead to artifacts of scene
modification in synthesized images. We utilize simulated images as surrogate
targets for a contrastive loss, while ensuring consistency by utilizing
features from a reverse translation network. Our method enables bidirectional
label-image translations, which is demonstrated in a variety of scenarios and
datasets, including laparoscopy, ultrasound, and driving scenes. By comparing
with state-of-the-art unpaired translation methods, our proposed method is
shown to generate realistic and scene-accurate translations.","['Lin Zhang', 'Tiziano Portenier', 'Orcun Goksel']",2023-02-21T14:36:18Z,http://arxiv.org/abs/2302.10698v1,"['cs.CV', 'cs.LG']","Translation,Semantic Label Maps,Images,Domain-Specific Simulations,Deep Learning,Contrastive Learning,Photorealistic,Unpaired Sets,Bidirectional,Laparoscopy"
"Requirements Engineering Framework for Human-centered Artificial
  Intelligence Software Systems","[Context] Artificial intelligence (AI) components used in building software
solutions have substantially increased in recent years. However, many of these
solutions focus on technical aspects and ignore critical human-centered
aspects. [Objective] Including human-centered aspects during requirements
engineering (RE) when building AI-based software can help achieve more
responsible, unbiased, and inclusive AI-based software solutions. [Method] In
this paper, we present a new framework developed based on human-centered AI
guidelines and a user survey to aid in collecting requirements for
human-centered AI-based software. We provide a catalog to elicit these
requirements and a conceptual model to present them visually. [Results] The
framework is applied to a case study to elicit and model requirements for
enhancing the quality of 360 degree~videos intended for virtual reality (VR)
users. [Conclusion] We found that our proposed approach helped the project team
fully understand the human-centered needs of the project to deliver.
Furthermore, the framework helped to understand what requirements need to be
captured at the initial stages against later stages in the engineering process
of AI-based software.","['Khlood Ahmad', 'Mohamed Abdelrazek', 'Chetan Arora', 'Arbind Agrahari Baniya', 'Muneera Bano', 'John Grundy']",2023-03-06T06:37:50Z,http://arxiv.org/abs/2303.02920v2,"['cs.SE', 'cs.AI']","Requirements Engineering,Human-centered,Artificial Intelligence,Software Systems,User Survey,Catalog,Conceptual Model,Case Study,360 degree videos,Virtual Reality"
Implementing Continuous HRTF Measurement in Near-Field,"Head-related transfer function (HRTF) is an essential component to create an
immersive listening experience over headphones for virtual reality (VR) and
augmented reality (AR) applications. Metaverse combines VR and AR to create
immersive digital experiences, and users are very likely to interact with
virtual objects in the near-field (NF). The HRTFs of such objects are highly
individualized and dependent on directions and distances. Hence, a significant
number of HRTF measurements at different distances in the NF would be needed.
Using conventional static stop-and-go HRTF measurement methods to acquire these
measurements would be time-consuming and tedious for human listeners. In this
paper, we propose a continuous measurement system targeted for the NF, and
efficiently capturing HRTFs in the horizontal plane within 45 secs. Comparative
experiments are performed on head and torso simulator (HATS) and human
listeners to evaluate system consistency and robustness.","['Ee-Leng Tan', 'Santi Peksi', 'Woon-Seng Gan']",2023-03-15T05:41:35Z,http://arxiv.org/abs/2303.08379v4,"['eess.AS', 'cs.SD']","HRTF,Near-Field,Virtual Reality,Augmented Reality,Metaverse,Immersive listening,Head and torso simulator,Horizontal plane,Continuous measurement,Comparative experiments"
"3D Gaze Vis: Sharing Eye Tracking Data Visualization for Collaborative
  Work in VR Environment","Conducting collaborative tasks, e.g., multi-user game, in virtual reality
(VR) could enable us to explore more immersive and effective experience.
However, for current VR systems, users cannot communicate properly with each
other via their gaze points, and this would interfere with users' mutual
understanding of the intention. In this study, we aimed to find the optimal eye
tracking data visualization , which minimized the cognitive interference and
improved the understanding of the visual attention and intention between users.
We designed three different eye tracking data visualizations: gaze cursor, gaze
spotlight and gaze trajectory in VR scene for a course of human heart , and
found that gaze cursor from doctors could help students learn complex 3D heart
models more effectively. To further explore, two students as a pair were asked
to finish a quiz in VR environment, with sharing gaze cursors with each other,
and obtained more efficiency and scores. It indicated that sharing eye tracking
data visualization could improve the quality and efficiency of collaborative
work in the VR environment.","['Song Zhao', 'Shiwei Cheng', 'Chenshuang Zhu']",2023-03-19T12:00:53Z,http://arxiv.org/abs/2303.10635v1,"['cs.HC', 'cs.CY']","eye tracking data visualization,VR environment,collaborative work,gaze points,visual attention,intention,gaze cursor,gaze spotlight,gaze trajectory,human heart."
"Augmented Reality in Service of Human Operations on the Moon: Insights
  from a Virtual Testbed","Future astronauts living and working on the Moon will face extreme
environmental conditions impeding their operational safety and performance.
While it has been suggested that Augmented Reality (AR) Head-Up Displays (HUDs)
could potentially help mitigate some of these adversities, the applicability of
AR in the unique lunar context remains underexplored. To address this
limitation, we have produced an accurate representation of the lunar setting in
virtual reality (VR) which then formed our testbed for the exploration of
prospective operational scenarios with aerospace experts. Herein we present
findings based on qualitative reflections made by the first 6 study
participants. AR was found instrumental in several use cases, including the
support of navigation and risk awareness. Major design challenges were likewise
identified, including the importance of redundancy and contextual
appropriateness. Drawing on these findings, we conclude by outlining directions
for future research aimed at developing AR-based assistive solutions tailored
to the lunar setting.","['Leonie Becker', 'Tommy Nilsson', 'Paul Topf Aguiar de Medeiros', 'Flavie Rometsch']",2023-03-19T15:32:14Z,http://arxiv.org/abs/2303.10686v1,"['cs.HC', 'cs.CY', '93B51, 97M50', 'H.1.2; H.5.2; I.3.8; J.m; K.8.2; J.6']","Augmented Reality,Human Operations,Moon,Virtual Testbed,Environmental Conditions,Aerospace,Navigation,Risk Awareness,Redundancy,Contextual Appropriateness"
Simultaneous Color Computer Generated Holography,"Computer generated holography has long been touted as the future of augmented
and virtual reality (AR/VR) displays, but has yet to be realized in practice.
Previous high-quality, color holographic displays have made either a 3$\times$
sacrifice on frame rate by using a sequential color illumination scheme or used
more than one spatial light modulator (SLM) and/or bulky, complex optical
setups. The reduced frame rate of sequential color introduces distracting
judder and color fringing in the presence of head motion while the form factor
of current simultaneous color systems is incompatible with a head-mounted
display. In this work, we propose a framework for simultaneous color holography
that allows the use of the full SLM frame rate while maintaining a compact and
simple optical setup. Simultaneous color holograms are optimized through the
use of a perceptual loss function, a physics-based neural network wavefront
propagator, and a camera-calibrated forward model. We measurably improve
hologram quality compared to other simultaneous color methods and move one step
closer to the realization of color holographic displays for AR/VR.","['Eric Markley', 'Nathan Matsuda', 'Florian Schiffers', 'Oliver Cossairt', 'Grace Kuo']",2023-03-20T17:23:02Z,http://arxiv.org/abs/2303.11287v2,"['physics.optics', 'cs.GR']","color computer generated holography,augmented reality,virtual reality,spatial light modulator,frame rate,optical setup,judder,color fringing,head-mounted display,perceptual loss function"
Perceptual Requirements for World-Locked Rendering in AR and VR,"Stereoscopic, head-tracked display systems can show users realistic,
world-locked virtual objects and environments. However, discrepancies between
the rendering pipeline and physical viewing conditions can lead to perceived
instability in the rendered content resulting in reduced immersion and,
potentially, visually-induced motion sickness. Precise requirements to achieve
perceptually stable world-locked rendering (WLR) are unknown due to the
challenge of constructing a wide field of view, distortion-free display with
highly accurate head and eye tracking. We present a system capable of rendering
virtual objects over real-world references without perceivable drift under such
constraints. This platform is used to study acceptable errors in render camera
position for WLR in augmented and virtual reality scenarios, where we find an
order of magnitude difference in perceptual sensitivity. We conclude with an
analytic model which examines changes to apparent depth and visual direction in
response to camera displacement errors.","['Phillip Guan', 'Eric Penner', 'Joel Hegland', 'Benjamin Letham', 'Douglas Lanman']",2023-03-28T01:18:21Z,http://arxiv.org/abs/2303.15666v2,"['cs.GR', 'cs.HC']","head-tracked display systems,virtual objects,rendering pipeline,immersion,motion sickness,rendering,distortion-free display,head tracking,eye tracking,virtual reality"
Monitoring mental workload by EEG during a game in Virtual Reality,"During an activity, knowing the mental workload (MWL) of the user allows to
improve the Human-Machine Interactions (HMI). Indeed, the MWL has an impact on
the individual and its interaction with the environment. Monitoring it is
therefore a crucial issue. In this context, we have created the virtual game
Back to Pizza which is based on the N-back task (commonly used for measuring
MWL). In this more playful variant, users must carry out orders from customers
of a pizza food truck. It is an interactive game that involves the audience of
the IHM'23 conference, choosing several parameters like the number of
ingredients. During this experience, the objective is to measure MWL in real
time through an ElectroEncephaloGraph (EEG) and visual feedback on MWL level is
given to the audience. With this demonstration, we propose to present a concept
of a virtual interactive game that measures MWL in real time.","['Lina-Estelle Linelle Louis', 'Saïd Moussaoui', 'Vincent Roualdes', 'Aurélien van Langhenhove', 'Sébastien Ravoux', 'Isabelle Milleville-Pennel']",2023-03-28T09:03:38Z,http://arxiv.org/abs/2303.15831v1,['cs.HC'],"mental workload,EEG,Virtual Reality,Human-Machine Interactions,N-back task,interactive game,ElectroEncephaloGraph,IHM'23,real time"
Consistent View Synthesis with Pose-Guided Diffusion Models,"Novel view synthesis from a single image has been a cornerstone problem for
many Virtual Reality applications that provide immersive experiences. However,
most existing techniques can only synthesize novel views within a limited range
of camera motion or fail to generate consistent and high-quality novel views
under significant camera movement. In this work, we propose a pose-guided
diffusion model to generate a consistent long-term video of novel views from a
single image. We design an attention layer that uses epipolar lines as
constraints to facilitate the association between different viewpoints.
Experimental results on synthetic and real-world datasets demonstrate the
effectiveness of the proposed diffusion model against state-of-the-art
transformer-based and GAN-based approaches.","['Hung-Yu Tseng', 'Qinbo Li', 'Changil Kim', 'Suhib Alsisan', 'Jia-Bin Huang', 'Johannes Kopf']",2023-03-30T17:59:22Z,http://arxiv.org/abs/2303.17598v1,['cs.CV'],"View synthesis,Pose-guided,Diffusion models,Virtual Reality,Camera motion,Novel views,Attention layer,Epipolar lines,Transformer-based,GAN-based"
"Exploring Crossmodal Interaction of Tactile and Visual Cues on
  Temperature Perception in Virtual Reality: a Preliminary Study","VEs are typically limited to visual and auditory cues; however, recent
results show that multiple sensory modalities increase the immersion. In this
study, an experimental protocol is proposed to recreate multiple tactile, in
particular thermal, sensations in VR. The aim is twofold: (1) studying the
performance of different devices for creating warm and cold sensations with
regards to their efficiency and acoustic disturbance; and (2) investigating the
interdependency between visual and tactile stimuli in the perception of
temperature. 14 participants performed two experimental studies. Our results
show no acoustic disturbance of the materials used. Spot projector is more
efficient than fan heater to create a warm sensation; fan + water spray is more
efficient than fan alone to create cold sensation. Moreover, no significant
contribution of visual cue on the thermal perception was found except for the
extremely cold simulation (snow visualization and thermal stimulation performed
with fan + water spray).","['Clémentine Helfenstein-Didier', 'Amira Dhouib', 'Florent Favre', 'Jonathan Pascal', 'Patrick Baert']",2023-04-02T07:24:33Z,http://arxiv.org/abs/2304.00476v2,['cs.HC'],"Crossmodal interaction,Tactile cues,Visual cues,Temperature perception,Virtual reality,Experimental protocol,Thermal sensations,Efficiency,Acoustic disturbance,Interdependency"
"Demonstration of a Standalone, Descriptive, and Predictive Digital Twin
  of a Floating Offshore Wind Turbine","Digital Twins bring several benefits for planning, operation, and maintenance
of remote offshore assets. In this work, we explain the digital twin concept
and the capability level scale in the context of wind energy. Furthermore, we
demonstrate a standalone digital twin, a descriptive digital twin, and a
prescriptive digital twin of an operational floating offshore wind turbine. The
standalone digital twin consists of the virtual representation of the wind
turbine and its operating environment. While at this level the digital twin
does not evolve with the physical turbine, it can be used during the planning-,
design-, and construction phases. At the next level, the descriptive digital
twin is built upon the standalone digital twin by enhancing the latter with
real data from the turbine. All the data is visualized in virtual reality for
informed decision-making. Besides being used for data bundling and
visualization, the descriptive digital twin forms the basis for diagnostic,
predictive, prescriptive, and autonomous tools. A predictive digital twin is
created through the use of weather forecasts, neural networks, and transfer
learning. Finally, digital twin technology is discussed in a much wider context
of ocean engineering.","['Florian Stadtmann', 'Henrik Gusdal Wassertheurer', 'Adil Rasheed']",2023-04-03T15:53:22Z,http://arxiv.org/abs/2304.01093v1,['eess.SP'],"Digital Twin,Wind Energy,Floating Offshore Wind Turbine,Virtual Reality,Data Visualization,Predictive Maintenance,Neural Networks,Transfer Learning,Ocean Engineering"
An Accessible Toolkit for 360 VR Studies,"Virtual reality is expected to play a significant role in the transformation
of education and psychological studies. The possibilities for its application
as a visual research method can be enhanced as established frameworks and
toolkits are made more available to users, not just developers, advocates, and
technical academics, enhancing its controlled study impact. With an accessible
first design approach, we can overcome accessibility constraints and tap into
new research potential. The open-sourced toolkit demonstrates how game engine
technologies can be utilized to immerse participants in a 360-video environment
with curated text displayed at pre-set intervals. Allowing for researchers to
guide participants through virtual experiences intuitively through a desktop
application while the study unfolds in the users VR headset.","['Corrie Green', 'Chloë Farr', 'Yang Jiang']",2023-04-07T13:59:57Z,http://arxiv.org/abs/2304.03652v3,"['cs.HC', 'cs.MM']","virtual reality,education,psychological studies,toolkit,visual research method,game engine technologies,360-video environment,desktop application,VR headset"
"Pilgrimage to Pureland: Art, Perception and the Wutai Mural VR
  Reconstruction","Virtual reality (VR) supports audiences to engage with cultural heritage
proactively. We designed an easy-to-access and guided Pilgrimage To Pureland VR
reconstruction of Dunhuang Mogao Grottoes to offer the general public an
accessible and engaging way to explore the Dunhuang murals. We put forward an
immersive VR reconstruction paradigm that can efficiently convert complex 2D
artwork into a VR environment. We reconstructed the Mt. Wutai pilgrimage mural
in Cave 61, Mogao Grottoes, Dunhuang, into an immersive VR environment and
created a plot-based and interactive experience that offers users a more
accessible solution to visit, understand and appreciate the complex religious,
historical, and artistic value of Dunhuang murals. \textcolor{black}{Our system
remarkably smoothed users' approaches to those elusive cultural heritages.
Appropriate adaptation of plots and 3D VR transfer consistent with the original
art style could enhance the accessibility of cultural heritages.","['Rongxuan Mu', 'Yuhe Nie', 'Kent Cao', 'Ruoxin You', 'Yinzong Wei', 'Xin Tong']",2023-04-15T08:42:51Z,http://arxiv.org/abs/2304.07511v1,['cs.HC'],"Virtual reality,cultural heritage,pilgrimage,Dunhuang,mural,immersive,2D artwork,plot-based,interactive,VR environment"
Traffic Characteristics of Extended Reality,"This tutorial paper analyzes the traffic characteristics of immersive
experiences with extended reality (XR) technologies, including Augmented
reality (AR), virtual reality (VR), and mixed reality (MR). The current trend
in XR applications is to offload the computation and rendering to an external
server and use wireless communications between the XR head-mounted display
(HMD) and the access points. This paradigm becomes essential owing to (1) its
high flexibility (in terms of user mobility) compared to remote rendering
through a wired connection, and (2) the high computing power available on the
server compared to local rendering (on HMD). The requirements to facilitate a
pleasant XR experience are analyzed in three aspects: capacity (throughput),
latency, and reliability. For capacity, two VR experiences are analyzed: a
human eye-like experience and an experience with the Oculus Quest 2 HMD. For
latency, the key components of the motion-to-photon (MTP) delay are discussed.
For reliability, the maximum packet loss rate (or the minimum packet delivery
rate) is studied for different XR scenarios. Specifically, the paper reviews
optimization techniques that were proposed to reduce the latency, conserve the
bandwidth, extend the scalability, and/or increase the reliability to satisfy
the stringent requirements of the emerging XR applications.","['Abdullah Alnajim', 'Seyedmohammad Salehi', 'Chien-Chung Shen', 'Malcolm Smith']",2023-04-16T22:17:29Z,http://arxiv.org/abs/2304.07908v1,"['cs.NI', 'cs.GR']","Extended Reality,Traffic Characteristics,Augmented Reality,Virtual Reality,Mixed Reality,XR technologies,XR head-mounted display,Wireless Communications,Latency,Reliability"
Effects of Clutter on Egocentric Distance Perception in Virtual Reality,"To assess the impact of clutter on egocentric distance perception, we
performed a mixed-design study with 60 participants in four different virtual
environments (VEs) with three levels of clutter. Additionally, we compared the
indoor/outdoor VE characteristics and the HMD's FOV. The participants wore a
backpack computer and a wide FOV head-mounted display (HMD) as they
blind-walked towards three distinct targets at distances of 3m, 4.5m, and 6m.
The HMD's field of view (FOV) was programmatically limited to
165{\deg}$\times$110{\deg}, 110{\deg}$\times$110{\deg}, or
45{\deg}$\times$35{\deg}. The results showed that increased clutter in the
environment led to more precise distance judgment and less underestimation,
independent of the FOV. In comparison to outdoor VEs, indoor VEs showed more
accurate distance judgment. Additionally, participants made more accurate
judgements while looking at the VEs through wider FOVs.","['Sina Masnadi', 'Yahya Hmaiti', 'Eugene Taranta', 'Joseph J. LaViola Jr']",2023-04-17T20:44:46Z,http://arxiv.org/abs/2304.08604v1,['cs.HC'],"clutter,egocentric distance perception,virtual reality,virtual environments,HMD,FOV,indoor,outdoor,distance judgment,underestimation"
"From Artifacts to Outcomes: Comparison of HMD VR, Desktop, and Slides
  Lectures for Food Microbiology Laboratory Instruction","Despite the value of VR (Virtual Reality) for educational purposes, the
instructional power of VR in Biology Laboratory education remains
under-explored. Laboratory lectures can be challenging due to students' low
motivation to learn abstract scientific concepts and low retention rate.
Therefore, we designed a VR-based lecture on fermentation and compared its
effectiveness with lectures using PowerPoint slides and a desktop application.
Grounded in the theory of distributed cognition and motivational theories, our
study examined how learning happens in each condition from students' learning
outcomes, behaviors, and perceptions. Our result indicates that VR facilitates
students' long-term retention to learn by cultivating their longer visual
attention and fostering a higher sense of immersion, though students'
short-term retention remains the same across all conditions. This study extends
current research on VR studies by identifying the characteristics of each
teaching artifact and providing design implications for integrating VR
technology into higher education.","['Fei Xue', 'Rongchen Guo', 'Siyuan Yao', 'Luxin Wang', 'Kwan-Liu Ma']",2023-04-19T13:49:20Z,http://arxiv.org/abs/2304.09661v1,['cs.HC'],"VR,Biology Laboratory education,Laboratory lectures,PowerPoint slides,desktop application,distributed cognition,motivational theories,learning outcomes,immersion,higher education"
"NeuralField-LDM: Scene Generation with Hierarchical Latent Diffusion
  Models","Automatically generating high-quality real world 3D scenes is of enormous
interest for applications such as virtual reality and robotics simulation.
Towards this goal, we introduce NeuralField-LDM, a generative model capable of
synthesizing complex 3D environments. We leverage Latent Diffusion Models that
have been successfully utilized for efficient high-quality 2D content creation.
We first train a scene auto-encoder to express a set of image and pose pairs as
a neural field, represented as density and feature voxel grids that can be
projected to produce novel views of the scene. To further compress this
representation, we train a latent-autoencoder that maps the voxel grids to a
set of latent representations. A hierarchical diffusion model is then fit to
the latents to complete the scene generation pipeline. We achieve a substantial
improvement over existing state-of-the-art scene generation models.
Additionally, we show how NeuralField-LDM can be used for a variety of 3D
content creation applications, including conditional scene generation, scene
inpainting and scene style manipulation.","['Seung Wook Kim', 'Bradley Brown', 'Kangxue Yin', 'Karsten Kreis', 'Katja Schwarz', 'Daiqing Li', 'Robin Rombach', 'Antonio Torralba', 'Sanja Fidler']",2023-04-19T16:13:21Z,http://arxiv.org/abs/2304.09787v1,['cs.CV'],"NeuralField-LDM,Scene Generation,Hierarchical Latent Diffusion Models,3D scenes,Latent Diffusion Models,Scene auto-encoder,Voxel grids,Latent representation,Hierarchical diffusion model"
"Neural Radiance Fields: Past, Present, and Future","The various aspects like modeling and interpreting 3D environments and
surroundings have enticed humans to progress their research in 3D Computer
Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall
et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in
Computer Graphics, Robotics, Computer Vision, and the possible scope of
High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D
models have gained traction from res with more than 1000 preprints related to
NeRFs published. This paper serves as a bridge for people starting to study
these fields by building on the basics of Mathematics, Geometry, Computer
Vision, and Computer Graphics to the difficulties encountered in Implicit
Representations at the intersection of all these disciplines. This survey
provides the history of rendering, Implicit Learning, and NeRFs, the
progression of research on NeRFs, and the potential applications and
implications of NeRFs in today's world. In doing so, this survey categorizes
all the NeRF-related research in terms of the datasets used, objective
functions, applications solved, and evaluation criteria for these applications.",['Ansh Mittal'],2023-04-20T02:17:08Z,http://arxiv.org/abs/2304.10050v2,['cs.CV'],"Neural Radiance Fields,3D Computer Vision,Computer Graphics,Machine Learning,Implicit Representations,High-Resolution,Low Storage,Augmented Reality,Virtual Reality,Implicit Learning"
"Pedestrian wayfinding behavior in a multi-story building: a
  comprehensive modeling study featuring route choice, wayfinding performance,
  and observation behavior","This paper proposes a comprehensive approach for modeling pedestrian
wayfinding behavior in complex buildings. This study employs two types of
discrete choice models (i.e., MNL and PSL) featuring pedestrian route choice
behavior, and three multivariate linear regression (MLR) models featuring the
overall wayfinding performance and observation behavior (e.g., hesitation
behavior and head rotation). Behavioral and questionnaire data featuring
pedestrian wayfinding behavior and personal information were collected using a
Virtual Reality experiment. Four wayfinding tasks were designed to determine
how personal, infrastructure, and route characteristics affect indoor
pedestrian wayfinding behavior on three levels, including route choice,
wayfinding performance, and observation behavior. We find that pedestrian route
choice behavior is primarily influenced by route characteristics, whereas
wayfinding performance is also influenced by personal characteristics.
Observation behavior is mainly influenced by task complexity, personal
characteristics, and local properties of the routes that offer route
information. To the best of our knowledge, this work represents the first
attempt to investigate the impact of the same comprehensive set of variables on
various metrics feature wayfinding behavior simultaneously.","['Yan Feng', 'Dorine C. Duives']",2023-04-20T08:22:10Z,http://arxiv.org/abs/2304.11167v1,"['cs.HC', 'cs.CY']","pedestrian wayfinding behavior,route choice,wayfinding performance,observation behavior,discrete choice models,multivariate linear regression,Virtual Reality experiment,infrastructure,personal characteristics,task complexity"
Inclusive AR/VR: Accessibility Barriers for Immersive Technologies,"Augmented and virtual reality (AR/VR) hold significant potential to transform
how we communicate, collaborate, and interact with others. However, there has
been a lack of work to date investigating accessibility barriers in relation to
immersive technologies for people with disabilities. To address current gaps in
knowledge, we led two multidisciplinary Sandpits with key stakeholders
(including academic researchers, AR/VR industry specialists, people with lived
experience of disability, assistive technologists, and representatives from
national charities and special needs colleges) to collaboratively explore and
identify existing challenges with AR and VR experiences. We present key themes
that emerged from Sandpit activities and map out the interaction barriers
identified across a spectrum of impairments (including physical, cognitive,
visual, and auditory disabilities). We conclude with recommendations for future
work addressing the challenges highlighted to support the development of more
inclusive AR and VR experiences.","['Chris Creed', 'Maadh Al-Kalbani', 'Arthur Theil', 'Sayan Sarcar', 'Ian Williams']",2023-04-26T11:36:10Z,http://arxiv.org/abs/2304.13465v1,['cs.HC'],"immersive technologies,augmented reality,virtual reality,accessibility barriers,disabilities,assistive technologists,interaction barriers,impairments,inclusive experiences,Sandpit activities"
High-Fidelity 3D Face Generation from Natural Language Descriptions,"Synthesizing high-quality 3D face models from natural language descriptions
is very valuable for many applications, including avatar creation, virtual
reality, and telepresence. However, little research ever tapped into this task.
We argue the major obstacle lies in 1) the lack of high-quality 3D face data
with descriptive text annotation, and 2) the complex mapping relationship
between descriptive language space and shape/appearance space. To solve these
problems, we build Describe3D dataset, the first large-scale dataset with
fine-grained text descriptions for text-to-3D face generation task. Then we
propose a two-stage framework to first generate a 3D face that matches the
concrete descriptions, then optimize the parameters in the 3D shape and texture
space with abstract description to refine the 3D face model. Extensive
experimental results show that our method can produce a faithful 3D face that
conforms to the input descriptions with higher accuracy and quality than
previous methods. The code and Describe3D dataset are released at
https://github.com/zhuhao-nju/describe3d .","['Menghua Wu', 'Hao Zhu', 'Linjia Huang', 'Yiyu Zhuang', 'Yuanxun Lu', 'Xun Cao']",2023-05-05T06:10:15Z,http://arxiv.org/abs/2305.03302v1,['cs.CV'],"3D face generation,natural language descriptions,dataset,text-to-3D face,shape space,appearance space,framework,parameters,texture space,experimental results"
MPMNet: A Data-Driven MPM Framework for Dynamic Fluid-Solid Interaction,"High-accuracy, high-efficiency physics-based fluid-solid interaction is
essential for reality modeling and computer animation in online games or
real-time Virtual Reality (VR) systems. However, the large-scale simulation of
incompressible fluid and its interaction with the surrounding solid environment
is either time-consuming or suffering from the reduced time/space resolution
due to the complicated iterative nature pertinent to numerical computations of
involved Partial Differential Equations (PDEs). In recent years, we have
witnessed significant growth in exploring a different, alternative data-driven
approach to addressing some of the existing technical challenges in
conventional model-centric graphics and animation methods. This paper showcases
some of our exploratory efforts in this direction. One technical concern of our
research is to address the central key challenge of how to best construct the
numerical solver effectively and how to best integrate
spatiotemporal/dimensional neural networks with the available MPM's pressure
solvers.","['Jin Li', 'Yang Gao', 'Ju Dai', 'Shuai Li', 'Aimin Hao', 'Hong Qin']",2023-05-05T06:48:11Z,http://arxiv.org/abs/2305.03315v1,['cs.GR'],"MPMNet,Data-Driven,MPM Framework,Fluid-Solid Interaction,Physics-based,Incompressible Fluid,Solid Environment,Partial Differential Equations,Spatiotemporal Neural Networks,Pressure Solvers"
"Cognitive and Physical Activities Impair Perception of Smartphone
  Vibrations","Vibration feedback is common in everyday devices, from virtual reality
systems to smartphones. However, cognitive and physical activities may impede
our ability to sense vibrations from devices. In this study, we develop and
characterize a smartphone platform to investigate how a shape-memory task
(cognitive activity) and walking (physical activity) impair human perception of
smartphone vibrations. We measured how Apple's Core Haptics Framework
parameters can be used for haptics research, namely how hapticIntensity
modulates amplitudes of 230 Hz vibrations. A 23-person user study found that
physical (p<0.001) and cognitive (p=0.004) activity increase vibration
perception thresholds. Cognitive activity also increases vibration response
time (p<0.001). This work also introduces a smartphone platform that can be
used for out-of-lab vibration perception testing. Researchers can use our
smartphone platform and results to design better haptic devices for diverse,
unique populations.","['Kyle T. Yoshida', 'Joel X. Kiernan', 'Rachel A. G. Adenekan', 'Steven H. Trinh', 'Alexis J. Lowber', 'Allison M. Okamura', 'Cara M. Nunez']",2023-05-11T04:22:24Z,http://arxiv.org/abs/2305.06556v1,['cs.HC'],"vibration feedback,cognitive activity,physical activity,smartphone platform,Core Haptics Framework,hapticIntensity,vibration perception thresholds,vibration response time,haptic devices"
"A Fusion Model: Towards a Virtual, Physical and Cognitive Integration
  and its Principles","Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital
twin, Metaverse and other related digital technologies have attracted much
attention in recent years. These new emerging technologies are changing the
world significantly. This research introduces a fusion model, i.e. Fusion
Universe (FU), where the virtual, physical, and cognitive worlds are merged
together. Therefore, it is crucial to establish a set of principles for the
fusion model that is compatible with our physical universe laws and principles.
This paper investigates several aspects that could affect immersive and
interactive experience; and proposes the fundamental principles for Fusion
Universe that can integrate physical and virtual world seamlessly.","['Hao Lan Zhang', 'Yun Xue', 'Yifan Lu', 'Sanghyuk Lee']",2023-05-17T06:34:22Z,http://arxiv.org/abs/2305.09992v1,"['cs.AI', 'cs.HC']","Virtual Reality,Augmented Reality,Mixed Reality,digital twin,Metaverse,fusion model,Fusion Universe,immersive experience,interactive experience,physical universe laws"
Extended-XRI Body Interfaces for Hyper-Connected Metaverse Environments,"Hybrid mixed-reality (XR) internet-of-things (IoT) research, here called XRI,
aims at a strong integration between physical and virtual objects,
environments, and agents wherein IoT-enabled edge devices are deployed for
sensing, context understanding, networked communication and control of device
actuators. Likewise, as augmented reality systems provide an immersive overlay
on the environments, and virtual reality provides fully immersive environments,
the merger of these domains leads to immersive smart spaces that are
hyper-connected, adaptive and dynamic components that anchor the metaverse to
real-world constructs. Enabling the human-in-the-loop to remain engaged and
connected across these virtual-physical hybrid environments requires advances
in user interaction that are multi-dimensional. This work investigates the
potential to transition the user interface to the human body as an
extended-reality avatar with hybrid extended-body interfaces that can interact
both with the physical and virtual sides of the metaverse. It contributes: i)
an overview of metaverses, XRI, and avatarization concepts, ii) a taxonomy
landscape for extended XRI body interfaces, iii) an architecture and potential
interactions for XRI body designs, iv) a prototype XRI body implementation
based on the architecture, v) a design-science evaluation, toward enabling
future design research directions.","['Jie Guan', 'Alexis Morris']",2023-06-01T19:11:18Z,http://arxiv.org/abs/2306.01096v1,['cs.HC'],"extended-XRI,body interfaces,hyper-connected,metaverse environments,IoT,augmented reality,virtual reality,immersive smart spaces,human-in-the-loop"
"Extending the Metaverse: Hyper-Connected Smart Environments with Mixed
  Reality and the Internet of Things","The metaverse, i.e., the collection of technologies that provide a virtual
twin of the real world via mixed reality, internet of things, and others, is
gaining prominence. However, the metaverse faces challenges as it grows toward
mainstream adoption. Among these is the lack of strong connections between
metaverse objects and traditional physical objects and environments, which
leads to inconsistencies for users within metaverse environments. To address
this issue, this work explores the design and development of a framework for
bridging the physical environment and the metaverse through the use of
internet-of-things objects and mixed reality designs. The contributions of this
include: i) an architectural framework for extending the metaverse, ii) design
prototypes using the framework. Together, this exploration charts the course
toward a more cohesive and hyper-connected metaverse smart environment.","['Jie Guan', 'Alexis Morris', 'Jay Irizawa']",2023-06-01T20:42:49Z,http://arxiv.org/abs/2306.01137v1,['cs.HC'],"Metaverse,Hyper-Connected,Smart Environments,Mixed Reality,Internet of Things,Architectural Framework,Design Prototypes,Physical Environment,Virtual Twin"
Weight Bank Addition Photonic Accelerator for Artificial Intelligence,"Neural networks powered by artificial intelligence play a pivotal role in
current estimation and classification applications due to the escalating
computational demands of evolving deep learning systems. The hindrances posed
by existing computational limitations threaten to impede the further
progression of these neural networks. In response to these issues, we propose
neuromorphic networks founded on photonics that offer superior processing speed
than electronic counterparts, thereby enhancing support for real time, three
dimensional, and virtual reality applications. The weight bank, an integral
component of these networks has a direct bearing on their overall performance.
Our study demonstrates the implementation of a weight bank utilizing parallelly
cascaded micro ring resonators. We present our observations on neuromorphic
networks based on silicon on insulators, where cascaded MRRs play a crucial
role in mitigating interchannel and intrachannel cross talk, a persistent issue
in wavelength division multiplexing systems. Additionally, we design a standard
silicon photonic accelerator to perform weight addition. Optimized to offer
increased speed and reduced energy consumption, this photonic accelerator
ensures comparable processing power to electronic devices.","['Wenwen Zhang', 'Hao Zhang']",2023-06-03T05:44:26Z,http://arxiv.org/abs/2306.02009v1,"['physics.optics', 'cs.ET']","Neural networks,Artificial intelligence,Deep learning systems,Neuromorphic networks,Photonics,Weight bank,Micro ring resonators,Silicon on insulators,Wavelength division multiplexing,Photonic accelerator"
Knowledge-Driven Robot Program Synthesis from Human VR Demonstrations,"Aging societies, labor shortages and increasing wage costs call for
assistance robots capable of autonomously performing a wide array of real-world
tasks. Such open-ended robotic manipulation requires not only powerful
knowledge representations and reasoning (KR&R) algorithms, but also methods for
humans to instruct robots what tasks to perform and how to perform them. In
this paper, we present a system for automatically generating executable robot
control programs from human task demonstrations in virtual reality (VR). We
leverage common-sense knowledge and game engine-based physics to semantically
interpret human VR demonstrations, as well as an expressive and general task
representation and automatic path planning and code generation, embedded into a
state-of-the-art cognitive architecture. We demonstrate our approach in the
context of force-sensitive fetch-and-place for a robotic shopping assistant.
The source code is available at
https://github.com/ease-crc/vr-program-synthesis.","['Benjamin Alt', 'Franklin Kenghagho Kenfack', 'Andrei Haidu', 'Darko Katic', 'Rainer Jäkel', 'Michael Beetz']",2023-06-05T09:37:53Z,http://arxiv.org/abs/2306.02739v2,"['cs.RO', 'cs.AI', '68T30', 'D.1; F.3; I.2']","Knowledge representation and reasoning,Human-robot interaction,Robot control,Task demonstration,Virtual reality,Path planning,Code generation,Cognitive architecture,Robot program synthesis,Force-sensitive tasks"
"VR.net: A Real-world Dataset for Virtual Reality Motion Sickness
  Research","Researchers have used machine learning approaches to identify motion sickness
in VR experience. These approaches demand an accurately-labeled, real-world,
and diverse dataset for high accuracy and generalizability. As a starting point
to address this need, we introduce `VR.net', a dataset offering approximately
12-hour gameplay videos from ten real-world games in 10 diverse genres. For
each video frame, a rich set of motion sickness-related labels, such as
camera/object movement, depth field, and motion flow, are accurately assigned.
Building such a dataset is challenging since manual labeling would require an
infeasible amount of time. Instead, we utilize a tool to automatically and
precisely extract ground truth data from 3D engines' rendering pipelines
without accessing VR games' source code. We illustrate the utility of VR.net
through several applications, such as risk factor detection and sickness level
prediction. We continuously expand VR.net and envision its next version
offering 10X more data than the current form. We believe that the scale,
accuracy, and diversity of VR.net can offer unparalleled opportunities for VR
motion sickness research and beyond.","['Elliott Wen', 'Chitralekha Gupta', 'Prasanth Sasikumar', 'Mark Billinghurst', 'James Wilmott', 'Emily Skow', 'Arindam Dey', 'Suranga Nanayakkara']",2023-06-06T03:43:11Z,http://arxiv.org/abs/2306.03381v1,['cs.AI'],"machine learning,dataset,virtual reality,motion sickness,labels,camera movement,depth field,motion flow,ground truth data,rendering pipelines"
"Evolution of 3GPP Standards Towards True Extended Reality (XR) Support
  in 6G Networks","Extended reality (XR) is a key innovation of 5G-advanced and beyond networks.
The diverse XR use-cases, including virtual reality, augmented reality, and
mixed reality, transform the way humans interact with surrounding environments.
Thus, XR technology enables true immersive experiences of novel services
spanning, e.g., e-commerce, healthcare, and education, respectively. However,
the efficient support of XR services over existing and future cellular systems
is highly challenging and requires multiple radio design improvements, due to
the unique XR traffic and performance characteristics. Thus, this article
surveys the state-of-art 3GPP standardization activities (release-18) for
integrating the XR service class into the 5G-advanced specifications,
highlighting the major XR performance challenges. Furthermore, the paper
introduces valuable insights and research directions for supporting true XR
services over the next-generation 6G networks, where multiple novel radio
design mindsets and protocol enhancements are proposed and evaluated using
extensive system level simulations, including solutions for application-native
dynamic performance reporting, traffic-dependent control channel design,
collaborative device aggregation for XR capacity boosting and offload,
respectively.","['Ali A. Esswie', 'Morris Repeta']",2023-06-06T20:57:35Z,http://arxiv.org/abs/2306.04012v1,"['eess.SP', 'cs.NI']","3GPP,Extended Reality,XR,6G Networks,Immersive Experiences,Radio Design,Performance Characteristics,Standardization Activities,Traffic-dependent Control Channel"
Circular Rectifiction of 3D Video and Efficient Modification of 3D-HEVC,"Video acquired from multiple cameras located along a line is often rectified
to video virtually obtained from cameras with ideally parallel optical axes
collocated on a single plane and principal points on a line. Such an approach
simplifies video processing including depth estimation and compression.
Nowadays, for many application video, like virtual reality or virtual
navigation, the content is often acquired by cameras located nearly on a circle
or on a part of that. Therefore, we introduce new operation of circular
rectification that results in multiview video virtually obtained from cameras
located on an ideal arc and with optical axes that are collocated on a single
plane and they intersect in a single point. For the circularly rectified video,
depth estimation and compression are simplified. The standard 3DHEVC codec was
designed for rectified video and its efficiency is limited for video acquired
from cameras located on an arc. Therefore, we developed a 3-D HEVC codec
modified in order to compress efficiently circularly rectified video. The
experiments demonstrate its better performance than for the standard 3D-HEVC
codec.","['Jarosław Samelak', 'Marek Domański']",2023-06-09T22:25:29Z,http://arxiv.org/abs/2306.06285v1,"['cs.MM', 'eess.IV']","Circular rectification,3D video,Modification,3D-HEVC,Compression,Cameras,Depth estimation,Multiview video,Virtual reality,Virtual navigation."
"A Mechanistic Transform Model for Synthesizing Eye Movement Data with
  Improved Realism","This manuscript demonstrates an improved model-based approach for synthetic
degradation of previously captured eye movement signals. Signals recorded on a
high-quality eye tracking sensor are transformed such that their resulting eye
tracking signal quality is similar to recordings captured on a low-quality
target device. The proposed model improves the realism of the degraded signals
versus prior approaches by introducing a mechanism for degrading spatial
accuracy and temporal precision. Moreover, a percentile-matching technique is
demonstrated for mimicking the relative distributional structure of the signal
quality characteristics of the target data set. The model is demonstrated to
improve realism on a per-feature and per-recording basis using data from an
EyeLink 1000 eye tracker and an SMI eye tracker embedded within a virtual
reality platform. The model improves the median classification accuracy
performance metric by 35.7% versus the benchmark model towards the ideal metric
of 50%. This paper also expands the literature by providing an
application-agnostic realism assessment workflow for synthetically generated
eye movement signals.","['Henry Griffith', 'Samantha Aziz', 'Dillon J Lohr', 'Oleg Komogortsev']",2023-06-14T19:31:24Z,http://arxiv.org/abs/2306.08712v1,['cs.HC'],"model-based approach,synthetic degradation,eye movement signals,eye tracking sensor,spatial accuracy,temporal precision,percentile-matching technique,signal quality characteristics,classification accuracy,realism assessment"
"Towards Large-Scale Incremental Dense Mapping using Robot-centric
  Implicit Neural Representation","Large-scale dense mapping is vital in robotics, digital twins, and virtual
reality. Recently, implicit neural mapping has shown remarkable reconstruction
quality. However, incremental large-scale mapping with implicit neural
representations remains problematic due to low efficiency, limited video
memory, and the catastrophic forgetting phenomenon. To counter these
challenges, we introduce the Robot-centric Implicit Mapping (RIM) technique for
large-scale incremental dense mapping. This method employs a hybrid
representation, encoding shapes with implicit features via a multi-resolution
voxel map and decoding signed distance fields through a shallow MLP. We
advocate for a robot-centric local map to boost model training efficiency and
curb the catastrophic forgetting issue. A decoupled scalable global map is
further developed to archive learned features for reuse and maintain constant
video memory consumption. Validation experiments demonstrate our method's
exceptional quality, efficiency, and adaptability across diverse scales and
scenes over advanced dense mapping methods using range sensors. Our system's
code will be accessible at https://github.com/HITSZ-NRSL/RIM.git.","['Jianheng Liu', 'Haoyao Chen']",2023-06-18T04:26:12Z,http://arxiv.org/abs/2306.10472v3,['cs.RO'],"large-scale mapping,incremental dense mapping,implicit neural representation,robot-centric,hybrid representation,multi-resolution voxel map,signed distance fields,catastrophic forgetting,model training efficiency,video memory"
"A neuro-symbolic approach for multimodal reference expression
  comprehension","Human-Machine Interaction (HMI) systems have gained huge interest in recent
years, with reference expression comprehension being one of the main
challenges. Traditionally human-machine interaction has been mostly limited to
speech and visual modalities. However, to allow for more freedom in
interaction, recent works have proposed the integration of additional
modalities, such as gestures in HMI systems. We consider such an HMI system
with pointing gestures and construct a table-top object picking scenario inside
a simulated virtual reality (VR) environment to collect data. Previous works
for such a task have used deep neural networks to classify the referred object,
which lacks transparency. In this work, we propose an interpretable and
compositional model, crucial to building robust HMI systems for real-world
application, based on a neuro-symbolic approach to tackle this task. Finally we
also show the generalizability of our model on unseen environments and report
the results.","['Aman Jain', 'Anirudh Reddy Kondapally', 'Kentaro Yamada', 'Hitomi Yanaka']",2023-06-19T06:24:42Z,http://arxiv.org/abs/2306.10717v1,['cs.HC'],"neuro-symbolic approach,multimodal,reference expression comprehension,human-machine interaction,gestures,virtual reality,deep neural networks,interpretable model,compositional model,generalizability"
"Performance Evaluation of Transport Protocols and Roadmap to a
  High-Performance Transport Design for Immersive Applications","Immersive technologies such as virtual reality (VR), augmented reality (AR),
and holograms will change users' digital experience. These immersive
technologies have a multitude of applications, including telesurgeries,
teleconferencing, Internet shopping, computer games, etc. Holographic-type
communication (HTC) is a type of augmented reality media that provides an
immersive experience to Internet users. However, HTC has different
characteristics and network requirements, and the existing network architecture
and transport protocols may not be able to cope with the stringent network
requirements of HTC. Therefore, in this paper, we provide an in-depth and
critical study of the transport protocols for HTC. We also discuss the
characteristics and the network requirements for HTC. Based on the performance
evaluation of the existing transport protocols, we propose a roadmap to design
new high-performance transport protocols for immersive applications.","['Inayat Ali', 'Seungwoo Hong', 'Pyung-koo Park', 'Tae Yeon Kim']",2023-06-29T05:31:02Z,http://arxiv.org/abs/2306.16692v2,['cs.NI'],"Immersive technologies,virtual reality,augmented reality,holograms,telesurgeries,teleconferencing,network architecture,transport protocols,performance evaluation,high-performance transport design"
"Standalone, Descriptive, and Predictive Digital Twin of an Onshore Wind
  Farm in Complex Terrain","In this work, a digital twin with standalone, descriptive, and predictive
capabilities is created for an existing onshore wind farm located in complex
terrain. A standalone digital twin is implemented with a
virtual-reality-enabled 3D interface using openly available data on the
turbines and their environment. Real SCADA data from the wind farm are used to
elevate the digital twin to the descriptive level. The data are complemented
with weather forecasts from a microscale model nested into Scandinavian
meteorological forecasts, and wind resources are visualized inside the
human-machine interface. Finally, the weather data are used to infer
predictions on the hourly power production of each turbine and the whole wind
farm with a 61 hours forecasting horizon. The digital twin provides a data
platform and interface for power predictions with a visual explanation of the
prediction, and it serves as a basis for future work on digital twins.","['Florian Stadtmann', 'Adil Rasheed', 'Tore Rasmussen']",2023-07-05T08:17:42Z,http://arxiv.org/abs/2307.02097v1,['eess.SP'],"Standalone,Descriptive,Predictive,Digital Twin,Onshore Wind Farm,Complex Terrain,SCADA data,Weather forecasts,Power production,Human-machine interface"
VR Job Interview Using a Gender-Swapped Avatar,"Virtual Reality (VR) has emerged as a potential solution for mitigating bias
in a job interview by hiding the applicants' demographic features. The current
study examines the use of a gender-swapped avatar in a virtual job interview
that affects the applicants' perceptions and their performance evaluated by
recruiters. With a mixed-method approach, we first conducted a lab experiment
(N=8) exploring how using a gender-swapped avatar in a virtual job interview
impacts perceived anxiety, confidence, competence, and ability to perform.
Then, a semi-structured interview investigated the participants' VR interview
experiences using an avatar. Our findings suggest that using gender-swapped
avatars may reduce the anxiety that job applicants will experience during the
interview. Also, the affinity diagram produced seven key themes highlighting
the advantages and limitations of VR as an interview platform. These findings
contribute to the emerging field of VR-based recruitment and have practical
implications for promoting diversity and inclusion in the hiring process.","['Jieun Kim', 'Hauke Sandhaus', 'Susan R. Fussell']",2023-07-09T18:53:38Z,http://arxiv.org/abs/2307.04247v1,"['cs.HC', 'H.5.m']","Virtual reality,VR,Job interview,Gender-swapped avatar,Bias mitigation,Demographic features,Mixed-method approach,Perception,Performance evaluation,Recruitment"
"Cross-Layer Assisted Early Congestion Control for Cloud VR Services in
  5G Edge Network","Cloud virtual reality (VR) has emerged as a promising technology, offering
users a highly immersive and easily accessible experience. However, the current
5G radio access network faces challenges in accommodating the bursty traffic
generated by multiple cloudVR flows simultaneously, leading to congestion at
the 5G base station and increased delays. In this research, we present a
comprehensive quantitative analysis that highlights the underlying causes for
the poor delay performance of cloudVR flows within the existing 5G protocol
stack and network. To address these issues, we propose a novel cross-layer
informationassisted congestion control mechanism deployed in the 5G edge
network. Experiment results show that our mechanism enhances the number of
concurrent flows meeting delay standards by 1.5x to 2.5x, while maintaining a
smooth network load. These findings underscore the potential of leveraging 5G
edge nodes as a valuable resource to effectively meet the anticipated demands
of future services.","['Wanghong Yang', 'Wenji Du', 'Baosen Zhao', 'Yongmao Ren', 'Jianan Sun', 'Xu Zhou']",2023-07-10T12:56:41Z,http://arxiv.org/abs/2307.04529v1,['cs.NI'],"Cloud virtual reality,5G,congestion control,cross-layer,edge network"
On the importance of illustration for mathematical research,"Mathematical understanding is built in many ways. Among these, illustration
has been a companion and tool for research for as long as research has taken
place. We use the term illustration to encompass any way one might bring a
mathematical idea into physical form or experience, including hand-made
diagrams or models, computer visualization, 3D printing, and virtual reality,
among many others. The very process of illustration itself challenges our
mathematical understanding and forces us to answer questions we may not have
posed otherwise. It can even make mathematics an experimental science, in which
immersive exploration of data and representations drive the cycle of problem,
conjecture, and proof. Today, modern technology for the first time places the
production of highly complicated models within the reach of many individual
mathematicians. Here, we sketch the rich history of illustration, highlight
important recent examples of its contribution to research, and examine how it
can be viewed as a discipline in its own right.","['Rémi Coulon', 'Gabriel Dorfsman-Hopkins', 'Edmund Harriss', 'Martin Skrodzki', 'Katherine E. Stange', 'Glen Whitney']",2023-07-10T15:25:29Z,http://arxiv.org/abs/2307.04636v2,"['math.HO', '01-02, 01A65, 01A67, 00-02']","illustration,mathematical research,mathematical understanding,hand-made diagrams,models,computer visualization,3D printing,virtual reality,modern technology"
"Semantic Communications System with Model Division Multiple Access and
  Controllable Coding Rate for Point Cloud","Point cloud, as a 3D representation, is widely used in autonomous driving,
virtual reality (VR), and augmented reality (AR). However, traditional
communication systems think that the point cloud's semantic information is
irrelevant to communication, which hinders the efficient transmission of point
clouds in the era of artificial intelligence (AI). This paper proposes a point
cloud based semantic communication system (PCSC), which uses AI-based encoding
techniques to extract the semantic information of the point cloud and joint
source-channel coding (JSCC) technology to overcome the distortion caused by
noise channels and solve the ""cliff effect"" in traditional communication. In
addition, the system realizes the controllable coding rate without fine-tuning
the network. The method analyzes the coded semantic vector's importance and
discards semantically-unimportant information, thereby improving the
transmission efficiency. Besides, PCSC and the recently proposed non-orthogonal
model division multiple access (MDMA) technology are combined to design a point
cloud MDMA transmission system (M-PCSC) for multi-user transmission. Relevant
experimental results show that the proposed method outperforms the traditional
method 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2
metrics. In terms of transmission, the proposed method can effectively solve
the ""cliff effect"" in the traditional methods.","['Xiaoyi Liu', 'Haotai Liang', 'Zhicheng Bao', 'Chen Dong', 'Xiaodong Xu']",2023-07-12T09:16:33Z,http://arxiv.org/abs/2307.06027v1,['cs.MM'],"semantic communication system,model division multiple access,coding rate,point cloud,joint source-channel coding,artificial intelligence,transmission efficiency,non-orthogonal,PSNR,distortion"
Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction,"While the 5G New Radio (NR) network promises a huge uplift of the uplink
throughput, the improvement can only be seen when the User Equipment (UE) is
connected to the high-frequency millimeter wave (mmWave) band. With the rise of
uplink-intensive smartphone applications such as the real-time transmission of
UHD 4K/8K videos, and Virtual Reality (VR)/Augmented Reality (AR) contents,
uplink throughput prediction plays a huge role in maximizing the users' quality
of experience (QoE). In this paper, we propose using a ConvLSTM-based neural
network to predict the future uplink throughput based on past uplink throughput
and RF parameters. The network is trained using the data from real-world drive
tests on commercial 5G SA networks while riding commuter trains, which
accounted for various frequency bands, handover, and blind spots. To make sure
our model can be practically implemented, we then limited our model to only use
the information available via Android API, then evaluate our model using the
data from both commuter trains and other methods of transportation. The results
show that our model reaches an average prediction accuracy of 98.9\% with an
average RMSE of 1.80 Mbps across all unseen evaluation scenarios.","['Kasidis Arunruangsirilert', 'Jiro Katto']",2023-07-23T20:01:18Z,http://arxiv.org/abs/2307.12417v1,"['cs.NI', 'cs.LG', 'eess.SP']","5G standalone,uplink throughput,User Equipment (UE),millimeter wave (mmWave),ConvLSTM,neural network,RF parameters,drive tests,Android API,prediction accuracy"
A Virtual Reality Game to Improve Physical and Cognitive Acuity,"We present the Virtual Human Benchmark (VHB) game to evaluate and improve
physical and cognitive acuity. VHB simulates in 3D the BATAK lightboard game,
which is designed to improve physical reaction and hand-eye coordination, on
the \textit{Oculus Rift} and \textit{Quest} headsets. The game comprises the
\textit{reaction}, \textit{accumulator} and \textit{sequence} modes; \bj{along}
with the \textit{reaction} and \textit{accumulator} modes which mimic BATAK
functionalities, the \textit{sequence} mode involves the user repeating a
sequence of illuminated targets with increasing complexity to train visual
memory and cognitive processing. A first version of the game (VHB v1) was
evaluated against the real-world BATAK by 20 users, and their feedback was
utilized to improve game design and obtain a second version (VHB v2). Another
study to evaluate VHB v2 was conducted with 20 users, whose results confirmed
that the deign improvements enhanced game usability and user experience in
multiple respects. Also, logging and visualization of performance data such as
\textit{reaction time}, \textit{speed between targets} and \textit{completed
sequence patterns} provides useful data for coaches/therapists monitoring
sports/rehabilitation regimens.","['Blooma John', 'Ramanathan Subramanian', 'Jayan Chirayath Kurian']",2023-08-03T01:26:18Z,http://arxiv.org/abs/2308.01492v1,"['cs.HC', 'I.3.7']","Virtual Reality,Game,Physical,Cognitive,Acuity,Oculus Rift,Reaction,Accumulator,Sequence,Visual Memory"
"CrossTalk: Intelligent Substrates for Language-Oriented Interaction in
  Video-Based Communication and Collaboration","Despite the advances and ubiquity of digital communication media such as
videoconferencing and virtual reality, they remain oblivious to the rich
intentions expressed by users. Beyond transmitting audio, videos, and messages,
we envision digital communication media as proactive facilitators that can
provide unobtrusive assistance to enhance communication and collaboration.
Informed by the results of a formative study, we propose three key design
concepts to explore the systematic integration of intelligence into
communication and collaboration, including the panel substrate, language-based
intent recognition, and lightweight interaction techniques. We developed
CrossTalk, a videoconferencing system that instantiates these concepts, which
was found to enable a more fluid and flexible communication and collaboration
experience.","['Haijun Xia', 'Tony Wang', 'Aditya Gunturu', 'Peiling Jiang', 'William Duan', 'Xiaoshuo Yao']",2023-08-07T05:40:01Z,http://arxiv.org/abs/2308.03311v1,"['cs.HC', 'cs.AI', 'cs.CL']","intelligent substrates,language-oriented interaction,video-based communication,collaboration,digital communication media,language-based intent recognition,interaction techniques,videoconferencing system,communication experience"
"FocusFlow: Leveraging Focal Depth for Gaze Interaction in Virtual
  Reality","Current gaze input methods for VR headsets predominantly utilize the gaze ray
as a pointing cursor, often neglecting depth information in it. This study
introduces FocusFlow, a novel gaze interaction technique that integrates focal
depth into gaze input dimensions, facilitating users to actively shift their
focus along the depth dimension for interaction. A detection algorithm to
identify the user's focal depth is developed. Based on this, a layer-based UI
is proposed, which uses focal depth changes to enable layer switch operations,
offering an intuitive hands-free selection method. We also designed visual cues
to guide users to adjust focal depth accurately and get familiar with the
interaction process. Preliminary evaluations demonstrate the system's
usability, and several potential applications are discussed. Through FocusFlow,
we aim to enrich the input dimensions of gaze interaction, achieving more
intuitive and efficient human-computer interactions on headset devices.","['Chenyang Zhang', 'Tiansu Chen', 'Rohan Nedungadi', 'Eric Shaffer', 'Elahe Soltanaghai']",2023-08-10T05:45:31Z,http://arxiv.org/abs/2308.05352v2,['cs.HC'],"Virtual reality,Gaze interaction,Focal depth,Detection algorithm,Layer-based UI,Visual cues,Usability,Human-computer interaction,Headset devices"
Deepsea: A Meta-ocean Prototype for Undersea Exploration,"Metaverse has attracted great attention from industry and academia in recent
years. Metaverse for the ocean (Meta-ocean) is the implementation of the
Metaverse technologies in virtual emersion of the ocean which is beneficial for
people yearning for the ocean. It has demonstrated great potential for tourism
and education with its strong immersion and appealing interactive user
experience. However, quite limited endeavors have been spent on exploring the
full possibility of Meta-ocean, especially in modeling the movements of marine
creatures. In this paper, we first investigate the technology status of
Metaverse and virtual reality (VR) and develop a prototype that builds the
Meta-ocean in VR devices with strong immersive visual effects. Then, we
demonstrate a method to model the undersea scene and marine creatures and
propose an optimized path algorithm based on the Catmull-Rom spline to model
the movements of marine life. Finally, we conduct a user study to analyze our
Meta-ocean prototype. This user study illustrates that our new prototype can
give us strong immersion and an appealing interactive user experience.","['Jinyu Li', 'Ping Hu', 'Weicheng Cui', 'Tianyi Huang', 'Shenghui Cheng']",2023-08-11T01:39:34Z,http://arxiv.org/abs/2308.05901v1,"['cs.HC', 'cs.GR']","Metaverse,Meta-ocean,virtual reality (VR),immersive visual effects,undersea scene,marine creatures,Catmull-Rom spline,path algorithm,user study,interactive user experience"
"Driver Heterogeneity in Willingness to Give Control to Conditional
  Automation","The driver's willingness to give (WTG) control in conditionally automated
driving is assessed in a virtual reality based driving-rig, through their
choice to give away driving control and through the extent to which automated
driving is adopted in a mixed-traffic environment. Within- and across-class
unobserved heterogeneity and locus of control variations are taken into
account. The choice of giving away control is modelled using the mixed logit
(MIXL) and mixed latent class (LCML) model. The significant latent segments of
the locus of control are developed into internalizers and externalizers by the
latent class model (LCM) based on the taste heterogeneity identified from the
MIXL model. Results suggest that drivers choose to ""giveAway"" control of the
vehicle when greater concentration/attentiveness is required (e.g., in the
nighttime) or when they are interested in performing a non-driving-related task
(NDRT). In addition, it is observed that internalizers demonstrate more
heterogeneity compared to externalizers in terms of WTG.","['Muhammad Sajjad Ansar', 'Nael Alsaleh', 'Bilal Farooq']",2023-08-12T00:53:04Z,http://arxiv.org/abs/2308.06426v1,"['cs.HC', 'cs.SY', 'econ.EM', 'eess.SY']","driver heterogeneity,willingness to give control,conditional automation,virtual reality,automated driving,mixed-traffic environment,locus of control,mixed logit model,latent class model"
"Accurate Eye Tracking from Dense 3D Surface Reconstructions using
  Single-Shot Deflectometry","Eye-tracking plays a crucial role in the development of virtual reality
devices, neuroscience research, and psychology. Despite its significance in
numerous applications, achieving an accurate, robust, and fast eye-tracking
solution remains a considerable challenge for current state-of-the-art methods.
While existing reflection-based techniques (e.g., ""glint tracking"") are
considered the most accurate, their performance is limited by their reliance on
sparse 3D surface data acquired solely from the cornea surface. In this paper,
we rethink the way how specular reflections can be used for eye tracking: We
propose a novel method for accurate and fast evaluation of the gaze direction
that exploits teachings from single-shot phase-measuring-deflectometry (PMD).
In contrast to state-of-the-art reflection-based methods, our method acquires
dense 3D surface information of both cornea and sclera within only one single
camera frame (single-shot). Improvements in acquired reflection surface
points(""glints"") of factors $>3300 \times$ are easily achievable. We show the
feasibility of our approach with experimentally evaluated gaze errors of only
$\leq 0.25^\circ$ demonstrating a significant improvement over the current
state-of-the-art.","['Jiazhang Wang', 'Tianfu Wang', 'Bingjie Xu', 'Oliver Cossairt', 'Florian Willomitzer']",2023-08-14T17:36:39Z,http://arxiv.org/abs/2308.07298v2,"['cs.CV', 'cs.HC', 'physics.optics']","eye-tracking,3D surface reconstructions,single-shot deflectometry,gaze direction,reflection-based techniques,specular reflections,phase-measuring-deflectometry,dense 3D surface information,glints,gaze errors"
"Extended Preintegration for Relative State Estimation of Leader-Follower
  Platform","Relative state estimation using exteroceptive sensors suffers from
limitations of the field of view (FOV) and false detection, that the
proprioceptive sensor (IMU) data are usually engaged to compensate. Recently
ego-motion constraint obtained by Inertial measurement unit (IMU)
preintegration has been extensively used in simultaneous localization and
mapping (SLAM) to alleviate the computation burden. This paper introduces an
extended preintegration incorporating the IMU preintegration of two platforms
to formulate the motion constraint of relative state. One merit of this
analytic constraint is that it can be seamlessly integrated into the unified
graph optimization framework to implement the relative state estimation in a
high-performance real-time tracking thread, another point is a full smoother
design with this precise constraint to optimize the 3D coordinate and refine
the state for the refinement thread. We compare extensively in simulations the
proposed algorithms with two existing approaches to confirm our outperformance.
In the real virtual reality (VR) application design with the proposed
estimator, we properly realize the visual tracking of the six degrees of
freedom (6DoF) controller suitable for almost all scenarios, including the
challenging environment with missing features, light mutation, dynamic scenes,
etc. The demo video is at https://www.youtube.com/watch?v=0idb9Ls2iAM. For the
benefit of the community, we make the source code public.","['Ruican Xia', 'Hailong Pei']",2023-08-15T11:55:35Z,http://arxiv.org/abs/2308.07723v1,"['cs.RO', 'cs.MA']","exteroceptive sensors,proprioceptive sensor,IMU,preintegration,relative state estimation,SLAM,graph optimization,real-time tracking,virtual reality,6DoF"
"Affective Digital Twins for Digital Human: Bridging the Gap in
  Human-Machine Affective Interaction","In recent years, metaverse and digital humans have become important research
and industry areas of focus. However, existing digital humans still lack
realistic affective traits, making emotional interaction with humans difficult.
Grounded in the developments of artificial intelligence, human-computer
interaction, virtual reality, and affective computing, this paper proposes the
concept and technical framework of ""Affective Digital Twins for Digital Human""
based on the philosophy of digital twin technology. The paper discusses several
key technical issues including affective modeling, affective perception,
affective encoding, and affective expression. Based on this, the paper conducts
a preliminary imagination of the future application prospects of affective
digital twins for digital human, while considering potential problems that may
need to be addressed.","['Feng Lu', 'Bo Liu']",2023-08-20T09:15:21Z,http://arxiv.org/abs/2308.10207v1,['cs.HC'],"metaverse,digital humans,affective traits,emotional interaction,artificial intelligence,human-computer interaction,virtual reality,affective computing,affective modeling,affective perception"
I-BaR: Integrated Balance Rehabilitation Framework,"Neurological diseases are observed in approximately one billion people
worldwide. A further increase is foreseen at the global level as a result of
population growth and aging. Individuals with neurological disorders often
experience cognitive, motor, sensory, and lower extremity dysfunctions. Thus,
the possibility of falling and balance problems arise due to the postural
control deficiencies that occur as a result of the deterioration in the
integration of multi-sensory information. We propose a novel rehabilitation
framework, Integrated Balance Rehabilitation (I-BaR), to improve the
effectiveness of the rehabilitation with objective assessment, individualized
therapy, convenience with different disability levels and adoption of an
assist-as-needed paradigm and, with an integrated rehabilitation process as a
whole, i.e., ankle-foot preparation, balance, and stepping phases,
respectively. Integrated Balance Rehabilitation allows patients to improve
their balance ability by providing multi-modal feedback: visual via utilization
of Virtual Reality; vestibular via anteroposterior and mediolateral
perturbations with the robotic platform; proprioceptive via haptic feedback.","['Tugce Ersoy', 'Pınar Kaya', 'Elif Hocaoglu', 'Ramazan Unal']",2023-08-21T15:06:14Z,http://arxiv.org/abs/2308.10777v1,"['eess.SY', 'cs.SY']","balance rehabilitation,neurological diseases,postural control,multi-sensory information,objective assessment,individualized therapy,assist-as-needed paradigm,Virtual Reality,vestibular,proprioceptive"
"Edge-Centric Space Rescaling with Redirected Walking for Dissimilar
  Physical-Virtual Space Registration","We propose a novel space-rescaling technique for registering dissimilar
physical-virtual spaces by utilizing the effects of adjusting physical space
with redirected walking. Achieving a seamless immersive Virtual Reality (VR)
experience requires overcoming the spatial heterogeneities between the physical
and virtual spaces and accurately aligning the VR environment with the user's
tracked physical space. However, existing space-matching algorithms that rely
on one-to-one scale mapping are inadequate when dealing with highly dissimilar
physical and virtual spaces, and redirected walking controllers could not
utilize basic geometric information from physical space in the virtual space
due to coordinate distortion. To address these issues, we apply relative
translation gains to partitioned space grids based on the main interactable
object's edge, which enables space-adaptive modification effects of physical
space without coordinate distortion. Our evaluation results demonstrate the
effectiveness of our algorithm in aligning the main object's edge, surface, and
wall, as well as securing the largest registered area compared to alternative
methods under all conditions. These findings can be used to create an immersive
play area for VR content where users can receive passive feedback from the
plane and edge in their physical environment.","['Dooyoung Kim', 'Woontack Woo']",2023-08-22T05:47:12Z,http://arxiv.org/abs/2308.11210v1,['cs.HC'],"space-rescaling,redirected walking,physical-virtual spaces,spatial heterogeneities,VR environment,scale mapping,space grids,relative translation gains,main object's edge,immersive play area"
"Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer
  and NearFarMix Augmentation","In computer vision, depth estimation is crucial for domains like robotics,
autonomous vehicles, augmented reality, and virtual reality. Integrating
semantics with depth enhances scene understanding through reciprocal
information sharing. However, the scarcity of semantic information in datasets
poses challenges. Existing convolutional approaches with limited local
receptive fields hinder the full utilization of the symbiotic potential between
depth and semantics. This paper introduces a dataset-invariant semi-supervised
strategy to address the scarcity of semantic information. It proposes the Depth
Semantics Symbiosis module, leveraging the Symbiotic Transformer for achieving
comprehensive mutual awareness by information exchange within both local and
global contexts. Additionally, a novel augmentation, NearFarMix is introduced
to combat overfitting and compensate both depth-semantic tasks by strategically
merging regions from two images, generating diverse and structurally consistent
samples with enhanced control. Extensive experiments on NYU-Depth-V2 and KITTI
datasets demonstrate the superiority of our proposed techniques in indoor and
outdoor environments.","['Md Awsafur Rahman', 'Shaikh Anowarul Fattah']",2023-08-28T08:33:45Z,http://arxiv.org/abs/2308.14400v1,"['cs.CV', 'cs.LG']","Semantic Depth Estimation,Symbiotic Transformer,NearFarMix Augmentation,Computer Vision,Robotics,Autonomous Vehicles,Augmented Reality,Virtual Reality,Convolutional Approaches,Semi-Supervised Strategy"
"Deep Imitation Learning for Humanoid Loco-manipulation through Human
  Teleoperation","We tackle the problem of developing humanoid loco-manipulation skills with
deep imitation learning. The difficulty of collecting task demonstrations and
training policies for humanoids with a high degree of freedom presents
substantial challenges. We introduce TRILL, a data-efficient framework for
training humanoid loco-manipulation policies from human demonstrations. In this
framework, we collect human demonstration data through an intuitive Virtual
Reality (VR) interface. We employ the whole-body control formulation to
transform task-space commands by human operators into the robot's joint-torque
actuation while stabilizing its dynamics. By employing high-level action
abstractions tailored for humanoid loco-manipulation, our method can
efficiently learn complex sensorimotor skills. We demonstrate the effectiveness
of TRILL in simulation and on a real-world robot for performing various
loco-manipulation tasks. Videos and additional materials can be found on the
project page: https://ut-austin-rpl.github.io/TRILL.","['Mingyo Seo', 'Steve Han', 'Kyutae Sim', 'Seung Hyeon Bang', 'Carlos Gonzalez', 'Luis Sentis', 'Yuke Zhu']",2023-09-05T05:05:05Z,http://arxiv.org/abs/2309.01952v2,['cs.RO'],"deep imitation learning,humanoid,loco-manipulation,deep learning,whole-body control,task-space commands,joint-torque actuation,sensorimotor skills,simulation,real-world robot"
"Exploring the Opportunities of AR for Enriching Storytelling with Family
  Photos between Grandparents and Grandchildren","Storytelling with family photos, as an important mode of reminiscence-based
activities, can be instrumental in promoting intergenerational communication
between grandparents and grandchildren by strengthening generation bonds and
shared family values. Motivated by challenges that existing technology
approaches encountered for improving intergenerational storytelling (e.g., the
need to hold the tablet, the potential view detachment from the physical world
in Virtual Reality (VR)), we sought to find new ways of using Augmented Reality
(AR) to support intergenerational storytelling, which offers new capabilities
(e.g., 3D models, new interactivity) to enhance the expression for the
storyteller. We conducted a two-part exploratory study, where pairs of
grandparents and grandchildren 1) participated in an in-person storytelling
activity with a semi-structured interview 2) and then a participatory design
session with AR technology probes that we designed to inspire their
exploration. Our findings revealed insights into the possible ways of
intergenerational storytelling, the feasibility and usages of AR in
facilitating it, and the key design implications for leveraging AR in
intergenerational storytelling.","['Zisu Li', 'Li Feng', 'Chen Liang', 'Yuru Huang', 'Mingming Fan']",2023-09-07T07:37:28Z,http://arxiv.org/abs/2309.03533v1,['cs.HC'],"AR,storytelling,family photos,grandparents,grandchildren,intergenerational communication,technology approaches,Virtual Reality,Augmented Reality,design implications."
Panoramas from Photons,"Scene reconstruction in the presence of high-speed motion and low
illumination is important in many applications such as augmented and virtual
reality, drone navigation, and autonomous robotics. Traditional motion
estimation techniques fail in such conditions, suffering from too much blur in
the presence of high-speed motion and strong noise in low-light conditions.
Single-photon cameras have recently emerged as a promising technology capable
of capturing hundreds of thousands of photon frames per second thanks to their
high speed and extreme sensitivity. Unfortunately, traditional computer vision
techniques are not well suited for dealing with the binary-valued photon data
captured by these cameras because these are corrupted by extreme Poisson noise.
Here we present a method capable of estimating extreme scene motion under
challenging conditions, such as low light or high dynamic range, from a
sequence of high-speed image frames such as those captured by a single-photon
camera. Our method relies on iteratively improving a motion estimate by
grouping and aggregating frames after-the-fact, in a stratified manner. We
demonstrate the creation of high-quality panoramas under fast motion and
extremely low light, and super-resolution results using a custom single-photon
camera prototype. For code and supplemental material see our
$\href{https://wisionlab.com/project/panoramas-from-photons/}{\text{project
webpage}}$.","['Sacha Jungerman', 'Atul Ingle', 'Mohit Gupta']",2023-09-07T16:07:31Z,http://arxiv.org/abs/2309.03811v1,['cs.CV'],"scene reconstruction,high-speed motion,low illumination,single-photon cameras,motion estimation,computer vision techniques,Poisson noise,extreme scene motion,high dynamic range,super-resolution"
ArtHDR-Net: Perceptually Realistic and Accurate HDR Content Creation,"High Dynamic Range (HDR) content creation has become an important topic for
modern media and entertainment sectors, gaming and Augmented/Virtual Reality
industries. Many methods have been proposed to recreate the HDR counterparts of
input Low Dynamic Range (LDR) images/videos given a single exposure or
multi-exposure LDRs. The state-of-the-art methods focus primarily on the
preservation of the reconstruction's structural similarity and the pixel-wise
accuracy. However, these conventional approaches do not emphasize preserving
the artistic intent of the images in terms of human visual perception, which is
an essential element in media, entertainment and gaming. In this paper, we
attempt to study and fill this gap. We propose an architecture called
ArtHDR-Net based on a Convolutional Neural Network that uses multi-exposed LDR
features as input. Experimental results show that ArtHDR-Net can achieve
state-of-the-art performance in terms of the HDR-VDP-2 score (i.e., mean
opinion score index) while reaching competitive performance in terms of PSNR
and SSIM.","['Hrishav Bakul Barua', 'Ganesh Krishnasamy', 'KokSheik Wong', 'Kalin Stefanov', 'Abhinav Dhall']",2023-09-07T16:40:49Z,http://arxiv.org/abs/2309.03827v1,"['cs.CV', 'cs.GR', 'cs.LG', 'cs.MM', 'eess.IV', 'I.2.10; I.4.5; I.3.3; I.4.3']","High Dynamic Range (HDR),Low Dynamic Range (LDR),content creation,Convolutional Neural Network,HDR-VDP-2 score,mean opinion score index,PSNR,SSIM,media,entertainment,gaming"
VR Accessibility in Distance Adult Education,"As virtual reality (VR) technology becomes more pervasive, it continues to
find multiple new uses beyond research laboratories. One of them is distance
adult education -- the potential of VR to provide valuable education
experiences is massive, despite the current barriers to its widespread
application. Nevertheless, recent trends demonstrate clearly that VR is on the
rise in education settings, and VR-only courses are becoming more popular
across the globe. This trend will continue as more affordable VR solutions are
released commercially, increasing the number of education institutions that
benefit from the technology. No accessibility guidelines exist at present that
are created specifically for the design, development, and use of VR hardware
and software in distance education. The purpose of this workshop is to address
this niche. It gathers researchers and practitioners who are interested in
education and intend to work together to formulate a set of practical
guidelines for the use of VR in distance adult education to make it accessible
to a wider range of people.","['Bartosz Muczyński', 'Kinga Skorupska', 'Katarzyna Abramczuk', 'Cezary Biele', 'Zbigniew Bohdanowicz', 'Daniel Cnotkowski', 'Jazmin Collins', 'Wiesław KopeāE, 'Jarosław Kowalski', 'Grzegorz Pochwatko', 'Thomas Logan']",2023-09-08T10:21:51Z,http://arxiv.org/abs/2309.04245v1,['cs.HC'],"VR technology,distance adult education,education experiences,barriers,VR-only courses,accessibility guidelines,design,development,VR hardware,software"
Poster: Enabling Flexible Edge-assisted XR,"Extended reality (XR) is touted as the next frontier of the digital future.
XR includes all immersive technologies of augmented reality (AR), virtual
reality (VR), and mixed reality (MR). XR applications obtain the real-world
context of the user from an underlying system, and provide rich, immersive, and
interactive virtual experiences based on the user's context in real-time. XR
systems process streams of data from device sensors, and provide
functionalities including perceptions and graphics required by the
applications. These processing steps are computationally intensive, and the
challenge is that they must be performed within the strict latency requirements
of XR. This poses limitations on the possible XR experiences that can be
supported on mobile devices with limited computing resources.
  In this XR context, edge computing is an effective approach to address this
problem for mobile users. The edge is located closer to the end users and
enables processing and storing data near them. In addition, the development of
high bandwidth and low latency network technologies such as 5G facilitates the
application of edge computing for latency-critical use cases [4, 11]. This work
presents an XR system for enabling flexible edge-assisted XR.","['Jin Heo', 'Ketan Bhardwaj', 'Ada Gavrilovska']",2023-09-08T18:34:34Z,http://arxiv.org/abs/2309.04548v1,"['cs.DC', 'cs.MM']","extended reality,immersive technologies,augmented reality,virtual reality,mixed reality,XR applications,device sensors,edge computing,high bandwidth,low latency"
Shared Telemanipulation with VR controllers in an anti slosh scenario,"Telemanipulation has become a promising technology that combines human
intelligence with robotic capabilities to perform tasks remotely. However, it
faces several challenges such as insufficient transparency, low immersion, and
limited feedback to the human operator. Moreover, the high cost of haptic
interfaces is a major limitation for the application of telemanipulation in
various fields, including elder care, where our research is focused. To address
these challenges, this paper proposes the usage of nonlinear model predictive
control for telemanipulation using low-cost virtual reality controllers,
including multiple control goals in the objective function. The framework
utilizes models for human input prediction and taskrelated models of the robot
and the environment. The proposed framework is validated on an UR5e robot arm
in the scenario of handling liquid without spilling. Further extensions of the
framework such as pouring assistance and collision avoidance can easily be
included.","['Max Grobbel', 'Balint Varga', 'Sören Hohmann']",2023-09-14T13:46:59Z,http://arxiv.org/abs/2309.07714v1,"['cs.RO', 'cs.SY', 'eess.SY']","Telemanipulation,VR controllers,anti slosh,transparency,immersion,feedback,haptic interfaces,nonlinear model predictive control,UR5e robot arm"
"Data-Driven Goal Recognition in Transhumeral Prostheses Using Process
  Mining Techniques","A transhumeral prosthesis restores missing anatomical segments below the
shoulder, including the hand. Active prostheses utilize real-valued, continuous
sensor data to recognize patient target poses, or goals, and proactively move
the artificial limb. Previous studies have examined how well the data collected
in stationary poses, without considering the time steps, can help discriminate
the goals. In this case study paper, we focus on using time series data from
surface electromyography electrodes and kinematic sensors to sequentially
recognize patients' goals. Our approach involves transforming the data into
discrete events and training an existing process mining-based goal recognition
system. Results from data collected in a virtual reality setting with ten
subjects demonstrate the effectiveness of our proposed goal recognition
approach, which achieves significantly better precision and recall than the
state-of-the-art machine learning techniques and is less confident when wrong,
which is beneficial when approximating smoother movements of prostheses.","['Zihang Su', 'Tianshi Yu', 'Nir Lipovetzky', 'Alireza Mohammadi', 'Denny Oetomo', 'Artem Polyvyanyy', 'Sebastian Sardina', 'Ying Tan', 'Nick van Beest']",2023-09-15T02:03:59Z,http://arxiv.org/abs/2309.08106v1,"['cs.RO', 'cs.AI', 'cs.CV', 'I.2.4; I.2.9']","transhumeral prosthesis,process mining techniques,goal recognition,surface electromyography electrodes,kinematic sensors,time series data,process mining-based goal recognition system,virtual reality,machine learning techniques"
Head-Related Transfer Function Interpolation with a Spherical CNN,"Head-related transfer functions (HRTFs) are crucial for spatial soundfield
reproduction in virtual reality applications. However, obtaining personalized,
high-resolution HRTFs is a time-consuming and costly task. Recently, deep
learning-based methods showed promise in interpolating high-resolution HRTFs
from sparse measurements. Some of these methods treat HRTF interpolation as an
image super-resolution task, which neglects spatial acoustic features. This
paper proposes a spherical convolutional neural network method for HRTF
interpolation. The proposed method realizes the convolution process by
decomposing and reconstructing HRTF through the Spherical Harmonics (SHs). The
SHs, an orthogonal function set defined on a sphere, allow the convolution
layers to effectively capture the spatial features of HRTFs, which are sampled
on a sphere. Simulation results demonstrate the effectiveness of the proposed
method in achieving accurate interpolation from sparse measurements,
outperforming the SH method and learning-based methods.","['Xingyu Chen', 'Fei Ma', 'Yile Zhang', 'Amy Bastine', 'Prasanga N. Samarasinghe']",2023-09-15T10:11:37Z,http://arxiv.org/abs/2309.08290v1,"['eess.AS', 'cs.SD']","Head-Related Transfer Function,HRTF,spatial soundfield reproduction,virtual reality applications,deep learning,interpolation,spherical CNN,Spherical Harmonics,convolutional neural network,SH method"
"TELESIM: A Modular and Plug-and-Play Framework for Robotic Arm
  Teleoperation using a Digital Twin","We present TELESIM, a modular and plug-and-play framework for direct
teleoperation of a robotic arm using a digital twin as the interface between
the user and the robotic system. We tested TELESIM by performing a user survey
with 37 participants on two different robots using two different control
modalities: a virtual reality controller and a finger mapping hardware
controller using different grasping systems. Users were asked to teleoperate
the robot to pick and place 3 cubes in a tower and to repeat this task as many
times as possible in 10 minutes, with only 5 minutes of training beforehand.
Our experimental results show that most users were able to succeed by building
at least a tower of 3 cubes regardless of the control modality or robot used,
demonstrating the user-friendliness of TELESIM.","['Florent P Audonnet', 'Jonathan Grizou', 'Andrew Hamilton', 'Gerardo Aragon-Camarasa']",2023-09-19T12:38:28Z,http://arxiv.org/abs/2309.10579v2,['cs.RO'],"Modular,Plug-and-Play,Robotic Arm,Teleoperation,Digital Twin,User Survey,Control Modalities,Grasping Systems,User-Friendliness"
"VALID: A perceptually validated Virtual Avatar Library for Inclusion and
  Diversity","As consumer adoption of immersive technologies grows, virtual avatars will
play a prominent role in the future of social computing. However, as people
begin to interact more frequently through virtual avatars, it is important to
ensure that the research community has validated tools to evaluate the effects
and consequences of such technologies. We present the first iteration of a new,
freely available 3D avatar library called the Virtual Avatar Library for
Inclusion and Diversity (VALID), which includes 210 fully rigged avatars with a
focus on advancing racial diversity and inclusion. We present a detailed
process for creating, iterating, and validating avatars of diversity. Through a
large online study (n=132) with participants from 33 countries, we provide
statistically validated labels for each avatar's perceived race and gender.
Through our validation study, we also advance knowledge pertaining to the
perception of an avatar's race. In particular, we found that avatars of some
races were more accurately identified by participants of the same race.","['Tiffany D. Do', 'Steve Zelenty', 'Mar Gonzalez-Franco', 'Ryan P. McMahan']",2023-09-19T19:57:03Z,http://arxiv.org/abs/2309.10902v2,['cs.HC'],"immersive technologies,virtual avatars,social computing,3D avatar library,racial diversity,inclusion,online study,validation study,perceived race,gender"
"Video Screens for Hearing Research: Transmittance and Reflectance of
  Professional and Other Fabrics","Virtual reality labs for hearing research are commonly designed to achieve
maximal acoustical accuracy of virtual environments. For a high immersion, 3D
video systems are applied, that ideally do not influence the acoustical
conditions. In labs with projection systems, the video screens have a
potentially strong influence depending on their size, their acoustical
transmittance and their acoustical reflectance. In this study, the acoustical
transmittance and reflectance of six professional acoustic screen fabrics and
13 general purpose fabrics were measured considering two tension conditions.
Additionally, the influence of a black backing was tested, which is needed to
reduce the optical transparency of fabrics. The measured transmission losses
range from -5 dB to -0.1 dB and the reflected sound pressure levels from -32 dB
to -4 dB. The best acoustical properties were measured for a chiffon fabric.","['Jan Heeren', 'Giso Grimm', 'Stephan Ewert', 'Volker Hohmann']",2023-09-20T16:07:10Z,http://arxiv.org/abs/2309.11430v2,['physics.med-ph'],"video screens,hearing research,acoustical transmittance,acoustical reflectance,fabrics,virtual reality,acoustical accuracy,3D video systems,projection systems"
Folding Rays: a Bimanual Occluded Target Interaction Technique,"As Virtual Reality becomes commonplace in the world, it is important for
developers to focus on user interaction with the virtual world. Currently,
there are limitations to some selection and navigation techniques that have not
yet been completely overcome. Focusing specifically on enhancing ray-casting,
we present the advanced technique of folding rays which allows for the
selection of occluded targets without any unnecessary physical navigation
around a virtual environment. By improving upon current approaches, our
technique allows for the selection of these targets without any manipulation of
the virtual environment itself using rays that can bend at user-determined
points. With their potential to be used in conjunction with teleportation as a
virtual navigation technique, folding rays can be used in a variety of
scenarios to enhance a user's interactive experience in virtual environments.","['DongHoon Kim', 'Preston Bruner', 'Isaac Cho']",2023-09-21T19:23:55Z,http://arxiv.org/abs/2309.12442v1,['cs.HC'],"Virtual Reality,User Interaction,Ray-casting,Occluded Targets,Bimanual Interaction,Navigation Techniques,Folding Rays,Teleportation,Virtual Environment,Interactive Experience."
Tiled Multiplane Images for Practical 3D Photography,"The task of synthesizing novel views from a single image has useful
applications in virtual reality and mobile computing, and a number of
approaches to the problem have been proposed in recent years. A Multiplane
Image (MPI) estimates the scene as a stack of RGBA layers, and can model
complex appearance effects, anti-alias depth errors and synthesize soft edges
better than methods that use textured meshes or layered depth images. And
unlike neural radiance fields, an MPI can be efficiently rendered on graphics
hardware. However, MPIs are highly redundant and require a large number of
depth layers to achieve plausible results. Based on the observation that the
depth complexity in local image regions is lower than that over the entire
image, we split an MPI into many small, tiled regions, each with only a few
depth planes. We call this representation a Tiled Multiplane Image (TMPI). We
propose a method for generating a TMPI with adaptive depth planes for
single-view 3D photography in the wild. Our synthesized results are comparable
to state-of-the-art single-view MPI methods while having lower computational
overhead.","['Numair Khan', 'Douglas Lanman', 'Lei Xiao']",2023-09-25T16:56:40Z,http://arxiv.org/abs/2309.14291v1,['cs.CV'],"Tiled Multiplane Images,3D photography,Multiplane Image,RGBA layers,depth layers,graphics hardware,neural radiance fields,depth complexity,local image regions"
"""Can You Move It?"": The Design and Evaluation of Moving VR Shots in
  Sport Broadcast","Virtual Reality (VR) broadcasting has seen widespread adoption in major
sports events, attributed to its ability to generate a sense of presence,
curiosity, and excitement among viewers. However, we have noticed that still
shots reveal a limitation in the movement of VR cameras and hinder the VR
viewing experience in current VR sports broadcasts. This paper aims to bridge
this gap by engaging in a quantitative user analysis to explore the design and
impact of dynamic VR shots on viewing experiences. We conducted two user
studies in a digital hockey game twin environment and asked participants to
evaluate their viewing experience through two questionnaires. Our findings
suggested that the viewing experiences demonstrated no notable disparity
between still and moving shots for single clips. However, when considering
entire events, moving shots improved the viewer's immersive experience, with no
notable increase in sickness compared to still shots. We further discuss the
benefits of integrating moving shots into VR sports broadcasts and present a
set of design considerations and potential improvements for future VR sports
broadcasting.","['Xiuqi Zhu', 'Chenyi Wang', 'Zichun Guo', 'Yifan Zhao', 'Yang Jiao']",2023-09-25T19:33:27Z,http://arxiv.org/abs/2309.14490v2,['cs.HC'],"Virtual Reality,VR broadcasting,Sports events,VR cameras,User analysis,Dynamic shots,Viewing experience,Immersive experience,Moving shots,VR sports broadcasting"
"Accurate and Interactive Visual-Inertial Sensor Calibration with
  Next-Best-View and Next-Best-Trajectory Suggestion","Visual-Inertial (VI) sensors are popular in robotics, self-driving vehicles,
and augmented and virtual reality applications. In order to use them for any
computer vision or state-estimation task, a good calibration is essential.
However, collecting informative calibration data in order to render the
calibration parameters observable is not trivial for a non-expert. In this
work, we introduce a novel VI calibration pipeline that guides a non-expert
with the use of a graphical user interface and information theory in collecting
informative calibration data with Next-Best-View and Next-Best-Trajectory
suggestions to calibrate the intrinsics, extrinsics, and temporal misalignment
of a VI sensor. We show through experiments that our method is faster, more
accurate, and more consistent than state-of-the-art alternatives. Specifically,
we show how calibrations with our proposed method achieve higher accuracy
estimation results when used by state-of-the-art VI Odometry as well as VI-SLAM
approaches. The source code of our software can be found on:
https://github.com/chutsu/yac.","['Christopher L. Choi', 'Binbin Xu', 'Stefan Leutenegger']",2023-09-25T20:22:16Z,http://arxiv.org/abs/2309.14514v1,['cs.CV'],"Visual-Inertial Sensor,Calibration,Next-Best-View,Next-Best-Trajectory,Graphical User Interface,Information Theory,Intrinsic Parameters,Extrinsic Parameters,Temporal Misalignment,VI Odometry"
IEEE 802.11be Wi-Fi 7: Feature Summary and Performance Evaluation,"While the pace of commercial scale application of Wi-Fi 6 accelerates, the
IEEE 802.11 Working Group is about to complete the development of a new
amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known
as Wi-Fi 7, which can be used to meet the demand for the throughput of 4K/8K
videos up to tens of Gbps and low-latency video applications such as virtual
reality (VR) and augmented reality (AR). Wi-Fi 7 not only scales Wi-Fi 6 with
doubled bandwidth, but also supports real-time applications, which brings
revolutionary changes to Wi-Fi. In this article, we start by introducing the
main objectives and timeline of Wi-Fi 7 and then list the latest key techniques
which promote the performance improvement of Wi-Fi 7. Finally, we validate the
most critical objectives of Wi-Fi 7 -- the potential up to 30 Gbps throughput
and lower latency. System-level simulation results suggest that by combining
the new techniques, Wi-Fi 7 achieves 30 Gbps throughput and lower latency than
Wi-Fi 6.","['Xiaoqian Liu', 'Yuhan Dong', 'Yiqing Li', 'Yousi Lin', 'Xun Yang', 'Ming Gan']",2023-09-27T19:09:19Z,http://arxiv.org/abs/2309.15951v1,"['cs.NI', 'eess.SP']","IEEE 802.11be,Wi-Fi 7,Extremely High Throughput (EHT),throughput,bandwidth,low-latency,virtual reality (VR),augmented reality (AR),system-level simulation"
Encountered-Type Haptic Display via Tracking Calibrated Robot,"In the past decades, a variety of haptic devices have been developed to
facilitate high-fidelity human-computer interaction (HCI) in virtual reality
(VR). In particular, passive haptic feedback can create a compelling sensation
based on real objects spatially overlapping with their virtual counterparts.
However, these approaches require pre-deployment efforts, hindering their
democratizing use in practice. We propose the Tracking Calibrated Robot (TCR),
a novel and general haptic approach to free developers from deployment efforts,
which can be potentially deployed in any scenario. Specifically, we augment the
VR with a collaborative robot that renders haptic contact in the real world
while the user touches a virtual object in the virtual world. The distance
between the user's finger and the robot end-effector is controlled over time.
The distance starts to smoothly reduce to zero when the user intends to touch
the virtual object. A mock user study tested users' perception of three virtual
objects, and the result shows that TCR is effective in terms of conveying
discriminative shape information.","['Chenxi Xiao', 'Yuan Tian']",2023-09-28T18:04:48Z,http://arxiv.org/abs/2309.16768v1,"['cs.RO', 'cs.HC']","haptic display,tracking,calibrated robot,virtual reality,haptic feedback,collaborative robot,end-effector,user study"
5G Network Slicing: Analysis of Multiple Machine Learning Classifiers,"The division of one physical 5G communications infrastructure into several
virtual network slices with distinct characteristics such as bandwidth,
latency, reliability, security, and service quality is known as 5G network
slicing. Each slice is a separate logical network that meets the requirements
of specific services or use cases, such as virtual reality, gaming, autonomous
vehicles, or industrial automation. The network slice can be adjusted
dynamically to meet the changing demands of the service, resulting in a more
cost-effective and efficient approach to delivering diverse services and
applications over a shared infrastructure. This paper assesses various machine
learning techniques, including the logistic regression model, linear
discriminant model, k-nearest neighbor's model, decision tree model, random
forest model, SVC BernoulliNB model, and GaussianNB model, to investigate the
accuracy and precision of each model on detecting network slices. The report
also gives an overview of 5G network slicing.","['Mirsad Malkoc', 'Hisham A. Kholidy']",2023-10-03T02:16:50Z,http://arxiv.org/abs/2310.01747v1,"['cs.CR', 'cs.LG']","5G,Network Slicing,Machine Learning Classifiers,Bandwidth,Latency,Reliability,Security,Service Quality,Logistic Regression,Decision Tree"
"Animatable Virtual Humans: Learning pose-dependent human representations
  in UV space for interactive performance synthesis","We propose a novel representation of virtual humans for highly realistic
real-time animation and rendering in 3D applications. We learn pose dependent
appearance and geometry from highly accurate dynamic mesh sequences obtained
from state-of-the-art multiview-video reconstruction. Learning pose-dependent
appearance and geometry from mesh sequences poses significant challenges, as it
requires the network to learn the intricate shape and articulated motion of a
human body. However, statistical body models like SMPL provide valuable
a-priori knowledge which we leverage in order to constrain the dimension of the
search space enabling more efficient and targeted learning and define
pose-dependency. Instead of directly learning absolute pose-dependent geometry,
we learn the difference between the observed geometry and the fitted SMPL
model. This allows us to encode both pose-dependent appearance and geometry in
the consistent UV space of the SMPL model. This approach not only ensures a
high level of realism but also facilitates streamlined processing and rendering
of virtual humans in real-time scenarios.","['Wieland Morgenstern', 'Milena T. Bagdasarian', 'Anna Hilsmann', 'Peter Eisert']",2023-10-05T15:49:44Z,http://arxiv.org/abs/2310.03615v1,"['cs.CV', 'cs.GR']","virtual humans,pose-dependent,human representations,UV space,interactive performance synthesis,dynamic mesh sequences,multiview-video reconstruction,statistical body models,SMPL model,real-time animation"
"Evaluating a VR System for Collecting Safety-Critical Vehicle-Pedestrian
  Interactions","Autonomous vehicles (AVs) require comprehensive and reliable pedestrian
trajectory data to ensure safe operation. However, obtaining data of
safety-critical scenarios such as jaywalking and near-collisions, or uncommon
agents such as children, disabled pedestrians, and vulnerable road users poses
logistical and ethical challenges. This paper evaluates a Virtual Reality (VR)
system designed to collect pedestrian trajectory and body pose data in a
controlled, low-risk environment. We substantiate the usefulness of such a
system through semi-structured interviews with professionals in the AV field,
and validate the effectiveness of the system through two empirical studies: a
first-person user evaluation involving 62 participants, and a third-person
evaluative survey involving 290 respondents. Our findings demonstrate that the
VR-based data collection system elicits realistic responses for capturing
pedestrian data in safety-critical or uncommon vehicle-pedestrian interaction
scenarios.","['Erica Weng', 'Kenta Mukoya', 'Deva Ramanan', 'Kris Kitani']",2023-10-09T17:23:20Z,http://arxiv.org/abs/2310.05882v1,['cs.HC'],"VR system,safety-critical,vehicle-pedestrian interactions,Autonomous vehicles (AVs),pedestrian trajectory data,jaywalking,near-collisions,uncommon agents,children,disabled pedestrians,vulnerable road users,Virtual Reality (VR)"
"Spatially Continuous Non-Contact Cold Sensation Presentation Based on
  Low-Temperature Airflows","Our perception of cold enriches our understanding of the world and allows us
to interact with it. Therefore, the presentation of cold sensations will be
beneficial in improving the sense of immersion and presence in virtual reality
and the metaverse. This study proposed a novel method for spatially continuous
cold sensation presentation based on low-temperature airflows. We defined the
shortest distance between two airflows perceived as different cold stimuli as a
local cold stimulus group discrimination threshold (LCSGDT). By setting the
distance between airflows within the LCSGDT, spatially continuous cold
sensations can be achieved with an optimal number of cold airflows. We
hypothesized that the LCSGDTs are related to the heat-transfer capability of
airflows and developed a model to relate them. We investigated the LCSGDTs at a
flow rate of 25 L/min and presentation distances ranging from 10 to 50 mm. The
results showed that under these conditions, the LCSGDTs are 131.4 $\pm$ 1.9 mm,
and the heat-transfer capacity of the airflow corresponding to these LCSGDTs is
an almost constant value, that is, 0.92.","['Koyo Makino', 'Jiayi Xu', 'Akiko Kaneko', 'Naoto Ienaga', 'Yoshihiro Kuroda']",2023-10-13T04:44:50Z,http://arxiv.org/abs/2310.08853v1,['cs.HC'],"Cold sensation,Spatially continuous,Non-contact,Low-temperature airflows,Immersion,Virtual reality,Metaverse,Heat-transfer capability,Model"
"In the user's eyes we find trust: Using gaze data as a predictor or
  trust in an artifical intelligence","Trust is essential for our interactions with others but also with artificial
intelligence (AI) based systems. To understand whether a user trusts an AI,
researchers need reliable measurement tools. However, currently discussed
markers mostly rely on expensive and invasive sensors, like
electroencephalograms, which may cause discomfort. The analysis of gaze data
has been suggested as a convenient tool for trust assessment. However, the
relationship between trust and several aspects of the gaze behaviour is not yet
fully understood. To provide more insights into this relationship, we propose a
exploration study in virtual reality where participants have to perform a
sorting task together with a simulated AI in a simulated robotic arm embedded
in a gaming. We discuss the potential benefits of this approach and outline our
study design in this submission.","['Martin Johannes Dechant', 'Olga Lukashova-Sanz', 'Siegfried Wahl']",2023-10-25T14:38:21Z,http://arxiv.org/abs/2310.16672v1,['cs.HC'],"trust,gaze data,artificial intelligence,measurement tools,electroencephalograms,gaze behavior,virtual reality,sorting task,robotic arm,study design"
A No-Reference Quality Assessment Method for Digital Human Head,"In recent years, digital humans have been widely applied in augmented/virtual
reality (A/VR), where viewers are allowed to freely observe and interact with
the volumetric content. However, the digital humans may be degraded with
various distortions during the procedure of generation and transmission.
Moreover, little effort has been put into the perceptual quality assessment of
digital humans. Therefore, it is urgent to carry out objective quality
assessment methods to tackle the challenge of digital human quality assessment
(DHQA). In this paper, we develop a novel no-reference (NR) method based on
Transformer to deal with DHQA in a multi-task manner. Specifically, the front
2D projections of the digital humans are rendered as inputs and the vision
transformer (ViT) is employed for the feature extraction. Then we design a
multi-task module to jointly classify the distortion types and predict the
perceptual quality levels of digital humans. The experimental results show that
the proposed method well correlates with the subjective ratings and outperforms
the state-of-the-art quality assessment methods.","['Yingjie Zhou', 'Zicheng Zhang', 'Wei Sun', 'Xiongkuo Min', 'Xianghe Ma', 'Guangtao Zhai']",2023-10-25T16:01:05Z,http://arxiv.org/abs/2310.16732v1,"['cs.CV', 'eess.IV']","digital human,quality assessment,no-reference method,Transformer,vision transformer,multi-task,distortion types,perceptual quality levels,subjective ratings,state-of-the-art."
"Translating Universal Scene Descriptions into Knowledge Graphs for
  Robotic Environment","Robots performing human-scale manipulation tasks require an extensive amount
of knowledge about their surroundings in order to perform their actions
competently and human-like. In this work, we investigate the use of virtual
reality technology as an implementation for robot environment modeling, and
present a technique for translating scene graphs into knowledge bases. To this
end, we take advantage of the Universal Scene Description (USD) format which is
an emerging standard for the authoring, visualization and simulation of complex
environments. We investigate the conversion of USD-based environment models
into Knowledge Graph (KG) representations that facilitate semantic querying and
integration with additional knowledge sources.","['Giang Hoang Nguyen', 'Daniel Bessler', 'Simon Stelter', 'Mihai Pomarlan', 'Michael Beetz']",2023-10-25T16:09:40Z,http://arxiv.org/abs/2310.16737v2,"['cs.RO', 'cs.AI', 'cs.GR']","universal scene description,knowledge graphs,robotic environment,virtual reality technology,scene graphs,knowledge bases,semantic querying,complex environments,simulation,environment models"
"Socially Beneficial Metaverse: Framework, Technologies, Applications,
  and Challenges","In recent years, the maturation of emerging technologies such as Virtual
Reality, Digital twins, and Blockchain has accelerated the realization of the
metaverse. As a virtual world independent of the real world, the metaverse will
provide users with a variety of virtual activities that bring great convenience
to society. In addition, the metaverse can facilitate digital twins, which
offers transformative possibilities for the industry. Thus, the metaverse has
attracted the attention of the industry, and a huge amount of capital is about
to be invested. However, the development of the metaverse is still in its
infancy and little research has been undertaken so far. We describe the
development of the metaverse. Next, we introduce the architecture of the
socially beneficial metaverse (SB-Metaverse) and we focus on the technologies
that support the operation of SB-Metaverse. In addition, we also present the
applications of SB-Metaverse. Finally, we discuss several challenges faced by
SB-Metaverse which must be addressed in the future.","['Xiaolong Xu', 'Xuanhong Zhou', 'Muhammad Bilal', 'Sherali Zeadally', 'Jon Crowcroft', 'Lianyong Qi', 'Shengjun Xue']",2023-10-26T09:24:24Z,http://arxiv.org/abs/2310.17260v1,"['cs.CY', '68U01, 68M11, 68U35', 'A.1; K.4']","Virtual Reality,Digital twins,Blockchain,Metaverse,Socially Beneficial,Architecture,Technologies,Applications,Challenges,Investment"
"Force Rendering and Its Evaluation of a Friction-based Walking Sensation
  Display for a Seated User","Most existing locomotion devices that represent the sensation of walking
target a user who is actually performing a walking motion. Here, we attempted
to represent the walking sensation, especially a kinesthetic sensation and
advancing feeling (the sense of moving forward) while the user remains seated.
To represent the walking sensation using a relatively simple device, we focused
on the force rendering and its evaluation of the longitudinal friction force
applied on the sole during walking. Based on the measurement of the friction
force applied on the sole during actual walking, we developed a novel friction
force display that can present the friction force without the influence of body
weight. Using performance evaluation testing, we found that the proposed method
can stably and rapidly display friction force. Also, we developed a virtual
reality (VR) walk-through system that is able to present the friction force
through the proposed device according to the avatar's walking motion in a
virtual world. By evaluating the realism, we found that the proposed device can
represent a more realistic advancing feeling than vibration feedback.","['Ginga Kato', 'Yoshihiro Kuroda', 'Kiyoshi Kiyokawa', 'Haruo Takemura']",2023-10-30T14:06:20Z,http://arxiv.org/abs/2310.19555v1,['cs.HC'],"force rendering,friction-based,walking sensation,seated user,kinesthetic sensation,advancing feeling,longitudinal friction force,performance evaluation,virtual reality,VR walk-through"
"fMRI-PTE: A Large-scale fMRI Pretrained Transformer Encoder for
  Multi-Subject Brain Activity Decoding","The exploration of brain activity and its decoding from fMRI data has been a
longstanding pursuit, driven by its potential applications in brain-computer
interfaces, medical diagnostics, and virtual reality. Previous approaches have
primarily focused on individual subject analysis, highlighting the need for a
more universal and adaptable framework, which is the core motivation behind our
work. In this work, we propose fMRI-PTE, an innovative auto-encoder approach
for fMRI pre-training, with a focus on addressing the challenges of varying
fMRI data dimensions due to individual brain differences. Our approach involves
transforming fMRI signals into unified 2D representations, ensuring consistency
in dimensions and preserving distinct brain activity patterns. We introduce a
novel learning strategy tailored for pre-training 2D fMRI images, enhancing the
quality of reconstruction. fMRI-PTE's adaptability with image generators
enables the generation of well-represented fMRI features, facilitating various
downstream tasks, including within-subject and cross-subject brain activity
decoding. Our contributions encompass introducing fMRI-PTE, innovative data
transformation, efficient training, a novel learning strategy, and the
universal applicability of our approach. Extensive experiments validate and
support our claims, offering a promising foundation for further research in
this domain.","['Xuelin Qian', 'Yun Wang', 'Jingyang Huo', 'Jianfeng Feng', 'Yanwei Fu']",2023-11-01T07:24:22Z,http://arxiv.org/abs/2311.00342v1,['cs.CV'],"fMRI,Pretrained Transformer Encoder,Brain Activity Decoding,Auto-encoder,fMRI signals,2D representations,Learning Strategy,Image Generators,Within-subject decoding,Cross-subject decoding"
"UniFolding: Towards Sample-efficient, Scalable, and Generalizable
  Robotic Garment Folding","This paper explores the development of UniFolding, a sample-efficient,
scalable, and generalizable robotic system for unfolding and folding various
garments. UniFolding employs the proposed UFONet neural network to integrate
unfolding and folding decisions into a single policy model that is adaptable to
different garment types and states. The design of UniFolding is based on a
garment's partial point cloud, which aids in generalization and reduces
sensitivity to variations in texture and shape. The training pipeline
prioritizes low-cost, sample-efficient data collection. Training data is
collected via a human-centric process with offline and online stages. The
offline stage involves human unfolding and folding actions via Virtual Reality,
while the online stage utilizes human-in-the-loop learning to fine-tune the
model in a real-world setting. The system is tested on two garment types:
long-sleeve and short-sleeve shirts. Performance is evaluated on 20 shirts with
significant variations in textures, shapes, and materials. More experiments and
videos can be found in the supplementary materials and on the website:
https://unifolding.robotflow.ai","['Han Xue', 'Yutong Li', 'Wenqiang Xu', 'Huanyu Li', 'Dongzhe Zheng', 'Cewu Lu']",2023-11-02T14:25:10Z,http://arxiv.org/abs/2311.01267v1,"['cs.RO', 'cs.AI', 'cs.CV']","UniFolding,Robotic system,UFONet neural network,Garment folding,Point cloud,Data collection,Human-in-the-loop learning,Sample-efficient,Generalization,Real-world setting"
"Deep Learning-based 3D Point Cloud Classification: A Systematic Survey
  and Outlook","In recent years, point cloud representation has become one of the research
hotspots in the field of computer vision, and has been widely used in many
fields, such as autonomous driving, virtual reality, robotics, etc. Although
deep learning techniques have achieved great success in processing regular
structured 2D grid image data, there are still great challenges in processing
irregular, unstructured point cloud data. Point cloud classification is the
basis of point cloud analysis, and many deep learning-based methods have been
widely used in this task. Therefore, the purpose of this paper is to provide
researchers in this field with the latest research progress and future trends.
First, we introduce point cloud acquisition, characteristics, and challenges.
Second, we review 3D data representations, storage formats, and commonly used
datasets for point cloud classification. We then summarize deep learning-based
methods for point cloud classification and complement recent research work.
Next, we compare and analyze the performance of the main methods. Finally, we
discuss some challenges and future directions for point cloud classification.","['Huang Zhang', 'Changshuo Wang', 'Shengwei Tian', 'Baoli Lu', 'Liping Zhang', 'Xin Ning', 'Xiao Bai']",2023-11-05T09:28:43Z,http://arxiv.org/abs/2311.02608v1,['cs.CV'],"deep learning,3D,point cloud,classification,computer vision,autonomous driving,virtual reality,robotics,data representations,datasets"
LDM3D-VR: Latent Diffusion Model for 3D VR,"Latent diffusion models have proven to be state-of-the-art in the creation
and manipulation of visual outputs. However, as far as we know, the generation
of depth maps jointly with RGB is still limited. We introduce LDM3D-VR, a suite
of diffusion models targeting virtual reality development that includes
LDM3D-pano and LDM3D-SR. These models enable the generation of panoramic RGBD
based on textual prompts and the upscaling of low-resolution inputs to
high-resolution RGBD, respectively. Our models are fine-tuned from existing
pretrained models on datasets containing panoramic/high-resolution RGB images,
depth maps and captions. Both models are evaluated in comparison to existing
related methods.","['Gabriela Ben Melech Stan', 'Diana Wofk', 'Estelle Aflalo', 'Shao-Yen Tseng', 'Zhipeng Cai', 'Michael Paulitsch', 'Vasudev Lal']",2023-11-06T16:12:10Z,http://arxiv.org/abs/2311.03226v1,"['cs.CV', 'cs.AI']","Latent diffusion models,3D VR,RGBD,diffusion models,virtual reality development,LDM3D-VR,LDM3D-pano,LDM3D-SR,depth maps,upscaling"
"A Comprehensive Review of Leap Motion Controller-based Hand Gesture
  Datasets","This paper comprehensively reviews hand gesture datasets based on Ultraleap's
leap motion controller, a popular device for capturing and tracking hand
gestures in real-time. The aim is to offer researchers and practitioners a
valuable resource for developing and evaluating gesture recognition algorithms.
The review compares various datasets found in the literature, considering
factors such as target domain, dataset size, gesture diversity, subject
numbers, and data modality. The strengths and limitations of each dataset are
discussed, along with the applications and research areas in which they have
been utilized. An experimental evaluation of the leap motion controller 2
device is conducted to assess its capabilities in generating gesture data for
various applications, specifically focusing on touchless interactive systems
and virtual reality. This review serves as a roadmap for researchers, aiding
them in selecting appropriate datasets for their specific gesture recognition
tasks and advancing the field of hand gesture recognition using leap motion
controller technology.","['Bharatesh Chakravarthi', 'Prabhu Prasad B M', 'Pavan Kumar B N']",2023-11-07T22:33:18Z,http://arxiv.org/abs/2311.04373v1,['cs.HC'],"Hand gesture datasets,Leap Motion Controller,Ultraleap,Gesture recognition algorithms,Gesture diversity,Data modality,Touchless interactive systems,Virtual reality,Hand gesture recognition"
Twitter Sentiment Analysis of Covid Vacciness,"In this paper, we look at a database of tweets sorted by various keywords
that could indicate the users sentiment towards covid vaccines. With social
media becoming such a prevalent source of opinion, sorting and ranking tweets
that hold important information such as opinions on covid vaccines is of utmost
importance. Two different ranking scales were used, and ranking a tweet in this
way could represent the difference between an opinion being lost and an opinion
being featured on the site, which affects the decisions and behavior of people,
and why researchers were interested in it. Using natural language processing
techniques, our aim is to determine and categorize opinions about covid
vaccines with the highest accuracy possible.","['Wenbo Zhu', 'Tiechuan Hu']",2023-11-08T06:16:04Z,http://arxiv.org/abs/2311.04479v1,"['cs.CL', 'cs.IR', 'cs.LG', 'cs.SI']","Twitter,Sentiment Analysis,Covid Vaccines,Database,Keywords,Social Media,Ranking Scales,Natural Language Processing,Categorize,Accuracy"
"Where Do We Meet? Key Factors Influencing Collaboration Across Meeting
  Spaces","Over the past years, there has been a shift towards online and hybrid meeting
forms in workplace environments, partly as a consequence of various COVID-19
restrictions. However, the decision-making process on how to best collaborate
with team members is predominantly driven by practical concerns. While there is
a significant body of literature about where to best meet, this knowledge is
fragmented across various disciplines and hard to use in novel meeting
solutions. We present the Cross-Space Collaboration model which identifies the
main factors that drive the features of in-person collaboration and the meeting
aspects that influence these factors such as cognitive load. We designed the
model to give guidance to teams and individuals on how to meet in order to have
a higher collaboration effectiveness. Finally, we outline how the model can
bring added value within new meeting solutions, next generation virtual reality
meeting spaces and educational settings.","['Isaac Valadez', 'Sandra Trullemans', 'Beat Signer']",2023-11-08T14:27:56Z,http://arxiv.org/abs/2311.04707v1,"['cs.HC', 'H.5.3; H.5.2']","online meetings,hybrid meetings,workplace environments,COVID-19 restrictions,collaboration,team members,decision-making process,in-person collaboration,meeting aspects,cognitive load"
"The Impact of Changes to Daylight Illumination level on Architectural
  experience in Offices Based on VR and EEG","This study investigates the influence of varying illumination levels on
architectural experiences by employing a comprehensive approach that combines
self-reported assessments and neurophysiological measurements. Thirty
participants were exposed to nine distinct illumination conditions in a
controlled virtual reality environment. Subjective assessments, collected
through questionnaires in which participants were asked to rate how pleasant,
interesting, exciting, calming, complex, bright and spacious they found the
space. Objective measurements of brain activity were collected by
electroencephalogram (EEG). Data analysis demonstrated that illumination levels
significantly influenced cognitive engagement and different architectural
experience indicators. This alignment between subjective assessment and EEG
data underscores the relationship between illuminance and architectural
experiences. The study bridges the gap between quantitative and qualitative
assessments, providing a deeper understanding of the intricate connection
between lighting conditions and human responses. These findings contribute to
the enhancement of environmental design based on neuroscientific insights,
emphasizing the critical role of well-considered daylighting design in
positively influencing occupants' cognitive and emotional states within built
environments.","['Pegah Payedar-Ardakani', 'Yousef Gorji-Mahlabani', 'Abdolhamid Ghanbaran', 'Reza Ebrahimpour']",2023-11-08T21:23:37Z,http://arxiv.org/abs/2311.05028v2,"['cs.HC', 'q-bio.NC']","Daylight illumination level,Architectural experience,Offices,VR,EEG,Neurophysiological measurements,Subjective assessments,Brain activity,Data analysis,Environmental design"
"A Practical Guide to Implementing Off-Axis Stereo Projection Using
  Existing Ray Tracing Libraries","Virtual reality (VR) renderers driving CAVEs and similar immersive
environments use the off-axis stereo camera model so that a tracked user can
move freely in front of the projection plane. Geometrically, off-axis
projection results in asymmetric viewing frusta and generalizes the ubiquitous
perspective camera model to support positioning off the center of the
projection plane. VR renderers often integrate with larger visualization
systems that rely on libraries for position tracking and pose estimates, for
ray tracing-based rendering, and for user interaction. We demonstrate different
strategies to implement off-axis stereo projection within the constraints of
given VR applications and ray tracing libraries. We aim for minimal to no
adjustments required to the internal camera representation of such libraries.
We include host and shader code with the article that can be directly
integrated in custom applications.","['Stefan Zellmann', 'Jeff Amstutz']",2023-11-10T06:04:39Z,http://arxiv.org/abs/2311.05887v2,['cs.GR'],"off-axis stereo projection,ray tracing libraries,virtual reality,CAVEs,immersive environments,perspective camera model,geometrically,frusta,visualization systems,pose estimates"
"Zero-Shot Segmentation of Eye Features Using the Segment Anything Model
  (SAM)","The advent of foundation models signals a new era in artificial intelligence.
The Segment Anything Model (SAM) is the first foundation model for image
segmentation. In this study, we evaluate SAM's ability to segment features from
eye images recorded in virtual reality setups. The increasing requirement for
annotated eye-image datasets presents a significant opportunity for SAM to
redefine the landscape of data annotation in gaze estimation. Our investigation
centers on SAM's zero-shot learning abilities and the effectiveness of prompts
like bounding boxes or point clicks. Our results are consistent with studies in
other domains, demonstrating that SAM's segmentation effectiveness can be
on-par with specialized models depending on the feature, with prompts improving
its performance, evidenced by an IoU of 93.34% for pupil segmentation in one
dataset. Foundation models like SAM could revolutionize gaze estimation by
enabling quick and easy image segmentation, reducing reliance on specialized
models and extensive manual annotation.","['Virmarie Maquiling', 'Sean Anthony Byrne', 'Diederick C. Niehorster', 'Marcus Nyström', 'Enkelejda Kasneci']",2023-11-14T11:05:08Z,http://arxiv.org/abs/2311.08077v2,"['cs.CV', 'cs.AI', 'cs.HC']","image segmentation,foundation model,zero-shot learning,eye features,Segment Anything Model (SAM),annotated datasets,gaze estimation,prompts,bounding boxes,IoU"
DSR-Diff: Depth Map Super-Resolution with Diffusion Model,"Color-guided depth map super-resolution (CDSR) improve the spatial resolution
of a low-quality depth map with the corresponding high-quality color map,
benefiting various applications such as 3D reconstruction, virtual reality, and
augmented reality. While conventional CDSR methods typically rely on
convolutional neural networks or transformers, diffusion models (DMs) have
demonstrated notable effectiveness in high-level vision tasks. In this work, we
present a novel CDSR paradigm that utilizes a diffusion model within the latent
space to generate guidance for depth map super-resolution. The proposed method
comprises a guidance generation network (GGN), a depth map super-resolution
network (DSRN), and a guidance recovery network (GRN). The GGN is specifically
designed to generate the guidance while managing its compactness. Additionally,
we integrate a simple but effective feature fusion module and a
transformer-style feature extraction module into the DSRN, enabling it to
leverage guided priors in the extraction, fusion, and reconstruction of
multi-model images. Taking into account both accuracy and efficiency, our
proposed method has shown superior performance in extensive experiments when
compared to state-of-the-art methods. Our codes will be made available at
https://github.com/shiyuan7/DSR-Diff.","['Yuan Shi', 'Bin Xia', 'Rui Zhu', 'Qingmin Liao', 'Wenming Yang']",2023-11-16T14:18:10Z,http://arxiv.org/abs/2311.09919v1,"['cs.CV', 'cs.AI']","Depth map super-resolution,Diffusion model,Color-guided,Guidance generation network,Latent space,Feature fusion module,Transformer-style feature extraction,Multi-model images,Superior performance,State-of-the-art."
Context-Dependent Memory in Situated Visualization,"Situated visualization presents data alongside their source context (physical
referent). While environmental factors influence memory recall (known as
Context-Dependent Memory or CDM), how physical context affects cognition in
real-world tasks such as working with visualizations in situated contexts is
unclear. This study explores the design space of information memorability in
situated visualization through the lens of CDM. We investigate the presence of
physical referents for creating contextual cues in desktop and Virtual Reality
(VR) environments. Across three studies (n=144), we observe a trend suggesting
a CDM effect due to contextual referent is more apparent in VR. Overall, we did
not find statistically significant evidence of a CDM effect due to the presence
of a referent. However, we did find a significant CDM effect for lighting
conditions. This suggests that representing the entire environment, rather than
the physical objects alone, may be necessary to provide sufficiently strong
contextual memory cues.","['Kadek Ananta Satriadi', 'Benjamin Tag', 'Tim Dwyer']",2023-11-21T02:01:12Z,http://arxiv.org/abs/2311.12288v1,['cs.HC'],"Situated visualization,Context-Dependent Memory,Cognition,Design space,Information memorability,Physical referents,Virtual Reality (VR),Contextual cues,Lighting conditions,Environment."
AR Visualization System for Ship Detection and Recognition Based on AI,"Augmented reality technology has been widely used in industrial design
interaction, exhibition guide, information retrieval and other fields. The
combination of artificial intelligence and augmented reality technology has
also become a future development trend. This project is an AR visualization
system for ship detection and recognition based on AI, which mainly includes
three parts: artificial intelligence module, Unity development module and
Hololens2AR module. This project is based on R3Det algorithm to complete the
detection and recognition of ships in remote sensing images. The recognition
rate of model detection trained on RTX 2080Ti can reach 96%. Then, the 3D model
of the ship is obtained by ship categories and information and generated in the
virtual scene. At the same time, voice module and UI interaction module are
added. Finally, we completed the deployment of the project on Hololens2 through
MRTK. The system realizes the fusion of computer vision and augmented reality
technology, which maps the results of object detection to the AR field, and
makes a brave step toward the future technological trend and intelligent
application.","['Ziqi Ye', 'Limin Huang', 'Yongji Wu', 'Min Hu']",2023-11-21T08:42:44Z,http://arxiv.org/abs/2311.12430v1,"['cs.CV', '68T07', 'I.2; I.4']","artificial intelligence,augmented reality,ship detection,recognition,R3Det algorithm,Unity,Hololens2AR,computer vision,MRTK"
"Performance Analysis Of Binaural Signal Matching (BSM) in the
  Time-Frequency Domain","The capture and reproduction of spatial audio is becoming increasingly
popular, with the mushrooming of applications in teleconferencing,
entertainment and virtual reality. Many binaural reproduction methods have been
developed and studied extensively for spherical and other specially designed
arrays. However, the recent increased popularity of wearable and mobile arrays
requires the development of binaural reproduction methods for these arrays. One
such method is binaural signal matching (BSM). However, to date this method has
only been investigated with fixed matched filters designed for long audio
recordings. With the aim of making the BSM method more adaptive to dynamic
environments, this paper analyzes BSM with a parameterized sound-field in the
time-frequency domain. The paper presents results of implementing the BSM
method on a sound-field that was decomposed into its direct and reverberant
components, and compares this implementation with the BSM computed for the
entire sound-field, to compare performance for binaural reproduction of
reverberant speech in a simulated environment.","['Ami Berger', 'Vladimir Tourbabin', 'Jacob Donley', 'Zamir Ben-Hur', 'Boaz Rafaely']",2023-11-22T13:38:37Z,http://arxiv.org/abs/2311.13390v2,['eess.AS'],"binaural signal matching,time-frequency domain,spatial audio,sound-field,matched filters,dynamic environments,reverberant speech,binaural reproduction,virtual reality,teleconferencing"
"Filasofia: A Framework for Streamlined Development of Real-Time Surgical
  Simulations","Virtual reality simulation has become a popular approach for training and
assessing medical students. It offers diverse scenarios, realistic visuals, and
quantitative performance metrics for objective evaluation. However, creating
these simulations can be time-consuming and complex, even for experienced
users. The SOFA framework is an open-source solution that efficiently simulates
finite element (FE) models in real-time. Yet, some users find it challenging to
navigate the software due to the numerous components required for a basic
simulation and their variability. Additionally, SOFA has limited visual
rendering capabilities, leading developers to integrate other software for
high-quality visuals. To address these issues, we developed Filasofia, a
dedicated framework that simplifies development, provides modern visualization,
and allows fine-tuning using SOFA objects. Our experiments demonstrate that
Filasofia outperforms conventional SOFA simulations, even with real-time
subdivision. Our design approach aims to streamline development while offering
flexibility for fine-tuning. Future work will focus on further simplification
of the development process for users.","['Vladimir Poliakov', 'Dzmitry Tsetserukou', 'Emmanuel Vander Poorten']",2023-11-24T14:33:41Z,http://arxiv.org/abs/2311.14508v1,['cs.SE'],"virtual reality simulation,medical students,SOFA framework,finite element models,real-time simulation,visual rendering,Filasofia,modern visualization,fine-tuning,development process"
PISA: Point-cloud-based Instructed Scene Augmentation,"Indoor scene augmentation has become an emerging topic in the field of
computer vision with applications in augmented and virtual reality. However,
existing scene augmentation methods mostly require a pre-built object database
with a given position as the desired location. In this paper, we propose the
first end-to-end multi-modal deep neural network that can generate point cloud
objects consistent with their surroundings, conditioned on text instructions.
Our model generates a seemly object in the appropriate position based on the
inputs of a query and point clouds, thereby enabling the creation of new
scenarios involving previously unseen layouts of objects. Database of
pre-stored CAD models is no longer needed. We use Point-E as our generative
model and introduce methods including quantified position prediction and Top-K
estimation to mitigate the false negative problems caused by ambiguous language
description. Moreover, we evaluate the ability of our model by demonstrating
the diversity of generated objects, the effectiveness of instruction, and
quantitative metric results, which collectively indicate that our model is
capable of generating realistic in-door objects. For a more thorough
evaluation, we also incorporate visual grounding as a metric to assess the
quality of the scenes generated by our model.","['Yiyang Luo', 'Ke Lin']",2023-11-26T06:40:16Z,http://arxiv.org/abs/2311.16501v1,['cs.CV'],"Point-cloud,Instructed scene augmentation,Computer vision,Deep neural network,Point clouds,Text instructions,CAD models,Generative model,Visual grounding,Quantitative metric"
"iMagLS: Interaural Level Difference with Magnitude Least-Squares Loss
  for Optimized First-Order Head-Related Transfer Function","Binaural reproduction for headphone-based listening is an active research
area due to its widespread use in evolving technologies such as augmented and
virtual reality (AR and VR). On the one hand, these applications demand high
quality spatial audio perception to preserve the sense of immersion. On the
other hand, recording devices may only have a few microphones, leading to
low-order representations such as first-order Ambisonics (FOA). However,
first-order Ambisonics leads to limited externalization and spatial resolution.
In this paper, a novel head-related transfer function (HRTF) preprocessing
optimization loss is proposed, and is minimized using nonlinear programming.
The new method, denoted iMagLS, involves the introduction of an interaural
level difference (ILD) error term to the now widely used MagLS optimization
loss for the lateral plane angles. Results indicate that the ILD error could be
substantially reduced, while the HRTF magnitude error remains similar to that
obtained with MagLS. These results could prove beneficial to the overall
spatial quality of first-order Ambisonics, while other reproduction methods
could also benefit from considering this modified loss.","['Or Berebi', 'Zamir Ben-Hur', 'David Lou Alon', 'Boaz Rafaely']",2023-11-28T11:25:12Z,http://arxiv.org/abs/2311.16702v1,"['eess.AS', 'cs.SD']","Interaural Level Difference,Magnitude Least-Squares Loss,Optimized,Head-Related Transfer Function,Spatial Audio Perception,Immersion,First-Order Ambisonics,Nonlinear Programming,HRTF Preprocessing,Externalization"
Introducing STRAUSS: A flexible sonification Python package,"We introduce STRAUSS (Sonification Tools and Resources for Analysis Using
Sound Synthesis) a modular, self-contained and flexible Python sonification
package, operating in a free and open source (FOSS) capacity. STRAUSS is
intended to be a flexible tool suitable for both scientific data exploration
and analysis as well as for producing sonifications that are suitable for
public outreach and artistic contexts. We explain the motivations behind
STRAUSS, and how these lead to our design choices. We also describe the basic
code structure and concepts. We then present output sonification examples,
specifically: (1) multiple representations of univariate data (i.e., single
data series) for data exploration; (2) how multi-variate data can be mapped
onto sound to help interpret how those data variables are related and; (3) a
full spatial audio example for immersive Virtual Reality. We summarise,
alluding to some of the future functionality as STRAUSS development
accelerates.","['James W. Trayford', 'Chris M. Harrison']",2023-11-28T14:58:53Z,http://arxiv.org/abs/2311.16847v1,"['cs.SD', 'astro-ph.IM', 'cs.HC', 'eess.AS']","sonification,Python package,FOSS,scientific data exploration,data analysis,public outreach,artistic contexts,code structure,spatial audio,Virtual Reality"
"A Metadata Generation System with Semantic Understanding for Video
  Retrieval in Film Production","In film production, metadata plays an important role in original raw video
indexing and classification within the industrial post-production software.
Inspired by deep visual-semantic methods, we propose an automated image
information extraction process to extend the diversity of metadata entities for
massive large-scale raw video searching and retrieval. In this paper, we
introduce the proposed system architecture and modules, integrating semantic
annotation models and user-demand-oriented information fusion. We conducted
experiments to validate the effectiveness of our system on Film Raw Video
Semantic Annotation Dataset (Film-RVSAD) and Slate Board Template Dataset
(SBTD), two benchmark datasets built for cinematography-related semantic
annotation and slate detection. Experimental results show that the proposed
system provides an effective strategy to improve the efficiency of metadata
generation and transformation, which is necessary and convenient for
collaborative work in the filmmaking process.","['Feilin Han', 'Zhaoxu Meng']",2023-11-30T17:07:21Z,http://arxiv.org/abs/2312.00104v1,['cs.MM'],"metadata generation,semantic understanding,video retrieval,film production,deep visual-semantic methods,automated image information extraction,raw video searching,semantic annotation models,information fusion,cinematography-related semantic annotation"
Study and Survey on Gesture Recognition Systems,"In recent years, there has been a considerable amount of research in the
Gesture Recognition domain, mainly owing to the technological advancements in
Computer Vision. Various new applications have been conceptualised and
developed in this field. This paper discusses the implementation of gesture
recognition systems in multiple sectors such as gaming, healthcare, home
appliances, industrial robots, and virtual reality. Different methodologies for
capturing gestures are compared and contrasted throughout this survey. Various
data sources and data acquisition techniques have been discussed. The role of
gestures in sign language has been studied and existing approaches have been
reviewed. Common challenges faced while building gesture recognition systems
have also been explored.","['Kshitij Deshpande', 'Varad Mashalkar', 'Kaustubh Mhaisekar', 'Amaan Naikwadi', 'Archana Ghotkar']",2023-12-01T07:29:30Z,http://arxiv.org/abs/2312.00392v1,['cs.CV'],"Gesture recognition,Systems,Computer Vision,Applications,Data acquisition techniques,Sign language,Methodologies,Challenges"
"Global Localization: Utilizing Relative Spatio-Temporal Geometric
  Constraints from Adjacent and Distant Cameras","Re-localizing a camera from a single image in a previously mapped area is
vital for many computer vision applications in robotics and augmented/virtual
reality. In this work, we address the problem of estimating the 6 DoF camera
pose relative to a global frame from a single image. We propose to leverage a
novel network of relative spatial and temporal geometric constraints to guide
the training of a Deep Network for localization. We employ simultaneously
spatial and temporal relative pose constraints that are obtained not only from
adjacent camera frames but also from camera frames that are distant in the
spatio-temporal space of the scene. We show that our method, through these
constraints, is capable of learning to localize when little or very sparse
ground-truth 3D coordinates are available. In our experiments, this is less
than 1% of available ground-truth data. We evaluate our method on 3 common
visual localization datasets and show that it outperforms other direct pose
estimation methods.","['Mohammad Altillawi', 'Zador Pataki', 'Shile Li', 'Ziyuan Liu']",2023-12-01T11:03:07Z,http://arxiv.org/abs/2312.00500v1,"['cs.CV', 'cs.RO']","global localization,spatio-temporal,geometric constraints,camera pose,deep network,relative pose,ground-truth data,visual localization,pose estimation"
How to Tune Autofocals: A Comparative Study of Advanced Tuning Methods,"This study comprehensively evaluates tuning methods for autofocal glasses
using virtual reality (VR), addressing the challenge of presbyopia. With aging,
presbyopia diminishes the eye's ability to focus on nearby objects, impacting
the quality of life for billions. Autofocals, employing focus-tunable lenses,
dynamically adjust optical power for each fixation, promising a more natural
visual experience than traditional bifocal or multifocal lenses. Our research
contrasts the most common tuning methods - manual, gaze-based, and vergence -
within a VR setup to mimic real-world scenarios. Utilizing the XTAL VR headset
equipped with eye-tracking, the study replicated autofocal scenarios, measuring
performance and usability through psychophysical tasks and NASA TLX surveys.
Results show varying strengths and weaknesses across methods, with gaze control
excelling in precision but not necessarily comfort and manual control providing
stability and predictability. The findings guide the selection of tuning
methods based on task requirements and user preferences, highlighting a balance
between precision and ease of use.","['Benedikt W. Hosp', 'Yannick Sauer', 'Björn Severitt', 'Rajat Agarwala', 'Siegfried Wahl']",2023-12-01T16:08:08Z,http://arxiv.org/abs/2312.00685v1,['cs.HC'],"autofocals,tuning methods,virtual reality,presbyopia,focus-tunable lenses,eye-tracking,psychophysical tasks,NASA TLX surveys,user preferences,precision"
"Analyze Drivers' Intervention Behavior During Autonomous Driving -- A
  VR-incorporated Approach","Given the rapid advance in ITS technologies, future mobility is pointing to
vehicular autonomy. However, there is still a long way before full automation,
and human intervention is required. This work sheds light on understanding
human drivers' intervention behavior involved in the operation of autonomous
vehicles (AVs) and utilizes this knowledge to improve the perception of
critical driving scenarios. Experiment environments were implemented where the
virtual reality (VR) and traffic micro-simulation are integrated, and tests
were carried out under typical and diverse traffic scenes. Performance
indicators such as the probability of intervention, accident rates are defined
and used to quantify and compare the risk levels. By offering novel insights
into drivers' intervention behavior, this work will help improve the
performances of the automated control under similar scenarios. Furthermore,
such an integrated and immersive tool for autonomous driving studies will be
valuable for research on human-to-automation trust. To the best knowledge of
the authors, this work is among the pioneer works making efforts into such
types of tools.",['Zheng Xu'],2023-12-04T06:36:57Z,http://arxiv.org/abs/2312.01669v1,"['cs.HC', 'cs.AI']","Autonomous driving,Intervention behavior,Virtual reality (VR),Traffic micro-simulation,Performance indicators,Accident rates,Risk levels,Automated control,Human-to-automation trust"
Deep-learning-driven end-to-end metalens imaging,"Recent advances in metasurface lenses (metalenses) have shown great potential
for opening a new era in compact imaging, photography, light detection and
ranging (LiDAR), and virtual reality/augmented reality (VR/AR) applications.
However, the fundamental trade-off between broadband focusing efficiency and
operating bandwidth limits the performance of broadband metalenses, resulting
in chromatic aberration, angular aberration, and a relatively low efficiency.
In this study, a deep-learning-based image restoration framework is proposed to
overcome these limitations and realize end-to-end metalens imaging, thereby
achieving aberration-free full-color imaging for mass-produced metalenses with
10-mm diameter. Neural-network-assisted metalens imaging achieved a high
resolution comparable to that of the ground truth image.","['Joonhyuk Seo', 'Jaegang Jo', 'Joohoon Kim', 'Joonho Kang', 'Chanik Kang', 'Seongwon Moon', 'Eunji Lee', 'Jehyeong Hong', 'Junsuk Rho', 'Haejun Chung']",2023-12-05T11:22:09Z,http://arxiv.org/abs/2312.02669v3,"['physics.optics', 'eess.IV']","deep learning,end-to-end,metalens,imaging,metasurface lenses,chromatic aberration,angular aberration,efficiency,neural network,resolution"
"Perspectives from Naive Participants and Experienced Social Science
  Researchers on Addressing Embodiment in a Virtual Cyberball Task","We describe the design of an immersive virtual Cyberball task that included
avatar customization, and user feedback on this design. We first created a
prototype of an avatar customization template and added it to a Cyberball
prototype built in the Unity3D game engine. Then, we conducted in-depth user
testing and feedback sessions with 15 Cyberball stakeholders: five naive
participants with no prior knowledge of Cyberball and ten experienced
researchers with extensive experience using the Cyberball paradigm. We report
the divergent perspectives of the two groups on the following design insights;
designing for intuitive use, inclusivity, and realistic experiences versus
minimalism. Participant responses shed light on how system design problems may
contribute to or perpetuate negative experiences when customizing avatars. They
also demonstrate the value of considering multiple stakeholders' feedback in
the design process for virtual reality, presenting a more comprehensive view in
designing future Cyberball prototypes and interactive systems for social
science research.","['Tao Long', 'Swati Pandita', 'Andrea Stevenson Won']",2023-12-05T17:09:59Z,http://arxiv.org/abs/2312.02897v1,"['cs.HC', 'cs.CY']","avatar customization,Cyberball task,Unity3D,user testing,feedback sessions,naive participants,experienced researchers,immersive virtual reality,design insights,interactive systems"
Quantum-Inspired Neural Network Model of Optical Illusions,"Ambiguous optical illusions have been a paradigmatic object of fascination,
research and inspiration in arts, psychology and video games. However, accurate
computational models of perception of ambiguous figures have been elusive. In
this paper, we design and train a deep neural network model to simulate the
human's perception of the Necker cube, an ambiguous drawing with several
alternating possible interpretations. Defining the weights of the neural
network connection using a quantum generator of truly random numbers, in
agreement with the emerging concepts of quantum artificial intelligence and
quantum cognition we reveal that the actual perceptual state of the Necker cube
is a qubit-like superposition of the two fundamental perceptual states
predicted by classical theories. Our results will find applications in video
games and virtual reality systems employed for training of astronauts and
operators of unmanned aerial vehicles. They will also be useful for researchers
working in the fields of machine learning and vision, psychology of perception
and quantum-mechanical models of human mind and decision-making.",['Ivan S. Maksymov'],2023-12-06T12:10:56Z,http://arxiv.org/abs/2312.03447v1,"['physics.soc-ph', 'cs.AI', 'cs.CV', 'quant-ph']","Quantum-Inspired Neural Network,Optical Illusions,Deep Neural Network,Necker Cube,Quantum Artificial Intelligence,Quantum Cognition,Superposition,Machine Learning,Vision,Quantum-Mechanical Models"
"Towards a Unified Naming Scheme for Thermo-Active Soft Actuators: A
  Review of Materials, Working Principles, and Applications","Soft robotics is a rapidly growing field that spans the fields of chemistry,
materials science, and engineering. Due to the diverse background of the field,
there have been contrasting naming schemes such as 'intelligent', 'smart' and
'adaptive' materials which add vagueness to the broad innovation among
literature. Therefore, a clear, functional and descriptive naming scheme is
proposed in which a previously vague name -- Soft Material for Soft Actuators
-- can remain clear and concise -- Phase-Change Elastomers for Artificial
Muscles. By synthesizing the working principle, material, and application into
a naming scheme, the searchability of soft robotics can be enhanced and applied
to other fields. The field of thermo-active soft actuators spans multiple
domains and requires added clarity. Thermo-active actuators have potential for
a variety of applications spanning virtual reality haptics to assistive
devices. This review offers a comprehensive guide to selecting the type of
thermo-active actuator when one has an application in mind. Additionally, it
discusses future directions and improvements that are necessary for
implementation.","['Trevor Exley', 'Emilly Hays', 'Daniel Johnson', 'Arian Moridani', 'Ramya Motati', 'Amir Jafari']",2023-12-11T15:25:52Z,http://arxiv.org/abs/2312.06445v1,['cs.RO'],"Thermo-Active Soft Actuators,Materials,Working Principles,Applications,Soft Robotics,Naming Scheme,Phase-Change Elastomers,Artificial Muscles,Virtual Reality Haptics"
"A Comparison of Interfaces for Learning How to Play a Mixed Reality
  Handpan","In the realm of music therapy, Virtual Reality (VR) has a long-standing
history of enriching human experiences through immersive applications, spanning
entertainment games, serious games, and professional training in various
fields. However, the untapped potential lies in using VR games to support
mindfulness through music. We present a new approach utilizing a virtual
environment to facilitate learning how to play the handpan -- an instrument in
the shape of a spherical dish with harmonically tuned notes used commonly in
the sound healing practice of mindfulness. In a preliminary study, we compared
six interfaces, where the highlighted path interface performed best. However,
participants expressed preference for the standard interface inspired by rhythm
games like Guitar Hero.","['Gavin Gosling', 'Ivan-teofil Catovic', 'Ghazal Bangash', 'Daniel MacCormick', 'Loutfouz Zaman']",2023-12-12T02:11:13Z,http://arxiv.org/abs/2312.06936v1,['cs.HC'],"virtual reality,mixed reality,handpan,music therapy,immersive applications,mindfulness,virtual environment,sound healing,interface,rhythm games"
Reconstruction of Sound Field through Diffusion Models,"Reconstructing the sound field in a room is an important task for several
applications, such as sound control and augmented (AR) or virtual reality (VR).
In this paper, we propose a data-driven generative model for reconstructing the
magnitude of acoustic fields in rooms with a focus on the modal frequency
range. We introduce, for the first time, the use of a conditional Denoising
Diffusion Probabilistic Model (DDPM) trained in order to reconstruct the sound
field (SF-Diff) over an extended domain. The architecture is devised in order
to be conditioned on a set of limited available measurements at different
frequencies and generate the sound field in target, unknown, locations. The
results show that SF-Diff is able to provide accurate reconstructions,
outperforming a state-of-the-art baseline based on kernel interpolation.","['Federico Miotello', 'Luca Comanducci', 'Mirco Pezzoli', 'Alberto Bernardini', 'Fabio Antonacci', 'Augusto Sarti']",2023-12-14T11:11:26Z,http://arxiv.org/abs/2312.08821v2,"['eess.AS', 'cs.LG', 'cs.SD', 'eess.SP']","Diffusion Models,Sound Field,Data-driven,Generative Model,Acoustic Fields,Modal Frequency Range,Conditional Denoising,Probabilistic Model,Reconstruction,Kernel Interpolation"
"Evaluating Augmented Reality Communication: How Can We Teach Procedural
  Skill in AR?","Augmented reality (AR) has great potential for use in healthcare
applications, especially remote medical training and supervision. In this
paper, we analyze the usage of an AR communication system to teach a medical
procedure, the placement of a central venous catheter (CVC) under ultrasound
guidance. We examine various AR communication and collaboration components,
including gestural communication, volumetric information, annotations,
augmented objects, and augmented screens. We compare how teaching in AR differs
from teaching through videoconferencing-based communication. Our results
include a detailed medical training steps analysis in which we compare how
verbal and visual communication differs between video and AR training. We
identify procedural steps in which medical experts give visual instructions
utilizing AR components. We examine the change in AR usage and interaction over
time and recognize patterns between users. Moreover, AR design recommendations
are given based on post-training interviews.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Neal Sikka', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Puja Sasankan', 'Christian Gütl']",2023-12-14T17:22:22Z,http://arxiv.org/abs/2312.09152v1,['cs.HC'],"Augmented reality,Communication,Procedural skill,Medical procedure,Central venous catheter,Ultrasound guidance,Gestural communication,Volumetric information,Annotations,Augmented objects"
Text2Immersion: Generative Immersive Scene with 3D Gaussians,"We introduce Text2Immersion, an elegant method for producing high-quality 3D
immersive scenes from text prompts. Our proposed pipeline initiates by
progressively generating a Gaussian cloud using pre-trained 2D diffusion and
depth estimation models. This is followed by a refining stage on the Gaussian
cloud, interpolating and refining it to enhance the details of the generated
scene. Distinct from prevalent methods that focus on single object or indoor
scenes, or employ zoom-out trajectories, our approach generates diverse scenes
with various objects, even extending to the creation of imaginary scenes.
Consequently, Text2Immersion can have wide-ranging implications for various
applications such as virtual reality, game development, and automated content
creation. Extensive evaluations demonstrate that our system surpasses other
methods in rendering quality and diversity, further progressing towards
text-driven 3D scene generation. We will make the source code publicly
accessible at the project page.","['Hao Ouyang', 'Kathryn Heal', 'Stephen Lombardi', 'Tiancheng Sun']",2023-12-14T18:58:47Z,http://arxiv.org/abs/2312.09242v1,"['cs.CV', 'cs.GR']","generative,immersive scene,3D,Gaussians,text2immersion,diffusion,depth estimation,virtual reality,game development,automated content creation"
"Eyes on teleporting: comparing locomotion techniques in Virtual Reality
  with respect to presence, sickness and spatial orientation","This work compares three locomotion techniques for an immersive VR
environment: two different types of teleporting (with and without animation)
and a manual (joystick-based) technique. We tested the effect of these
techniques on visual motion sickness, spatial awareness, presence, subjective
pleasantness, and perceived difficulty of operating the navigation. We
collected eye tracking and head and body orientation data to investigate the
relationships between motion, vection, and sickness. Our study confirms some
results already discussed in the literature regarding the reduced invasiveness
and the high usability of instant teleport while increasing the evidence
against the hypothesis of reduced spatial awareness induced by this technique.
We reinforce the evidence about the issues of extending teleporting with
animation. Furthermore, we offer some new evidence of a benefit to the user
experience of the manual technique and the correlation of the sickness felt in
this condition with head movements. The findings of this study contribute to
the ongoing debate on the development of guidelines on navigation interfaces in
specific VR environments.","['Ariel Caputo', 'Massimo Zancanaro', 'Andrea Giachetti']",2023-12-15T12:21:37Z,http://arxiv.org/abs/2312.09737v1,['cs.HC'],"teleporting,locomotion techniques,Virtual Reality,motion sickness,spatial awareness,presence,vection,eye tracking,navigation interfaces"
"Emotion Based Prediction in the Context of Optimized Trajectory Planning
  for Immersive Learning","In the virtual elements of immersive learning, the use of Google Expedition
and touch-screen-based emotion are examined. The objective is to investigate
possible ways to combine these technologies to enhance virtual learning
environments and learners emotional engagement. Pedagogical application,
affordances, and cognitive load are the corresponding measures that are
involved. Students will gain insight into the reason behind their significantly
higher post-assessment Prediction Systems scores compared to preassessment
scores through this work that leverages technology. This suggests that it is
effective to include emotional elements in immersive learning scenarios. The
results of this study may help develop new strategies by leveraging the
features of immersive learning technology in educational technologies to
improve virtual reality and augmented reality experiences. Furthermore, the
effectiveness of immersive learning environments can be raised by utilizing
magnetic, optical, or hybrid trackers that considerably improve object
tracking.","['Akey Sungheetha', 'Rajesh Sharma R', 'Chinnaiyan R']",2023-12-18T09:24:35Z,http://arxiv.org/abs/2312.11576v2,"['cs.HC', 'cs.CV', 'cs.MM']","optimized trajectory planning,immersive learning,emotion-based prediction,virtual reality,augmented reality,Google Expedition,touch-screen-based emotion,pedagogical application,cognitive load,immersive learning technology"
"Recursive Camera Painting: A Method for Real-Time Painterly Renderings
  of 3D Scenes","In this work, we present the recursive camera-painting approach to obtain
painterly smudging in real-time rendering applications. We have implemented
recursive camera painting as both a GPU-based ray-tracing and in a Virtual
Reality game environment. Using this approach, we can obtain dynamic 3D
Paintings in real-time. In a camera painting, each pixel has a separate
associated camera whose parameters are computed from a corresponding image of
the same size. In recursive camera painting, we use the rendered images to
compute new camera parameters. When we apply this process a few times, it
creates painterly images that can be viewed as real-time 3D dynamic paintings.
These visual results are not surprising since multi-view techniques help to
obtain painterly effects.","['Ergun Akleman', 'Cassie Mullins', 'Christopher Morrison', 'David Oh']",2023-12-01T21:15:20Z,http://arxiv.org/abs/2312.12392v1,['cs.GR'],"recursive camera painting,real-time,painterly renderings,3D scenes,GPU-based ray-tracing,Virtual Reality,dynamic 3D Paintings,multi-view techniques"
"Virtual Reality-Assisted Physiotherapy for Visuospatial Neglect
  Rehabilitation: A Proof-of-Concept Study","This study explores a VR-based intervention for Visuospatial neglect (VSN), a
post-stroke condition. It aims to develop a VR task utilizing interactive
visual-audio cues to improve sensory-motor training and assess its impact on
VSN patients' engagement and performance. Collaboratively designed with
physiotherapists, the VR task uses directional and auditory stimuli to alert
and direct patients, tested over 12 sessions with two individuals. Results show
a consistent decrease in task completion variability and positive patient
feedback, highlighting the VR task's potential for enhancing engagement and
suggesting its feasibility in rehabilitation. The study underlines the
significance of collaborative design in healthcare technology and advocates for
further research with a larger sample size to confirm the benefits of VR in VSN
treatment, as well as its applicability to other multimodal disorders.","['Andrew Danso', 'Patti Nijhuis', 'Alessandro Ansani', 'Martin Hartmann', 'Gulnara Minkkinen', 'Geoff Luck', 'Joshua S. Bamford', 'Sarah Faber', 'Kat Agres', 'Solange Glasser', 'Teppo Särkämö', 'Rebekah Rousi', 'Marc R. Thompson']",2023-12-19T18:35:01Z,http://arxiv.org/abs/2312.12399v1,['cs.HC'],"Virtual Reality,Physiotherapy,Visuospatial Neglect,Rehabilitation,VR-based intervention,Sensory-motor training,Visual-audio cues,Directional stimuli,Auditory stimuli"
Human101: Training 100+FPS Human Gaussians in 100s from 1 View,"Reconstructing the human body from single-view videos plays a pivotal role in
the virtual reality domain. One prevalent application scenario necessitates the
rapid reconstruction of high-fidelity 3D digital humans while simultaneously
ensuring real-time rendering and interaction. Existing methods often struggle
to fulfill both requirements. In this paper, we introduce Human101, a novel
framework adept at producing high-fidelity dynamic 3D human reconstructions
from 1-view videos by training 3D Gaussians in 100 seconds and rendering in
100+ FPS. Our method leverages the strengths of 3D Gaussian Splatting, which
provides an explicit and efficient representation of 3D humans. Standing apart
from prior NeRF-based pipelines, Human101 ingeniously applies a Human-centric
Forward Gaussian Animation method to deform the parameters of 3D Gaussians,
thereby enhancing rendering speed (i.e., rendering 1024-resolution images at an
impressive 60+ FPS and rendering 512-resolution images at 100+ FPS).
Experimental results indicate that our approach substantially eclipses current
methods, clocking up to a 10 times surge in frames per second and delivering
comparable or superior rendering quality. Code and demos will be released at
https://github.com/longxiang-ai/Human101.","['Mingwei Li', 'Jiachen Tao', 'Zongxin Yang', 'Yi Yang']",2023-12-23T13:41:56Z,http://arxiv.org/abs/2312.15258v1,['cs.CV'],"human body reconstruction,3D digital humans,single-view videos,real-time rendering,interaction,3D Gaussians,Gaussian Splatting,NeRF-based pipelines,rendering speed,frames per second"
"Tail-Learning: Adaptive Learning Method for Mitigating Tail Latency in
  Autonomous Edge Systems","In the realm of edge computing, the increasing demand for high Quality of
Service (QoS), particularly in dynamic multimedia streaming applications (e.g.,
Augmented Reality/Virtual Reality and online gaming), has prompted the need for
effective solutions. Nevertheless, adopting an edge paradigm grounded in
distributed computing has exacerbated the issue of tail latency. Given a
limited variety of multimedia services supported by edge servers and the
dynamic nature of user requests, employing traditional queuing methods to model
tail latency in distributed edge computing is challenging, substantially
exacerbating head-of-line (HoL) blocking. In response to this challenge, we
have developed a learning-based scheduling method to mitigate the overall tail
latency, which adaptively selects appropriate edge servers for execution as
incoming distributed tasks vary with unknown size. To optimize the utilization
of the edge computing paradigm, we leverage Laplace transform techniques to
theoretically derive an upper bound for the response time of edge servers.
Subsequently, we integrate this upper bound into reinforcement learning to
facilitate tail learning and enable informed decisions for autonomous
distributed scheduling. The experiment results demonstrate the efficiency in
reducing tail latency compared to existing methods.","['Cheng Zhang', 'Yinuo Deng', 'Hailiang Zhao', 'Tianlv Chen', 'Shuiguang Deng']",2023-12-28T08:16:54Z,http://arxiv.org/abs/2312.16883v1,['cs.DC'],"Adaptive Learning,Tail Latency,Edge Systems,Quality of Service (QoS),Multimedia Streaming,Distributed Computing,Queuing Methods,Laplace Transform,Reinforcement Learning"
Synthetic Data Applications in Finance,"Synthetic data has made tremendous strides in various commercial settings
including finance, healthcare, and virtual reality. We present a broad overview
of prototypical applications of synthetic data in the financial sector and in
particular provide richer details for a few select ones. These cover a wide
variety of data modalities including tabular, time-series, event-series, and
unstructured arising from both markets and retail financial applications. Since
finance is a highly regulated industry, synthetic data is a potential approach
for dealing with issues related to privacy, fairness, and explainability.
Various metrics are utilized in evaluating the quality and effectiveness of our
approaches in these applications. We conclude with open directions in synthetic
data in the context of the financial domain.","['Vamsi K. Potluru', 'Daniel Borrajo', 'Andrea Coletta', 'Niccolò Dalmasso', 'Yousef El-Laham', 'Elizabeth Fons', 'Mohsen Ghassemi', 'Sriram Gopalakrishnan', 'Vikesh Gosai', 'Eleonora KreačiāE, 'Ganapathy Mani', 'Saheed Obitayo', 'Deepak Paramanand', 'Natraj Raman', 'Mikhail Solonin', 'Srijan Sood', 'Svitlana Vyetrenko', 'Haibei Zhu', 'Manuela Veloso', 'Tucker Balch']",2023-12-29T21:49:23Z,http://arxiv.org/abs/2401.00081v2,"['cs.LG', 'q-fin.GN']","synthetic data,finance,data modalities,tabular data,time-series data,event-series data,unstructured data,privacy,fairness,explainability"
"Immersive Serious Games for Learning Physics Concepts: The Case of
  Density","Training students in basic concepts of physics, such as the ones related to
mass, volume, or density, is much more complicated than just stating the
underlying definitions and laws. One of the reasons for this is that most
students have deeply rooted delusions and misconceptions about the behavior of
objects, sometimes close to magical thinking. Many innovative and promising
technologies, in particular Virtual Reality (VR), can be used to enhance
student learning. We compared the effectiveness of a serious immersive game in
teaching the concept of density in various conditions: a 2D version in an
embedded web browser and a 3D immersive game in VR. We also developed a
specific questionnaire to assess students' knowledge improvement. Primary
results have shown an increase in learning efficiency using VR. Also, most
students were able to see the shortcomings of their initial theories and revise
them, which means that they improved their understanding of this topic.","['Iuliia Zhurakovskaia', 'Jeanne Vézien', 'Cécile de Hosson', 'Patrick Bourdot']",2024-01-03T16:48:39Z,http://arxiv.org/abs/2401.01831v1,['cs.GR'],"serious games,physics concepts,density,Virtual Reality (VR),immersive game,learning efficiency,delusions,misconceptions,mass,volume"
"Survey of 3D Human Body Pose and Shape Estimation Methods for
  Contemporary Dance Applications","3D human body shape and pose estimation from RGB images is a challenging
problem with potential applications in augmented/virtual reality, healthcare
and fitness technology and virtual retail. Recent solutions have focused on
three types of inputs: i) single images, ii) multi-view images and iii) videos.
In this study, we surveyed and compared 3D body shape and pose estimation
methods for contemporary dance and performing arts, with a special focus on
human body pose and dressing, camera viewpoint, illumination conditions and
background conditions. We demonstrated that multi-frame methods, such as PHALP,
provide better results than single-frame method for pose estimation when
dancers are performing contemporary dances.","['Darshan Venkatrayappa', 'Alain Tremeau', 'Damien Muselet', 'Philippe Colantoni']",2024-01-04T17:51:44Z,http://arxiv.org/abs/2401.02383v2,"['cs.CV', 'cs.AI']","3D human body pose estimation,shape estimation,RGB images,augmented reality,virtual reality,healthcare,fitness technology,virtual retail,multi-view images,videos"
"FurniScene: A Large-scale 3D Room Dataset with Intricate Furnishing
  Scenes","Indoor scene generation has attracted significant attention recently as it is
crucial for applications of gaming, virtual reality, and interior design.
Current indoor scene generation methods can produce reasonable room layouts but
often lack diversity and realism. This is primarily due to the limited coverage
of existing datasets, including only large furniture without tiny furnishings
in daily life. To address these challenges, we propose FurniScene, a
large-scale 3D room dataset with intricate furnishing scenes from interior
design professionals. Specifically, the FurniScene consists of 11,698 rooms and
39,691 unique furniture CAD models with 89 different types, covering things
from large beds to small teacups on the coffee table. To better suit
fine-grained indoor scene layout generation, we introduce a novel Two-Stage
Diffusion Scene Model (TSDSM) and conduct an evaluation benchmark for various
indoor scene generation based on FurniScene. Quantitative and qualitative
evaluations demonstrate the capability of our method to generate highly
realistic indoor scenes. Our dataset and code will be publicly available soon.","['Genghao Zhang', 'Yuxi Wang', 'Chuanchen Luo', 'Shibiao Xu', 'Zhaoxiang Zhang', 'Man Zhang', 'Junran Peng']",2024-01-07T12:34:45Z,http://arxiv.org/abs/2401.03470v2,"['cs.CV', 'cs.AI']","3D room dataset,furnishing scenes,indoor scene generation,furniture CAD models,fine-grained indoor scene layout,Two-Stage Diffusion Scene Model,evaluation benchmark,realistic indoor scenes,dataset,code"
Neural Ambisonics encoding for compact irregular microphone arrays,"Ambisonics encoding of microphone array signals can enable various spatial
audio applications, such as virtual reality or telepresence, but it is
typically designed for uniformly-spaced spherical microphone arrays. This paper
proposes a method for Ambisonics encoding that uses a deep neural network (DNN)
to estimate a signal transform from microphone inputs to Ambisonics signals.
The approach uses a DNN consisting of a U-Net structure with a learnable
preprocessing as well as a loss function consisting of mean average error,
spatial correlation, and energy preservation components. The method is
validated on two microphone arrays with regular and irregular shapes having
four microphones, on simulated reverberant scenes with multiple sources. The
results of the validation show that the proposed method can meet or exceed the
performance of a conventional signal-independent Ambisonics encoder on a number
of error metrics.","['Mikko Heikkinen', 'Archontis Politis', 'Tuomas Virtanen']",2024-01-11T13:49:54Z,http://arxiv.org/abs/2401.05916v1,"['eess.AS', 'cs.SD']","Neural network,Ambisonics encoding,Microphone arrays,Deep learning,Spatial audio,Virtual reality,Telepresence,U-Net,Signal transform,Reverberant scenes"
AAMDM: Accelerated Auto-regressive Motion Diffusion Model,"Interactive motion synthesis is essential in creating immersive experiences
in entertainment applications, such as video games and virtual reality.
However, generating animations that are both high-quality and contextually
responsive remains a challenge. Traditional techniques in the game industry can
produce high-fidelity animations but suffer from high computational costs and
poor scalability. Trained neural network models alleviate the memory and speed
issues, yet fall short on generating diverse motions. Diffusion models offer
diverse motion synthesis with low memory usage, but require expensive reverse
diffusion processes. This paper introduces the Accelerated Auto-regressive
Motion Diffusion Model (AAMDM), a novel motion synthesis framework designed to
achieve quality, diversity, and efficiency all together. AAMDM integrates
Denoising Diffusion GANs as a fast Generation Module, and an Auto-regressive
Diffusion Model as a Polishing Module. Furthermore, AAMDM operates in a
lower-dimensional embedded space rather than the full-dimensional pose space,
which reduces the training complexity as well as further improves the
performance. We show that AAMDM outperforms existing methods in motion quality,
diversity, and runtime efficiency, through comprehensive quantitative analyses
and visual comparisons. We also demonstrate the effectiveness of each
algorithmic component through ablation studies.","['Tianyu Li', 'Calvin Qiao', 'Guanqiao Ren', 'KangKang Yin', 'Sehoon Ha']",2023-12-02T23:52:21Z,http://arxiv.org/abs/2401.06146v1,"['cs.CV', 'cs.GR']","motion synthesis,interactive,neural network models,diffusion models,Auto-regressive Motion Diffusion Model,Denoising Diffusion GANs,embedded space,pose space,quality,diversity"
Apple Vision Pro: Comments in Healthcare,"This paper objectively analyzes the emerging discourse surrounding Apple
Vision Pro's application in healthcare and medical education. Released in June
2023, Apple Vision Pro represents a significant advancement in spatial
computing, combining augmented and virtual reality to create new possibilities
in digital interaction. We aim to compile and present recent articles. We used
PubMed, IEEE Xplore, Google Scholar, and JSTOR. Non-academic publications were
excluded. The results were six commentaries, one a pre-print. All were majorly
optimistic, with one mentioning VR/AR sickness. For future research directions,
we stress the need for continued exploration of Apple Vision Pro's capabilities
and limitations and expect expert opinions to englobe this discussion.","['Ezequiel Santos', 'Vanessa Castillo']",2024-01-13T01:39:31Z,http://arxiv.org/abs/2401.08685v4,['cs.HC'],"Apple Vision Pro,healthcare,medical education,spatial computing,augmented reality,virtual reality,digital interaction,PubMed,IEEE Xplore,expert opinions"
"OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed
  Reality","One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.","['Aditya Sharma', 'Luke Yoffe', 'Tobias Höllerer']",2024-01-17T04:52:40Z,http://arxiv.org/abs/2401.08973v1,"['cs.CV', 'cs.AI', 'cs.CL']","Augmented Reality,Object Placement,Open-Vocabulary,Vision-Language Models,State-of-the-art Method,Benchmark,Virtual Objects,User Studies,Metrics"
GPAvatar: Generalizable and Precise Head Avatar from Image(s),"Head avatar reconstruction, crucial for applications in virtual reality,
online meetings, gaming, and film industries, has garnered substantial
attention within the computer vision community. The fundamental objective of
this field is to faithfully recreate the head avatar and precisely control
expressions and postures. Existing methods, categorized into 2D-based warping,
mesh-based, and neural rendering approaches, present challenges in maintaining
multi-view consistency, incorporating non-facial information, and generalizing
to new identities. In this paper, we propose a framework named GPAvatar that
reconstructs 3D head avatars from one or several images in a single forward
pass. The key idea of this work is to introduce a dynamic point-based
expression field driven by a point cloud to precisely and effectively capture
expressions. Furthermore, we use a Multi Tri-planes Attention (MTA) fusion
module in the tri-planes canonical field to leverage information from multiple
input images. The proposed method achieves faithful identity reconstruction,
precise expression control, and multi-view consistency, demonstrating promising
results for free-viewpoint rendering and novel view synthesis.","['Xuangeng Chu', 'Yu Li', 'Ailing Zeng', 'Tianyu Yang', 'Lijian Lin', 'Yunfei Liu', 'Tatsuya Harada']",2024-01-18T18:56:34Z,http://arxiv.org/abs/2401.10215v1,['cs.CV'],"head avatar reconstruction,virtual reality,computer vision,3D head avatars,point cloud,Multi Tri-planes Attention (MTA) fusion module,free-viewpoint rendering,novel view synthesis"
"Design Frameworks for Spatial Zone Agents in XRI Metaverse Smart
  Environments","The spatial XR-IoT (XRI) Zone Agents concept combines Extended Reality (XR),
the Internet of Things (IoT), and spatial computing concepts to create
hyper-connected spaces for metaverse applications; envisioning space as zones
that are social, smart, scalable, expressive, and agent-based. These zone
agents serve as applications and agents (partners, assistants, or guides) for
users co-living and co-operating together in a shared spatial context. The zone
agent concept is toward reducing the gap between the physical environment
(space) and the classical two-dimensional user interface, through space-based
interactions for future metaverse applications. This integration aims to enrich
user engagement with their environments through intuitive and immersive
experiences and pave the way for innovative human-machine interaction in smart
spaces. Contributions include: i) a theoretical framework for creating XRI
zone/space-agents using Mixed-Reality Agents (MiRAs) and XRI theory, ii) agent
and scene design for spatial zone agents, and iii) prototype and user
interaction design scenario concepts for human-to-space agent relationships in
an early immersive smart-space application.","['Jie Guan', 'Jiamin Liu', 'Alexis Morris']",2024-01-19T22:03:36Z,http://arxiv.org/abs/2401.11040v1,['cs.HC'],"Spatial Zone Agents,XRI,IoT,spatial computing,metaverse,agent-based,XR,Mixed-Reality Agents,user interface,human-machine interaction"
Motion-enhanced Holography,"Holographic displays, which enable pixel-level depth control and aberration
correction, are considered the key technology for the next-generation virtual
reality (VR) and augmented reality (AR) applications. However, traditional
holographic systems suffer from limited spatial bandwidth product (SBP), which
makes them impossible to reproduce \textit{realistic} 3D displays.
Time-multiplexed holography creates different speckle patterns over time and
then averages them to achieve a speckle-free 3D display. However, this approach
requires spatial light modulators (SLMs) with ultra-fast refresh rates, and
current algorithms cannot update holograms at such speeds. To overcome the
aforementioned challenge, we proposed a novel architecture, motion-enhanced
holography, that achieves \textit{realistic} 3D holographic displays without
artifacts by continuously shifting a special hologram. We introduced an
iterative algorithm to synthesize motion-enhanced holograms and demonstrated
that our method achieved a 10 dB improvement in the peak signal-to-noise ratio
(PSNR) of 3D focal stacks in numerical simulations compared to traditional
holographic systems. Furthermore, we validated this idea in optical experiments
utilizing a high-speed and high-precision programmable three-axis displacement
stage to display full-color and high-quality 3D focal stacks.","['Zhenxing Dong', 'Yuye Ling', 'Yan Li', 'Yikai Su']",2024-01-23T07:43:11Z,http://arxiv.org/abs/2401.12537v1,['physics.optics'],"Holography,Spatial light modulators (SLMs),Realistic 3D displays,Time-multiplexed holography,Speckle patterns,Motion-enhanced holography,Algorithms,Peak signal-to-noise ratio (PSNR),Focal stacks,Optical experiments"
"A Survey on Indoor Visible Light Positioning Systems: Fundamentals,
  Applications, and Challenges","The growing demand for location-based services in areas like virtual reality,
robot control, and navigation has intensified the focus on indoor localization.
Visible light positioning (VLP), leveraging visible light communications (VLC),
becomes a promising indoor positioning technology due to its high accuracy and
low cost. This paper provides a comprehensive survey of VLP systems. In
particular, since VLC lays the foundation for VLP, we first present a detailed
overview of the principles of VLC. The performance of each positioning
algorithm is also compared in terms of various metrics such as accuracy,
coverage, and orientation limitation. Beyond the physical layer studies, the
network design for a VLP system is also investigated, including multi-access
technologies resource allocation, and light-emitting diode (LED) placements.
Next, the applications of the VLP systems are overviewed. Finally, this paper
outlines open issues, challenges, and future research directions for the
research field. In a nutshell, this paper constitutes the first holistic survey
on VLP from state-of-the-art studies to practical uses.","['Zhiyu Zhu', 'Yang Yang', 'Mingzhe Chen', 'Caili Guo', 'Julian Cheng', 'Shuguang Cui']",2024-01-25T02:20:51Z,http://arxiv.org/abs/2401.13893v1,['eess.SP'],"Indoor Visible Light Positioning Systems,Fundamentals,Applications,Challenges,Visible Light Communications,VLC,Indoor Localization,Positioning Algorithm,Light-Emitting Diode,LED"
"""May I Speak?"": Multi-modal Attention Guidance in Social VR Group
  Conversations","In this paper, we present a novel multi-modal attention guidance method
designed to address the challenges of turn-taking dynamics in meetings and
enhance group conversations within virtual reality (VR) environments.
Recognizing the difficulties posed by a confined field of view and the absence
of detailed gesture tracking in VR, our proposed method aims to mitigate the
challenges of noticing new speakers attempting to join the conversation. This
approach tailors attention guidance, providing a nuanced experience for highly
engaged participants while offering subtler cues for those less engaged,
thereby enriching the overall meeting dynamics. Through group interview
studies, we gathered insights to guide our design, resulting in a prototype
that employs ""light"" as a diegetic guidance mechanism, complemented by spatial
audio. The combination creates an intuitive and immersive meeting environment,
effectively directing users' attention to new speakers. An evaluation study,
comparing our method to state-of-the-art attention guidance approaches,
demonstrated significantly faster response times (p < 0.001), heightened
perceived conversation satisfaction (p < 0.001), and preference (p < 0.001) for
our method. Our findings contribute to the understanding of design implications
for VR social attention guidance, opening avenues for future research and
development.","['Geonsun Lee', 'Dae Yeol Lee', 'Guan-Ming Su', 'Dinesh Manocha']",2024-01-27T21:29:29Z,http://arxiv.org/abs/2401.15507v1,['cs.HC'],"multi-modal,attention guidance,social VR,group conversations,turn-taking dynamics,virtual reality,gesture tracking,attention guidance approaches,meeting dynamics,spatial audio"
"Hand-Centric Motion Refinement for 3D Hand-Object Interaction via
  Hierarchical Spatial-Temporal Modeling","Hands are the main medium when people interact with the world. Generating
proper 3D motion for hand-object interaction is vital for applications such as
virtual reality and robotics. Although grasp tracking or object manipulation
synthesis can produce coarse hand motion, this kind of motion is inevitably
noisy and full of jitter. To address this problem, we propose a data-driven
method for coarse motion refinement. First, we design a hand-centric
representation to describe the dynamic spatial-temporal relation between hands
and objects. Compared to the object-centric representation, our hand-centric
representation is straightforward and does not require an ambiguous projection
process that converts object-based prediction into hand motion. Second, to
capture the dynamic clues of hand-object interaction, we propose a new
architecture that models the spatial and temporal structure in a hierarchical
manner. Extensive experiments demonstrate that our method outperforms previous
methods by a noticeable margin.","['Yuze Hao', 'Jianrong Zhang', 'Tao Zhuo', 'Fuan Wen', 'Hehe Fan']",2024-01-29T09:17:51Z,http://arxiv.org/abs/2401.15987v1,['cs.CV'],"3D motion,hand-object interaction,hierarchical spatial-temporal modeling,grasp tracking,object manipulation synthesis"
Diffusion-based Light Field Synthesis,"Light fields (LFs), conducive to comprehensive scene radiance recorded across
angular dimensions, find wide applications in 3D reconstruction, virtual
reality, and computational photography.However, the LF acquisition is
inevitably time-consuming and resource-intensive due to the mainstream
acquisition strategy involving manual capture or laborious software
synthesis.Given such a challenge, we introduce LFdiff, a straightforward yet
effective diffusion-based generative framework tailored for LF synthesis, which
adopts only a single RGB image as input.LFdiff leverages disparity estimated by
a monocular depth estimation network and incorporates two distinctive
components: a novel condition scheme and a noise estimation network tailored
for LF data.Specifically, we design a position-aware warping condition scheme,
enhancing inter-view geometry learning via a robust conditional signal.We then
propose DistgUnet, a disentanglement-based noise estimation network, to harness
comprehensive LF representations.Extensive experiments demonstrate that LFdiff
excels in synthesizing visually pleasing and disparity-controllable light
fields with enhanced generalization capability.Additionally, comprehensive
results affirm the broad applicability of the generated LF data, spanning
applications like LF super-resolution and refocusing.","['Ruisheng Gao', 'Yutong Liu', 'Zeyu Xiao', 'Zhiwei Xiong']",2024-02-01T13:13:16Z,http://arxiv.org/abs/2402.00575v1,['cs.CV'],"light field,diffusion,synthesis,generative framework,monocular depth estimation,disparity,condition scheme,noise estimation network,disentanglement,LF data"
Deep Room Impulse Response Completion,"Rendering immersive spatial audio in virtual reality (VR) and video games
demands a fast and accurate generation of room impulse responses (RIRs) to
recreate auditory environments plausibly. However, the conventional methods for
simulating or measuring long RIRs are either computationally intensive or
challenged by low signal-to-noise ratios. This study is propelled by the
insight that direct sound and early reflections encapsulate sufficient
information about room geometry and absorption characteristics. Building upon
this premise, we propose a novel task termed ""RIR completion,"" aimed at
synthesizing the late reverberation given only the early portion (50 ms) of the
response. To this end, we introduce DECOR, Deep Exponential Completion Of Room
impulse responses, a deep neural network structured as an autoencoder designed
to predict multi-exponential decay envelopes of filtered noise sequences. The
interpretability of DECOR's output facilitates its integration with diverse
rendering techniques. The proposed method is compared against an adapted
state-of-the-art network, and comparable performance shows promising results
supporting the feasibility of the RIR completion task. The RIR completion can
be widely adapted to enhance RIR generation tasks where fast late reverberation
approximation is required.","['Jackie Lin', 'Georg Götz', 'Sebastian J. Schlecht']",2024-02-01T18:55:37Z,http://arxiv.org/abs/2402.00859v1,['eess.AS'],"room impulse response,RIRs,spatial audio,virtual reality,deep neural network,DECOR,reverberation,autoencoder,rendering techniques,late reverberation"
"Cybersickness Detection through Head Movement Patterns: A Promising
  Approach","Despite the widespread adoption of Virtual Reality (VR) technology,
cybersickness remains a barrier for some users. This research investigates head
movement patterns as a novel physiological marker for cybersickness detection.
Unlike traditional markers, head movements provide a continuous, non-invasive
measure that can be easily captured through the sensors embedded in all
commercial VR headsets. We used a publicly available dataset from a VR
experiment involving 75 participants and analyzed head movements across six
axes. An extensive feature extraction process was then performed on the head
movement dataset and its derivatives, including velocity, acceleration, and
jerk. Three categories of features were extracted, encompassing statistical,
temporal, and spectral features. Subsequently, we employed the Recursive
Feature Elimination method to select the most important and effective features.
In a series of experiments, we trained a variety of machine learning
algorithms. The results demonstrate a 76% accuracy and 83% precision in
predicting cybersickness in the subjects based on the head movements. This
study contribution to the cybersickness literature lies in offering a
preliminary analysis of a new source of data and providing insight into the
relationship of head movements and cybersickness.","['Masoud Salehi', 'Nikoo Javadpour', 'Brietta Beisner', 'Mohammadamin Sanaei', 'Stephen B. Gilbert']",2024-02-05T04:49:59Z,http://arxiv.org/abs/2402.02725v2,"['cs.LG', 'eess.SP']","Cybersickness,Head movement patterns,Physiological marker,VR technology,Feature extraction,Machine learning algorithms"
"Binaural sound source localization using a hybrid time and frequency
  domain model","This paper introduces a new approach to sound source localization using
head-related transfer function (HRTF) characteristics, which enable precise
full-sphere localization from raw data. While previous research focused
primarily on using extensive microphone arrays in the frontal plane, this
arrangement often encountered limitations in accuracy and robustness when
dealing with smaller microphone arrays. Our model proposes using both time and
frequency domain for sound source localization while utilizing Deep Learning
(DL) approach. The performance of our proposed model, surpasses the current
state-of-the-art results. Specifically, it boasts an average angular error of
$0.24 degrees and an average Euclidean distance of 0.01 meters, while the known
state-of-the-art gives average angular error of 19.07 degrees and average
Euclidean distance of 1.08 meters. This level of accuracy is of paramount
importance for a wide range of applications, including robotics, virtual
reality, and aiding individuals with cochlear implants (CI).","['Gil Geva', 'Olivier Warusfel', 'Shlomo Dubnov', 'Tammuz Dubnov', 'Amir Amedi', 'Yacov Hel-Or']",2024-02-06T10:28:07Z,http://arxiv.org/abs/2402.03867v1,"['cs.SD', 'eess.AS']","sound source localization,binaural,time domain,frequency domain,head-related transfer function,HRTF,deep learning,angular error,Euclidean distance,microphone arrays"
"Transferring facade labels between point clouds with semantic octrees
  while considering change detection","Point clouds and high-resolution 3D data have become increasingly important
in various fields, including surveying, construction, and virtual reality.
However, simply having this data is not enough; to extract useful information,
semantic labeling is crucial. In this context, we propose a method to transfer
annotations from a labeled to an unlabeled point cloud using an octree
structure. The structure also analyses changes between the point clouds. Our
experiments confirm that our method effectively transfers annotations while
addressing changes. The primary contribution of this project is the development
of the method for automatic label transfer between two different point clouds
that represent the same real-world object. The proposed method can be of great
importance for data-driven deep learning algorithms as it can also allow
circumventing stochastic transfer learning by deterministic label transfer
between datasets depicting the same objects.","['Sophia Schwarz', 'Tanja Pilz', 'Olaf Wysocki', 'Ludwig Hoegner', 'Uwe Stilla']",2024-02-09T16:43:34Z,http://arxiv.org/abs/2402.06531v1,"['cs.CV', 'cs.LG']","point clouds,semantic labeling,octree structure,change detection,label transfer,deep learning algorithms"
3D Gaussian as a New Vision Era: A Survey,"3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the
field of Computer Graphics, offering explicit scene representation and novel
view synthesis without the reliance on neural networks, such as Neural Radiance
Fields (NeRF). This technique has found diverse applications in areas such as
robotics, urban mapping, autonomous navigation, and virtual reality/augmented
reality, just name a few. Given the growing popularity and expanding research
in 3D Gaussian Splatting, this paper presents a comprehensive survey of
relevant papers from the past year. We organize the survey into taxonomies
based on characteristics and applications, providing an introduction to the
theoretical underpinnings of 3D Gaussian Splatting. Our goal through this
survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a
valuable reference for seminal works in the field, and inspire future research
directions, as discussed in our concluding section.","['Ben Fei', 'Jingyi Xu', 'Rui Zhang', 'Qingyuan Zhou', 'Weidong Yang', 'Ying He']",2024-02-11T12:33:08Z,http://arxiv.org/abs/2402.07181v1,"['cs.CV', 'cs.GR']","3D Gaussian Splatting,Computer Graphics,Scene representation,View synthesis,Neural networks,Robotics,Urban mapping,Autonomous navigation,Virtual reality,Augmented reality"
Digital Twins Below the Surface: Enhancing Underwater Teleoperation,"Subsea exploration, inspection, and intervention operations heavily rely on
remotely operated vehicles (ROVs). However, the inherent complexity of the
underwater environment presents significant challenges to the operators of
these vehicles. This paper delves into the challenges associated with
navigation and maneuvering tasks in the teleoperation of ROVs, such as reduced
situational awareness and heightened teleoperator workload. To address these
challenges, we introduce an underwater Digital Twin (DT) system designed to
enhance underwater teleoperation, enable autonomous navigation, support system
monitoring, and facilitate system testing through simulation. Our approach
involves a dynamic representation of the underwater robot and its environment
using desktop virtual reality, as well as the integration of mapping,
localization, path planning and simulation capabilities within the DT system.
Our research demonstrates the system's adaptability, versatility and
feasibility, highlighting significant challenges and, in turn, improving the
teleoperators' situational awareness and reducing their workload.","['Favour O. Adetunji', 'Niamh Ellis', 'Maria Koskinopoulou', 'Ignacio Carlucho', 'Yvan R. Petillot']",2024-02-12T10:39:17Z,http://arxiv.org/abs/2402.07556v1,['cs.RO'],"Digital Twins,Underwater Teleoperation,Remotely Operated Vehicles,Navigation,Maneuvering,Autonomous Navigation,System Monitoring,Simulation,Mapping,Path Planning"
CrossGaze: A Strong Method for 3D Gaze Estimation in the Wild,"Gaze estimation, the task of predicting where an individual is looking, is a
critical task with direct applications in areas such as human-computer
interaction and virtual reality. Estimating the direction of looking in
unconstrained environments is difficult, due to the many factors that can
obscure the face and eye regions. In this work we propose CrossGaze, a strong
baseline for gaze estimation, that leverages recent developments in computer
vision architectures and attention-based modules. Unlike previous approaches,
our method does not require a specialised architecture, utilizing already
established models that we integrate in our architecture and adapt for the task
of 3D gaze estimation. This approach allows for seamless updates to the
architecture as any module can be replaced with more powerful feature
extractors. On the Gaze360 benchmark, our model surpasses several
state-of-the-art methods, achieving a mean angular error of 9.94 degrees. Our
proposed model serves as a strong foundation for future research and
development in gaze estimation, paving the way for practical and accurate gaze
prediction in real-world scenarios.","['Andy CătrunāE, 'Adrian Cosma', 'Emilian Rădoi']",2024-02-13T09:20:26Z,http://arxiv.org/abs/2402.08316v1,['cs.CV'],"gaze estimation,3D,CrossGaze,computer vision,attention-based modules,architecture,feature extractors,benchmark,mean angular error"
"Saliency-aware End-to-end Learned Variable-Bitrate 360-degree Image
  Compression","Effective compression of 360$^\circ$ images, also referred to as
omnidirectional images (ODIs), is of high interest for various virtual reality
(VR) and related applications. 2D image compression methods ignore the
equator-biased nature of ODIs and fail to address oversampling near the poles,
leading to inefficient compression when applied to ODI. We present a new
learned saliency-aware 360$^\circ$ image compression architecture that
prioritizes bit allocation to more significant regions, considering the unique
properties of ODIs. By assigning fewer bits to less important regions,
significant data size reduction can be achieved while maintaining high visual
quality in the significant regions. To the best of our knowledge, this is the
first study that proposes an end-to-end variable-rate model to compress
360$^\circ$ images leveraging saliency information. The results show
significant bit-rate savings over the state-of-the-art learned and traditional
ODI compression methods at similar perceptual visual quality.","['Oguzhan Gungordu', 'A. Murat Tekalp']",2024-02-14T00:13:39Z,http://arxiv.org/abs/2402.08862v1,['eess.IV'],"360-degree images,compression,saliency-aware,variable-bitrate,end-to-end,virtual reality,omnidirectional images,bit allocation,oversampling,learned model"
"GeoBotsVR: A Robotics Learning Game for Beginners with Hands-on Learning
  Simulation","This article introduces GeoBotsVR, an easily accessible virtual reality game
that combines elements of puzzle-solving with robotics learning and aims to
cultivate interest and motivation in robotics, programming, and electronics
among individuals with limited experience in these domains. The game allows
players to build and customize a two-wheeled mobile robot using various robotic
components and use their robot to solve various procedurally-generated puzzles
in a diverse range of environments. An innovative aspect is the inclusion of a
repair feature, requiring players to address randomly generated electronics and
programming issues with their robot through hands-on manipulation. GeoBotsVR is
designed to be immersive, replayable, and practical application-based, offering
an enjoyable and accessible tool for beginners to acquaint themselves with
robotics. The game simulates a hands-on learning experience and does not
require prior technical knowledge, making it a potentially valuable resource
for beginners to get an engaging introduction to the field of robotics.",['Syed T. Mubarrat'],2024-02-15T02:15:58Z,http://arxiv.org/abs/2402.09662v4,"['cs.HC', 'cs.RO']","GeoBotsVR,Robotics Learning,Virtual Reality,Hands-on Learning,Puzzle-solving,Programming,Electronics,Mobile Robot,Repair Feature,Procedurally-generated"
"Federated Prompt-based Decision Transformer for Customized VR Services
  in Mobile Edge Computing System","This paper investigates resource allocation to provide heterogeneous users
with customized virtual reality (VR) services in a mobile edge computing (MEC)
system. We first introduce a quality of experience (QoE) metric to measure user
experience, which considers the MEC system's latency, user attention levels,
and preferred resolutions. Then, a QoE maximization problem is formulated for
resource allocation to ensure the highest possible user experience,which is
cast as a reinforcement learning problem, aiming to learn a generalized policy
applicable across diverse user environments for all MEC servers. To learn the
generalized policy, we propose a framework that employs federated learning (FL)
and prompt-based sequence modeling to pre-train a common decision model across
MEC servers, which is named FedPromptDT. Using FL solves the problem of
insufficient local MEC data while protecting user privacy during offline
training. The design of prompts integrating user-environment cues and
user-preferred allocation improves the model's adaptability to various user
environments during online execution.","['Tailin Zhou', 'Jiadong Yu', 'Jun Zhang', 'Danny H. K. Tsang']",2024-02-15T05:56:35Z,http://arxiv.org/abs/2402.09729v1,"['cs.AI', 'cs.SY', 'eess.SY']","Federated learning,Prompt-based,Decision transformer,Customized VR services,Mobile edge computing,Resource allocation,Quality of experience,Reinforcement learning,User experience,MEC servers"
"A critical analysis of cognitive load measurement methods for evaluating
  the usability of different types of interfaces: guidelines and framework for
  Human-Computer Interaction","Usability testing is an essential part of product design, particularly for
user interfaces. To enhance the reliability of usability evaluations, employing
cognitive load measurement methods can be highly effective in assessing the
mental effort required to complete tasks during user testing. This review aims
to provide an overview of the most suitable cognitive load measurement methods
for evaluating various types of user interfaces, serving as a valuable resource
for guiding usability assessments. To bridge the existing gap in the
literature, a systematic review was conducted, analyzing 76 articles with
experimental study designs that met the eligibility criteria. The review
encompasses different methods of measuring cognitive load applicable to
assessing the usability of diverse user interfaces, including computer
software, information systems, video games, web and mobile applications,
robotics, and virtual reality applications. The results highlight the most
widely utilized cognitive load measurement methods in software usability, their
respective usage percentages, and their application in evaluating the usability
of each user interface type. Additionally, the advantages and disadvantages of
each method are discussed. Furthermore, the review proposes a framework to
assist usability testers in selecting an appropriate cognitive load measurement
method for conducting accurate usability evaluations.","['Ali Darejeh', 'Nadine Marcusa', 'Gelareh Mohammadi', 'John Sweller']",2024-02-19T04:25:35Z,http://arxiv.org/abs/2402.11820v1,['cs.HC'],"cognitive load measurement methods,usability,human-computer interaction,user interfaces,mental effort,user testing,software usability,information systems,video games,web applications"
"Advancements in Point Cloud-Based 3D Defect Detection and Classification
  for Industrial Systems: A Comprehensive Survey","In recent years, 3D point clouds (PCs) have gained significant attention due
to their diverse applications across various fields such as computer vision
(CV), condition monitoring, virtual reality, robotics, autonomous driving etc.
Deep learning (DL) has proven effective in leveraging 3D PCs to address various
challenges previously encountered in 2D vision. However, the application of
deep neural networks (DNN) to process 3D PCs presents its own set of
challenges. To address these challenges, numerous methods have been proposed.
This paper provides an in-depth review of recent advancements in DL-based
condition monitoring (CM) using 3D PCs, with a specific focus on defect shape
classification and segmentation within industrial applications for operational
and maintenance purposes. Recognizing the crucial role of these aspects in
industrial maintenance, the paper provides insightful observations that offer
perspectives on the strengths and limitations of the reviewed DL-based PC
processing methods. This synthesis of knowledge aims to contribute to the
understanding and enhancement of CM processes, particularly within the
framework of remaining useful life (RUL), in industrial systems.","['Anju Rani', 'Daniel Ortiz-Arroyo', 'Petar Durdevic']",2024-02-20T11:18:40Z,http://arxiv.org/abs/2402.12923v1,['cs.CV'],"Point Clouds,Defect Detection,Classification,Industrial Systems,Deep Learning,Condition Monitoring,3D Vision,Neural Networks,Maintenance,Segmentation"
"VLPose: Bridging the Domain Gap in Pose Estimation with Language-Vision
  Tuning","Thanks to advances in deep learning techniques, Human Pose Estimation (HPE)
has achieved significant progress in natural scenarios. However, these models
perform poorly in artificial scenarios such as painting and sculpture due to
the domain gap, constraining the development of virtual reality and augmented
reality. With the growth of model size, retraining the whole model on both
natural and artificial data is computationally expensive and inefficient. Our
research aims to bridge the domain gap between natural and artificial scenarios
with efficient tuning strategies. Leveraging the potential of language models,
we enhance the adaptability of traditional pose estimation models across
diverse scenarios with a novel framework called VLPose. VLPose leverages the
synergy between language and vision to extend the generalization and robustness
of pose estimation models beyond the traditional domains. Our approach has
demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO,
respectively, compared to state-of-the-art tuning strategies.","['Jingyao Li', 'Pengguang Chen', 'Xuan Ju', 'Hong Xu', 'Jiaya Jia']",2024-02-22T11:21:54Z,http://arxiv.org/abs/2402.14456v1,['cs.CV'],"pose estimation,domain gap,deep learning,virtual reality,augmented reality,tuning strategies,language models,generalization,robustness,VLPose"
Seamless Human Motion Composition with Blended Positional Encodings,"Conditional human motion generation is an important topic with many
applications in virtual reality, gaming, and robotics. While prior works have
focused on generating motion guided by text, music, or scenes, these typically
result in isolated motions confined to short durations. Instead, we address the
generation of long, continuous sequences guided by a series of varying textual
descriptions. In this context, we introduce FlowMDM, the first diffusion-based
model that generates seamless Human Motion Compositions (HMC) without any
postprocessing or redundant denoising steps. For this, we introduce the Blended
Positional Encodings, a technique that leverages both absolute and relative
positional encodings in the denoising chain. More specifically, global motion
coherence is recovered at the absolute stage, whereas smooth and realistic
transitions are built at the relative stage. As a result, we achieve
state-of-the-art results in terms of accuracy, realism, and smoothness on the
Babel and HumanML3D datasets. FlowMDM excels when trained with only a single
description per motion sequence thanks to its Pose-Centric Cross-ATtention,
which makes it robust against varying text descriptions at inference time.
Finally, to address the limitations of existing HMC metrics, we propose two new
metrics: the Peak Jerk and the Area Under the Jerk, to detect abrupt
transitions.","['German Barquero', 'Sergio Escalera', 'Cristina Palmero']",2024-02-23T18:59:40Z,http://arxiv.org/abs/2402.15509v1,['cs.CV'],"Conditional human motion generation,Virtual reality,Gaming,Robotics,Diffusion-based model,Blended Positional Encodings,Global motion coherence,Pose-Centric Cross-Attention,Babel dataset,HumanML3D dataset,Peak Jerk,Area Under the Jerk"
"Designing for Human Operations on the Moon: Challenges and Opportunities
  of Navigational HUD Interfaces","Future crewed missions to the Moon will face significant environmental and
operational challenges, posing risks to the safety and performance of
astronauts navigating its inhospitable surface. Whilst head-up displays (HUDs)
have proven effective in providing intuitive navigational support on Earth, the
design of novel human-spaceflight solutions typically relies on costly and
time-consuming analogue deployments, leaving the potential use of lunar HUDs
largely under-explored. This paper explores an alternative approach by
simulating navigational HUD concepts in a high-fidelity Virtual Reality (VR)
representation of the lunar environment. In evaluating these concepts with
astronauts and other aerospace experts (n=25), our mixed methods study
demonstrates the efficacy of simulated analogues in facilitating rapid design
assessments of early-stage HUD solutions. We illustrate this by elaborating key
design challenges and guidelines for future lunar HUDs. In reflecting on the
limitations of our approach, we propose directions for future design
exploration of human-machine interfaces for the Moon.","['Leonie Bensch', 'Tommy Nilsson', 'Jan Wulkop', 'Paul de Medeiros', 'Nicolas Daniel Herzberger', 'Michael Preutenborbeck', 'Andreas Gerndt', 'Frank Flemisch', 'Florian Dufresne', 'Georgia Albuquerque', 'Aidan Cowley']",2024-02-24T02:35:27Z,http://arxiv.org/abs/2402.15692v1,"['cs.HC', '93B51, 97M50', 'H.1.2; I.3.8; J.4; J.m; K.8.2']","Human operations,Moon,Navigational HUD interfaces,Crewed missions,Astronauts,Head-up displays (HUDs),Virtual Reality (VR),Aerospace experts,Design assessments,Human-machine interfaces"
Swarm Body: Embodied Swarm Robots,"The human brain's plasticity allows for the integration of artificial body
parts into the human body. Leveraging this, embodied systems realize intuitive
interactions with the environment. We introduce a novel concept: embodied swarm
robots. Swarm robots constitute a collective of robots working in harmony to
achieve a common objective, in our case, serving as functional body parts.
Embodied swarm robots can dynamically alter their shape, density, and the
correspondences between body parts and individual robots. We contribute an
investigation of the influence on embodiment of swarm robot-specific factors
derived from these characteristics, focusing on a hand. Our paper is the first
to examine these factors through virtual reality (VR) and real-world robot
studies to provide essential design considerations and applications of embodied
swarm robots. Through quantitative and qualitative analysis, we identified a
system configuration to achieve the embodiment of swarm robots.","['Sosuke Ichihashi', 'So Kuroki', 'Mai Nishimura', 'Kazumi Kasaura', 'Takefumi Hiraki', 'Kazutoshi Tanaka', 'Shigeo Yoshida']",2024-02-24T14:51:11Z,http://arxiv.org/abs/2402.15830v2,"['cs.HC', 'cs.ET', 'cs.RO']","embodied systems,swarm robots,body parts,shape alteration,density alteration,body-robot correspondences,embodiment,virtual reality (VR),design considerations"
Human Shape and Clothing Estimation,"Human shape and clothing estimation has gained significant prominence in
various domains, including online shopping, fashion retail, augmented reality
(AR), virtual reality (VR), and gaming. The visual representation of human
shape and clothing has become a focal point for computer vision researchers in
recent years. This paper presents a comprehensive survey of the major works in
the field, focusing on four key aspects: human shape estimation, fashion
generation, landmark detection, and attribute recognition. For each of these
tasks, the survey paper examines recent advancements, discusses their strengths
and limitations, and qualitative differences in approaches and outcomes. By
exploring the latest developments in human shape and clothing estimation, this
survey aims to provide a comprehensive understanding of the field and inspire
future research in this rapidly evolving domain.","['Aayush Gupta', 'Aditya Gulati', 'Himanshu', 'Lakshya LNU']",2024-02-28T04:00:57Z,http://arxiv.org/abs/2402.18032v1,['cs.CV'],"human shape estimation,clothing estimation,online shopping,fashion retail,augmented reality,virtual reality,gaming,computer vision,landmark detection,attribute recognition"
"3DTextureTransformer: Geometry Aware Texture Generation for Arbitrary
  Mesh Topology","Learning to generate textures for a novel 3D mesh given a collection of 3D
meshes and real-world 2D images is an important problem with applications in
various domains such as 3D simulation, augmented and virtual reality, gaming,
architecture, and design. Existing solutions either do not produce high-quality
textures or deform the original high-resolution input mesh topology into a
regular grid to make this generation easier but also lose the original mesh
topology. In this paper, we present a novel framework called the
3DTextureTransformer that enables us to generate high-quality textures without
deforming the original, high-resolution input mesh. Our solution, a hybrid of
geometric deep learning and StyleGAN-like architecture, is flexible enough to
work on arbitrary mesh topologies and also easily extensible to texture
generation for point cloud representations. Our solution employs a
message-passing framework in 3D in conjunction with a StyleGAN-like
architecture for 3D texture generation. The architecture achieves
state-of-the-art performance among a class of solutions that can learn from a
collection of 3D geometry and real-world 2D images while working with any
arbitrary mesh topology.","['Dharma KC', 'Clayton T. Morrison']",2024-03-07T05:01:07Z,http://arxiv.org/abs/2403.04225v1,['cs.CV'],"3D mesh,Texture generation,Mesh topology,Geometric deep learning,StyleGAN,Message-passing framework,Point cloud representations,3D texture generation,High-resolution input mesh,Arbitrary mesh topologies"
"To Reach the Unreachable: Exploring the Potential of VR Hand Redirection
  for Upper Limb Rehabilitation","Rehabilitation therapies are widely employed to assist people with motor
impairments in regaining control over their affected body parts. Nevertheless,
factors such as fatigue and low self-efficacy can hinder patient compliance
during extensive rehabilitation processes. Utilizing hand redirection in
virtual reality (VR) enables patients to accomplish seemingly more challenging
tasks, thereby bolstering their motivation and confidence. While previous
research has investigated user experience and hand redirection among
able-bodied people, its effects on motor-impaired people remain unexplored. In
this paper, we present a VR rehabilitation application that harnesses hand
redirection. Through a user study and semi-structured interviews, we examine
the impact of hand redirection on the rehabilitation experiences of people with
motor impairments and its potential to enhance their motivation for upper limb
rehabilitation. Our findings suggest that patients are not sensitive to hand
movement inconsistency, and the majority express interest in incorporating hand
redirection into future long-term VR rehabilitation programs.","['Peixuan Xiong', 'Yukai Zhang', 'Nandi Zhang', 'Shihan Fu', 'Xin Li', 'Yadan Zheng', 'Jinni Zhou', 'Xiquan Hu', 'Mingming Fan']",2024-03-08T12:41:47Z,http://arxiv.org/abs/2403.05264v1,['cs.HC'],"rehabilitation therapies,motor impairments,virtual reality,hand redirection,upper limb rehabilitation,user study,semi-structured interviews,user experience,motivation"
A Unified Framework for Underwater Metaverse with Optical Perception,"With the advancement of AI technology and increasing attention to deep-sea
exploration, the underwater Metaverse is gradually emerging. This paper
explores the concept of underwater Metaverse, emerging virtual reality systems
and services aimed at simulating and enhancing virtual experience of marine
environments. First, we discuss potential applications of underwater Metaverse
in underwater scientific research and marine conservation. Next, we present the
architecture and supporting technologies of the underwater Metaverse, including
high-resolution underwater imageing technologies and image processing
technologies for rendering a realistic virtual world. Based on this, we present
a use case for building a realistic underwater virtual world using underwater
quantum imaging-generated artificial intelligence (QI-GAI) technology. The
results demonstrate the effectiveness of the underwater Metaverse framework in
simulating complex underwater environments, thus validating its potential in
providing high-quality, interactive underwater virtual experiences. Finally,
the paper examines the future development directions of underwater Metaverse,
and provides new perspectives for marine science and conservation.","['Jingyang Cao', 'Mu Zhou', 'Jiacheng Wang', 'Guangyuan Liu', 'Dusit Niyato', 'Shiwen Mao', 'Zhu Han', 'Jiawen Kang']",2024-02-21T02:38:27Z,http://arxiv.org/abs/2403.05567v1,['cs.HC'],"AI technology,deep-sea exploration,underwater Metaverse,virtual reality systems,marine environments,underwater scientific research,marine conservation,image processing technologies,high-resolution underwater imaging technologies,underwater quantum imaging"
"Exploring the Impact of Interconnected External Interfaces in Autonomous
  Vehicleson Pedestrian Safety and Experience","Policymakers advocate for the use of external Human-Machine Interfaces
(eHMIs) to allow autonomous vehicles (AVs) to communicate their intentions or
status. Nonetheless, scalability concerns in complex traffic scenarios arise,
such as potentially increasing pedestrian cognitive load or conveying
contradictory signals. Building upon precursory works, our study explores
'interconnected eHMIs,' where multiple AV interfaces are interconnected to
provide pedestrians with clear and unified information. In a virtual reality
study (N=32), we assessed the effectiveness of this concept in improving
pedestrian safety and their crossing experience. We compared these results
against two conditions: no eHMIs and unconnected eHMIs. Results indicated
interconnected eHMIs enhanced safety feelings and encouraged cautious
crossings. However, certain design elements, such as the use of the colour red,
led to confusion and discomfort. Prior knowledge slightly influenced
perceptions of interconnected eHMIs, underscoring the need for refined user
education. We conclude with practical implications and future eHMI design
research directions.","['Tram Thi Minh Tran', 'Callum Parker', 'Marius Hoggenmuller', 'Yiyuan Wang', 'Martin Tomitsch']",2024-03-08T23:38:56Z,http://arxiv.org/abs/2403.05725v2,['cs.HC'],"external Human-Machine Interfaces,autonomous vehicles,pedestrian safety,interconnected eHMIs,virtual reality study,pedestrian experience,cognitive load,contradictory signals,design elements,user education."
"PAPER-HILT: Personalized and Adaptive Privacy-Aware Early-Exit for
  Reinforcement Learning in Human-in-the-Loop Systems","Reinforcement Learning (RL) has increasingly become a preferred method over
traditional rule-based systems in diverse human-in-the-loop (HITL) applications
due to its adaptability to the dynamic nature of human interactions. However,
integrating RL in such settings raises significant privacy concerns, as it
might inadvertently expose sensitive user information. Addressing this, our
paper focuses on developing PAPER-HILT, an innovative, adaptive RL strategy
through exploiting an early-exit approach designed explicitly for privacy
preservation in HITL environments. This approach dynamically adjusts the
tradeoff between privacy protection and system utility, tailoring its operation
to individual behavioral patterns and preferences. We mainly highlight the
challenge of dealing with the variable and evolving nature of human behavior,
which renders static privacy models ineffective. PAPER-HILT's effectiveness is
evaluated through its application in two distinct contexts: Smart Home
environments and Virtual Reality (VR) Smart Classrooms. The empirical results
demonstrate PAPER-HILT's capability to provide a personalized equilibrium
between user privacy and application utility, adapting effectively to
individual user needs and preferences. On average for both experiments, utility
(performance) drops by 24%, and privacy (state prediction) improves by 31%.","['Mojtaba Taherisadr', 'Salma Elmalaki']",2024-03-09T10:24:12Z,http://arxiv.org/abs/2403.05864v1,"['cs.LG', 'cs.CR', 'cs.HC', 'F.2.2', 'I.2.7']","Reinforcement Learning,Human-in-the-Loop Systems,Privacy Preservation,Early-Exit Approach,Dynamic Adjustment,Privacy Models,Smart Home,Virtual Reality,Utility,Privacy"
Harmonious Group Choreography with Trajectory-Controllable Diffusion,"Creating group choreography from music has gained attention in cultural
entertainment and virtual reality, aiming to coordinate visually cohesive and
diverse group movements. Despite increasing interest, recent works face
challenges in achieving aesthetically appealing choreography, primarily for two
key issues: multi-dancer collision and single-dancer foot slide. To address
these issues, we propose a Trajectory-Controllable Diffusion (TCDiff), a novel
approach that harnesses non-overlapping trajectories to facilitate coherent
dance movements. Specifically, to tackle dancer collisions, we introduce a
Dance-Beat Navigator capable of generating trajectories for multiple dancers
based on the music, complemented by a Distance-Consistency loss to maintain
appropriate spacing among trajectories within a reasonable threshold. To
mitigate foot sliding, we present a Footwork Adaptor that utilizes trajectory
displacement from adjacent frames to enable flexible footwork, coupled with a
Relative Forward-Kinematic loss to adjust the positioning of individual
dancers' root nodes and joints. Extensive experiments demonstrate that our
method achieves state-of-the-art results.","['Yuqin Dai', 'Wanlu Zhu', 'Ronghui Li', 'Zeping Ren', 'Xiangzheng Zhou', 'Xiu Li', 'Jun Li', 'Jian Yang']",2024-03-10T12:11:34Z,http://arxiv.org/abs/2403.06189v1,['cs.CV'],"group choreography,trajectory-controllable diffusion,dance movements,dancer collisions,foot sliding,music-based trajectory generation,distance-consistency loss,footwork adaptor,relative forward-kinematic loss,state-of-the-art results"
"Designing for Projection-based Communication between Autonomous Vehicles
  and Pedestrians","Recent studies have investigated new approaches for communicating an
autonomous vehicle's (AV) intent and awareness to pedestrians. This paper adds
to this body of work by presenting the design and evaluation of in-situ
projections on the road. Our design combines common traffic light patterns with
aesthetic visual elements. We describe the iterative design process and the
prototyping methods used in each stage. The final design concept was
represented as a virtual reality simulation and evaluated with 18 participants
in four different street crossing scenarios, which included three scenarios
that simulated various degrees of system errors. We found that different design
elements were able to support participants' confidence in their decision even
when the AV failed to correctly detect their presence. We also identified
elements in our design that needed to be more clearly communicated. Based on
these findings, the paper presents a series of design recommendations for
projection-based communication between AVs and pedestrians.","['Trung Thanh Nguyen', 'Kai Hollander', 'Marius Hoggenmueller', 'Callum Parker', 'Martin Tomitsch']",2024-03-11T04:40:24Z,http://arxiv.org/abs/2403.06429v1,['cs.HC'],"Projection-based communication,Autonomous vehicles,Pedestrians,Design,Evaluation,Traffic light patterns,Virtual reality simulation,System errors,Design recommendations"
"Designing Wearable Augmented Reality Concepts to Support Scalability in
  Autonomous Vehicle-Pedestrian Interaction","Wearable augmented reality (AR) offers new ways for supporting the
interaction between autonomous vehicles (AVs) and pedestrians due to its
ability to integrate timely and contextually relevant data into the user's
field of view. This article presents novel wearable AR concepts that assist
crossing pedestrians in multi-vehicle scenarios where several AVs frequent the
road from both directions. Three concepts with different communication
approaches for signaling responses from multiple AVs to a crossing request, as
well as a conventional pedestrian push button, were simulated and tested within
a virtual reality environment. The results showed that wearable AR is a
promising way to reduce crossing pedestrians' cognitive load when the design
offers both individual AV responses and a clear signal to cross. The
willingness of pedestrians to adopt a wearable AR solution, however, is subject
to different factors, including costs, data privacy, technical defects,
liability risks, maintenance duties, and form factors. We further found that
all participants favored sending a crossing request to AVs rather than waiting
for the vehicles to detect their intentions-pointing to an important gap and
opportunity in the current AV-pedestrian interaction literature.","['Tram Thi Minh Tran', 'Callum Parker', 'Yiyuan Wang', 'Martin Tomitsch']",2024-03-08T23:47:29Z,http://arxiv.org/abs/2403.07006v1,['cs.HC'],"wearable augmented reality,autonomous vehicles,pedestrians,scalability,interaction,multi-vehicle scenarios,virtual reality,cognitive load,data privacy,liability risks"
"Dynamic Field of View Reduction Related to Subjective Sickness Measures
  in an HMD-based Data Analysis Task","Various factors influence the degree of cybersickness a user can suffer in an
immersive virtual environment, some of which can be controlled without adapting
the virtual environment itself. When using HMDs, one example is the size of the
field of view. However, the degree to which factors like this can be
manipulated without affecting the user negatively in other ways is limited.
Another prominent characteristic of cybersickness is that it affects
individuals very differently. Therefore, to account for both the possible
disruptive nature of alleviating factors and the high interpersonal variance, a
promising approach may be to intervene only in cases where users experience
discomfort symptoms, and only as much as necessary. Thus, we conducted a first
experiment, where the field of view was decreased when people feel
uncomfortable, to evaluate the possible positive impact on sickness and
negative influence on presence. While we found no significant evidence for any
of these possible effects, interesting further results and observations were
made.","['Daniel Zielasko', 'Alexander Meißner', 'Sebastian Freitag', 'Benjamin Weyers', 'Torsten W. Kuhlen']",2024-03-12T18:01:10Z,http://arxiv.org/abs/2403.07992v1,['cs.HC'],"Dynamic Field of View Reduction,Subjective Sickness Measures,HMD,Cybersickness,Immersive Virtual Environment,Discomfort Symptoms,Interpersonal Variance,Experiment,Sickness,Presence"
STMPL: Human Soft-Tissue Simulation,"In various applications, such as virtual reality and gaming, simulating the
deformation of soft tissues in the human body during interactions with external
objects is essential. Traditionally, Finite Element Methods (FEM) have been
employed for this purpose, but they tend to be slow and resource-intensive. In
this paper, we propose a unified representation of human body shape and soft
tissue with a data-driven simulator of non-rigid deformations. This approach
enables rapid simulation of realistic interactions.
  Our method builds upon the SMPL model, which generates human body shapes
considering rigid transformations. We extend SMPL by incorporating a soft
tissue layer and an intuitive representation of external forces applied to the
body during object interactions. Specifically, we mapped the 3D body shape and
soft tissue and applied external forces to 2D UV maps. Leveraging a UNET
architecture designed for 2D data, our approach achieves high-accuracy
inference in real time. Our experiment shows that our method achieves plausible
deformation of the soft tissue layer, even for unseen scenarios.","['Anton Agafonov', 'Lihi Zelnik-Manor']",2024-03-13T08:49:40Z,http://arxiv.org/abs/2403.08344v1,"['cs.CV', 'cs.GR', 'cs.LG']","soft tissue,simulation,finite element methods,non-rigid deformations,SMPL model,external forces,3D body shape,UV maps,UNET architecture"
"ShareYourReality: Investigating Haptic Feedback and Agency in Virtual
  Avatar Co-embodiment","Virtual co-embodiment enables two users to share a single avatar in Virtual
Reality (VR). During such experiences, the illusion of shared motion control
can break during joint-action activities, highlighting the need for
position-aware feedback mechanisms. Drawing on the perceptual crossing
paradigm, we explore how haptics can enable non-verbal coordination between
co-embodied participants. In a within-subjects study (20 participant pairs), we
examined the effects of vibrotactile haptic feedback (None, Present) and avatar
control distribution (25-75%, 50-50%, 75-25%) across two VR reaching tasks
(Targeted, Free-choice) on participants Sense of Agency (SoA), co-presence,
body ownership, and motion synchrony. We found (a) lower SoA in the free-choice
with haptics than without, (b) higher SoA during the shared targeted task, (c)
co-presence and body ownership were significantly higher in the free-choice
task, (d) players hand motions synchronized more in the targeted task. We
provide cautionary considerations when including haptic feedback mechanisms for
avatar co-embodiment experiences.","['Karthikeya Puttur Venkatraj', 'Wo Meijer', 'Monica Perusquía-Hernández', 'Gijs Huisman', 'Abdallah El Ali']",2024-03-13T09:23:53Z,http://arxiv.org/abs/2403.08363v1,"['cs.HC', 'cs.CY', 'H.5.m']","Haptic Feedback,Virtual Reality,Avatar Co-embodiment,Sense of Agency,Co-presence,Body Ownership,Motion Synchrony,Vibrotactile,Perceptual Crossing,Position-aware"
Gaussian Splatting in Style,"Scene stylization extends the work of neural style transfer to three spatial
dimensions. A vital challenge in this problem is to maintain the uniformity of
the stylized appearance across a multi-view setting. A vast majority of the
previous works achieve this by optimizing the scene with a specific style
image. In contrast, we propose a novel architecture trained on a collection of
style images, that at test time produces high quality stylized novel views. Our
work builds up on the framework of 3D Gaussian splatting. For a given scene, we
take the pretrained Gaussians and process them using a multi resolution hash
grid and a tiny MLP to obtain the conditional stylised views. The explicit
nature of 3D Gaussians give us inherent advantages over NeRF-based methods
including geometric consistency, along with having a fast training and
rendering regime. This enables our method to be useful for vast practical use
cases such as in augmented or virtual reality applications. Through our
experiments, we show our methods achieve state-of-the-art performance with
superior visual quality on various indoor and outdoor real-world data.","['Abhishek Saroha', 'Mariia Gladkova', 'Cecilia Curreli', 'Tarun Yenamandra', 'Daniel Cremers']",2024-03-13T13:06:31Z,http://arxiv.org/abs/2403.08498v1,['cs.CV'],"neural style transfer,scene stylization,3D Gaussian splatting,multi-view setting,stylized appearance,style image,architecture,pretrained Gaussians,multi resolution hash grid,conditional stylised views"
"Bury Me Here --The New Genre of Narrative Design Game Based on Immersive
  Storytelling","Virtual reality games always provide the player with the most verisimilitude
experience. With the advancement of VR hardware, it may become mainstream how
people feel and attach to a virtual world. The paper discusses a possible
solution to finding a better balance between the two classical genres of VR
games, sensory stimulation and storytelling. To this end, we designed a game
named ""Bury Me Here,"" in which players can find an emotional bond between the
game protagonist and themselves. The game includes four sections, the departure
from the hometown, the travel on the train, the work in the office, and the
life in the penthouse. At the game's end, the protagonist returns to his
country yard and spends the rest of his life there. All the sections are
designed to tell a stranger's life story to the player, making them experience
someone else's life path and bonding an emotional connection between the player
and the protagonist through storytelling. Results show that the game provides
an immersive visual experience and has emotive sparks echo in players' minds.","['Zhongsheng Li', 'Wuji Li', 'Yudong He']",2024-03-13T18:46:50Z,http://arxiv.org/abs/2403.08903v1,['cs.HC'],"narrative design,game,immersive storytelling,virtual reality,VR hardware,sensory stimulation,protagonist,emotional bond,life story,immersive visual experience"
The Full-scale Assembly Simulation Testbed (FAST) Dataset,"In recent years, numerous researchers have begun investigating how virtual
reality (VR) tracking and interaction data can be used for a variety of machine
learning purposes, including user identification, predicting cybersickness, and
estimating learning gains. One constraint for this research area is the dearth
of open datasets. In this paper, we present a new open dataset captured with
our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset
consists of data collected from 108 participants (50 females, 56 males, 2
non-binary) learning how to assemble two distinct full-scale structures in VR.
In addition to explaining how the dataset was collected and describing the data
included, we discuss how the dataset may be used by future researchers.","['Alec G. Moore', 'Tiffany D. Do', 'Nayan N. Chawla', 'Antonia Jimenez Iriarte', 'Ryan P. McMahan']",2024-03-13T21:30:01Z,http://arxiv.org/abs/2403.08969v1,"['cs.HC', 'cs.LG']","virtual reality,tracking data,interaction data,machine learning,open dataset,VR-based,simulation,assembly,learning gains,user identification"
"MambaTalk: Efficient Holistic Gesture Synthesis with Selective State
  Space Models","Gesture synthesis is a vital realm of human-computer interaction, with
wide-ranging applications across various fields like film, robotics, and
virtual reality. Recent advancements have utilized the diffusion model and
attention mechanisms to improve gesture synthesis. However, due to the high
computational complexity of these techniques, generating long and diverse
sequences with low latency remains a challenge. We explore the potential of
state space models (SSMs) to address the challenge, implementing a two-stage
modeling strategy with discrete motion priors to enhance the quality of
gestures. Leveraging the foundational Mamba block, we introduce MambaTalk,
enhancing gesture diversity and rhythm through multimodal integration.
Extensive experiments demonstrate that our method matches or exceeds the
performance of state-of-the-art models.","['Zunnan Xu', 'Yukang Lin', 'Haonan Han', 'Sicheng Yang', 'Ronghui Li', 'Yachao Zhang', 'Xiu Li']",2024-03-14T15:10:54Z,http://arxiv.org/abs/2403.09471v1,"['cs.CV', 'cs.HC']","gesture synthesis,state space models,multimodal integration,computational complexity,attention mechanisms,diffusion model,Mamba block,latency,gesture diversity,rhythm"
"Enhancing Bandwidth Efficiency for Video Motion Transfer Applications
  using Deep Learning Based Keypoint Prediction","We propose a deep learning based novel prediction framework for enhanced
bandwidth reduction in motion transfer enabled video applications such as video
conferencing, virtual reality gaming and privacy preservation for patient
health monitoring. To model complex motion, we use the First Order Motion Model
(FOMM) that represents dynamic objects using learned keypoints along with their
local affine transformations. Keypoints are extracted by a self-supervised
keypoint detector and organized in a time series corresponding to the video
frames. Prediction of keypoints, to enable transmission using lower frames per
second on the source device, is performed using a Variational Recurrent Neural
Network (VRNN). The predicted keypoints are then synthesized to video frames
using an optical flow estimator and a generator network. This efficacy of
leveraging keypoint based representations in conjunction with VRNN based
prediction for both video animation and reconstruction is demonstrated on three
diverse datasets. For real-time applications, our results show the
effectiveness of our proposed architecture by enabling up to 2x additional
bandwidth reduction over existing keypoint based video motion transfer
frameworks without significantly compromising video quality.","['Xue Bai', 'Tasmiah Haque', 'Sumit Mohan', 'Yuliang Cai', 'Byungheon Jeong', 'Adam Halasz', 'Srinjoy Das']",2024-03-17T20:36:43Z,http://arxiv.org/abs/2403.11337v1,"['cs.CV', 'cs.AI']","bandwidth efficiency,deep learning,keypoint prediction,motion transfer,video applications,First Order Motion Model (FOMM),keypoints,Variational Recurrent Neural Network (VRNN),optical flow estimator,generator network"
"Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons
  Learnt from Designing Two Future Urban Interfaces","Augmented reality (AR) has the potential to fundamentally change how people
engage with increasingly interactive urban environments. However, many
challenges exist in designing and evaluating these new urban AR experiences,
such as technical constraints and safety concerns associated with outdoor AR.
We contribute to this domain by assessing the use of virtual reality (VR) for
simulating wearable urban AR experiences, allowing participants to interact
with future AR interfaces in a realistic, safe and controlled setting. This
paper describes two wearable urban AR applications (pedestrian navigation and
autonomous mobility) simulated in VR. Based on a thematic analysis of interview
data collected across the two studies, we found that the VR simulation
successfully elicited feedback on the functional benefits of AR concepts and
the potential impact of urban contextual factors, such as safety concerns,
attentional capacity, and social considerations. At the same time, we
highlighted the limitations of this approach in terms of assessing the AR
interface's visual quality and providing exhaustive contextual information. The
paper concludes with recommendations for simulating wearable urban AR
experiences in VR.","['Tram Thi Minh Tran', 'Callum Parker', 'Marius Hoggenmüller', 'Luke Hespanhol', 'Martin Tomitsch']",2024-03-18T00:05:16Z,http://arxiv.org/abs/2403.11377v1,['cs.HC'],"wearable,urban augmented reality,VR,future interfaces,simulation,pedestrian navigation,autonomous mobility,safety concerns,attentional capacity"
"Towards Massive Interaction with Generalist Robotics: A Systematic
  Review of XR-enabled Remote Human-Robot Interaction Systems","The rising interest of generalist robots seek to create robots with
versatility to handle multiple tasks in a variety of environments, and human
will interact with such robots through immersive interfaces. In the context of
human-robot interaction (HRI), this survey provides an exhaustive review of the
applications of extended reality (XR) technologies in the field of remote HRI.
We developed a systematic search strategy based on the PRISMA methodology. From
the initial 2,561 articles selected, 100 research papers that met our inclusion
criteria were included. We categorized and summarized the domain in detail,
delving into XR technologies, including augmented reality (AR), virtual reality
(VR), and mixed reality (MR), and their applications in facilitating intuitive
and effective remote control and interaction with robotic systems. The survey
highlights existing articles on the application of XR technologies, user
experience enhancement, and various interaction designs for XR in remote HRI,
providing insights into current trends and future directions. We also
identified potential gaps and opportunities for future research to improve
remote HRI systems through XR technology to guide and inform future XR and
robotics research.","['Xian Wang', 'Luyao Shen', 'Lik-Hang Lee']",2024-03-18T00:22:30Z,http://arxiv.org/abs/2403.11384v3,"['cs.HC', 'cs.RO']","generalist robots,human-robot interaction,extended reality,XR technologies,augmented reality,virtual reality,mixed reality,remote control,interaction designs,robotics"
Just Undo It: Exploring Undo Mechanics in Multi-User Virtual Reality,"With the proliferation of VR and a metaverse on the horizon, many multi-user
activities are migrating to the VR world, calling for effective collaboration
support. As one key feature, traditional collaborative systems provide users
with undo mechanics to reverse errors and other unwanted changes. While undo
has been extensively researched in this domain and is now considered industry
standard, it is strikingly absent for VR systems in research and industry. This
work addresses this research gap by exploring different undo techniques for
basic object manipulation in different collaboration modes in VR. We conducted
a study involving 32 participants organized in teams of two. Here, we studied
users' performance and preferences in a tower stacking task, varying the
available undo techniques and their mode of collaboration. The results suggest
that users desire and use undo in VR and that the choice of the undo technique
impacts users' performance and social connection.","['Julian Rasch', 'Florian Perzl', 'Yannick Weiss', 'Florian Müller']",2024-03-18T13:06:43Z,http://arxiv.org/abs/2403.11756v1,['cs.HC'],"Virtual reality,Multi-user,Undo mechanics,Collaboration support,Object manipulation,Collaboration modes,Study,Performance,Preferences,Undo techniques"
Intention Action Anticipation Model with Guide-Feedback Loop Mechanism,"Anticipating human intention from videos has broad applications, such as
automatic driving, robot assistive technology, and virtual reality. This study
addresses the problem of intention action anticipation using egocentric video
sequences to estimate actions that indicate human intention. We propose a
Hierarchical Complete-Recent (HCR) information fusion model that makes full use
of the features of the entire video sequence (i.e., complete features) and the
features of the video tail sequence (i.e., recent features). The HCR model has
two primary mechanisms. The Guide-Feedback Loop (GFL) mechanism is proposed to
model the relation between one recent feature and one complete feature. Based
on GFL, the MultiComplete-Recent Feature Aggregation (MCRFA) module is proposed
to model the relation of one recent feature with multiscale complete features.
Based on GFL and MCRFA, the HCR model can hierarchically explore the rich
interrelationships between multiscale complete features and multiscale recent
features. Through comparative and ablation experiments, we validate the
effectiveness of our model on two well-known public datasets: EPIC-Kitchens and
EGTEA Gaze+.","['Zongnan Ma', 'Fuchun Zhang', 'Zhixiong Nan', 'Yao Ge']",2024-03-19T05:21:12Z,http://arxiv.org/abs/2403.12450v1,['cs.CV'],"egocentric video sequences,intention action anticipation,Hierarchical Complete-Recent (HCR) information fusion model,Guide-Feedback Loop (GFL) mechanism,MultiComplete-Recent Feature Aggregation (MCRFA) module,multiscale complete features,multiscale recent features,EPIC-Kitchens,EGTEA Gaze+"
RGBD GS-ICP SLAM,"Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.","['Seongbo Ha', 'Jiung Yeon', 'Hyeonwoo Yu']",2024-03-19T08:49:48Z,http://arxiv.org/abs/2403.12550v2,['cs.CV'],"SLAM,dense representation,neural scene representation,3D Gaussian representation,Generalized Iterative Closest Point,3D Gaussian Splatting,covariance,keyframe selection,tracking accuracy,mapping quality"
Experimental Studies of Metaverse Streaming,"Metaverse aims to construct a large, unified, immersive, and shared digital
realm by combining various technologies, namely XR (extended reality),
blockchain, and digital twin, among others. This article explores the Metaverse
from the perspective of multimedia communication by conducting and analyzing
real-world experiments on four different Metaverse platforms: VR (virtual
reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual
City. We first investigate the traffic patterns and network performance in the
three VR platforms. After raising the challenges of the Metaverse streaming and
investigating the potential methods to enhance Metaverse performance, we
propose a remote rendering architecture and verify its advantages through a
prototype involving the campus network and MR multimodal interaction by
comparison with local rendering.","['Haopeng Wang', 'Roberto Martinez-Velazquez', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",2024-03-22T14:57:12Z,http://arxiv.org/abs/2403.15256v1,"['cs.MM', 'cs.NI']","Metaverse,streaming,XR,blockchain,digital twin,multimedia communication,VR,MR,traffic patterns,network performance"
Visual Highlighting for Situated Brushing and Linking,"Brushing and linking is widely used for visual analytics in desktop
environments. However, using this approach to link many data items between
situated (e.g., a virtual screen with data) and embedded views (e.g.,
highlighted objects in the physical environment) is largely unexplored. To this
end, we study the effectiveness of visual highlighting techniques in helping
users identify and link physical referents to brushed data marks in a situated
scatterplot. In an exploratory virtual reality user study (N=20), we evaluated
four highlighting techniques under different physical layouts and tasks. We
discuss the effectiveness of these techniques, as well as implications for the
design of brushing and linking operations in situated analytics.","['Nina Doerr', 'Benjamin Lee', 'Katarina Baricova', 'Dieter Schmalstieg', 'Michael Sedlmair']",2024-03-22T16:17:51Z,http://arxiv.org/abs/2403.15321v2,['cs.HC'],"visual analytics,brushing and linking,situated views,physical referents,scatterplot,virtual reality,user study,highlighting techniques,physical layouts,design implications"
"Designing Upper-Body Gesture Interaction with and for People with Spinal
  Muscular Atrophy in VR","Recent research proposed gaze-assisted gestures to enhance interaction within
virtual reality (VR), providing opportunities for people with motor impairments
to experience VR. Compared to people with other motor impairments, those with
Spinal Muscular Atrophy (SMA) exhibit enhanced distal limb mobility, providing
them with more design space. However, it remains unknown what gaze-assisted
upper-body gestures people with SMA would want and be able to perform. We
conducted an elicitation study in which 12 VR-experienced people with SMA
designed upper-body gestures for 26 VR commands, and collected 312 user-defined
gestures. Participants predominantly favored creating gestures with their
hands. The type of tasks and participants' abilities influence their choice of
body parts for gesture design. Participants tended to enhance their body
involvement and preferred gestures that required minimal physical effort, and
were aesthetically pleasing. Our research will contribute to creating better
gesture-based input methods for people with motor impairments to interact with
VR.","['Jingze Tian', 'Yingna Wang', 'Keye Yu', 'Liyi Xu', 'Junan Xie', 'Franklin Mingzhe Li', 'Yafeng Niu', 'Mingming Fan']",2024-03-24T11:50:49Z,http://arxiv.org/abs/2403.16107v1,['cs.HC'],"gesture interaction,spinal muscular atrophy,virtual reality,motor impairments,distal limb mobility,upper-body gestures,elicitation study,user-defined gestures,gesture design"
A Geometric Perspective on Fusing Gaussian Distributions on Lie Groups,"Stochastic inference on Lie groups plays a key role in state estimation
problems such as; inertial navigation, visual inertial odometry, pose
estimation in virtual reality, etc. A key problem is fusing independent
concentrated Gaussian distributions defined at different reference points on
the group. In this paper we approximate distributions at different points in
the group in a single set of exponential coordinates and then use classical
Gaussian fusion to obtain the fused posteriori in those coordinates. We
consider several approximations including the exact Jacobian of the change of
coordinate map, first and second order Taylor's expansions of the Jacobian, and
parallel transport with and without curvature correction associated with the
underlying geometry of the Lie group. Preliminary results on SO(3) demonstrate
that a novel approximation using parallel transport with curvature correction
achieves similar accuracy to the state-of-the-art optimisation based algorithms
at a fraction of the computational cost.","['Yixiao Ge', 'Pieter van Goor', 'Robert Mahony']",2024-03-25T04:11:52Z,http://arxiv.org/abs/2403.16411v2,"['eess.SY', 'cs.SY']","Lie groups,Gaussian distributions,fusion,exponential coordinates,Jacobian,Taylor's expansions,parallel transport,curvature correction,SO(3),state estimation"
Low-Latency Neural Stereo Streaming,"The rise of new video modalities like virtual reality or autonomous driving
has increased the demand for efficient multi-view video compression methods,
both in terms of rate-distortion (R-D) performance and in terms of delay and
runtime. While most recent stereo video compression approaches have shown
promising performance, they compress left and right views sequentially, leading
to poor parallelization and runtime performance. This work presents Low-Latency
neural codec for Stereo video Streaming (LLSS), a novel parallel stereo video
coding method designed for fast and efficient low-latency stereo video
streaming. Instead of using a sequential cross-view motion compensation like
existing methods, LLSS introduces a bidirectional feature shifting module to
directly exploit mutual information among views and encode them effectively
with a joint cross-view prior model for entropy coding. Thanks to this design,
LLSS processes left and right views in parallel, minimizing latency; all while
substantially improving R-D performance compared to both existing neural and
conventional codecs.","['Qiqi Hou', 'Farzad Farhadzadeh', 'Amir Said', 'Guillaume Sautiere', 'Hoang Le']",2024-03-26T17:11:51Z,http://arxiv.org/abs/2403.17879v1,"['cs.CV', 'eess.IV']","neural,stereo,streaming,video compression,low-latency,parallelization,codec,motion compensation,entropy coding,R-D performance"
"Neighbor-Environment Observer: An Intelligent Agent for Immersive
  Working Companionship","Human-computer symbiosis is a crucial direction for the development of
artificial intelligence. As intelligent systems become increasingly prevalent
in our work and personal lives, it is important to develop strategies to
support users across physical and virtual environments. While technological
advances in personal digital devices, such as personal computers and virtual
reality devices, can provide immersive experiences, they can also disrupt
users' awareness of their surroundings and enhance the frustration caused by
disturbances. In this paper, we propose a joint observation strategy for
artificial agents to support users across virtual and physical environments. We
introduce a prototype system, neighbor-environment observer (NEO), that
utilizes non-invasive sensors to assist users in dealing with disruptions to
their immersive experience. System experiments evaluate NEO from different
perspectives and demonstrate the effectiveness of the joint observation
strategy. A user study is conducted to evaluate its usability. The results show
that NEO could lessen users' workload with the learned user preference. We
suggest that the proposed strategy can be applied to various smart home
scenarios.","['Zhe Sun', 'Qixuan Liang', 'Meng Wang', 'Zhenliang Zhang']",2024-03-27T08:11:45Z,http://arxiv.org/abs/2403.18331v1,['cs.HC'],"artificial intelligence,intelligent systems,immersive experiences,virtual reality devices,non-invasive sensors,user study,smart home scenarios"
"Thelxinoë: Recognizing Human Emotions Using Pupillometry and Machine
  Learning","In this study, we present a method for emotion recognition in Virtual Reality
(VR) using pupillometry. We analyze pupil diameter responses to both visual and
auditory stimuli via a VR headset and focus on extracting key features in the
time-domain, frequency-domain, and time-frequency domain from VR generated
data. Our approach utilizes feature selection to identify the most impactful
features using Maximum Relevance Minimum Redundancy (mRMR). By applying a
Gradient Boosting model, an ensemble learning technique using stacked decision
trees, we achieve an accuracy of 98.8% with feature engineering, compared to
84.9% without it. This research contributes significantly to the Thelxino\""e
framework, aiming to enhance VR experiences by integrating multiple sensor data
for realistic and emotionally resonant touch interactions. Our findings open
new avenues for developing more immersive and interactive VR environments,
paving the way for future advancements in virtual touch technology.","['Darlene Barker', 'Haim Levkowitz']",2024-03-27T21:14:17Z,http://arxiv.org/abs/2403.19014v1,"['cs.LG', 'cs.HC']","pupillometry,machine learning,emotion recognition,Virtual Reality,feature selection,time-domain,frequency-domain,time-frequency domain,Gradient Boosting,sensor data"
"Using Deep Learning to Increase Eye-Tracking Robustness, Accuracy, and
  Precision in Virtual Reality","Algorithms for the estimation of gaze direction from mobile and video-based
eye trackers typically involve tracking a feature of the eye that moves through
the eye camera image in a way that covaries with the shifting gaze direction,
such as the center or boundaries of the pupil. Tracking these features using
traditional computer vision techniques can be difficult due to partial
occlusion and environmental reflections. Although recent efforts to use machine
learning (ML) for pupil tracking have demonstrated superior results when
evaluated using standard measures of segmentation performance, little is known
of how these networks may affect the quality of the final gaze estimate. This
work provides an objective assessment of the impact of several contemporary
ML-based methods for eye feature tracking when the subsequent gaze estimate is
produced using either feature-based or model-based methods. Metrics include the
accuracy and precision of the gaze estimate, as well as drop-out rate.","['Kevin Barkevich', 'Reynold Bailey', 'Gabriel J. Diaz']",2024-03-28T18:43:25Z,http://arxiv.org/abs/2403.19768v1,['cs.CV'],"deep learning,eye-tracking,robustness,accuracy,precision,virtual reality,machine learning,segmentation,feature tracking,gaze estimate"
"Hierarchical Deep Learning for Intention Estimation of Teleoperation
  Manipulation in Assembly Tasks","In human-robot collaboration, shared control presents an opportunity to
teleoperate robotic manipulation to improve the efficiency of manufacturing and
assembly processes. Robots are expected to assist in executing the user's
intentions. To this end, robust and prompt intention estimation is needed,
relying on behavioral observations. The framework presents an intention
estimation technique at hierarchical levels i.e., low-level actions and
high-level tasks, by incorporating multi-scale hierarchical information in
neural networks. Technically, we employ hierarchical dependency loss to boost
overall accuracy. Furthermore, we propose a multi-window method that assigns
proper hierarchical prediction windows of input data. An analysis of the
predictive power with various inputs demonstrates the predominance of the deep
hierarchical model in the sense of prediction accuracy and early intention
identification. We implement the algorithm on a virtual reality (VR) setup to
teleoperate robotic hands in a simulation with various assembly tasks to show
the effectiveness of online estimation.","['Mingyu Cai', 'Karankumar Patel', 'Soshi Iba', 'Songpo Li']",2024-03-28T18:45:43Z,http://arxiv.org/abs/2403.19770v1,"['cs.RO', 'cs.AI', 'cs.LG']","deep learning,intention estimation,teleoperation,manipulation,assembly tasks,hierarchical levels,neural networks,hierarchical dependency loss,prediction accuracy"
"Enhancing Empathy in Virtual Reality: An Embodied Approach to Mindset
  Modulation","A growth mindset has shown promising outcomes for increasing empathy ability.
However, stimulating a growth mindset in VR-based empathy interventions is
under-explored. In the present study, we implemented prosocial VR content, Our
Neighbor Hero, focusing on embodying a virtual character to modulate players'
mindsets. The virtual body served as a stepping stone, enabling players to
identify with the character and cultivate a growth mindset as they followed
mission instructions. We considered several implementation factors to assist
players in positioning within the VR experience, including positive feedback,
content difficulty, background lighting, and multimodal feedback. We conducted
an experiment to investigate the intervention's effectiveness in increasing
empathy. Our findings revealed that the VR content and mindset training
encouraged participants to improve their growth mindsets and empathic motives.
This VR content was developed for college students to enhance their empathy and
teamwork skills. It has the potential to improve collaboration in
organizational and community environments.","['Seoyeon Bae', 'Yoon Kyung Lee', 'Jungcheol Lee', 'Jaeheon Kim', 'Haeseong Jeon', 'Seung-Hwan Lim', 'Byung-Cheol Kim', 'Sowon Hahn']",2024-03-30T09:29:23Z,http://arxiv.org/abs/2404.00300v1,['cs.HC'],"Empathy,Virtual Reality,Embodied Approach,Mindset Modulation,Growth Mindset,Prosocial VR Content,Virtual Body,Implementation Factors,Multimodal Feedback,Empathy Training"
"AIGCOIQA2024: Perceptual Quality Assessment of AI Generated
  Omnidirectional Images","In recent years, the rapid advancement of Artificial Intelligence Generated
Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated
omnidirectional images hold significant potential for Virtual Reality (VR) and
Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have
also been widely studied. AI-generated omnidirectional images exhibit unique
distortions compared to natural omnidirectional images, however, there is no
dedicated Image Quality Assessment (IQA) criteria for assessing them. This
study addresses this gap by establishing a large-scale AI generated
omnidirectional image IQA database named AIGCOIQA2024 and constructing a
comprehensive benchmark. We first generate 300 omnidirectional images based on
5 AIGC models utilizing 25 text prompts. A subjective IQA experiment is
conducted subsequently to assess human visual preferences from three
perspectives including quality, comfortability, and correspondence. Finally, we
conduct a benchmark experiment to evaluate the performance of state-of-the-art
IQA models on our database. The database will be released to facilitate future
research.","['Liu Yang', 'Huiyu Duan', 'Long Teng', 'Yucheng Zhu', 'Xiaohong Liu', 'Menghan Hu', 'Xiongkuo Min', 'Guangtao Zhai', 'Patrick Le Callet']",2024-04-01T10:08:23Z,http://arxiv.org/abs/2404.01024v1,"['cs.CV', 'eess.IV']","Artificial Intelligence Generated Content,Omnidirectional Images,Image Quality Assessment,Virtual Reality,Augmented Reality,AI Models,IQA Database,Benchmark,State-of-the-Art Models"
"Generative AI for Immersive Communication: The Next Frontier in
  Internet-of-Senses Through 6G","Over the past two decades, the Internet-of-Things (IoT) has been a
transformative concept, and as we approach 2030, a new paradigm known as the
Internet of Senses (IoS) is emerging. Unlike conventional Virtual Reality (VR),
IoS seeks to provide multi-sensory experiences, acknowledging that in our
physical reality, our perception extends far beyond just sight and sound; it
encompasses a range of senses. This article explores existing technologies
driving immersive multi-sensory media, delving into their capabilities and
potential applications. This exploration includes a comparative analysis
between conventional immersive media streaming and a proposed use case that
leverages semantic communication empowered by generative Artificial
Intelligence (AI). The focal point of this analysis is the substantial
reduction in bandwidth consumption by 99.93% in the proposed scheme. Through
this comparison, we aim to underscore the practical applications of generative
AI for immersive media while addressing the challenges and outlining future
trajectories.","['Nassim Sehad', 'Lina Bariah', 'Wassim Hamidouche', 'Hamed Hellaoui', 'Riku Jäntti', 'Mérouane Debbah']",2024-04-02T07:57:05Z,http://arxiv.org/abs/2404.01713v1,"['cs.CL', 'cs.AI', 'cs.HC', 'cs.MM', 'cs.NI']","Generative AI,Immersive Communication,Internet-of-Senses,6G,Multi-sensory experiences,Virtual Reality,Semantic communication,Bandwidth consumption,Artificial Intelligence,Immersive media."
3D scene generation from scene graphs and self-attention,"Synthesizing realistic and diverse indoor 3D scene layouts in a controllable
fashion opens up applications in simulated navigation and virtual reality. As
concise and robust representations of a scene, scene graphs have proven to be
well-suited as the semantic control on the generated layout. We present a
variant of the conditional variational autoencoder (cVAE) model to synthesize
3D scenes from scene graphs and floor plans. We exploit the properties of
self-attention layers to capture high-level relationships between objects in a
scene, and use these as the building blocks of our model. Our model, leverages
graph transformers to estimate the size, dimension and orientation of the
objects in a room while satisfying relationships in the given scene graph. Our
experiments shows self-attention layers leads to sparser (7.9x compared to
Graphto3D) and more diverse scenes (16%).","['Pietro Bonazzi', 'Mengqi Wang', 'Diego Martin Arroyo', 'Fabian Manhardt', 'Nico Messikomer', 'Federico Tombari', 'Davide Scaramuzza']",2024-04-02T12:26:17Z,http://arxiv.org/abs/2404.01887v3,['cs.CV'],"3D scene generation,scene graphs,self-attention,conditional variational autoencoder,floor plans,graph transformers,relationships,objects,sparsity,diversity"
"A Change of Scenery: Transformative Insights from Retrospective VR
  Embodied Perspective-Taking of Conflict With a Close Other","Close relationships are irreplaceable social resources, yet prone to
high-risk conflict. Building on findings from the fields of HCI, virtual
reality, and behavioral therapy, we evaluate the unexplored potential of
retrospective VR-embodied perspective-taking to fundamentally influence
conflict resolution in close others. We develop a biographically-accurate
Retrospective Embodied Perspective-Taking system (REPT) and conduct a
mixed-methods evaluation of its influence on close others' reflection and
communication, compared to video-based reflection methods currently used in
therapy (treatment as usual, or TAU). Our key findings provide evidence that
REPT was able to significantly improve communication skills and positive
sentiment of both partners during conflict, over TAU. The qualitative data also
indicated that REPT surpassed basic perspective-taking by exclusively
stimulating users to embody and reflect on both their own and their partner's
experiences at the same level. In light of these findings, we provide
implications and an agenda for social embodiment in HCI design: conceptualizing
the use of `embodied social cognition,' and envisioning socially-embodied
experiences as an interactive context.","['Seraphina Yong', 'Leo Cui', 'Evan Suma Rosenberg', 'Svetlana Yarosh']",2024-04-02T20:06:19Z,http://arxiv.org/abs/2404.02277v1,['cs.HC'],"HCI,virtual reality,behavioral therapy,conflict resolution,perspective-taking,mixed-methods evaluation,communication skills,social embodiment,embodied social cognition"
"Fusion of Mixture of Experts and Generative Artificial Intelligence in
  Mobile Edge Metaverse","In the digital transformation era, Metaverse offers a fusion of virtual
reality (VR), augmented reality (AR), and web technologies to create immersive
digital experiences. However, the evolution of the Metaverse is slowed down by
the challenges of content creation, scalability, and dynamic user interaction.
Our study investigates an integration of Mixture of Experts (MoE) models with
Generative Artificial Intelligence (GAI) for mobile edge computing to
revolutionize content creation and interaction in the Metaverse. Specifically,
we harness an MoE model's ability to efficiently manage complex data and
complex tasks by dynamically selecting the most relevant experts running
various sub-models to enhance the capabilities of GAI. We then present a novel
framework that improves video content generation quality and consistency, and
demonstrate its application through case studies. Our findings underscore the
efficacy of MoE and GAI integration to redefine virtual experiences by offering
a scalable, efficient pathway to harvest the Metaverse's full potential.","['Guangyuan Liu', 'Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Abbas Jamalipour', 'Shiwen Mao', 'Dong In Kim']",2024-04-04T09:37:59Z,http://arxiv.org/abs/2404.03321v1,['cs.NI'],"Fusion,Mixture of Experts,Generative Artificial Intelligence,Mobile Edge,Metaverse,Virtual Reality,Augmented Reality,Scalability,Content Creation,Interaction"
"A Realistic Surgical Simulator for Non-Rigid and Contact-Rich
  Manipulation in Surgeries with the da Vinci Research Kit","Realistic real-time surgical simulators play an increasingly important role
in surgical robotics research, such as surgical robot learning and automation,
and surgical skills assessment. Although there are a number of existing
surgical simulators for research, they generally lack the ability to simulate
the diverse types of objects and contact-rich manipulation tasks typically
present in surgeries, such as tissue cutting and blood suction. In this work,
we introduce CRESSim, a realistic surgical simulator based on PhysX 5 for the
da Vinci Research Kit (dVRK) that enables simulating various contact-rich
surgical tasks involving different surgical instruments, soft tissue, and body
fluids. The real-world dVRK console and the master tool manipulator (MTM)
robots are incorporated into the system to allow for teleoperation through
virtual reality (VR). To showcase the advantages and potentials of the
simulator, we present three examples of surgical tasks, including tissue
grasping and deformation, blood suction, and tissue cutting. These tasks are
performed using the simulated surgical instruments, including the large needle
driver, suction irrigator, and curved scissor, through VR-based teleoperation.","['Yafei Ou', 'Sadra Zargarzadeh', 'Paniz Sedighi', 'Mahdi Tavakoli']",2024-04-08T22:01:28Z,http://arxiv.org/abs/2404.05888v1,['cs.RO'],"surgical simulator,non-rigid,contact-rich manipulation,surgeries,da Vinci Research Kit,PhysX 5,dVRK,surgical instruments,soft tissue,virtual reality (VR)"
HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields,"In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.","['Arnab Dey', 'Di Yang', 'Antitza Dantcheva', 'Jean Martinet']",2024-04-09T09:23:04Z,http://arxiv.org/abs/2404.06152v1,"['cs.CV', 'cs.AI']","Neural Radiance Fields,Human Biomechanic Features,Image Encoder,Skeleton Estimation,Neural Rendering,Volume Rendering,Feature Maps,Heatmaps,Augmented Reality,Virtual Reality"
"Shifting the Paradigm: Estimating Heterogeneous Treatment Effects in the
  Development of Walkable Cities Design","The transformation of urban environments to accommodate growing populations
has profoundly impacted public health and well-being. This paper addresses the
critical challenge of estimating the impact of urban design interventions on
diverse populations. Traditional approaches, reliant on questionnaires and
stated preference techniques, are limited by recall bias and capturing the
complex dynamics between environmental attributes and individual
characteristics. To address these challenges, we integrate Virtual Reality (VR)
with observational causal inference methods to estimate heterogeneous treatment
effects, specifically employing Targeted Maximum Likelihood Estimation (TMLE)
for its robustness against model misspecification. Our innovative approach
leverages VR-based experiment to collect data that reflects perceptual and
experiential factors. The result shows the heterogeneous impacts of urban
design elements on public health and underscore the necessity for personalized
urban design interventions. This study not only extends the application of TMLE
to built environment research but also informs public health policy by
illuminating the nuanced effects of urban design on mental well-being and
advocating for tailored strategies that foster equitable, health-promoting
urban spaces.","['Jie Zhu', 'Bojing Liao']",2024-04-12T02:42:44Z,http://arxiv.org/abs/2404.08208v2,"['stat.AP', 'stat.ME']","urban design,heterogeneous treatment effects,observational causal inference,Virtual Reality (VR),Targeted Maximum Likelihood Estimation (TMLE),public health,urban environments,personalized interventions,well-being,built environment"
"Bridging the Gap: Advancements in Technology to Support Dementia Care --
  A Scoping Review","Dementia has serious consequences for the daily life of the person affected
due to the decline in the their cognitive, behavioral and functional abilities.
Caring for people living with dementia can be challenging and distressing.
Innovative solutions are becoming essential to enrich the lives of those
impacted and alleviate caregiver burdens. This scoping review, spanning
literature from 2010 to July 2023 in the field of Human-Computer Interaction
(HCI), offers a comprehensive look at how interactive technology contributes to
dementia care. Emphasizing technology's role in addressing the unique needs of
people with dementia (PwD) and their caregivers, this review encompasses
assistive devices, mobile applications, sensors, and GPS tracking. Delving into
challenges encountered in clinical and home-care settings, it succinctly
outlines the influence of cutting-edge technologies, such as wearables, virtual
reality, robots, and artificial intelligence, in supporting individuals with
dementia and their caregivers. We categorize current dementia-related
technologies into six groups based on their intended use and function: 1) daily
life monitoring, 2) daily life support, 3) social interaction and
communication, 4) well-being enhancement, 5) cognitive support, and 6)
caregiver support.","['Yong Ma', 'Oda Elise Nordberg', 'Jessica Hubbers', 'Yuchong Zhang', 'Arvid Rongve', 'Miroslav Bachinski', 'Morten Fjeld']",2024-04-15T11:36:58Z,http://arxiv.org/abs/2404.09685v1,['cs.HC'],"technology,dementia care,Human-Computer Interaction,interactive technology,assistive devices,mobile applications,sensors,GPS tracking,wearables,artificial intelligence"
A Calibrated and Automated Simulator for Innovations in 5G,"The rise of 5G deployments has created the environment for many emerging
technologies to flourish. Self-driving vehicles, Augmented and Virtual Reality,
and remote operations are examples of applications that leverage 5G networks'
support for extremely low latency, high bandwidth, and increased throughput.
However, the complex architecture of 5G hinders innovation due to the lack of
accessibility to testbeds or realistic simulators with adequate 5G
functionalities. Also, configuring and managing simulators are complex and time
consuming. Finally, the lack of adequate representative data hinders the
data-driven designs in 5G campaigns. Thus, we calibrated a system-level
open-source simulator, Simu5G, following 3GPP guidelines to enable faster
innovation in the 5G domain. Furthermore, we developed an API for automatic
simulator configuration without knowing the underlying architectural details.
Finally, we demonstrate the usage of the calibrated and automated simulator by
developing an ML-based anomaly detection in a 5G Radio Access Network (RAN).","['Conrado Boeira', 'Antor Hasan', 'Khaleda Papry', 'Yue Ju', 'Zhongwen Zhu', 'Israat Haque']",2024-04-16T15:17:23Z,http://arxiv.org/abs/2404.10643v1,"['cs.NI', 'eess.SP']","5G,Simulator,Innovations,Self-driving vehicles,Augmented Reality,Virtual Reality,Latency,Bandwidth,Throughput,API"
"Novel View Synthesis for Cinematic Anatomy on Mobile and Immersive
  Displays","Interactive photorealistic visualization of 3D anatomy (i.e., Cinematic
Anatomy) is used in medical education to explain the structure of the human
body. It is currently restricted to frontal teaching scenarios, where the
demonstrator needs a powerful GPU and high-speed access to a large storage
device where the dataset is hosted. We demonstrate the use of novel view
synthesis via compressed 3D Gaussian splatting to overcome this restriction and
to enable students to perform cinematic anatomy on lightweight mobile devices
and in virtual reality environments. We present an automatic approach for
finding a set of images that captures all potentially seen structures in the
data. By mixing closeup views with images from a distance, the splat
representation can recover structures up to the voxel resolution. The use of
Mip-Splatting enables smooth transitions when the focal length is increased.
Even for GB datasets, the final renderable representation can usually be
compressed to less than 70 MB, enabling interactive rendering on low-end
devices using rasterization.","['Simon Niedermayr', 'Christoph Neuhauser', 'Kaloian Petkov', 'Klaus Engel', 'Rüdiger Westermann']",2024-04-17T11:49:43Z,http://arxiv.org/abs/2404.11285v1,['cs.GR'],"photorealistic visualization,3D anatomy,Gaussian splatting,novel view synthesis,virtual reality,Mip-Splatting,interactive rendering,low-end devices,voxel resolution,compressed rendering"
"Establishing a Baseline for Gaze-driven Authentication Performance in
  VR: A Breadth-First Investigation on a Very Large Dataset","This paper performs the crucial work of establishing a baseline for
gaze-driven authentication performance to begin answering fundamental research
questions using a very large dataset of gaze recordings from 9202 people with a
level of eye tracking (ET) signal quality equivalent to modern consumer-facing
virtual reality (VR) platforms. The size of the employed dataset is at least an
order-of-magnitude larger than any other dataset from previous related work.
Binocular estimates of the optical and visual axes of the eyes and a minimum
duration for enrollment and verification are required for our model to achieve
a false rejection rate (FRR) of below 3% at a false acceptance rate (FAR) of 1
in 50,000. In terms of identification accuracy which decreases with gallery
size, we estimate that our model would fall below chance-level accuracy for
gallery sizes of 148,000 or more. Our major findings indicate that gaze
authentication can be as accurate as required by the FIDO standard when driven
by a state-of-the-art machine learning architecture and a sufficiently large
training dataset.","['Dillon Lohr', 'Michael J. Proulx', 'Oleg Komogortsev']",2024-04-17T23:33:34Z,http://arxiv.org/abs/2404.11798v1,"['cs.CV', 'cs.HC']","gaze-driven authentication,performance,VR,dataset,eye tracking,signal quality,false rejection rate,false acceptance rate,identification accuracy,machine learning architecture"
"Using Capability Maps Tailored to Arm Range of Motion in VR Exergames
  for Rehabilitation","Many neurological conditions, e.g., a stroke, can cause patients to
experience upper limb (UL) motor impairments that hinder their daily
activities. For such patients, while rehabilitation therapy is key for
regaining autonomy and restoring mobility, its long-term nature entails ongoing
time commitment and it is often not sufficiently engaging. Virtual reality (VR)
can transform rehabilitation therapy into engaging game-like tasks that can be
tailored to patient-specific activities, set goals, and provide rehabilitation
assessment. Yet, most VR systems lack built-in methods to track progress over
time and alter rehabilitation programs accordingly. We propose using arm
kinematic modeling and capability maps to allow a VR system to understand a
user's physical capability and limitation. Next, we suggest two use cases for
the VR system to utilize the user's capability map for tailoring rehabilitation
programs. Finally, for one use case, it is shown that the VR system can
emphasize and assess the use of specific UL joints.","['Christian Lourido', 'Zaid Waghoo', 'Hassam Khan Wazir', 'Nishtha Bhagat', 'Vikram Kapila']",2024-04-18T20:53:47Z,http://arxiv.org/abs/2404.12504v1,"['cs.HC', 'cs.RO']","capability maps,arm range of motion,VR exergames,rehabilitation,neurological conditions,upper limb motor impairments,virtual reality,arm kinematic modeling,limitation,rehabilitation programs"
"Impact of Vibrotactile Triggers on Mental Well-Being through ASMR
  Experience in VR","Watching Autonomous Sensory Meridian Response (ASMR) videos is a popular
approach to support mental well-being, as the triggered ASMR tingling sensation
supports de-stressing and regulating emotions. Therefore, there is increasing
research on how to efficiently trigger ASMR tingling sensation. Tactile
sensation remains unexplored because current popular ASMR approaches focus on
the visual and audio channels. In this study, we explored the impact of tactile
feedback on triggering ASMR tingling sensation in a Virtual Reality (VR)
environment. Through two experimental studies, we investigated the relaxation
effect of a tactile-enabled ASMR experience, as well as the impact of
vibrotactile triggers on the ASMR experience. Our results showed that
vibrotactile feedback is effective in increasing the likelihood of ASMR
tingling sensation and enhancing the feeling of comfort, relaxation, and
enjoyment.","['Danyang Peng', 'Tanner Person', 'Ximing Shen', 'Yun Suen Pai', 'Giulia Barbareschi', 'Shengyin Li', 'Kouta Minamizawa']",2024-04-19T01:19:18Z,http://arxiv.org/abs/2404.12567v1,['cs.HC'],"Vibrotactile triggers,Mental well-being,ASMR experience,Virtual Reality (VR),Tactile sensation"
"CloudFort: Enhancing Robustness of 3D Point Cloud Classification Against
  Backdoor Attacks via Spatial Partitioning and Ensemble Prediction","The increasing adoption of 3D point cloud data in various applications, such
as autonomous vehicles, robotics, and virtual reality, has brought about
significant advancements in object recognition and scene understanding.
However, this progress is accompanied by new security challenges, particularly
in the form of backdoor attacks. These attacks involve inserting malicious
information into the training data of machine learning models, potentially
compromising the model's behavior. In this paper, we propose CloudFort, a novel
defense mechanism designed to enhance the robustness of 3D point cloud
classifiers against backdoor attacks. CloudFort leverages spatial partitioning
and ensemble prediction techniques to effectively mitigate the impact of
backdoor triggers while preserving the model's performance on clean data. We
evaluate the effectiveness of CloudFort through extensive experiments,
demonstrating its strong resilience against the Point Cloud Backdoor Attack
(PCBA). Our results show that CloudFort significantly enhances the security of
3D point cloud classification models without compromising their accuracy on
benign samples. Furthermore, we explore the limitations of CloudFort and
discuss potential avenues for future research in the field of 3D point cloud
security. The proposed defense mechanism represents a significant step towards
ensuring the trustworthiness and reliability of point-cloud-based systems in
real-world applications.","['Wenhao Lan', 'Yijun Yang', 'Haihua Shen', 'Shan Li']",2024-04-22T09:55:50Z,http://arxiv.org/abs/2404.14042v1,['cs.CV'],"3D point cloud,Classification,Backdoor attacks,Spatial partitioning,Ensemble prediction,Machine learning models,Defense mechanism,Point Cloud Backdoor Attack (PCBA),Security,Trustworthiness"
"Quantitative Evaluation of driver's situation awareness in virtual
  driving through Eye tracking analysis","In driving tasks, the driver's situation awareness of the surrounding
scenario is crucial for safety driving. However, current methods of measuring
situation awareness mostly rely on subjective questionnaires, which interrupt
tasks and lack non-intrusive quantification. To address this issue, our study
utilizes objective gaze motion data to provide an interference-free
quantification method for situation awareness. Three quantitative scores are
proposed to represent three different levels of awareness: perception,
comprehension, and projection, and an overall score of situation awareness is
also proposed based on above three scores. To validate our findings, we
conducted experiments where subjects performed driving tasks in a virtual
reality simulated environment. All the four proposed situation awareness scores
have clearly shown a significant correlation with driving performance. The
proposed not only illuminates a new path for understanding and evaluating the
situation awareness but also offers a satisfying proxy for driving performance.","['Yunxiang Jiang', 'Qing Xu', 'Kai Zhen', 'Yu Chen']",2024-04-23T08:02:36Z,http://arxiv.org/abs/2404.14817v2,"['cs.HC', 'cs.GR']","driver's situation awareness,virtual driving,Eye tracking analysis,gaze motion data,perception,comprehension,projection,driving performance,quantitative evaluation"
"BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse
  Sampling","Eye tracking is becoming an increasingly important task domain in emerging
computing platforms such as Augmented/Virtual Reality (AR/VR). Today's eye
tracking system suffers from long end-to-end tracking latency and can easily
eat up half of the power budget of a mobile VR device. Most existing
optimization efforts exclusively focus on the computation pipeline by
optimizing the algorithm and/or designing dedicated accelerators while largely
ignoring the front-end of any eye tracking pipeline: the image sensor. This
paper makes a case for co-designing the imaging system with the computing
system. In particular, we propose the notion of ""in-sensor sparse sampling"",
whereby the pixels are drastically downsampled (by 20x) within the sensor. Such
in-sensor sampling enhances the overall tracking efficiency by significantly
reducing 1) the power consumption of the sensor readout chain and sensor-host
communication interfaces, two major power contributors, and 2) the work done on
the host, which receives and operates on far fewer pixels. With careful reuse
of existing pixel circuitry, our proposed BLISSCAM requires little hardware
augmentation to support the in-sensor operations. Our synthesis results show up
to 8.2x energy reduction and 1.4x latency reduction over existing eye tracking
pipelines.","['Yu Feng', 'Tianrui Ma', 'Yuhao Zhu', 'Xuan Zhang']",2024-04-24T08:41:35Z,http://arxiv.org/abs/2404.15733v1,['cs.AR'],"eye tracking,in-sensor sparse sampling,imaging system,computation pipeline,power consumption,sensor readout chain,sensor-host communication interfaces,hardware augmentation,energy reduction,latency reduction"
"Meta-Object: Interactive and Multisensory Virtual Object Learned from
  the Real World for the Post-Metaverse","With the proliferation of wearable Augmented Reality/Virtual Reality (AR/VR)
devices, ubiquitous virtual experiences seamlessly integrate into daily life
through metaverse platforms. To support immersive metaverse experiences akin to
reality, we propose a next-generation virtual object, a meta-object, a
property-embedded virtual object that contains interactive and multisensory
characteristics learned from the real world. Current virtual objects differ
significantly from real-world objects due to restricted sensory feedback based
on limited physical properties. To leverage meta-objects in the metaverse,
three key components are needed: meta-object modeling and property embedding,
interaction-adaptive multisensory feedback, and an intelligence
simulation-based post-metaverse platform. Utilizing meta-objects that enable
both on-site and remote users to interact as if they were engaging with real
objects could contribute to the advent of the post-metaverse era through
wearable AR/VR devices.","['Dooyoung Kim', 'Taewook Ha', 'Jinseok Hong', 'Seonji Kim', 'Selin Choi', 'Heejeong Ko', 'Woontack Woo']",2024-04-26T06:22:21Z,http://arxiv.org/abs/2404.17179v2,"['cs.HC', 'cs.ET']","Meta-object,Virtual object,Multisensory feedback,Metaverse,Augmented Reality,Virtual Reality,Interaction,Property embedding,Simulation,Wearable devices"
Exploring Vulnerabilities in Remote VR User Studies,"This position paper explores the possibilities and challenges of using
Virtual Reality (VR) in remote user studies. Highlighting the immersive nature
of VR, the paper identifies key vulnerabilities, including varying technical
proficiency, privacy concerns, ethical considerations, and data security risks.
To address these issues, proposed mitigation strategies encompass comprehensive
onboarding, prioritized informed consent, implementing privacy-by-design
principles, and adherence to ethical guidelines. Secure data handling,
including encryption and disposal protocols, is advocated. In conclusion, while
remote VR studies present unique opportunities, carefully considering and
implementing mitigation strategies is essential to uphold reliability, ethical
integrity, and security, ensuring responsible and effective use of VR in user
research. Ongoing efforts are crucial for adapting to the evolving landscape of
VR technology in user studies.","['Viktorija Paneva', 'Florian Alt']",2024-04-17T11:47:35Z,http://arxiv.org/abs/2404.17588v1,['cs.HC'],"Virtual Reality,User Studies,Vulnerabilities,Technical Proficiency,Privacy Concerns,Ethical Considerations,Data Security Risks,Mitigation Strategies,Encryption,Privacy-by-Design"
"VisAnywhere: Developing Multi-platform Scientific Visualization
  Applications","Scientists often explore and analyze large-scale scientific simulation data
by leveraging two- and three-dimensional visualizations. The data and tasks can
be complex and therefore best supported using myriad display technologies, from
mobile devices to large high-resolution display walls to virtual reality
headsets. Using a simulation of neuron connections in the human brain, we
present our work leveraging various web technologies to create a multi-platform
scientific visualization application. Users can spread visualization and
interaction across multiple devices to support flexible user interfaces and
both co-located and remote collaboration. Drawing inspiration from responsive
web design principles, this work demonstrates that a single codebase can be
adapted to develop scientific visualization applications that operate
everywhere.","['Thomas Marrinan', 'Madeleine Moeller', 'Alina Kanayinkal', 'Victor A. Mateevitsi', 'Michael E. Papka']",2024-04-26T13:09:16Z,http://arxiv.org/abs/2404.17619v1,"['cs.HC', 'cs.GR']","scientific visualization,multi-platform,simulation data,display technologies,web technologies,neuron connections,flexible user interfaces,collaboration,responsive web design,codebase"
"Integrating Visuo-tactile Sensing with Haptic Feedback for Teleoperated
  Robot Manipulation","Telerobotics enables humans to overcome spatial constraints and allows them
to physically interact with the environment in remote locations. However, the
sensory feedback provided by the system to the operator is often purely visual,
limiting the operator's dexterity in manipulation tasks. In this work, we
address this issue by equipping the robot's end-effector with high-resolution
visuotactile GelSight sensors. Using low-cost MANUS-Gloves, we provide the
operator with haptic feedback about forces acting at the points of contact in
the form of vibration signals. We propose two different methods for estimating
these forces; one based on estimating the movement of markers on the sensor
surface and one deep-learning approach. Additionally, we integrate our system
into a virtual-reality teleoperation pipeline in which a human operator
controls both arms of a Tiago robot while receiving visual and haptic feedback.
We believe that integrating haptic feedback is a crucial step for dexterous
manipulation in teleoperated robotic systems.","['Noah Becker', 'Erik Gattung', 'Kay Hansel', 'Tim Schneider', 'Yaonan Zhu', 'Yasuhisa Hasegawa', 'Jan Peters']",2024-04-30T14:22:33Z,http://arxiv.org/abs/2404.19585v1,['cs.RO'],"Visuo-tactile sensing,Haptic feedback,Teleoperated robot manipulation,Telerobotics,GelSight sensors,MANUS-Gloves,Deep learning,Virtual reality,Teleoperation,Dexterous manipulation"
"A deep causal inference model for fully-interpretable travel behaviour
  analysis","Transport policy assessment often involves causal questions, yet the causal
inference capabilities of traditional travel behavioural models are at best
limited. We present the deep CAusal infeRence mOdel for traveL behavIour
aNAlysis (CAROLINA), a framework that explicitly models causality in travel
behaviour, enhances predictive accuracy, and maintains interpretability by
leveraging causal inference, deep learning, and traditional discrete choice
modelling. Within this framework, we introduce a Generative Counterfactual
model for forecasting human behaviour by adapting the Normalizing Flow method.
Through the case studies of virtual reality-based pedestrian crossing
behaviour, revealed preference travel behaviour from London, and synthetic
data, we demonstrate the effectiveness of our proposed models in uncovering
causal relationships, prediction accuracy, and assessing policy interventions.
Our results show that intervention mechanisms that can reduce pedestrian stress
levels lead to a 38.5% increase in individuals experiencing shorter waiting
times. Reducing the travel distances in London results in a 47% increase in
sustainable travel modes.","['Kimia Kamal', 'Bilal Farooq']",2024-05-02T20:06:06Z,http://arxiv.org/abs/2405.01708v1,['cs.LG'],"causal inference,travel behaviour,deep learning,discrete choice modelling,Generative Counterfactual model,Normalizing Flow method,pedestrian crossing behaviour,revealed preference,synthetic data"
"Perception in Pixels: Understanding Avatar Representation in
  Video-Mediated Collaborative Interactions","Despite the abundance of research concerning virtual reality (VR) avatars,
the impact of screen-based or augmented reality (AR) avatars for real-world
applications remain relatively unexplored. Notably, there is a lack of research
examining video-mediated collaborative interaction experiences using AR avatars
for goal-directed group activities. This study bridges this gap with a
mixed-methods, quasi-experimental user study that investigates video-based
small-group interactions when employing AR avatars as opposed to traditional
video for user representation. We found that the use of avatars positively
influenced self-esteem and video-based collaboration satisfaction. In addition,
our group interview findings highlight experiences and perceptions regarding
the dynamic use of avatars in video-mediated collaborative interactions,
including benefits, challenges, and factors that would influence a decision to
use avatars. This study contributes an empirical understanding of avatar
representation in mediating video-based collaborative interactions,
implications and perceptions surrounding the adoption of AR avatars, and a
comprehensive comparison of key characteristics between user representations.","['Pitch Sinlapanuntakul', 'Mark Zachry']",2024-05-06T20:48:37Z,http://arxiv.org/abs/2405.03844v1,"['cs.HC', 'H.5.1; H.5.3; J.4']","avatar representation,virtual reality,augmented reality,video-mediated collaborative interactions,user study,small-group interactions,self-esteem,collaboration satisfaction,group interview,user representation"
Adversary-Guided Motion Retargeting for Skeleton Anonymization,"Skeleton-based motion visualization is a rising field in computer vision,
especially in the case of virtual reality (VR). With further advancements in
human-pose estimation and skeleton extracting sensors, more and more
applications that utilize skeleton data have come about. These skeletons may
appear to be anonymous but they contain embedded personally identifiable
information (PII). In this paper we present a new anonymization technique that
is based on motion retargeting, utilizing adversary classifiers to further
remove PII embedded in the skeleton. Motion retargeting is effective in
anonymization as it transfers the movement of the user onto the a dummy
skeleton. In doing so, any PII linked to the skeleton will be based on the
dummy skeleton instead of the user we are protecting. We propose a
Privacy-centric Deep Motion Retargeting model (PMR) which aims to further clear
the retargeted skeleton of PII through adversarial learning. In our
experiments, PMR achieves motion retargeting utility performance on par with
state of the art models while also reducing the performance of privacy attacks.","['Thomas Carr', 'Depeng Xu', 'Aidong Lu']",2024-05-08T21:18:02Z,http://arxiv.org/abs/2405.05428v1,"['cs.CV', 'cs.CR', 'cs.LG']","Skeleton-based motion visualization,Virtual reality,Human-pose estimation,Adversary classifiers,Personally identifiable information,Anonymization technique,Motion retargeting,Dummy skeleton,Privacy-centric Deep Motion Retargeting model"
"Expanding Accessibility in Immersive Virtual Spaces: A Comprehensive
  Approach for All Disabilities","In the early stages of the COVID-19 pandemic, many events and conferences
hastily converted to a virtual format, and many commercial ventures promptly
developed tools promising seamless transitions to virtual spaces. In
particular, efforts to expand and monetize augmented and virtual reality
environments increased. While these spaces increased accessibility for some,
others were left behind. In 2024, many events returned to on-site venues, yet
virtual spaces remain central in academic and research communities,
particularly for disabled scholars. As such, in this paper, we advocate for
continued virtual access and improved virtual spaces; we also identify some
potentially overlooked harms in immersive and embodied virtual spaces.","['Cecilia Aragon', 'Melissa Vosen Callens', 'Stacy M. Branham', 'Cali Anicha', 'Brianna Blaser', 'Canan Bilen-Green']",2024-04-23T17:50:11Z,http://arxiv.org/abs/2405.05910v1,['cs.HC'],"immersive virtual spaces,accessibility,disabilities,augmented reality,virtual reality,virtual access,virtual spaces,embodied virtual spaces"
"Risk of Harm in VR Dating from the Perspective of Women and LGBTQIA+
  Stakeholders","Virtual reality (VR) dating introduces novel opportunities for romantic
interactions, but it also raises concerns about new harms that typically occur
separately in traditional dating apps and general-purpose social VR
environments. Given the subjectivity in which VR dating experiences can be
considered harmful it is imperative to involve user stakeholders in
anticipating harms and formulating preventative designs. Towards this goal with
conducted participatory design workshops with 17 stakeholders identified as
women and/or LGBTQIA+; demographics that are at elevated risk of harm in online
dating and social VR. Findings reveal that participants are concerned with two
categories of harm in VR dating: those that occur through the transition of
interaction across virtual and physical modalities, and harms stemming from
expectations of sexual interaction in VR.","['Devin Tebbe', 'Meryem Barkallah', 'Braeden Burger', 'Douglas Zytko']",2024-04-23T17:49:42Z,http://arxiv.org/abs/2405.05914v1,['cs.HC'],"risk of harm,VR dating,women,LGBTQIA+,stakeholders,participatory design workshops,online dating,social VR,virtual modality,sexual interaction"
From Virtual Gains to Real Pains: Potential Harms of Immersive Exergames,"Digitalization and virtualization are parts of our everyday lives in almost
all aspects ranging from work, education, and communication to entertainment. A
novel step in this direction is the widespread interest in extended reality
(XR) [2]. The newest consumer-ready head-mounted displays (HMD) such as Meta
Quest 3 or Apple Vision Pro, have reached unprecedented levels of visual
fidelity, interaction capabilities, and computational power. The built-in
pass-through features of these headsets enable both virtual reality (VR) and
augmented reality (AR) with the same devices. However, the immersive nature of
these experiences is not the only groundbreaking difference from established
forms of media.","['Sebastian Cmentowski', 'Sukran Karaosmanoglu', 'Frank Steinicke']",2024-04-23T17:48:59Z,http://arxiv.org/abs/2405.05915v1,['cs.HC'],"virtualization,extended reality (XR),head-mounted displays (HMD),visual fidelity,interaction capabilities,computational power,pass-through features,virtual reality (VR),augmented reality (AR),immersive nature"
Understanding Emotional Hijacking in Metaverse,"Emotions are an integral part of being human, and experiencing a range of
emotions is what makes life rich and vibrant. From basic emotions like anger,
fear, happiness, and sadness to more complex ones like excitement and grief,
emotions help us express ourselves and connect with the world around us. In
recent years, researchers have begun adopting virtual reality (VR) technology
to evoke emotions as realistically as possible and quantify the strength of
emotions from the electroencephalogram (EEG) signals measured from the brain to
understand human emotions in realistic situations better. This is achieved by
creating a sense of presence in the virtual environment, the feeling that the
user is there. For instance, [6] studied the excitement of a rollercoaster ride
in VR, and [5] studied the fear of navigating in a VR cave.","['Syed Ali Asif', 'Philip Gable', 'Chien-Chung Shen', 'Yan-Ming Chiou']",2024-04-23T17:35:15Z,http://arxiv.org/abs/2405.05929v1,['cs.HC'],"Emotions,Emotional Hijacking,Metaverse,Virtual Reality,Electroencephalogram (EEG),Presence,Rollercoaster,Navigation,Fear,Excitement"
Unveiling the Era of Spatial Computing,"The evolution of User Interfaces marks a significant transition from
traditional command-line interfaces to more intuitive graphical and touch-based
interfaces, largely driven by the emergence of personal computing devices. The
advent of spatial computing and Extended Reality technologies further pushes
the boundaries, promising a fusion of physical and digital realms through
interactive environments. This paper delves into the progression from All
Realities technologies encompassing Augmented Reality, Virtual Reality, and
Mediated Reality to spatial computing, highlighting their conceptual
differences and applications. We explore enabling technologies such as
Artificial Intelligence, the Internet of Things, 5G, cloud and edge computing,
and blockchain that underpin the development of spatial computing. We further
scrutinize the initial forays into commercial spatial computing devices, with a
focus on Apple's Vision Pro, evaluating its technological advancements
alongside the challenges it faces. Through this examination, we aim to provide
insights into the potential of spatial computing to revolutionize our
interaction with digital information and the physical world.",['Hanzhong Cao'],2024-05-11T03:44:45Z,http://arxiv.org/abs/2405.06895v1,['cs.HC'],"spatial computing,Extended Reality,Augmented Reality,Virtual Reality,Mediated Reality,Artificial Intelligence,Internet of Things,5G,cloud computing,blockchain."
The Impact of 2D and 3D Gamified VR on Learning American Sign Language,"Sign language has been extensively studied as a means of facilitating
effective communication between hearing individuals and the deaf community.
With the continuous advancements in virtual reality (VR) and gamification
technologies, an increasing number of studies have begun to explore the
application of these emerging technologies in sign language learning. This
paper describes a user study that compares the impact of 2D and 3D games on the
user experience in ASL learning. Empirical evidence gathered through
questionnaires supports the positive impact of 3D game environments on user
engagement and overall experience, particularly in relation to attractiveness,
usability, and efficiency. Moreover, initial findings demonstrate a similar
behaviour of 2D and 3D games in terms of enhancing user experience. Finally,
the study identifies areas where improvements can be made to enhance the
dependability and clarity of 3D game environments. These findings contribute to
the understanding of how game-based approaches, and specifically the
utilisation of 3D environments, can positively influence the learning
experience of ASL.","['Jindi Wang', 'Ioannis Ivrissimtzis', 'Zhaoxing Li', 'Lei Shi']",2024-05-14T19:00:40Z,http://arxiv.org/abs/2405.08908v1,['cs.HC'],"virtual reality,gamification,American Sign Language,user study,2D games,3D games,user experience,ASL learning,game-based approaches,3D environments"
"Discussing Risks and Benefits in the Future of Hybrid Rehabilitation and
  Fitness in Mixed Reality","In a world where in-person context transitions more into remote and hybrid
concepts, we should consider new concepts of interaction in health and
rehabilitation and what advantages and disadvantages they bring. One of the
rising topics is mixed reality, where we can use the advantages of immersive
3D, 360-degree environments. Meanwhile, physical activity is further decreasing
and with it negative effects increase through sedentary behaviour or wrong and
untrained movements. In this position paper, we discuss these new risks and
potential benefits of mixed reality technology when used for rehabilitation and
fitness. We conclude with suggesting better feedback and guidance for physical
movement and tasks at home. Improving feedback and guidance for participants
could be achieved through using new technologies like virtual reality and
motion tracking.","['Jana Franceska Funke', 'Enrico Rukzio']",2024-05-16T12:49:52Z,http://arxiv.org/abs/2405.10059v1,['cs.HC'],"hybrid rehabilitation,fitness,mixed reality,interaction,3D environments,sedentary behaviour,physical movement,feedback,virtual reality,motion tracking"
"PLASMA -- Platform for Service Management in Digital Remote Maintenance
  Applications","To support maintenance and servicing of industrial machines, service
processes are even today often performed manually and analogously, although
supportive technologies such as augmented reality, virtual reality and digital
platforms already exist. In many cases, neither technicians on-site nor remote
experts have all the essential information and options for suitable actions
available. Existing service products and platforms do not cover all the
required functions in practice in order to map end-to-end processes. PLASMA is
a concept for a Cloud-based remote maintenance platform designed to meet these
demands. But for a real-life implementation of PLASMA, security measures are
essential as we show in this paper.","['Natascha Stumpp', 'Doris Aschenbrenner', 'Manuel Stahl', 'Andreas Aßmuth']",2024-05-20T07:15:41Z,http://arxiv.org/abs/2405.11836v1,['cs.DC'],"PLASMA,Service Management,Digital Remote Maintenance,Augmented Reality,Virtual Reality,Digital Platforms,Cloud-based,Security Measures,Industrial Machines,End-to-end Processes"
"Extended Reality (XR) Codec Adaptation in 5G using Multi-Agent
  Reinforcement Learning with Attention Action Selection","Extended Reality (XR) services will revolutionize applications over 5th and
6th generation wireless networks by providing seamless virtual and augmented
reality experiences. These applications impose significant challenges on
network infrastructure, which can be addressed by machine learning algorithms
due to their adaptability. This paper presents a Multi- Agent Reinforcement
Learning (MARL) solution for optimizing codec parameters of XR traffic,
comparing it to the Adjust Packet Size (APS) algorithm. Our cooperative
multi-agent system uses an Optimistic Mixture of Q-Values (oQMIX) approach for
handling Cloud Gaming (CG), Augmented Reality (AR), and Virtual Reality (VR)
traffic. Enhancements include an attention mechanism and slate-Markov Decision
Process (MDP) for improved action selection. Simulations show our solution
outperforms APS with average gains of 30.1%, 15.6%, 16.5% 50.3% in XR index,
jitter, delay, and Packet Loss Ratio (PLR), respectively. APS tends to increase
throughput but also packet losses, whereas oQMIX reduces PLR, delay, and jitter
while maintaining goodput.","['Pedro Enrique Iturria-Rivera', 'Raimundas Gaigalas', 'Medhat Elsayed', 'Majid Bavand', 'Yigit Ozcan', 'Melike Erol-Kantarci']",2024-05-24T18:34:00Z,http://arxiv.org/abs/2405.15872v1,['cs.NI'],"Extended Reality,XR,5G,Multi-Agent Reinforcement Learning,Attention Mechanism,Codec Parameters,Cloud Gaming,Augmented Reality,Virtual Reality,Q-Values"
"Advancing Behavior Generation in Mobile Robotics through High-Fidelity
  Procedural Simulations","This paper introduces YamaS, a simulator integrating Unity3D Engine with
Robotic Operating System for robot navigation research and aims to facilitate
the development of both Deep Reinforcement Learning (Deep-RL) and Natural
Language Processing (NLP). It supports single and multi-agent configurations
with features like procedural environment generation, RGB vision, and dynamic
obstacle navigation. Unique to YamaS is its ability to construct single and
multi-agent environments, as well as generating agent's behaviour through
textual descriptions. The simulator's fidelity is underscored by comparisons
with the real-world Yamabiko Beego robot, demonstrating high accuracy in sensor
simulations and spatial reasoning. Moreover, YamaS integrates Virtual Reality
(VR) to augment Human-Robot Interaction (HRI) studies, providing an immersive
platform for developers and researchers. This fusion establishes YamaS as a
versatile and valuable tool for the development and testing of autonomous
systems, contributing to the fields of robot simulation and AI-driven training
methodologies.","['Victor A. Kich', 'Jair A. Bottega', 'Raul Steinmetz', 'Ricardo B. Grando', 'Ayanori Yorozu', 'Akihisa Ohya']",2024-05-27T04:31:55Z,http://arxiv.org/abs/2405.16818v1,"['cs.RO', 'cs.HC']","Procedural simulations,Unity3D Engine,Robotic Operating System,Deep Reinforcement Learning,Natural Language Processing,RGB vision,Dynamic obstacle navigation,Virtual Reality,Human-Robot Interaction,Autonomous systems"
"Human4DiT: Free-view Human Video Generation with 4D Diffusion
  Transformer","We present a novel approach for generating high-quality, spatio-temporally
coherent human videos from a single image under arbitrary viewpoints. Our
framework combines the strengths of U-Nets for accurate condition injection and
diffusion transformers for capturing global correlations across viewpoints and
time. The core is a cascaded 4D transformer architecture that factorizes
attention across views, time, and spatial dimensions, enabling efficient
modeling of the 4D space. Precise conditioning is achieved by injecting human
identity, camera parameters, and temporal signals into the respective
transformers. To train this model, we curate a multi-dimensional dataset
spanning images, videos, multi-view data and 3D/4D scans, along with a
multi-dimensional training strategy. Our approach overcomes the limitations of
previous methods based on GAN or UNet-based diffusion models, which struggle
with complex motions and viewpoint changes. Through extensive experiments, we
demonstrate our method's ability to synthesize realistic, coherent and
free-view human videos, paving the way for advanced multimedia applications in
areas such as virtual reality and animation. Our project website is
https://human4dit.github.io.","['Ruizhi Shao', 'Youxin Pang', 'Zerong Zheng', 'Jingxiang Sun', 'Yebin Liu']",2024-05-27T17:53:29Z,http://arxiv.org/abs/2405.17405v1,['cs.CV'],"human video generation,4D diffusion transformer,U-Nets,diffusion transformers,global correlations,viewpoints,time,spatial dimensions,multi-dimensional dataset,GAN-based diffusion models"
"Encouraging Bystander Assistance for Urban Robots: Introducing Playful
  Robot Help-Seeking as a Strategy","Robots in urban environments will inevitably encounter situations beyond
their capabilities (e.g., delivery robots unable to press traffic light
buttons), necessitating bystander assistance. These spontaneous collaborations
possess challenges distinct from traditional human-robot collaboration,
requiring design investigation and tailored interaction strategies. This study
investigates playful help-seeking as a strategy to encourage such bystander
assistance. We compared our designed playful help-seeking concepts against two
existing robot help-seeking strategies: verbal speech and emotional expression.
To assess these strategies and their impact on bystanders' experience and
attitudes towards urban robots, we conducted a virtual reality evaluation study
with 24 participants. Playful help-seeking enhanced people's willingness to
help robots, a tendency more pronounced in scenarios requiring greater physical
effort. Verbal help-seeking was perceived less polite, raising stronger
discomfort assessments. Emotional expression help-seeking elicited empathy
while leading to lower cognitive trust. The triangulation of quantitative and
qualitative results highlights considerations for robot help-seeking from
bystanders.","['Xinyan Yu', 'Marius Hoggenmueller', 'Martin Tomitsch']",2024-05-29T10:06:34Z,http://arxiv.org/abs/2405.18951v1,['cs.HC'],"urban robots,bystander assistance,playful help-seeking,human-robot collaboration,design investigation,interaction strategies,virtual reality evaluation,verbal help-seeking,emotional expression help-seeking,empathy"
"SMPLX-Lite: A Realistic and Drivable Avatar Benchmark with Rich Geometry
  and Texture Annotations","Recovering photorealistic and drivable full-body avatars is crucial for
numerous applications, including virtual reality, 3D games, and tele-presence.
Most methods, whether reconstruction or generation, require large numbers of
human motion sequences and corresponding textured meshes. To easily learn a
drivable avatar, a reasonable parametric body model with unified topology is
paramount. However, existing human body datasets either have images or textured
models and lack parametric models which fit clothes well. We propose a new
parametric model SMPLX-Lite-D, which can fit detailed geometry of the scanned
mesh while maintaining stable geometry in the face, hand and foot regions. We
present SMPLX-Lite dataset, the most comprehensive clothing avatar dataset with
multi-view RGB sequences, keypoints annotations, textured scanned meshes, and
textured SMPLX-Lite-D models. With the SMPLX-Lite dataset, we train a
conditional variational autoencoder model that takes human pose and facial
keypoints as input, and generates a photorealistic drivable human avatar.","['Yujiao Jiang', 'Qingmin Liao', 'Zhaolong Wang', 'Xiangru Lin', 'Zongqing Lu', 'Yuxi Zhao', 'Hanqing Wei', 'Jingrui Ye', 'Yu Zhang', 'Zhijing Shao']",2024-05-30T01:53:39Z,http://arxiv.org/abs/2405.19609v1,"['cs.CV', 'cs.GR']","drivable avatar,benchmark,geometry,texture annotations,parametric body model,topology,parametric model,SMPLX-Lite-D,RGB sequences,keypoints annotations"
Disks in Nearby Young Stellar Associations Found Via Virtual Reality,"The Disk Detective citizen science project recently released a new catalog of
disk candidates found by visual inspection of images from NASA's Wide-Field
Infrared Survey Explorer (WISE) mission and other surveys. We applied this new
catalog of well-vetted disk candidates to search for new members of nearby
young stellar associations (YSAs) using a novel technique based on Gaia data
and virtual reality (VR). We examined AB Doradus, Argus, $\beta$ Pictoris,
Carina, Columba, Octans-Near, Tucana-Horologium, and TW Hya by displaying them
in VR together with other nearby stars, color-coded to show infrared excesses
found via Disk Detective. Using this method allows us to find new association
members in mass regimes where isochrones are degenerate. We propose ten new YSA
members with infrared excesses: three of AB Doradus (HD 44775, HD 40540 and HD
44510), one of $\beta$ Pictoris (HD 198472), two of Octans-Near (HD 157165 and
BD+35 2953), and four disk-hosting members of a combined population of Carina,
Columba and Tucana-Horologium: CPD-57 937, HD 274311, HD 41992, and WISEA
J092521.90-673224.8. This last object (J0925) appears to be an extreme debris
disk with a fractional infrared luminosity of $3.7 \times 10^{-2}$. We also
propose two new members of AB Doradus that do not show infrared excesses: TYC
6518-1857-1 and CPD-25 1292. We find HD 15115 appears to be a member of
Tucana-Horologium rather than $\beta$ Pictoris. We advocate for membership in
Columba-Carina of HD 30447, CPD-35 525, and HD 35841. Finally, we propose that
three M dwarfs, previously considered members of Tuc-Hor are better considered
a separate association, tentatively called ``Smethells 165''.","['Susan Higashio', 'Marc J. Kuchner', 'Steven M. Silverberg', 'Matthew A. Brandt', 'Thomas G. Grubb', 'Jonathan Gagné', 'John H. Debes', 'Joshua Schlieder', 'John P. Wisniewski', 'Stewart Slocum', 'Alissa S. Bans', 'Shambo Bhattacharjee', 'Joseph R. Biggs', 'Milton K. D. Bosch', 'Tadeas Cernohous', 'Katharina Doll', 'Hugo A. Durantini Luca', 'Alexandru Enachioaie', 'Phillip Griffith Sr.', 'Joshua Hamilton', 'Jonathan Holden', 'Michiharu Hyogo', 'Dawoon Jung', 'Lily Lau', 'Fernanda Piñiero Art Piipuu', 'Lisa Stiller', 'Disk Detective Collaboration']",2022-05-18T18:00:08Z,http://arxiv.org/abs/2205.09133v1,"['astro-ph.SR', 'astro-ph.EP']","Disk candidates,Young stellar associations,Virtual Reality,Gaia data,Infrared excesses,Debris disk,M dwarfs,Isochrones,Citizen science,Wide-Field Infrared Survey Explorer"
"Locality and low-dimensions in the prediction of natural experience from
  fMRI","Functional Magnetic Resonance Imaging (fMRI) provides dynamical access into
the complex functioning of the human brain, detailing the hemodynamic activity
of thousands of voxels during hundreds of sequential time points. One approach
towards illuminating the connection between fMRI and cognitive function is
through decoding; how do the time series of voxel activities combine to provide
information about internal and external experience? Here we seek models of fMRI
decoding which are balanced between the simplicity of their interpretation and
the effectiveness of their prediction. We use signals from a subject immersed
in virtual reality to compare global and local methods of prediction applying
both linear and nonlinear techniques of dimensionality reduction. We find that
the prediction of complex stimuli is remarkably low-dimensional, saturating
with less than 100 features. In particular, we build effective models based on
the decorrelated components of cognitive activity in the classically-defined
Brodmann areas. For some of the stimuli, the top predictive areas were
surprisingly transparent, including Wernicke's area for verbal instructions,
visual cortex for facial and body features, and visual-temporal regions for
velocity. Direct sensory experience resulted in the most robust predictions,
with the highest correlation ($c \sim 0.8$) between the predicted and
experienced time series of verbal instructions. Techniques based on non-linear
dimensionality reduction (Laplacian eigenmaps) performed similarly. The
interpretability and relative simplicity of our approach provides a conceptual
basis upon which to build more sophisticated techniques for fMRI decoding and
offers a window into cognitive function during dynamic, natural experience.","['Francois G. Meyer', 'Greg J. Stephens']",2007-12-07T20:21:18Z,http://arxiv.org/abs/0712.1219v2,"['q-bio.NC', 'stat.ML']","fMRI,prediction,decoding,voxel activities,dimensionality reduction,Brodmann areas,cognitive activity,sensory experience,Laplacian eigenmaps"
"Framework for Dynamic Evaluation of Muscle Fatigue in Manual Handling
  Work","Muscle fatigue is defined as the point at which the muscle is no longer able
to sustain the required force or work output level. The overexertion of muscle
force and muscle fatigue can induce acute pain and chronic pain in human body.
When muscle fatigue is accumulated, the functional disability can be resulted
as musculoskeletal disorders (MSD). There are several posture exposure analysis
methods useful for rating the MSD risks, but they are mainly based on static
postures. Even in some fatigue evaluation methods, muscle fatigue evaluation is
only available for static postures, but not suitable for dynamic working
process. Meanwhile, some existing muscle fatigue models based on physiological
models cannot be easily used in industrial ergonomic evaluations. The external
dynamic load is definitely the most important factor resulting muscle fatigue,
thus we propose a new fatigue model under a framework for evaluating fatigue in
dynamic working processes. Under this framework, virtual reality system is
taken to generate virtual working environment, which can be interacted with the
work with haptic interfaces and optical motion capture system. The motion
information and load information are collected and further processed to
evaluate the overall work load of the worker based on dynamic muscle fatigue
models and other work evaluation criterions and to give new information to
characterize the penibility of the task in design process.","['Liang Ma', 'Fouad Bennis', 'Damien Chablat', 'Wei Zhang']",2008-09-18T15:15:58Z,http://arxiv.org/abs/0809.3181v1,['cs.RO'],"muscle fatigue,manual handling,dynamic evaluation,overexertion,chronic pain,musculoskeletal disorders (MSD),posture exposure analysis,fatigue evaluation methods,dynamic working process,ergonomic evaluations"
Multi-camera Realtime 3D Tracking of Multiple Flying Animals,"Automated tracking of animal movement allows analyses that would not
otherwise be possible by providing great quantities of data. The additional
capability of tracking in realtime - with minimal latency - opens up the
experimental possibility of manipulating sensory feedback, thus allowing
detailed explorations of the neural basis for control of behavior. Here we
describe a new system capable of tracking the position and body orientation of
animals such as flies and birds. The system operates with less than 40 msec
latency and can track multiple animals simultaneously. To achieve these
results, a multi target tracking algorithm was developed based on the Extended
Kalman Filter and the Nearest Neighbor Standard Filter data association
algorithm. In one implementation, an eleven camera system is capable of
tracking three flies simultaneously at 60 frames per second using a gigabit
network of nine standard Intel Pentium 4 and Core 2 Duo computers. This
manuscript presents the rationale and details of the algorithms employed and
shows three implementations of the system. An experiment was performed using
the tracking system to measure the effect of visual contrast on the flight
speed of Drosophila melanogaster. At low contrasts, speed is more variable and
faster on average than at high contrasts. Thus, the system is already a useful
tool to study the neurobiology and behavior of freely flying animals. If
combined with other techniques, such as `virtual reality'-type computer
graphics or genetic manipulation, the tracking system would offer a powerful
new way to investigate the biology of flying animals.","['Andrew D. Straw', 'Kristin Branson', 'Titus R. Neumann', 'Michael H. Dickinson']",2010-01-25T01:40:40Z,http://arxiv.org/abs/1001.4297v1,['cs.CV'],"multi-camera,3D tracking,animals,realtime,latency,tracking algorithm,Extended Kalman Filter,Nearest Neighbor Standard Filter,neural basis,behavior"
"The Design of Circuit-Measuring Collaborative Learning System with
  Embedded Broker","Recently, the academic community has been giving much attention to
Cooperative Learning System, a group learning method combined with pedagogy and
social psychology. It allows group members to gain knowledge through
collaborations and interactions. Nowadays, most Internet cooperative learning
systems are designed to provide students mainly with a convenient online
environment to study theoretical courses but rarely with an online environment
to operate practical instruments. Hence, this paper designed a 3D online
cooperative learning system for operating virtual instruments with
circuit-measuring function. By integrating with Virtual Reality, Remote Control
Parameter Transmission and embedded system techniques, this system gives
learners not only a cooperative learning environment via networking to jointly
operate the 3D virtual instruments (for example, multi-meters, power supplies
and oscilloscopes) but also the functions of instant messages and 3D puzzles to
interact with one another. Therefore, learners can effectively improve learning
interests and results.","['Fu-Chien Kao', 'Siang-Ru Wang', 'Ting-Hao Huang']",2010-02-05T10:24:28Z,http://arxiv.org/abs/1002.1181v1,['cs.CY'],"Cooperative Learning System,Circuit-Measuring,Embedded System,Virtual Reality,Remote Control Parameter Transmission,Online Environment,Practical Instruments,Collaborative Learning,Social Psychology"
"Fatigue evaluation in maintenance and assembly operations by digital
  human simulation","Virtual human techniques have been used a lot in industrial design in order
to consider human factors and ergonomics as early as possible. The physical
status (the physical capacity of virtual human) has been mostly treated as
invariable in the current available human simulation tools, while indeed the
physical capacity varies along time in an operation and the change of the
physical capacity depends on the history of the work as well. Virtual Human
Status is proposed in this paper in order to assess the difficulty of manual
handling operations, especially from the physical perspective. The decrease of
the physical capacity before and after an operation is used as an index to
indicate the work difficulty. The reduction of physical strength is simulated
in a theoretical approach on the basis of a fatigue model in which fatigue
resistances of different muscle groups were regressed from 24 existing maximum
endurance time (MET) models. A framework based on digital human modeling
technique is established to realize the comparison of physical status. An
assembly case in airplane assembly is simulated and analyzed under the
framework. The endurance time and the decrease of the joint moment strengths
are simulated. The experimental result in simulated operations under laboratory
conditions confirms the feasibility of the theoretical approach.","['Liang Ma', 'Damien Chablat', 'Fouad Bennis', 'Wei Zhang', 'Bo Hu', 'François Guillaume']",2010-06-30T07:09:03Z,http://arxiv.org/abs/1006.5787v1,['cs.RO'],"maintenance,assembly operations,digital human simulation,virtual human techniques,physical capacity,virtual human status,manual handling operations,physical perspective,fatigue model,muscle groups"
Visual definition of procedures for automatic virtual scene generation,"With more and more digital media, especially in the field of virtual reality
where detailed and convincing scenes are much required, procedural scene
generation is a big helping tool for artists. A problem is that defining scene
descriptions through these procedures usually requires a knowledge in formal
language grammars, programming theory and manually editing textual files using
a strict syntax, making it less intuitive to use. Luckily, graphical user
interfaces has made a lot of tasks on computers easier to perform and out of
the belief that creating computer programs can also be one of them, visual
programming languages (VPLs) have emerged. The goal in VPLs is to shift more
work from the programmer to the integrated development environment (IDE),
making programming an user-friendlier task.
  In this thesis, an approach of using a VPL for defining procedures that
automatically generate virtual scenes is presented. The methods required to
build a VPL are presented, including a novel method of generating readable code
in a structured programming language. Also, the methods for achieving basic
principles of VPLs will be shown -- suitable visual presentation of information
and guiding the programmer in the right direction using constraints. On the
other hand, procedural generation methods are presented in the context of
visual programming -- adapting the application programming interface (API) of
these methods to better serve the user. The main focus will be on the methods
for urban modeling, such as building, city layout and details generation with
random number generation used to create non-deterministic scenes.",['Drazen Lucanin'],2012-02-10T16:58:00Z,http://arxiv.org/abs/1202.2868v1,"['cs.GR', 'cs.PL']","procedural scene generation,visual programming languages (VPLs),integrated development environment (IDE),structured programming language,application programming interface (API),urban modeling,random number generation"
Suppressing nano-scale stick-slip motion by feedback,"When a micro cantilever with a nano-scale tip is manipulated on a substrate
with atomic-scale roughness, the periodic lateral frictional force and
stochastic fluctuations may induce stick-slip motion of the cantilever tip,
which greatly decreases the precision of the nano manipulation. This unwanted
motion cannot be reduced by open-loop control especially when there exist
parameter uncertainties in the system model, and thus needs to introduce
feedback control. However, real-time feedback cannot be realized by the
existing virtual reality virtual feedback techniques based on the position
sensing capacity of the atomic force microscopy (AFM). To solve this problem,
we propose a new method to design real-time feedback control based on the force
sensing approach to compensate for the disturbances and thus reduce the
stick-slip motion of the cantilever tip. Theoretical analysis and numerical
simulations show that the controlled motion of the cantilever tip tracks the
desired trajectory with much higher precision. Further investigation shows that
our proposal is robust under various parameter uncertainties. Our study opens
up new perspectives of real-time nano manipulation.","['Jing Zhang', 'Re-Bing Wu', 'Lei Miao', 'Ning Xi', 'Chun-Wen Li', 'Yue-Chao Wang', 'Tzyh-Jong Tarn']",2012-04-06T07:48:29Z,http://arxiv.org/abs/1204.1424v1,['cond-mat.mes-hall'],"nano-scale,stick-slip motion,feedback,micro cantilever,atomic-scale roughness,lateral frictional force,stochastic fluctuations,open-loop control,real-time feedback,force sensing."
"Ambiguity of large scale temperature reconstructions from artificial
  tree growth in millennial climate simulations","The ambiguity of temperature reconstructions is assessed using pseudo tree
growth series in the virtual reality of two simulations of the climate of the
last millennium. The simple, process-based Vaganov-Shashkin-Lite (VS-Lite) code
calculates tree growth responses controlled by a limited number of climatic
parameters. Growth limitation by different ambient climate conditions allows
for possible nonlinearity and non-stationarity in the pseudo tree growth
series. Statistical reconstructions of temperature are achieved from simulated
tree growth for random selections of pseudo-proxy locations by simple local
regression and composite plus scaling techniques to address additional
ambiguities in paleoclimate reconstructions besides the known uncertainty and
shortcomings of the reconstruction methods. A systematic empirical evaluation
shows that the interrelations between simulated target and reconstructed
temperatures undergo strong variations with possibly pronounced
misrepresentations of temperatures. Thus (i) centennial scale inter-annual
correlations can be very weak; (ii) the decadal range of reconstructed
temperatures may be as large as the range of the temperature variations over
the considered time-period; (iii) decadal variability is under-represented in
the reconstructions. The misrepresentations are in part due to the temporally
varying temperature-growth relations and to an apparent lack of decadal scale
variability in the simulated pseudo-growth series compared to the local
temperatures.","['Oliver Bothe', 'Davide Zanchettin']",2012-07-10T09:39:47Z,http://arxiv.org/abs/1207.2279v1,['physics.ao-ph'],"temperature reconstructions,artificial tree growth,millennial climate simulations,Vaganov-Shashkin-Lite,climatic parameters,pseudo-proxy locations,paleoclimate reconstructions,local regression,composite plus scaling techniques,paleoclimate reconstructions"
Pre-conceptual Design Assessment of DEMO Remote Maintenance,"EDFA, as part of the Power Plant Physics and Technology programme, has been
working on the pre-conceptual design of a Demonstration Power Plant (DEMO). As
part of this programme, a review of the remote maintenance strategy considered
maintenance solutions compatible with expected environmental conditions, whilst
showing potential for meeting the plant availability targets. A key finding was
that, for practical purposes, the expected radiation levels prohibit the use of
complex remote handling operations to replace the first wall. In 2012/13, these
remote maintenance activities were further extended, providing an insight into
the requirements, constraints and challenges. In particular, the assessment of
blanket and divertor maintenance, in light of the expected radiation conditions
and availability, has elaborated the need for a very different approach from
that of ITER. This activity has produced some very informative virtual reality
simulations of the blanket segments and pipe removal that are exceptionally
valuable in communicating the complexity and scale of the required operations.
Through these simulations, estimates of the maintenance task durations have
been possible demonstrating that a full replacement of the blankets within 6
months could be achieved. The design of the first wall, including the need to
use sacrificial limiters must still be investigated. In support of the
maintenance operations, a first indication of the requirements of an Active
Maintenance Facility (AMF) has been elaborated.","['A. Loving', 'O. Crofts', 'N. Sykes', 'D. Iglesias', 'M. Coleman', 'J. Thomas', 'J. Harman', 'U. Fischer', 'J. Sanz', 'M. Siuko', 'M. Mittwollen', 'others']",2013-09-27T10:22:54Z,http://arxiv.org/abs/1309.7194v1,"['physics.plasm-ph', 'physics.ins-det']","DEMO,remote maintenance,radiation levels,remote handling operations,blanket maintenance,divertor maintenance,virtual reality simulations,maintenance task durations,sacrificial limiters,Active Maintenance Facility"
"Real-Time Human-Computer Interaction Based on Face and Hand Gesture
  Recognition","At the present time, hand gestures recognition system could be used as a more
expected and useable approach for human computer interaction. Automatic hand
gesture recognition system provides us a new tactic for interactive with the
virtual environment. In this paper, a face and hand gesture recognition system
which is able to control computer media player is offered. Hand gesture and
human face are the key element to interact with the smart system. We used the
face recognition scheme for viewer verification and the hand gesture
recognition in mechanism of computer media player, for instance, volume
down/up, next music and etc. In the proposed technique, first, the hand gesture
and face location is extracted from the main image by combination of skin and
cascade detector and then is sent to recognition stage. In recognition stage,
first, the threshold condition is inspected then the extracted face and gesture
will be recognized. In the result stage, the proposed technique is applied on
the video dataset and the high precision ratio acquired. Additional the
recommended hand gesture recognition method is applied on static American Sign
Language (ASL) database and the correctness rate achieved nearby 99.40%. also
the planned method could be used in gesture based computer games and virtual
reality.","['Reza Azad', 'Babak Azad', 'Nabil Belhaj Khalifa', 'Shahram Jamali']",2014-08-07T11:38:20Z,http://arxiv.org/abs/1408.1549v1,['cs.CV'],"human-computer interaction,face recognition,hand gesture recognition,virtual environment,viewer verification,media player control,skin detector,cascade detector,threshold condition,American Sign Language (ASL)"
Physical Light as a Metaphor for Inner Light,"The metaphor between physical light and inner light has a long history that
permeates diverse languages and cultures. This paper outlines a system for
using basic principles from optics to visually represent psychological states
and processes such as ideation, enlightenment, mindfulness, and fragmentation
versus integrity, as well as situations that occur between people involving
phenomena such as honest versus deceptive communication, and understanding
versus misunderstanding. The paper summarizes two ongoing projects based on
this system: The Light and Enlightenment art installation project, and the
Soultracker virtual reality project. These projects enable people to depict
their inner lives and external worlds including situations and relationships
with others, both as they are and as they could be, and explore alternative
paths for navigating challenges and living to their fullest potential. The
projects aim to be of clinical value as therapeutic tools, as well as of
pedagogical value by providing a concrete language for depicting aspects of
human nature that can otherwise seem elusive and intangible.",['Liane Gabora'],2014-09-03T12:39:00Z,http://arxiv.org/abs/1409.1064v3,"['q-bio.NC', 'cs.CG', 'cs.MM']","physical light,inner light,optics,ideation,enlightenment,mindfulness,fragmentation,integrity,communication,misunderstanding"
"Parkinson's disease patient rehabilitation using gaming platforms:
  lessons learnt","Parkinson's disease (PD) is a progressive neurodegenerative movement disorder
where motor dysfunction gradually increases as the disease progress. In
addition to administering dopaminergic PD-specific drugs, attending
neurologists strongly recommend regular exercise combined with physiotherapy.
However, because of the long-term nature of the disease, patients following
traditional rehabilitation programs may get bored, lose interest and eventually
drop out as a direct result of the repeatability and predictability of the
prescribed exercises. Technology supported opportunities to liven up a daily
exercise schedule have appeared in the form of character-based, virtual reality
games which promote physical training in a non-linear and looser fashion and
provide an experience that varies from one game loop the next. Such
""exergames"", a word that results from the amalgamation of the words ""exercise""
and ""game"" challenge patients into performing movements of varying complexity
in a playful and immersive virtual environment. Today's game consoles such as
Nintendo's Wii, Sony PlayStation Eye and Microsoft's Kinect sensor present new
opportunities to infuse motivation and variety to an otherwise mundane
physiotherapy routine. In this paper we present some of these approaches,
discuss their suitability for these PD patients, mainly on the basis of demands
made on balance, agility and gesture precision, and present design principles
that exergame platforms must comply with in order to be suitable for PD
patients.","['Ioannis Pachoulakis', 'Nikolaos Papadopoulos', 'Cleanthe Spanaki']",2015-11-09T07:36:22Z,http://arxiv.org/abs/1511.02589v1,"['cs.CY', 'cs.CV', 'I.6.3; I.6.8']","Parkinson's disease,rehabilitation,gaming platforms,physiotherapy,exergames,virtual reality,neurodegenerative disorder,motor dysfunction,exercise,game consoles"
Image segmentation of cross-country scenes captured in IR spectrum,"Computer vision has become a major source of information for autonomous
navigation of robots of various types, self-driving cars, military robots and
mars/lunar rovers are some examples. Nevertheless, the majority of methods
focus on analysing images captured in visible spectrum. In this manuscript we
elaborate on the problem of segmenting cross-country scenes captured in IR
spectrum. For this purpose we proposed employing salient features. Salient
features are robust to variations in scale, brightness and view angle. We
suggest the Speeded-Up Robust Features as a basis for our salient features for
a number of reasons discussed in the paper. We also provide a comparison of two
SURF implementations. The SURF features are extracted from images of different
terrain types. For every feature we estimate a terrain class membership
function. The membership values are obtained by means of either the multi-layer
perceptron or nearest neighbours. The features' class membership values and
their spatial positions are then applied to estimate class membership values
for all pixels in the image. To decrease the effect of segmentation blinking
that is caused by rapid switching between different terrain types and to speed
up segmentation, we are tracking camera position and predict features'
positions. The comparison of the multi-layer perception and the nearest
neighbour classifiers is presented in the paper. The error rate of the terrain
segmentation using the nearest neighbours obtained on the testing set is
16.6+-9.17%.",['Artem Lenskiy'],2016-04-08T20:14:46Z,http://arxiv.org/abs/1604.02469v1,"['cs.CV', '68T10', 'I.4.7; I.4.8; I.5.1']","image segmentation,cross-country scenes,IR spectrum,salient features,Speeded-Up Robust Features,SURF,terrain types,multi-layer perceptron,nearest neighbours,segmentation blinking"
Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",['Emad Barsoum'],2016-04-21T06:55:42Z,http://arxiv.org/abs/1604.06195v1,['cs.CV'],"hand pose estimation,articulated,depth sensor,discriminative,generative,hybrid methods,augmented reality,virtual reality,wearable devices,kinematic parameters"
Collaborative visual analytics of radio surveys in the Big Data era,"Radio survey datasets comprise an increasing number of individual
observations stored as sets of multidimensional data. In large survey projects,
astronomers commonly face limitations regarding: 1) interactive visual
analytics of sufficiently large subsets of data; 2) synchronous and
asynchronous collaboration; and 3) documentation of the discovery workflow. To
support collaborative data inquiry, we present encube, a large-scale
comparative visual analytics framework. Encube can utilise advanced
visualization environments such as the CAVE2 (a hybrid 2D and 3D virtual
reality environment powered with a 100 Tflop/s GPU-based supercomputer and 84
million pixels) for collaborative analysis of large subsets of data from radio
surveys. It can also run on standard desktops, providing a capable visual
analytics experience across the display ecology. Encube is composed of four
primary units enabling compute-intensive processing, advanced visualisation,
dynamic interaction, parallel data query, along with data management. Its
modularity will make it simple to incorporate astronomical analysis packages
and Virtual Observatory capabilities developed within our community. We discuss
how encube builds a bridge between high-end display systems (such as CAVE2) and
the classical desktop, preserving all traces of the work completed on either
platform -- allowing the research process to continue wherever you are.","['Dany Vohl', 'Christopher J. Fluke', 'Amr H. Hassan', 'David G. Barnes', 'Virginia A. Kilborn']",2016-12-03T04:11:44Z,http://arxiv.org/abs/1612.00920v1,['astro-ph.IM'],"Collaborative visual analytics,Radio surveys,Big Data,Multidimensional data,Advanced visualization,Virtual reality environment,GPU-based supercomputer,Parallel data query,Data management,Astronomical analysis."
"A feasibility study on SSVEP-based interaction with motivating and
  immersive virtual and augmented reality","Non-invasive steady-state visual evoked potential (SSVEP) based
brain-computer interface (BCI) systems offer high bandwidth compared to other
BCI types and require only minimal calibration and training. Virtual reality
(VR) has been already validated as effective, safe, affordable and motivating
feedback modality for BCI experiments. Augmented reality (AR) enhances the
physical world by superimposing informative, context sensitive, computer
generated content. In the context of BCI, AR can be used as a friendlier and
more intuitive real-world user interface, thereby facilitating a more seamless
and goal directed interaction. This can improve practicality and usability of
BCI systems and may help to compensate for their low bandwidth. In this
feasibility study, three healthy participants had to finish a complex
navigation task in immersive VR and AR conditions using an online SSVEP BCI.
Two out of three subjects were successful in all conditions. To our knowledge,
this is the first work to present an SSVEP BCI that operates using target
stimuli integrated in immersive VR and AR (head-mounted display and camera).
This research direction can benefit patients by introducing more intuitive and
effective real-world interaction (e.g. smart home control). It may also be
relevant for user groups that require or benefit from hands free operation
(e.g. due to temporary situational disability).","['Josef Faller', 'Brendan Z. Allison', 'Clemens Brunner', 'Reinhold Scherer', 'Dieter Schmalstieg', 'Gert Pfurtscheller', 'Christa Neuper']",2017-01-15T01:58:47Z,http://arxiv.org/abs/1701.03981v1,"['cs.HC', 'cs.GR']","SSVEP,brain-computer interface,virtual reality,augmented reality,BCI systems,navigation task,immersive VR,immersive AR,head-mounted display,camera"
Real-time Teaching Cues for Automated Surgical Coaching,"With introduction of new technologies in the operating room like the da Vinci
Surgical System, training surgeons to use them effectively and efficiently is
crucial in the delivery of better patient care. Coaching by an expert surgeon
is effective in teaching relevant technical skills, but current methods to
deliver effective coaching are limited and not scalable. We present a virtual
reality simulation-based framework for automated virtual coaching in surgical
education. We implement our framework within the da Vinci Skills Simulator. We
provide three coaching modes ranging from a hands-on teacher (continuous
guidance) to a handsoff guide (assistance upon request). We present six
teaching cues targeted at critical learning elements of a needle passing task,
which are shown to the user based on the coaching mode. These cues are
graphical overlays which guide the user, inform them about sub-par performance,
and show relevant video demonstrations. We evaluated our framework in a pilot
randomized controlled trial with 16 subjects in each arm. In a post-study
questionnaire, participants reported high comprehension of feedback, and
perceived improvement in performance. After three practice repetitions of the
task, the control arm (independent learning) showed better motion efficiency
whereas the experimental arm (received real-time coaching) had better
performance of learning elements (as per the ACS Resident Skills Curriculum).
We observed statistically higher improvement in the experimental group based on
one of the metrics (related to needle grasp orientation). In conclusion, we
developed an automated coach that provides real-time cues for surgical training
and demonstrated its feasibility.","['Anand Malpani', 'S. Swaroop Vedula', 'Henry C. Lin', 'Gregory D. Hager', 'Russell H. Taylor']",2017-04-24T19:54:34Z,http://arxiv.org/abs/1704.07436v1,"['cs.RO', 'cs.HC']","technologies,da Vinci Surgical System,surgical coaching,virtual reality simulation,coaching modes,teaching cues,needle passing task,pilot randomized controlled trial,motion efficiency"
A Lightweight Approach for On-the-Fly Reflectance Estimation,"Estimating surface reflectance (BRDF) is one key component for complete 3D
scene capture, with wide applications in virtual reality, augmented reality,
and human computer interaction. Prior work is either limited to controlled
environments (\eg gonioreflectometers, light stages, or multi-camera domes), or
requires the joint optimization of shape, illumination, and reflectance, which
is often computationally too expensive (\eg hours of running time) for
real-time applications. Moreover, most prior work requires HDR images as input
which further complicates the capture process. In this paper, we propose a
lightweight approach for surface reflectance estimation directly from $8$-bit
RGB images in real-time, which can be easily plugged into any 3D
scanning-and-fusion system with a commodity RGBD sensor. Our method is
learning-based, with an inference time of less than 90ms per scene and a model
size of less than 340K bytes. We propose two novel network architectures,
HemiCNN and Grouplet, to deal with the unstructured input data from multiple
viewpoints under unknown illumination. We further design a loss function to
resolve the color-constancy and scale ambiguity. In addition, we have created a
large synthetic dataset, SynBRDF, which comprises a total of $500$K RGBD images
rendered with a physically-based ray tracer under a variety of natural
illumination, covering $5000$ materials and $5000$ shapes. SynBRDF is the first
large-scale benchmark dataset for reflectance estimation. Experiments on both
synthetic data and real data show that the proposed method effectively recovers
surface reflectance, and outperforms prior work for reflectance estimation in
uncontrolled environments.","['Kihwan Kim', 'Jinwei Gu', 'Stephen Tyree', 'Pavlo Molchanov', 'Matthias Nießner', 'Jan Kautz']",2017-05-19T19:45:57Z,http://arxiv.org/abs/1705.07162v2,['cs.CV'],"surface reflectance,3D scene capture,virtual reality,augmented reality,human computer interaction,RGB images,real-time,network architectures,loss function,dataset"
"Applying advanced machine learning models to classify
  electro-physiological activity of human brain for use in biometric
  identification","In this article we present the results of our research related to the study
of correlations between specific visual stimulation and the elicited brain's
electro-physiological response collected by EEG sensors from a group of
participants. We will look at how the various characteristics of visual
stimulation affect the measured electro-physiological response of the brain and
describe the optimal parameters found that elicit a steady-state visually
evoked potential (SSVEP) in certain parts of the cerebral cortex where it can
be reliably perceived by the electrode of the EEG device. After that, we
continue with a description of the advanced machine learning pipeline model
that can perform confident classification of the collected EEG data in order to
(a) reliably distinguish signal from noise (about 85% validation score) and (b)
reliably distinguish between EEG records collected from different human
participants (about 80% validation score). Finally, we demonstrate that the
proposed method works reliably even with an inexpensive (less than $100)
consumer-grade EEG sensing device and with participants who do not have
previous experience with EEG technology (EEG illiterate). All this in
combination opens up broad prospects for the development of new types of
consumer devices, [e.g.] based on virtual reality helmets or augmented reality
glasses where EEG sensor can be easily integrated. The proposed method can be
used to improve an online user experience by providing [e.g.] password-less
user identification for VR / AR applications. It can also find a more advanced
application in intensive care units where collected EEG data can be used to
classify the level of conscious awareness of patients during anesthesia or to
automatically detect hardware failures by classifying the input signal as
noise.",['Iaroslav Omelianenko'],2017-08-03T14:50:02Z,http://arxiv.org/abs/1708.01167v1,['cs.LG'],"machine learning models,electro-physiological activity,human brain,biometric identification,visual stimulation,EEG sensors,steady-state visually evoked potential (SSVEP),classification,validation score,consumer-grade EEG sensing"
Artistic style transfer for videos and spherical images,"Manually re-drawing an image in a certain artistic style takes a professional
artist a long time. Doing this for a video sequence single-handedly is beyond
imagination. We present two computational approaches that transfer the style
from one image (for example, a painting) to a whole video sequence. In our
first approach, we adapt to videos the original image style transfer technique
by Gatys et al. based on energy minimization. We introduce new ways of
initialization and new loss functions to generate consistent and stable
stylized video sequences even in cases with large motion and strong occlusion.
Our second approach formulates video stylization as a learning problem. We
propose a deep network architecture and training procedures that allow us to
stylize arbitrary-length videos in a consistent and stable way, and nearly in
real time. We show that the proposed methods clearly outperform simpler
baselines both qualitatively and quantitatively. Finally, we propose a way to
adapt these approaches also to 360 degree images and videos as they emerge with
recent virtual reality hardware.","['Manuel Ruder', 'Alexey Dosovitskiy', 'Thomas Brox']",2017-08-13T21:17:59Z,http://arxiv.org/abs/1708.04538v3,['cs.CV'],"Artistic style transfer,Videos,Spherical images,Image style transfer,Energy minimization,Loss functions,Deep network architecture,Training procedures,Virtual reality hardware"
"Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D
  Representations","This paper focuses on the problem of learning 6-DOF grasping with a parallel
jaw gripper in simulation. We propose the notion of a geometry-aware
representation in grasping based on the assumption that knowledge of 3D
geometry is at the heart of interaction. Our key idea is constraining and
regularizing grasping interaction learning through 3D geometry prediction.
Specifically, we formulate the learning of deep geometry-aware grasping model
in two steps: First, we learn to build mental geometry-aware representation by
reconstructing the scene (i.e., 3D occupancy grid) from RGBD input via
generative 3D shape modeling. Second, we learn to predict grasping outcome with
its internal geometry-aware representation. The learned outcome prediction
model is used to sequentially propose grasping solutions via
analysis-by-synthesis optimization. Our contributions are fourfold: (1) To best
of our knowledge, we are presenting for the first time a method to learn a
6-DOF grasping net from RGBD input; (2) We build a grasping dataset from
demonstrations in virtual reality with rich sensory and interaction
annotations. This dataset includes 101 everyday objects spread across 7
categories, additionally, we propose a data augmentation strategy for effective
learning; (3) We demonstrate that the learned geometry-aware representation
leads to about 10 percent relative performance improvement over the baseline
CNN on grasping objects from our dataset. (4) We further demonstrate that the
model generalizes to novel viewpoints and object instances.","['Xinchen Yan', 'Jasmine Hsu', 'Mohi Khansari', 'Yunfei Bai', 'Arkanath Pathak', 'Abhinav Gupta', 'James Davidson', 'Honglak Lee']",2017-08-24T08:09:04Z,http://arxiv.org/abs/1708.07303v4,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']","6-DOF,Grasping,Interaction,Deep,Geometry-aware,3D,Representation,RGBD,Generative,Modeling"
"Artificial Neural Networks-Based Machine Learning for Wireless Networks:
  A Tutorial","Next-generation wireless networks must support ultra-reliable, low-latency
communication and intelligently manage a massive number of Internet of Things
(IoT) devices in real-time, within a highly dynamic environment. This need for
stringent communication quality-of-service (QoS) requirements as well as mobile
edge and core intelligence can only be realized by integrating fundamental
notions of artificial intelligence (AI) and machine learning across the
wireless infrastructure and end-user devices. In this context, this paper
provides a comprehensive tutorial that introduces the main concepts of machine
learning, in general, and artificial neural networks (ANNs), in particular, and
their potential applications in wireless communications. For this purpose, we
present a comprehensive overview on a number of key types of neural networks
that include feed-forward, recurrent, spiking, and deep neural networks. For
each type of neural network, we present the basic architecture and training
procedure, as well as the associated challenges and opportunities. Then, we
provide an in-depth overview on the variety of wireless communication problems
that can be addressed using ANNs, ranging from communication using unmanned
aerial vehicles to virtual reality and edge caching.For each individual
application, we present the main motivation for using ANNs along with the
associated challenges while also providing a detailed example for a use case
scenario and outlining future works that can be addressed using ANNs. In a
nutshell, this article constitutes one of the first holistic tutorials on the
development of machine learning techniques tailored to the needs of future
wireless networks.","['Mingzhe Chen', 'Ursula Challita', 'Walid Saad', 'Changchuan Yin', 'Mérouane Debbah']",2017-10-09T02:33:43Z,http://arxiv.org/abs/1710.02913v2,"['cs.IT', 'cs.AI', 'math.IT']","Artificial Neural Networks,Machine Learning,Wireless Networks,Internet of Things (IoT) devices,Quality-of-Service (QoS) requirements,Mobile Edge,Core Intelligence,Neural Networks,Feed-forward,Recurrent,Spiking,Deep Neural Networks."
"Estimation of optimal encoding ladders for tiled 360° VR video in
  adaptive streaming systems","Given the significant industrial growth of demand for virtual reality (VR),
360{\deg} video streaming is one of the most important VR applications that
require cost-optimal solutions to achieve widespread proliferation of VR
technology. Because of its inherent variability of data-intensive content types
and its tiled-based encoding and streaming, 360{\deg} video requires new
encoding ladders in adaptive streaming systems to achieve cost-optimal and
immersive streaming experiences. In this context, this paper targets both the
provider's and client's perspectives and introduces a new content-aware
encoding ladder estimation method for tiled 360{\deg} VR video in adaptive
streaming systems. The proposed method first categories a given 360{\deg} video
using its features of encoding complexity and estimates the visual distortion
and resource cost of each bitrate level based on the proposed distortion and
resource cost models. An optimal encoding ladder is then formed using the
proposed integer linear programming (ILP) algorithm by considering practical
constraints. Experimental results of the proposed method are compared with the
recommended encoding ladders of professional streaming service providers.
Evaluations show that the proposed encoding ladders deliver better results
compared to the recommended encoding ladders in terms of objective quality for
360{\deg} video, providing optimal encoding ladders using a set of service
provider's constraint parameters.","['Cagri Ozcinar', 'Ana De Abreu', 'Sebastian Knorr', 'Aljosa Smolic']",2017-11-09T13:07:45Z,http://arxiv.org/abs/1711.03362v1,['cs.MM'],"encoding ladders,tiled 360° VR video,adaptive streaming systems,content-aware,encoding complexity,visual distortion,resource cost models,integer linear programming (ILP) algorithm,objective quality,service provider's constraint parameters."
"Reliable Video Streaming over mmWave with Multi Connectivity and Network
  Coding","The next generation of multimedia applications will require the
telecommunication networks to support a higher bitrate than today, in order to
deliver virtual reality and ultra-high quality video content to the users. Most
of the video content will be accessed from mobile devices, prompting the
provision of very high data rates by next generation (5G) cellular networks. A
possible enabler in this regard is communication at mmWave frequencies, given
the vast amount of available spectrum that can be allocated to mobile users;
however, the harsh propagation environment at such high frequencies makes it
hard to provide a reliable service. This paper presents a reliable video
streaming architecture for mmWave networks, based on multi connectivity and
network coding, and evaluates its performance using a novel combination of the
ns-3 mmWave module, real video traces and the network coding library Kodo. The
results show that it is indeed possible to reliably stream video over cellular
mmWave links, while the combination of multi connectivity and network coding
can support high video quality with low latency.","['Matteo Drago', 'Tommy Azzino', 'Michele Polese', 'Cedomir Stefanovic', 'Michele Zorzi']",2017-11-16T15:49:56Z,http://arxiv.org/abs/1711.06154v2,"['cs.NI', 'cs.IT', 'math.IT']","mmWave,Multi Connectivity,Network Coding,Cellular Networks,5G,Multimedia Applications,Virtual Reality,Video Streaming,ns-3,Kodo"
Cascaded 3D Full-body Pose Regression from Single Depth Image at 100 FPS,"There are increasing real-time live applications in virtual reality, where it
plays an important role in capturing and retargetting 3D human pose. But it is
still challenging to estimate accurate 3D pose from consumer imaging devices
such as depth camera. This paper presents a novel cascaded 3D full-body pose
regression method to estimate accurate pose from a single depth image at 100
fps. The key idea is to train cascaded regressors based on Gradient Boosting
algorithm from pre-recorded human motion capture database. By incorporating
hierarchical kinematics model of human pose into the learning procedure, we can
directly estimate accurate 3D joint angles instead of joint positions. The
biggest advantage of this model is that the bone length can be preserved during
the whole 3D pose estimation procedure, which leads to more effective features
and higher pose estimation accuracy. Our method can be used as an
initialization procedure when combining with tracking methods. We demonstrate
the power of our method on a wide range of synthesized human motion data from
CMU mocap database, Human3.6M dataset and real human movements data captured in
real time. In our comparison against previous 3D pose estimation methods and
commercial system such as Kinect 2017, we achieve the state-of-the-art
accuracy.","['Shihong Xia', 'Zihao Zhang', 'Le Su']",2017-11-22T04:24:43Z,http://arxiv.org/abs/1711.08126v2,['cs.GR'],"3D pose regression,Depth image,Human pose estimation,Gradient Boosting algorithm,Kinematics model,Joint angles,Bone length preservation,Pose estimation accuracy,Tracking methods,State-of-the-art accuracy"
Non-Orthogonal Multiple Access for 5G and Beyond,"Driven by the rapid escalation of the wireless capacity requirements imposed
by advanced multimedia applications (e.g., ultra-high-definition video, virtual
reality etc.), as well as the dramatically increasing demand for user access
required for the Internet of Things (IoT), the fifth generation (5G) networks
face challenges in terms of supporting large-scale heterogeneous data traffic.
Non-orthogonal multiple access (NOMA), which has been recently proposed for the
3rd generation partnership projects long-term evolution advanced (3GPP-LTE-A),
constitutes a promising technology of addressing the above-mentioned challenges
in 5G networks by accommodating several users within the same orthogonal
resource block. By doing so, significant bandwidth efficiency enhancement can
be attained over conventional orthogonal multiple access (OMA) techniques. This
motivated numerous researchers to dedicate substantial research contributions
to this field. In this context, we provide a comprehensive overview of the
state-of-the-art in power-domain multiplexing aided NOMA, with a focus on the
theoretical NOMA principles, multiple antenna aided NOMA design, on the
interplay between NOMA and cooperative transmission, on the resource control of
NOMA, on the co-existence of NOMA with other emerging potential 5G techniques
and on the comparison with other NOMA variants. We highlight the main
advantages of power-domain multiplexing NOMA compared to other existing NOMA
techniques. We summarize the challenges of existing research contributions of
NOMA and provide potential solutions. Finally, we offer some design guidelines
for NOMA systems and identify promising research opportunities for the future.","['Yuanwei Liu', 'Zhijin Qin', 'Maged Elkashlan', 'Zhiguo Ding', 'Arumugam Nallanathan', 'Lajos Hanzo']",2018-08-01T11:40:25Z,http://arxiv.org/abs/1808.00277v1,"['cs.IT', 'math.IT']","Non-Orthogonal Multiple Access,5G,Internet of Things,3GPP-LTE-A,Bandwidth Efficiency,Power-Domain Multiplexing,Multiple Antenna,Cooperative Transmission,Resource Control,Emerging Techniques"
Deep Appearance Models for Face Rendering,"We introduce a deep appearance model for rendering the human face. Inspired
by Active Appearance Models, we develop a data-driven rendering pipeline that
learns a joint representation of facial geometry and appearance from a
multiview capture setup. Vertex positions and view-specific textures are
modeled using a deep variational autoencoder that captures complex nonlinear
effects while producing a smooth and compact latent representation.
View-specific texture enables the modeling of view-dependent effects such as
specularity. In addition, it can also correct for imperfect geometry stemming
from biased or low resolution estimates. This is a significant departure from
the traditional graphics pipeline, which requires highly accurate geometry as
well as all elements of the shading model to achieve realism through
physically-inspired light transport. Acquiring such a high level of accuracy is
difficult in practice, especially for complex and intricate parts of the face,
such as eyelashes and the oral cavity. These are handled naturally by our
approach, which does not rely on precise estimates of geometry. Instead, the
shading model accommodates deficiencies in geometry though the flexibility
afforded by the neural network employed. At inference time, we condition the
decoding network on the viewpoint of the camera in order to generate the
appropriate texture for rendering. The resulting system can be implemented
simply using existing rendering engines through dynamic textures with flat
lighting. This representation, together with a novel unsupervised technique for
mapping images to facial states, results in a system that is naturally suited
to real-time interactive settings such as Virtual Reality (VR).","['Stephen Lombardi', 'Jason Saragih', 'Tomas Simon', 'Yaser Sheikh']",2018-08-01T15:13:48Z,http://arxiv.org/abs/1808.00362v1,"['cs.GR', 'cs.CV']","Deep Appearance Models,Face Rendering,Active Appearance Models,Facial Geometry,View-specific Textures,Deep Variational Autoencoder,View-dependent Effects,Shading Model,Neural Network,Rendering Engines"
"The Effectiveness of Traditional Tools and Computer-Aided Technologies
  for Health and Safety Training in the Construction Sector: A Systematic
  Review","For workers, the exposure to on-site hazards can result in fatalities and
serious injuries. To improve safety outcomes, different approaches have been
implemented for health and safety training in the construction sector, such as
traditional tools and computer-aided technologies (e.g., serious games and
virtual reality). However, the effectiveness of these approaches has been
barely explored. In order to bridge this gap, a systematic review of existing
studies was conducted. Unlike previous review studies in this field that
focused on uncovering the technology characters and challenges, this study
mainly evaluated the effectiveness of training using traditional tools and
computer-aided technologies on the well-being of individuals. Measures of the
effectiveness included knowledge acquisition, unsafe behaviour alteration, and
injury rate reduction. Results indicated that: 1. the effectiveness of
traditional tools is sufficiently supported by statistical evidence; and 2. the
use of computer-aided technologies has evidence to support its effectiveness,
but more solid evidence is required to support this statement. It was also
found that the overall performance of computer-aided technologies is superior
in several technical aspects compared to traditional tools, namely,
representing actual workplace situations, providing text-free interfaces,
having better user engagement, and being more cost-efficient. Finally, using
the systematic review findings, a theoretical framework is proposed as a
potential solution to help future research in this field systematically examine
the effectiveness and usability of their approaches. This framework is
theoretical in nature and requires further validation. A further study is
therefore proposed to test and validate this framework.","['Yifan Gao', 'Vicente Gonzalez', 'Tak Wing Yiu']",2018-08-06T01:09:21Z,http://arxiv.org/abs/1808.02021v1,['cs.CY'],"traditional tools,computer-aided technologies,health and safety training,construction sector,systematic review,serious games,virtual reality,knowledge acquisition,unsafe behaviour,injury rate reduction"
"RoboTurk: A Crowdsourcing Platform for Robotic Skill Learning through
  Imitation","Imitation Learning has empowered recent advances in learning robotic
manipulation tasks by addressing shortcomings of Reinforcement Learning such as
exploration and reward specification. However, research in this area has been
limited to modest-sized datasets due to the difficulty of collecting large
quantities of task demonstrations through existing mechanisms. This work
introduces RoboTurk to address this challenge. RoboTurk is a crowdsourcing
platform for high quality 6-DoF trajectory based teleoperation through the use
of widely available mobile devices (e.g. iPhone). We evaluate RoboTurk on three
manipulation tasks of varying timescales (15-120s) and observe that our user
interface is statistically similar to special purpose hardware such as virtual
reality controllers in terms of task completion times. Furthermore, we observe
that poor network conditions, such as low bandwidth and high delay links, do
not substantially affect the remote users' ability to perform task
demonstrations successfully on RoboTurk. Lastly, we demonstrate the efficacy of
RoboTurk through the collection of a pilot dataset; using RoboTurk, we
collected 137.5 hours of manipulation data from remote workers, amounting to
over 2200 successful task demonstrations in 22 hours of total system usage. We
show that the data obtained through RoboTurk enables policy learning on
multi-step manipulation tasks with sparse rewards and that using larger
quantities of demonstrations during policy learning provides benefits in terms
of both learning consistency and final performance. For additional results,
videos, and to download our pilot dataset, visit
$\href{http://roboturk.stanford.edu/}{\texttt{roboturk.stanford.edu}}$","['Ajay Mandlekar', 'Yuke Zhu', 'Animesh Garg', 'Jonathan Booher', 'Max Spero', 'Albert Tung', 'Julian Gao', 'John Emmons', 'Anchit Gupta', 'Emre Orbay', 'Silvio Savarese', 'Li Fei-Fei']",2018-11-07T08:01:21Z,http://arxiv.org/abs/1811.02790v1,"['cs.RO', 'cs.AI', 'cs.LG']","Imitation learning,Robotic skill learning,Crowdsourcing platform,6-DoF trajectory,Teleoperation,Mobile devices,Network conditions,Task demonstrations,Policy learning,Sparse rewards"
Spherical clustering of users navigating 360° content,"In Virtual Reality (VR) applications, understanding how users explore the
omnidirectional content is important to optimize content creation, to develop
user-centric services, or even to detect disorders in medical applications.
Clustering users based on their common navigation patterns is a first direction
to understand users behaviour. However, classical clustering techniques fail in
identifying these common paths, since they are usually focused on minimizing a
simple distance metric. In this paper, we argue that minimizing the distance
metric does not necessarily guarantee to identify users that experience similar
navigation path in the VR domain. Therefore, we propose a graph-based method to
identify clusters of users who are attending the same portion of the spherical
content over time. The proposed solution takes into account the spherical
geometry of the content and aims at clustering users based on the actual
overlap of displayed content among users. Our method is tested on real VR user
navigation patterns. Results show that our solution leads to clusters in which
at least 85% of the content displayed by one user is shared among the other
users belonging to the same cluster.","['Silvia Rossi', 'Francesca De Simone', 'Pascal Frossard', 'Laura Toni']",2018-11-13T09:51:09Z,http://arxiv.org/abs/1811.05185v2,"['cs.MM', 'eess.IV']","Spherical clustering,Users,Navigating,360° content,Virtual Reality (VR),Omnidirectional content,Content creation,User-centric services,Disorders,Medical applications"
"Human Intention Estimation based on Hidden Markov Model Motion
  Validation for Safe Flexible Robotized Warehouses","With the substantial growth of logistics businesses the need for larger
warehouses and their automation arises, thus using robots as assistants to
human workers is becoming a priority. In order to operate efficiently and
safely, robot assistants or the supervising system should recognize human
intentions in real-time. Theory of mind (ToM) is an intuitive human conception
of other humans' mental state, i.e., beliefs and desires, and how they cause
behavior. In this paper we propose a ToM based human intention estimation
algorithm for flexible robotized warehouses. We observe human's, i.e., worker's
motion and validate it with respect to the goal locations using generalized
Voronoi diagram based path planning. These observations are then processed by
the proposed hidden Markov model framework which estimates worker intentions in
an online manner, capable of handling changing environments. To test the
proposed intention estimation we ran experiments in a real-world laboratory
warehouse with a worker wearing Microsoft Hololens augmented reality glasses.
Furthermore, in order to demonstrate the scalability of the approach to larger
warehouses, we propose to use virtual reality digital warehouse twins in order
to realistically simulate worker behavior. We conducted intention estimation
experiments in the larger warehouse digital twin with up to 24 running robots.
We demonstrate that the proposed framework estimates warehouse worker
intentions precisely and in the end we discuss the experimental results.","['Tomislav PetkoviāE, 'David Puljiz', 'Ivan MarkoviāE, 'Björn Hein']",2018-11-20T14:32:31Z,http://arxiv.org/abs/1811.08269v1,['cs.RO'],"human intention estimation,hidden Markov model,motion validation,flexible robotized warehouses,theory of mind (ToM),generalized Voronoi diagram,path planning,online estimation,Microsoft Hololens,virtual reality"
"MAT-Fly: An Educational Platform for Simulating Unmanned Aerial Vehicles
  Aimed to Detect and Track Moving Objects","The main motivation of this work is to propose a simulation approach for a
specific task within the Unmanned Aerial Vehicle (UAV) field, i.e., the visual
detection and tracking of arbitrary moving objects. In particular, it is
described MAT-Fly, a numerical simulation platform for multi-rotor aircraft
characterized by the ease of use and control development. The platform is based
on Matlab and the MathWorks Virtual Reality (VR) and Computer Vision System
(CVS) toolboxes that work together to simulate the behavior of a quad-rotor
while tracking a car that moves along a nontrivial path. The VR toolbox has
been chosen due to the familiarity that students have with Matlab and because
it does not require a notable effort by the user for the learning and
development phase thanks to its simple structure. The overall architecture is
quite modular so that each block can be easily replaced with others simplifying
the code reuse and the platform customization. Some simple testbeds are
presented to show the validity of the approach and how the platform works. The
simulator is released as open-source, making it possible to go through any part
of the system, and available for educational purposes.","['Giuseppe Silano', 'Luigi Iannelli']",2019-03-31T10:56:47Z,http://arxiv.org/abs/1904.00378v4,"['cs.RO', 'cs.SY']","Mat-Fly,Educational Platform,Simulating,Unmanned Aerial Vehicles,Moving Objects,Simulation Approach,Virtual Reality,Computer Vision System,Matlab,Multi-Rotor Aircraft"
Detecting driver distraction using stimuli-response EEG analysis,"Detecting driver distraction is a significant concern for future intelligent
transportation systems. We present a new approach for identifying distracted
driving behavior by evaluating a stimulus and response interaction with the
brain signals in two ways. First, measuring the driver response through EEG by
creating various types of distraction stimuli such as reading, texting, calling
and using phone camera (risk odds ratio of these activities determined by NHTSA
study). Second, using a survey, comparing driver's order/perception of severity
of distraction with the derived distraction index from EEG bands. A 14
electrodes headset was used to record the brain signals while driving in the
pilot study with two subjects and a single dry electrode headset with 13
subjects in the main study. We used a naturalistic driving study as opposed to
a virtual reality driving simulator to perform the distracted driving
maneuvers, consisting of over 100 short duration trials (three to five seconds)
for a subject. We overcame a big challenge in EEG analysis - reducing the
number of electrodes by isolating one electrode (FC5) from 14 electrode
locations to identify certain distractions. Our machine learning methods
achieved a mean accuracy (averaged over the subjects and tasks) of 91.54 +/-
5.23% to detect a distracted driving event and 76.99 +/- 8.63% to distinguish
between the five distraction cases in our study (read, text, call, and
snapshot) using a single electrode. The quantification of distracted driving
detailed in this paper is necessary to guide future policies in road safety.
Our system addresses the safety concerns resulting from driver distraction and
aims to bring about behavioral changes in drivers.","['Garima Bajwa', 'Mohamed Fazeen', 'Ram Dantu']",2019-04-19T07:09:32Z,http://arxiv.org/abs/1904.09100v1,['cs.HC'],"driver distraction,EEG analysis,stimuli-response,brain signals,distraction stimuli,NHTSA study,distraction index,electrodes,naturalistic driving study,machine learning methods"
"Mid-Air Haptics in Aviation -- creating the sensation of touch where
  there is nothing but thin air","The exciting new technology known as mid-air haptics has been adopted by
several industries including Automotive and Entertainment, however it has yet
to emerge in simulated pilot training or in real-life flight decks. Full-flight
simulators are expensive to manufacture, maintain and operate. Not only that,
each simulator is limited to one aircraft type, which is inefficient for the
majority of airlines that have several in service. With the growing trend in
touchscreen instrumentation, cockpit displays require the pilot's attention to
be drawn away from their view out of the window. But by using gesture
recognition interfaces combined with mid-air haptic feedback, we can mitigate
this shortcoming while also adding another dimension to the existing technology
for pilots already familiar with using legacy cockpits, complete with
traditional instrumentation. Meanwhile, simulation environments using augmented
and virtual reality technology offers quality immersive training to the extent
that pilots can go from hundreds of hours of simulated training to being
responsible for hundreds of lives on their very first flight. The software
re-programmability and dynamic richness afforded by mid-air haptic technologies
combined with a basic full-motion platform could allow for an interchange of
instrumentation layouts thus enhancing simulation immersiveness and
environments. Finally, by borrowing and exploring concepts within the
automotive sector, this concept paper presents how flight deck design could
evolve by adopting this technology. If pilot testimony suggests that they can
adapt to virtual objects, can this replace physical controls?","['Alex Girdler', 'Orestis Georgiou']",2020-01-06T09:14:31Z,http://arxiv.org/abs/2001.01445v1,['cs.HC'],"Mid-Air Haptics,Aviation,Full-flight simulators,Touchscreen instrumentation,Gesture recognition interfaces,Cockpit displays,Augmented reality,Virtual reality,Simulation environments,Flight deck design"
"Conceptual Design and Preliminary Results of a VR-based Radiation Safety
  Training System for Interventional Radiologists","Recent studies have reported an increased risk of developing brain and neck
tumors, as well as cataracts, in practitioners in interventional radiology
(IR). Occupational radiation protection in IR has been a top concern for
regulatory agencies and professional societies. To help minimize occupational
radiation exposure in IR, we conceptualized a virtual reality (VR) based
radiation safety training system to help operators understand complex radiation
fields and to avoid high radiation areas through game-like interactive
simulations. The preliminary development of the system has yielded results
suggesting that the training system can calculate and report the radiation
exposure after each training session based on a database precalculated from
computational phantoms and Monte Carlo simulations and the position information
provided in real-time by the MS Hololens headset worn by trainee. In addition,
real-time dose rate and cumulative dose will be displayed to the trainee by MS
Hololens to help them adjust their practice. This paper presents the conceptual
design of the overall hardware and software design, as well as preliminary
results to combine MS HoloLens headset and complex 3D X-ray field spatial
distribution data to create a mixed reality environment for safety training
purpose in IR.","['Yi Guo', 'Li Mao', 'Gongsen Zhang', 'Zhi Chen', 'Xi Pei', 'X. George Xu']",2020-01-14T15:02:47Z,http://arxiv.org/abs/2001.04839v1,"['physics.med-ph', 'cs.HC']","Interventional Radiology,Radiation Safety,Virtual Reality,Occupational Radiation Protection,Monte Carlo simulations,Computational Phantoms,MS HoloLens,Dose Rate,Mixed Reality"
Review: deep learning on 3D point clouds,"Point cloud is point sets defined in 3D metric space. Point cloud has become
one of the most significant data format for 3D representation. Its gaining
increased popularity as a result of increased availability of acquisition
devices, such as LiDAR, as well as increased application in areas such as
robotics, autonomous driving, augmented and virtual reality. Deep learning is
now the most powerful tool for data processing in computer vision, becoming the
most preferred technique for tasks such as classification, segmentation, and
detection. While deep learning techniques are mainly applied to data with a
structured grid, point cloud, on the other hand, is unstructured. The
unstructuredness of point clouds makes use of deep learning for its processing
directly very challenging. Earlier approaches overcome this challenge by
preprocessing the point cloud into a structured grid format at the cost of
increased computational cost or lost of depth information. Recently, however,
many state-of-the-arts deep learning techniques that directly operate on point
cloud are being developed. This paper contains a survey of the recent
state-of-the-art deep learning techniques that mainly focused on point cloud
data. We first briefly discussed the major challenges faced when using deep
learning directly on point cloud, we also briefly discussed earlier approaches
which overcome the challenges by preprocessing the point cloud into a
structured grid. We then give the review of the various state-of-the-art deep
learning approaches that directly process point cloud in its unstructured form.
We introduced the popular 3D point cloud benchmark datasets. And we also
further discussed the application of deep learning in popular 3D vision tasks
including classification, segmentation and detection.","['Saifullahi Aminu Bello', 'Shangshu Yu', 'Cheng Wang']",2020-01-17T12:55:23Z,http://arxiv.org/abs/2001.06280v1,['cs.CV'],"3D point clouds,deep learning,data format,LiDAR,robotics,autonomous driving,augmented reality,virtual reality,classification,segmentation"
Multi-operator Network Sharing for Massive IoT,"Recent study predicts that by 2020 up to 50 billion IoT devices will be
connected to the Internet, straining the capacity of wireless network that has
already been overloaded with data-hungry mobile applications, such as
high-definition video streaming and virtual reality(VR)/augmented reality(AR).
How to accommodate the demand for both massive scale of IoT devices and
high-speed cellular services in the physically limited spectrum without
significantly increasing the operational and infrastructure costs is one of the
main challenges for operators. In this article, we introduce a new
multi-operator network sharing framework that supports the coexistence of IoT
and high-speed cellular services. Our framework is based on the radio access
network (RAN) sharing architecture recently introduced by 3GPP as a promising
solution for operators to improve their resource utilization and reduce the
system roll-out cost. We evaluate the performance of our proposed framework
using the real base station location data in the city of Dublin collected from
two major operators in Ireland. Numerical results show that our proposed
framework can almost double the total number of IoT devices that can be
supported and coexist with other cellular services compared with the case
without network sharing.","['Yong Xiao', 'Marwan Krunz', 'Tao Shu']",2020-01-25T08:16:28Z,http://arxiv.org/abs/2001.09276v1,"['cs.NI', 'cs.IT', 'eess.SP', 'math.IT']","IoT,network sharing,wireless network,radio access network,3GPP,resource utilization,infrastructure costs,base station,cellular services,spectrum"
"Optimal Streaming of 360 VR Videos with Perfect, Imperfect and Unknown
  FoV Viewing Probabilities","In this paper, we investigate wireless streaming of multi-quality tiled 360
virtual reality (VR) videos from a multi-antenna server to multiple
single-antenna users in a multi-carrier system. To capture the impact of
field-of-view (FoV) prediction, we consider three cases of FoV viewing
probability distributions, i.e., perfect, imperfect and unknown FoV viewing
probability distributions, and use the average total utility, worst average
total utility and worst total utility as the respective performance metrics. We
adopt rate splitting with successive decoding for efficient transmission of
multiple sets of tiles of different 360 VR videos to their requesting users. In
each case, we optimize the encoding rates of the tiles, minimum encoding rates
of the FoVs, rates of the common and private messages and transmission
beamforming vectors to maximize the total utility. The problems in the three
cases are all challenging nonconvex optimization problems. We successfully
transform the problem in each case into a difference of convex (DC) programming
problem with a differentiable objective function, and obtain a suboptimal
solution using concave-convex procedure (CCCP). Finally, numerical results
demonstrate the proposed solutions achieve notable gains over existing schemes
in all three cases. To the best of our knowledge, this is the first work
revealing the impact of FoV prediction and its accuracy on the performance of
streaming of multi-quality tiled 360 VR videos.","['Lingzhi Zhao', 'Ying Cui', 'Chengjun Guo', 'Zhi Liu']",2020-09-02T12:14:14Z,http://arxiv.org/abs/2009.01753v1,"['cs.IT', 'math.IT']","streaming,360 VR videos,FoV,viewing probabilities,multi-antenna,multi-carrier system,rate splitting,encoding rates,DC programming"
"Approaches, Challenges, and Applications for Deep Visual Odometry:
  Toward to Complicated and Emerging Areas","Visual odometry (VO) is a prevalent way to deal with the relative
localization problem, which is becoming increasingly mature and accurate, but
it tends to be fragile under challenging environments. Comparing with classical
geometry-based methods, deep learning-based methods can automatically learn
effective and robust representations, such as depth, optical flow, feature,
ego-motion, etc., from data without explicit computation. Nevertheless, there
still lacks a thorough review of the recent advances of deep learning-based VO
(Deep VO). Therefore, this paper aims to gain a deep insight on how deep
learning can profit and optimize the VO systems. We first screen out a number
of qualifications including accuracy, efficiency, scalability, dynamicity,
practicability, and extensibility, and employ them as the criteria. Then, using
the offered criteria as the uniform measurements, we detailedly evaluate and
discuss how deep learning improves the performance of VO from the aspects of
depth estimation, feature extraction and matching, pose estimation. We also
summarize the complicated and emerging areas of Deep VO, such as mobile robots,
medical robots, augmented reality and virtual reality, etc. Through the
literature decomposition, analysis, and comparison, we finally put forward a
number of open issues and raise some future research directions in this field.","['Ke Wang', 'Sai Ma', 'Junlan Chen', 'Fan Ren']",2020-09-06T08:25:23Z,http://arxiv.org/abs/2009.02672v1,"['cs.CV', 'cs.AI', 'cs.RO']","Deep Visual Odometry,Relative Localization,Deep Learning,Depth Estimation,Feature Extraction,Pose Estimation,Mobile Robots,Medical Robots,Augmented Reality,Virtual Reality"
Dynamic Future Net: Diversified Human Motion Generation,"Human motion modelling is crucial in many areas such as computer graphics,
vision and virtual reality. Acquiring high-quality skeletal motions is
difficult due to the need for specialized equipment and laborious manual
post-posting, which necessitates maximizing the use of existing data to
synthesize new data. However, it is a challenge due to the intrinsic motion
stochasticity of human motion dynamics, manifested in the short and long terms.
In the short term, there is strong randomness within a couple frames, e.g. one
frame followed by multiple possible frames leading to different motion styles;
while in the long term, there are non-deterministic action transitions. In this
paper, we present Dynamic Future Net, a new deep learning model where we
explicitly focuses on the aforementioned motion stochasticity by constructing a
generative model with non-trivial modelling capacity in temporal stochasticity.
Given limited amounts of data, our model can generate a large number of
high-quality motions with arbitrary duration, and visually-convincing
variations in both space and time. We evaluate our model on a wide range of
motions and compare it with the state-of-the-art methods. Both qualitative and
quantitative results show the superiority of our method, for its robustness,
versatility and high-quality.","['Wenheng Chen', 'He Wang', 'Yi Yuan', 'Tianjia Shao', 'Kun Zhou']",2020-08-25T02:31:41Z,http://arxiv.org/abs/2009.05109v1,['cs.CV'],"1. Human motion modelling
2. Skeletal motions
3. Data synthesis
4. Motion stochasticity
5. Deep learning model
6. Temporal stochasticity
7. Generative model
8. High-quality motions
9. Spatial variations
10. State-of-the-art methods"
Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,"Gaze tracking is an essential component of next generation displays for
virtual reality and augmented reality applications. Traditional camera-based
gaze trackers used in next generation displays are known to be lacking in one
or multiple of the following metrics: power consumption, cost, computational
complexity, estimation accuracy, latency, and form-factor. We propose the use
of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to
traditional camera-based gaze tracking approaches while taking all of these
metrics into consideration. We begin by developing a rendering-based simulation
framework for understanding the relationship between light sources and a
virtual model eyeball. Findings from this framework are used for the placement
of LEDs and photodiodes. Our first prototype uses a neural network to obtain an
average error rate of 2.67{\deg} at 400Hz while demanding only 16mW. By
simplifying the implementation to using only LEDs, duplexed as light
transceivers, and more minimal machine learning model, namely a light-weight
supervised Gaussian process regression algorithm, we show that our second
prototype is capable of an average error rate of 1.57{\deg} at 250 Hz using 800
mW.","['Richard Li', 'Eric Whitmire', 'Michael Stengel', 'Ben Boudaoud', 'Jan Kautz', 'David Luebke', 'Shwetak Patel', 'Kaan Akşit']",2020-09-15T05:50:13Z,http://arxiv.org/abs/2009.06875v2,"['eess.SY', 'cs.HC', 'cs.SY']","gaze tracking,spatially-sparse single-pixel detectors,photodiodes,light-emitting diodes,neural network,error rate,light transceivers,Gaussian process regression,power consumption,computational complexity"
"Leveraging Local and Global Descriptors in Parallel to Search
  Correspondences for Visual Localization","Visual localization to compute 6DoF camera pose from a given image has wide
applications such as in robotics, virtual reality, augmented reality, etc. Two
kinds of descriptors are important for the visual localization. One is global
descriptors that extract the whole feature from each image. The other is local
descriptors that extract the local feature from each image patch usually
enclosing a key point. More and more methods of the visual localization have
two stages: at first to perform image retrieval by global descriptors and then
from the retrieval feedback to make 2D-3D point correspondences by local
descriptors. The two stages are in serial for most of the methods. This simple
combination has not achieved superiority of fusing local and global
descriptors. The 3D points obtained from the retrieval feedback are as the
nearest neighbor candidates of the 2D image points only by global descriptors.
Each of the 2D image points is also called a query local feature when
performing the 2D-3D point correspondences. In this paper, we propose a novel
parallel search framework, which leverages advantages of both local and global
descriptors to get nearest neighbor candidates of a query local feature.
Specifically, besides using deep learning based global descriptors, we also
utilize local descriptors to construct random tree structures for obtaining
nearest neighbor candidates of the query local feature. We propose a new
probabilistic model and a new deep learning based local descriptor when
constructing the random trees. A weighted Hamming regularization term to keep
discriminativeness after binarization is given in the loss function for the
proposed local descriptor. The loss function co-trains both real and binary
descriptors of which the results are integrated into the random trees.","['Pengju Zhang', 'Yihong Wu', 'Bingxi Liu']",2020-09-23T01:49:03Z,http://arxiv.org/abs/2009.10891v1,['cs.CV'],"Visual localization,Global descriptors,Local descriptors,Image retrieval,2D-3D point correspondences,Deep learning,Random trees,Nearest neighbor candidates,Probabilistic model,Hamming regularization term"
"Controlling wheelchairs by body motions: A learning framework for the
  adaptive remapping of space","Learning to operate a vehicle is generally accomplished by forming a new
cognitive map between the body motions and extrapersonal space. Here, we
consider the challenge of remapping movement-to-space representations in
survivors of spinal cord injury, for the control of powered wheelchairs. Our
goal is to facilitate this remapping by developing interfaces between residual
body motions and navigational commands that exploit the degrees of freedom that
disabled individuals are most capable to coordinate. We present a new framework
for allowing spinal cord injured persons to control powered wheelchairs through
signals derived from their residual mobility. The main novelty of this approach
lies in substituting the more common joystick controllers of powered
wheelchairs with a sensor shirt. This allows the whole upper body of the user
to operate as an adaptive joystick. Considerations about learning and risks
have lead us to develop a safe testing environment in 3D Virtual Reality. A
Personal Augmented Reality Immersive System (PARIS) allows us to analyse
learning skills and provide users with an adequate training to control a
simulated wheelchair through the signals generated by body motions in a safe
environment. We provide a description of the basic theory, of the development
phases and of the operation of the complete system. We also present preliminary
results illustrating the processing of the data and supporting of the
feasibility of this approach.","['Tauseef Gulrez', 'Alessandro Tognetti', 'Alon Fishbach', 'Santiago Acosta', 'Christopher Scharver', 'Danilo De Rossi', 'Ferdinando A. Mussa-Ivaldi']",2011-07-27T05:30:40Z,http://arxiv.org/abs/1107.5387v1,"['cs.RO', 'cs.AI', 'cs.NE']","learning framework,adaptive remapping,spinal cord injury,powered wheelchairs,sensor shirt,joystick controllers,3D Virtual Reality,Personal Augmented Reality Immersive System,feasibility"
SemanticPaint: A Framework for the Interactive Segmentation of 3D Scenes,"We present an open-source, real-time implementation of SemanticPaint, a
system for geometric reconstruction, object-class segmentation and learning of
3D scenes. Using our system, a user can walk into a room wearing a depth camera
and a virtual reality headset, and both densely reconstruct the 3D scene and
interactively segment the environment into object classes such as 'chair',
'floor' and 'table'. The user interacts physically with the real-world scene,
touching objects and using voice commands to assign them appropriate labels.
These user-generated labels are leveraged by an online random forest-based
machine learning algorithm, which is used to predict labels for previously
unseen parts of the scene. The entire pipeline runs in real time, and the user
stays 'in the loop' throughout the process, receiving immediate feedback about
the progress of the labelling and interacting with the scene as necessary to
refine the predicted segmentation.","['Stuart Golodetz', 'Michael Sapienza', 'Julien P. C. Valentin', 'Vibhav Vineet', 'Ming-Ming Cheng', 'Anurag Arnab', 'Victor A. Prisacariu', 'Olaf Kähler', 'Carl Yuheng Ren', 'David W. Murray', 'Shahram Izadi', 'Philip H. S. Torr']",2015-10-13T15:06:03Z,http://arxiv.org/abs/1510.03727v1,"['cs.CV', 'I.2.10']","interactive segmentation,3D scenes,SemanticPaint,geometric reconstruction,object-class segmentation,learning,depth camera,virtual reality,machine learning,random forest-based algorithm"
Adaptive 360 VR Video Streaming: Divide and Conquer!,"While traditional multimedia applications such as games and videos are still
popular, there has been a significant interest in the recent years towards new
3D media such as 3D immersion and Virtual Reality (VR) applications, especially
360 VR videos. 360 VR video is an immersive spherical video where the user can
look around during playback. Unfortunately, 360 VR videos are extremely
bandwidth intensive, and therefore are difficult to stream at acceptable
quality levels. In this paper, we propose an adaptive bandwidth-efficient 360
VR video streaming system using a divide and conquer approach. In our approach,
we propose a dynamic view-aware adaptation technique to tackle the huge
streaming bandwidth demands of 360 VR videos. We spatially divide the videos
into multiple tiles while encoding and packaging, use MPEG-DASH SRD to describe
the spatial relationship of tiles in the 360-degree space, and prioritize the
tiles in the Field of View (FoV). In order to describe such tiled
representations, we extend MPEG-DASH SRD to the 3D space of 360 VR videos. We
spatially partition the underlying 3D mesh, and construct an efficient 3D
geometry mesh called hexaface sphere to optimally represent a tiled 360 VR
video in the 3D space. Our initial evaluation results report up to 72%
bandwidth savings on 360 VR video streaming with minor negative quality impacts
compared to the baseline scenario when no adaptations is applied.","['Mohammad Hosseini', 'Viswanathan Swaminathan']",2016-09-28T02:07:12Z,http://arxiv.org/abs/1609.08729v5,"['cs.MM', 'cs.GR', 'cs.NI']","Adaptive,360 VR Video Streaming,Divide and Conquer,3D media,Virtual Reality,Spatial division,MPEG-DASH SRD,Field of View,3D mesh,Bandwidth savings"
"Augmenting the thermal flux experiment: a mixed reality approach with
  the HoloLens","In the field of Virtual Reality (VR) and Augmented Reality (AR) technologies
have made huge progress during the last years and also reached the field of
education. The virtuality continuum, ranging from pure virtuality on one side
to the real world on the other has been successfully covered by the use of
immersive technologies like head-mounted displays, which allow to embed virtual
objects into the real surroundings, leading to a Mixed Reality (MR) experience.
In such an environment digital and real objects do not only co-exist, but
moreover are also able to interact with each other in real-time. These concepts
can be used to merge human perception of reality with digitally visualized
sensor data and thereby making the invisible visible. As a first example, in
this paper we introduce alongside the basic idea of this column an
MR-experiment in thermodynamics for a laboratory course for freshman students
in physics or other science and engineering subjects which uses physical data
from mobile devices for analyzing and displaying physical phenomena to
students.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-09-05T11:52:18Z,http://arxiv.org/abs/1709.01342v1,['physics.ed-ph'],"Virtual Reality,Augmented Reality,mixed reality,HoloLens,immersive technologies,head-mounted displays,sensor data,thermodynamics,laboratory course,physical phenomena"
"Complete End-To-End Low Cost Solution To a 3D Scanning System with
  Integrated Turntable","3D reconstruction is a technique used in computer vision which has a wide
range of applications in areas like object recognition, city modelling, virtual
reality, physical simulations, video games and special effects. Previously, to
perform a 3D reconstruction, specialized hardwares were required. Such systems
were often very expensive and was only available for industrial or research
purpose. With the rise of the availability of high-quality low cost 3D sensors,
it is now possible to design inexpensive complete 3D scanning systems. The
objective of this work was to design an acquisition and processing system that
can perform 3D scanning and reconstruction of objects seamlessly. In addition,
the goal of this work also included making the 3D scanning process fully
automated by building and integrating a turntable alongside the software. This
means the user can perform a full 3D scan only by a press of a few buttons from
our dedicated graphical user interface. Three main steps were followed to go
from acquisition of point clouds to the finished reconstructed 3D model. First,
our system acquires point cloud data of a person/object using inexpensive
camera sensor. Second, align and convert the acquired point cloud data into a
watertight mesh of good quality. Third, export the reconstructed model to a 3D
printer to obtain a proper 3D print of the model.","['Saed Khawaldeh', 'Tajwar Abrar Aleef', 'Usama Pervaiz', 'Vu Hoang Minh', 'Yeman Brhane Hagos']",2017-09-03T13:40:23Z,http://arxiv.org/abs/1709.02247v1,['cs.CV'],"3D reconstruction,computer vision,low cost,3D scanning system,turntable,point cloud data,mesh,graphical user interface,3D printer"
"APPD: Adaptive and Precise Pupil Boundary Detection using Entropy of
  Contour Gradients","Eye tracking spreads through a vast area of applications from ophthalmology,
assistive technologies to gaming and virtual reality. Precisely detecting the
pupil's contour and center is the very first step in many of these tasks, hence
needs to be performed accurately. Although detection of pupil is a simple
problem when it is entirely visible; occlusions and oblique view angles
complicate the solution. In this study, we propose APPD, an adaptive and
precise pupil boundary detection method that is able to infer whether entire
pupil is in clearly visible by a heuristic that estimates the shape of a
contour in a computationally efficient way. Thus, a faster detection is
performed with the assumption of no occlusions. If the heuristic fails, a more
comprehensive search among extracted image features is executed to maintain
accuracy. Furthermore, the algorithm can find out if there is no pupil as an
helpful information for many applications. We provide a dataset containing 3904
high resolution eye images collected from 12 subjects and perform an extensive
set of experiments to obtain quantitative results in terms of accuracy,
localization and timing. The proposed method outperforms three other state of
the art algorithms and has an average execution time $\sim$5 ms in
single-thread on a standard laptop computer for 720p images.","['Cihan Topal', 'Halil Ibrahim Cakir', 'Cuneyt Akinlar']",2017-09-19T12:09:34Z,http://arxiv.org/abs/1709.06366v2,['cs.CV'],"Adaptive,Precise,Pupil Boundary Detection,Entropy,Contour Gradients,Heuristic,Occlusions,Image Features,Dataset,Execution Time"
Multi-level Chaotic Maps for 3D Textured Model Encryption,"With rapid progress of Virtual Reality and Augmented Reality technologies, 3D
contents are the next widespread media in many applications. Thus, the
protection of 3D models is primarily important. Encryption of 3D models is
essential to maintain confidentiality. Previous work on encryption of 3D
surface model often consider the point clouds, the meshes and the textures
individually. In this work, a multi-level chaotic maps models for 3D textured
encryption was presented by observing the different contributions for
recognizing cipher 3D models between vertices (point cloud), polygons and
textures. For vertices which make main contribution for recognizing, we use
high level 3D Lu chaotic map to encrypt them. For polygons and textures which
make relatively smaller contributions for recognizing, we use 2D Arnold's cat
map and 1D Logistic map to encrypt them, respectively. The experimental results
show that our method can get similar performance with the other method use the
same high level chaotic map for point cloud, polygons and textures, while we
use less time. Besides, our method can resist more method of attacks such as
statistic attack, brute-force attack, correlation attack.","['Xin Jin', 'Shuyun Zhu', 'Le Wu', 'Geng Zhao', 'Xiaodong Li', 'Quan Zhou', 'Huimin Lu']",2017-09-25T08:20:17Z,http://arxiv.org/abs/1709.08364v2,"['cs.CV', 'cs.CR']","3D models,Encryption,Chaotic maps,Textured model,Virtual Reality,Augmented Reality,Point clouds,Meshes,Vertices,Polygons"
"Automatic Error Analysis of Human Motor Performance for Interactive
  Coaching in Virtual Reality","In the context of fitness coaching or for rehabilitation purposes, the motor
actions of a human participant must be observed and analyzed for errors in
order to provide effective feedback. This task is normally carried out by human
coaches, and it needs to be solved automatically in technical applications that
are to provide automatic coaching (e.g. training environments in VR). However,
most coaching systems only provide coarse information on movement quality, such
as a scalar value per body part that describes the overall deviation from the
correct movement. Further, they are often limited to static body postures or
rather simple movements of single body parts. While there are many approaches
to distinguish between different types of movements (e.g., between walking and
jumping), the detection of more subtle errors in a motor performance is less
investigated. We propose a novel approach to classify errors in sports or
rehabilitation exercises such that feedback can be delivered in a rapid and
detailed manner: Homogeneous sub-sequences of exercises are first temporally
aligned via Dynamic Time Warping. Next, we extract a feature vector from the
aligned sequences, which serves as a basis for feature selection using Random
Forests. The selected features are used as input for Support Vector Machines,
which finally classify the movement errors. We compare our algorithm to a well
established state-of-the-art approach in time series classification, 1-Nearest
Neighbor combined with Dynamic Time Warping, and show our algorithm's
superiority regarding classification quality as well as computational cost.","['Felix Hülsmann', 'Stefan Kopp', 'Mario Botsch']",2017-09-26T17:01:32Z,http://arxiv.org/abs/1709.09131v1,['cs.AI'],"human motor performance,interactive coaching,virtual reality,error analysis,feedback,automatic coaching,training environments,feature selection,Support Vector Machines"
Holoscopic 3D Micro-Gesture Database for Wearable Device Interaction,"With the rapid development of augmented reality (AR) and virtual reality (VR)
technology, human-computer interaction (HCI) has been greatly improved for
gaming interaction of AR and VR control. The finger micro-gesture is one of the
important interactive methods for HCI applications such as in the Google Soli
and Microsoft Kinect projects. However, the progress in this research is slow
due to the lack of high quality public available database. In this paper,
holoscopic 3D camera is used to capture high quality micro-gesture images and a
new unique holoscopic 3D micro-gesture (HoMG) database is produced. The
principle of the holoscopic 3D camera is based on the fly viewing system to see
the objects. HoMG database recorded the image sequence of 3 conventional
gestures from 40 participants under different settings and conditions. For the
purpose of micro-gesture recognition, HoMG has a video subset with 960 videos
and a still image subset with 30635 images. Initial micro-gesture recognition
on both subsets has been conducted using traditional 2D image and video
features and popular classifiers and some encouraging performance has been
achieved. The database will be available for the research communities and speed
up the research in this area.","['Yi Liu', 'Hongying Meng', 'Mohammad Rafiq Swash', 'Yona Falinie A. Gaus', 'Rui Qin']",2017-12-15T07:49:04Z,http://arxiv.org/abs/1712.05570v2,"['cs.HC', '68U99']","holoscopic 3D,micro-gesture,database,wearable device,interaction,augmented reality,virtual reality,human-computer interaction,HCI,recognition"
"The ParallelEye Dataset: Constructing Large-Scale Artificial Scenes for
  Traffic Vision Research","Video image datasets are playing an essential role in design and evaluation
of traffic vision algorithms. Nevertheless, a longstanding inconvenience
concerning image datasets is that manually collecting and annotating
large-scale diversified datasets from real scenes is time-consuming and prone
to error. For that virtual datasets have begun to function as a proxy of real
datasets. In this paper, we propose to construct large-scale artificial scenes
for traffic vision research and generate a new virtual dataset called
""ParallelEye"". First of all, the street map data is used to build 3D scene
model of Zhongguancun Area, Beijing. Then, the computer graphics, virtual
reality, and rule modeling technologies are utilized to synthesize large-scale,
realistic virtual urban traffic scenes, in which the fidelity and geography
match the real world well. Furthermore, the Unity3D platform is used to render
the artificial scenes and generate accurate ground-truth labels, e.g.,
semantic/instance segmentation, object bounding box, object tracking, optical
flow, and depth. The environmental conditions in artificial scenes can be
controlled completely. As a result, we present a viable implementation pipeline
for constructing large-scale artificial scenes for traffic vision research. The
experimental results demonstrate that this pipeline is able to generate
photorealistic virtual datasets with low modeling time and high accuracy
labeling.","['Xuan Li', 'Kunfeng Wang', 'Yonglin Tian', 'Lan Yan', 'Fei-Yue Wang']",2017-12-22T11:16:19Z,http://arxiv.org/abs/1712.08394v1,['cs.CV'],"large-scale artificial scenes,traffic vision research,dataset,computer graphics,virtual reality,rule modeling technologies,Unity3D platform,ground-truth labels,object tracking,optical flow"
"Can Autism be Catered with Artificial Intelligence-Assisted Intervention
  Technology? A Literature Review","This article presents an extensive literature review of technology based
intervention methodologies for individuals facing Autism Spectrum Disorder
(ASD). Reviewed methodologies include: contemporary Computer Aided Systems
(CAS), Computer Vision Assisted Technologies (CVAT) and Virtual Reality (VR) or
Artificial Intelligence (AI)-Assisted interventions. The research over the past
decade has provided enough demonstrations that individuals with ASD have a
strong interest in technology based interventions, which are useful in both,
clinical settings as well as at home and classrooms. Despite showing great
promise, research in developing an advanced technology based intervention that
is clinically quantitative for ASD is minimal. Moreover, the clinicians are
generally not convinced about the potential of the technology based
interventions due to non-empirical nature of published results. A major reason
behind this lack of acceptability is that a vast majority of studies on
distinct intervention methodologies do not follow any specific standard or
research design. We conclude from our findings that there remains a gap between
the research community of computer science, psychology and neuroscience to
develop an AI assisted intervention technology for individuals suffering from
ASD. Following the development of a standardized AI based intervention
technology, a database needs to be developed, to devise effective AI
algorithms.","['Muhammad Shoaib Jaliawala', 'Rizwan Ahmed Khan']",2018-03-14T09:56:39Z,http://arxiv.org/abs/1803.05181v5,"['cs.HC', 'cs.AI', 'cs.LG']","Autism Spectrum Disorder (ASD),Computer Aided Systems (CAS),Computer Vision Assisted Technologies (CVAT),Virtual Reality (VR),Artificial Intelligence (AI),intervention methodologies,technology based interventions,AI algorithms,standardized intervention technology,research design"
"FastTrack: Minimizing Stalls for CDN-based Over-the-top Video Streaming
  Systems","Traffic for internet video streaming has been rapidly increasing and is
further expected to increase with the higher definition videos and IoT
applications, such as 360 degree videos and augmented virtual reality
applications. While efficient management of heterogeneous cloud resources to
optimize the quality of experience is important, existing work in this problem
space often left out important factors. In this paper, we present a model for
describing a today's representative system architecture for video streaming
applications, typically composed of a centralized origin server and several CDN
sites. Our model comprehensively considers the following factors: limited
caching spaces at the CDN sites, allocation of CDN for a video request, choice
of different ports from the CDN, and the central storage and bandwidth
allocation. With the model, we focus on minimizing a performance metric, stall
duration tail probability (SDTP), and present a novel, yet efficient, algorithm
to solve the formulated optimization problem. The theoretical bounds with
respect to the SDTP metric are also analyzed and presented. Our extensive
simulation results demonstrate that the proposed algorithms can significantly
improve the SDTP metric, compared to the baseline strategies. Small-scale video
streaming system implementation in a real cloud environment further validates
our results.","['Abubakr Alabbasi', 'Vaneet Aggarwal', 'Tian Lan', 'Yu Xiang', 'Moo-Ryong Ra', 'Yih-Farn R. Chen']",2018-06-30T10:24:28Z,http://arxiv.org/abs/1807.01147v1,"['cs.NI', 'cs.DC', 'cs.MM', 'cs.SY']","CDN,video streaming,IoT applications,cloud resources,caching,optimization,bandwidth allocation,algorithm,simulation results"
"Two at Once: Enhancing Learning and Generalization Capacities via
  IBN-Net","Convolutional neural networks (CNNs) have achieved great successes in many
computer vision problems. Unlike existing works that designed CNN architectures
to improve performance on a single task of a single domain and not
generalizable, we present IBN-Net, a novel convolutional architecture, which
remarkably enhances a CNN's modeling ability on one domain (e.g. Cityscapes) as
well as its generalization capacity on another domain (e.g. GTA5) without
finetuning. IBN-Net carefully integrates Instance Normalization (IN) and Batch
Normalization (BN) as building blocks, and can be wrapped into many advanced
deep networks to improve their performances. This work has three key
contributions. (1) By delving into IN and BN, we disclose that IN learns
features that are invariant to appearance changes, such as colors, styles, and
virtuality/reality, while BN is essential for preserving content related
information. (2) IBN-Net can be applied to many advanced deep architectures,
such as DenseNet, ResNet, ResNeXt, and SENet, and consistently improve their
performance without increasing computational cost. (3) When applying the
trained networks to new domains, e.g. from GTA5 to Cityscapes, IBN-Net achieves
comparable improvements as domain adaptation methods, even without using data
from the target domain. With IBN-Net, we won the 1st place on the WAD 2018
Challenge Drivable Area track, with an mIoU of 86.18%.","['Xingang Pan', 'Ping Luo', 'Jianping Shi', 'Xiaoou Tang']",2018-07-25T05:51:15Z,http://arxiv.org/abs/1807.09441v3,['cs.CV'],"Convolutional neural networks,IBN-Net,Instance Normalization,Batch Normalization,Domain adaptation,Computer vision,CNN architectures,Generalization capacity,Deep networks,Domain transfer"
"The Effects of Visual and Control Latency on Piloting a Quadcopter using
  a Head-Mounted Display","Recent research has proposed teleoperation of robotic and aerial vehicles
using head motion tracked by a head-mounted display (HMD). First-person views
of the vehicles are usually captured by onboard cameras and presented to users
through the display panels of HMDs. This provides users with a direct,
immersive and intuitive interface for viewing and control. However, a typically
overlooked factor in such designs is the latency introduced by the vehicle
dynamics. As head motion is coupled with visual updates in such applications,
visual and control latency always exists between the issue of control commands
by head movements and the visual feedback received at the completion of the
attitude adjustment. This causes a discrepancy between the intended motion, the
vestibular cue and the visual cue and may potentially result in simulator
sickness. No research has been conducted on how various levels of visual and
control latency introduced by dynamics in robots or aerial vehicles affect
users' performance and the degree of simulator sickness elicited. Thus, it is
uncertain how much performance is degraded by latency and whether such designs
are comfortable from the perspective of users. To address these issues, we
studied a prototyped scenario of a head motion controlled quadcopter using an
HMD. We present a virtual reality (VR) paradigm to systematically assess the
effects of visual and control latency in simulated drone control scenarios.","['Jingbo Zhao', 'Robert S. Allison', 'Margarita Vinnikov', 'Sion Jennings']",2018-07-29T23:18:54Z,http://arxiv.org/abs/1807.11123v3,['cs.HC'],"visual latency,control latency,piloting,quadcopter,head-mounted display,teleoperation,robotic,aerial vehicles,simulator sickness,virtual reality"
"AniCode: Authoring Coded Artifacts for Network-Free Personalized
  Animations","Time-based media (videos, synthetic animations, and virtual reality
experiences) are used for communication, in applications such as manufacturers
explaining the operation of a new appliance to consumers and scientists
illustrating the basis of a new conclusion. However, authoring time-based media
that are effective and personalized for the viewer remains a challenge. We
introduce AniCode, a novel framework for authoring and consuming time-based
media. An author encodes a video animation in a printed code, and affixes the
code to an object. A consumer uses a mobile application to capture an image of
the object and code, and to generate a video presentation on the fly.
Importantly, AniCode presents the video personalized in the consumer's visual
context. Our system is designed to be low cost and easy to use. By not
requiring an internet connection, and through animations that decode correctly
only in the intended context, AniCode enhances privacy of communication using
time-based media. Animation schemes in the system include a series of 2D and 3D
geometric transformations, color transformation, and annotation. We demonstrate
the AniCode framework with sample applications from a wide range of domains,
including product ""how to"" examples, cultural heritage, education, creative
art, and design. We evaluate the ease of use and effectiveness of our system
with a user study.","['Zeyu Wang', 'Shiyu Qiu', 'Qingyang Chen', 'Alexander Ringlein', 'Julie Dorsey', 'Holly Rushmeier']",2018-07-31T01:38:27Z,http://arxiv.org/abs/1807.11627v1,['cs.GR'],"AniCode,Coded Artifacts,Network-Free,Personalized Animations,Time-based Media,Authoring,Video Animation,Mobile Application,Privacy,Animation Schemes"
"Energy-Efficient Mobile-Edge Computation Offloading for Applications
  with Shared Data","Mobile-edge computation offloading (MECO) has been recognized as a promising
solution to alleviate the burden of resource-limited Internet of Thing (IoT)
devices by offloading computation tasks to the edge of cellular networks (also
known as {\em cloudlet}). Specifically, latency-critical applications such as
virtual reality (VR) and augmented reality (AR) have inherent collaborative
properties since part of the input/output data are shared by different users in
proximity. In this paper, we consider a multi-user fog computing system, in
which multiple single-antenna mobile users running applications featuring
shared data can choose between (partially) offloading their individual tasks to
a nearby single-antenna cloudlet for remote execution and performing pure local
computation. The mobile users' energy minimization is formulated as a convex
problem, subject to the total computing latency constraint, the total energy
constraints for individual data downloading, and the computing frequency
constraints for local computing, for which classical Lagrangian duality can be
applied to find the optimal solution. Based upon the semi-closed form solution,
the shared data proves to be transmitted by only one of the mobile users
instead of multiple ones. Besides, compared to those baseline algorithms
without considering the shared data property or the mobile users' local
computing capabilities, the proposed joint computation offloading and
communications resource allocation provides significant energy saving.","['Xiangyu He', 'Hong Xing', 'Yue Chen', 'Arumugam Nallanathan']",2018-09-04T13:52:13Z,http://arxiv.org/abs/1809.00966v1,"['cs.IT', 'math.IT']","Energy-Efficient,Mobile-Edge Computation Offloading,Shared Data,Internet of Things (IoT),Fog Computing,Cloudlet,Lagrangian Duality,Computing Latency,Energy Minimization,Virtual Reality (VR)"
"Usability of the Size, Spacing, and Depth of Virtual Buttons on
  Head-Mounted Displays","Virtual reality (VR) allows users to see and manipulate virtual scenes and
items through input devices, like head-mounted displays. In this study, the
effects of button size, spacing, and depth on the usability of virtual buttons
in VR environments were investigated. Task completion time, number of errors,
and subjective preferences were collected to test different levels of the
button size, spacing, and depth. The experiment was conducted in a desktop
setting with Oculus Rift and Leap motion. A total of 18 subjects performed a
button selection task. The optimal levels of button size and spacing within the
experimental conditions are 25 mm and between 5 mm and 9 mm, respectively.
Button sizes of 15 mm with 1-mm spacing were too small to be used in VR
environments. A trend of decreasing task completion time and the number of
errors was observed as button size and spacing increased. However, large size
and spacing may cause fatigue, due to continuous extension of the arms. For
depth effects, the touch method took a shorter task completion time. However,
the push method recorded a smaller number of errors, owing to the visual
push-feedback. In this paper, we discuss advantages and disadvantages in
detail. The results can be applied to many different application areas with VR
HMD.","['Kyudong Park', 'Dohyeon Kim', 'Sung H. Han']",2018-09-16T08:01:11Z,http://arxiv.org/abs/1809.05833v1,['cs.HC'],"virtual reality,button size,spacing,depth,head-mounted displays,usability,task completion time,errors,subjective preferences,Oculus Rift"
The Impact of Correlated Blocking on Millimeter-Wave Personal Networks,"Due to its potential to support high data rates at low latency with
reasonable interference isolation, millimeter-wave (mmWave) communications has
emerged as a promising solution for wireless personal-area networks (WPAN) and
an enabler for emerging applications such as high-resolution untethered virtual
reality. At mmWave, signals are prone to blockage by objects in the
environment, including human bodies. Most mmWave systems utilize directional
antennas in order to overcome the significant path loss. In this paper, we
consider the effects of blockage and antenna directivity on the performance of
a mmWave WPAN. Similar to related work, we assume that the interferers are in
arbitrary locations and the blockages are drawn from a random point process.
However, unlike related work that assumes independent blocking, we carefully
account for the possibility of correlated blocking, which arises when two
interferers are close to each other and therefore an obstruction that blocks
the first interferer may likely block the second interferer. Closed form
expressions for the blockage correlation coefficient and the distribution of
the SINR are provided for the case of two dominant interferers and a fixed
number of blockages drawn from a binomial point process. Finally, the effects
of antenna directivity and the spatial randomness of the interferers are taken
into account, resulting in SINR curves that fully account for correlated
blocking, which are compared against curves that neglect correlation. The
results provide insight into the validity of the commonly held assumption of
independent blocking and the improved accuracy that can be obtained when the
blocking correlation is taken into account.","['Enass Hriba', 'Matthew C. Valenti']",2018-09-22T02:53:01Z,http://arxiv.org/abs/1809.08372v1,"['cs.IT', 'math.IT']","Millimeter-wave communications,WPAN,Interference isolation,Directional antennas,Path loss,Blockage,Antenna directivity,Interferers,SINR,Correlated blocking"
"gpuRIR: A Python Library for Room Impulse Response Simulation with GPU
  Acceleration","The Image Source Method (ISM) is one of the most employed techniques to
calculate acoustic Room Impulse Responses (RIRs), however, its computational
complexity grows fast with the reverberation time of the room and its
computation time can be prohibitive for some applications where a huge number
of RIRs are needed. In this paper, we present a new implementation that
dramatically improves the computation speed of the ISM by using Graphic
Processing Units (GPUs) to parallelize both the simulation of multiple RIRs and
the computation of the images inside each RIR. Additional speedups were
achieved by exploiting the mixed precision capabilities of the newer GPUs and
by using lookup tables. We provide a Python library under GNU license that can
be easily used without any knowledge about GPU programming and we show that it
is about 100 times faster than other state of the art CPU libraries. It may
become a powerful tool for many applications that need to perform a large
number of acoustic simulations, such as training machine learning systems for
audio signal processing, or for real-time room acoustics simulations for
immersive multimedia systems, such as augmented or virtual reality.","['David Diaz-Guerra', 'Antonio Miguel', 'Jose R. Beltran']",2018-10-26T15:05:04Z,http://arxiv.org/abs/1810.11359v4,"['eess.AS', 'cs.SD']","GPU acceleration,Room Impulse Response,Image Source Method,Computational complexity,Graphic Processing Units,Parallelize,Mixed precision,Lookup tables,Python library,Acoustic simulations"
C2TCP: A Flexible Cellular TCP to Meet Stringent Delay Requirements,"Since current widely available network protocols/systems are mainly
throughput-oriented designs, meeting stringent delay requirements of new
applications such as virtual reality and vehicle-to-vehicle communications on
cellular network requires new network protocol/system designs. C2TCP is an
effort toward that new design direction.
  C2TCP is inspired by in-network active queue management (AQM) designs such as
RED and CoDel and motivated by lack of a flexible end-to-end (e2e) approach
which can adapt itself to different applications' QoS requirements without
modifying any network devices. It copes with unique challenges in cellular
networks for achieving ultra-low latency (including highly variable channels,
deep per-user buffers, self-inflicted queuing delays, radio uplink/downlink
scheduling delays) and intends to satisfy stringent delay requirements of
different applications while maximizing the throughput. C2TCP works on top of
classic throughput-oriented TCP and accommodates various target delays without
requiring any channel prediction, network state profiling, or complicated rate
adjustment mechanisms.
  We have evaluated C2TCP in both real-world environment and extensive
trace-based emulations and compared its performance with different TCP variants
and state-of-the-art schemes including PCC-Vivace, Google's BBR, Verus, Sprout,
TCP Westwood, and Cubic. Results show that C2TCP outperforms all these schemes
and achieves lower average delay, jitter, and 95th percentile delay for
packets.","['Soheil Abbasloo', 'Yang Xu', 'H. Jonathan Chao']",2018-10-29T19:09:05Z,http://arxiv.org/abs/1810.13241v2,['cs.NI'],"cellular network,TCP,delay requirements,network protocol,active queue management,latency,throughput,QoS,channel prediction,TCP variants"
"Multi-tier Caching Analysis in CDN-based Over-the-top Video Streaming
  Systems","Internet video traffic has been been rapidly increasing and is further
expected to increase with the emerging 5G applications such as higher
definition videos, IoT and augmented/virtual reality applications. As end-users
consume video in massive amounts and in an increasing number of ways, the
content distribution network (CDN) should be efficiently managed to improve the
system efficiency. The streaming service can include multiple caching tiers, at
the distributed servers and the edge routers, and efficient content management
at these locations affect the quality of experience (QoE) of the end users. In
this paper, we propose a model for video streaming systems, typically composed
of a centralized origin server, several CDN sites, and edge-caches located
closer to the end user. We comprehensively consider different systems design
factors including the limited caching space at the CDN sites, allocation of CDN
for a video request, choice of different ports (or paths) from the CDN and the
central storage, bandwidth allocation, the edge-cache capacity, and the caching
policy. We focus on minimizing a performance metric, stall duration tail
probability (SDTP), and present a novel and efficient algorithm accounting for
the multiple design flexibilities. The theoretical bounds with respect to the
SDTP metric are also analyzed and presented. The implementation on a
virtualized cloud system managed by Openstack demonstrate that the proposed
algorithms can significantly improve the SDTP metric, compared to the baseline
strategies.","['Abubakr O. Al-Abbasi', 'Vaneet Aggarwal', 'Moo-Ryong Ra']",2019-02-11T01:23:00Z,http://arxiv.org/abs/1902.04067v1,"['cs.NI', 'cs.MM', 'cs.PF']","CDN,caching,video streaming,edge routers,content management,QoE,caching tiers,caching policy,Openstack,SDTP"
"Personalized On-line Adaptation of Kinematic Synergies for
  Human-Prosthesis Interfaces","Synergies have been adopted in prosthetic limb applications to reduce
complexity of design, but typically involve a single synergy setting for a
population and ignore individual preference or adaptation capacity. However,
personalization of the synergy setting is necessary for the effective operation
of the prosthetic device. Two major challenges hinder the personalization of
synergies in human-prosthesis interfaces. The first is related to the process
of human motor adaptation and the second to the variation in motor learning
dynamics of individuals. In this paper, a systematic personalization of
kinematic synergies for human-prosthesis interfaces using on-line measurements
from each individual is proposed. The task of reaching using the upper-limb is
described by an objective function and the interface is parameterized by a
kinematic synergy. Consequently, personalizing the interface for a given
individual can be formulated as finding an optimal personalized parameter. A
structure to model the observed motor behavior that allows for the personalized
traits of motor preference and motor learning is proposed, and subsequently
used in an on-line optimization scheme to identify the synergies for an
individual. The knowledge of the common features contained in the model enables
on-line adaptation of the human-prosthesis interface to happen concurrently to
human motor adaptation without the need to re-tune the personalization
algorithm for each individual. Human-in-the-loop experimental results with
able-bodied subjects, performed in a virtual reality environment to emulate
amputation and prosthesis use, show that the proposed personalization algorithm
was effective in obtaining optimal synergies with a fast uniform convergence
speed across a group of individuals.","['Ricardo Garcia-Rosas', 'Ying Tan', 'Denny Oetomo', 'Chris Manzie', 'Peter Choong']",2019-02-19T22:06:30Z,http://arxiv.org/abs/1902.07313v2,['cs.RO'],"kinematic synergies,personalization,prosthetic device,human motor adaptation,motor learning dynamics,on-line measurements,upper-limb reaching,parameterized interface,motor behavior model,optimization scheme"
Parallel Rendering and Large Data Visualization,"We are living in the big data age: An ever increasing amount of data is being
produced through data acquisition and computer simulations. While large scale
analysis and simulations have received significant attention for cloud and
high-performance computing, software to efficiently visualise large data sets
is struggling to keep up.
  Visualization has proven to be an efficient tool for understanding data, in
particular visual analysis is a powerful tool to gain intuitive insight into
the spatial structure and relations of 3D data sets. Large-scale visualization
setups are becoming ever more affordable, and high-resolution tiled display
walls are in reach even for small institutions. Virtual reality has arrived in
the consumer space, making it accessible to a large audience.
  This thesis addresses these developments by advancing the field of parallel
rendering. We formalise the design of system software for large data
visualization through parallel rendering, provide a reference implementation of
a parallel rendering framework, introduce novel algorithms to accelerate the
rendering of large amounts of data, and validate this research and development
with new applications for large data visualization. Applications built using
our framework enable domain scientists and large data engineers to better
extract meaning from their data, making it feasible to explore more data and
enabling the use of high-fidelity visualization installations to see more
detail of the data.",['Stefan Eilemann'],2019-02-23T08:44:59Z,http://arxiv.org/abs/1902.08755v1,"['cs.GR', 'cs.PF', 'I.3.2.a; I.3.7.g; I.3.8; I.3.6.a; I.6.9.f']","Parallel rendering,Large data visualization,System software,Rendering framework,Algorithms,Data visualization,Virtual reality,High-fidelity visualization,Cloud computing,High-performance computing"
scenery: Flexible Virtual Reality Visualization on the Java VM,"Life science today involves computational analysis of a large amount and
variety of data, such as volumetric data acquired by state-of-the-art
microscopes, or mesh data from analysis of such data or simulations.
Visualization is often the first step in making sense of data, and a crucial
part of building and debugging analysis pipelines. It is therefore important
that visualizations can be quickly prototyped, as well as developed or embedded
into full applications. In order to better judge spatiotemporal relationships,
immersive hardware, such as Virtual or Augmented Reality (VR/AR) headsets and
associated controllers are becoming invaluable tools. In this work we introduce
scenery, a flexible VR/AR visualization framework for the Java VM that can
handle mesh and large volumetric data, containing multiple views, timepoints,
and color channels. scenery is free and open-source software, works on all
major platforms, and uses the Vulkan or OpenGL rendering APIs. We introduce
scenery's main features and example applications, such as its use in VR for
microscopy, in the biomedical image analysis software Fiji, or for visualizing
agent-based simulations.","['Ulrik Günther', 'Tobias Pietzsch', 'Aryaman Gupta', 'Kyle I. S. Harrington', 'Pavel Tomancak', 'Stefan Gumhold', 'Ivo F. Sbalzarini']",2019-06-16T17:01:20Z,http://arxiv.org/abs/1906.06726v3,['cs.GR'],"Virtual Reality,Visualization,Java VM,Volumetric Data,Mesh Data,Immersive Hardware,VR/AR,Scenery,Vulkan,OpenGL"
"Toward the Internet of No Things: The Role of O2O Communications and
  Extended Reality","Future fully interconnected virtual reality (VR) systems and the Tactile
Internet diminish the boundary between virtual (online) and real (offline)
worlds, while extending the digital and physical capabilities of humans via
edge computing and teleoperated robots, respectively. In this paper, we focus
on the Internet of No Things as an extension of immersive VR from virtual to
real environments, where human-intended Internet services - either digital or
physical - appear when needed and disappear when not needed. We first introduce
the concept of integrated online-to-offline (O2O) communications, which treats
online and offline channels as complementary to bridge the virtual and physical
worlds and provide O2O multichannel experiences. We then elaborate on the
emerging extended reality (XR), which brings the different forms of
virtual/augmented/mixed reality together to realize the entire
reality-virtuality continuum and, more importantly, supports human-machine
interaction as envisioned by the Tactile Internet, while posing challenges to
conventional handhelds, e.g., smartphones. Building on the so-called
invisible-to-visible (I2V) technology concept, we present our extrasensory
perception network (ESPN) and investigate how O2O communications and XR can be
combined for the nonlocal extension of human ""sixth-sense"" experiences in space
and time. We conclude by putting our ideas in perspective of the 6G vision.","['Martin Maier', 'Amin Ebrahimzadeh']",2019-06-16T17:41:23Z,http://arxiv.org/abs/1906.06738v1,['cs.NI'],"Internet of No Things,O2O communications,Extended Reality,Tactile Internet,edge computing,teleoperated robots,online-to-offline communications,extended reality,human-machine interaction,sixth-sense experiences"
An Algorithm for Transmitting VR Video Based on Adaptive Modulation,"Virtual reality (VR) is making waves around the world recently. However,
traditional video streaming is not suitable for VR video because of the huge
size and view switch requirements of VR videos. Since the view of each user is
limited, it is unnecessary to send the whole 360-degree scene at high quality
which can be a heavy burden for the transmission system. Assuming filed-of-view
(FoV) of each user can be predicted with high probability, we can divide the
video screen into partitions and send those partitions which will appear in FoV
at high quality. Hence, we propose an novel strategy for VR video streaming.
First, we define a quality-of-experience metric to measure the viewing
experience of users and define a channel model to reflect the fluctuation of
the wireless channel. Next, we formulate the optimization problem and find its
feasible solution by convex optimization. In order to improve bandwidth
efficiency, we also add adaptive modulation to this part. Finally, we compare
our algorithm with other VR streaming algorithm in the simulation. It turns out
that our algorithm outperforms other algorithms.","['Jie Feng', 'Yongpeng Wu', 'Guangtao Zhai', 'Ning Liu', 'Wenjun Zhang']",2019-06-27T00:54:02Z,http://arxiv.org/abs/1906.11402v1,"['cs.NI', 'cs.IT', 'eess.SP', 'math.IT']","VR video,adaptive modulation,video streaming,view switch,360-degree scene,field-of-view (FoV),quality-of-experience metric,channel model,convex optimization,bandwidth efficiency"
"Automatic Detection of Myocontrol Failures Based upon Situational
  Context Information","Myoelectric control systems for assistive devices are still unreliable. The
user's input signals can become unstable over time due to e.g. fatigue,
electrode displacement, or sweat. Hence, such controllers need to be constantly
updated and heavily rely on user feedback. In this paper, we present an
automatic failure detection method which learns when plausible predictions
become unreliable and model updates are necessary. Our key insight is to
enhance the control system with a set of generative models that learn sensible
behaviour for a desired task from human demonstration. We illustrate our
approach on a grasping scenario in Virtual Reality, in which the user is asked
to grasp a bottle on a table. From demonstration our model learns the
reach-to-grasp motion from a resting position to two grasps (power grasp and
tridigital grasp) and how to predict the most adequate grasp from local
context, e.g. tridigital grasp on the bottle cap or around the bottleneck. By
measuring the error between new grasp attempts and the model prediction, the
system can effectively detect which input commands do not reflect the user's
intention. We evaluated our model in two cases: i) with both position and
rotation information of the wrist pose, and ii) with only rotational
information. Our results show that our approach detects statistically highly
significant differences in error distributions with p < 0.001 between
successful and failed grasp attempts in both cases.","['Karoline Heiwolt', 'Claudio Zito', 'Markus Nowak', 'Claudio Castellini', 'Rustam Stolkin']",2019-06-27T11:46:15Z,http://arxiv.org/abs/1906.11564v1,['cs.RO'],"Myoelectric control systems,Failure detection,Generative models,Human demonstration,Virtual Reality,Grasping scenario,Error distributions,Wrist pose,Rotation information"
"Robust GPU-based Virtual Reality Simulation of Radio Frequency Ablations
  for Various Needle Geometries and Locations","Purpose: Radio-frequency ablations play an important role in the therapy of
malignant liver lesions. The navigation of a needle to the lesion poses a
challenge for both the trainees and intervening physicians. Methods: This
publication presents a new GPU-based, accurate method for the simulation of
radio-frequency ablations for lesions at the needle tip in general and for an
existing visuo-haptic 4D VR simulator. The method is implemented real-time
capable with Nvidia CUDA. Results: It performs better than a literature method
concerning the theoretical characteristic of monotonic convergence of the
bioheat PDE and a in vitro gold standard with significant improvements (p <
0.05) in terms of Pearson correlations. It shows no failure modes or
theoretically inconsistent individual simulation results after the initial
phase of 10 seconds. On the Nvidia 1080 Ti GPU it achieves a very high frame
rendering performance of >480 Hz. Conclusion: Our method provides a more robust
and safer real-time ablation planning and intraoperative guidance technique,
especially avoiding the over-estimation of the ablated tissue death zone, which
is risky for the patient in terms of tumor recurrence. Future in vitro
measurements and optimization shall further improve the conservative estimate.","['Niclas Kath', 'Heinz Handels', 'Andre Mastmeyer']",2019-07-11T15:53:36Z,http://arxiv.org/abs/1907.05709v1,"['physics.med-ph', 'cs.CV', 'eess.IV']","GPU-based,Virtual Reality,Radio Frequency Ablations,Needle Geometries,Nvidia CUDA,Bioheat PDE,Pearson correlations,Frame rendering performance,Ablation planning"
"Single Image based Head Pose Estimation with Spherical Parameterization
  and 3D Morphing","Head pose estimation plays a vital role in various applications, e.g.,
driverassistance systems, human-computer interaction, virtual reality
technology, and so on. We propose a novel geometry based algorithm for
accurately estimating the head pose from a single 2D face image at a very low
computational cost. Specifically, the rectangular coordinates of only four
non-coplanar feature points from a predefined 3D facial model as well as the
corresponding ones automatically/ manually extracted from a 2D face image are
first normalized to exclude the effect of external factors (i.e., scale factor
and translation parameters). Then, the four normalized 3D feature points are
represented in spherical coordinates with reference to the uniquely determined
sphere by themselves. Due to the spherical parameterization, the coordinates of
feature points can then be morphed along all the three directions in the
rectangular coordinates effectively. Finally, the rotation matrix indicating
the head pose is obtained by minimizing the Euclidean distance between the
normalized 2D feature points and the 2D re-projections of morphed 3D feature
points. Comprehensive experimental results over two popular databases, i.e.,
Pointing'04 and Biwi Kinect, demonstrate that the proposed algorithm can
estimate head poses with higher accuracy and lower run time than
state-of-the-art geometry based methods. Even compared with start-of-the-art
learning based methods or geometry based methods with additional depth
information, our algorithm still produces comparable performance.","['Hui Yuan', 'Mengyu Li', 'Junhui Hou', 'Jimin Xiao']",2019-07-22T10:16:30Z,http://arxiv.org/abs/1907.09217v3,"['cs.CV', 'eess.IV']","Head pose estimation,Spherical parameterization,3D morphing,Geometry-based algorithm,2D face image,Feature points,Rotation matrix,Euclidean distance,Databases,Computational cost"
xR-EgoPose: Egocentric 3D Human Pose from an HMD Camera,"We present a new solution to egocentric 3D body pose estimation from
monocular images captured from a downward looking fish-eye camera installed on
the rim of a head mounted virtual reality device. This unusual viewpoint, just
2 cm. away from the user's face, leads to images with unique visual appearance,
characterized by severe self-occlusions and strong perspective distortions that
result in a drastic difference in resolution between lower and upper body. Our
contribution is two-fold. Firstly, we propose a new encoder-decoder
architecture with a novel dual branch decoder designed specifically to account
for the varying uncertainty in the 2D joint locations. Our quantitative
evaluation, both on synthetic and real-world datasets, shows that our strategy
leads to substantial improvements in accuracy over state of the art egocentric
pose estimation approaches. Our second contribution is a new large-scale
photorealistic synthetic dataset - xR-EgoPose - offering 383K frames of high
quality renderings of people with a diversity of skin tones, body shapes,
clothing, in a variety of backgrounds and lighting conditions, performing a
range of actions. Our experiments show that the high variability in our new
synthetic training corpus leads to good generalization to real world footage
and to state of the art results on real world datasets with ground truth.
Moreover, an evaluation on the Human3.6M benchmark shows that the performance
of our method is on par with top performing approaches on the more classic
problem of 3D human pose from a third person viewpoint.","['Denis Tome', 'Patrick Peluse', 'Lourdes Agapito', 'Hernan Badino']",2019-07-23T17:58:03Z,http://arxiv.org/abs/1907.10045v1,['cs.CV'],"egocentric 3D body pose,HMD camera,fish-eye camera,encoder-decoder architecture,2D joint locations,synthetic dataset,photorealistic,renderings,diversity,generalization"
"Augmented Reality Applied to LEGO Construction: AR-based Building
  Instructions with High Accuracy & Precision and Realistic Object-Hand
  Occlusions","BRICKxAR is a novel Augmented Reality (AR) instruction method for
construction toys such as LEGO. With BRICKxAR, physical LEGO construction is
guided by virtual bricks. Compared with the state-of-the-art, accuracy of the
virtual - physical model alignment is significantly improved through a new
design of marker-based registration, which can achieve an average error less
than 1mm throughout the model. Realistic object occlusion is accomplished to
reveal the true spatial relationship between physical and virtual bricks. LEGO
players' hand detection and occlusion are realized to visualize the correct
spatial relationship between real hands and virtual bricks, and allow virtual
bricks to be ""grasped"" by real hands. The integration of these features makes
AR instructions possible for small-parts assembly, validated through a working
AR prototype for constructing LEGO Arc de Triomphe, quantitative measures of
the accuracies of registration and occlusions, and heuristic evaluation of AR
instruction features.",['Wei Yan'],2019-07-29T17:44:14Z,http://arxiv.org/abs/1907.12549v4,"['cs.HC', 'cs.CY', 'H.5.1; J.6']","Augmented Reality,LEGO,Construction,AR-based,Building Instructions,Accuracy,Precision,Object-Hand Occlusions"
"Synthetic Video Generation for Robust Hand Gesture Recognition in
  Augmented Reality Applications","Hand gestures are a natural means of interaction in Augmented Reality and
Virtual Reality (AR/VR) applications. Recently, there has been an increased
focus on removing the dependence of accurate hand gesture recognition on
complex sensor setup found in expensive proprietary devices such as the
Microsoft HoloLens, Daqri and Meta Glasses. Most such solutions either rely on
multi-modal sensor data or deep neural networks that can benefit greatly from
abundance of labelled data. Datasets are an integral part of any deep learning
based research. They have been the principal reason for the substantial
progress in this field, both, in terms of providing enough data for the
training of these models, and, for benchmarking competing algorithms. However,
it is becoming increasingly difficult to generate enough labelled data for
complex tasks such as hand gesture recognition. The goal of this work is to
introduce a framework capable of generating photo-realistic videos that have
labelled hand bounding box and fingertip that can help in designing, training,
and benchmarking models for hand-gesture recognition in AR/VR applications. We
demonstrate the efficacy of our framework in generating videos with diverse
backgrounds.","['Varun Jain', 'Shivam Aggarwal', 'Suril Mehta', 'Ramya Hebbalaguppe']",2019-11-04T16:32:07Z,http://arxiv.org/abs/1911.01320v3,['cs.CV'],"hand gestures,augmented reality,virtual reality,synthetic video generation,robust,gesture recognition,deep learning,labelled data,framework,photo-realistic videos"
"Virtual Co-Embodiment: Evaluation of the Sense of Agency while Sharing
  the Control of a Virtual Body among Two Individuals","In this paper, we introduce a concept called ''virtual co-embodiment'', which
enables a user to share their virtual avatar with another entity (e.g., another
user, robot, or autonomous agent). We describe a proof-of-concept in which two
users can be immersed from a first-person perspective in a virtual environment
and can have complementary levels of control (total, partial, or none) over a
shared avatar. In addition, we conducted an experiment to investigate the
influence of users' level of control over the shared avatar and prior knowledge
of their actions on the users' sense of agency and motor actions. The results
showed that participants are good at estimating their real level of control but
significantly overestimate their sense of agency when they can anticipate the
motion of the avatar. Moreover, participants performed similar body motions
regardless of their real control over the avatar. The results also revealed
that the internal dimension of the locus of control, which is a personality
trait, is negatively correlated with the user's perceived level of control. The
combined results unfold a new range of applications in the fields of
virtual-reality-based training and collaborative teleoperation, where users
would be able to share their virtual body.","['Rebecca Fribourg', 'Nami Ogawa', 'Ludovic Hoyet', 'Ferran Argelaguet', 'Takuji Narumi', 'Michitaka Hirose', 'Anatole Lécuyer']",2019-11-08T10:22:00Z,http://arxiv.org/abs/1911.03166v2,['cs.HC'],"virtual co-embodiment,sense of agency,shared control,virtual avatar,first-person perspective,level of control,sense of agency,motor actions,locus of control,virtual reality"
"Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous
  Multi-Lane Driving","Autonomous driving promises to transform road transport. Multi-vehicle and
multi-lane scenarios, however, present unique challenges due to constrained
navigation and unpredictable vehicle interactions. Learning-based
methods---such as deep reinforcement learning---are emerging as a promising
approach to automatically design intelligent driving policies that can cope
with these challenges. Yet, the process of safely learning multi-vehicle
driving behaviours is hard: while collisions---and their near-avoidance---are
essential to the learning process, directly executing immature policies on
autonomous vehicles raises considerable safety concerns. In this article, we
present a safe and efficient framework that enables the learning of driving
policies for autonomous vehicles operating in a shared workspace, where the
absence of collisions cannot be guaranteed. Key to our learning procedure is a
sim2real approach that uses real-world online policy adaptation in a
mixed-reality setup, where other vehicles and static obstacles exist in the
virtual domain. This allows us to perform safe learning by simulating (and
learning from) collisions between the learning agent(s) and other objects in
virtual reality. Our results demonstrate that, after only a few runs in
mixed-reality, collisions are significantly reduced.","['Rupert Mitchell', 'Jenny Fletcher', 'Jacopo Panerati', 'Amanda Prorok']",2019-11-26T17:08:40Z,http://arxiv.org/abs/1911.11699v2,"['cs.RO', 'cs.AI', 'cs.LG', 'cs.MA', 'I.2.6; I.2.9']","multi-vehicle,mixed-reality,reinforcement learning,autonomous driving,deep reinforcement learning,navigation,vehicle interactions,sim2real,online policy adaptation"
"Evaluating the Data Quality of Eye Tracking Signals from a Virtual
  Reality System: Case Study using SMI's Eye-Tracking HTC Vive","We evaluated the data quality of SMI's tethered eye-tracking head-mounted
display based on the HTC Vive (ET-HMD) during a random saccade task. We
measured spatial accuracy, spatial precision, temporal precision, linearity,
and crosstalk. We proposed the use of a non-parametric spatial precision
measure based on the median absolute deviation (MAD). Our linearity analysis
considered both the slope and adjusted R-squared of a best-fitting line. We
were the first to test for a quadratic component to crosstalk. We prepended a
calibration task to the random saccade task and evaluated 2 methods to employ
this user-supplied calibration. For this, we used a unique binning approach to
choose samples to be included in the recalibration analyses. We compared our
quality measures between the ET-HMD and our EyeLink 1000 (SR-Research, Ottawa,
Ontario, CA). We found that the ET-HMD had significantly better spatial
accuracy and linearity fit than our EyeLink, but both devices had similar
spatial precision and linearity slope. We also found that, while the EyeLink
had no significant crosstalk, the ET-HMD generally exhibited quadratic
crosstalk. Fourier analysis revealed that the binocular signal was a low-pass
filtered version of the monocular signal. Such filtering resulted in the
binocular signal being useless for the study of high-frequency components such
as saccade dynamics.","['Dillon J. Lohr', 'Lee Friedman', 'Oleg V. Komogortsev']",2019-12-04T16:22:56Z,http://arxiv.org/abs/1912.02083v1,['cs.HC'],"Data quality,Eye tracking signals,Virtual reality system,SMI,Eye-tracking,HTC Vive,Spatial accuracy,Temporal precision,Linearity,Crosstalk"
"Brain perfusion mediates the relationship between miRNA levels and
  postural control","Transcriptomics, regional cerebral blood flow (rCBF), and a spatial motor
virtual reality task were integrated using mediation analysis in a novel
demonstration of ""imaging omics"". Data collected in NCAA Division I football
athletes cleared for play before in-season training showed significant
relationships in a) elevated levels of miR-30d and miR-92a to elevated putamen
rCBF, (b) elevated putamen rCBF to compromised balance scores, and (c)
compromised balance scores to elevated miRNA levels. rCBF acted as a mediator
variable (minimum 70% mediation, significant Sobel's test) between abnormal
miRNA levels and compromised balance scores. Given the involvement of these
miRNAs in inflammation and immune function, and that vascular perfusion is a
component of the inflammatory response, these findings support a chronic
inflammatory model of repetitive head acceleration events (HAEs). rCBF, a
systems biology measure, was necessary for miRNA to affect behavior. These
results suggest miRNA as a potential diagnostic biomarker for repetitive HAEs.","['Yufen Chen', 'Amy A Herrold', 'Zoran Martinovich', 'Anne J Blood', 'Nicole Vike', 'Alexa E Walter', 'Jaroslaw Harezlak', 'Peter H Seidenberg', 'Manish Bhomia', 'Barbara Knollmann-Ritschel', 'James L Reilly', 'Eric A Nauman', 'Thomas M Talavage', 'Linda Papa', 'Semyon Slobounov', 'Hans C Breiter']",2019-12-05T22:16:54Z,http://arxiv.org/abs/1912.02901v1,['q-bio.NC'],"Transcriptomics,regional cerebral blood flow,mediation analysis,miRNA levels,postural control"
"Training Deep Neural Networks to Detect Repeatable 2D Features Using
  Large Amounts of 3D World Capture Data","Image space feature detection is the act of selecting points or parts of an
image that are easy to distinguish from the surrounding image region. By
combining a repeatable point detection with a descriptor, parts of an image can
be matched with one another, which is useful in applications like estimating
pose from camera input or rectifying images. Recently, precise indoor tracking
has started to become important for Augmented and Virtual reality as it is
necessary to allow positioning of a headset in 3D space without the need for
external tracking devices. Several modern feature detectors use homographies to
simulate different viewpoints, not only to train feature detection and
description, but test them as well. The problem is that, often, views of indoor
spaces contain high depth disparity. This makes the approximation that a
homography applied to an image represents a viewpoint change inaccurate. We
claim that in order to train detectors to work well in indoor environments,
they must be robust to this type of geometry, and repeatable under true
viewpoint change instead of homographies. Here we focus on the problem of
detecting repeatable feature locations under true viewpoint change. To this
end, we generate labeled 2D images from a photo-realistic 3D dataset. These
images are used for training a neural network based feature detector. We
further present an algorithm for automatically generating labels of repeatable
2D features, and present a fast, easy to use test algorithm for evaluating a
detector in an 3D environment.","['Alexander Mai', 'Joseph Menke', 'Allen Yang']",2019-12-09T21:28:50Z,http://arxiv.org/abs/1912.04384v1,['cs.CV'],"deep neural networks,2D features,3D world capture data,feature detection,descriptor,pose estimation,indoor tracking,homographies,viewpoint change,labeled dataset"
"One Point, One Object: Simultaneous 3D Object Segmentation and 6-DOF
  Pose Estimation","We propose a single-shot method for simultaneous 3D object segmentation and
6-DOF pose estimation in pure 3D point clouds scenes based on a consensus that
\emph{one point only belongs to one object}, i.e., each point has the potential
power to predict the 6-DOF pose of its corresponding object. Unlike the
recently proposed methods of the similar task, which rely on 2D detectors to
predict the projection of 3D corners of the 3D bounding boxes and the 6-DOF
pose must be estimated by a PnP like spatial transformation method, ours is
concise enough not to require additional spatial transformation between
different dimensions. Due to the lack of training data for many objects, the
recently proposed 2D detection methods try to generate training data by using
rendering engine and achieve good results. However, rendering in 3D space along
with 6-DOF is relatively difficult. Therefore, we propose an augmented reality
technology to generate the training data in semi-virtual reality 3D space. The
key component of our method is a multi-task CNN architecture that can
simultaneously predicts the 3D object segmentation and 6-DOF pose estimation in
pure 3D point clouds.
  For experimental evaluation, we generate expanded training data for two
state-of-the-arts 3D object datasets \cite{PLCHF}\cite{TLINEMOD} by using
Augmented Reality technology (AR). We evaluate our proposed method on the two
datasets. The results show that our method can be well generalized into
multiple scenarios and provide performance comparable to or better than the
state-of-the-arts.","['Hongsen Liu', 'Yang Cong', 'Yandong Tang']",2019-12-27T13:48:03Z,http://arxiv.org/abs/1912.12095v1,['cs.CV'],"3D object segmentation,6-DOF pose estimation,point clouds,multi-task CNN architecture,augmented reality technology,training data,rendering engine,spatial transformation,object detection,state-of-the-arts"
"QoE Management of Multimedia Streaming Services in Future Networks: A
  Tutorial and Survey","We provide in this paper a tutorial and a comprehensive survey of QoE
management solutions in current and future networks. We start with a high level
description of QoE management for multimedia services, which integrates QoE
modelling, monitoring, and optimization. This followed by a discussion of HTTP
Adaptive Streaming (HAS) solutions as the dominant technique for streaming
videos over the best-effort Internet. We then summarize the key elements in
SDN/NFV along with an overview of ongoing research projects, standardization
activities and use cases related to SDN, NFV, and other emerging applications.
We provide a survey of the state-of-the-art of QoE management techniques
categorized into three different groups: a) QoE-aware/driven strategies using
SDN and/or NFV; b) QoE-aware/driven approaches for adaptive streaming over
emerging architectures such as multi-access edge computing, cloud/fog
computing, and information-centric networking; and c) extended QoE management
approaches in new domains such as immersive augmented and virtual reality,
mulsemedia and video gaming applications. Based on the review, we present a
list of identified future QoE management challenges regarding emerging
multimedia applications, network management and orchestration, network slicing
and collaborative service management in softwarized networks. Finally, we
provide a discussion on future research directions with a focus on emerging
research areas in QoE management, such as QoE-oriented business models,
QoE-based big data strategies, and scalability issues in QoE optimization.","['Alcardo Alex Barakabitze', 'Nabajeet Barman', 'Arslan Ahmad', 'Saman Zadtootaghaj', 'Lingfen Sun', 'Maria G. Martini', 'Luigi Atzori']",2019-12-28T14:50:48Z,http://arxiv.org/abs/1912.12467v1,"['cs.NI', 'cs.MM', 'eess.SP']","QoE management,Multimedia streaming services,Future networks,Tutorial,Survey,QoE modelling,Monitoring,Optimization,HTTP Adaptive Streaming (HAS),SDN,NFV"
"Smart Context-aware Rejuvenation of Engagement on Urban Ambient
  Augmented Things","The concern over global urbanization trend imposes smart-city as enabling
information and communication technology (ICT) to improve urban governance.
However, the light trance on better living space is stimulated by socioeconomic
impact of escalated senior generation. Hence, ambient assisted living (AAL)
emerges for the autonomous provisioning of pervasive things or objects from
relevant perturbation for advanced scientific instrumentation. Meanwhile,
citizens are observed in being transfixed by lively stimuli of monotonous urban
events with the advent of virtual reality or augmented things. Thus, due to the
involvement of situation-awareness or contextualization,
engagement/participation information as a utility promises to improve urban
experience. However, it is complex to grapple meaningful concepts due to
personalization obstacles, such as citizen psychology, information gap,
service-visualization. Moreover, recommended practices deficit adaptation to
monochromatic choice, disparate impairments, mobility and annotation-richness
in urban space. Hence, rejuvenation of engagement relates to monitoring and
quantification of 'service consumption and graceful degradation' of experience.
However, paramount challenges are imposed on this stipulation, such as,
unobservability, independence and composite relationship of contexts.
Therefore, a parametric Bayesian based model is envisioned to address
observability and scalability of contexts and its conjugal relationship with
engagement. Last but not the least, systematic framework is demonstrated, which
pinpoints key goals of context-aware engagement from participants' opinions,
usages and feed-backs.","['Rossi Kamal', 'Choong Seon Hong']",2016-03-09T13:59:40Z,http://arxiv.org/abs/1603.03732v1,['cs.CY'],"smart-city,ambient assisted living,pervasive things,augmented reality,situation-awareness,engagement,personalization,urban space,graceful degradation,Bayesian model"
Exploring Computation-Communication Tradeoffs in Camera Systems,"Cameras are the defacto sensor. The growing demand for real-time and
low-power computer vision, coupled with trends towards high-efficiency
heterogeneous systems, has given rise to a wide range of image processing
acceleration techniques at the camera node and in the cloud. In this paper, we
characterize two novel camera systems that use acceleration techniques to push
the extremes of energy and performance scaling, and explore the
computation-communication tradeoffs in their design. The first case study
targets a camera system designed to detect and authenticate individual faces,
running solely on energy harvested from RFID readers. We design a
multi-accelerator SoC design operating in the sub-mW range, and evaluate it
with real-world workloads to show performance and energy efficiency
improvements over a general purpose microprocessor. The second camera system
supports a 16-camera rig processing over 32 Gb/s of data to produce real-time
3D-360 degree virtual reality video. We design a multi-FPGA processing pipeline
that outperforms CPU and GPU configurations by up to 10x in computation time,
producing panoramic stereo video directly from the camera rig at 30 frames per
second. We find that an early data reduction step, either before complex
processing or offloading, is the most critical optimization for in-camera
systems.","['Amrita Mazumdar', 'Thierry Moreau', 'Sung Kim', 'Meghan Cowan', 'Armin Alaghi', 'Luis Ceze', 'Mark Oskin', 'Visvesh Sathe']",2017-06-12T22:11:55Z,http://arxiv.org/abs/1706.03864v2,['cs.AR'],"camera systems,computation-communication tradeoffs,computer vision,image processing acceleration techniques,energy efficiency,performance scaling,SoC design,FPGA processing pipeline,data reduction,virtual reality video"
"Human decisions in moral dilemmas are largely described by
  Utilitarianism: virtual car driving study provides guidelines for ADVs","Ethical thought experiments such as the trolley dilemma have been
investigated extensively in the past, showing that humans act in a utilitarian
way, trying to cause as little overall damage as possible. These trolley
dilemmas have gained renewed attention over the past years; especially due to
the necessity of implementing moral decisions in autonomous driving vehicles.
We conducted a set of experiments in which participants experienced modified
trolley dilemmas as the driver in a virtual reality environment. Participants
had to make decisionsbetween two discrete options: driving on one of two lanes
where different obstacles came into view. Obstacles included a variety of
human-like avatars of different ages and group sizes. Furthermore, we tested
the influence of a sidewalk as a potential safe harbor and a condition
implicating a self-sacrifice. Results showed that subjects, in general, decided
in a utilitarian manner, sparing the highest number of avatars possible with a
limited influence of the other variables. Our findings support that human
behavior is in line with the utilitarian approach to moral decision making.
This may serve as a guideline for the implementation of moral decisions in
ADVs.","['Maximilian Alexander Wächter', 'Anja Faulhaber', 'Felix Blind', 'Silja Timm', 'Anke Dittmer', 'Leon René Sütfeld', 'Achim Stephan', 'Gordon Pipa', 'Peter König']",2017-06-22T14:10:39Z,http://arxiv.org/abs/1706.07332v2,['cs.CY'],"Utilitarianism,Moral dilemmas,Autonomous driving vehicles,Virtual reality,Trolley dilemma,Human decisions,Ethical thought experiments,Safe harbor,Self-sacrifice,Utilitarian approach"
"Photosensor Oculography: Survey and Parametric Analysis of Designs using
  Model-Based Simulation","This paper presents a renewed overview of photosensor oculography (PSOG), an
eye-tracking technique based on the principle of using simple photosensors to
measure the amount of reflected (usually infrared) light when the eye rotates.
Photosensor oculography can provide measurements with high precision, low
latency and reduced power consumption, and thus it appears as an attractive
option for performing eye-tracking in the emerging head-mounted interaction
devices, e.g. augmented and virtual reality (AR/VR) headsets. In our current
work we employ an adjustable simulation framework as a common basis for
performing an exploratory study of the eye-tracking behavior of different
photosensor oculography designs. With the performed experiments we explore the
effects from the variation of some basic parameters of the designs on the
resulting accuracy and cross-talk, which are crucial characteristics for the
seamless operation of human-computer interaction applications based on
eye-tracking. Our experimental results reveal the design trade-offs that need
to be adopted to tackle the competing conditions that lead to optimum
performance of different eye-tracking characteristics. We also present the
transformations that arise in the eye-tracking output when sensor shifts occur,
and assess the resulting degradation in accuracy for different combinations of
eye movements and sensor shifts.","['Ioannis Rigas', 'Hayes Raffle', 'Oleg V. Komogortsev']",2017-07-17T23:31:57Z,http://arxiv.org/abs/1707.05413v2,"['cs.CV', 'cs.HC']","photosensor oculography,eye-tracking,infrared light,precision,latency,power consumption,head-mounted interaction devices,simulation framework,accuracy"
Security and Privacy Approaches in Mixed Reality: A Literature Survey,"Mixed reality (MR) technology development is now gaining momentum due to
advances in computer vision, sensor fusion, and realistic display technologies.
With most of the research and development focused on delivering the promise of
MR, there is only barely a few working on the privacy and security implications
of this technology. This survey paper aims to put in to light these risks, and
to look into the latest security and privacy work on MR. Specifically, we list
and review the different protection approaches that have been proposed to
ensure user and data security and privacy in MR. We extend the scope to include
work on related technologies such as augmented reality (AR), virtual reality
(VR), and human-computer interaction (HCI) as crucial components, if not the
origins, of MR, as well as numerous related work from the larger area of mobile
devices, wearables, and Internet-of-Things (IoT). We highlight the lack of
investigation, implementation, and evaluation of data protection approaches in
MR. Further challenges and directions on MR security and privacy are also
discussed.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-02-15T23:33:45Z,http://arxiv.org/abs/1802.05797v3,"['cs.CR', 'cs.CY', 'cs.HC']","Mixed Reality,Security,Privacy,Computer Vision,Sensor Fusion,Protection Approaches,Augmented Reality,Virtual Reality,Human-Computer Interaction,Internet-of-Things"
"Perceptual Quality Assessment of Immersive Images Considering Peripheral
  Vision Impact","Conventional images/videos are often rendered within the central vision area
of the human visual system (HVS) with uniform quality. Recent virtual reality
(VR) device with head mounted display (HMD) extends the field of view (FoV)
significantly to include both central and peripheral vision areas. It exhibits
the unequal image quality sensation among these areas because of the
non-uniform distribution of photoreceptors on our retina. We propose to study
the sensation impact on the image subjective quality with respect to the
eccentric angle $\theta$ across different vision areas. Often times, image
quality is controlled by the quantization stepsize $q$ and spatial resolution
$s$, separately and jointly. Therefore, the sensation impact can be understood
by exploring the $q$ and/or $s$ in terms of the $\theta$, resulting in
self-adaptive analytical models that have shown quite impressive accuracy
through independent cross validations. These models can further be applied to
give different quality weights at different regions, so as to significantly
reduce the transmission data size but without subjective quality loss. As
demonstrated in a gigapixel imaging system, we have shown that the image
rendering can be speed up about 10$\times$ with the model guided unequal
quality scales, in comparison to the the legacy scheme with uniform quality
scales everywhere.","['Peiyao Guo', 'Qiu Shen', 'Zhan Ma', 'David J. Brady', 'Yao Wang']",2018-02-25T19:15:33Z,http://arxiv.org/abs/1802.09065v1,['cs.MM'],"immersive images,perceptual quality assessment,peripheral vision,virtual reality,head mounted display,field of view,photoreceptors,eccentric angle,quantization stepsize,spatial resolution"
"Multimodal Sensor-Based Semantic 3D Mapping for a Large-Scale
  Environment","Semantic 3D mapping is one of the most important fields in robotics, and has
been used in many applications, such as robot navigation, surveillance, and
virtual reality. In general, semantic 3D mapping is mainly composed of 3D
reconstruction and semantic segmentation. As these technologies evolve, there
has been great progress in semantic 3D mapping in recent years. Furthermore,
the number of robotic applications requiring semantic information in 3D mapping
to perform high-level tasks has increased, and many studies on semantic 3D
mapping have been published. Existing methods use a camera for both 3D
reconstruction and semantic segmentation. However, this is not suitable for
large-scale environments and has the disadvantage of high computational
complexity. To address this problem, we propose a multimodal sensor-based
semantic 3D mapping system using a 3D Lidar combined with a camera. In this
study, we build a 3D map by estimating odometry based on a global positioning
system (GPS) and an inertial measurement unit (IMU), and use the latest 2D
convolutional neural network (CNN) for semantic segmentation. To build a
semantic 3D map, we integrate the 3D map with semantic information by using
coordinate transformation and Bayes' update scheme. In order to improve the
semantic 3D map, we propose a 3D refinement process to correct wrongly
segmented voxels and remove traces of moving vehicles in the 3D map. Through
experiments on challenging sequences, we demonstrate that our method
outperforms state-of-the-art methods in terms of accuracy and intersection over
union (IoU). Thus, our method can be used for various applications that require
semantic information in 3D map.","['Jongmin Jeong', 'Tae Sung Yoon', 'Jin Bae Park']",2018-02-28T06:02:55Z,http://arxiv.org/abs/1802.10271v1,['cs.RO'],"Semantic mapping,3D reconstruction,Semantic segmentation,Multimodal sensor,Lidar,Convolutional neural network (CNN),Global positioning system (GPS),Inertial measurement unit (IMU),Bayes' update scheme,Intersection over union (IoU)"
"Measurement of exceptional motion in VR video contents for VR sickness
  assessment using deep convolutional autoencoder","This paper proposes a new objective metric of exceptional motion in VR video
contents for VR sickness assessment. In VR environment, VR sickness can be
caused by several factors which are mismatched motion, field of view, motion
parallax, viewing angle, etc. Similar to motion sickness, VR sickness can
induce a lot of physical symptoms such as general discomfort, headache, stomach
awareness, nausea, vomiting, fatigue, and disorientation. To address the
viewing safety issues in virtual environment, it is of great importance to
develop an objective VR sickness assessment method that predicts and analyses
the degree of VR sickness induced by the VR content. The proposed method takes
into account motion information that is one of the most important factors in
determining the overall degree of VR sickness. In this paper, we detect the
exceptional motion that is likely to induce VR sickness. Spatio-temporal
features of the exceptional motion in the VR video content are encoded using a
convolutional autoencoder. For objectively assessing the VR sickness, the level
of exceptional motion in VR video content is measured by using the
convolutional autoencoder as well. The effectiveness of the proposed method has
been successfully evaluated by subjective assessment experiment using simulator
sickness questionnaires (SSQ) in VR environment.","['Hak Gu Kim', 'Wissam J. Baddar', 'Heoun-taek Lim', 'Hyunwook Jeong', 'Yong Man Ro']",2018-04-11T11:41:47Z,http://arxiv.org/abs/1804.03939v1,['cs.CV'],"VR video content,VR sickness,exceptional motion,deep convolutional autoencoder,motion sickness,field of view,motion parallax,viewing angle,spatio-temporal features,subjective assessment."
"Rate-Utility Optimized Streaming of Volumetric Media for Augmented
  Reality","Volumetric media, popularly known as holograms, need to be delivered to users
using both on-demand and live streaming, for new augmented reality (AR) and
virtual reality (VR) experiences. As in video streaming, hologram streaming
must support network adaptivity and fast startup, but must also moderate large
bandwidths, multiple simultaneously streaming objects, and frequent user
interaction, which requires low delay. In this paper, we introduce the first
system to our knowledge designed specifically for streaming volumetric media.
The system reduces bandwidth by introducing 3D tiles, and culling them or
reducing their level of detail depending on their relation to the user's view
frustum and distance to the user. Our system reduces latency by introducing a
window-based buffer, which in contrast to a queue-based buffer allows
insertions near the head of the buffer rather than only at the tail of the
buffer, to respond quickly to user interaction. To allocate bits between
different tiles across multiple objects, we introduce a simple greedy yet
provably optimal algorithm for rate-utility optimization. We introduce utility
measures based not only on the underlying quality of the representation, but on
the level of detail relative to the user's viewpoint and device resolution.
Simulation results show that the proposed algorithm provides superior quality
compared to existing video-streaming approaches adapted to hologram streaming,
in terms of utility and user experience over variable, throughput-constrained
networks.","['Jounsup Park', 'Philip A. Chou', 'Jenq-Neng Hwang']",2018-04-26T02:49:53Z,http://arxiv.org/abs/1804.09864v1,['cs.MM'],"Volumetric media,Streaming,Augmented reality,Virtual reality,Bandwidth,User interaction,Latency,3D tiles,Buffer,Rate-utility optimization."
FMHash: Deep Hashing of In-Air-Handwriting for User Identification,"Many mobile systems and wearable devices, such as Virtual Reality (VR) or
Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID
and password for signing into a virtual website. However, they are usually
equipped with gesture capture interfaces to allow the user to interact with the
system directly with hand gestures. Although gesture-based authentication has
been well-studied, less attention is paid to the gesture-based user
identification problem, which is essentially an input method of account ID and
an efficient searching and indexing method of a database of gesture signals. In
this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification
framework that can generate a compact binary hash code from a piece of
in-air-handwriting of an ID string. This hash code enables indexing and fast
search of a large account database using the in-air-handwriting by a hash
table. To demonstrate the effectiveness of the framework, we implemented a
prototype and achieved >99.5% precision and >92.6% recall with exact hash code
match on a dataset of 200 accounts collected by us. The ability of hashing
in-air-handwriting pattern to binary code can be used to achieve convenient
sign-in and sign-up with in-air-handwriting gesture ID on future mobile and
wearable systems connected to the Internet.","['Duo Lu', 'Dijiang Huang', 'Anshul Rai']",2018-06-10T02:15:29Z,http://arxiv.org/abs/1806.03574v2,"['cs.CV', 'D.4.6; I.5.4']","deep hashing,in-air-handwriting,user identification,gesture capture interfaces,binary hash code,database indexing,hash table,precision,recall,wearable systems"
"Edge Cloud Offloading Algorithms: Issues, Methods, and Perspectives","Mobile devices supporting the ""Internet of Things"" (IoT), often have limited
capabilities in computation, battery energy, and storage space, especially to
support resource-intensive applications involving virtual reality (VR),
augmented reality (AR), multimedia delivery and artificial intelligence (AI),
which could require broad bandwidth, low response latency and large
computational power. Edge cloud or edge computing is an emerging topic and
technology that can tackle the deficiency of the currently centralized-only
cloud computing model and move the computation and storage resource closer to
the devices in support of the above-mentioned applications. To make this
happen, efficient coordination mechanisms and ""offloading"" algorithms are
needed to allow the mobile devices and the edge cloud to work together
smoothly. In this survey paper, we investigate the key issues, methods, and
various state-of-the-art efforts related to the offloading problem. We adopt a
new characterizing model to study the whole process of offloading from mobile
devices to the edge cloud. Through comprehensive discussions, we aim to draw an
overall ""big picture"" on the existing efforts and research directions. Our
study also indicates that the offloading algorithms in edge cloud have
demonstrated profound potentials for future technology and application
development.","['Jianyu Wang', 'Jianli Pan', 'Flavio Esposito', 'Prasad Calyam', 'Zhicheng Yang', 'Prasant Mohapatra']",2018-06-16T05:50:46Z,http://arxiv.org/abs/1806.06191v1,['cs.NI'],"Edge Cloud,Offloading Algorithms,Internet of Things (IoT),Virtual Reality (VR),Augmented Reality (AR),Multimedia Delivery,Artificial Intelligence (AI),Bandwidth,Latency,Computational Power"
"Towards Commodity, Web-Based Augmented Reality Applications for Research
  and Education in Chemistry and Structural Biology","This article reports prototype web apps that use commodity, open-source
technologies for augmented and virtual reality to provide immersive,
interactive human-computer interfaces for chemistry, structural biology and
related disciplines. The examples, which run in any standard web browser and
are accessible at
https://lucianoabriata.altervista.org/jsinscience/arjs/armodeling/ together
with demo videos, showcase applications that could go well beyond pedagogy,
i.e. advancing actual utility in research settings: molecular visualization at
atomistic and coarse-grained levels in interactive immersive 3D, coarse-grained
modeling of molecular physics and chemistry, and on-the-fly calculation of
experimental observables and overlay onto experimental data. From this
playground, I depict perspectives on how these emerging technologies might
couple in the future to neural network-based quantum mechanical calculations,
advanced forms of human-computer interaction such as speech-based
communication, and sockets for concurrent collaboration through the internet
-all technologies that are today maturing in web browsers- to deliver the next
generation of tools for truly interactive, immersive molecular modeling that
can streamline human thought and intent with the numerical processing power of
computers.",['Luciano A. Abriata'],2018-06-21T17:21:17Z,http://arxiv.org/abs/1806.08332v5,"['cs.HC', 'cs.ET', 'cs.MM', 'physics.bio-ph', 'q-bio.BM']","commodity,web-based,augmented reality,applications,research,education,chemistry,structural biology,immersive,interactive"
"Shape-from-Mask: A Deep Learning Based Human Body Shape Reconstruction
  from Binary Mask Images","3D content creation is referred to as one of the most fundamental tasks of
computer graphics. And many 3D modeling algorithms from 2D images or curves
have been developed over the past several decades. Designers are allowed to
align some conceptual images or sketch some suggestive curves, from front,
side, and top views, and then use them as references in constructing a 3D model
automatically or manually. However, to the best of our knowledge, no studies
have investigated on 3D human body reconstruction in a similar manner. In this
paper, we propose a deep learning based reconstruction of 3D human body shape
from 2D orthographic views. A novel CNN-based regression network, with two
branches corresponding to frontal and lateral views respectively, is designed
for estimating 3D human body shape from 2D mask images. We train our networks
separately to decouple the feature descriptors which encode the body parameters
from different views, and fuse them to estimate an accurate human body shape.
In addition, to overcome the shortage of training data required for this
purpose, we propose some significantly data augmentation schemes for 3D human
body shapes, which can be used to promote further research on this topic.
Extensive experimen- tal results demonstrate that visually realistic and
accurate reconstructions can be achieved effectively using our algorithm.
Requiring only binary mask images, our method can help users create their own
digital avatars quickly, and also make it easy to create digital human body for
3D game, virtual reality, online fashion shopping.","['Zhongping Ji', 'Xiao Qi', 'Yigang Wang', 'Gang Xu', 'Peng Du', 'Qing Wu']",2018-06-22T04:00:37Z,http://arxiv.org/abs/1806.08485v1,"['cs.GR', 'cs.CV']","Deep learning,Human body shape reconstruction,Binary mask images,3D modeling,CNN-based regression network,Data augmentation,3D content creation,Computer graphics,Digital avatars,Virtual reality."
Wireless Network Intelligence at the Edge,"Fueled by the availability of more data and computing power, recent
breakthroughs in cloud-based machine learning (ML) have transformed every
aspect of our lives from face recognition and medical diagnosis to natural
language processing. However, classical ML exerts severe demands in terms of
energy, memory and computing resources, limiting their adoption for resource
constrained edge devices. The new breed of intelligent devices and high-stake
applications (drones, augmented/virtual reality, autonomous systems, etc.),
requires a novel paradigm change calling for distributed, low-latency and
reliable ML at the wireless network edge (referred to as edge ML). In edge ML,
training data is unevenly distributed over a large number of edge nodes, which
have access to a tiny fraction of the data. Moreover training and inference is
carried out collectively over wireless links, where edge devices communicate
and exchange their learned models (not their private data). In a first of its
kind, this article explores key building blocks of edge ML, different neural
network architectural splits and their inherent tradeoffs, as well as
theoretical and technical enablers stemming from a wide range of mathematical
disciplines. Finally, several case studies pertaining to various high-stake
applications are presented demonstrating the effectiveness of edge ML in
unlocking the full potential of 5G and beyond.","['Jihong Park', 'Sumudu Samarakoon', 'Mehdi Bennis', 'Mérouane Debbah']",2018-12-07T00:17:01Z,http://arxiv.org/abs/1812.02858v2,"['cs.IT', 'cs.LG', 'cs.NI', 'math.IT']","Wireless Network,Intelligence,Edge,Machine Learning,Neural Network,Training Data,Inference,Wireless Links,Edge Devices,5G"
"Somatic Practices for Understanding Real, Imagined, and Virtual
  Realities","In most VR experiences, the visual sense dominates other modes of sensory
input, encouraging non-visual senses to respond as if the visual were real. The
simulated visual world thus becomes a sort of felt actuality, where the
'actual' physical body and environment can 'drop away', opening up
possibilities for designing entirely new kinds of experience. Most VR
experiences place visual sensory input (of the simulated environment) in the
perceptual foreground, and the physical body in the background. In what
follows, we discuss methods for resolving the apparent tension which arises
from VR's prioritization of visual perception. We specifically aim to
understand how somatic techniques encouraging participants to 'attend to their
attention' enable them to access more subtle aspects of sensory phenomena in a
VR experience, bound neither by rigid definitions of vision-based virtuality
nor body-based corporeality. During a series of workshops, we implemented
experimental somatic-dance practices to better understand perceptual and
imaginative subtleties that arise for participants whilst they are embedded in
a multi-person VR framework. Our preliminary observations suggest that somatic
methods can be used to design VR experiences which enable (i) a tactile quality
or felt sense of phenomena in the virtual environment (VE), (ii) lingering
impacts on participant imagination even after the VR headset is taken off, and
(iii) an expansion of imaginative potential.","['Lisa May Thomas', 'Helen M. Deeks', 'Alex J. Jones', 'Oussama Metatla', 'David R. Glowacki']",2019-01-11T10:06:46Z,http://arxiv.org/abs/1901.03536v1,"['cs.HC', 'cs.CY', 'cs.MM']","somatic practices,virtual realities,sensory input,visual perception,somatic techniques,sensory phenomena,VR experience,virtual environment,imaginative potential"
"The RobotriX: An eXtremely Photorealistic and Very-Large-Scale Indoor
  Dataset of Sequences with Robot Trajectories and Interactions","Enter the RobotriX, an extremely photorealistic indoor dataset designed to
enable the application of deep learning techniques to a wide variety of robotic
vision problems. The RobotriX consists of hyperrealistic indoor scenes which
are explored by robot agents which also interact with objects in a visually
realistic manner in that simulated world. Photorealistic scenes and robots are
rendered by Unreal Engine into a virtual reality headset which captures gaze so
that a human operator can move the robot and use controllers for the robotic
hands; scene information is dumped on a per-frame basis so that it can be
reproduced offline to generate raw data and ground truth labels. By taking this
approach, we were able to generate a dataset of 38 semantic classes totaling 8M
stills recorded at +60 frames per second with full HD resolution. For each
frame, RGB-D and 3D information is provided with full annotations in both
spaces. Thanks to the high quality and quantity of both raw information and
annotations, the RobotriX will serve as a new milestone for investigating 2D
and 3D robotic vision tasks with large-scale data-driven techniques.","['Alberto Garcia-Garcia', 'Pablo Martinez-Gonzalez', 'Sergiu Oprea', 'John Alejandro Castro-Vargas', 'Sergio Orts-Escolano', 'Jose Garcia-Rodriguez', 'Alvaro Jover-Alvarez']",2019-01-19T12:49:56Z,http://arxiv.org/abs/1901.06514v1,"['cs.CV', 'cs.LG', 'cs.RO']","RobotriX,photorealistic,indoor dataset,deep learning,robotic vision,Unreal Engine,RGB-D,3D information,semantic classes,ground truth labels"
Real-time 3D Face-Eye Performance Capture of a Person Wearing VR Headset,"Teleconference or telepresence based on virtual reality (VR) headmount
display (HMD) device is a very interesting and promising application since HMD
can provide immersive feelings for users. However, in order to facilitate
face-to-face communications for HMD users, real-time 3D facial performance
capture of a person wearing HMD is needed, which is a very challenging task due
to the large occlusion caused by HMD. The existing limited solutions are very
complex either in setting or in approach as well as lacking the performance
capture of 3D eye gaze movement. In this paper, we propose a convolutional
neural network (CNN) based solution for real-time 3D face-eye performance
capture of HMD users without complex modification to devices. To address the
issue of lacking training data, we generate massive pairs of HMD face-label
dataset by data synthesis as well as collecting VR-IR eye dataset from multiple
subjects. Then, we train a dense-fitting network for facial region and an eye
gaze network to regress 3D eye model parameters. Extensive experimental results
demonstrate that our system can efficiently and effectively produce in real
time a vivid personalized 3D avatar with the correct identity, pose, expression
and eye motion corresponding to the HMD user.","['Guoxian Song', 'Jianfei Cai', 'Tat-Jen Cham', 'Jianmin Zheng', 'Juyong Zhang', 'Henry Fuchs']",2019-01-21T01:58:15Z,http://arxiv.org/abs/1901.06765v1,['cs.CV'],"Real-time,3D,Face-Eye,Performance Capture,VR Headset,Convolutional Neural Network,Eye Gaze Movement,Data Synthesis,Training Data"
"Immersive VR as a Tool to Enhance Relaxation for Undergraduate Students
  with the Aim of Reducing Anxiety - A Pilot Study","Despite extensive use in related domains, Virtual Reality (VR) for
generalised anxiety disorder (GAD) has received little previous attention. We
report upon a VR environment created for the Oculus Rift and Unreal Engine 4
(UE4) to investigate the potential of a VR simulation to be used as an anxiety
management tool. We introduce the broad topic of GAD and related publications
on the application of VR to this, and similar, mental health conditions. We
then describe the development of a real time simulation tool, based upon the
passive VR experience of a tranquil, rural alpine scene experienced from a
seated position with head tracking. Evaluation focused upon qualitative
feedback on the application. Testing was carried out over the period of two
weeks on a sample group of eleven students studying at Nottingham Trent
University. All participants were asked to complete the Depression, Anxiety and
Stress Scale - 21 Items (DASS21) at the beginning and at the end of the study
order to assess their profile, and hence suitability to comment upon the
software. Qualitative feedback was very encouraging, with all participants
reporting that they believed the experience helped and that they would consider
utilising it if it was available. Additionally, a psychologist was asked to
test the application to provide a specialist opinion on whether it would be
appropriate for use as an anxiety management tool. The results highlight
several areas for improvement but are positive overall in terms of its
potential as a therapeutic tool.","['James Lewis', 'Benedikte Rorstad']",2019-03-04T12:44:31Z,http://arxiv.org/abs/1903.01210v1,['cs.HC'],"Immersive VR,Relaxation,Undergraduate Students,Anxiety,Pilot Study,Virtual Reality,Oculus Rift,Unreal Engine 4,Anxiety Management Tool,Qualitative Feedback"
HTML5 MSE Playback of MPEG 360 VR Tiled Streaming,"Virtual Reality (VR) and 360-degree video streaming have gained significant
attention in recent years. First standards have been published in order to
avoid market fragmentation. For instance, 3GPP released its first VR
specification to enable 360-degree video streaming over 5G networks which
relies on several technologies specified in ISO/IEC 23090-2, also known as
MPEG-OMAF. While some implementations of OMAF-compatible players have already
been demonstrated at several trade shows, so far, no web browser-based
implementations have been presented. In this demo paper we describe a
browser-based JavaScript player implementation of the most advanced media
profile of OMAF: HEVC-based viewport-dependent OMAF video profile, also known
as tile-based streaming, with multi-resolution HEVC tiles. We also describe the
applied workarounds for the implementation challenges we encountered with
state-of-the-art HTML5 browsers. The presented implementation was tested in the
Safari browser with support of HEVC video through the HTML5 Media Source
Extensions API. In addition, the WebGL API was used for rendering, using
region-wise packing metadata as defined in OMAF.","['Dimitri Podborski', 'Jangwoo Son', 'Gurdeep Singh Bhullar', 'Robert Skupin', 'Yago Sanchez', 'Cornelius Hellge', 'Thomas Schierl']",2019-03-07T15:02:27Z,http://arxiv.org/abs/1903.02971v2,['cs.MM'],"HTML5,MSE,MPEG,360 VR,tiled streaming,VR,360-degree video,HEVC,OMAF,WebGL"
Holdable Haptic Device for 4-DOF Motion Guidance,"Hand-held haptic devices can allow for greater freedom of motion and larger
workspaces than traditional grounded haptic devices. They can also provide more
compelling haptic sensations to the users' fingertips than many wearable haptic
devices because reaction forces can be distributed over a larger area of skin
far away from the stimulation site. This paper presents a hand-held kinesthetic
gripper that provides guidance cues in four degrees of freedom (DOF). 2-DOF
tangential forces on the thumb and index finger combine to create cues to
translate or rotate the hand. We demonstrate the device's capabilities in a
three-part user study. First, users moved their hands in response to haptic
cues before receiving instruction or training. Then, they trained on cues in
eight directions in a forced-choice task. Finally, they repeated the first
part, now knowing what each cue intended to convey. Users were able to
discriminate each cue over 90% of the time. Users moved correctly in response
to the guidance cues both before and after the training and indicated that the
cues were easy to follow. The results show promise for holdable kinesthetic
devices in haptic feedback and guidance for applications such as virtual
reality, medical training, and teleoperation.","['Julie M. Walker', 'Nabil Zemiti', 'Philippe Poignet', 'Allison M. Okamura']",2019-03-07T19:54:44Z,http://arxiv.org/abs/1903.03150v1,['cs.RO'],"haptic devices,motion guidance,4-DOF,kinesthetic gripper,degrees of freedom,user study,haptic cues,virtual reality,medical training,teleoperation"
"A Path Planning Framework for a Flying Robot in Close Proximity of
  Humans","We present a path planning framework that takes into account the human's
safety perception in the presence of a flying robot. The framework addresses
two objectives: (i) estimation of the uncertain parameters of the proposed
safety perception model based on test data collected using Virtual Reality (VR)
testbed, and (ii) offline optimal control computation using the estimated
safety perception model. Due to the unknown factors in the human tests data, it
is not suitable to use standard regression techniques that minimize the mean
squared error (MSE). We propose to use a Hidden Markov model (HMM) approach
where human's attention is considered as a hidden state to infer whether the
data samples are relevant to learn the safety perception model. The HMM
approach improved log-likelihood over the standard least squares solution. For
path planning, we use Bernstein polynomials for discretization, as the
resulting path remains within the convex hull of the control points, providing
guarantees for deconfliction with obstacles at low computational cost. An
example of optimal trajectory generation using the learned human model is
presented. The optimal trajectory generated using the proposed model results in
reasonable safety distance from the human. In contrast, the paths generated
using the standard regression model have undesirable shapes due to overfitting.
The example demonstrates that the HMM approach has robustness to the unknown
factors compared to the standard MSE model.","['Hyung-Jin Yoon', 'Christopher Widdowson', 'Thiago Marinho', 'Ranxiao Frances Wang', 'Naira Hovakimyan']",2019-03-12T19:10:50Z,http://arxiv.org/abs/1903.05156v1,['cs.RO'],"path planning,flying robot,safety perception,Virtual Reality (VR) testbed,optimal control,estimation,Hidden Markov model (HMM),Bernstein polynomials,trajectory generation,obstacles"
"Capture, Learning, and Synthesis of 3D Speaking Styles","Audio-driven 3D facial animation has been widely explored, but achieving
realistic, human-like performance is still unsolved. This is due to the lack of
available 3D datasets, models, and standard evaluation metrics. To address
this, we introduce a unique 4D face dataset with about 29 minutes of 4D scans
captured at 60 fps and synchronized audio from 12 speakers. We then train a
neural network on our dataset that factors identity from facial motion. The
learned model, VOCA (Voice Operated Character Animation) takes any speech
signal as input - even speech in languages other than English - and
realistically animates a wide range of adult faces. Conditioning on subject
labels during training allows the model to learn a variety of realistic
speaking styles. VOCA also provides animator controls to alter speaking style,
identity-dependent facial shape, and pose (i.e. head, jaw, and eyeball
rotations) during animation. To our knowledge, VOCA is the only realistic 3D
facial animation model that is readily applicable to unseen subjects without
retargeting. This makes VOCA suitable for tasks like in-game video, virtual
reality avatars, or any scenario in which the speaker, speech, or language is
not known in advance. We make the dataset and model available for research
purposes at http://voca.is.tue.mpg.de.","['Daniel Cudeiro', 'Timo Bolkart', 'Cassidy Laidlaw', 'Anurag Ranjan', 'Michael J. Black']",2019-05-08T14:16:37Z,http://arxiv.org/abs/1905.03079v1,['cs.CV'],"Capture,Learning,Synthesis,3D facial animation,4D face dataset,Neural network,VOCA,Speaking styles,Facial motion,Animator controls"
OpenEDS: Open Eye Dataset,"We present a large scale data set, OpenEDS: Open Eye Dataset, of eye-images
captured using a virtual-reality (VR) head mounted display mounted with two
synchronized eyefacing cameras at a frame rate of 200 Hz under controlled
illumination. This dataset is compiled from video capture of the eye-region
collected from 152 individual participants and is divided into four subsets:
(i) 12,759 images with pixel-level annotations for key eye-regions: iris, pupil
and sclera (ii) 252,690 unlabelled eye-images, (iii) 91,200 frames from
randomly selected video sequence of 1.5 seconds in duration and (iv) 143 pairs
of left and right point cloud data compiled from corneal topography of eye
regions collected from a subset, 143 out of 152, participants in the study. A
baseline experiment has been evaluated on OpenEDS for the task of semantic
segmentation of pupil, iris, sclera and background, with the mean
intersectionover-union (mIoU) of 98.3 %. We anticipate that OpenEDS will create
opportunities to researchers in the eye tracking community and the broader
machine learning and computer vision community to advance the state of
eye-tracking for VR applications. The dataset is available for download upon
request at https://research.fb.com/programs/openeds-challenge","['Stephan J. Garbin', 'Yiru Shen', 'Immo Schuetz', 'Robert Cavin', 'Gregory Hughes', 'Sachin S. Talathi']",2019-04-30T17:47:53Z,http://arxiv.org/abs/1905.03702v2,"['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']","OpenEDS,Eye Dataset,virtual-reality,eyefacing cameras,pixel-level annotations,semantic segmentation,intersection-over-union,eye tracking,machine learning,computer vision"
Wireless Edge Computing with Latency and Reliability Guarantees,"Edge computing is an emerging concept based on distributing computing,
storage, and control services closer to end network nodes. Edge computing lies
at the heart of the fifth generation (5G) wireless systems and beyond. While
current state-of-the-art networks communicate, compute, and process data in a
centralized manner (at the cloud), for latency and compute-centric
applications, both radio access and computational resources must be brought
closer to the edge, harnessing the availability of computing and
storage-enabled small cell base stations in proximity to the end devices.
Furthermore, the network infrastructure must enable a distributed edge
decision-making service that learns to adapt to the network dynamics with
minimal latency and optimize network deployment and operation accordingly. This
article will provide a fresh look to the concept of edge computing by first
discussing the applications that the network edge must provide, with a special
emphasis on the ensuing challenges in enabling ultra-reliable and low-latency
edge computing services for mission-critical applications such as virtual
reality (VR), vehicle-to-everything (V2X), edge artificial intelligence (AI),
and so forth. Furthermore, several case studies where the edge is key are
explored followed by insights and prospect for future work.","['Mohammed S. Elbamby', 'Cristina Perfecto', 'Chen-Feng Liu', 'Jihong Park', 'Sumudu Samarakoon', 'Xianfu Chen', 'Mehdi Bennis']",2019-05-13T23:25:10Z,http://arxiv.org/abs/1905.05316v1,"['cs.NI', 'eess.SP']","edge computing,latency,reliability guarantees,5G,radio access,small cell base stations,distributed decision-making,network deployment,ultra-reliable,low-latency"
"A Qualitative Post-Experience Method for Evaluating Changes in VR
  Presence Experience Over Time","A particular measure to evaluate a head-mounted display (HMD) based
experience is the state of feeling present in virtual reality. Interruptions of
a presence experience - break in presence (BIP) - appearing over time, need to
be detected to assess and improve an application. Existing methods either lack
in taking these BIPs into account - questionnaires - or are complex in their
application and evaluation - physiological and behavioral measures -. To
provide a practical approach, we propose a post-experience method in which the
users reflect on their experience by drawing a line, indicating their
experienced state of presence, in a paper-based drawing template. The amplitude
of the drawn line represents the variation of their presence experience over
time. We propose a descriptive model that describes temporal variations in the
drawings by the definition of relevant points over time - e.g., putting on the
HMD -, phases of the experience - e.g., transition into VR - and parameters -
e.g., the transition time -. The descriptive model enables us to objectively
evaluate user drawings and represent the course of the drawings by a defined
set of parameters. An exploratory user study (N=30) showed that the drawings
are very consistent, the method can detect all BIPs and shows good indications
for representing the intensity of a BIP. With our method practitioners and
researchers can accelerate the evaluation and optimization of experiences by
evaluating BIPs. The possibility to store objective parameters paves the way
for automated evaluation methods and big data approaches.","['Christian Mai', 'Heinrich Hußmann']",2019-05-14T15:36:17Z,http://arxiv.org/abs/1905.05673v1,['cs.HC'],"Virtual reality,Presence experience,Break in presence,Head-mounted display,User study,Evaluation method,Descriptive model,Post-experience method,Parameters,Automated evaluation"
"FlexNGIA: A Flexible Internet Architecture for the Next-Generation
  Tactile Internet","From virtual reality and telepresence, to augmented reality, holoportation,
and remotely controlled robotics, these future network applications promise an
unprecedented development for society, economics and culture by revolutionizing
the way we live, learn, work and play. In order to deploy such futuristic
applications and to cater to their performance requirements, recent trends
stressed the need for the Tactile Internet, an Internet that, according to the
International Telecommunication Union, combines ultra low latency with
extremely high availability, reliability and security. Unfortunately, today's
Internet falls short when it comes to providing such stringent requirements due
to several fundamental limitations in the design of the current network
architecture and communication protocols. This brings the need to rethink the
network architecture and protocols, and efficiently harness recent
technological advances in terms of virtualization and network softwarization to
design the Tactile Internet of the future.
  In this paper, we start by analyzing the characteristics and requirements of
future networking applications. We then highlight the limitations of the
traditional network architecture and protocols and their inability to cater to
these requirements. Afterward, we put forward a novel network architecture
adapted to the Tactile Internet called FlexNGIA, a Flexible Next-Generation
Internet Architecture. We then describe some use-cases where we discuss the
potential mechanisms and control loops that could be offered by FlexNGIA in
order to ensure the required performance and reliability guarantees for future
applications. Finally, we identify the key research challenges to further
develop FlexNGIA towards a full-fledged architecture for the future Tactile
Internet.","['Mohamed Faten Zhani', 'Hesham ElBakoury']",2019-05-17T07:24:51Z,http://arxiv.org/abs/1905.07137v2,"['cs.NI', 'cs.ET']","Tactile Internet,Network architecture,Communication protocols,Ultra low latency,Virtualization,Network softwarization,FlexNGIA,Control loops,Performance requirements,Next-Generation Internet Architecture"
"From heterogeneous data to heterogeneous public: thoughts on transmedia
  applications for digital heritage research and dissemination","In recent years, we have seen a tenfold increase in volume and complexity of
digital data acquired for cultural heritage documentation. Meanwhile, open data
and open science have become leading trends in digital humanities. The
convergence of those two parameters compels us to deliver, in an interoperable
fashion, datasets that are vastly heterogeneous both in content and format and,
moreover, in such a way that they fit the expectation of a broad array of
researchers and an even broader public audience. Tackling those issues is one
of the main goal of the ""HeritageS"" digital platform project supported by the
""Intelligence des Patrimoines"" research program. This platform is designed to
allow research projects from many interdisciplinary fields to share, integrate
and valorize cultural and natural heritage datasets related to the Loire
Valley. In this regard, one of our main project is the creation of the
""Renaissance Transmedia Lab"". Its core element is a website which acts as a hub
to access various interactive experiences linked to project about the
Renaissance period: augmented web-documentary, serious game, virtual reality,
3D application. We expect to leverage those transmedia experiences to foster
better communication between researchers and the public while keeping the
quality of scientific discourse. By presenting the current and upcoming
productions, we intend to share our experience with other participants:
preparatory work and how we cope with researchers to produce, in concertation,
tailor-made experiences that convey the desired scientific discourse while
remaining appealing to the general public.","['Damien Vurpillot', 'Perrine Pittet', 'Johann Forte', 'Benoist Pierre']",2019-05-22T07:07:34Z,http://arxiv.org/abs/1905.08988v1,"['cs.HC', 'cs.DL']","digital data,cultural heritage,open data,open science,digital humanities,interoperable,transmedia,augmented reality,virtual reality,3D application."
"Bounding Queue Delay in Cellular Networks to Support Ultra-Low Latency
  Applications","Most of the current active queue management (AQM) designs have major issues
including severe hardship of being tuned for highly fluctuated cellular access
link bandwidths. Consequently, most of the cellular network providers either
give up using AQMs or use conservative offline configurations for them.
However, these choices will significantly impact the performance of the
emerging interactive and highly delay sensitive applications such as virtual
reality and vehicle-to-vehicle communications.
  Therefore, in this paper, we investigate the problems of existing AQM schemes
and show that they are not suitable options to support ultra-low latency
applications in a highly dynamic network such as current and future cellular
networks. Moreover, we believe that achieving good performance does not
necessarily come from complex drop rate calculation algorithms or complicated
AQM techniques. Consequently, we propose BoDe an extremely simple and
deployment friendly AQM scheme to bound the queuing delay of served packets and
support ultra-low latency applications.
  We have evaluated BoDe in extensive trace-based evaluations using cellular
traces from 3 different service providers in the US and compared its
performance with state-of-the-art AQM designs including CoDel and PIE under a
variety of streaming applications, video conferencing applications, and various
recently proposed TCP protocols. Results show that despite BoDe's simple
design, it outperforms other schemes and achieves significantly lower queuing
delay in all tested scenarios.","['Soheil Abbasloo', 'H. Jonathan Chao']",2019-08-02T16:55:03Z,http://arxiv.org/abs/1908.00953v1,['cs.NI'],"queue delay,cellular networks,ultra-low latency,active queue management,bandwidth,virtual reality,vehicle-to-vehicle communications,BoDe,packet queuing"
"DronePick: Object Picking and Delivery Teleoperation with the Drone
  Controlled by a Wearable Tactile Display","We report on the teleoperation system DronePick which provides remote object
picking and delivery by a human-controlled quadcopter. The main novelty of the
proposed system is that the human user continuously gets the visual and haptic
feedback for accurate teleoperation. DronePick consists of a quadcopter
equipped with a magnetic grabber, a tactile glove with finger motion tracking
sensor, hand tracking system, and the Virtual Reality (VR) application. The
human operator teleoperates the quadcopter by changing the position of the
hand. The proposed vibrotactile patterns representing the location of the
remote object relative to the quadcopter are delivered to the glove. It helps
the operator to determine when the quadcopter is right above the object. When
the ""pick"" command is sent by clasping the hand in the glove, the quadcopter
decreases its altitude and the magnetic grabber attaches the target object. The
whole scenario is in parallel simulated in VR. The air flow from the quadcopter
and the relative positions of VR objects help the operator to determine the
exact position of the delivered object to be picked. The experiments showed
that the vibrotactile patterns were recognized by the users at the high
recognition rates: the average 99% recognition rate and the average 2.36s
recognition time. The real-life implementation of DronePick featuring object
picking and delivering to the human was developed and tested.","['Roman Ibrahimov', 'Evgeny Tsykunov', 'Vladimir Shirokun', 'Andrey Somov', 'Dzmitry Tsetserukou']",2019-08-07T03:49:46Z,http://arxiv.org/abs/1908.02432v1,"['cs.RO', 'cs.HC']","DronePick,Object picking,Delivery,Teleoperation,Wearable tactile display,Quadcopter,Magnetic grabber,Tactile glove,Virtual Reality (VR),Vibrotactile patterns"
"Epistemological approach in immersive virtual environments and the
  neurophysiology learning process","Currently virtual reality (VR) usage in training processes is increasing due
to their usefulness in the learning processes based on visual information
empowered. The information in virtual environments is perceived by sight, sound
and touch, but the relationship or impact that these stimuli can have on the
oscillatory activity of the brain such as the processing, propagation and
synchronization of information still needs to be established in relation to the
cognitive load of attention. Therefore, this study seeks to identify the
suggested epistemological basis through literature review and current research
agendas in the relationship that exists between the immersive virtual
environment and the neurophysiology of learning processes by means of the
analysis of visual information. The suggested dimensional modeling of this
research is composed by the theory of information processing which allows the
incorporation of learning through stimuli with the use of attention, perception
and storage by means of information management and the Kolb's learning model
which defines the perception and processing of information as dimensions of
learning. Regarding to the neurophysiology of learning, the literature has
established he links between the prefrontal cortex and working memory within
the process of information management. The challenges and advances discussed in
this research are based in the relationship between the identified constructs
(Income Stimuli, Information Management and Cognitive Processing) and the
establishment of a research agenda on how to identify the necessary indicators
to measure memory and attention in the virtual immersion environments.",['Cesar R. Salas Guerra'],2019-08-04T05:41:11Z,http://arxiv.org/abs/1908.05240v1,"['q-bio.NC', 'cs.HC']","virtual reality,neurophysiology,learning process,information processing,cognitive load,attention,perception,working memory,stimuli,immersive virtual environments"
On VR Spatial Query for Dual Entangled Worlds,"With the rapid advent of Virtual Reality (VR) technology and virtual tour
applications, there is a research need on spatial queries tailored for
simultaneous movements in both the physical and virtual worlds. Traditional
spatial queries, designed mainly for one world, do not consider the entangled
dual worlds in VR. In this paper, we first investigate the fundamental
shortest-path query in VR as the building block for spatial queries, aiming to
avoid hitting boundaries and obstacles in the physical environment by
leveraging Redirected Walking (RW) in Computer Graphics. Specifically, we first
formulate Dual-world Redirected-walking Obstacle-free Path (DROP) to find the
minimum-distance path in the virtual world, which is constrained by the RW cost
in the physical world to ensure immersive experience in VR. We prove DROP is
NP-hard and design a fully polynomial-time approximation scheme, Dual Entangled
World Navigation (DEWN), by finding Minimum Immersion Loss Range (MIL Range).
Afterward, we show that the existing spatial query algorithms and index
structures can leverage DEWN as a building block to support kNN and range
queries in the dual worlds of VR. Experimental results and a user study with
implementation in HTC VIVE manifest that DEWN outperforms the baselines with
smoother RW operations in various VR scenarios.","['Shao-Heng Ko', 'Ying-Chun Lin', 'Hsu-Chao Lai', 'Wang-Chien Lee', 'De-Nian Yang']",2019-08-23T06:53:07Z,http://arxiv.org/abs/1908.08691v1,"['cs.DS', 'F.2.2; G.2.2']","Virtual Reality,Spatial Query,Dual Worlds,Redirected Walking,Computer Graphics,NP-hard,Approximation Scheme,Index Structures,kNN,Range Queries"
"Human Visual Attention Prediction Boosts Learning & Performance of
  Autonomous Driving Agents","Autonomous driving is a multi-task problem requiring a deep understanding of
the visual environment. End-to-end autonomous systems have attracted increasing
interest as a method of learning to drive without exhaustively programming
behaviours for different driving scenarios. When humans drive, they rely on a
finely tuned sensory system which enables them to quickly acquire the
information they need while filtering unnecessary details. This ability to
identify task-specific high-interest regions within an image could be
beneficial to autonomous driving agents and machine learning systems in
general. To create a system capable of imitating human gaze patterns and visual
attention, we collect eye movement data from human drivers in a virtual reality
environment. We use this data to train deep neural networks predicting where
humans are most likely to look when driving. We then use the outputs of this
trained network to selectively mask driving images using a variety of masking
techniques. Finally, autonomous driving agents are trained using these masked
images as input. Upon comparison, we found that a dual-branch architecture
which processes both raw and attention-masked images substantially outperforms
all other models, reducing error in control signal predictions by 25.5\%
compared to a standard end-to-end model trained only on raw images.","['Alexander Makrigiorgos', 'Ali Shafti', 'Alex Harston', 'Julien Gerard', 'A. Aldo Faisal']",2019-09-11T12:25:22Z,http://arxiv.org/abs/1909.05003v1,"['cs.CV', 'cs.RO']","visual attention,autonomous driving,deep neural networks,human gaze patterns,machine learning,eye movement data,virtual reality,masking techniques,dual-branch architecture,control signal predictions"
"Scenior: An Immersive Visual Scripting system based on VR Software
  Design Patterns for Experiential Training","Virtual reality (VR) has re-emerged as a low-cost, highly accessible consumer
product, and training on simulators is rapidly becoming standard in many
industrial sectors. However, the available systems are either focusing on
gaming context, featuring limited capabilities or they support only content
creation of virtual environments without any rapid prototyping and
modification. In this project, we propose a code-free, visual scripting
platform to replicate gamified training scenarios through rapid prototyping and
VR software design patterns. We implemented and compared two authoring tools:
a) visual scripting and b) VR editor for the rapid reconstruction of VR
training scenarios. Our visual scripting module is capable to generate training
applications utilizing a node-based scripting system whereas the VR editor
gives user/developer the ability to customize and populate new VR training
scenarios directly from the virtual environment. We also introduce action
prototypes, a new software design pattern suitable to replicate behavioral
tasks for VR experiences. In addition, we present the training scenegraph
architecture as the main model to represent training scenarios on a modular,
dynamic and highly adaptive acyclic graph based on a structured educational
curriculum. Finally, a user-based evaluation of the proposed solution indicated
that users - regardless of their programming expertise - can effectively use
the tools to create and modify training scenarios in VR.","['Paul Zikas', 'George Papagiannakis', 'Nick Lydatakis', 'Steve Kateros', 'Stavroula Ntoa', 'Ilia Adami', 'Constantine Stephanidis']",2019-09-12T14:37:13Z,http://arxiv.org/abs/1909.05719v2,"['cs.GR', 'cs.HC', '97B70, 97P10,', 'K.3.1; I.3.6; D.1.7']","immersive visual scripting,VR software,design patterns,experiential training,rapid prototyping,authoring tools,gamified training scenarios,node-based scripting,VR editor,training scenegraph architecture"
A True AR Authoring Tool for Interactive Virtual Museums,"In this work, a new and innovative way of spatial computing that appeared
recently in the bibliography called True Augmented Reality (AR), is employed in
cultural heritage preservation. This innovation could be adapted by the Virtual
Museums of the future to enhance the quality of experience. It emphasises, the
fact that a visitor will not be able to tell, at a first glance, if the
artefact that he/she is looking at is real or not and it is expected to draw
the visitors' interest. True AR is not limited to artefacts but extends even to
buildings or life-sized character simulations of statues. It provides the best
visual quality possible so that the users will not be able to tell the real
objects from the augmented ones. Such applications can be beneficial for future
museums, as with True AR, 3D models of various exhibits, monuments, statues,
characters and buildings can be reconstructed and presented to the visitors in
a realistic and innovative way. We also propose our Virtual Reality Sample
application, a True AR playground featuring basic components and tools for
generating interactive Virtual Museum applications, alongside a 3D
reconstructed character (the priest of Asinou church) facilitating the
storyteller of the augmented experience.","['Efstratios Geronikolakis', 'Paul Zikas', 'Steve Kateros', 'Nick Lydatakis', 'Stelios Georgiou', 'Mike Kentros', 'George Papagiannakis']",2019-09-20T11:10:23Z,http://arxiv.org/abs/1909.09429v4,"['cs.GR', 'cs.HC', '68U05', 'I.3.8; I.3.7']","True Augmented Reality,spatial computing,Virtual Museums,cultural heritage preservation,artefacts,3D models,monuments,statues,buildings,interactive Virtual Museum"
"Adaptive Generation of Phantom Limbs Using Visible Hierarchical
  Autoencoders","This paper proposed a hierarchical visible autoencoder in the adaptive
phantom limbs generation according to the kinetic behavior of functional
body-parts, which are measured by heterogeneous kinetic sensors. The proposed
visible hierarchical autoencoder consists of interpretable and multi-correlated
autoencoder pipelines, which is directly derived from the hierarchical network
described in forest data-structure. According to specified kinetic script
(e.g., dancing, running, etc.) and users' physical conditions, hierarchical
network is extracted from human musculoskeletal network, which is fabricated by
multiple body components (e.g., muscle, bone, and joints, etc.) that are
bio-mechanically, functionally, or nervously correlated with each other and
exhibit mostly non-divergent kinetic behaviors. Multi-layer perceptron (MLP)
regressor models, as well as several variations of autoencoder models, are
investigated for the sequential generation of missing or dysfunctional limbs.
The resulting kinematic behavior of phantom limbs will be constructed using
virtual reality and augmented reality (VR/AR), actuators, and potentially
controller for a prosthesis (an artificial device that replaces a missing body
part). The addressed work aims to develop practical innovative exercise methods
that (1) engage individuals at all ages, including those with a chronic health
condition(s) and/or disability, in regular physical activities, (2) accelerate
the rehabilitation of patients, and (3) release users' phantom limb pain. The
physiological and psychological impact of the addressed work will critically be
assessed in future work.","['Dakila Ledesma', 'Yu Liang', 'Dalei Wu']",2019-10-02T19:54:19Z,http://arxiv.org/abs/1910.01191v1,"['cs.HC', 'cs.LG']","adaptive generation,phantom limbs,visible hierarchical autoencoders,kinetic behavior,hierarchical network,musculoskeletal network,multi-layer perceptron (MLP),autoencoder models,virtual reality,augmented reality (VR/AR)"
"Secondary Inputs for Measuring User Engagement in Immersive VR Education
  Environments","This paper presents an experiment to assess the feasibility of using
secondary input data as a method of determining user engagement in immersive
virtual reality (VR). The work investigates whether secondary data (biosignals)
acquired from users are useful as a method of detecting levels of
concentration, stress, relaxation etc. in immersive environments, and if they
could be used to create an affective feedback loop in immersive VR
environments, including educational contexts. A VR Experience was developed in
the Unity game engine, with three different levels, each designed to expose the
user in one of three different states (relaxation, concentration, stress).
While in the VR Experience users physiological responses were measured using
ECG and EEG sensors. After the experience users completed questionnaires to
establish their perceived state during the levels, and to established the
usability of the system. Next a comparison between the reported levels of
emotion and the measured signals is presented, which show a strong
correspondence between the two measures indicating that biosignals are a useful
indicator of emotional state while in VR. Finally we make some recommendations
on the practicalities of using biosensors, and design considerations for their
incorporation in to a VR system, with particular focus on their integration in
to task-based training and educational virtual environments.","['David Murphy', 'Conor Higgins']",2019-10-03T16:39:28Z,http://arxiv.org/abs/1910.01586v1,"['cs.HC', 'cs.GR', 'cs.MM']","User Engagment,Immersive VR,Secondary Inputs,Biosignals,Concentration,Stress,Relaxation,ECG,EEG,Educational Virtual Environments"
"Prediction, Communication, and Computing Duration Optimization for VR
  Video Streaming","Proactive tile-based video streaming can avoid motion-to-photon latency of
wireless virtual reality (VR) by computing and delivering the predicted tiles
to be requested before playback. All existing works either focus on designing
predictors or allocating computing and communications resources. Yet to avoid
the latency, the successively executed prediction, communication, and computing
tasks should be accomplished within a predetermined time. Moreover, the quality
of experience (QoE) of proactive VR streaming depends on the worst performance
of the three tasks. In this paper, we jointly optimize the duration of the
observation window for predicting tiles and the durations for computing and
transmitting the predicted tiles, aimed at balancing the performance for three
tasks to maximize the QoE given arbitrary predictor and configured resources.
We obtain the closed-form optimal solution by decomposing the formulated
problem equivalently into two subproblems. With the optimized durations, we
find a resource-limited region where the QoE increases rapidly with configured
resources, and a prediction-limited region where the QoE can be improved more
efficiently with a better predictor. Simulation results using three existing
predictors and a real dataset validate the analysis and demonstrate the gain
from the joint optimization over non-optimized counterparts.","['Xing Wei', 'Chenyang Yang', 'Shengqian Han']",2019-10-30T14:27:58Z,http://arxiv.org/abs/1910.13884v5,"['cs.IT', 'cs.MM', 'eess.SP', 'math.IT']","Prediction,Communication,Computing,Duration Optimization,VR Video Streaming,Tile-based,Motion-to-photon latency,Quality of Experience (QoE),Observational window,Resource allocation"
"Isness: Using Multi-Person VR to Design Peak Mystical-Type Experiences
  Comparable to Psychedelics","Studies combining psychotherapy with psychedelic drugs (PsiDs) have
demonstrated positive outcomes that are often associated with PsiDs' ability to
induce 'mystical-type' experiences (MTEs) - i.e., subjective experiences whose
characteristics include a sense of connectedness, transcendence, and
ineffability. We suggest that both PsiDs and virtual reality can be situated on
a broader spectrum of psychedelic technologies. To test this hypothesis, we
used concepts, methods, and analysis strategies from PsiD research to design
and evaluate 'Isness', a multi-person VR journey where participants experience
the collective emergence, fluctuation, and dissipation of their bodies as
energetic essences. A study (N=57) analyzing participant responses to a
commonly used PsiD experience questionnaire (MEQ30) indicates that Isness
participants had MTEs comparable to those reported in double-blind clinical
studies after high doses of psilocybin & LSD. Within a supportive setting and
conceptual framework, VR phenomenology can create the conditions for MTEs from
which participants derive insight and meaning.","['David R. Glowacki', 'Mark D. Wonnacott', 'Rachel Freire', 'Becca R. Glowacki', 'Ella M. Gale', 'James E. Pike', 'Tiu de Haan', 'Mike Chatziapostolou', 'Oussama Metatla']",2020-02-03T18:58:09Z,http://arxiv.org/abs/2002.00940v2,['cs.HC'],"psychotherapy,psychedelic drugs,mystical-type experiences,virtual reality,psychedelic technologies,Isness,energetic essences,MEQ30,psilocybin,LSD"
"A comparison of mobile VR display running on an ordinary smartphone with
  standard PC display for P300-BCI stimulus presentation","A brain-computer interface (BCI) based on electroencephalography (EEG) is a
promising technology for enhancing virtual reality (VR) applications-in
particular, for gaming. We focus on the so-called P300-BCI, a stable and
accurate BCI paradigm relying on the recognition of a positive event-related
potential (ERP) occurring in the EEG about 300 ms post-stimulation. We
implemented a basic version of such a BCI displayed on an ordinary and
affordable smartphone-based head-mounted VR device: that is, a mobile and
passive VR system (with no electronic components beyond the smartphone). The
mobile phone performed the stimuli presentation, EEG synchronization (tagging)
and feedback display. We compared the ERPs and the accuracy of the BCI on the
VR device with a traditional BCI running on a personal computer (PC). We also
evaluated the impact of subjective factors on the accuracy. The study was
within-subjects, with 21 participants and one session in each modality. No
significant difference in BCI accuracy was found between the PC and VR systems,
although the P200 ERP was significantly wider and larger in the VR system as
compared to the PC system.","['Grégoire Cattan', 'Anton Andreev', 'Cesar Mendoza', 'Marco Congedo']",2020-02-06T17:04:17Z,http://arxiv.org/abs/2002.02358v1,"['cs.HC', 'cs.GR']","brain-computer interface,electroencephalography,P300-BCI,virtual reality,stimulus presentation,smartphone,PC,event-related potential,EEG synchronization,ERP"
"Adaptive Task Partitioning at Local Device or Remote Edge Server for
  Offloading in MEC","Mobile edge computing (MEC) is one of the promising solutions to process
computational-intensive tasks for the emerging time-critical Internet-of-Things
(IoT) use cases, e.g., virtual reality (VR), augmented reality (AR), autonomous
vehicle. The latency can be reduced further, when a task is partitioned and
computed by multiple edge servers' (ESs) collaboration. However, the
state-of-the-art work studies the MEC-enabled offloading based on a static
framework, which partitions tasks at either the local user equipment (UE) or
the primary ES. The dynamic selection between the two offloading schemes has
not been well studied yet. In this paper, we investigate a dynamic offloading
framework in a multi-user scenario. Each UE can decide who partitions a task
according to the network status, e.g., channel quality and allocated
computation resource. Based on the framework, we model the latency to complete
a task, and formulate an optimization problem to minimize the average latency
among UEs. The problem is solved by jointly optimizing task partitioning and
the allocation of the communication and computation resources. The numerical
results show that, compared with the static offloading schemes, the proposed
algorithm achieves the lower latency in all tested scenarios. Moreover, both
mathematical derivation and simulation illustrate that the wireless channel
quality difference between a UE and different ESs can be used as an important
criterion to determine the right scheme.","['Jianhui Liu', 'Qi Zhang']",2020-02-12T09:13:07Z,http://arxiv.org/abs/2002.04858v1,"['cs.NI', 'eess.SP']","Adaptive Task Partitioning,Local Device,Remote Edge Server,Offloading,MEC,Latency,Task Partitioning,Edge Servers,Multi-user Scenario,Computation Resources"
"Mobile Communications, Computing and Caching Resources Optimization for
  Coded Caching with Device Computing","Edge caching and computing have been regarded as an efficient approach to
tackle the wireless spectrum crunch problem. In this paper, we design a general
coded caching with device computing strategy for content computation, e.g.,
virtual reality (VR) rendering, to minimize the average transmission bandwidth
with the caching capacity and the energy constraints of each mobile device, and
the maximum tolerable delay constraint of each task. The key enabler is that
because both coded data and stored data can be the data before or after
computing, the proposed scheme has numerous edge computing and caching paths
corresponding to different bandwidth requirement. We thus formulate a joint
coded caching and computing optimization problem to decide whether the mobile
devices cache the input data or the output data, which tasks to be coded cached
and which tasks to compute locally. The optimization problem is shown to be 0-1
nonconvex nonsmooth programming and can be decomposed into the computation
programming and the coded caching programming. We prove the convergence of the
computation programming problem by utilizing the alternating direction method
of multipliers (ADMM), and a stationary point can be obtained. For the coded
cache programming, we design a low complexity algorithm to obtain an acceptable
solution. Numerical results demonstrate that the proposed scheme provides a
significant bandwidth saving by taking full advantage of the caching and
computing capability of mobile devices.","['Yingjiao Li', 'Zhiyong Chen', 'Meixia Tao']",2020-02-14T15:53:34Z,http://arxiv.org/abs/2002.06090v1,"['cs.IT', 'eess.SP', 'math.IT']","Mobile Communications,Computing,Caching Resources Optimization,Coded Caching,Device Computing,Edge Computing,Transmission Bandwidth,Energy Constraints,Virtual Reality (VR)"
"Is Deadline Oblivious Scheduling Efficient for Controlling Real-Time
  Traffic in Cellular Downlink Systems?","The emergence of bandwidth-intensive latency-critical traffic in 5G Networks,
such as Virtual Reality, has motivated interest in wireless resource allocation
problems for flows with hard-deadlines. Attempting to solve this problem brings
about two challenges: (i) The flow arrival and the channel state are not known
to the Base Station (BS) apriori, thus, the allocation decisions need to be
made online. (ii) Wireless resource allocation algorithms that attempt to
maximize a reward will likely be unfair, causing unacceptable service for some
users. We model the problem as an online convex optimization problem. We
propose a primal-dual Deadline-Oblivious (DO) algorithm, and show it is
approximately 3.6-competitive. Furthermore, we show via simulations that our
algorithm tracks the prescient offline solution very closely, significantly
outperforming several existing algorithms. In the second part, we impose a
stochastic constraint on the allocation, requiring a guarantee that each user
achieves a certain timely throughput (amount of traffic delivered within the
deadline over a period of time). We propose the Long-term Fair Deadline
Oblivious (LFDO) algorithm for that setup. We combine the Lyapunov framework
with analysis of online algorithms, to show that LFDO retains the
high-performance of DO, while satisfying the long-term stochastic constraints.","['Sherif ElAzzouni', 'Eylem Ekici', 'Ness Shroff']",2020-02-15T23:52:01Z,http://arxiv.org/abs/2002.06474v1,"['cs.NI', 'math.OC']","Real-Time Traffic,Cellular Downlink Systems,Bandwidth-Intensive,Latency-Critical Traffic,Wireless Resource Allocation,Hard-Deadlines,Online Convex Optimization,Deadline-Oblivious Algorithm,Stochastic Constraint"
"Familiarization tours for first-time users of highly automated cars:
  Comparing the effects of virtual environments with different levels of
  interaction fidelity","Research in aviation and driving has highlighted the importance of training
as an effective approach to reduce the costs associated with the supervisory
role of the human in automated systems. However, only a few studies have
investigated the effect of pre-trip familiarization tours on highly automated
driving. In the present study, a driving simulator experiment compared the
effectiveness of four familiarization groups, control, video, low fidelity
virtual reality (VR), and high fidelity VR on automation trust and driving
performance in several critical and non-critical transition tasks. The results
revealed the positive impact of familiarization tours on trust, takeover, and
handback performance at the first time of measurement. Takeover quality only
improved when practice was presented in high-fidelity VR. After three times of
exposure to transition requests, trust and transition performance of all groups
converged to those of the high fidelity VR group, demonstrating that: a)
experiencing automation failures during the training may reduce costs
associated with first failures in highly automated driving; b) the VR tour with
high level of interaction fidelity is superior to other types of
familiarization tour, and c) uneducated and less-educated drivers learn about
automation by experiencing it. Knowledge resulting from this research could
help develop cost-effective familiarization tours for highly automated vehicles
in dealerships and car rental centers.","['Mahdi Ebnali', 'Richard Lamb', 'Razieh Fathi']",2020-02-19T02:44:34Z,http://arxiv.org/abs/2002.07968v1,['cs.HC'],"training,familiarization tours,highly automated cars,virtual environments,interaction fidelity,automation trust,driving performance,driving simulator experiment."
Taurus: A Data Plane Architecture for Per-Packet ML,"Emerging applications -- cloud computing, the internet of things, and
augmented/virtual reality -- demand responsive, secure, and scalable datacenter
networks. These networks currently implement simple, per-packet, data-plane
heuristics (e.g., ECMP and sketches) under a slow, millisecond-latency control
plane that runs data-driven performance and security policies. However, to meet
applications' service-level objectives (SLOs) in a modern data center, networks
must bridge the gap between line-rate, per-packet execution and complex
decision making.
  In this work, we present the design and implementation of Taurus, a data
plane for line-rate inference. Taurus adds custom hardware based on a flexible,
parallel-patterns (MapReduce) abstraction to programmable network devices, such
as switches and NICs; this new hardware uses pipelined SIMD parallelism to
enable per-packet MapReduce operations (e.g., inference). Our evaluation of a
Taurus switch ASIC -- supporting several real-world models -- shows that Taurus
operates orders of magnitude faster than a server-based control plane while
increasing area by 3.8% and latency for line-rate ML models by up to 221 ns.
Furthermore, our Taurus FPGA prototype achieves full model accuracy and detects
two orders of magnitude more events than a state-of-the-art control-plane
anomaly-detection system.","['Tushar Swamy', 'Alexander Rucker', 'Muhammad Shahbaz', 'Ishan Gaur', 'Kunle Olukotun']",2020-02-12T09:18:36Z,http://arxiv.org/abs/2002.08987v2,"['cs.NI', 'cs.LG', 'cs.PF']","Data Plane Architecture,Per-Packet,ML,Line-rate,Inference,Custom Hardware,Programmable Network Devices,MapReduce Abstraction,Switch ASIC,FPGA"
"Blind Omnidirectional Image Quality Assessment with Viewport Oriented
  Graph Convolutional Networks","Quality assessment of omnidirectional images has become increasingly urgent
due to the rapid growth of virtual reality applications. Different from
traditional 2D images and videos, omnidirectional contents can provide
consumers with freely changeable viewports and a larger field of view covering
the $360^{\circ}\times180^{\circ}$ spherical surface, which makes the objective
quality assessment of omnidirectional images more challenging. In this paper,
motivated by the characteristics of the human vision system (HVS) and the
viewing process of omnidirectional contents, we propose a novel Viewport
oriented Graph Convolution Network (VGCN) for blind omnidirectional image
quality assessment (IQA). Generally, observers tend to give the subjective
rating of a 360-degree image after passing and aggregating different viewports
information when browsing the spherical scenery. Therefore, in order to model
the mutual dependency of viewports in the omnidirectional image, we build a
spatial viewport graph. Specifically, the graph nodes are first defined with
selected viewports with higher probabilities to be seen, which is inspired by
the HVS that human beings are more sensitive to structural information. Then,
these nodes are connected by spatial relations to capture interactions among
them. Finally, reasoning on the proposed graph is performed via graph
convolutional networks. Moreover, we simultaneously obtain global quality using
the entire omnidirectional image without viewport sampling to boost the
performance according to the viewing experience. Experimental results
demonstrate that our proposed model outperforms state-of-the-art full-reference
and no-reference IQA metrics on two public omnidirectional IQA databases.","['Jiahua Xu', 'Wei Zhou', 'Zhibo Chen']",2020-02-21T05:54:20Z,http://arxiv.org/abs/2002.09140v2,"['eess.IV', 'cs.MM']","omnidirectional images,quality assessment,viewport,graph convolutional networks,human vision system,spatial relations,graph convolutional networks,IQA metrics"
"Impact of Visuomotor Feedback on the Embodiment of Virtual Hands
  Detached from the Body","It has been shown that mere observation of body discontinuity leads to
diminished body ownership. However, the impact of body discontinuity has mainly
been investigated in conditions where participants observe a collocated static
virtual body from a first-person perspective. This study explores the influence
of body discountinuity on the sense of embodiment, when rich visuomotor
correlations between a real and an artificial virtual body are established. In
two experiments, we evaluated body ownership and motor performance, when
participants interacted in virtual reality either using virtual hands connected
or disconnected from a body. We found that even under the presence of congruent
visuomotor feedback, mere observation of body discontinuity resulted in
diminished embodiment. Contradictory evidence was found in relation to motor
performance, where further research is needed to understand the role of visual
body discontinuity in motor tasks. Preliminary findings on physiological
reactions to a threat were also assessed, indicating that body visual
discontinuity does not differently impact threat-related skin conductance
responses. The present results are in accordance with past evidence showing
that body discontinuity negatively impacts embodiment. However, further
research is needed to understand the influence of visuomotor feedback and body
morphological congruency on motor performance and threat-related physiological
reactions.","['Sofia Seinfeld', 'Jörg Müller']",2020-02-27T10:30:32Z,http://arxiv.org/abs/2002.12020v3,"['cs.HC', 'H.5.m']","visuomotor feedback,embodiment,virtual hands,body discontinuity,virtual body,virtual reality,body ownership,motor performance,visual body discontinuity,skin conductance responses"
Human-like Planning for Reaching in Cluttered Environments,"Humans, in comparison to robots, are remarkably adept at reaching for objects
in cluttered environments. The best existing robot planners are based on random
sampling of configuration space -- which becomes excessively high-dimensional
with large number of objects. Consequently, most planners often fail to
efficiently find object manipulation plans in such environments. We addressed
this problem by identifying high-level manipulation plans in humans, and
transferring these skills to robot planners. We used virtual reality to capture
human participants reaching for a target object on a tabletop cluttered with
obstacles. From this, we devised a qualitative representation of the task space
to abstract the decision making, irrespective of the number of obstacles. Based
on this representation, human demonstrations were segmented and used to train
decision classifiers. Using these classifiers, our planner produced a list of
waypoints in task space. These waypoints provided a high-level plan, which
could be transferred to an arbitrary robot model and used to initialise a local
trajectory optimiser. We evaluated this approach through testing on unseen
human VR data, a physics-based robot simulation, and a real robot (dataset and
code are publicly available). We found that the human-like planner outperformed
a state-of-the-art standard trajectory optimisation algorithm, and was able to
generate effective strategies for rapid planning -- irrespective of the number
of obstacles in the environment.","['Mohamed Hasan', 'Matthew Warburton', 'Wisdom C. Agboh', 'Mehmet R. Dogar', 'Matteo Leonetti', 'He Wang', 'Faisal Mushtaq', 'Mark Mon-Williams', 'Anthony G. Cohn']",2020-02-28T14:28:50Z,http://arxiv.org/abs/2002.12738v2,"['cs.RO', 'cs.LG']","planning,reaching,cluttered environments,object manipulation,high-level manipulation plans,virtual reality,decision making,trajectory optimisation,robot model,waypoint"
"3D dynamic hand gestures recognition using the Leap Motion sensor and
  convolutional neural networks","Defining methods for the automatic understanding of gestures is of paramount
importance in many application contexts and in Virtual Reality applications for
creating more natural and easy-to-use human-computer interaction methods. In
this paper, we present a method for the recognition of a set of non-static
gestures acquired through the Leap Motion sensor. The acquired gesture
information is converted in color images, where the variation of hand joint
positions during the gesture are projected on a plane and temporal information
is represented with color intensity of the projected points. The classification
of the gestures is performed using a deep Convolutional Neural Network (CNN). A
modified version of the popular ResNet-50 architecture is adopted, obtained by
removing the last fully connected layer and adding a new layer with as many
neurons as the considered gesture classes. The method has been successfully
applied to the existing reference dataset and preliminary tests have already
been performed for the real-time recognition of dynamic gestures performed by
users.","['Katia Lupinetti', 'Andrea Ranieri', 'Franca Giannini', 'Marina Monti']",2020-03-03T11:05:35Z,http://arxiv.org/abs/2003.01450v3,"['cs.CV', 'cs.LG', 'eess.IV']","3D,dynamic hand gestures,recognition,Leap Motion sensor,convolutional neural networks,gestures classification,deep learning,ResNet-50 architecture,real-time recognition"
"DeFINE: Delayed Feedback based Immersive Navigation Environment for
  Studying Goal-Directed Human Navigation","With the advent of consumer-grade products for presenting an immersive
virtual environment (VE), there is a growing interest in utilizing VEs for
testing human navigation behavior. However, preparing a VE still requires a
high level of technical expertise in computer graphics and virtual reality,
posing a significant hurdle to embracing the emerging technology. To address
this issue, this paper presents Delayed Feedback based Immersive Navigation
Environment (DeFINE), a framework that allows for easy creation and
administration of navigation tasks within customizable VEs via intuitive
graphical user interfaces and simple settings files. Importantly, DeFINE has a
built-in capability to provide performance feedback to participants during an
experiment, a feature that is critically missing in other similar frameworks.
To show the usability of DeFINE from both experimentalists' and participants'
perspectives, a demonstration was made in which participants navigated to a
hidden goal location with feedback that differentially weighted speed and
accuracy of their responses. In addition, the participants evaluated DeFINE in
terms of its ease of use, required workload, and proneness to induce
cybersickness. The demonstration exemplified typical experimental manipulations
DeFINE accommodates and what types of data it can collect for characterizing
participants' task performance. With its out-of-the-box functionality and
potential customizability due to open-source licensing, DeFINE makes VEs more
accessible to many researchers.","['Kshitij Tiwari', 'Ville Kyrki', 'Allen Cheung', 'Naohide Yamamoto']",2020-03-06T11:00:12Z,http://arxiv.org/abs/2003.03133v2,"['cs.HC', 'cs.AI', 'cs.LG', 'cs.RO']","immersive navigation,delayed feedback,virtual environment,human navigation behavior,computer graphics,virtual reality,graphical user interface,performance feedback,experimental manipulation,task performance"
"Reinforcement Learning Driven Adaptive VR Streaming with Optical Flow
  Based QoE","With the merit of containing full panoramic content in one camera, Virtual
Reality (VR) and 360-degree videos have attracted more and more attention in
the field of industrial cloud manufacturing and training. Industrial Internet
of Things (IoT), where many VR terminals needed to be online at the same time,
can hardly guarantee VR's bandwidth requirement. However, by making use of
users' quality of experience (QoE) awareness factors, including the relative
moving speed and depth difference between the viewpoint and other content,
bandwidth consumption can be reduced. In this paper, we propose OFB-VR (Optical
Flow Based VR), an interactive method of VR streaming that can make use of VR
users' QoE awareness to ease the bandwidth pressure. The Just-Noticeable
Difference through Optical Flow Estimation (JND-OFE) is explored to quantify
users' awareness of quality distortion in 360-degree videos. Accordingly, a
novel 360-degree videos QoE metric based on PSNR and JND-OFE (PSNR-OF) is
proposed. With the help of PSNR-OF, OFB-VR proposes a versatile-size tiling
scheme to lessen the tiling overhead. A Reinforcement Learning(RL) method is
implemented to make use of historical data to perform Adaptive BitRate(ABR).
For evaluation, we take two prior VR streaming schemes, Pano and Plato, as
baselines. Vast evaluations show that our system can increase the mean PSNR-OF
score by 9.5-15.8% while maintaining the same rebuffer ratio compared with Pano
and Plato in a fluctuate LTE bandwidth dataset. Evaluation results show that
OFB-VR is a promising prototype for actual interactive industrial VR. A
prototype of OFB-VR can be found in https://github.com/buptexplorers/OFB-VR.","['Wei Quan', 'Yuxuan Pan', 'Bin Xiang', 'Lin Zhang']",2020-03-17T08:47:34Z,http://arxiv.org/abs/2003.07583v1,['cs.MM'],"Reinforcement Learning,Adaptive VR Streaming,Optical Flow,QoE,Industrial IoT,Bandwidth,Just-Noticeable Difference,PSNR,Tiling Scheme,ABR"
SceneCAD: Predicting Object Alignments and Layouts in RGB-D Scans,"We present a novel approach to reconstructing lightweight, CAD-based
representations of scanned 3D environments from commodity RGB-D sensors. Our
key idea is to jointly optimize for both CAD model alignments as well as layout
estimations of the scanned scene, explicitly modeling inter-relationships
between objects-to-objects and objects-to-layout. Since object arrangement and
scene layout are intrinsically coupled, we show that treating the problem
jointly significantly helps to produce globally-consistent representations of a
scene. Object CAD models are aligned to the scene by establishing dense
correspondences between geometry, and we introduce a hierarchical layout
prediction approach to estimate layout planes from corners and edges of the
scene.To this end, we propose a message-passing graph neural network to model
the inter-relationships between objects and layout, guiding generation of a
globally object alignment in a scene. By considering the global scene layout,
we achieve significantly improved CAD alignments compared to state-of-the-art
methods, improving from 41.83% to 58.41% alignment accuracy on SUNCG and from
50.05% to 61.24% on ScanNet, respectively. The resulting CAD-based
representations makes our method well-suited for applications in content
creation such as augmented- or virtual reality.","['Armen Avetisyan', 'Tatiana Khanova', 'Christopher Choy', 'Denver Dash', 'Angela Dai', 'Matthias Nießner']",2020-03-27T20:17:00Z,http://arxiv.org/abs/2003.12622v1,['cs.CV'],"CAD-based representations,RGB-D scans,object alignments,layout estimations,scene layout,hierarchical layout prediction,graph neural network,object CAD models,dense correspondences,content creation"
"Towards an immersive user interface for waypoint navigation of a mobile
  robot","In this paper, we investigate the utility of head-mounted display (HMD)
interfaces for navigation of mobile robots. We focus on the selection of
waypoint positions for the robot, whilst maintaining an egocentric view of the
robot's environment. Inspired by virtual reality (VR) gaming, we propose a
target selection method that uses the 6 degrees-of-freedom tracked controllers
of a commercial VR headset. This allows an operator to point to the desired
target position, in the vicinity of the robot, which the robot then
autonomously navigates towards. A user study (37 participants) was conducted to
examine the efficacy of this control strategy when compared to direct control,
both with and without a communication delay. The results of the experiment
showed that participants were able to learn how to use the novel system
quickly, and the majority of participants reported a preference for waypoint
control. Across all recorded metrics (task performance, operator workload and
usability) the proposed waypoint control interface was not significantly
affected by the communication delay, in contrast to direct control. The
simulated experiment indicated that a real-world implementation of the proposed
interface could be effective, but also highlighted the need to manage the
negative effects of HMDs - particularly VR sickness.","['Greg Baker', 'Tom Bridgwater', 'Paul Bremner', 'Manuel Giuliani']",2020-03-28T11:35:26Z,http://arxiv.org/abs/2003.12772v1,"['cs.RO', 'H.5.2; J.2']","immersive user interface,waypoint navigation,mobile robot,head-mounted display (HMD),virtual reality (VR),6 degrees-of-freedom,tracked controllers,operator workload,communication delay"
"Development and Validation of Pictographic Scales for Rapid Assessment
  of Affective States in Virtual Reality","This paper describes the development and validation of a continuous
pictographic scale for self-reported assessment of affective states in virtual
environments. The developed tool, called Morph A Mood (MAM), consists of a 3D
character whose facial expression can be adjusted with simple controller
gestures according to the perceived affective state to capture valence and
arousal scores. It was tested against the questionnaires Pick-A-Mood (PAM) and
Self-Assessment Manikin (SAM) in an experiment in which the participants (N =
32) watched several one-minute excerpts from music videos of the DEAP database
within a virtual environment and assessed their mood after each clip. The
experiment showed a high correlation with regard to valence, but only a
moderate one with regard to arousal. No statistically significant differences
were found between the SAM ratings of this experiment and MAM, but between the
valence values of MAM and the DEAP database and between the arousal values of
MAM and PAM. In terms of user experience, MAM and PAM hardly differ.
Furthermore, the experiment showed that assessments inside virtual environments
are significantly faster than with paper-pencil methods, where media devices
such as headphones and display goggles must be put on and taken off.","['Christian Krüger', 'Tanja KojiāE, 'Luis Meier', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2020-03-31T18:02:22Z,http://arxiv.org/abs/2004.00034v1,['cs.MM'],"pictographic scales,affective states,virtual reality,Morph A Mood,valence,arousal,Pick-A-Mood,Self-Assessment Manikin,DEAP database,user experience"
TSception: A Deep Learning Framework for Emotion Detection Using EEG,"In this paper, we propose a deep learning framework, TSception, for emotion
detection from electroencephalogram (EEG). TSception consists of temporal and
spatial convolutional layers, which learn discriminative representations in the
time and channel domains simultaneously. The temporal learner consists of
multi-scale 1D convolutional kernels whose lengths are related to the sampling
rate of the EEG signal, which learns multiple temporal and frequency
representations. The spatial learner takes advantage of the asymmetry property
of emotion responses at the frontal brain area to learn the discriminative
representations from the left and right hemispheres of the brain. In our study,
a system is designed to study the emotional arousal in an immersive virtual
reality (VR) environment. EEG data were collected from 18 healthy subjects
using this system to evaluate the performance of the proposed deep learning
network for the classification of low and high emotional arousal states. The
proposed method is compared with SVM, EEGNet, and LSTM. TSception achieves a
high classification accuracy of 86.03%, which outperforms the prior methods
significantly (p<0.05). The code is available at
https://github.com/deepBrains/TSception","['Yi Ding', 'Neethu Robinson', 'Qiuhao Zeng', 'Duo Chen', 'Aung Aung Phyo Wai', 'Tih-Shih Lee', 'Cuntai Guan']",2020-04-02T02:10:07Z,http://arxiv.org/abs/2004.02965v2,"['eess.SP', 'cs.LG', 'stat.ML']","deep learning,emotion detection,EEG,temporal convolutional layers,spatial convolutional layers,frontal brain area,emotional arousal,virtual reality,SVM,LSTM"
6G Communication: Envisioning the Key Issues and Challenges,"In 2030, we are going to evidence the 6G mobile communication technology,
which will enable the Internet of Everything. Yet 5G has to be experienced by
people worldwide and B5G has to be developed; the researchers have already
started planning, visioning, and gathering requirements of the 6G. Moreover,
many countries have already initiated the research on 6G. 6G promises
connecting every smart device to the Internet from smartphone to intelligent
vehicles. 6G will provide sophisticated and high QoS such as holographic
communication, augmented reality/virtual reality and many more. Also, it will
focus on Quality of Experience (QoE) to provide rich experiences from 6G
technology. Notably, it is very important to vision the issues and challenges
of 6G technology, otherwise, promises may not be delivered on time. The
requirements of 6G poses new challenges to the research community. To achieve
desired parameters of 6G, researchers are exploring various alternatives.
Hence, there are diverse research challenges to envision, from devices to
softwarization. Therefore, in this article, we discuss the future issues and
challenges to be faced by the 6G technology. We have discussed issues and
challenges from every aspect from hardware to the enabling technologies which
will be utilized by 6G.","['Sabuzima Nayak', 'Ripon Patgiri']",2020-04-07T13:49:20Z,http://arxiv.org/abs/2004.04024v3,"['eess.SP', 'cs.NI', '68-02, 68M10', 'C.2; I.2']","6G communication,Internet of Everything,B5G,QoS,holographic communication,augmented reality,virtual reality,Quality of Experience (QoE),softwarization,enabling technologies"
"Exploring Extended Reality with ILLIXR: A New Playground for
  Architecture Research","As we enter the era of domain-specific architectures, systems researchers
must understand the requirements of emerging application domains. Augmented and
virtual reality (AR/VR) or extended reality (XR) is one such important domain.
This paper presents ILLIXR, the first open source end-to-end XR system (1) with
state-of-the-art components, (2) integrated with a modular and extensible
multithreaded runtime, (3) providing an OpenXR compliant interface to XR
applications (e.g., game engines), and (4) with the ability to report (and
trade off) several quality of experience (QoE) metrics. We analyze performance,
power, and QoE metrics for the complete ILLIXR system and for its individual
components. Our analysis reveals several properties with implications for
architecture and systems research. These include demanding performance, power,
and QoE requirements, a large diversity of critical tasks, inter-dependent
execution pipelines with challenges in scheduling and resource management, and
a large tradeoff space between performance/power and human perception related
QoE metrics. ILLIXR and our analysis have the potential to propel new
directions in architecture and systems research in general, and impact XR in
particular. ILLIXR is open-source and available at https://illixr.github.io","['Muhammad Huzaifa', 'Rishi Desai', 'Samuel Grayson', 'Xutao Jiang', 'Ying Jing', 'Jae Lee', 'Fang Lu', 'Yihan Pang', 'Joseph Ravichandran', 'Finn Sinclair', 'Boyuan Tian', 'Hengzhi Yuan', 'Jeffrey Zhang', 'Sarita V. Adve']",2020-03-26T01:17:29Z,http://arxiv.org/abs/2004.04643v2,"['cs.DC', 'cs.ET']","extended reality,ILLIXR,architecture research,domain-specific architectures,systems researchers,augmented reality,virtual reality,XR,OpenXR,quality of experience (QoE)"
"SIGVerse: A cloud-based VR platform for research on social and embodied
  human-robot interaction","Common sense and social interaction related to daily-life environments are
considerably important for autonomous robots, which support human activities.
One of the practical approaches for acquiring such social interaction skills
and semantic information as common sense in human activity is the application
of recent machine learning techniques. Although recent machine learning
techniques have been successful in realizing automatic manipulation and driving
tasks, it is difficult to use these techniques in applications that require
human-robot interaction experience. Humans have to perform several times over a
long term to show embodied and social interaction behaviors to robots or
learning systems. To address this problem, we propose a cloud-based immersive
virtual reality (VR) platform which enables virtual human-robot interaction to
collect the social and embodied knowledge of human activities in a variety of
situations. To realize the flexible and reusable system, we develop a real-time
bridging mechanism between ROS and Unity, which is one of the standard
platforms for developing VR applications. We apply the proposed system to a
robot competition field named RoboCup@Home to confirm the feasibility of the
system in a realistic human-robot interaction scenario. Through demonstration
experiments at the competition, we show the usefulness and potential of the
system for the development and evaluation of social intelligence through
human-robot interaction. The proposed VR platform enables robot systems to
collect social experiences with several users in a short time. The platform
also contributes in providing a dataset of social behaviors, which would be a
key aspect for intelligent service robots to acquire social interaction skills
based on machine learning techniques.","['Tetsunari Inamura', 'Yoshiaki Mizuchi']",2020-05-02T13:02:54Z,http://arxiv.org/abs/2005.00825v1,"['cs.RO', 'cs.HC']","cloud-based,VR platform,human-robot interaction,social interaction,embodied knowledge,machine learning techniques,virtual reality,ROS,Unity,social intelligence"
Towards Occlusion-Aware Multifocal Displays,"The human visual system uses numerous cues for depth perception, including
disparity, accommodation, motion parallax and occlusion. It is incumbent upon
virtual-reality displays to satisfy these cues to provide an immersive user
experience. Multifocal displays, one of the classic approaches to satisfy the
accommodation cue, place virtual content at multiple focal planes, each at a di
erent depth. However, the content on focal planes close to the eye do not
occlude those farther away; this deteriorates the occlusion cue as well as
reduces contrast at depth discontinuities due to leakage of the defocus blur.
This paper enables occlusion-aware multifocal displays using a novel ConeTilt
operator that provides an additional degree of freedom -- tilting the light
cone emitted at each pixel of the display panel. We show that, for scenes with
relatively simple occlusion con gurations, tilting the light cones provides the
same e ect as physical occlusion. We demonstrate that ConeTilt can be easily
implemented by a phase-only spatial light modulator. Using a lab prototype, we
show results that demonstrate the presence of occlusion cues and the increased
contrast of the display at depth edges.","['Jen-Hao Rick Chang', 'Anat Levin', 'B. V. K. Vijaya Kumar', 'Aswin C. Sankaranarayanan']",2020-05-02T23:51:11Z,http://arxiv.org/abs/2005.00946v1,"['eess.IV', 'cs.CV', 'physics.optics']","occlusion,multifocal displays,depth perception,accommodation cue,virtual-reality displays,light cone,spatial light modulator,contrast,depth edges,immersion"
Accessibility in 360-degree video players,"Any media experience must be fully inclusive and accessible to all users
regardless of their ability. With the current trend towards immersive
experiences, such as Virtual Reality (VR) and 360-degree video, it becomes key
that these environments are adapted to be fully accessible. However, until
recently the focus has been mostly on adapting the existing techniques to fit
immersive displays, rather than considering new approaches for accessibility
designed specifically for these increasingly relevant media experiences. This
paper surveys a wide range of 360-degree video players and examines the
features they include for dealing with accessibility, such as Subtitles, Audio
Description, Sign Language, User Interfaces, and other interaction features,
like voice control and support for multi-screen scenarios. These features have
been chosen based on guidelines from standardization contributions, like in the
World Wide Web Consortium (W3C) and the International Communication Union
(ITU), and from research contributions for making 360-degree video consumption
experiences accessible. The in-depth analysis has been part of a research
effort towards the development of a fully inclusive and accessible 360-degree
video player. The paper concludes by discussing how the newly developed player
has gone above and beyond the existing solutions and guidelines, by providing
accessibility features that meet the expectations for a widely used immersive
medium, like 360-degree video.","['Chris Hughes', 'Mario Montagud']",2020-05-07T10:40:47Z,http://arxiv.org/abs/2005.03373v1,['cs.MM'],"Accessibility,360-degree video,Virtual Reality (VR),Immersive experiences,Subtitles,Audio description,Sign language,User interfaces,Voice control,Multi-screen scenarios"
Imposing Regulation on Advanced Algorithms,"This book discusses the necessity and perhaps urgency for the regulation of
algorithms on which new technologies rely; technologies that have the potential
to re-shape human societies. From commerce and farming to medical care and
education, it is difficult to find any aspect of our lives that will not be
affected by these emerging technologies. At the same time, artificial
intelligence, deep learning, machine learning, cognitive computing, blockchain,
virtual reality and augmented reality, belong to the fields most likely to
affect law and, in particular, administrative law. The book examines
universally applicable patterns in administrative decisions and judicial
rulings. First, similarities and divergence in behavior among the different
cases are identified by analyzing parameters ranging from geographical location
and administrative decisions to judicial reasoning and legal basis. As it turns
out, in several of the cases presented, sources of general law, such as
competition or labor law, are invoked as a legal basis, due to the lack of
current specialized legislation. This book also investigates the role and
significance of national and indeed supranational regulatory bodies for
advanced algorithms and considers ENISA, an EU agency that focuses on network
and information security, as an interesting candidate for a European regulator
of advanced algorithms. Lastly, it discusses the involvement of representative
institutions in algorithmic regulation.",['Fotios Fitsilis'],2020-05-16T20:26:54Z,http://arxiv.org/abs/2005.08092v1,"['cs.CY', 'cs.AI', '68T01, 68T99', 'K.5.0; K.5.2; K.5.m']","regulation,algorithms,technologies,artificial intelligence,deep learning,machine learning,cognitive computing,blockchain,virtual reality,augmented reality"
"Perceptual Quality Assessment of Omnidirectional Images as Moving Camera
  Videos","Omnidirectional images (also referred to as static 360{\deg} panoramas)
impose viewing conditions much different from those of regular 2D images. How
do humans perceive image distortions in immersive virtual reality (VR)
environments is an important problem which receives less attention. We argue
that, apart from the distorted panorama itself, two types of VR viewing
conditions are crucial in determining the viewing behaviors of users and the
perceived quality of the panorama: the starting point and the exploration time.
We first carry out a psychophysical experiment to investigate the interplay
among the VR viewing conditions, the user viewing behaviors, and the perceived
quality of 360{\deg} images. Then, we provide a thorough analysis of the
collected human data, leading to several interesting findings. Moreover, we
propose a computational framework for objective quality assessment of 360{\deg}
images, embodying viewing conditions and behaviors in a delightful way.
Specifically, we first transform an omnidirectional image to several video
representations using different user viewing behaviors under different viewing
conditions. We then leverage advanced 2D full-reference video quality models to
compute the perceived quality. We construct a set of specific quality measures
within the proposed framework, and demonstrate their promises on three VR
quality databases.","['Xiangjie Sui', 'Kede Ma', 'Yiru Yao', 'Yuming Fang']",2020-05-21T10:03:40Z,http://arxiv.org/abs/2005.10547v2,"['eess.IV', 'cs.CV']","Perceptual Quality Assessment,Omnidirectional Images,Moving Camera,Videos,Virtual Reality,Psychophysical Experiment,User Viewing Behaviors,Computational Framework,Objective Quality Assessment,Full-Reference Video Quality Models"
Emotion-robust EEG Classification for Motor Imagery,"Developments in Brain Computer Interfaces (BCIs) are empowering those with
severe physical afflictions through their use in assistive systems. Common
methods of achieving this is via Motor Imagery (MI), which maps brain signals
to code for certain commands. Electroencephalogram (EEG) is preferred for
recording brain signal data on account of it being non-invasive. Despite their
potential utility, MI-BCI systems are yet confined to research labs. A major
cause for this is lack of robustness of such systems. As hypothesized by two
teams during Cybathlon 2016, a particular source of the system's vulnerability
is the sharp change in the subject's state of emotional arousal. This work aims
towards making MI-BCI systems resilient to such emotional perturbations. To do
so, subjects are exposed to high and low arousal-inducing virtual reality (VR)
environments before recording EEG data. The advent of COVID-19 compelled us to
modify our methodology. Instead of training machine learning algorithms to
classify emotional arousal, we opt for classifying subjects that serve as proxy
for each state. Additionally, MI models are trained for each subject instead of
each arousal state. As training subjects to use MI-BCI can be an arduous and
time-consuming process, reducing this variability and increasing robustness can
considerably accelerate the acceptance and adoption of assistive technologies
powered by BCI.",['Abdul Moeed'],2020-05-23T17:31:07Z,http://arxiv.org/abs/2005.13523v1,"['eess.SP', 'cs.HC', 'cs.LG', 'stat.ML']","Brain Computer Interfaces,Motor Imagery,Electroencephalogram,EEG Classification,Emotional arousal,Virtual Reality (VR) environments,Machine learning algorithms,Robustness,Assistive technologies,BCI"
"Design and Implementation of a Virtual 3D Educational Environment to
  improve Deaf Education","Advances in NLP, knowledge representation and computer graphic technologies
can provide us insights into the development of educational tool for Deaf
people. Actual education materials and tools for deaf pupils present several
problems, since textbooks are designed to support normal students in the
classroom and most of them are not suitable for people with hearing
disabilities. Virtual Reality (VR) technologies appear to be a good tool and a
promising framework in the education of pupils with hearing disabilities. In
this paper, we present a current research tasks surrounding the design and
implementation of a virtual 3D educational environment based on X3D and H-Anim
standards. The system generates and animates automatically Sign language
sentence from a semantic representation that encode the whole meaning of the
Arabic input text. Some aspects and issues in Sign language generation will be
discussed, including the model of Sign representation that facilitate reuse and
reduces the time of Sign generation, conversion of semantic components to sign
features representation with regard to Sign language linguistics
characteristics and how to generate realistic smooth gestural sequences using
X3D content to performs transition between signs for natural-looking of
animated avatar. Sign language sentences were evaluated by Algerian native Deaf
people. The goal of the project is the development of a machine translation
system from Arabic to Algerian Sign Language that can be used as educational
tool for Deaf children in algerian primary schools.",['Abdelaziz Lakhfif'],2020-05-29T22:56:43Z,http://arxiv.org/abs/2006.00114v1,"['cs.CL', 'cs.GR', 'cs.HC']","Deaf Education,Virtual 3D Educational Environment,NLP,Knowledge Representation,Computer Graphics,Virtual Reality,X3D,H-Anim standards,Sign language,Semantic Representation"
"RoadNet-RT: High Throughput CNN Architecture and SoC Design for
  Real-Time Road Segmentation","In recent years, convolutional neural network has gained popularity in many
engineering applications especially for computer vision. In order to achieve
better performance, often more complex structures and advanced operations are
incorporated into the neural networks, which results very long inference time.
For time-critical tasks such as autonomous driving and virtual reality,
real-time processing is fundamental. In order to reach real-time process speed,
a light-weight, high-throughput CNN architecture namely RoadNet-RT is proposed
for road segmentation in this paper. It achieves 90.33% MaxF score on test set
of KITTI road segmentation task and 8 ms per frame when running on GTX 1080
GPU. Comparing to the state-of-the-art network, RoadNet-RT speeds up the
inference time by a factor of 20 at the cost of only 6.2% accuracy loss. For
hardware design optimization, several techniques such as depthwise separable
convolution and non-uniformed kernel size convolution are customized designed
to further reduce the processing time. The proposed CNN architecture has been
successfully implemented on an FPGA ZCU102 MPSoC platform that achieves the
computation capability of 83.05 GOPS. The system throughput reaches 327.9
frames per second with image size 1216x176.","['Lin Bai', 'Yecheng Lyu', 'Xinming Huang']",2020-06-13T14:12:23Z,http://arxiv.org/abs/2006.07644v2,"['eess.IV', 'cs.CV']","convolutional neural network,CNN architecture,SoC design,real-time processing,road segmentation,inference time,FPGA,MPSoC,throughput,computation capability"
A Benchmarking Framework for Interactive 3D Applications in the Cloud,"With the growing popularity of cloud gaming and cloud virtual reality (VR),
interactive 3D applications have become a major type of workloads for the
cloud. However, despite their growing importance, there is limited public
research on how to design cloud systems to efficiently support these
applications, due to the lack of an open and reliable research infrastructure,
including benchmarks and performance analysis tools. The challenges of
generating human-like inputs under various system/application randomness and
dissecting the performance of complex graphics systems make it very difficult
to design such an infrastructure. In this paper, we present the design of a
novel cloud graphics rendering research infrastructure, Pictor. Pictor employs
AI to mimic human interactions with complex 3D applications. It can also
provide in-depth performance measurements for the complex software and hardware
stack used for cloud 3D graphics rendering. With Pictor, we designed a
benchmark suite with six interactive 3D applications. Performance analyses were
conducted with these benchmarks to characterize 3D applications in the cloud
and reveal new performance bottlenecks. To demonstrate the effectiveness of
Pictor, we also implemented two optimizations to address two performance
bottlenecks discovered in a state-of-the-art cloud 3D-graphics rendering
system, which improved the frame rate by 57.7% on average.","['Tianyi Liu', 'Sen He', 'Sunzhou Huang', 'Danny Tsang', 'Lingjia Tang', 'Jason Mars', 'Wei Wang']",2020-06-23T23:11:30Z,http://arxiv.org/abs/2006.13378v2,"['cs.DC', 'cs.GR']","cloud computing,interactive 3D applications,benchmarking framework,performance analysis,cloud systems,3D graphics rendering,performance bottlenecks,AI,benchmarks,virtual reality"
"The 2MASS redshift survey galaxy group catalogue derived from a
  graph-theory based friends-of-friends algorithm","We present the galaxy group catalogue for the recently-completed 2MASS
Redshift Survey (2MRS, Macri2019) which consists of 44572 redshifts, including
1041 new measurements for galaxies mostly located within the Zone of Avoidance.
The galaxy group catalogue is generated by using a novel, graph-theory based,
modified version of the Friends-of-Friends algorithm. Several graph-theory
examples are presented throughout this paper, including a new method for
identifying substructures within groups. The results and graph-theory methods
have been thoroughly interrogated against previous 2MRS group catalogues and a
Theoretical Astrophysical Observatory (TAO) mock by making use of cutting-edge
visualization techniques including immersive facilities, a digital planetarium,
and virtual reality. This has resulted in a stable and robust catalogue with
on-sky positions and line-of-sight distances within 0.5 Mpc and 2 Mpc,
respectively, and has recovered all major groups and clusters. The final
catalogue consists of 3022 groups, resulting in the most complete ""whole-sky""
galaxy group catalogue to date. We determine the 3D positions of these groups,
as well as their luminosity and comoving distances, observed and corrected
number of members, richness metric, velocity dispersion, and estimates of
$R_{200}$ and $M_{200}$. We present three additional data products, i.e. the
2MRS galaxies found in groups, a catalogue of subgroups, and a catalogue of 687
new group candidates with no counterparts in previous 2MRS-based analyses.","['T. S. Lambert', 'R. C. Kraan-Korteweg', 'T. H. Jarrett', 'L. M. Macri']",2020-07-01T16:12:27Z,http://arxiv.org/abs/2007.00581v1,"['astro-ph.GA', 'astro-ph.CO']","2MASS redshift survey,galaxy group catalogue,graph-theory,Friends-of-Friends algorithm,substructures,visualization techniques,on-sky positions,line-of-sight distances,groups,clusters"
"RGB-D-based Framework to Acquire, Visualize and Measure the Human Body
  for Dietetic Treatments","This research aims to improve dietetic-nutritional treatment using
state-of-the-art RGB-D sensors and virtual reality (VR) technology. Recent
studies show that adherence to treatment can be improved using multimedia
technologies. However, there are few studies using 3D data and VR technologies
for this purpose. On the other hand, obtaining 3D measurements of the human
body and analyzing them over time (4D) in patients undergoing dietary treatment
is a challenging field. The main contribution of the work is to provide a
framework to study the effect of 4D body model visualization on adherence to
obesity treatment. The system can obtain a complete 3D model of a body using
low-cost technology, allowing future straightforward transference with
sufficient accuracy and realistic visualization, enabling the analysis of the
evolution (4D) of the shape during the treatment of obesity. The 3D body models
will be used for studying the effect of visualization on adherence to obesity
treatment using 2D and VR devices. Moreover, we will use the acquired 3D models
to obtain measurements of the body. An analysis of the accuracy of the proposed
methods for obtaining measurements with both synthetic and real objects has
been carried out.","['Andrés Fuster-Guilló', 'Jorge Azorín-López', 'Marcelo Saval-Calvo', 'Juan Miguel Castillo-Zaragoza', 'Nahuel Garcia-DUrso', 'Robert B Fisher']",2020-07-02T09:30:47Z,http://arxiv.org/abs/2007.00981v1,['cs.CV'],"RGB-D sensors,virtual reality,3D data,VR technologies,4D body model,obesity treatment,visualization,measurements,accuracy,dietary treatment"
"Deep Learning for Wireless Communications: An Emerging Interdisciplinary
  Paradigm","Wireless communications are envisioned to bring about dramatic changes in the
future, with a variety of emerging applications, such as virtual reality (VR),
Internet of things (IoT), etc., becoming a reality. However, these compelling
applications have imposed many new challenges, including unknown channel
models, low-latency requirement in large-scale super-dense networks, etc. The
amazing success of deep learning (DL) in various fields, particularly in
computer science, has recently stimulated increasing interest in applying it to
address those challenges. Hence, in this review, a pair of dominant
methodologies of using DL for wireless communications are investigated. The
first one is DL-based architecture design, which breaks the classical
model-based block design rule of wireless communications in the past decades.
The second one is DL-based algorithm design, which will be illustrated by
several examples in a series of typical techniques conceived for 5G and beyond.
Their principles, key features, and performance gains will be discussed.
Furthermore, open problems and future research opportunities will also be
pointed out, highlighting the interplay between DL and wireless communications.
We expect that this review can stimulate more novel ideas and exciting
contributions for intelligent wireless communications.","['Linglong Dai', 'Ruicheng Jiao', 'Fumiyuki Adachi', 'H. Vincent Poor', 'Lajos Hanzo']",2020-07-12T10:18:12Z,http://arxiv.org/abs/2007.05952v1,"['eess.SP', 'cs.IT', 'math.IT']","Deep Learning,Wireless Communications,Interdisciplinary,Virtual Reality,Internet of Things,Channel Models,Low-latency,Super-dense Networks,5G,Algorithm Design"
Tomography Based Learning for Load Distribution through Opaque Networks,"Applications such as virtual reality and online gaming require low delays for
acceptable user experience. A key task for over-the-top (OTT) service providers
who provide these applications is sending traffic through the networks to
minimize delays. OTT traffic is typically generated from multiple data centers
which are multi-homed to several network ingresses. However, information about
the path characteristics of the underlying network from the ingresses to
destinations is not explicitly available to OTT services. These can only be
inferred from external probing. In this paper, we combine network tomography
with machine learning to minimize delays. We consider this problem in a general
setting where traffic sources can choose a set of ingresses through which their
traffic enter a black box network. The problem in this setting can be viewed as
a reinforcement learning problem with constraints on a continuous action space,
which to the best of our knowledge have not been investigated by the machine
learning community. Key technical challenges to solving this problem include
the high dimensionality of the problem and handling constraints that are
intrinsic to networks. Evaluation results show that our methods achieve up to
60% delay reductions in comparison to standard heuristics. Moreover, the
methods we develop can be used in a centralized manner or in a distributed
manner by multiple independent agents.","['Shenghe Xu', 'Murali Kodialam', 'T. V. Lakshman', 'Shivendra S. Panwar']",2020-07-18T21:52:21Z,http://arxiv.org/abs/2007.09521v1,"['cs.NI', 'cs.LG']","Tomography,Load distribution,Opaque networks,OTT services,Machine learning,Network ingresses,Network tomography,Delays,Reinforcement learning,Continuous action space"
"Sound Field Translation and Mixed Source Model for Virtual Applications
  with Perceptual Validation","Non-interactive and linear experiences like cinema film offer high quality
surround sound audio to enhance immersion, however the listener's experience is
usually fixed to a single acoustic perspective. With the rise of virtual
reality, there is a demand for recording and recreating real-world experiences
in a way that allows for the user to interact and move within the reproduction.
Conventional sound field translation techniques take a recording and expand it
into an equivalent environment of virtual sources. However, the finite sampling
of a commercial higher order microphone produces an acoustic sweet-spot in the
virtual reproduction. As a result, the technique remains to restrict the
listener's navigable region. In this paper, we propose a method for listener
translation in an acoustic reproduction that incorporates a mixture of
near-field and far-field sources in a sparsely expanded virtual environment. We
perceptually validate the method through a Multiple Stimulus with Hidden
Reference and Anchor (MUSHRA) experiment. Compared to the planewave benchmark,
the proposed method offers both improved source localizability and robustness
to spectral distortions at translated positions. A cross-examination with
numerical simulations demonstrated that the sparse expansion relaxes the
inherent sweet-spot constraint, leading to the improved localizability for
sparse environments. Additionally, the proposed method is seen to better
reproduce the intensity and binaural room impulse response spectra of
near-field environments, further supporting the strong perceptual results.","['Lachlan Birnie', 'Thushara Abhayapala', 'Vladimir Tourbabin', 'Prasanga Samarasinghe']",2020-07-23T05:16:01Z,http://arxiv.org/abs/2007.11795v1,"['eess.AS', 'cs.SD']","sound field translation,mixed source model,virtual applications,surround sound audio,virtual reality,acoustic perspective,higher order microphone,acoustic sweet-spot,navigable region,near-field sources,far-field sources,perceptual validation"
IEEE 802.11be-Wi-Fi 7: New Challenges and Opportunities,"With the emergence of 4k/8k video, the throughput requirement of video
delivery will keep grow to tens of Gbps. Other new high-throughput and
low-latency video applications including augmented reality (AR), virtual
reality (VR), and online gaming, are also proliferating. Due to the related
stringent requirements, supporting these applications over wireless local area
network (WLAN) is far beyond the capabilities of the new WLAN standard -- IEEE
802.11ax. To meet these emerging demands, the IEEE 802.11 will release a new
amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known
as Wireless-Fidelity (Wi-Fi) 7. This article provides the comprehensive survey
on the key medium access control (MAC) layer techniques and physical layer
(PHY) techniques being discussed in the EHT task group, including the
channelization and tone plan, multiple resource units (multi-RU) support, 4096
quadrature amplitude modulation (4096-QAM), preamble designs, multiple link
operations (e.g., multi-link aggregation and channel access), multiple input
multiple output (MIMO) enhancement, multiple access point (multi-AP)
coordination (e.g., multi-AP joint transmission), enhanced link adaptation and
retransmission protocols (e.g., hybrid automatic repeat request (HARQ)). This
survey covers both the critical technologies being discussed in EHT standard
and the related latest progresses from worldwide research. Besides, the
potential developments beyond EHT are discussed to provide some possible future
research directions for WLAN.","['Cailian Deng', 'Xuming Fang', 'Xiao Han', 'Xianbin Wang', 'Li Yan', 'Rong He', 'Yan Long', 'Yuchen Guo']",2020-07-27T09:40:28Z,http://arxiv.org/abs/2007.13401v3,['eess.SP'],"IEEE 802.11be,Wi-Fi 7,Throughput,Video delivery,Augmented reality,Virtual reality,Online gaming,WLAN,MAC layer,PHY layer."
"Teacher-Student Training and Triplet Loss for Facial Expression
  Recognition under Occlusion","In this paper, we study the task of facial expression recognition under
strong occlusion. We are particularly interested in cases where 50% of the face
is occluded, e.g. when the subject wears a Virtual Reality (VR) headset. While
previous studies show that pre-training convolutional neural networks (CNNs) on
fully-visible (non-occluded) faces improves the accuracy, we propose to employ
knowledge distillation to achieve further improvements. First of all, we employ
the classic teacher-student training strategy, in which the teacher is a CNN
trained on fully-visible faces and the student is a CNN trained on occluded
faces. Second of all, we propose a new approach for knowledge distillation
based on triplet loss. During training, the goal is to reduce the distance
between an anchor embedding, produced by a student CNN that takes occluded
faces as input, and a positive embedding (from the same class as the anchor),
produced by a teacher CNN trained on fully-visible faces, so that it becomes
smaller than the distance between the anchor and a negative embedding (from a
different class than the anchor), produced by the student CNN. Third of all, we
propose to combine the distilled embeddings obtained through the classic
teacher-student strategy and our novel teacher-student strategy based on
triplet loss into a single embedding vector. We conduct experiments on two
benchmarks, FER+ and AffectNet, with two CNN architectures, VGG-f and VGG-face,
showing that knowledge distillation can bring significant improvements over the
state-of-the-art methods designed for occluded faces in the VR setting.","['Mariana-Iuliana Georgescu', 'Radu Tudor Ionescu']",2020-08-03T16:41:19Z,http://arxiv.org/abs/2008.01003v2,"['cs.CV', 'cs.LG']","facial expression recognition,occlusion,knowledge distillation,teacher-student training,triplet loss,convolutional neural networks,Virtual Reality,CNN architectures,FER+,AffectNet"
"Silhouette Games: An Interactive One-Way Mirror Approach to Watching
  Players in VR","Watching others play is a key ingredient of digital games and an important
aspect of games user research. However, spectatorship is not very popular in
virtual reality, as such games strongly rely on one's feelings of presence. In
other words, the head-mounted display creates a barrier between the player and
the audience. We contribute an alternative watching approach consisting of two
major components: a dynamic view frustum that renders the game scene from the
current spectator position and a one-way mirror in front of the screen. This
mirror, together with our silhouetting algorithm, allows seeing the player's
reflection at the correct position in the virtual world. An exploratory survey
emphasizes the overall positive experience of the viewers in our setup. In
particular, the participants enjoyed their ability to explore the virtual
surrounding via physical repositioning and to observe the blended player during
object manipulations. Apart from requesting a larger screen, the participants
expressed a strong need to interact with the player. Consequently, we suggest
utilizing our technology as a foundation for novel playful experiences with the
overarching goal to transform the passive spectator into a collocated player.","['Andrey Krekhov', 'Daniel PreuÁE, 'Sebastian Cmentowski', 'Jens Krüger']",2020-08-06T11:25:10Z,http://arxiv.org/abs/2008.02582v1,['cs.HC'],"Silhouette Games,Interactive,One-Way Mirror,VR,Spectatorship,View Frustum,Reflection,Silhouetting Algorithm,Virtual World,Player Interaction"
Attention-based 3D Object Reconstruction from a Single Image,"Recently, learning-based approaches for 3D reconstruction from 2D images have
gained popularity due to its modern applications, e.g., 3D printers, autonomous
robots, self-driving cars, virtual reality, and augmented reality. The computer
vision community has applied a great effort in developing functions to
reconstruct the full 3D geometry of objects and scenes. However, to extract
image features, they rely on convolutional neural networks, which are
ineffective in capturing long-range dependencies. In this paper, we propose to
substantially improve Occupancy Networks, a state-of-the-art method for 3D
object reconstruction. For such we apply the concept of self-attention within
the network's encoder in order to leverage complementary input features rather
than those based on local regions, helping the encoder to extract global
information. With our approach, we were capable of improving the original work
in 5.05% of mesh IoU, 0.83% of Normal Consistency, and more than 10X the
Chamfer-L1 distance. We also perform a qualitative study that shows that our
approach was able to generate much more consistent meshes, confirming its
increased generalization power over the current state-of-the-art.","['Andrey Salvi', 'Nathan Gavenski', 'Eduardo Pooch', 'Felipe Tasoniero', 'Rodrigo Barros']",2020-08-11T14:51:18Z,http://arxiv.org/abs/2008.04738v1,['cs.CV'],"3D reconstruction,attention-based,single image,convolutional neural networks,Occupancy Networks,self-attention,global information,mesh IoU,Normal Consistency,Chamfer-L1 distance"
"Exploring Connections Between Cosmos & Mind Through Six Interactive Art
  Installations in ""As Above As Below""","Are there parallels between the furthest reaches of our universe, and the
foundations of thought, awareness, perception, and emotion? What are the
connections between the webs and structures that define both? What are the
differences? ""As Above As Below"" was an exhibition that examined these
questions. It consisted of six artworks, each of them the product of a
collaboration that included at least one artist, astrophysicist, and
neuroscientist. The installations explored new parallels between intergalactic
and neuronal networks through media such as digital projection, virtual
reality, and interactive multimedia, and served to illustrate diverse
collaboration practices and ways to communicate across very different fields.","['Mark Neyrinck', 'Tamira Elul', 'Michael Silver', 'Esther Mallouh', 'Miguel Aragón-Calvo', 'Sarah Banducci', 'Cory Bloyd', 'Thea Boodhoo', 'Benedikt Diemer', 'Bridget Falck', 'Dan Feldman', 'Yoon Chung Han', 'Jeffrey Kruk', 'Soo Jung Kwak', 'Yagiz Mungan', 'Miguel Novelo', 'Rushi Patel', 'Purin Phanichphant', 'Joel Primack', 'Olaf Sporns', 'Forest Stearns', 'Anastasia Victor', 'David Weinberg', 'Natalie M. Zahr']",2020-08-13T14:51:12Z,http://arxiv.org/abs/2008.05942v4,"['physics.pop-ph', 'astro-ph.CO', 'physics.bio-ph']","Cosmos,Mind,Interactive Art,Installations,As Above As Below,Universe,Thought,Awareness,Perception,Emotion"
"Generative View Synthesis: From Single-view Semantics to Novel-view
  Images","Content creation, central to applications such as virtual reality, can be a
tedious and time-consuming. Recent image synthesis methods simplify this task
by offering tools to generate new views from as little as a single input image,
or by converting a semantic map into a photorealistic image. We propose to push
the envelope further, and introduce Generative View Synthesis (GVS), which can
synthesize multiple photorealistic views of a scene given a single semantic
map. We show that the sequential application of existing techniques, e.g.,
semantics-to-image translation followed by monocular view synthesis, fail at
capturing the scene's structure. In contrast, we solve the semantics-to-image
translation in concert with the estimation of the 3D layout of the scene, thus
producing geometrically consistent novel views that preserve semantic
structures. We first lift the input 2D semantic map onto a 3D layered
representation of the scene in feature space, thereby preserving the semantic
labels of 3D geometric structures. We then project the layered features onto
the target views to generate the final novel-view images. We verify the
strengths of our method and compare it with several advanced baselines on three
different datasets. Our approach also allows for style manipulation and image
editing operations, such as the addition or removal of objects, with simple
manipulations of the input style images and semantic maps respectively. Visit
the project page at https://gvsnet.github.io.","['Tewodros Habtegebrial', 'Varun Jampani', 'Orazio Gallo', 'Didier Stricker']",2020-08-20T17:48:16Z,http://arxiv.org/abs/2008.09106v2,"['cs.CV', 'cs.LG', 'eess.IV']","Generative View Synthesis,Semantics-to-image translation,Novel-view images,3D layout,Semantic map,Photorealistic image,Monocular view synthesis,3D geometric structures,Style manipulation"
"A Single Frame and Multi-Frame Joint Network for 360-degree Panorama
  Video Super-Resolution","Spherical videos, also known as \ang{360} (panorama) videos, can be viewed
with various virtual reality devices such as computers and head-mounted
displays. They attract large amount of interest since awesome immersion can be
experienced when watching spherical videos. However, capturing, storing and
transmitting high-resolution spherical videos are extremely expensive. In this
paper, we propose a novel single frame and multi-frame joint network (SMFN) for
recovering high-resolution spherical videos from low-resolution inputs. To take
advantage of pixel-level inter-frame consistency, deformable convolutions are
used to eliminate the motion difference between feature maps of the target
frame and its neighboring frames. A mixed attention mechanism is devised to
enhance the feature representation capability. The dual learning strategy is
exerted to constrain the space of solution so that a better solution can be
found. A novel loss function based on the weighted mean square error is
proposed to emphasize on the super-resolution of the equatorial regions. This
is the first attempt to settle the super-resolution of spherical videos, and we
collect a novel dataset from the Internet, MiG Panorama Video, which includes
204 videos. Experimental results on 4 representative video clips demonstrate
the efficacy of the proposed method. The dataset and code are available at
https://github.com/lovepiano/SMFN_For_360VSR.","['Hongying Liu', 'Zhubo Ruan', 'Chaowei Fang', 'Peng Zhao', 'Fanhua Shang', 'Yuanyuan Liu', 'Lijun Wang']",2020-08-24T11:09:54Z,http://arxiv.org/abs/2008.10320v1,"['cs.CV', 'cs.AI', 'stat.ML']","360-degree video,Panorama video,Super-resolution,Spherical videos,Virtual reality devices,Deformable convolutions,Mixed attention mechanism,Dual learning strategy,Loss function,Equatorial regions"
The challenges of Scheduling and Resource Allocation in IEEE 802.11ad/ay,"The IEEE 802.11ad WiFi amendment enables short-range multi-gigabit
communications in the unlicensed 60~GHz spectrum, unlocking new interesting
applications such as wireless Augmented and Virtual Reality. The
characteristics of the mmWave band and directional communications allow
increasing the system throughput by scheduling pairs of nodes with low
cross-interfering channels in the same time-frequency slot. On the other hand,
this requires significantly more signaling overhead. Furthermore, IEEE 802.11ad
introduces a hybrid MAC characterized by two different channel access
mechanisms: contention-based and contention-free access periods. The
coexistence of both access period types and the directionality typical of
mmWave increase the channel access and scheduling complexity in IEEE 802.11ad
compared to previous WiFi versions. Hence, to provide the Quality of Service
(QoS) performance required by demanding applications, a proper resource
scheduling mechanism that takes into account both directional communications
and the newly added features of this WiFi amendment is needed. In this paper,
we present a brief but comprehensive review of the open problems and challenges
associated with channel access in IEEE 802.11ad and propose a workflow to
tackle them via both heuristic and learning-based methods.","['Salman Mohebi', 'Mattia Lecci', 'Andrea Zanella', 'Michele Zorzi']",2020-08-27T07:28:28Z,http://arxiv.org/abs/2008.11959v1,['cs.NI'],"Scheduling,Resource Allocation,IEEE 802.11ad,WiFi,mmWave,Channel Access,Directional Communications,Quality of Service (QoS),Hybrid MAC,Signaling Overhead"
Interface Design for HCI Classroom: From Learners' Perspective,"Having a good Human-Computer Interaction (HCI) design is challenging.
Previous works have contributed significantly to fostering HCI, including
design principle with report study from the instructor view. The questions of
how and to what extent students perceive the design principles are still left
open. To answer this question, this paper conducts a study of HCI adoption in
the classroom. The studio-based learning method was adapted to teach 83
graduate and undergraduate students in 16 weeks long with four activities. A
standalone presentation tool for instant online peer feedback during the
presentation session was developed to help students justify and critique
other's work. Our tool provides a sandbox, which supports multiple application
types, including Web-applications, Object Detection, Web-based Virtual Reality
(VR), and Augmented Reality (AR). After presenting one assignment and two
projects, our results showed that students acquired a better understanding of
the Golden Rules principle over time, which was demonstrated by the development
of visual interface design. The Wordcloud reveals the primary focus was on the
user interface and shed some light on students' interest in user experience.
The inter-rater score indicates the agreement among students that they have the
same level of understanding of the principles. The results show a high level of
guideline compliance with HCI principles, in which we witnessed variations in
visual cognitive styles. Regardless of diversity in visual preference, the
students presented high consistency and a similar perspective on adopting HCI
design principles. The results also elicited suggestions into the development
of the HCI curriculum in the future.","['Huyen N. Nguyen', 'Vinh T. Nguyen', 'Tommy Dang']",2020-10-04T18:49:24Z,http://arxiv.org/abs/2010.01651v1,"['cs.HC', 'H.5.2; H.1.2; K.3.2']","Human-Computer Interaction (HCI),design principles,interface design,studio-based learning method,peer feedback,sandbox,Web-applications,Object Detection,Virtual Reality (VR),Augmented Reality (AR)"
Interactive Visualization of Atmospheric Effects for Celestial Bodies,"We present an atmospheric model tailored for the interactive visualization of
planetary surfaces. As the exploration of the solar system is progressing with
increasingly accurate missions and instruments, the faithful visualization of
planetary environments is gaining increasing interest in space research,
mission planning, and science communication and education. Atmospheric effects
are crucial in data analysis and to provide contextual information for
planetary data. Our model correctly accounts for the non-linear path of the
light inside the atmosphere (in Earth's case), the light absorption effects by
molecules and dust particles, such as the ozone layer and the Martian dust, and
a wavelength-dependent phase function for Mie scattering. The mode focuses on
interactivity, versatility, and customization, and a comprehensive set of
interactive controls make it possible to adapt its appearance dynamically. We
demonstrate our results using Earth and Mars as examples. However, it can be
readily adapted for the exploration of other atmospheres found on, for example,
of exoplanets. For Earth's atmosphere, we visually compare our results with
pictures taken from the International Space Station and against the CIE clear
sky model. The Martian atmosphere is reproduced based on available scientific
data, feedback from domain experts, and is compared to images taken by the
Curiosity rover. The work presented here has been implemented in the OpenSpace
system, which enables interactive parameter setting and real-time feedback
visualization targeting presentations in a wide range of environments, from
immersive dome theaters to virtual reality headsets.","['Jonathas Costa', 'Alexander Bock', 'Carter Emmart', 'Charles Hansen', 'Anders Ynnerman', 'Claudio Silva']",2020-10-07T17:28:26Z,http://arxiv.org/abs/2010.03534v1,"['cs.HC', 'astro-ph.EP', 'astro-ph.IM']","atmospheric effects,interactive visualization,planetary surfaces,solar system,mission planning,data analysis,light absorption,Mie scattering,interactivity,exoplanets"
Camera Travel for Immersive Colonography,"Immersive Colonography allows medical professionals to navigate inside the
intricate tubular geometries of subject-specific 3D colon images using Virtual
Reality displays. Typically, camera travel is performed via Fly-Through or
Fly-Over techniques that enable semi-automatic traveling through a constrained,
well-defined path at user controlled speeds. However, Fly-Through is known to
limit the visibility of lesions located behind or inside haustral folds, while
Fly-Over requires splitting the entire colon visualization into two specific
halves. In this paper, we study the effect of immersive Fly-Through and
Fly-Over techniques on lesion detection, and introduce a camera travel
technique that maintains a fixed camera orientation throughout the entire
medial axis path. While these techniques have been studied in non-VR desktop
environments, their performance is yet not well understood in VR setups. We
performed a comparative study to ascertain which camera travel technique is
more appropriate for constrained path navigation in Immersive Colonography. To
this end, we asked 18 participants to navigate inside a 3D colon to find
specific marks. Our results suggest that the Fly-Over technique may lead to
enhanced lesion detection at the cost of higher task completion times, while
the Fly-Through method may offer a more balanced trade-off between both speed
and effectiveness, whereas the fixed camera orientation technique provided
seemingly inferior performance results. Our study further provides design
guidelines and informs future work.","['Soraia F. Paulo', 'Daniel Medeiros', 'Pedro Borges', 'Joaquim Jorge', 'Daniel Simões Lopes']",2020-10-15T14:42:50Z,http://arxiv.org/abs/2010.07798v1,"['cs.HC', 'H.5.2']","Immersive Colonography,Camera travel,Virtual Reality,Fly-Through,Fly-Over,Lesion detection,Constrained path navigation,Medical professionals,VR setups,Comparative study"
SHREC 2020 track: 6D Object Pose Estimation,"6D pose estimation is crucial for augmented reality, virtual reality, robotic
manipulation and visual navigation. However, the problem is challenging due to
the variety of objects in the real world. They have varying 3D shape and their
appearances in captured images are affected by sensor noise, changing lighting
conditions and occlusions between objects. Different pose estimation methods
have different strengths and weaknesses, depending on feature representations
and scene contents. At the same time, existing 3D datasets that are used for
data-driven methods to estimate 6D poses have limited view angles and low
resolution. To address these issues, we organize the Shape Retrieval Challenge
benchmark on 6D pose estimation and create a physically accurate simulator that
is able to generate photo-realistic color-and-depth image pairs with
corresponding ground truth 6D poses. From captured color and depth images, we
use this simulator to generate a 3D dataset which has 400 photo-realistic
synthesized color-and-depth image pairs with various view angles for training,
and another 100 captured and synthetic images for testing. Five research groups
register in this track and two of them submitted their results. Data-driven
methods are the current trend in 6D object pose estimation and our evaluation
results show that approaches which fully exploit the color and geometric
features are more robust for 6D pose estimation of reflective and texture-less
objects and occlusion. This benchmark and comparative evaluation results have
the potential to further enrich and boost the research of 6D object pose
estimation and its applications.","['Honglin Yuan', 'Remco C. Veltkamp', 'Georgios Albanis', 'Nikolaos Zioulis', 'Dimitrios Zarpalas', 'Petros Daras']",2020-10-19T09:45:42Z,http://arxiv.org/abs/2010.09355v1,"['cs.CV', 'cs.LG', 'cs.RO']","6D object pose estimation,augmented reality,virtual reality,robotic manipulation,visual navigation,sensor noise,lighting conditions,occlusions,3D datasets,data-driven methods"
"Correlation-aware Cooperative Multigroup Broadcast 360° Video
  Delivery Network: A Hierarchical Deep Reinforcement Learning Approach","With the stringent requirement of receiving video from unmanned aerial
vehicle (UAV) from anywhere in the stadium of sports events and the
significant-high per-cell throughput for video transmission to virtual reality
(VR) users, a promising solution is a cell-free multi-group broadcast (CF-MB)
network with cooperative reception and broadcast access points (AP). To explore
the benefit of broadcasting user-correlated decode-dependent video resources to
spatially correlated VR users, the network should dynamically schedule the
video and cluster APs into virtual cells for a different group of VR users with
overlapped video requests. By decomposition the problem into scheduling and
association sub-problems, we first introduce the conventional
non-learning-based scheduling and association algorithms, and a centralized
deep reinforcement learning (DRL) association approach based on the rainbow
agent with a convolutional neural network (CNN) to generate decisions from
observation. To reduce its complexity, we then decompose the association
problem into multiple sub-problems, resulting in a networked-distributed
Partially Observable Markov decision process (ND-POMDP). To solve it, we
propose a multi-agent deep DRL algorithm. To jointly solve the coupled
association and scheduling problems, we further develop a hierarchical
federated DRL algorithm with scheduler as meta-controller, and association as
the controller. Our simulation results shown that our CF-MB network can
effectively handle real-time video transmission from UAVs to VR users. Our
proposed learning architectures is effective and scalable for a
high-dimensional cooperative association problem with increasing APs and VR
users. Also, our proposed algorithms outperform non-learning based methods with
significant performance improvement.","['Fenghe Hu', 'Yansha Deng', 'A. Hamid Aghvami']",2020-10-21T23:31:35Z,http://arxiv.org/abs/2010.11347v3,['eess.SP'],"Cooperative,Multigroup,Broadcast,Hierarchical,Deep Reinforcement Learning,UAV,Virtual Reality,AP,Convolutional Neural Network,Partially Observable Markov Decision Process"
"On Benchmarking Iris Recognition within a Head-mounted Display for AR/VR
  Application","Augmented and virtual reality is being deployed in different fields of
applications. Such applications might involve accessing or processing critical
and sensitive information, which requires strict and continuous access control.
Given that Head-Mounted Displays (HMD) developed for such applications commonly
contains internal cameras for gaze tracking purposes, we evaluate the
suitability of such setup for verifying the users through iris recognition. In
this work, we first evaluate a set of iris recognition algorithms suitable for
HMD devices by investigating three well-established handcrafted feature
extraction approaches, and to complement it, we also present the analysis using
four deep learning models. While taking into consideration the minimalistic
hardware requirements of stand-alone HMD, we employ and adapt a recently
developed miniature segmentation model (EyeMMS) for segmenting the iris.
Further, to account for non-ideal and non-collaborative capture of iris, we
define a new iris quality metric that we termed as Iris Mask Ratio (IMR) to
quantify the iris recognition performance. Motivated by the performance of iris
recognition, we also propose the continuous authentication of users in a
non-collaborative capture setting in HMD. Through the experiments on a publicly
available OpenEDS dataset, we show that performance with EER = 5% can be
achieved using deep learning methods in a general setting, along with high
accuracy for continuous user authentication.","['Fadi Boutros', 'Naser Damer', 'Kiran Raja', 'Raghavendra Ramachandra', 'Florian Kirchbuchner', 'Arjan Kuijper']",2020-10-20T17:05:11Z,http://arxiv.org/abs/2010.11700v1,"['cs.CV', 'cs.LG']","Iris recognition,Head-mounted Display (HMD),Augmented reality (AR),Virtual reality (VR),Gaze tracking,Feature extraction,Deep learning models,Segmentation model,Iris quality metric,Continuous authentication"
"Continuous Operator Authentication for Teleoperated Systems Using Hidden
  Markov Models","In this paper, we present a novel approach for continuous operator
authentication in teleoperated robotic processes based on Hidden Markov Models
(HMM). While HMMs were originally developed and widely used in speech
recognition, they have shown great performance in human motion and activity
modeling. We make an analogy between human language and teleoperated robotic
processes (i.e. words are analogous to a teleoperator's gestures, sentences are
analogous to the entire teleoperated task or process) and implement HMMs to
model the teleoperated task. To test the continuous authentication performance
of the proposed method, we conducted two sets of analyses. We built a virtual
reality (VR) experimental environment using a commodity VR headset (HTC Vive)
and haptic feedback enabled controller (Sensable PHANToM Omni) to simulate a
real teleoperated task. An experimental study with 10 subjects was then
conducted. We also performed simulated continuous operator authentication by
using the JHU-ISI Gesture and Skill Assessment Working Set (JIGSAWS). The
performance of the model was evaluated based on the continuous (real-time)
operator authentication accuracy as well as resistance to a simulated
impersonation attack. The results suggest that the proposed method is able to
achieve 70% (VR experiment) and 81% (JIGSAW dataset) continuous classification
accuracy with as short as a 1-second sample window. It is also capable of
detecting the impersonation attack in real-time.","['Junjie Yan', 'Kevin Huang', 'Kyle Lindgren', 'Tamara Bonaci', 'Howard Jay Chizeck']",2020-10-27T02:33:10Z,http://arxiv.org/abs/2010.14006v3,['cs.CR'],"continuous operator authentication,teleoperated systems,Hidden Markov Models,HMM,virtual reality,haptic feedback,JHU-ISI Gesture and Skill Assessment Working Set,JIGSAWS,real-time,impersonation attack"
"An Unsupervised Approach towards Varying Human Skin Tone Using
  Generative Adversarial Networks","With the increasing popularity of augmented and virtual reality, retailers
are now focusing more towards customer satisfaction to increase the amount of
sales. Although augmented reality is not a new concept but it has gained much
needed attention over the past few years. Our present work is targeted towards
this direction which may be used to enhance user experience in various virtual
and augmented reality based applications. We propose a model to change skin
tone of a person. Given any input image of a person or a group of persons with
some value indicating the desired change of skin color towards fairness or
darkness, this method can change the skin tone of the persons in the image.
This is an unsupervised method and also unconstrained in terms of pose,
illumination, number of persons in the image etc. The goal of this work is to
reduce the time and effort which is generally required for changing the skin
tone using existing applications (e.g., Photoshop) by professionals or novice.
To establish the efficacy of this method we have compared our result with that
of some popular photo editor and also with the result of some existing
benchmark method related to human attribute manipulation. Rigorous experiments
on different datasets show the effectiveness of this method in terms of
synthesizing perceptually convincing outputs.","['Debapriya Roy', 'Diganta Mukherjee', 'Bhabatosh Chanda']",2020-10-30T06:27:03Z,http://arxiv.org/abs/2010.16092v1,['cs.CV'],"unsupervised approach,human skin tone,generative adversarial networks,augmented reality,virtual reality,user experience,image manipulation,skin color,fairness,darkness"
"Reliability Enhancement for VR Delivery in Mobile-Edge Empowered
  Dual-Connectivity Sub-6 GHz and mmWave HetNets","The reliability of current virtual reality (VR) delivery is low due to the
limited resources on VR head-mounted displays (HMDs) and the transmission rate
bottleneck of sub-6 GHz networks. In this paper, we propose a dual-connectivity
sub-6 GHz and mmWave heterogeneous network architecture empowered by mobile
edge capability. The core idea of the proposed architecture is to utilize the
complementary advantages of sub-6 GHz links and mmWave links to conduct a
collaborative edge resource design, which aims to improve the reliability of VR
delivery. From the perspective of stochastic geometry, we analyze the
reliability of VR delivery and theoretically demonstrate that sub-6 GHz links
can be used to enhance the reliability of VR delivery despite the large mmWave
bandwidth. Based on our analytical work, we formulate a joint caching and
computing optimization problem with the goal to maximize the reliability of VR
delivery. By analyzing the coupling caching and computing strategies at HMDs,
sub-6 GHz and mmWave base stations (BSs), we further transform the problem into
a multiple-choice multi-dimension knapsack problem. A best-first branch and
bound algorithm and a difference of convex programming algorithm are proposed
to obtain the optimal and sub-optimal solution, respectively. Numerical results
demonstrate the performance improvement using the proposed algorithms, and
reveal that caching more monocular videos at sub-6 GHz BSs and more
stereoscopic videos at mmWave BSs can improve the VR delivery reliability
efficiently.","['Zhuojia Gu', 'Hancheng Lu', 'Peilin Hong', 'Yongdong Zhang']",2020-11-20T09:43:38Z,http://arxiv.org/abs/2011.10293v2,['cs.NI'],"virtual reality,delivery,mobile-edge,sub-6 GHz,mmWave,HetNets,stochastic geometry,caching,computing,algorithm"
"Towards the Development of 3D Engine Assembly Simulation Learning Module
  for Senior High School","The focus of the study is to develop a 3D engine assembly simulation learning
module to address the lack of equipment in one senior high school in the
Philippines. The study used mixed-method to determine the considerations needed
in developing an application for educational use particularly among
laboratory/practical subjects like engine assembly. The study used ISO 25010
quality standards in evaluating the application(n=153 students and 3 ICT
experts).Results showed that the application is moderately acceptable(overall
mean = 3.52) under ISO 25010 quality standards. The study created an engine
simulation learning assembly in which teachers can use to augment their lesson.
The study also highlights the applicability of using 3D-related technologies
for practical and laboratory subjects particularly highly technical-related
subjects. Future studies may develop a similar application in the same context
using mobile and other emerging technology(i.e., Virtual Reality, Augmented
Reality) as well as making the content more customizable. Effectivity of the
system in an actual setting is also worth pursuing. The study highlighted the
potential use of 3D technology in a classroom setting.","['John Paul P. Miranda', 'Jaymark A. Yambao', 'Jhon Asley M. Marcelo', 'Christopher Robert N. Gonzales', 'Vee-jay T. Mungcal']",2020-11-19T10:45:29Z,http://arxiv.org/abs/2011.12767v1,['cs.CY'],"3D engine assembly,simulation,learning module,senior high school,mixed-method,ISO 25010,educational application,laboratory,practical subjects,3D-related technologies"
"cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex
  Polytope","During the last years, the emerging field of Augmented & Virtual Reality
(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop
low cost high-quality AR systems where computing poweris in demand. Feature
points are extensively used in these real-time frame-rate and 3D applications,
thereforeefficient high-speed feature detectors are necessary. Corners are such
special features and often are used as thefirst step in the marker alignment in
Augmented Reality (AR). Corners are also used in image registration
andrecognition, tracking, SLAM, robot path finding and 2D or 3D object
detection and retrieval. Therefore thereis a large number of corner detection
algorithms but most of them are too computationally intensive for use
inreal-time applications of any complexity. Many times the border of the image
is a convex polygon. For thisspecial, but quite common case, we have developed
a specific algorithm, cMinMax. The proposed algorithmis faster, approximately
by a factor of 5 compared to the widely used Harris Corner Detection algorithm.
Inaddition is highly parallelizable. The algorithm is suitable for the fast
registration of markers in augmentedreality systems and in applications where a
computationally efficient real time feature detector is necessary.The algorithm
can also be extended to N-dimensional polyhedrons.","['Dimitrios Chamzas', 'Constantinos Chamzas', 'Konstantinos Moustakas']",2020-11-28T00:32:11Z,http://arxiv.org/abs/2011.14035v3,"['cs.CV', 'cs.GR']","corners,N-dimensional convex polytope,feature points,feature detectors,augmented reality,real-time applications,algorithm,Harris Corner Detection,parallelizable,marker registration"
Freeform imaging system design with multiple reflection surfaces,"Reflective imaging systems form an important part of photonic devices such as
spectrometers, telescopes, augmented and virtual reality headsets or
lithography platforms. Reflective optics provide unparalleled spectral
performance and can be used to reduce overall volume and weight. So far, most
reflective designs have focused on two or three reflections, while
four-reflection freeform designs can deliver a higher light throughput (faster
F-number) as well as a larger field-of-view (FOV). However, advanced optical
design strategies for four-reflection freeform systems have been rarely
reported in literature. This is due to the increased complexity in solution
space but also the fact that additional mirrors hinder a cost-effective
realization (manufacture, alignment, etc.).
  Recently, we have proposed a novel design method to directly calculate the
freeform surface coefficients while merely knowing the mirror positions and
tilts. Consequently, this method allows laymen with basic optical design
knowledge to calculate 'first time right' freeform imaging systems in a matter
of minutes. This contrasts with most common freeform design processes, which
requires considerable experience, intuition or guesswork. Firstly, we
demonstrate the effectiveness of the proposed method for a four-mirror
high-throughput telescope with 250mm-focal-length, F/2.5 and a wide rectangular
FOV of 8.5{\deg} x 25.5{\deg}. In a subsequent step, we propose an effective
three-mirror but four-reflection imaging system, which consists of two freeform
mirrors and one double-reflection spherical mirror. Compared with common
three-mirror and three-reflection imagers, our novel multi-reflection system
shows unprecedented possibilities for an economic implementation while
drastically reducing the overall volume.","['Yunfeng Nie', 'David R. Shafer', 'Heidi Ottevaere', 'Hugo Thienpont', 'Fabian Duerr']",2020-11-30T14:02:44Z,http://arxiv.org/abs/2011.14802v1,"['physics.optics', 'astro-ph.IM']","freeform design,imaging system,reflection surfaces,optical design,mirror positions,mirror tilts,light throughput,field-of-view,spectral performance,optical design strategies"
"We Dare You: A Lifecycle Study of a Substitutional Reality Installation
  in a Museum Space","In this article, we present a lifecycle study of We Dare You, a
Substitutional Reality (SR) installation that combines visual and tactile
stimuli. The installation is set up in a center for architecture, and invites
visitors to explore its facade while playing with vertigo, in a visual Virtual
Reality (VR) environment that replicates the surrounding physical space of the
installation. Drawing on an ethnographic approach, including observations and
interviews, we researched the exhibit from its opening, through the initial
months plagued by technical problems, its subsequent success as a social and
playful installation, on to its closure, due to COVID-19, and its subsequent
reopening. Our findings explore the challenges caused by both the hybrid nature
of the installation, as well as the visitor' playful use of the installation
which made the experience social and performative - but also caused some
problems. We also discuss the problems We Dare You faced in light of hygiene
demands due to COVID-19. The analysis contrasts the design processes and
expectations of stakeholders with the audience's playful appropriation, which
led the stakeholders to see the installation as both a success and a failure.
Evaluating the design and redesign through use on behalf of visitors, we argue
that an approach that further opens up the post-production experience to a
process of continuous redesign based on the user input - what has been termed
""design-after-design"" - could facilitate the design of similar experiences in
the museum and heritage sector, supporting a participatory agenda in the design
process, and helping to resolve the tension between stakeholders' expectations
and visitors' playful appropriations.","['Petros Ioannidis', 'Lina Eklund', 'Anders Sundnes Løvlie']",2020-12-03T09:49:18Z,http://arxiv.org/abs/2012.01792v1,['cs.HC'],"Substitutional Reality,Installation,Virtual Reality,Ethnographic approach,Stakeholders,Design processes,User input,Participatory agenda,Museum,Heritage sector"
"Optimizing sensors placement in complex networks for localization of
  hidden signal source: A review","As the world becomes more and more interconnected, our everyday objects
become part of the Internet of Things, and our lives get more and more mirrored
in virtual reality, where every piece of~information, including misinformation,
fake news and malware, can spread very fast practically anonymously. To
suppress such uncontrolled spread, efficient computer systems and algorithms
capable to~track down such malicious information spread have to be developed.
Currently, the most effective methods for source localization are based on
sensors which provide the times at which they detect the~spread. We investigate
the problem of the optimal placement of such sensors in complex networks and
propose a new graph measure, called Collective Betweenness, which we compare
against four other metrics. Extensive numerical tests are performed on
different types of complex networks over the wide ranges of densities of
sensors and stochasticities of signal. In these tests, we discovered clear
difference in comparative performance of the investigated optimal placement
methods between real or scale-free synthetic networks versus narrow degree
distribution networks. The former have a clear region for any given method's
dominance in contrast to the latter where the performance maps are less
homogeneous. We find that while choosing the best method is very network and
spread dependent, there are two methods that consistently stand out. High
Variance Observers seem to do very well for spread with low stochasticity
whereas Collective Betwenness, introduced in this paper, thrives when the
spread is highly unpredictable.","['Robert Paluch', 'Łukasz G. Gajewski', 'Janusz A. Hołyst', 'Boleslaw K. Szymanski']",2020-12-03T12:45:29Z,http://arxiv.org/abs/2012.01876v1,"['cs.SI', 'cs.CY', 'physics.data-an']","sensors,placement,complex networks,localization,signal source,graph measure,Collective Betweenness,stochasticity,scale-free networks,optimal placement."
E3D: Event-Based 3D Shape Reconstruction,"3D shape reconstruction is a primary component of augmented/virtual reality.
Despite being highly advanced, existing solutions based on RGB, RGB-D and Lidar
sensors are power and data intensive, which introduces challenges for
deployment in edge devices. We approach 3D reconstruction with an event camera,
a sensor with significantly lower power, latency and data expense while
enabling high dynamic range. While previous event-based 3D reconstruction
methods are primarily based on stereo vision, we cast the problem as multi-view
shape from silhouette using a monocular event camera. The output from a moving
event camera is a sparse point set of space-time gradients, largely sketching
scene/object edges and contours. We first introduce an event-to-silhouette
(E2S) neural network module to transform a stack of event frames to the
corresponding silhouettes, with additional neural branches for camera pose
regression. Second, we introduce E3D, which employs a 3D differentiable
renderer (PyTorch3D) to enforce cross-view 3D mesh consistency and fine-tune
the E2S and pose network. Lastly, we introduce a 3D-to-events simulation
pipeline and apply it to publicly available object datasets and generate
synthetic event/silhouette training pairs for supervised learning.","['Alexis Baudron', 'Zihao W. Wang', 'Oliver Cossairt', 'Aggelos K. Katsaggelos']",2020-12-09T18:23:21Z,http://arxiv.org/abs/2012.05214v2,['cs.CV'],"3D shape reconstruction,event camera,RGB-D,Lidar sensors,stereo vision,neural network,3D differentiable renderer,PyTorch3D,supervised learning"
Artificial Intelligence at the Edge,"The Internet of Things (IoT) and edge computing applications aim to support a
variety of societal needs, including the global pandemic situation that the
entire world is currently experiencing and responses to natural disasters.
  The need for real-time interactive applications such as immersive video
conferencing, augmented/virtual reality, and autonomous vehicles, in education,
healthcare, disaster recovery and other domains, has never been higher. At the
same time, there have been recent technological breakthroughs in highly
relevant fields such as artificial intelligence (AI)/machine learning (ML),
advanced communication systems (5G and beyond), privacy-preserving
computations, and hardware accelerators. 5G mobile communication networks
increase communication capacity, reduce transmission latency and error, and
save energy -- capabilities that are essential for new applications. The
envisioned future 6G technology will integrate many more technologies,
including for example visible light communication, to support groundbreaking
applications, such as holographic communications and high precision
manufacturing. Many of these applications require computations and analytics
close to application end-points: that is, at the edge of the network, rather
than in a centralized cloud. AI techniques applied at the edge have tremendous
potential both to power new applications and to need more efficient operation
of edge infrastructure. However, it is critical to understand where to deploy
AI systems within complex ecosystems consisting of advanced applications and
the specific real-time requirements towards AI systems.","['Elisa Bertino', 'Sujata Banerjee']",2020-12-10T02:08:47Z,http://arxiv.org/abs/2012.05410v1,"['cs.CY', 'cs.AI']","Artificial Intelligence,Edge Computing,Internet of Things,Machine Learning,5G,6G,Communication Systems,Privacy-Preserving Computations,Hardware Accelerators"
CUDA-Optimized real-time rendering of a Foveated Visual System,"The spatially-varying field of the human visual system has recently received
a resurgence of interest with the development of virtual reality (VR) and
neural networks. The computational demands of high resolution rendering desired
for VR can be offset by savings in the periphery, while neural networks trained
with foveated input have shown perceptual gains in i.i.d and o.o.d
generalization. In this paper, we present a technique that exploits the CUDA
GPU architecture to efficiently generate Gaussian-based foveated images at high
definition (1920x1080 px) in real-time (165 Hz), with a larger number of
pooling regions than previous Gaussian-based foveation algorithms by several
orders of magnitude, producing a smoothly foveated image that requires no
further blending or stitching, and that can be well fit for any contrast
sensitivity function. The approach described can be adapted from Gaussian
blurring to any eccentricity-dependent image processing and our algorithm can
meet demand for experimentation to evaluate the role of spatially-varying
processing across biological and artificial agents, so that foveation can be
added easily on top of existing systems rather than forcing their redesign
(emulated foveated renderer). Altogether, this paper demonstrates how a GPU,
with a CUDA block-wise architecture, can be employed for radially-variant
rendering, with opportunities for more complex post-processing to ensure a
metameric foveation scheme. Code is provided.","['Elian Malkin', 'Arturo Deza', 'Tomaso Poggio']",2020-12-15T22:43:04Z,http://arxiv.org/abs/2012.08655v1,"['eess.IV', 'cs.CV', 'cs.LG', 'q-bio.NC']","CUDA,real-time rendering,Foveated Visual System,virtual reality,neural networks,high resolution rendering,Gaussian-based foveated images,high definition,pooling regions,contrast sensitivity function"
Deep Learning-Based Human Pose Estimation: A Survey,"Human pose estimation aims to locate the human body parts and build human
body representation (e.g., body skeleton) from input data such as images and
videos. It has drawn increasing attention during the past decade and has been
utilized in a wide range of applications including human-computer interaction,
motion analysis, augmented reality, and virtual reality. Although the recently
developed deep learning-based solutions have achieved high performance in human
pose estimation, there still remain challenges due to insufficient training
data, depth ambiguities, and occlusion. The goal of this survey paper is to
provide a comprehensive review of recent deep learning-based solutions for both
2D and 3D pose estimation via a systematic analysis and comparison of these
solutions based on their input data and inference procedures. More than 250
research papers since 2014 are covered in this survey. Furthermore, 2D and 3D
human pose estimation datasets and evaluation metrics are included.
Quantitative performance comparisons of the reviewed methods on popular
datasets are summarized and discussed. Finally, the challenges involved,
applications, and future research directions are concluded. A regularly updated
project page is provided: \url{https://github.com/zczcwh/DL-HPE}","['Ce Zheng', 'Wenhan Wu', 'Chen Chen', 'Taojiannan Yang', 'Sijie Zhu', 'Ju Shen', 'Nasser Kehtarnavaz', 'Mubarak Shah']",2020-12-24T18:49:06Z,http://arxiv.org/abs/2012.13392v5,"['cs.CV', 'cs.GR', 'cs.MM']","Deep learning,Human pose estimation,Body skeleton,Human-computer interaction,Motion analysis,Augmented reality,Virtual reality,Training data,2D pose estimation,3D pose estimation"
"Interpersonal distance in VR: reactions of older adults to the presence
  of a virtual agent","The rapid development of virtual reality technology has increased its
availability and, consequently, increased the number of its possible
applications. The interest in the new medium has grown due to the entertainment
industry (games, VR experiences and movies). The number of freely available
training and therapeutic applications is also increasing. Contrary to popular
opinion, new technologies are also adopted by older adults. Creating virtual
environments tailored to the needs and capabilities of older adults requires
intense research on the behaviour of these participants in the most common
situations, towards commonly used elements of the virtual environment, in
typical sceneries. Comfortable immersion in a virtual environment is key to
achieving the impression of presence. Presence is, in turn, necessary to obtain
appropriate training, persuasive and therapeutic effects. A virtual agent (a
humanoid representation of an algorithm or artificial intelligence) is often an
element of the virtual environment interface. Maintaining an appropriate
distance to the agent is, therefore, a key parameter for the creator of the VR
experience. Older (65+) participants maintain greater distance towards an agent
(a young white male) than younger ones (25-35). It may be caused by differences
in the level of arousal, but also cultural norms. As a consequence, VR
developers are advised to use algorithms that maintain the agent at the
appropriate distance, depending on the user's age.","['Grzegorz Pochwatko', 'Barbara Karpowicz', 'Anna Chrzanowska', 'Wiesław KopeāE]",2021-01-05T17:06:03Z,http://arxiv.org/abs/2101.01652v1,"['cs.HC', 'cs.CY', 'cs.MM', 'H.5.2; H.5.m']","virtual reality,older adults,virtual agent,interpersonal distance,immersion,presence,virtual environment,virtual agent interface,VR developers,algorithms"
"Learning Ultrasound Rendering from Cross-Sectional Model Slices for
  Simulated Training","Purpose. Given the high level of expertise required for navigation and
interpretation of ultrasound images, computational simulations can facilitate
the training of such skills in virtual reality. With ray-tracing based
simulations, realistic ultrasound images can be generated. However, due to
computational constraints for interactivity, image quality typically needs to
be compromised.
  Methods. We propose herein to bypass any rendering and simulation process at
interactive time, by conducting such simulations during a non-time-critical
offline stage and then learning image translation from cross-sectional model
slices to such simulated frames. We use a generative adversarial framework with
a dedicated generator architecture and input feeding scheme, which both
substantially improve image quality without increase in network parameters.
Integral attenuation maps derived from cross-sectional model slices,
texture-friendly strided convolutions, providing stochastic noise and input
maps to intermediate layers in order to preserve locality are all shown herein
to greatly facilitate such translation task.
  Results. Given several quality metrics, the proposed method with only tissue
maps as input is shown to provide comparable or superior results to a
state-of-the-art that uses additional images of low-quality ultrasound
renderings. An extensive ablation study shows the need and benefits from the
individual contributions utilized in this work, based on qualitative examples
and quantitative ultrasound similarity metrics. To that end, a local histogram
statistics based error metric is proposed and demonstrated for visualization of
local dissimilarities between ultrasound images.","['Lin Zhang', 'Tiziano Portenier', 'Orcun Goksel']",2021-01-20T21:58:19Z,http://arxiv.org/abs/2101.08339v1,"['eess.IV', 'cs.CV']","ultrasound rendering,cross-sectional model slices,simulated training,ray-tracing,generative adversarial framework,attenuation maps,strided convolutions,stochastic noise,texture-friendly,error metric"
Towards Sneaking as a Playful Input Modality for Virtual Environments,"Using virtual reality setups, users can fade out of their surroundings and
dive fully into a thrilling and appealing virtual environment. The success of
such immersive experiences depends heavily on natural and engaging interactions
with the virtual world. As developers tend to focus on intuitive hand controls,
other aspects of the broad range of full-body capabilities are easily left
vacant. One repeatedly overlooked input modality is the user's gait. Even
though users may walk physically to explore the environment, it usually does
not matter how they move. However, gait-based interactions, using the variety
of information contained in human gait, could offer interesting benefits for
immersive experiences. For instance, stealth VR-games could profit from this
additional range of interaction fidelity in the form of a sneaking-based input
modality. In our work, we explore the potential of sneaking as a playful input
modality for virtual environments. Therefore, we discuss possible
sneaking-based gameplay mechanisms and develop three technical approaches,
including precise foot-tracking and two abstraction levels. Our evaluation
reveals the potential of sneaking-based interactions in IVEs, offering unique
challenges and thrilling gameplay. For these interactions, precise tracking of
individual footsteps is unnecessary, as a more abstract approach focusing on
the players' intention offers the same experience while providing better
comprehensible feedback. Based on these findings, we discuss the broader
potential and individual strengths of our gait-centered interactions.","['Sebastian Cmentowski', 'Andrey Krekhov', 'André Zenner', 'Daniel Kucharski', 'Jens Krüger']",2021-02-03T11:55:28Z,http://arxiv.org/abs/2102.02024v3,['cs.HC'],"virtual reality,input modality,virtual environments,gait,interaction fidelity,immersive experiences,gameplay mechanisms,foot-tracking,abstraction levels"
"TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and
  Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector,
  and Eye Movement Types","We present TEyeD, the world's largest unified public data set of eye images
taken with head-mounted devices. TEyeD was acquired with seven different
head-mounted eye trackers. Among them, two eye trackers were integrated into
virtual reality (VR) or augmented reality (AR) devices. The images in TEyeD
were obtained from various tasks, including car rides, simulator rides, outdoor
sports activities, and daily indoor activities. The data set includes 2D and 3D
landmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and
eye movement types for all images. Landmarks and semantic segmentation are
provided for the pupil, iris and eyelids. Video lengths vary from a few minutes
to several hours. With more than 20 million carefully annotated images, TEyeD
provides a unique, coherent resource and a valuable foundation for advancing
research in the field of computer vision, eye tracking and gaze estimation in
modern VR and AR applications.
  Download: Just connect via FTP as user TEyeDUser and without password to
nephrit.cs.uni-tuebingen.de (ftp://nephrit.cs.uni-tuebingen.de).","['Wolfgang Fuhl', 'Gjergji Kasneci', 'Enkelejda Kasneci']",2021-02-03T15:48:22Z,http://arxiv.org/abs/2102.02115v3,"['eess.IV', 'cs.CV']","real-world eye images,pupil,eyelid,iris,2D landmarks,3D landmarks,3D eyeball,gaze vector,eye movement types,segmentation"
"Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for
  Autonomous Vehicle-to-Pedestrian Communication","Providing pedestrians and other vulnerable road users with a clear indication
about a fully autonomous vehicle status and intentions is crucial to make them
coexist. In the last few years, a variety of external interfaces have been
proposed, leveraging different paradigms and technologies including
vehicle-mounted devices (like LED panels), short-range on-road projections, and
road infrastructure interfaces (e.g., special asphalts with embedded displays).
These designs were experimented in different settings, using mockups, specially
prepared vehicles, or virtual environments, with heterogeneous evaluation
metrics. Promising interfaces based on Augmented Reality (AR) have been
proposed too, but their usability and effectiveness have not been tested yet.
This paper aims to complement such body of literature by presenting a
comparison of state-of-the-art interfaces and new designs under common
conditions. To this aim, an immersive Virtual Reality-based simulation was
developed, recreating a well-known scenario represented by pedestrians crossing
in urban environments under non-regulated conditions. A user study was then
performed to investigate the various dimensions of vehicle-to-pedestrian
interaction leveraging objective and subjective metrics. Even though no
interface clearly stood out over all the considered dimensions, one of the AR
designs achieved state-of-the-art results in terms of safety and trust, at the
cost of higher cognitive effort and lower intuitiveness compared to LED panels
showing anthropomorphic features. Together with rankings on the various
dimensions, indications about advantages and drawbacks of the various
alternatives that emerged from this study could provide important information
for next developments in the field.","['F. Gabriele Pratticò', 'Fabrizio Lamberti', 'Alberto Cannavò', 'Lia Morra', 'Paolo Montuschi']",2021-02-04T18:03:06Z,http://arxiv.org/abs/2102.02783v1,"['cs.HC', 'cs.AI', 'cs.GR']","Augmented Reality,Autonomous Vehicles,Interfaces,Pedestrian Communication,Virtual Reality,User Study"
"Haptic-enabled Mixed Reality System for Mixed-initiative Remote Robot
  Control","Robots assist in many areas that are considered unsafe for humans to operate.
For instance, in handling pandemic diseases such as the recent Covid-19
outbreak and other outbreaks like Ebola, robots can assist in reaching areas
dangerous for humans and do simple tasks such as pick up the correct medicine
(among a set of bottles prescribed) and deliver to patients. In such cases, it
might not be good to rely on the fully autonomous operation of robots. Since
many mobile robots are fully functional with low-level tasks such as grabbing
and moving, we consider the mixed-initiative control where the user can guide
the robot remotely to finish such tasks. For this mixed-initiative control, the
user controlling the robot needs to visualize a 3D scene as seen by the robot
and guide it. Mixed reality can virtualize reality and immerse users in the 3D
scene that is reconstructed from the real-world environment. This technique
provides the user more freedom such as choosing viewpoints at view time. In
recent years, benefiting from the high-quality data from Light Detection and
Ranging (LIDAR) and RGBD cameras, mixed reality is widely used to build
networked platforms to improve the performance of robot teleoperations and
robot-human collaboration, and enhanced feedback for mixed-initiative control.
In this paper, we proposed a novel haptic-enabled mixed reality system, that
provides haptic interfaces to interact with the virtualized environments and
give remote guidance for mobile robots towards high-level tasks. The
experimental results show the effectiveness and flexibility of the proposed
haptic enabled mixed reality system.","['Yuan Tian', 'Lianjun Li', 'Andrea Fumagalli', 'Yonas Tadesse', 'Balakrishnan Prabhakaran']",2021-02-06T06:15:15Z,http://arxiv.org/abs/2102.03521v2,['cs.RO'],"Haptic-enabled,Mixed Reality System,Mixed-initiative,Remote Robot Control,3D scene,Mixed reality,LIDAR,RGBD cameras,Teleoperations,Robot-human collaboration"
Manipulability optimization for multi-arm teleoperation,"Teleoperation provides a way for human operators to guide robots in
situations where full autonomy is challenging or where direct human
intervention is required. It can also be an important tool to teach robots in
order to achieve autonomous behaviour later on. The increased availability of
collaborative robot arms and Virtual Reality (VR) devices provides ample
opportunity for development of novel teleoperation methods. Since robot arms
are often kinematically different from human arms, mapping human motions to a
robot in real-time is not trivial. Additionally, a human operator might steer
the robot arm toward singularities or its workspace limits, which can lead to
undesirable behaviour. This is further accentuated for the orchestration of
multiple robots. In this paper, we present a VR interface targeted to multi-arm
payload manipulation, which can closely match real-time input motion. Allowing
the user to manipulate the payload rather than mapping their motions to
individual arms we are able to simultaneously guide multiple collaborative
arms. By releasing a single rotational degree of freedom, and by using a local
optimization method, we can improve each arm's manipulability index, which in
turn lets us avoid kinematic singularities and workspace limitations. We apply
our approach to predefined trajectories as well as real-time teleoperation on
different robot arms and compare performance in terms of end effector position
error and relevant joint motion metrics.","['Florian Kennel-Maushart', 'Roi Poranne', 'Stelian Coros']",2021-02-10T13:14:20Z,http://arxiv.org/abs/2102.05414v2,['cs.RO'],"teleoperation,manipulability optimization,multi-arm,collaborative robot arms,Virtual Reality (VR) devices,kinematic singularities,workspace limitations,rotational degree of freedom,manipulability index"
"Millimeter Wave MIMO based Depth Maps for Wireless Virtual and Augmented
  Reality","Augmented and virtual reality systems (AR/VR) are rapidly becoming key
components of the wireless landscape. For immersive AR/VR experience, these
devices should be able to construct accurate depth perception of the
surrounding environment. Current AR/VR devices rely heavily on using RGB-D
depth cameras to achieve this goal. The performance of these depth cameras,
however, has clear limitations in several scenarios, such as the cases with
shiny objects, dark surfaces, and abrupt color transition among other
limitations. In this paper, we propose a novel solution for AR/VR depth map
construction using mmWave MIMO communication transceivers. This is motivated by
the deployment of advanced mmWave communication systems in future AR/VR devices
for meeting the high data rate demands and by the interesting propagation
characteristics of mmWave signals. Accounting for the constraints on these
systems, we develop a comprehensive framework for constructing accurate and
high-resolution depth maps using mmWave systems. In this framework, we
developed new sensing beamforming codebook approaches that are specific for the
depth map construction objective. Using these codebooks, and leveraging tools
from successive interference cancellation, we develop a joint beam processing
approach that can construct high-resolution depth maps using practical mmWave
antenna arrays. Extensive simulation results highlight the potential of the
proposed solution in building accurate depth maps. Further, these simulations
show the promising gains of mmWave based depth perception compared to RGB-based
approaches in several important use cases.","['Abdelrahman Taha', 'Qi Qu', 'Sam Alex', 'Ping Wang', 'William L. Abbott', 'Ahmed Alkhateeb']",2021-02-11T18:57:58Z,http://arxiv.org/abs/2102.06198v2,"['eess.SP', 'cs.IT', 'math.IT']","millimeter wave,MIMO,depth maps,wireless,virtual reality,augmented reality,mmWave communication,beamforming,interference cancellation,antenna arrays"
Key Technologies for Networked Virtual Environments,"Thanks to the improvements experienced in technology in the last few years,
most especially in virtual reality systems, the number and potential of
networked virtual environments or NVEs and their users are increasing. NVEs aim
to give distributed users a feeling of immersion in a virtual world and the
possibility of interacting with other users or with virtual objects inside it,
like when they interact in the real world. Being able to provide that feeling
and natural interactions when the users are geographically separated is one of
the goals of these systems. Nevertheless, this goal is especially sensitive to
different issues, such as different connections with heterogeneous throughput
or different network latencies, which can lead to consistency and
synchronization problems and, thus, to a worsening of the users' quality of
experience or QoE. With the purpose of solving these issues, researchers have
proposed and evaluated numerous technical solutions, in fields like network
architectures, data distribution and filtering, resource balancing, computing
models, predictive modeling and synchronization in NVEs. This paper gathers and
classifies them, summarizing their advantages and disadvantages, using a new
way of classification. With the current increase of the number of NVEs and the
multiple solutions proposed so far, this work aims to become a useful tool and
a starting point not only for future researchers in this field but also for
those who are new in NVEs development, in which guaranteeing a good users' QoE
is essential.","['Juan González', 'Fernando Boronat', 'Almanzor Sapena', 'Javier Pastor']",2021-02-19T10:32:06Z,http://arxiv.org/abs/2102.09847v2,"['cs.NI', 'cs.HC', 'cs.MM']","virtual reality systems,networked virtual environments,immersion,interactions,consistency,synchronization,network architectures,data distribution,resource balancing"
"AI-Augmented Behavior Analysis for Children with Developmental
  Disabilities: Building Towards Precision Treatment","Autism spectrum disorder is a developmental disorder characterized by
significant social, communication, and behavioral challenges. Individuals
diagnosed with autism, intellectual, and developmental disabilities (AUIDD)
typically require long-term care and targeted treatment and teaching. Effective
treatment of AUIDD relies on efficient and careful behavioral observations done
by trained applied behavioral analysts (ABAs). However, this process
overburdens ABAs by requiring the clinicians to collect and analyze data,
identify the problem behaviors, conduct pattern analysis to categorize and
predict categorical outcomes, hypothesize responsiveness to treatments, and
detect the effects of treatment plans. Successful integration of digital
technologies into clinical decision-making pipelines and the advancements in
automated decision-making using Artificial Intelligence (AI) algorithms
highlights the importance of augmenting teaching and treatments using novel
algorithms and high-fidelity sensors. In this article, we present an
AI-Augmented Learning and Applied Behavior Analytics (AI-ABA) platform to
provide personalized treatment and learning plans to AUIDD individuals. By
defining systematic experiments along with automated data collection and
analysis, AI-ABA can promote self-regulative behavior using reinforcement-based
augmented or virtual reality and other mobile platforms. Thus, AI-ABA could
assist clinicians to focus on making precise data-driven decisions and increase
the quality of individualized interventions for individuals with AUIDD.","['Shadi Ghafghazi', 'Amarie Carnett', 'Leslie Neely', 'Arun Das', 'Paul Rad']",2021-02-21T16:15:40Z,http://arxiv.org/abs/2102.10635v2,"['cs.CY', 'cs.AI']","behavior analysis,developmental disabilities,autism spectrum disorder,applied behavioral analysts,Artificial Intelligence,machine learning,treatment plans,digital technologies,personalized treatment,behavioral observations"
"Defining Preferred and Natural Robot Motions in Immersive Telepresence
  from a First-Person Perspective","This paper presents some early work and future plans regarding how the
autonomous motions of a telepresence robot affect a person embodied in the
robot through a head-mounted display. We consider the preferences, comfort, and
the perceived naturalness of aspects of piecewise linear paths compared to the
same aspects on a smooth path. In a user study, thirty-six subjects (eighteen
females) watched panoramic videos of three different paths through a simulated
museum in virtual reality and responded to questionnaires regarding each path.
We found that comfort had a strong effect on path preference, and that the
subjective feeling of naturalness also had a strong effect on path preference,
even though people consider different things as natural. We describe a
categorization of the responses regarding the naturalness of the robot's motion
and provide a recommendation on how this can be applied more broadly. Although
immersive robotic telepresence is increasingly being used for remote education,
clinical care, and to assist people with disabilities or mobility
complications, the full potential of this technology is limited by issues
related to user experience. Our work addresses these shortcomings and will
enable the future personalization of telepresence experiences for the
improvement of overall remote communication and the enhancement of the feeling
of presence in a remote location.","['Katherine J. Mimnaugh', 'Markku Suomalainen', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-02-25T07:40:23Z,http://arxiv.org/abs/2102.12719v1,['cs.RO'],"immersive telepresence,robot motions,autonomous motions,telepresence robot,head-mounted display,piecewise linear paths,smooth path,user study,virtual reality,naturalness"
"Surgical Visual Domain Adaptation: Results from the MICCAI 2020
  SurgVisDom Challenge","Surgical data science is revolutionizing minimally invasive surgery by
enabling context-aware applications. However, many challenges exist around
surgical data (and health data, more generally) needed to develop context-aware
models. This work - presented as part of the Endoscopic Vision (EndoVis)
challenge at the Medical Image Computing and Computer Assisted Intervention
(MICCAI) 2020 conference - seeks to explore the potential for visual domain
adaptation in surgery to overcome data privacy concerns. In particular, we
propose to use video from virtual reality (VR) simulations of surgical
exercises in robotic-assisted surgery to develop algorithms to recognize tasks
in a clinical-like setting. We present the performance of the different
approaches to solve visual domain adaptation developed by challenge
participants. Our analysis shows that the presented models were unable to learn
meaningful motion based features form VR data alone, but did significantly
better when small amount of clinical-like data was also made available. Based
on these results, we discuss promising methods and further work to address the
problem of visual domain adaptation in surgical data science. We also release
the challenge dataset publicly at https://www.synapse.org/surgvisdom2020.","['Aneeq Zia', 'Kiran Bhattacharyya', 'Xi Liu', 'Ziheng Wang', 'Satoshi Kondo', 'Emanuele Colleoni', 'Beatrice van Amsterdam', 'Razeen Hussain', 'Raabid Hussain', 'Lena Maier-Hein', 'Danail Stoyanov', 'Stefanie Speidel', 'Anthony Jarc']",2021-02-26T18:45:28Z,http://arxiv.org/abs/2102.13644v1,['cs.CV'],"minimally invasive surgery,surgical data science,context-aware models,Endoscopic Vision,Medical Image Computing and Computer Assisted Intervention,visual domain adaptation,robotic-assisted surgery,virtual reality,motion based features,challenge dataset"
"Measuring Presence in Augmented Reality Environments: Design and a First
  Test of a Questionnaire","Augmented Reality (AR) enriches a user's real environment by adding spatially
aligned virtual objects (3D models, 2D textures, textual annotations, etc) by
means of special display technologies. These are either worn on the body or
placed in the working environment. From a technical point of view, AR faces
three major challenges: (1) to generate a high quality rendering, (2) to
precisely register (in position and orientation) the virtual objects (VOs) with
the real environment, and (3) to do so in interactive real-time (Regenbrecht,
Wagner, and Baratoff, 2002). The goal is to create the impression that the VOs
are part of the real environment. Therefore, and similar to definitions of
virtual reality (Steuer, 1992), it makes sense to define AR from a
psychological point of view: Augmented Reality conveys the impression that VOs
are present in the real environment. In order to evaluate how well this goal is
reached, a psychological measurement of this type of presence is necessary. In
the following, we will describe technological features of AR systems that make
a special questionnaire version necessary, describe our approach to the
questionnaire development, and the data collection strategy. Finally we will
present first results of the application of the questionnaire in a recent study
with 385 participants.","['Holger Regenbrecht', 'Thomas Schubert']",2021-03-04T04:46:19Z,http://arxiv.org/abs/2103.02831v1,['cs.HC'],"Augmented Reality,Virtual Objects,Rendering,Registration,Real-Time,Presence,Questionnaire,Measurement,Data Collection,Study"
"Light Field Image Coding Using VVC standard and View Synthesis based on
  Dual Discriminator GAN","Light field (LF) technology is considered as a promising way for providing a
high-quality virtual reality (VR) content. However, such an imaging technology
produces a large amount of data requiring efficient LF image compression
solutions. In this paper, we propose a LF image coding method based on a view
synthesis and view quality enhancement techniques. Instead of transmitting all
the LF views, only a sparse set of reference views are encoded and transmitted,
while the remaining views are synthesized at the decoder side. The transmitted
views are encoded using the versatile video coding (VVC) standard and are used
as reference views to synthesize the dropped views. The selection of
non-reference dropped views is performed using a rate-distortion optimization
based on the VVC temporal scalability. The dropped views are reconstructed
using the LF dual discriminator GAN (LF-D2GAN) model. In addition, to ensure
that the quality of the views is consistent, at the decoder, a quality
enhancement procedure is performed on the reconstructed views allowing smooth
navigation across views. Experimental results show that the proposed method
provides high coding performance and overcomes the state-of-the-art LF image
compression methods by -36.22% in terms of BD-BR and 1.35 dB in BD-PSNR. The
web page of this work is available at
https://naderbakir79.github.io/LFD2GAN.html.","['Nader Bakir', 'Wassim Hamidouche', 'Sid Ahmed Fezza', 'Khouloud Samrouth', 'Olivier Deforges']",2021-03-06T22:01:41Z,http://arxiv.org/abs/2103.04201v1,['eess.IV'],"Light field,LF image coding,View synthesis,VVC standard,Dual discriminator GAN,VR content,Data compression,Rate-distortion optimization,Temporal scalability,Quality enhancement"
"How Privacy-Preserving are Line Clouds? Recovering Scene Details from 3D
  Lines","Visual localization is the problem of estimating the camera pose of a given
image with respect to a known scene. Visual localization algorithms are a
fundamental building block in advanced computer vision applications, including
Mixed and Virtual Reality systems. Many algorithms used in practice represent
the scene through a Structure-from-Motion (SfM) point cloud and use 2D-3D
matches between a query image and the 3D points for camera pose estimation. As
recently shown, image details can be accurately recovered from SfM point clouds
by translating renderings of the sparse point clouds to images. To address the
resulting potential privacy risks for user-generated content, it was recently
proposed to lift point clouds to line clouds by replacing 3D points by randomly
oriented 3D lines passing through these points. The resulting representation is
unintelligible to humans and effectively prevents point cloud-to-image
translation. This paper shows that a significant amount of information about
the 3D scene geometry is preserved in these line clouds, allowing us to
(approximately) recover the 3D point positions and thus to (approximately)
recover image content. Our approach is based on the observation that the
closest points between lines can yield a good approximation to the original 3D
points. Code is available at https://github.com/kunalchelani/Line2Point.","['Kunal Chelani', 'Fredrik Kahl', 'Torsten Sattler']",2021-03-08T21:32:43Z,http://arxiv.org/abs/2103.05086v1,['cs.CV'],"privacy-preserving,line clouds,3D,scene details,visual localization,camera pose estimation,Structure-from-Motion,point cloud,Mixed Reality,Virtual Reality"
"Remote Virtual Showdown: A Collaborative Virtual Reality Game for People
  with Visual Impairments","Many researchers have developed VR systems for people with visual impairments
by using various audio feedback techniques. However, there has been much less
study of collaborative VR systems in which people with visual impairments and
people with able-body can participate together. Therefore, we developed a VR
showdown game which is similar to a real Showdown game in which two players can
play together in the same virtual environment. We incorporate auditory distance
perception using the HRTF (Head Related Transform Function) based on a spatial
position in VR. We developed two modes in the showdown game. One is the PVA
(Player vs. Agent) mode in which people with visual impairments can play alone
and the PVP (Player vs. Player) mode in which people with visual impairments
can play with another player in the network environment. We conducted our user
studies by comparing the performances of people with visual impairments and
people with able-body. The user study results show that people with visual
impairments won 67.6% of the games when competing against people with
able-body. This paper reports an example of a collaborative VR system for
people with visual impairments and also design guideline for developing VR
systems for people with visual impairments.","['Hojun Aan', 'Sangsun Han', 'Hyeonkyu Kim', 'Jimoon Kim', 'Pilhyoun Yoon', 'Kibum Kim']",2021-03-30T08:20:54Z,http://arxiv.org/abs/2103.16153v2,['cs.HC'],"Collaborative VR systems,Visual impairments,Audio feedback techniques,Auditory distance perception,HRTF,Virtual environment,PVA mode,PVP mode,User studies"
"Hybrid Policy Learning for Energy-Latency Tradeoff in MEC-Assisted VR
  Video Service","Virtual reality (VR) is promising to fundamentally transform a broad spectrum
of industry sectors and the way humans interact with virtual content. However,
despite unprecedented progress, current networking and computing
infrastructures are incompetent to unlock VR's full potential. In this paper,
we consider delivering the wireless multi-tile VR video service over a mobile
edge computing (MEC) network. The primary goal is to minimize the system
latency/energy consumption and to arrive at a tradeoff thereof. To this end, we
first cast the time-varying view popularity as a model-free Markov chain to
effectively capture its dynamic characteristics. After jointly assessing the
caching and computing capacities on both the MEC server and the VR playback
device, a hybrid policy is then implemented to coordinate the dynamic caching
replacement and the deterministic offloading, so as to fully utilize the system
resources. The underlying multi-objective problem is reformulated as a
partially observable Markov decision process, and a deep deterministic policy
gradient algorithm is proposed to iteratively learn its solution, where a long
short-term memory neural network is embedded to continuously predict the
dynamics of the unobservable popularity. Simulation results demonstrate the
superiority of the proposed scheme in achieving a trade-off between the energy
efficiency and the latency reduction over the baseline methods.","['Chong Zheng', 'Shengheng Liu', 'Yongming Huang', 'Luxi Yang']",2021-04-02T13:17:11Z,http://arxiv.org/abs/2104.01036v1,"['cs.NI', 'cs.LG']","MEC,Virtual reality,Energy consumption,Latency,Tradeoff,Markov chain,Caching,Deep deterministic policy gradient algorithm,Neural network"
Beaming Displays,"Existing near-eye display designs struggle to balance between multiple
trade-offs such as form factor, weight, computational requirements, and battery
life. These design trade-offs are major obstacles on the path towards an
all-day usable near-eye display. In this work, we address these trade-offs by,
paradoxically, \textit{removing the display} from near-eye displays. We present
the beaming displays, a new type of near-eye display system that uses a
projector and an all passive wearable headset. We modify an off-the-shelf
projector with additional lenses. We install such a projector to the
environment to beam images from a distance to a passive wearable headset. The
beaming projection system tracks the current position of a wearable headset to
project distortion-free images with correct perspectives. In our system, a
wearable headset guides the beamed images to a user's retina, which are then
perceived as an augmented scene within a user's field of view. In addition to
providing the system design of the beaming display, we provide a physical
prototype and show that the beaming display can provide resolutions as high as
consumer-level near-eye displays. We also discuss the different aspects of the
design space for our proposal.","['Yuta Itoh', 'Takumi Kaminokado', 'Kaan Aksit']",2021-04-08T14:24:39Z,http://arxiv.org/abs/2104.03800v1,"['cs.HC', 'cs.GR']","beaming displays,near-eye display,projector,wearable headset,resolution,system design,trade-offs,battery life,computational requirements,form factor"
"A context-aware pedestrian trajectory prediction framework for automated
  vehicles","With the unprecedented shift towards automated urban environments in recent
years, a new paradigm is required to study pedestrian behaviour. Studying
pedestrian behaviour in futuristic scenarios requires modern data sources that
consider both the Automated Vehicle (AV) and pedestrian perspectives. Current
open datasets on AVs predominantly fail to account for the latter, as they do
not include an adequate number of events and associated details that involve
pedestrian and vehicle interactions. To address this issue, we propose using
Virtual Reality (VR) data as a complementary resource to current datasets,
which can be designed to measure pedestrian behaviour under specific
conditions. In this research, we focus on the context-aware pedestrian
trajectory prediction framework for automated vehicles at mid-block
unsignalized crossings. For this purpose, we develop a novel multi-input
network of Long Short-Term Memory (LSTM) and fully connected dense layers. In
addition to past trajectories, the proposed framework incorporates pedestrian
head orientations and distance to the upcoming vehicles as sequential input
data. By merging the sequential data with contextual information of the
environment, we train a model to predict the future pedestrian trajectory. Our
results show that the prediction error and overfitting to the training data are
reduced by considering contextual information in the model. To analyze the
application of the methods to real AV data, the proposed framework is trained
and applied to pedestrian trajectory extracted from an open-access video
dataset. Finally, by implementing a game theory-based model interpretability
method, we provide detailed insights and propose recommendations to improve the
current automated vehicle sensing systems from a pedestrian-oriented point of
view.","['Arash Kalatian', 'Bilal Farooq']",2021-04-16T14:04:17Z,http://arxiv.org/abs/2104.08123v3,['cs.HC'],"context-aware,pedestrian trajectory prediction,automated vehicles,Virtual Reality,Long Short-Term Memory (LSTM),dense layers,overfitting,model interpretability,game theory"
Spatial Privacy-aware VR streaming,"Proactive tile-based virtual reality (VR) video streaming employs the current
tracking data of a user to predict future requested tiles, then renders and
delivers the predicted tiles before playback. Very recently, privacy protection
in proactive VR video streaming starts to raise concerns. However, existing
privacy protection may fail even with privacy-preserve federated learning. This
is because when the future requested tiles can be predicted accurately, the
user-behavior-related data can still be recovered from the predicted tiles. In
this paper, we consider how to protect privacy even with accurate predictors
and investigate the impact of privacy requirement on the quality of experience
(QoE). To this end, we first add extra \textit{camouflaged} tile requests to
the real tile requests and model the privacy requirement as the \textit{spatial
degree of privacy} (sDoP). By ensuring sDoP, the real tile requests can be
hidden and privacy can be protected. Then, we jointly optimize the durations
for prediction, computing, and transmitting, aimed at maximizing the
privacy-aware QoE given arbitrary predictor and configured resources. From the
obtained optimal closed-form solution, we find that the impacts of sDoP on the
QoE are two sides of the same coin. On the one side the increase of sDoP
improves the capability of communication and computing hence improves QoE. On
the other side it degrades the prediction performance hence degrades the QoE.
The overall impact depends on which factor dominates the QoE. Simulation with
two predictors on a real dataset verifies the analysis and shows that the
overall impact of sDoP is to improve the QoE.","['Xing Wei', 'Chenyang Yang']",2021-04-29T07:53:02Z,http://arxiv.org/abs/2104.14170v2,"['cs.MM', 'eess.IV', 'eess.SP']","privacy,VR streaming,tile-based,spatial privacy,virtual reality,federated learning,predictor,quality of experience(QoE),spatial degree of privacy(sDoP),computing"
"Evaluating Metrics for Standardized Benchmarking of Remote Presence
  Systems","To reduce the need for business-related air travel and its associated energy
consumption and carbon footprint, the U.S. Department of Energy's ARPA-E is
supporting a research project called SCOTTIE - Systematic Communication
Objectives and Telecommunications Technology Investigations and Evaluations.
SCOTTIE tests virtual and augmented reality platforms in a functional
comparison with face-to-face (FtF) interactions to derive travel replacement
thresholds for common industrial training scenarios. The primary goal of Study
1 is to match the communication effectiveness and learning outcomes obtained
from a FtF control using virtual reality (VR) training scenarios in which a
local expert with physical equipment trains a remote apprentice without
physical equipment immediately present. This application scenario is
commonplace in industrial settings where access to expensive equipment and
materials is limited and a number of apprentices must travel to a central
location in order to undergo training. Supplying an empirically validated
virtual training alternative constitutes a readily adoptable use-case for
businesses looking to reduce time and monetary expenditures associated with
travel. The technology used for three different virtual presence technologies
was strategically selected for feasibility, relatively low cost, business
relevance, and potential for impact through transition. The authors suggest
that the results of this study might generalize to the challenge of virtual
conferences.","['Charles Peasley', 'Rachel Dianiska', 'Emily Oldham', 'Nicholas Wilson', 'Stephen Gilbert', 'Peggy Wu', 'Brett Israelsen', 'James Oliver']",2021-05-04T21:36:53Z,http://arxiv.org/abs/2105.01772v1,"['cs.HC', 'cs.CY']","Standardized Benchmarking,Remote Presence Systems,ARPA-E,SCOTTIE,Virtual Reality,Augmented Reality,Communication Effectiveness,Learning Outcomes,Training Scenarios,Virtual Training"
"Comparing Field Trips, VR Experiences and Video Representations on
  Spatial Layout Learning in Complex Buildings","This study aimed to compare and investigate the efficacy of the real-world
experiences, immersive virtual reality (IVR) experiences, and video walkthrough
representations on layout-learning in a complex building. A quasi-experimental,
intervention, and delayed post-test research design was used among three
groups: real-world, IVR, and video walkthrough representation. A total of 41
first-year design students from architecture, and game design departments were
attended the study. Design students were selected as they already know how to
communicate graphically the layout of a building on paper. IVR and video
walkthrough groups experienced the representations of the building by
themselves, but real-world group experienced the building within a group as
imitating a design education field trip. After 10 days, a total of 26
participants out of 41 tested for spatial recall performance. Recall
performances were measured as an indicator of layout learning by analysing the
plan sketches drawn by the participants. Results showed IVR group recalled
significantly more spatial memories compared with the real-world and video
walkthrough representation groups. Real-world group performed the worst among
three groups. This result was interpreted to three conclusions. First, the
layout learning tends to be overly sensitive to distractions and lower levels
of distraction may lead to better levels of layout learning. Second, for the
domain of layout knowledge learning, a remarkably simple IVR model is enough.
Third, although real-world experiences give direct and richer sensory
information about the spaces, layout knowledge acquisition from the surrounding
environment requires psychologically active wayfinding decisions.","['Cetin Tuker', 'Togan Tong']",2021-05-05T10:35:19Z,http://arxiv.org/abs/2105.01968v1,['cs.HC'],"field trips,VR experiences,video representations,spatial layout learning,complex buildings,immersive virtual reality,design students,recall performance,plan sketches,layout knowledge acquisition"
"3D Displays: Their Evolution, Inherent Challenges & Future Perspectives","The popularity of 3D displays has risen drastically over the past few decades
but these displays are still merely a novelty compared to their true potential.
The development has mostly focused on Head Mounted Displays (HMD) development
for Virtual Reality and in general ignored non-HMD 3D displays. This is due to
the inherent difficulty in the creation of these displays and their
impracticability in general use due to cost, performance, and lack of
meaningful use cases. In fairness to the hardware manufacturers who have made
striking innovations in this field, there has been a dereliction of duty of
software developers and researchers in terms of developing software to best
utilize these displays.
  This paper will seek to identify what areas of future software development
could mitigate this dereliction. To achieve this goal, the paper will first
examine the current state of the art and perform a comparative analysis on
different types of 3D displays, from this analysis a clear researcher gap
exists in terms of software development for Light field displays which are the
current state of the art of non-HMD-based 3D displays.
  The paper will then outline six distinct areas where the context-awareness
concept will allow for non-HMD-based 3D displays in particular light field
displays that can not only compete but surpass their HMD-based brethren for
many specific use cases.","['Xingyu Pan', 'Xuanhui Xu', 'Soumyabrata Dev', 'Abraham G Campbell']",2021-05-18T09:34:25Z,http://arxiv.org/abs/2105.08390v1,['cs.HC'],"3D displays,Evolution,Challenges,Future perspectives,Head Mounted Displays,Virtual Reality,Software development,Light field displays,Context-awareness"
"Power-Efficient Wireless Streaming of Multi-Quality Tiled 360 VR Video
  in MIMO-OFDMA Systems","In this paper, we study the optimal wireless streaming of a multi-quality
tiled 360 virtual reality (VR) video from a multi-antenna server to multiple
single-antenna users in a multiple-input multiple-output (MIMO)-orthogonal
frequency division multiple access (OFDMA) system. In the scenario without user
transcoding, we jointly optimize beamforming and subcarrier, transmission
power, and rate allocation to minimize the total transmission power. This
problem is a challenging mixed discretecontinuous optimization problem. We
obtain a globally optimal solution for small multicast groups, an
asymptotically optimal solution for a large antenna array, and a suboptimal
solution for the general case. In the scenario with user transcoding, we
jointly optimize the quality level selection, beamforming, and subcarrier,
transmission power, and rate allocation to minimize the weighted sum of the
average total transmission power and the transcoding power. This problem is a
two-timescale mixed discrete-continuous optimization problem, which is even
more challenging than the problem for the scenario without user transcoding. We
obtain a globally optimal solution for small multicast groups, an
asymptotically optimal solution for a large antenna array, and a low-complexity
suboptimal solution for the general case. Finally, numerical results
demonstrate the significant gains of proposed solutions over the existing
solutions. significant gains of proposed solutions over the existing solutions.","['Chengjun Guo', 'Lingzhi Zhao', 'Ying Cui', 'Zhi Liu', 'Derrick Wing Kwan Ng']",2021-04-13T14:00:39Z,http://arxiv.org/abs/2105.09865v1,['eess.SP'],"Wireless streaming,Multi-quality,Tiled 360 VR video,MIMO,OFDMA Systems,Beamforming,Transmission power,Rate allocation,User transcoding,Antenna array"
Effects of VR Gaming and Game Genre on Player Experience,"With the increasing availability of modern virtual reality (VR) headsets, the
use and applications of VR technology for gaming purposes have become more
pervasive than ever. Despite the growing popularity of VR gaming, user studies
into how it might affect the player experience (PX) during the gameplay are
scarce. Accordingly, the current study investigated the effects of VR gaming
and game genre on PX. We compared PX metrics for two game genres, strategy
(more interactive) and racing (less interactive), across two gaming platforms,
VR and traditional desktop gaming. Participants were randomly assigned to one
of the gaming platforms, played both a strategy and racing game on their
corresponding platform, and provided PX ratings. Results revealed that,
regardless of the game genre, participants in the VR gaming condition
experienced a greater level of sense of presence than did those in the desktop
gaming condition. That said, results showed that the two gaming platforms did
not significantly differ from one another in PX ratings. As for the effect of
game genre, participants provided greater PX ratings for the strategy game than
for the racing game, regardless of whether the game was played on a VR headset
or desktop computer. Collectively, these results indicate that although VR
gaming affords a greater sense of presence in the game environment, this
increase in presence does not seem to translate into a more satisfactory PX
when playing either a strategy or racing game.","['Michael Carroll', 'Ethan Osborne', 'Caglar Yildirim']",2021-05-22T16:09:28Z,http://arxiv.org/abs/2105.10754v1,"['cs.HC', 'cs.MM']","VR gaming,game genre,player experience,PX metrics,strategy game,racing game,gaming platforms,sense of presence,VR headset,desktop gaming"
QoE Driven VR 360 Video Massive MIMO Transmission,"Massive multiple-input and multiple-output (MIMO) enables ultra-high
throughput and low latency for tile-based adaptive virtual reality (VR) 360
video transmission in wireless network. In this paper, we consider a massive
MIMO system where multiple users in a single-cell theater watch an identical VR
360 video. Based on tile prediction, base station (BS) deliveries the tiles in
predicted field of view (FoV) to users. By introducing practical supplementary
transmission for missing tiles and unacceptable VR sickness, we propose the
first stable transmission scheme for VR video. we formulate an integer
non-linear programming (INLP) problem to maximize users' average quality of
experience (QoE) score. Moreover, we derive the achievable spectral efficiency
(SE) expression of predictive tile groups and the approximately achievable SE
expression of missing tile groups, respectively. Analytically, the overall
throughput is related to the number of tile groups and the length of pilot
sequences. By exploiting the relationship between the structure of viewport
tiles and SE expression, we propose a multi-lattice multi-stream grouping
method aimed at improving the overall throughput for VR video transmission.
Moreover, we analyze the relationship between QoE objective and number of
predictive tile. We transform the original INLP problem into an integer linear
programming problem by setting the predictive tiles groups as some constants.
With variable relaxation and recovery, we obtain the optimal average QoE.
Extensive simulation results validate that the proposed algorithm effectively
improves QoE.","['Long Teng', 'Guangtao Zhai', 'Yongpeng Wu', 'Xiongkuo Min', 'Wenjun Zhang', 'Zhi Ding', 'Chengshang Xiao']",2021-06-15T14:08:35Z,http://arxiv.org/abs/2106.08165v1,"['cs.IT', 'eess.SP', 'math.IT']","QoE,VR,360 video,Massive MIMO,transmission,spectral efficiency,tile prediction,viewport tiles,throughput,INLP"
"Scale-Consistent Fusion: from Heterogeneous Local Sampling to Global
  Immersive Rendering","Image-based geometric modeling and novel view synthesis based on sparse,
large-baseline samplings are challenging but important tasks for emerging
multimedia applications such as virtual reality and immersive telepresence.
Existing methods fail to produce satisfactory results due to the limitation on
inferring reliable depth information over such challenging reference
conditions. With the popularization of commercial light field (LF) cameras,
capturing LF images (LFIs) is as convenient as taking regular photos, and
geometry information can be reliably inferred. This inspires us to use a sparse
set of LF captures to render high-quality novel views globally. However, fusion
of LF captures from multiple angles is challenging due to the scale
inconsistency caused by various capture settings. To overcome this challenge,
we propose a novel scale-consistent volume rescaling algorithm that robustly
aligns the disparity probability volumes (DPV) among different captures for
scale-consistent global geometry fusion. Based on the fused DPV projected to
the target camera frustum, novel learning-based modules have been proposed
(i.e., the attention-guided multi-scale residual fusion module, and the
disparity field guided deep re-regularization module) which comprehensively
regularize noisy observations from heterogeneous captures for high-quality
rendering of novel LFIs. Both quantitative and qualitative experiments over the
Stanford Lytro Multi-view LF dataset show that the proposed method outperforms
state-of-the-art methods significantly under different experiment settings for
disparity inference and LF synthesis.","['Wenpeng Xing', 'Jie Chen', 'Zaifeng Yang', 'Qiang Wang']",2021-06-17T14:27:08Z,http://arxiv.org/abs/2106.09548v1,['cs.CV'],"geometric modeling,view synthesis,sparse sampling,light field cameras,disparity probability volumes,volume rescaling algorithm,global geometry fusion,deep learning,multi-view dataset,novel view synthesis"
Latency-aware and Survivable Mapping of VNFs in 5G Network Edge Cloud,"Network Functions Virtualization (NFV) and Multi-access Edge Computing (MEC)
play crucial roles in 5G networks for dynamically provisioning diverse
communication services with heterogeneous service requirements. In particular,
while NFV improves flexibility and scalability by softwarizing physical network
functions as Virtual Network Functions (VNFs), MEC enables to provide
delay-sensitive/time-critical services by moving computing facilities to the
network edge. However, these new paradigms introduce challenges in terms of
latency, availability, and resource allocation. In this paper, we first explore
MEC cloud facility location selection and then latency-aware placement of VNFs
in different selected locations of NFV enabled MEC cloud facilities in order to
meet the ultra-low latency requirements of different applications (e.g.,
Tactile Internet, virtual reality, and mission-critical applications).
Furthermore, we also aim to guarantee the survivability of VNFs and an edge
server against failures in resource limited MEC cloud facility due to software
bugs, configuration faults, etc. To this end, we formulate the problem of
latency-aware and survivable mapping of VNFs in different MEC cloud facilities
as an Integer Linear Programming (ILP) to minimize the overall service
provisioning cost, and show that the problem is NP-hard. Owing to the high
computational complexity of solving the ILP, we propose a simulated annealing
based heuristic algorithm to obtain near-optimal solution in polynomial time.
With extensive simulations, we show the effectiveness of our proposed solution
in a real-world network topology, which performs close to the optimal solution.","['Prabhu Kaliyammal Thiruvasagam', 'Abhishek Chakraborty', 'C. Siva Ram Murthy']",2021-06-17T23:57:08Z,http://arxiv.org/abs/2106.09849v1,['cs.NI'],"NFV,MEC,VNFs,5G,latency-aware,survivable mapping,edge cloud,ILP,simulated annealing,network topology"
"SeqNetVLAD vs PointNetVLAD: Image Sequence vs 3D Point Clouds for
  Day-Night Place Recognition","Place Recognition is a crucial capability for mobile robot localization and
navigation. Image-based or Visual Place Recognition (VPR) is a challenging
problem as scene appearance and camera viewpoint can change significantly when
places are revisited. Recent VPR methods based on ``sequential
representations'' have shown promising results as compared to traditional
sequence score aggregation or single image based techniques. In parallel to
these endeavors, 3D point clouds based place recognition is also being explored
following the advances in deep learning based point cloud processing. However,
a key question remains: is an explicit 3D structure based place representation
always superior to an implicit ``spatial'' representation based on sequence of
RGB images which can inherently learn scene structure. In this extended
abstract, we attempt to compare these two types of methods by considering a
similar ``metric span'' to represent places. We compare a 3D point cloud based
method (PointNetVLAD) with image sequence based methods (SeqNet and others) and
showcase that image sequence based techniques approach, and can even surpass,
the performance achieved by point cloud based methods for a given metric span.
These performance variations can be attributed to differences in data richness
of input sensors as well as data accumulation strategies for a mobile robot.
While a perfect apple-to-apple comparison may not be feasible for these two
different modalities, the presented comparison takes a step in the direction of
answering deeper questions regarding spatial representations, relevant to
several applications like Autonomous Driving and Augmented/Virtual Reality.
Source code available publicly https://github.com/oravus/seqNet.","['Sourav Garg', 'Michael Milford']",2021-06-22T02:05:32Z,http://arxiv.org/abs/2106.11481v1,"['cs.CV', 'cs.AI', 'cs.IR', 'cs.LG', 'cs.RO']","SeqNetVLAD,PointNetVLAD,Image sequence,3D point clouds,Place recognition,Visual place recognition,Deep learning,RGB images,Spatial representation,Metric span"
Directions for 3D User Interface Research from Consumer VR Games,"With the continuing development of affordable immersive virtual reality (VR)
systems, there is now a growing market for consumer content. The current form
of consumer systems is not dissimilar to the lab-based VR systems of the past
30 years: the primary input mechanism is a head-tracked display and one or two
tracked hands with buttons and joysticks on hand-held controllers. Over those
30 years, a very diverse academic literature has emerged that covers design and
ergonomics of 3D user interfaces (3DUIs). However, the growing consumer market
has engaged a very broad range of creatives that have built a very diverse set
of designs. Sometimes these designs adopt findings from the academic
literature, but other times they experiment with completely novel or
counter-intuitive mechanisms. In this paper and its online adjunct, we report
on novel 3DUI design patterns that are interesting from both design and
research perspectives: they are highly novel, potentially broadly re-usable
and/or suggest interesting avenues for evaluation. The supplemental material,
which is a living document, is a crowd-sourced repository of interesting
patterns. This paper is a curated snapshot of those patterns that were
considered to be the most fruitful for further elaboration.","['Anthony Steed', 'Tuukka M. Takala', 'Daniel Archer', 'Wallace Lages', 'Robert W. Lindeman']",2021-06-23T19:05:01Z,http://arxiv.org/abs/2106.12633v1,"['cs.HC', 'cs.GR']","immersive virtual reality,consumer content,VR systems,3D user interfaces,design,ergonomics,creatives,design patterns,evaluation,repository"
FOVQA: Blind Foveated Video Quality Assessment,"Previous blind or No Reference (NR) video quality assessment (VQA) models
largely rely on features drawn from natural scene statistics (NSS), but under
the assumption that the image statistics are stationary in the spatial domain.
Several of these models are quite successful on standard pictures. However, in
Virtual Reality (VR) applications, foveated video compression is regaining
attention, and the concept of space-variant quality assessment is of interest,
given the availability of increasingly high spatial and temporal resolution
contents and practical ways of measuring gaze direction. Distortions from
foveated video compression increase with increased eccentricity, implying that
the natural scene statistics are space-variant. Towards advancing the
development of foveated compression / streaming algorithms, we have devised a
no-reference (NR) foveated video quality assessment model, called FOVQA, which
is based on new models of space-variant natural scene statistics (NSS) and
natural video statistics (NVS). Specifically, we deploy a space-variant
generalized Gaussian distribution (SV-GGD) model and a space-variant
asynchronous generalized Gaussian distribution (SV-AGGD) model of mean
subtracted contrast normalized (MSCN) coefficients and products of neighboring
MSCN coefficients, respectively. We devise a foveated video quality predictor
that extracts radial basis features, and other features that capture
perceptually annoying rapid quality fall-offs. We find that FOVQA achieves
state-of-the-art (SOTA) performance on the new 2D LIVE-FBT-FCVR database, as
compared with other leading FIQA / VQA models. we have made our implementation
of FOVQA available at: http://live.ece.utexas.edu/research/Quality/FOVQA.zip.","['Yize Jin', 'Anjul Patney', 'Richard Webb', 'Alan Bovik']",2021-06-24T21:38:22Z,http://arxiv.org/abs/2106.13328v1,"['eess.IV', 'cs.CV']","Blind,Foveated,Video Quality Assessment,NR,Virtual Reality,Compression,Space-variant,Natural Scene Statistics,Foveated Compression,Streaming"
"Passing a Non-verbal Turing Test: Evaluating Gesture Animations
  Generated from Speech","People communicate using both speech and non-verbal signals such as gestures,
face expression or body pose. Non-verbal signals impact the meaning of the
spoken utterance in an abundance of ways. An absence of non-verbal signals
impoverishes the process of communication. Yet, when users are represented as
avatars, it is difficult to translate non-verbal signals along with the speech
into the virtual world without specialized motion-capture hardware. In this
paper, we propose a novel, data-driven technique for generating gestures
directly from speech. Our approach is based on the application of Generative
Adversarial Neural Networks (GANs) to model the correlation rather than
causation between speech and gestures. This approach approximates neuroscience
findings on how non-verbal communication and speech are correlated. We create a
large dataset which consists of speech and corresponding gestures in a 3D human
pose format from which our model learns the speaker-specific correlation. We
evaluate the proposed technique in a user study that is inspired by the Turing
test. For the study, we animate the generated gestures on a virtual character.
We find that users are not able to distinguish between the generated and the
recorded gestures. Moreover, users are able to identify our synthesized
gestures as related or not related to a given utterance. Code and videos are
available at https://github.com/mrebol/Gestures-From-Speech","['Manuel Rebol', 'Christian Gütl', 'Krzysztof Pietroszek']",2021-07-01T19:38:43Z,http://arxiv.org/abs/2107.00712v2,['cs.CV'],"gesture animations,non-verbal signals,speech,Generative Adversarial Neural Networks (GANs),neuroscience,communication,motion-capture hardware,user study,Turing test,3D human pose format"
Telelife: The Future of Remote Living,"In recent years, everyday activities such as work and socialization have
steadily shifted to more remote and virtual settings. With the COVID-19
pandemic, the switch from physical to virtual has been accelerated, which has
substantially affected various aspects of our lives, including business,
education, commerce, healthcare, and personal life. This rapid and large-scale
switch from in-person to remote interactions has revealed that our current
technologies lack functionality and are limited in their ability to recreate
interpersonal interactions. To help address these limitations in the future, we
introduce ""Telelife,"" a vision for the near future that depicts the potential
means to improve remote living better aligned with how we interact, live and
work in the physical world. Telelife encompasses novel synergies of
technologies and concepts such as digital twins, virtual prototyping, and
attention and context-aware user interfaces with innovative hardware that can
support ultrarealistic graphics, user state detection, and more. These ideas
will guide the transformation of our daily lives and routines soon, targeting
the year 2035. In addition, we identify opportunities across high-impact
applications in domains related to this vision of Telelife. Along with a recent
survey of relevant fields such as human-computer interaction, pervasive
computing, and virtual reality, the directions outlined in this paper will
guide future research on remote living.","['Jason Orlosky', 'Misha Sra', 'Kenan BektaŁE, 'Huaishu Peng', 'Jeeeun Kim', ""Nataliya Kos'myna"", 'Tobias Hollerer', 'Anthony Steed', 'Kiyoshi Kiyokawa', 'Kaan Akşit']",2021-07-07T01:05:27Z,http://arxiv.org/abs/2107.02965v1,['cs.HC'],"remote living,virtual settings,COVID-19 pandemic,technologies,interpersonal interactions,Telelife,digital twins,virtual prototyping,user interfaces,ultrarealistic graphics"
"Effects of Task Type and Wall Appearance on Collision Behavior in
  Virtual Environments","Driven by the games community, virtual reality setups have lately evolved
into affordable and consumer-ready mobile headsets. However, despite these
promising improvements, it remains challenging to convey immersive and engaging
VR games as players are usually limited to experience the virtual world by
vision and hearing only. One prominent example of such open challenges is the
disparity between the real surroundings and the virtual environment. As virtual
obstacles usually do not have a physical counterpart, players might walk
through walls enclosing the level. Thus, past research mainly focussed on
multisensory collision feedback to deter players from ignoring obstacles.
However, the underlying causative reasons for such unwanted behavior have
mostly remained unclear.
  Our work investigates how task types and wall appearances influence the
players' incentives to walk through virtual walls. Therefore, we conducted a
user study, confronting the participants with different task motivations and
walls of varying opacity and realism. Our evaluation reveals that players
generally adhere to realistic behavior, as long as the experience feels
interesting and diverse. Furthermore, we found that opaque walls excel in
deterring subjects from cutting short, whereas different degrees of realism had
no significant influence on walking trajectories. Finally, we use collected
player feedback to discuss individual reasons for the observed behavior.","['Sebastian Cmentowski', 'Jens Krüger']",2021-07-18T13:07:40Z,http://arxiv.org/abs/2107.08439v2,['cs.HC'],"virtual reality,collision behavior,task type,wall appearance,immersive experience,user study,multisensory feedback,obstacle avoidance,player behavior,realism"
Dynamic Portal Occlusion for Precomputed Interactive Sound Propagation,"An immersive audio-visual experience in games and virtual reality requires
fast calculation of diffraction-based acoustic effects. To maintain
plausibility, the effects must retain spatial smoothness on source and listener
motion within geometrically complex scenes. Precomputed wave-based techniques
can render such results at low runtime CPU cost, but remain limited to static
scenes. Modeling the occlusion effect of dynamic portals such as doors present
an unresolved challenge to maintain audio-visual consistency. We present a fast
solution implementable as a drop-in extension to existing precomputed systems.
Key is a novel portal-search method that leverages precomputed propagation
delay and direction data to find portals intervening the diffracted shortest
path connecting dynamic source and listener at runtime. The method scales
linearly with number of portals in worst case, far cheaper than explicit global
path search that scales with scene area. We discuss culling techniques to
accelerate further. The search algorithm is combined with geometric-acoustic
approximations to model the additional direct and indirect energy loss from
intervening portals depending on their dynamic closure state. We demonstrate
plausible audio-visual animations within our system integrated with Unreal
Engine 4 (TM) and AudioKinetic Wwise (TM).",['Nikunj Raghuvanshi'],2021-07-24T07:04:53Z,http://arxiv.org/abs/2107.11548v2,"['cs.SD', 'cs.GR', 'eess.AS', 'I.3.7; H.5.5']","Interactive sound propagation,Dynamic portal occlusion,Precomputed techniques,Diffraction-based acoustic effects,Spatial smoothness,Geometrically complex scenes,Propagation delay,Portal-search method,Global path search,Culling techniques"
Object Wake-up: 3D Object Rigging from a Single Image,"Given a single image of a general object such as a chair, could we also
restore its articulated 3D shape similar to human modeling, so as to animate
its plausible articulations and diverse motions? This is an interesting new
question that may have numerous downstream augmented reality and virtual
reality applications. Comparing with previous efforts on object manipulation,
our work goes beyond 2D manipulation and rigid deformation, and involves
articulated manipulation. To achieve this goal, we propose an automated
approach to build such 3D generic objects from single images and embed
articulated skeletons in them. Specifically, our framework starts by
reconstructing the 3D object from an input image. Afterwards, to extract
skeletons for generic 3D objects, we develop a novel skeleton prediction method
with a multi-head structure for skeleton probability field estimation by
utilizing the deep implicit functions. A dataset of generic 3D objects with
ground-truth annotated skeletons is collected. Empirically our approach is
demonstrated with satisfactory performance on public datasets as well as our
in-house dataset; our results surpass those of the state-of-the-arts by a
noticeable margin on both 3D reconstruction and skeleton prediction.","['Ji Yang', 'Xinxin Zuo', 'Sen Wang', 'Zhenbo Yu', 'Xingyu Li', 'Bingbing Ni', 'Minglun Gong', 'Li Cheng']",2021-08-05T16:20:12Z,http://arxiv.org/abs/2108.02708v3,['cs.CV'],"Object wake-up,3D object rigging,single image,articulated 3D shape,object manipulation,rigid deformation,articulated manipulation,automated approach,generic objects,articulated skeletons,skeleton prediction,deep implicit functions,dataset,ground-truth,3D reconstruction."
"BEHAVIOR: Benchmark for Everyday Household Activities in Virtual,
  Interactive, and Ecological Environments","We introduce BEHAVIOR, a benchmark for embodied AI with 100 activities in
simulation, spanning a range of everyday household chores such as cleaning,
maintenance, and food preparation. These activities are designed to be
realistic, diverse, and complex, aiming to reproduce the challenges that agents
must face in the real world. Building such a benchmark poses three fundamental
difficulties for each activity: definition (it can differ by time, place, or
person), instantiation in a simulator, and evaluation. BEHAVIOR addresses these
with three innovations. First, we propose an object-centric, predicate
logic-based description language for expressing an activity's initial and goal
conditions, enabling generation of diverse instances for any activity. Second,
we identify the simulator-agnostic features required by an underlying
environment to support BEHAVIOR, and demonstrate its realization in one such
simulator. Third, we introduce a set of metrics to measure task progress and
efficiency, absolute and relative to human demonstrators. We include 500 human
demonstrations in virtual reality (VR) to serve as the human ground truth. Our
experiments demonstrate that even state of the art embodied AI solutions
struggle with the level of realism, diversity, and complexity imposed by the
activities in our benchmark. We make BEHAVIOR publicly available at
behavior.stanford.edu to facilitate and calibrate the development of new
embodied AI solutions.","['Sanjana Srivastava', 'Chengshu Li', 'Michael Lingelbach', 'Roberto Martín-Martín', 'Fei Xia', 'Kent Vainio', 'Zheng Lian', 'Cem Gokmen', 'Shyamal Buch', 'C. Karen Liu', 'Silvio Savarese', 'Hyowon Gweon', 'Jiajun Wu', 'Li Fei-Fei']",2021-08-06T23:36:23Z,http://arxiv.org/abs/2108.03332v1,"['cs.RO', 'cs.AI', 'cs.CV']","benchmark,embodied AI,simulation,household chores,activities,object-centric,predicate logic,simulator,metrics,virtual reality"
Automatic Gaze Analysis: A Survey of Deep Learning based Approaches,"Eye gaze analysis is an important research problem in the field of Computer
Vision and Human-Computer Interaction. Even with notable progress in the last
10 years, automatic gaze analysis still remains challenging due to the
uniqueness of eye appearance, eye-head interplay, occlusion, image quality, and
illumination conditions. There are several open questions, including what are
the important cues to interpret gaze direction in an unconstrained environment
without prior knowledge and how to encode them in real-time. We review the
progress across a range of gaze analysis tasks and applications to elucidate
these fundamental questions, identify effective methods in gaze analysis, and
provide possible future directions. We analyze recent gaze estimation and
segmentation methods, especially in the unsupervised and weakly supervised
domain, based on their advantages and reported evaluation metrics. Our analysis
shows that the development of a robust and generic gaze analysis method still
needs to address real-world challenges such as unconstrained setup and learning
with less supervision. We conclude by discussing future research directions for
designing a real-world gaze analysis system that can propagate to other domains
including Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and
Human Computer Interaction (HCI). Project Page:
https://github.com/i-am-shreya/EyeGazeSurvey}{https://github.com/i-am-shreya/EyeGazeSurvey","['Shreya Ghosh', 'Abhinav Dhall', 'Munawar Hayat', 'Jarrod Knibbe', 'Qiang Ji']",2021-08-12T00:30:39Z,http://arxiv.org/abs/2108.05479v3,['cs.CV'],"Automatic gaze analysis,Deep learning,Eye gaze,Computer vision,Human-computer Interaction,Gaze estimation,Segmentation,Unsupervised learning,Weakly supervised learning,Real-time encoding"
"Gaze-Contingent Retinal Speckle Suppression for Perceptually-Matched
  Foveated Holographic Displays","Computer-generated holographic (CGH) displays show great potential and are
emerging as the next-generation displays for augmented and virtual reality, and
automotive heads-up displays. One of the critical problems harming the wide
adoption of such displays is the presence of speckle noise inherent to
holography, that compromises its quality by introducing perceptible artifacts.
Although speckle noise suppression has been an active research area, the
previous works have not considered the perceptual characteristics of the Human
Visual System (HVS), which receives the final displayed imagery. However, it is
well studied that the sensitivity of the HVS is not uniform across the visual
field, which has led to gaze-contingent rendering schemes for maximizing the
perceptual quality in various computer-generated imagery. Inspired by this, we
present the first method that reduces the ""perceived speckle noise"" by
integrating foveal and peripheral vision characteristics of the HVS, along with
the retinal point spread function, into the phase hologram computation.
Specifically, we introduce the anatomical and statistical retinal receptor
distribution into our computational hologram optimization, which places a
higher priority on reducing the perceived foveal speckle noise while being
adaptable to any individual's optical aberration on the retina. Our method
demonstrates superior perceptual quality on our emulated holographic display.
Our evaluations with objective measurements and subjective studies demonstrate
a significant reduction of the human perceived noise.","['Praneeth Chakravarthula', 'Zhan Zhang', 'Okan Tursun', 'Piotr Didyk', 'Qi Sun', 'Henry Fuchs']",2021-08-10T15:27:53Z,http://arxiv.org/abs/2108.06192v1,"['cs.HC', 'cs.MM', 'eess.IV', 'physics.optics']","Retinal speckle suppression,Foveated holographic displays,Computer-generated holography,Speckle noise suppression,Human Visual System (HVS),Gaze-contingent rendering,Retinal point spread function,Retinal receptor distribution,Computational hologram optimization,Perceived noise"
TUM-VIE: The TUM Stereo Visual-Inertial Event Dataset,"Event cameras are bio-inspired vision sensors which measure per pixel
brightness changes. They offer numerous benefits over traditional, frame-based
cameras, including low latency, high dynamic range, high temporal resolution
and low power consumption. Thus, these sensors are suited for robotics and
virtual reality applications. To foster the development of 3D perception and
navigation algorithms with event cameras, we present the TUM-VIE dataset. It
consists of a large variety of handheld and head-mounted sequences in indoor
and outdoor environments, including rapid motion during sports and high dynamic
range scenarios. The dataset contains stereo event data, stereo grayscale
frames at 20Hz as well as IMU data at 200Hz. Timestamps between all sensors are
synchronized in hardware. The event cameras contain a large sensor of 1280x720
pixels, which is significantly larger than the sensors used in existing stereo
event datasets (at least by a factor of ten). We provide ground truth poses
from a motion capture system at 120Hz during the beginning and end of each
sequence, which can be used for trajectory evaluation. TUM-VIE includes
challenging sequences where state-of-the art visual SLAM algorithms either fail
or result in large drift. Hence, our dataset can help to push the boundary of
future research on event-based visual-inertial perception algorithms.","['Simon Klenk', 'Jason Chui', 'Nikolaus Demmel', 'Daniel Cremers']",2021-08-16T19:53:56Z,http://arxiv.org/abs/2108.07329v1,['cs.CV'],"event camera,stereo data,IMU data,sensor synchronization,dataset,3D perception,navigation algorithms,motion capture system,visual SLAM,visual-inertial perception"
Stochastic Scene-Aware Motion Prediction,"A long-standing goal in computer vision is to capture, model, and
realistically synthesize human behavior. Specifically, by learning from data,
our goal is to enable virtual humans to navigate within cluttered indoor scenes
and naturally interact with objects. Such embodied behavior has applications in
virtual reality, computer games, and robotics, while synthesized behavior can
be used as a source of training data. This is challenging because real human
motion is diverse and adapts to the scene. For example, a person can sit or lie
on a sofa in many places and with varying styles. It is necessary to model this
diversity when synthesizing virtual humans that realistically perform
human-scene interactions. We present a novel data-driven, stochastic motion
synthesis method that models different styles of performing a given action with
a target object. Our method, called SAMP, for Scene-Aware Motion Prediction,
generalizes to target objects of various geometries while enabling the
character to navigate in cluttered scenes. To train our method, we collected
MoCap data covering various sitting, lying down, walking, and running styles.
We demonstrate our method on complex indoor scenes and achieve superior
performance compared to existing solutions. Our code and data are available for
research at https://samp.is.tue.mpg.de.","['Mohamed Hassan', 'Duygu Ceylan', 'Ruben Villegas', 'Jun Saito', 'Jimei Yang', 'Yi Zhou', 'Michael Black']",2021-08-18T17:56:17Z,http://arxiv.org/abs/2108.08284v1,['cs.CV'],"computer vision,human behavior,virtual humans,motion prediction,stochastic method,scene-aware,data-driven,MoCap data,indoor scenes,virtual reality"
Proceedings of the 1st International Workshop on Adaptive Cyber Defense,"The 1st International Workshop on Adaptive Cyber Defense was held as part of
the 2021 International Joint Conference on Artificial Intelligence. This
workshop was organized to share research that explores unique applications of
Artificial Intelligence (AI) and Machine Learning (ML) as foundational
capabilities for the pursuit of adaptive cyber defense. The cyber domain cannot
currently be reliably and effectively defended without extensive reliance on
human experts. Skilled cyber defenders are in short supply and often cannot
respond fast enough to cyber threats.
  Building on recent advances in AI and ML the Cyber defense research community
has been motivated to develop new dynamic and sustainable defenses through the
adoption of AI and ML techniques to both cyber and non-cyber settings. Bridging
critical gaps between AI and Cyber researchers and practitioners can accelerate
efforts to create semi-autonomous cyber defenses that can learn to recognize
and respond to cyber attacks or discover and mitigate weaknesses in cooperation
with other cyber operation systems and human experts. Furthermore, these
defenses are expected to be adaptive and able to evolve over time to thwart
changes in attacker behavior, changes in the system health and readiness, and
natural shifts in user behavior over time.
  The Workshop (held on August 19th and 20th 2021 in Montreal-themed virtual
reality) was comprised of technical presentations and a panel discussion
focused on open problems and potential research solutions. Workshop submissions
were peer reviewed by a panel of domain experts with a proceedings consisting
of 10 technical articles exploring challenging problems of critical importance
to national and global security. Participation in this workshop offered new
opportunities to stimulate research and innovation in the emerging domain of
adaptive and autonomous cyber defense.","['Damian Marriott', 'Kimberly Ferguson-Walter', 'Sunny Fugate', 'Marco Carvalho']",2021-08-19T03:41:48Z,http://arxiv.org/abs/2108.08476v1,"['cs.CR', 'cs.AI']","Artificial Intelligence,Machine Learning,Cyber Defense,AI,ML,Cyber Attacks,Adaptive Defense,Autonomous Defense,Cyber Operation Systems,Semi-autonomous Defense"
"Two-In-One: A Design Space for Mapping Unimanual Input into Bimanual
  Interactions in VR for Users with Limited Movement","Virtual Reality (VR) applications often require users to perform actions with
two hands when performing tasks and interacting with objects in virtual
environments. Although bimanual interactions in VR can resemble real-world
interactions -- thus increasing realism and improving immersion -- they can
also pose significant accessibility challenges to people with limited mobility,
such as for people who have full use of only one hand. An opportunity exists to
create accessible techniques that take advantage of users' abilities, but
designers currently lack structured tools to consider alternative approaches.
To begin filling this gap, we propose Two-in-One, a design space that
facilitates the creation of accessible methods for bimanual interactions in VR
from unimanual input. Our design space comprises two dimensions, bimanual
interactions and computer assistance, and we provide a detailed examination of
issues to consider when creating new unimanual input techniques that map to
bimanual interactions in VR. We used our design space to create three
interaction techniques that we subsequently implemented for a subset of
bimanual interactions and received user feedback through a video elicitation
study with 17 people with limited mobility. Our findings explore complex
tradeoffs associated with autonomy and agency and highlight the need for
additional settings and methods to make VR accessible to people with limited
mobility.","['Momona Yamagami', 'Sasa Junuzovic', 'Mar Gonzalez-Franco', 'Eyal Ofek', 'Edward Cutrell', 'John R. Porter', 'Andrew D. Wilson', 'Martez E. Mott']",2021-08-27T16:52:50Z,http://arxiv.org/abs/2108.12390v3,['cs.HC'],"design space,unimanual input,bimanual interactions,VR,limited movement,accessibility challenges,interaction techniques,user feedback,virtual reality"
Developing Virtual Reality Activities for the Astro 101 Class and Lab,"We report on our ongoing efforts to develop, implement, and test VR
activities for the introductory astronomy course and laboratory. Specifically,
we developed immersive activities for two challenging ""3D"" concepts: Moon
phases, and stellar parallax. For Moon phases, we built a simulation on the
Universe Sandbox platform and developed a set of activities that included
flying to different locations/viewpoints and moving the Moon by hand. This
allowed the students to create and experience the phases and the eclipses from
different vantage points, including seeing the phases of the Earth from the
Moon. We tested the efficacy of these activities on a large cohort (N=116) of
general education astronomy students, drawing on our experience with a previous
VR Moon phase exercise (Blanco (2019)). We were able to determine that VRbased
techniques perform comparably well against other teaching methods. We also
worked with the studentrun VR Club at San Diego State University, using the
Unity software engine to create a simulated space environment, where students
could kinesthetically explore stellar parallax - both by moving themselves and
by measuring parallactic motion while traveling in an orbit. The students then
derived a quantitative distance estimate using the parallax angle they measured
while in the virtual environment. Future plans include an immersive VR activity
to demonstrate the Hubble expansion and measure the age of the Universe. These
serve as examples of how one develops VR activities from the ground up, with
associated pitfalls and tradeoffs.","['Gur Windmiller', 'Philip Blanco', 'William F. Welsh']",2021-09-03T15:58:53Z,http://arxiv.org/abs/2109.01592v1,['physics.ed-ph'],"Virtual Reality,Astro 101,Astronomy,Universe Sandbox,Moon phases,Stellar parallax,VR activities,Unity software,Hubble expansion"
"GeneNet VR: Interactive visualization of large-scale biological networks
  using a standalone headset","Visualizations are an essential part of biomedical analysis result
interpretation. Often, interactive networks are used to visualize the data.
However, the high interconnectivity, and high dimensionality of the data often
results in information overload, making it hard to interpret the results. To
address the information overload problem, existing solutions typically either
use data reduction, reduced interactivity, or expensive hardware. We propose
using the affordable Oculus Quest Virtual Reality (VR) headset for interactive
visualization of large-scale biological networks.
  We present the design and implementation of our solution, GeneNet VR, and we
evaluate its scalability and usability using large gene-to-gene interaction
networks. We achieve the 72 FPS required by the Oculus performance guidelines
for the largest of our networks (2693 genes) using both a GPU and the Oculus
Quest standalone. We found from our interviews with biomedical researchers that
GeneNet VR is innovative, interesting, and easy to use for novice VR users.
  We believe affordable hardware like the Oculus Quest has a big potential for
biological data analysis. However, additional work is required to evaluate its
benefits to improve knowledge discovery for real data analysis use cases.
  GeneNet VR is open-sourced: https://github.com/kolibrid/GeneNet-VR. A video
demonstrating GeneNet VR used to explore large biological networks:
https://youtu.be/N4QDZiZqVNY.","['Álvaro Martínez Fernández', 'Lars Ailo Bongo', 'Edvard Pedersen']",2021-09-07T08:38:34Z,http://arxiv.org/abs/2109.02937v1,"['cs.GR', 'cs.HC', 'cs.SI']","biological networks,interactive visualization,Oculus Quest,gene-to-gene interaction,scalability,usability,GPU,knowledge discovery,data analysis"
CrowdDriven: A New Challenging Dataset for Outdoor Visual Localization,"Visual localization is the problem of estimating the position and orientation
from which a given image (or a sequence of images) is taken in a known scene.
It is an important part of a wide range of computer vision and robotics
applications, from self-driving cars to augmented/virtual reality systems.
Visual localization techniques should work reliably and robustly under a wide
range of conditions, including seasonal, weather, illumination and man-made
changes. Recent benchmarking efforts model this by providing images under
different conditions, and the community has made rapid progress on these
datasets since their inception. However, they are limited to a few geographical
regions and often recorded with a single device. We propose a new benchmark for
visual localization in outdoor scenes, using crowd-sourced data to cover a wide
range of geographical regions and camera devices with a focus on the failure
cases of current algorithms. Experiments with state-of-the-art localization
approaches show that our dataset is very challenging, with all evaluated
methods failing on its hardest parts. As part of the dataset release, we
provide the tooling used to generate it, enabling efficient and effective 2D
correspondence annotation to obtain reference poses.","['Ara Jafarzadeh', 'Manuel Lopez Antequera', 'Pau Gargallo', 'Yubin Kuang', 'Carl Toft', 'Fredrik Kahl', 'Torsten Sattler']",2021-09-09T19:25:48Z,http://arxiv.org/abs/2109.04527v1,['cs.CV'],"visual localization,dataset,outdoor scenes,crowd-sourced data,benchmark,geographical regions,camera devices,failure cases,localization approaches,reference poses"
Myopic Bike and Say Hi: Games for Empathizing with The Myopic,"Myopia is an eye condition that makes it difficult for people to focus on
faraway objects. It has become one of the most serious eye conditions worldwide
and negatively impacts the quality of life of those who suffer from it.
Although myopia is prevalent, many non-myopic people have misconceptions about
it and encounter challenges empathizing with myopia situations and those who
suffer from it. In this research, we developed two virtual reality (VR) games,
(1) Myopic Bike and (2) Say Hi, to provide a means for the non-myopic
population to experience the frustration and difficulties of myopic people. Our
two games simulate two inconvenient daily life scenarios (riding a bicycle and
greeting someone on the street) that myopic people encounter when not wearing
glasses. We evaluated four participants' game experiences through
questionnaires and semi-structured interviews. Overall, our two VR games can
create an engaging and non-judgmental experience for the non-myopic population
to better understand and empathize with those who suffer from myopia.","['Xiang Li', 'Xiaohang Tang', 'Xin Tong', 'Rakesh Patibanda', ""Florian 'Floyd' Mueller"", 'Hai-Ning Liang']",2021-09-11T14:34:46Z,http://arxiv.org/abs/2109.05292v4,['cs.HC'],"myopia,eye condition,virtual reality,VR games,myopic people,non-myopic,empathy,quality of life,misconceptions,daily life scenarios"
"Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations
  Using Deep Spatio-Temporal Graph CNNs","Several applications such as autonomous driving, augmented reality and
virtual reality require a precise prediction of the 3D human pose. Recently, a
new problem was introduced in the field to predict the 3D human poses from
observed 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN
model that predicts the future 3D skeleton poses in a single pass from the 2D
ones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction
between the skeleton joints by exploiting their spatial configuration. This is
being achieved by formulating the problem as a graph structure while learning a
suitable graph adjacency kernel. By the design, Skeleton-Graph predicts the
future 3D poses without divergence in the long-term, unlike prior works. We
also introduce a new metric that measures the divergence of predictions in the
long term. Our results show an FDE improvement of at least 27% and an ADE of 4%
on both the GTA-IM and PROX datasets respectively in comparison with prior
works. Also, we are 88% and 93% less divergence on the long-term motion
prediction in comparison with prior works on both GTA-IM and PROX datasets.
Code is available at https://github.com/abduallahmohamed/Skeleton-Graph.git","['Abduallah Mohamed', 'Huancheng Chen', 'Zhangyang Wang', 'Christian Claudel']",2021-09-21T15:33:40Z,http://arxiv.org/abs/2109.10257v2,"['cs.CV', 'cs.RO']","3D motion prediction,2D observations,deep learning,spatio-temporal,graph CNNs,skeleton poses,graph structure,adjacency kernel,long-term prediction,divergence."
"Accuracy Evaluation of Touch Tasks in Commodity Virtual and Augmented
  Reality Head-Mounted Displays","An increasing number of consumer-oriented head-mounted displays (HMD) for
augmented and virtual reality (AR/VR) are capable of finger and hand tracking.
We report on the accuracy of off-the-shelf VR and AR HMDs when used for
touch-based tasks such as pointing or drawing. Specifically, we report on the
finger tracking accuracy of the VR head-mounted displays Oculus Quest, Vive Pro
and the Leap Motion controller, when attached to a VR HMD, as well as the
finger tracking accuracy of the AR head-mounted displays Microsoft HoloLens 2
and Magic Leap. We present the results of two experiments in which we compare
the accuracy for absolute and relative pointing tasks using both human
participants and a robot. The results suggest that HTC Vive has a lower spatial
accuracy than the Oculus Quest and Leap Motion and that the Microsoft HoloLens
2 provides higher spatial accuracy than Magic Leap One. These findings can
serve as decision support for researchers and practitioners in choosing which
systems to use in the future.","['Daniel Schneider', 'Verena Biener', 'Alexander Otte', 'Travis Gesslein', 'Philipp Gagel', 'Cuauhtli Campos', 'Klen ČopiāEPucihar', 'Matjaž Kljun', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson', 'Jens Grubert']",2021-09-22T09:21:29Z,http://arxiv.org/abs/2109.10607v1,"['cs.HC', 'I.3.7']","commodity,virtual reality,augmented reality,head-mounted displays,finger tracking,tracking accuracy,pointing tasks,drawing tasks,spatial accuracy,decision support"
"GAN-based Reactive Motion Synthesis with Class-aware Discriminators for
  Human-human Interaction","Creating realistic characters that can react to the users' or another
character's movement can benefit computer graphics, games and virtual reality
hugely. However, synthesizing such reactive motions in human-human interactions
is a challenging task due to the many different ways two humans can interact.
While there are a number of successful researches in adapting the generative
adversarial network (GAN) in synthesizing single human actions, there are very
few on modelling human-human interactions. In this paper, we propose a
semi-supervised GAN system that synthesizes the reactive motion of a character
given the active motion from another character. Our key insights are two-fold.
First, to effectively encode the complicated spatial-temporal information of a
human motion, we empower the generator with a part-based long short-term memory
(LSTM) module, such that the temporal movement of different limbs can be
effectively modelled. We further include an attention module such that the
temporal significance of the interaction can be learned, which enhances the
temporal alignment of the active-reactive motion pair. Second, as the reactive
motion of different types of interactions can be significantly different, we
introduce a discriminator that not only tells if the generated movement is
realistic or not, but also tells the class label of the interaction. This
allows the use of such labels in supervising the training of the generator. We
experiment with the SBU and the HHOI datasets. The high quality of the
synthetic motion demonstrates the effective design of our generator, and the
discriminability of the synthesis also demonstrates the strength of our
discriminator.","['Qianhui Men', 'Hubert P. H. Shum', 'Edmond S. L. Ho', 'Howard Leung']",2021-10-01T13:13:07Z,http://arxiv.org/abs/2110.00380v1,"['cs.GR', 'cs.CV']","GAN,Reactive Motion Synthesis,Class-aware Discriminators,Human-human Interaction,Generative Adversarial Network,Spatial-temporal Information,Long Short-Term Memory (LSTM),Attention Module,Discriminator,Synthetic Motion"
Metameric Varifocal Holography,"Computer-Generated Holography (CGH) offers the potential for genuine,
high-quality three-dimensional visuals. However, fulfilling this potential
remains a practical challenge due to computational complexity and visual
quality issues. We propose a new CGH method that exploits gaze-contingency and
perceptual graphics to accelerate the development of practical holographic
display systems. Firstly, our method infers the user's focal depth and
generates images only at their focus plane without using any moving parts.
Second, the images displayed are metamers; in the user's peripheral vision,
they need only be statistically correct and blend with the fovea seamlessly.
Unlike previous methods, our method prioritises and improves foveal visual
quality without causing perceptually visible distortions at the periphery. To
enable our method, we introduce a novel metameric loss function that robustly
compares the statistics of two given images for a known gaze location. In
parallel, we implement a model representing the relation between holograms and
their image reconstructions. We couple our differentiable loss function and
model to metameric varifocal holograms using a stochastic gradient descent
solver. We evaluate our method with an actual proof-of-concept holographic
display, and we show that our CGH method leads to practical and perceptually
three-dimensional image reconstructions.","['David R. Walton', 'Koray Kavaklı', 'Rafael Kuffner dos Anjos', 'David Swapp', 'Tim Weyrich', 'Hakan Urey', 'Anthony Steed', 'Tobias Ritschel', 'Kaan Akşit']",2021-10-05T12:20:21Z,http://arxiv.org/abs/2110.01981v2,"['cs.GR', 'physics.optics']","Metameric Varifocal Holography,Computer-Generated Holography,CGH method,Gaze-contingency,Perceptual graphics,Focal depth,Metamers,Foveal visual quality,Metameric loss function,Holographic display systems"
FoV Privacy-aware VR Streaming,"Proactive tile-based virtual reality (VR) video streaming can use the trace
of FoV and eye movement to predict future requested tiles, then renders and
delivers the predicted tiles before playback. The quality of experience (QoE)
depends on the combined effect of tile prediction and consumed resources.
Recently, it has been found that with the FoV and eye movement data collected
for a user, one can infer the identity and preference of the user. Existing
works investigate the privacy protection for eye movement, but never address
how to protect the privacy in terms of FoV and how the privacy protection
affects the QoE. In this paper, we strive to characterize and satisfy the FoV
privacy requirement. We consider ""trading resources for privacy"". We first add
camouflaged tile requests around the real FoV and define spatial degree of
privacy (SDoP) as a normalized number of camouflaged tile requests. By
consuming more resources to ensure SDoP, the real FoVs can be hidden. Then, we
proceed to analyze the impacts of SDoP on the QoE by jointly optimizing the
durations for prediction, computing, and transmission that maximizes the QoE
given arbitrary predictor, configured resources, and SDoP. We find that a
larger SDoP requires more resources but degrades the performance of tile
prediction. Simulation with state-of-the-art predictors on a real dataset
verifies the analysis and shows that a user requiring a larger SDoP can be
served with better QoE.","['Xing Wei', 'Chenyang Yang']",2021-10-20T07:27:46Z,http://arxiv.org/abs/2110.10417v1,['cs.MM'],"FoV,VR,streaming,QoE,privacy protection,eye movement,resources,SDoP,prediction,tile-based"
"On games and simulators as a platform for development of artificial
  intelligence for command and control","Games and simulators can be a valuable platform to execute complex
multi-agent, multiplayer, imperfect information scenarios with significant
parallels to military applications: multiple participants manage resources and
make decisions that command assets to secure specific areas of a map or
neutralize opposing forces. These characteristics have attracted the artificial
intelligence (AI) community by supporting development of algorithms with
complex benchmarks and the capability to rapidly iterate over new ideas. The
success of artificial intelligence algorithms in real-time strategy games such
as StarCraft II have also attracted the attention of the military research
community aiming to explore similar techniques in military counterpart
scenarios. Aiming to bridge the connection between games and military
applications, this work discusses past and current efforts on how games and
simulators, together with the artificial intelligence algorithms, have been
adapted to simulate certain aspects of military missions and how they might
impact the future battlefield. This paper also investigates how advances in
virtual reality and visual augmentation systems open new possibilities in human
interfaces with gaming platforms and their military parallels.","['Vinicius G. Goecks', 'Nicholas Waytowich', 'Derrik E. Asher', 'Song Jun Park', 'Mark Mittrick', 'John Richardson', 'Manuel Vindiola', 'Anne Logie', 'Mark Dennison', 'Theron Trout', 'Priya Narayanan', 'Alexander Kott']",2021-10-21T17:39:58Z,http://arxiv.org/abs/2110.11305v1,"['cs.LG', 'cs.MA', 'I.2.6; I.6.3; A.1']","games,simulators,artificial intelligence,command and control,multi-agent,multiplayer,imperfect information,real-time strategy,military applications,virtual reality"
"Generational Frameshifts in Technology: Computer Science and
  Neurosurgery, The VR Use Case","We are at a unique moment in history where there is a confluence of
technologies which will synergistically come together to transform the practice
of neurosurgery. These technological transformations will be all-encompassing,
including improved tools and methods for intraoperative performance of
neurosurgery, scalable solutions for asynchronous neurosurgical training and
simulation, as well as broad aggregation of operative data allowing fundamental
changes in quality assessment, billing, outcome measures, and dissemination of
surgical best practices. The ability to perform surgery more safely and more
efficiently while capturing the operative details and parsing each component of
the operation will open an entirely new epoch advancing our field and all
surgical specialties. The digitization of all components within the operating
room will allow us to leverage the various fields within computer and
computational science to obtain new insights that will improve care and
delivery of the highest quality neurosurgery regardless of location. The
democratization of neurosurgery is at hand and will be driven by our
development, extraction, and adoption of these tools of the modern world.
Virtual reality provides a good example of how consumer-facing technologies are
finding a clear role in industry and medicine and serves as a notable example
of the confluence of various computer science technologies creating a novel
paradigm for scaling human ability and interactions. The authors describe the
technology ecosystem that has come and highlight a myriad of computational and
data sciences that will be necessary to enable the operating room of the near
future.","['Samuel R. Browd', 'Maya Sharma', 'Chetan Sharma']",2021-10-08T20:02:17Z,http://arxiv.org/abs/2110.15719v2,"['cs.HC', 'cs.AI', 'cs.CV', 'cs.NE', 'q-bio.OT']","Generational Frameshifts,Technology,Computer Science,Neurosurgery,VR,Synergistically,Neurosurgical Training,Simulation,Operative Data,Quality Assessment,Outcome Measures"
Optimal Targeted Advertising Strategy For Secure Wireless Edge Metaverse,"Recently, Metaverse has attracted increasing attention from both industry and
academia, because of the significant potential to integrate real and digital
worlds ever more seamlessly. By combining advanced wireless communications,
edge computing and virtual reality (VR) technologies into Metaverse, a
multidimensional, intelligent and powerful wireless edge Metaverse is created
for future human society. In this paper, we design a privacy preserving
targeted advertising strategy for the wireless edge Metaverse. Specifically, a
Metaverse service provider (MSP) allocates bandwidth to the VR users so that
the users can access Metaverse from edge access points. To protect users'
privacy, the covert communication technique is used in the downlink. Then, the
MSP can offer high-quality access services to earn more profits. Motivated by
the concept of ""covert"", targeted advertising is used to promote the sale of
bandwidth and ensure that the advertising strategy cannot be detected by
competitors who may make counter-offer and by attackers who want to disrupt the
services. We derive the best advertising strategy in terms of budget input,
with the help of the Vidale-Wolfe model and Hamiltonian function. Furthermore,
we propose a novel metric named Meta-Immersion to represent the user's
experience feelings. The performance evaluation shows that the MSP can boost
its revenue with an optimal targeted advertising strategy, especially compared
with that without the advertising.","['Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Dong In Kim', 'Chunyan Miao']",2021-10-31T14:25:08Z,http://arxiv.org/abs/2111.00511v2,"['cs.IT', 'math.IT']","wireless edge,metaverse,targeted advertising,privacy preserving,covert communication,bandwidth allocation,edge access points,advertising strategy,budget input,revenue boost"
"MetroLoc: Metro Vehicle Mapping and Localization with
  LiDAR-Camera-Inertial Integration","We propose an accurate and robust multi-modal sensor fusion framework,
MetroLoc, towards one of the most extreme scenarios, the large-scale metro
vehicle localization and mapping. MetroLoc is built atop an IMU-centric state
estimator that tightly couples light detection and ranging (LiDAR), visual, and
inertial information with the convenience of loosely coupled methods. The
proposed framework is composed of three submodules: IMU odometry,
LiDAR-inertial odometry (LIO), and Visual-inertial odometry (VIO). The IMU is
treated as the primary sensor, which achieves the observations from LIO and VIO
to constrain the accelerometer and gyroscope biases. Compared to previous
point-only LIO methods, our approach leverages more geometry information by
introducing both line and plane features into motion estimation. The VIO also
utilizes the environmental structure information by employing both lines and
points. Our proposed method has been extensively tested in the long-during
metro environments with a maintenance vehicle. Experimental results show the
system more accurate and robust than the state-of-the-art approaches with
real-time performance. Besides, we develop a series of Virtual Reality (VR)
applications towards efficient, economical, and interactive rail vehicle state
and trackside infrastructure monitoring, which has already been deployed to an
outdoor testing railroad.","['Yusheng Wang', 'Weiwei Song', 'Yi Zhang', 'Fei Huang', 'Zhiyong Tu', 'Yidong Lou']",2021-11-01T08:22:08Z,http://arxiv.org/abs/2111.00762v1,['cs.RO'],"MetroLoc,sensor fusion,LiDAR,IMU,odometry,VIO,VR,rail vehicle,localization"
Adding Safety Rules to Surgeon-Authored VR Training,"Introduction: Safety criteria in surgical VR training are typically
hard-coded and informally summarized. The Virtual Reality (VR) content creation
interface, TIPS-author, for the Toolkit for Illustration of Procedures in
Surgery (TIPS) allows surgeon-educators (SEs) to create laparoscopic
VR-training modules with force feedback. TIPS-author initializes anatomy shape
and physical properties selected by the SE accessing a cloud data base of
physics-enabled pieces of anatomy. Methods: A new addition to TIPS-author are
safety rules that are set by the SE and are automatically monitored during
simulation. Errors are recorded as visual snapshots for feedback to the
trainee. This paper reports on the implementation and opportunistic evaluation
of the snap-shot mechanism as a trainee feedback mechanism. TIPS was field
tested at two surgical conferences, one before and one after adding the
snapshot feature. Results: While other ratings of TIPS remained unchanged for
an overall Likert scale score of 5.24 out of 7 (7 equals very useful), the
rating of the statement `The TIPS interface helps learners understand the force
necessary to explore the anatomy' improved from 5.04 to 5.35 out of 7 after the
snapshot mechanism was added. Conclusions: The ratings indicate the viability
of the TIPS open-source2 E-authored surgical training units. Presenting
SE-determined procedural missteps via the snapshot mechanism at the end of the
training increases acceptance","['Ruiliang Gao', 'Sergei Kurenov', 'Erik W. Black', 'Jorg Peters']",2021-11-03T21:12:00Z,http://arxiv.org/abs/2111.02523v1,"['math.NA', 'cs.HC', 'cs.NA']","safety rules,surgeon-educators,VR training,force feedback,laparoscopic,anatomy,simulation,feedback mechanism,open-source,procedural missteps"
A QoE Model in Point Cloud Video Streaming,"Point cloud video has been widely used by augmented reality (AR) and virtual
reality (VR) applications as it allows users to have an immersive experience of
six degrees of freedom (6DoFs). Yet there is still a lack of research on
quality of experience (QoE) model of point cloud video streaming, which cannot
provide optimization metric for streaming systems. Besides, position and color
information contained in each pixel of point cloud video, and viewport distance
effect caused by 6DoFs viewing procedure make the traditional objective quality
evaluation metric cannot be directly used in point cloud video streaming
system. In this paper we first analyze the subjective and objective factors
related to QoE model. Then an experimental system to simulate point cloud video
streaming is setup and detailed subjective quality evaluation experiments are
carried out. Based on collected mean opinion score (MOS) data, we propose a QoE
model for point cloud video streaming. We also verify the model by actual
subjective scoring, and the results show that the proposed QoE model can
accurately reflect users' visual perception. We also make the experimental
database public to promote the QoE research of point cloud video streaming.","['Jie Li', 'Xiao Wang', 'Zhi Liu', 'Qiyue Li']",2021-11-04T16:29:43Z,http://arxiv.org/abs/2111.02985v4,"['cs.MM', 'eess.IV']","point cloud video,streaming,quality of experience,QoE model,augmented reality,virtual reality,6 degrees of freedom,mean opinion score,subjective quality evaluation,experimental database"
"Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with
  Depth and Cross Modal Attention","Binaural audio gives the listener an immersive experience and can enhance
augmented and virtual reality. However, recording binaural audio requires
specialized setup with a dummy human head having microphones in left and right
ears. Such a recording setup is difficult to build and setup, therefore mono
audio has become the preferred choice in common devices. To obtain the same
impact as binaural audio, recent efforts have been directed towards lifting
mono audio to binaural audio conditioned on the visual input from the scene.
Such approaches have not used an important cue for the task: the distance of
different sound producing objects from the microphones. In this work, we argue
that depth map of the scene can act as a proxy for inducing distance
information of different objects in the scene, for the task of audio
binauralization. We propose a novel encoder-decoder architecture with a
hierarchical attention mechanism to encode image, depth and audio feature
jointly. We design the network on top of state-of-the-art transformer networks
for image and depth representation. We show empirically that the proposed
method outperforms state-of-the-art methods comfortably for two challenging
public datasets FAIR-Play and MUSIC-Stereo. We also demonstrate with
qualitative results that the method is able to focus on the right information
required for the task. The project details are available at
\url{https://krantiparida.github.io/projects/bmonobinaural.html}","['Kranti Kumar Parida', 'Siddharth Srivastava', 'Gaurav Sharma']",2021-11-15T19:07:39Z,http://arxiv.org/abs/2111.08046v1,"['cs.CV', 'cs.SD', 'eess.AS']","Binaural audio,Mono audio,Depth map,Cross modal attention,Encoder-decoder architecture,Hierarchical attention mechanism,Transformer networks,FAIR-Play dataset,MUSIC-Stereo dataset"
"Teacher-Student Training and Triplet Loss to Reduce the Effect of
  Drastic Face Occlusion","We study a series of recognition tasks in two realistic scenarios requiring
the analysis of faces under strong occlusion. On the one hand, we aim to
recognize facial expressions of people wearing Virtual Reality (VR) headsets.
On the other hand, we aim to estimate the age and identify the gender of people
wearing surgical masks. For all these tasks, the common ground is that half of
the face is occluded. In this challenging setting, we show that convolutional
neural networks (CNNs) trained on fully-visible faces exhibit very low
performance levels. While fine-tuning the deep learning models on occluded
faces is extremely useful, we show that additional performance gains can be
obtained by distilling knowledge from models trained on fully-visible faces. To
this end, we study two knowledge distillation methods, one based on
teacher-student training and one based on triplet loss. Our main contribution
consists in a novel approach for knowledge distillation based on triplet loss,
which generalizes across models and tasks. Furthermore, we consider combining
distilled models learned through conventional teacher-student training or
through our novel teacher-student training based on triplet loss. We provide
empirical evidence showing that, in most cases, both individual and combined
knowledge distillation methods bring statistically significant performance
improvements. We conduct experiments with three different neural models (VGG-f,
VGG-face, ResNet-50) on various tasks (facial expression recognition, gender
recognition, age estimation), showing consistent improvements regardless of the
model or task.","['Mariana-Iuliana Georgescu', 'Georgian Duta', 'Radu Tudor Ionescu']",2021-11-20T11:13:46Z,http://arxiv.org/abs/2111.10561v1,"['cs.CV', 'cs.LG']","Teacher-student training,Triplet loss,Face occlusion,Virtual reality,Convolutional neural networks,Knowledge distillation,Facial expression recognition,Gender recognition,Age estimation,Neural models"
"Analysis of pedestrian stress level using GSR sensor in virtual
  immersive reality","Level of emotional arousal of one's body changes in response to external
stimuli in an environment. Given the risks involved while crossing streets,
particularly at unsignalized mid-block crosswalks, one can expect a change in
the stress level of pedestrians. In this study, we investigate the levels and
changes in pedestrian stress, under different road crossing scenarios in
immersive virtual reality. To measure the stress level of pedestrians, we used
Galvanic Skin Response (GSR) sensors. To collect the required data for the
model, Virtual Immersive Reality Environment (VIRE) tool is used, which enables
us to measure participants' stress levels in a controlled environment. The
results suggested that the density of vehicles has a positive effect, meaning
as the density of vehicles increases, so does the stress level for pedestrians.
It was noted that younger pedestrians have a lower amount of stress when
crossing as compared to older pedestrians which have higher amounts of stress.
Geometric variables have an impact on the stress level of pedestrians. The
greater the number of lanes the greater the observed stress, which is due to
the crossing distance increasing, while the walking speed remains the same.","['Mahwish Mudassar', 'Arash Kalatian', 'Bilal Farooq']",2021-11-22T19:33:55Z,http://arxiv.org/abs/2111.11492v2,['cs.HC'],"pedestrian stress level,GSR sensor,virtual immersive reality,emotional arousal,external stimuli,mid-block crosswalks,Galvanic Skin Response,VIRE,density of vehicles,geometric variables"
Neuronal Learning Analysis using Cycle-Consistent Adversarial Networks,"Understanding how activity in neural circuits reshapes following task
learning could reveal fundamental mechanisms of learning. Thanks to the recent
advances in neural imaging technologies, high-quality recordings can be
obtained from hundreds of neurons over multiple days or even weeks. However,
the complexity and dimensionality of population responses pose significant
challenges for analysis. Existing methods of studying neuronal adaptation and
learning often impose strong assumptions on the data or model, resulting in
biased descriptions that do not generalize. In this work, we use a variant of
deep generative models called - CycleGAN, to learn the unknown mapping between
pre- and post-learning neural activities recorded $\textit{in vivo}$. We
develop an end-to-end pipeline to preprocess, train and evaluate calcium
fluorescence signals, and a procedure to interpret the resulting deep learning
models. To assess the validity of our method, we first test our framework on a
synthetic dataset with known ground-truth transformation. Subsequently, we
applied our method to neural activities recorded from the primary visual cortex
of behaving mice, where the mice transition from novice to expert-level
performance in a visual-based virtual reality experiment. We evaluate model
performance on generated calcium signals and their inferred spike trains. To
maximize performance, we derive a novel approach to pre-sort neurons such that
convolutional-based networks can take advantage of the spatial information that
exists in neural activities. In addition, we incorporate visual explanation
methods to improve the interpretability of our work and gain insights into the
learning process as manifested in the cellular activities. Together, our
results demonstrate that analyzing neuronal learning processes with data-driven
deep unsupervised methods holds the potential to unravel changes in an unbiased
way.","['Bryan M. Li', 'Theoklitos Amvrosiadis', 'Nathalie Rochefort', 'Arno Onken']",2021-11-25T13:24:19Z,http://arxiv.org/abs/2111.13073v1,"['q-bio.NC', 'cs.LG']","neuronal learning,cycle-consistent adversarial networks,deep generative models,calcium fluorescence signals,neural activities,spike trains,convolutional-based networks,visual explanation methods"
Optical Wireless Sytems for Spine and Leaf Data Center Downlinks,"The continually growing demands for traffic as a result of advanced
technologies in 5G and 6G systems offering services with intensive demands such
as IoT and virtual reality applications has resulted in significant performance
expectations of data center networks (DCNs). More specifically, DCNs are
expected to meet high bandwidth connectivity, high throughput, low latency, and
high scalability requirements. However, the current wired DCN architectures
introduce large cabling requirements and limit the ability to reconfigure data
centres as they expand. To that end, wireless technologies such as Optical
Wireless Communication (OWC) have been proposed as a viable and cost-effective
solution to meet the aforementioned requirements. This paper proposes the use
of Infrared (IR) OWC systems that employ Wavelength Division Multiplexing (WDM)
to enhance the DCN communication in the downlink direction; i.e. from Access
Points (APs) in the ceiling, connected to spine switches, to receivers attached
to the top of the racks representing leaf switches. The proposed systems
utilize Angle Diversity Transmitters (ADTs) mounted on the room ceiling to
facilitate inter-rack communication. Two different optical receiver types are
considered, namely Angle Diversity Receivers (ADRs) and Wide Field-of-View
Receivers (WFOVR). The simulation (i.e. channel modeling) results show that our
proposed data center links achieve good data rates in the data centre up to 15
Gbps.","['Abrar S. Alhazmi', 'Sanaa H. Mohamed and', 'T. E. H. El-Gorashi', 'Jaafar M. H. Elmirghani']",2021-11-30T11:26:24Z,http://arxiv.org/abs/2111.15301v1,"['cs.NI', 'eess.SP']","Optical Wireless Systems,Spine,Leaf,Data Center,Downlinks,Optical Wireless Communication,Wavelength Division Multiplexing,Angle Diversity Transmitters,Angle Diversity Receivers,Wide Field-of-View Receivers"
"ORCLSim: A System Architecture for Studying Bicyclist and Pedestrian
  Physiological Behavior Through Immersive Virtual Environments","Injuries and fatalities for vulnerable road users, especially bicyclists and
pedestrians, are on the rise. To better inform design for vulnerable road
users, we need to conduct more studies to evaluate how bicyclist and pedestrian
behavior and physiological states change in different roadway designs and
contextual settings. Previous research highlights the advantages of Immersive
Virtual Environment (IVE) in conducting bicyclist and pedestrian studies. These
environments do not put participants at risk of getting injured, are low-cost
compared to on-road or naturalistic studies and allow researchers to fully
control variables of interest. In this paper, we propose a framework ORCLSim,
to support human sensing techniques within IVE to evaluate bicyclist and
pedestrian physiological and behavioral changes in different contextual
settings. To showcase this framework, we present two case studies where we
collect and analyze pilot data from five participants' physiological and
behavioral responses in an IVE setting, representing real-world roadway
segments and traffic conditions. Results from these case studies indicate that
physiological data is sensitive to road environment changes and real-time
events, especially changes in heart rate and gaze behavior. Additionally, our
preliminary data indicates participants may respond differently to various
roadway settings (e.g., intersections with or without traffic signal). By
analyzing these changes, we can identify how participants' stress levels and
cognitive load is impacted by the simulated surrounding environment. The
ORCLSim system architecture can be further utilized for future studies in
users' behavioral and physiological responses in different virtual reality
settings.","['Xiang Guo', 'Austin Angulo', 'Erin Robartes', 'T. Donna Chen', 'Arsalan Heydarian']",2021-12-06T23:31:29Z,http://arxiv.org/abs/2112.03420v1,"['cs.HC', 'cs.MM', 'J.4']","system architecture,bicyclist,pedestrian,physiological behavior,immersive virtual environments,vulnerable road users,human sensing techniques,contextual settings,physiological data,cognitive load"
"Assistive Tele-op: Leveraging Transformers to Collect Robotic Task
  Demonstrations","Sharing autonomy between robots and human operators could facilitate data
collection of robotic task demonstrations to continuously improve learned
models. Yet, the means to communicate intent and reason about the future are
disparate between humans and robots. We present Assistive Tele-op, a virtual
reality (VR) system for collecting robot task demonstrations that displays an
autonomous trajectory forecast to communicate the robot's intent. As the robot
moves, the user can switch between autonomous and manual control when desired.
This allows users to collect task demonstrations with both a high success rate
and with greater ease than manual teleoperation systems. Our system is powered
by transformers, which can provide a window of potential states and actions far
into the future -- with almost no added computation time. A key insight is that
human intent can be injected at any location within the transformer sequence if
the user decides that the model-predicted actions are inappropriate. At every
time step, the user can (1) do nothing and allow autonomous operation to
continue while observing the robot's future plan sequence, or (2) take over and
momentarily prescribe a different set of actions to nudge the model back on
track. We host the videos and other supplementary material at
https://sites.google.com/view/assistive-teleop.","['Henry M. Clever', 'Ankur Handa', 'Hammad Mazhar', 'Kevin Parker', 'Omer Shapira', 'Qian Wan', 'Yashraj Narang', 'Iretiayo Akinola', 'Maya Cakmak', 'Dieter Fox']",2021-12-09T18:58:44Z,http://arxiv.org/abs/2112.05129v1,['cs.RO'],"Assistive Tele-op,Transformers,Robotic Task Demonstrations,Autonomy,Virtual Reality,Robot Task Demonstrations,Manual Control,Teleoperation Systems"
UrbanRama: Navigating Cities in Virtual Reality,"Exploring large virtual environments, such as cities, is a central task in
several domains, such as gaming and urban planning. VR systems can greatly help
this task by providing an immersive experience; however, a common issue with
viewing and navigating a city in the traditional sense is that users can either
obtain a local or a global view, but not both at the same time, requiring them
to continuously switch between perspectives, losing context and distracting
them from their analysis. In this paper, our goal is to allow users to navigate
to points of interest without changing perspectives. To accomplish this, we
design an intuitive navigation interface that takes advantage of the strong
sense of spatial presence provided by VR. We supplement this interface with a
perspective that warps the environment, called UrbanRama, based on a
cylindrical projection, providing a mix of local and global views. The design
of this interface was performed as an iterative process in collaboration with
architects and urban planners. We conducted a qualitative and a quantitative
pilot user study to evaluate UrbanRama and the results indicate the
effectiveness of our system in reducing perspective changes, while ensuring
that the warping doesn't affect distance and orientation perception.","['Shaoyu Chen', 'Fabio Miranda', 'Nivan Ferreira', 'Marcos Lage', 'Harish Doraiswamy', 'Corinne Brenner', 'Connor Defanti', 'Michael Koutsoubis', 'Luc Wilson', 'Ken Perlin', 'Claudio Silva']",2021-12-11T22:19:14Z,http://arxiv.org/abs/2112.06082v1,"['cs.HC', 'cs.GR']","Virtual Reality,Urban Planning,VR systems,Navigation,City,Spatial Presence,User Study,Perspective,Warping,Cylindrical Projection"
"Extending 3-DoF Metrics to Model User Behaviour Similarity in 6-DoF
  Immersive Applications","Immersive reality technologies, such as Virtual and Augmented Reality, have
ushered a new era of user-centric systems, in which every aspect of the
coding--delivery--rendering chain is tailored to the interaction of the users.
Understanding the actual interactivity and behaviour of the users is still an
open challenge and a key step to enabling such a user-centric system. Our main
goal is to extend the applicability of existing behavioural methodologies for
studying user navigation in the case of 6 Degree-of-Freedom (DoF).
Specifically, we first compare the navigation in 6-DoF with its 3-DoF
counterpart highlighting the main differences and novelties. Then, we define
new metrics aimed at better modelling behavioural similarities between users in
a 6-DoF system. We validate and test our solutions on real navigation paths of
users interacting with dynamic volumetric media in 6-DoF Virtual Reality
conditions. Our results show that metrics that consider both user position and
viewing direction better perform in detecting user similarity while navigating
in a 6-DoF system. Having easy-to-use but robust metrics that underpin multiple
tools and answer the question ``how do we detect if two users look at the same
content?"" open the gate to new solutions for a user-centric system.","['Silvia Rossi', 'Irene Viola', 'Laura Toni', 'Pablo Cesar']",2021-12-17T09:29:06Z,http://arxiv.org/abs/2112.09402v2,"['cs.HC', 'cs.MM']","3-DoF,6-DoF,Immersive Applications,User Behaviour,Metrics,User-Centric System,User Navigation,Virtual Reality,Augmented Reality"
"Pseudo-Haptic Button for Improving User Experience of Mid-Air
  Interaction in VR","Mid-air interaction is one of the promising interaction modalities in virtual
reality (VR) due to its merits in naturalness and intuitiveness, but the
interaction suffers from the lack of haptic feedback as no force or
vibrotactile feedback can be provided in mid-air. As a breakthrough to
compensate for this insufficiency, the application of pseudo-haptic features
which create the visuo-haptic illusion without actual physical haptic stimulus
can be explored. Therefore, this study aimed to investigate the effect of four
pseudo-haptic features: proximity feedback, protrusion, hit effect, and
penetration blocking on user experience for free-hand mid-air button
interaction in VR. We conducted a user study on 21 young subjects to collect
user ratings on various aspects of user experience while users were freely
interacting with 16 buttons with different combinations of four features.
Results indicated that all investigated features significantly improved user
experience in terms of haptic illusion, embodiment, sense of reality,
spatiotemporal perception, satisfaction, and hedonic quality. In addition,
protrusion and hit effect were more beneficial in comparison with the other two
features. It is recommended to utilize the four proposed pseudo-haptic features
in 3D user interfaces (UIs) to make users feel more pleased and amused, but
caution is needed when using proximity feedback together with other features.
The findings of this study could be helpful for VR developers and UI designers
in providing better interactive buttons in the 3D interfaces.","['Woojoo Kim', 'Shuping Xiong']",2021-12-21T06:27:45Z,http://arxiv.org/abs/2112.11007v1,"['cs.HC', 'H.5.1; H.5.2']","Pseudo-haptic,User Experience,Mid-air Interaction,VR,Pseudo-haptic Features,Proximity Feedback,Protrusion,Hit Effect,Penetration Blocking,User Study"
Metaverse Shape of Your Life for Future: A bibliometric snapshot,"The metaverse was first introduced in 1992. Many people saw Metaverse as a
new word but the concept of Metaverse is not a new term. However, Zuckerberg's
press release drew all the attention to the Metaverse. This study presents a
bibliometric evaluation of metaverse technology, which has been discussed in
the literature since the nineties. A field study is carried out especially for
the metaverse, which is a new and trendy subject. In this way, descriptive
information is presented on journals, institutions, prominent researchers, and
countries in the field, as well as extra evaluation on the prominent topics in
the field and researchers with heavy citations. In our study, which was carried
out by extracting the data of all documents between the years 1990-2021 from
the Web of Science database, it was seen that there were few studies in the
literature in the historical process for the metaverse, whose popularity has
reached its peak in recent months. In addition, it is seen that the subject is
handled intensively with virtual reality and augmented reality technologies,
and the education sector and digital marketing fields show interest in the
field. Metaverse will probably have entered many areas of our lives in the next
15-20 years, shape our lives by taking advantage of the opportunities of
developing technology.",['Muhammet Damar'],2021-12-08T13:46:16Z,http://arxiv.org/abs/2112.12068v1,"['cs.DL', 'cs.CY']","metaverse,bibliometric evaluation,technology,virtual reality,augmented reality,education sector,digital marketing,field study,researchers,Web of Science"
"Multi-modal 3D Human Pose Estimation with 2D Weak Supervision in
  Autonomous Driving","3D human pose estimation (HPE) in autonomous vehicles (AV) differs from other
use cases in many factors, including the 3D resolution and range of data,
absence of dense depth maps, failure modes for LiDAR, relative location between
the camera and LiDAR, and a high bar for estimation accuracy. Data collected
for other use cases (such as virtual reality, gaming, and animation) may
therefore not be usable for AV applications. This necessitates the collection
and annotation of a large amount of 3D data for HPE in AV, which is
time-consuming and expensive. In this paper, we propose one of the first
approaches to alleviate this problem in the AV setting. Specifically, we
propose a multi-modal approach which uses 2D labels on RGB images as weak
supervision to perform 3D HPE. The proposed multi-modal architecture
incorporates LiDAR and camera inputs with an auxiliary segmentation branch. On
the Waymo Open Dataset, our approach achieves a 22% relative improvement over
camera-only 2D HPE baseline, and 6% improvement over LiDAR-only model. Finally,
careful ablation studies and parts based analysis illustrate the advantages of
each of our contributions.","['Jingxiao Zheng', 'Xinwei Shi', 'Alexander Gorban', 'Junhua Mao', 'Yang Song', 'Charles R. Qi', 'Ting Liu', 'Visesh Chari', 'Andre Cornman', 'Yin Zhou', 'Congcong Li', 'Dragomir Anguelov']",2021-12-22T18:57:16Z,http://arxiv.org/abs/2112.12141v1,['cs.CV'],"3D human pose estimation,Autonomous driving,Multi-modal approach,Weak supervision,LiDAR,Camera,Waymo Open Dataset,Ablation studies"
Heterogenous Networks: From small cells to 5G NR-U,"With the exponential increase in mobile users, the mobile data demand has
grown tremendously. To meet these demands, cellular operators are constantly
innovating to enhance the capacity of cellular systems. Consequently, operators
have been reusing the licensed spectrum spatially, by deploying 4G/LTE small
cells (e.g., Femto Cells) in the past. However, despite the use of small cells,
licensed spectrum will be unable to meet the consistently rising data traffic
because of data-intensive applications such as augmented reality or virtual
reality (AR/VR) and on-the-go high-definition video streaming. Applications
such AR/VR and online gaming not only place extreme data demands on the
network, but are also latency-critical. To meet the QoS guarantees, cellular
operators have begun leveraging the unlicensed spectrum by coexisting with
Wi-Fi in the 5 GHz band. The standardizing body 3GPP, has prescribed cellular
standards for fair unlicensed coexistence with Wi-Fi, namely LTE Licensed
Assisted Access (LAA), New Radio in unlicensed (NR-U), and NR in Millimeter.
The rapid roll-out of LAA deployments in developed nations like the US, offers
an opportunity to study and analyze the performance of unlicensed coexistence
networks through real-world ground truth. Thus, this paper presents a
high-level overview of past, present, and future of the research in small cell
and unlicensed coexistence communication technologies. It outlines the vision
for future research work in the recently allocated unlicensed spectrum: The 6
GHz band, where the latest Wi-Fi standard, IEEE 802.11ax, will coexist with the
latest cellular technology, 5G New Radio (NR) in unlicensed.","['Vanlin Sathya', 'Srikant Manas Kala', 'Kalpana Naidu']",2021-12-28T18:01:34Z,http://arxiv.org/abs/2112.14240v1,['cs.NI'],"Heterogeneous Networks,5G NR-U,small cells,cellular operators,licensed spectrum,4G/LTE,Femto Cells,data traffic,augmented reality,virtual reality (AR/VR),online gaming,QoS guarantees,unlicensed spectrum,Wi-Fi,5 GHz band,3GPP,LTE Licensed Assisted Access (LAA),New Radio in unlicensed (NR-U),NR in Millimeter,LAA deployments,coexistence networks,unlicensed coexistence communication technologies,6 GHz band,IEEE 802.11ax"
"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting:
  Traditional Approaches, Deep Learning, and Open Challenges","Fifth generation (5G) network and beyond envision massive Internet of Things
(IoT) rollout to support disruptive applications such as extended reality (XR),
augmented/virtual reality (AR/VR), industrial automation, autonomous driving,
and smart everything which brings together massive and diverse IoT devices
occupying the radio frequency (RF) spectrum. Along with the spectrum crunch and
throughput challenges, such a massive scale of wireless devices exposes
unprecedented threat surfaces. RF fingerprinting is heralded as a candidate
technology that can be combined with cryptographic and zero-trust security
measures to ensure data privacy, confidentiality, and integrity in wireless
networks. Motivated by the relevance of this subject in the future
communication networks, in this work, we present a comprehensive survey of RF
fingerprinting approaches ranging from a traditional view to the most recent
deep learning (DL)-based algorithms. Existing surveys have mostly focused on a
constrained presentation of the wireless fingerprinting approaches, however,
many aspects remain untold. In this work, however, we mitigate this by
addressing every aspect - background on signal intelligence (SIGINT),
applications, relevant DL algorithms, systematic literature review of RF
fingerprinting techniques spanning the past two decades, discussion on
datasets, and potential research avenues - necessary to elucidate this topic to
the reader in an encyclopedic manner.","['Anu Jagannath', 'Jithin Jagannath', 'Prem Sagar Pattanshetty Vasanth Kumar']",2022-01-03T14:42:53Z,http://arxiv.org/abs/2201.00680v3,"['cs.LG', 'cs.AI']","Radio Frequency Fingerprinting,Deep Learning,Internet of Things (IoT),Extended Reality (XR),Augmented/Virtual Reality (AR/VR),Industrial Automation,Autonomous Driving,Wireless Networks,Data Privacy,Confidentiality"
Stay in Touch! Shape and Shadow Influence Surface Contact in XR Displays,"The information provided to a person's visual system by extended reality (XR)
displays is not a veridical match to the information provided by the real
world. Due in part to graphical limitations in XR head-mounted displays (HMDs),
which vary by device, our perception of space may be altered. However, we do
not yet know which properties of virtual objects rendered by HMDs --
particularly augmented reality displays -- influence our ability to understand
space. In the current research, we evaluate how immersive graphics affect
spatial perception across three unique XR displays: virtual reality (VR), video
see-through augmented reality (VST AR), and optical see-through augmented
reality (OST AR). We manipulated the geometry of the presented objects as well
as the shading techniques for objects' cast shadows. Shape and shadow were
selected for evaluation as they play an important role in determining where an
object is in space by providing points of contact between an object and its
environment -- be it real or virtual. Our results suggest that a
non-photorealistic (NPR) shading technique, in this case for cast shadows, may
be used to improve depth perception by enhancing perceived surface contact in
XR. Further, the benefit of NPR graphics is more pronounced in AR than in VR
displays. One's perception of ground contact is influenced by an object's
shape, as well. However, the relationship between shape and surface contact
perception is more complicated.","['Haley Adams', 'Holly Gagnon', 'Sarah Creem-Regehr', 'Jeanine Stefanucci', 'Bobby Bodenheimer']",2022-01-06T02:00:41Z,http://arxiv.org/abs/2201.01889v1,"['cs.HC', 'cs.GR']","XR displays,spatial perception,immersive graphics,virtual reality,augmented reality,shading techniques,cast shadows,non-photorealistic (NPR),depth perception,surface contact perception"
De-rendering 3D Objects in the Wild,"With increasing focus on augmented and virtual reality applications (XR)
comes the demand for algorithms that can lift objects from images and videos
into representations that are suitable for a wide variety of related 3D tasks.
Large-scale deployment of XR devices and applications means that we cannot
solely rely on supervised learning, as collecting and annotating data for the
unlimited variety of objects in the real world is infeasible. We present a
weakly supervised method that is able to decompose a single image of an object
into shape (depth and normals), material (albedo, reflectivity and shininess)
and global lighting parameters. For training, the method only relies on a rough
initial shape estimate of the training objects to bootstrap the learning
process. This shape supervision can come for example from a pretrained depth
network or - more generically - from a traditional structure-from-motion
pipeline. In our experiments, we show that the method can successfully
de-render 2D images into a decomposed 3D representation and generalizes to
unseen object categories. Since in-the-wild evaluation is difficult due to the
lack of ground truth data, we also introduce a photo-realistic synthetic test
set that allows for quantitative evaluation.","['Felix Wimbauer', 'Shangzhe Wu', 'Christian Rupprecht']",2022-01-06T23:50:09Z,http://arxiv.org/abs/2201.02279v2,['cs.CV'],"augmented reality,virtual reality,3D objects,XR applications,weakly supervised method,shape,material,global lighting parameters,training,synthetic test set"
"Instant Reality: Gaze-Contingent Perceptual Optimization for 3D Virtual
  Reality Streaming","Media streaming has been adopted for a variety of applications such as
entertainment, visualization, and design. Unlike video/audio streaming where
the content is usually consumed sequentially, 3D applications such as gaming
require streaming 3D assets to facilitate client-side interactions such as
object manipulation and viewpoint movement. Compared to audio and video
streaming, 3D streaming often requires larger data sizes and yet lower latency
to ensure sufficient rendering quality, resolution, and latency for perceptual
comfort. Thus, streaming 3D assets can be even more challenging than streaming
audios/videos, and existing solutions often suffer from long loading time or
limited quality.
  To address this critical and timely issue, we propose a
perceptually-optimized progressive 3D streaming method for spatial quality and
temporal consistency in immersive interactions. Based on the human visual
mechanisms in the frequency domain, our model selects and schedules the
streaming dataset for optimal spatial-temporal quality. We also train a neural
network for our model to accelerate this decision process for real-time
client-server applications. We evaluate our method via subjective studies and
objective analysis under varying network conditions (from 3G to 5G) and client
devices (HMD and traditional displays), and demonstrate better visual quality
and temporal consistency than alternative solutions.","['Shaoyu Chen', 'Budmonde Duinkharjav', 'Xin Sun', 'Li-Yi Wei', 'Stefano Petrangeli', 'Jose Echevarria', 'Claudio Silva', 'Qi Sun']",2022-01-10T17:29:29Z,http://arxiv.org/abs/2201.03484v1,['cs.HC'],"3D virtual reality,streaming,perceptual optimization,gaze-contingent,spatial quality,temporal consistency,neural network,client-server applications,immersive interactions,subjective studies"
Matching-based Service Offloading for Compute-less Driven IoT Networks,"With the advent of the Internet of Things (IoT) and 5G networks, edge
computing is offering new opportunities for business model and use cases
innovations. Service providers can now virtualize the cloud beyond the data
center to meet the latency, data sovereignty, reliability, and interoperability
requirements. Yet, many new applications (e.g., augmented reality, virtual
reality, artificial intelligence) are computation-intensive and
delay-sensitivity. These applications are invoked heavily with similar inputs
that could lead to the same output. Compute-less networks aim to implement a
network with a minimum amount of computation and communication. This can be
realized by offloading prevalent services to the edge and thus minimizing
communication in the core network and eliminating redundant computations using
the computation reuse concept. In this paper, we present matching-based
services offloading schemes for compute-less IoT networks. We adopt the
matching theory to match service offloading to the appropriate edge server(s).
Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme
that aims to offload the most invoked and highly reusable services to the
appropriate edge servers. We further extend WHISTLE to provide horizontal
one-to-many computation reuse sharing among edge servers which leads to
bouncing less computation back to the cloud. We evaluate the efficiency and
effectiveness of WHISTLE with a real-world dataset. The obtained findings show
that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the
computation up to 77%, and decrease the communication up to 71%. Theoretical
analyses also prove the stability of the designed schemes.","['Boubakr Nour', 'Soumaya Cherkaoui']",2022-01-11T20:55:19Z,http://arxiv.org/abs/2201.04195v1,['cs.NI'],"IoT,5G networks,edge computing,compute-less networks,service offloading,matching theory,edge servers,computation reuse,WHISTLE,efficiency"
"SRVIO: Super Robust Visual Inertial Odometry for dynamic environments
  and challenging Loop-closure conditions","There has been extensive research on visual localization and odometry for
autonomous robots and virtual reality during the past decades. Traditionally,
this problem has been solved with the help of expensive sensors, such as
lidars. Nowadays, the focus of the leading research in this field is on robust
localization using more economic sensors, such as cameras and IMUs.
Consequently, geometric visual localization methods have become more accurate
in time. However, these methods still suffer from significant loss and
divergence in challenging environments, such as a room full of moving people.
Scientists started using deep neural networks (DNNs) to mitigate this problem.
The main idea behind using DNNs is to better understand challenging aspects of
the data and overcome complex conditions such as the movement of a dynamic
object in front of the camera that covers the full view of the camera, extreme
lighting conditions, and high speed of the camera. Prior end-to-end DNN methods
have overcome some of these challenges. However, no general and robust
framework is available to overcome all challenges together. In this paper, we
have combined geometric and DNN-based methods to have the generality and speed
of geometric SLAM frameworks and overcome most of these challenging conditions
with the help of DNNs and deliver the most robust framework so far. To do so,
we have designed a framework based on Vins-Mono, and show that it is able to
achieve state-of-the-art results on TUM-Dynamic, TUM-VI, ADVIO, and EuRoC
datasets compared to geometric and end-to-end DNN based SLAMs. Our proposed
framework could also achieve outstanding results on extreme simulated cases
resembling the aforementioned challenges.","['Ali Samadzadeh', 'Ahmad Nickabadi']",2022-01-14T10:52:04Z,http://arxiv.org/abs/2201.05386v2,['cs.CV'],"Super Robust Visual Inertial Odometry,Dynamic Environments,Loop-closure conditions,Visual Localization,Odometry,Autonomous Robots,Virtual Reality,Geometric Visual Localization Methods,Deep Neural Networks (DNNs),SLAM Frameworks"
"On the impact of VR assessment on the Quality of Experience of Highly
  Realistic Digital Humans","Fuelled by the increase in popularity of virtual and augmented reality
applications, point clouds have emerged as a popular 3D format for acquisition
and rendering of digital humans, thanks to their versatility and real-time
capabilities. Due to technological constraints and real-time rendering
limitations, however, the visual quality of dynamic point cloud contents is
seldom evaluated using virtual and augmented reality devices, instead relying
on prerecorded videos displayed on conventional 2D screens. In this study, we
evaluate how the visual quality of point clouds representing digital humans is
affected by compression distortions. In particular, we compare three different
viewing conditions based on the degrees of freedom that are granted to the
viewer: passive viewing (2DTV), head rotation (3DoF), and rotation and
translation (6DoF), to understand how interacting in the virtual space affects
the perception of quality. We provide both quantitative and qualitative results
of our evaluation involving 78 participants, and we make the data publicly
available. To the best of our knowledge, this is the first study evaluating the
quality of dynamic point clouds in virtual reality, and comparing it to
traditional viewing settings. Results highlight the dependency of visual
quality on the content under test, and limitations in the way current data sets
are used to evaluate compression solutions. Moreover, influencing factors in
quality evaluation in VR, and shortcomings in how point cloud encoding
solutions handle visually-lossless compression, are discussed.","['Irene Viola', 'Shishir Subramanyam', 'Jie Li', 'Pablo Cesar']",2022-01-19T16:37:08Z,http://arxiv.org/abs/2201.07701v1,"['cs.MM', 'cs.HC']","virtual reality,augmented reality,point clouds,digital humans,compression distortions,viewing conditions,degrees of freedom,quantitative results,qualitative results"
Semantic-Aware Implicit Neural Audio-Driven Video Portrait Generation,"Animating high-fidelity video portrait with speech audio is crucial for
virtual reality and digital entertainment. While most previous studies rely on
accurate explicit structural information, recent works explore the implicit
scene representation of Neural Radiance Fields (NeRF) for realistic generation.
In order to capture the inconsistent motions as well as the semantic difference
between human head and torso, some work models them via two individual sets of
NeRF, leading to unnatural results. In this work, we propose Semantic-aware
Speaking Portrait NeRF (SSP-NeRF), which creates delicate audio-driven
portraits using one unified set of NeRF. The proposed model can handle the
detailed local facial semantics and the global head-torso relationship through
two semantic-aware modules. Specifically, we first propose a Semantic-Aware
Dynamic Ray Sampling module with an additional parsing branch that facilitates
audio-driven volume rendering. Moreover, to enable portrait rendering in one
unified neural radiance field, a Torso Deformation module is designed to
stabilize the large-scale non-rigid torso motions. Extensive evaluations
demonstrate that our proposed approach renders more realistic video portraits
compared to previous methods. Project page:
https://alvinliu0.github.io/projects/SSP-NeRF","['Xian Liu', 'Yinghao Xu', 'Qianyi Wu', 'Hang Zhou', 'Wayne Wu', 'Bolei Zhou']",2022-01-19T18:54:41Z,http://arxiv.org/abs/2201.07786v1,"['cs.CV', 'cs.GR', 'cs.LG', 'cs.MM', 'cs.SD', 'eess.AS']","Semantic-aware,Implicit Neural,Audio-Driven,Portrait Generation,Neural Radiance Fields,Torso Deformation,Ray Sampling,Volume Rendering,Facial Semantics,Non-rigid"
"Resource Provisioning in Edge Computing for Latency Sensitive
  Applications","Low-Latency IoT applications such as autonomous vehicles, augmented/virtual
reality devices and security applications require high computation resources to
make decisions on the fly. However, these kinds of applications cannot tolerate
offloading their tasks to be processed on a cloud infrastructure due to the
experienced latency. Therefore, edge computing is introduced to enable low
latency by moving the tasks processing closer to the users at the edge of the
network. The edge of the network is characterized by the heterogeneity of edge
devices forming it; thus, it is crucial to devise novel solutions that take
into account the different physical resources of each edge device. In this
paper, we propose a resource representation scheme, allowing each edge device
to expose its resource information to the supervisor of the edge node through
the mobile edge computing application programming interfaces proposed by
European Telecommunications Standards Institute. The information about the edge
device resource is exposed to the supervisor of the EN each time a resource
allocation is required. To this end, we leverage a Lyapunov optimization
framework to dynamically allocate resources at the edge devices. To test our
proposed model, we performed intensive theoretical and experimental simulations
on a testbed to validate the proposed scheme and its impact on different
system's parameters. The simulations have shown that our proposed approach
outperforms other benchmark approaches and provides low latency and optimal
resource consumption.","['Amine Abouaomar', 'Soumaya Cherkaoui', 'Zoubeir Mlika', 'Abdellatif Kobbane']",2022-01-27T22:46:04Z,http://arxiv.org/abs/2201.11837v1,['cs.NI'],"Resource Provisioning,Edge Computing,Latency Sensitive Applications,IoT,Edge Devices,Mobile Edge Computing,European Telecommunications Standards Institute,Lyapunov Optimization,Resource Allocation,Testbed"
"Spherical Convolution empowered FoV Prediction in 360-degree Video
  Multicast with Limited FoV Feedback","Field of view (FoV) prediction is critical in 360-degree video multicast,
which is a key component of the emerging Virtual Reality (VR) and Augmented
Reality (AR) applications. Most of the current prediction methods combining
saliency detection and FoV information neither take into account that the
distortion of projected 360-degree videos can invalidate the weight sharing of
traditional convolutional networks, nor do they adequately consider the
difficulty of obtaining complete multi-user FoV information, which degrades the
prediction performance. This paper proposes a spherical convolution-empowered
FoV prediction method, which is a multi-source prediction framework combining
salient features extracted from 360-degree video with limited FoV feedback
information. A spherical convolution neural network (CNN) is used instead of a
traditional two-dimensional CNN to eliminate the problem of weight sharing
failure caused by video projection distortion. Specifically, salient
spatial-temporal features are extracted through a spherical convolution-based
saliency detection model, after which the limited feedback FoV information is
represented as a time-series model based on a spherical convolution-empowered
gated recurrent unit network. Finally, the extracted salient video features are
combined to predict future user FoVs. The experimental results show that the
performance of the proposed method is better than other prediction methods.","['Jie Li', 'Ling Han', 'Cong Zhang', 'Qiyue Li', 'Zhi Liu']",2022-01-29T08:32:19Z,http://arxiv.org/abs/2201.12525v1,"['cs.CV', 'cs.MM']","360-degree video,FoV prediction,spherical convolution,saliency detection,Virtual Reality (VR),Augmented Reality (AR),convolutional neural network (CNN),multi-source prediction,gated recurrent unit network,salient features"
Hair Color Digitization through Imaging and Deep Inverse Graphics,"Hair appearance is a complex phenomenon due to hair geometry and how the
light bounces on different hair fibers. For this reason, reproducing a specific
hair color in a rendering environment is a challenging task that requires
manual work and expert knowledge in computer graphics to tune the result
visually. While current hair capture methods focus on hair shape estimation
many applications could benefit from an automated method for capturing the
appearance of a physical hair sample, from augmented/virtual reality to hair
dying development. Building on recent advances in inverse graphics and material
capture using deep neural networks, we introduce a novel method for hair color
digitization. Our proposed pipeline allows capturing the color appearance of a
physical hair sample and renders synthetic images of hair with a similar
appearance, simulating different hair styles and/or lighting environments.
Since rendering realistic hair images requires path-tracing rendering, the
conventional inverse graphics approach based on differentiable rendering is
untractable. Our method is based on the combination of a controlled imaging
device, a path-tracing renderer, and an inverse graphics model based on
self-supervised machine learning, which does not require to use differentiable
rendering to be trained. We illustrate the performance of our hair digitization
method on both real and synthetic images and show that our approach can
accurately capture and render hair color.","['Robin Kips', 'Panagiotis-Alexandros Bokaris', 'Matthieu Perrot', 'Pietro Gori', 'Isabelle Bloch']",2022-02-08T08:57:04Z,http://arxiv.org/abs/2202.03723v1,"['cs.GR', 'cs.AI', 'cs.CV', 'stat.ML']","hair color,digitization,imaging,deep inverse graphics,hair appearance,rendering environment,computer graphics,hair capture methods,deep neural networks,inverse graphics model"
Artificial Intelligence for the Metaverse: A Survey,"Along with the massive growth of the Internet from the 1990s until now,
various innovative technologies have been created to bring users breathtaking
experiences with more virtual interactions in cyberspace. Many virtual
environments with thousands of services and applications, from social networks
to virtual gaming worlds, have been developed with immersive experience and
digital transformation, but most are incoherent instead of being integrated
into a platform. In this context, metaverse, a term formed by combining meta
and universe, has been introduced as a shared virtual world that is fueled by
many emerging technologies, such as fifth-generation networks and beyond,
virtual reality, and artificial intelligence (AI). Among such technologies, AI
has shown the great importance of processing big data to enhance immersive
experience and enable human-like intelligence of virtual agents. In this
survey, we make a beneficial effort to explore the role of AI in the foundation
and development of the metaverse. We first deliver a preliminary of AI,
including machine learning algorithms and deep learning architectures, and its
role in the metaverse. We then convey a comprehensive investigation of AI-based
methods concerning six technical aspects that have potentials for the
metaverse: natural language processing, machine vision, blockchain, networking,
digital twin, and neural interface, and being potential for the metaverse.
Subsequently, several AI-aided applications, such as healthcare, manufacturing,
smart cities, and gaming, are studied to be deployed in the virtual worlds.
Finally, we conclude the key contribution of this survey and open some future
research directions in AI for the metaverse.","['Thien Huynh-The', 'Quoc-Viet Pham', 'Xuan-Qui Pham', 'Thanh Thi Nguyen', 'Zhu Han', 'Dong-Seong Kim']",2022-02-15T03:34:56Z,http://arxiv.org/abs/2202.10336v1,"['cs.CY', 'cs.AI', 'cs.LG']","Artificial Intelligence,Metaverse,Virtual Reality,Machine Learning,Deep Learning,Natural Language Processing,Blockchain,Networking,Digital Twin,Neural Interface"
"A Novel Hand Gesture Detection and Recognition system based on
  ensemble-based Convolutional Neural Network","Nowadays, hand gesture recognition has become an alternative for
human-machine interaction. It has covered a large area of applications like 3D
game technology, sign language interpreting, VR (virtual reality) environment,
and robotics. But detection of the hand portion has become a challenging task
in computer vision and pattern recognition communities. Deep learning algorithm
like convolutional neural network (CNN) architecture has become a very popular
choice for classification tasks, but CNN architectures suffer from some
problems like high variance during prediction, overfitting problem and also
prediction errors. To overcome these problems, an ensemble of CNN-based
approaches is presented in this paper. Firstly, the gesture portion is detected
by using the background separation method based on binary thresholding. After
that, the contour portion is extracted, and the hand region is segmented. Then,
the images have been resized and fed into three individual CNN models to train
them in parallel. In the last part, the output scores of CNN models are
averaged to construct an optimal ensemble model for the final prediction. Two
publicly available datasets (labeled as Dataset-1 and Dataset-2) containing
infrared images and one self-constructed dataset have been used to validate the
proposed system. Experimental results are compared with the existing
state-of-the-art approaches, and it is observed that our proposed ensemble
model outperforms other existing proposed methods.","['Abir Sen', 'Tapas Kumar Mishra', 'Ratnakar Dash']",2022-02-25T06:46:58Z,http://arxiv.org/abs/2202.12519v1,['cs.CV'],"hand gesture detection,recognition system,ensemble-based,Convolutional Neural Network,deep learning algorithm,CNN architecture,overfitting problem,binary thresholding,background separation"
"Integrating Immersive Technologies for Algorithmic Design in
  Architecture","Architectural design practice has radically evolved over the course of its
history, due to technological improvements that gave rise to advanced automated
tools for many design tasks. Traditional paper drawings and scale models are
now accompanied by 2D and 3D Computer-Aided Architectural Design (CAAD)
software.
  While such tools improved in many ways, including performance and accuracy
improvements, the modalities of user interaction have mostly remained the same,
with 2D interfaces displayed on 2D screens. The maturation of Augmented Reality
(AR) and Virtual Reality (VR) technology has led to some level of integration
of these immersive technologies into architectural practice, but mostly limited
to visualisation purposes, e.g. to show a finished project to a potential
client.
  We posit that there is potential to employ such technologies earlier in the
architectural design process and therefore explore that possibility with a
focus on Algorithmic Design (AD), a CAAD paradigm that relies on (often visual)
algorithms to generate geometries. The main goal of this dissertation is to
demonstrate that AR and VR can be adopted for AD activities.
  To verify that claim, we follow an iterative prototype-based methodology to
develop research prototype software tools and evaluate them. The three
developed prototypes provide evidence that integrating immersive technologies
into the AD toolset provides opportunities for architects to improve their
workflow and to better present their creations to clients. Based on our
contributions and the feedback we gathered from architectural students and
other researchers that evaluated the developed prototypes, we additionally
provide insights as to future perspectives in the field.",['Adrien Coppens'],2022-02-25T14:18:04Z,http://arxiv.org/abs/2202.12722v1,"['cs.HC', 'cs.GR']","Immersive Technologies,Algorithmic Design,Architecture,Computer-Aided Architectural Design,Augmented Reality,Virtual Reality,Geometries,Prototypes,Workflow,Visualization"
"Large-Scale 3D Semantic Reconstruction for Automated Driving Vehicles
  with Adaptive Truncated Signed Distance Function","The Large-scale 3D reconstruction, texturing and semantic mapping are
nowadays widely used for automated driving vehicles, virtual reality and
automatic data generation. However, most approaches are developed for RGB-D
cameras with colored dense point clouds and not suitable for large-scale
outdoor environments using sparse LiDAR point clouds. Since a 3D surface can be
usually observed from multiple camera images with different view poses, an
optimal image patch selection for the texturing and an optimal semantic class
estimation for the semantic mapping are still challenging.
  To address these problems, we propose a novel 3D reconstruction, texturing
and semantic mapping system using LiDAR and camera sensors. An Adaptive
Truncated Signed Distance Function is introduced to describe surfaces
implicitly, which can deal with different LiDAR point sparsities and improve
model quality. The from this implicit function extracted triangle mesh map is
then textured from a series of registered camera images by applying an optimal
image patch selection strategy. Besides that, a Markov Random Field-based data
fusion approach is proposed to estimate the optimal semantic class for each
triangle mesh. Our approach is evaluated on a synthetic dataset, the KITTI
dataset and a dataset recorded with our experimental vehicle. The results show
that the 3D models generated using our approach are more accurate in comparison
to using other state-of-the-art approaches. The texturing and semantic mapping
achieve also very promising results.","['Haohao Hu', 'Hexing Yang', 'Jian Wu', 'Xiao Lei', 'Frank Bieder', 'Jan-Hendrik Pauls', 'Christoph Stiller']",2022-02-28T15:11:25Z,http://arxiv.org/abs/2202.13855v1,['cs.CV'],"Large-scale 3D reconstruction,Semantic mapping,Automated driving vehicles,LiDAR point clouds,Truncated Signed Distance Function,Texturing,Camera sensors,Image patch selection,Semantic class estimation"
"Towards Optimal Path Allocation for Unreliable Reconfigurable
  Intelligent Surfaces","Terahertz (THz) communications and reconfigurable intelligent surfaces (RISs)
have been recently proposed to enable various powerful indoor applications,
such as wireless virtual reality (VR). For an efficient servicing of VR users,
an efficient THz path allocation solution becomes a necessity. Assuming the RIS
component is the most critical one in enabling the service, we investigate the
impact of RIS hardware failure on path allocation performance. To this end, we
study a THz network that employs THz operated RISs acting as base stations,
serving VR users. We propose a Semi-Markov decision Process (SMDP)-based path
allocation model to ensure the reliability of THz connection, while maximizing
the total long-term expected system reward, considering the system gains, costs
of link utilization, and the penalty of RIS failure. The SMDP-based model of
the RIS system is formulated by defining the state space, action space, reward
model, and transition probability distribution. We propose an optimal iterative
algorithm for path allocation that decides the next action at each system
state. The results show the average reward and VR service blocking probability
under different scenarios and with various VR service arrivals and RIS failure
rates, as first step towards feasible VR services over unreliable THz RIS.","['Mounir Bensalem', 'Anna Engelmann', 'Admela Jukan']",2022-03-01T10:42:41Z,http://arxiv.org/abs/2203.00344v2,"['cs.NI', 'eess.SP']","Terahertz communications,reconfigurable intelligent surfaces,path allocation,Semi-Markov decision Process,RIS hardware failure,THz network,VR users,system reward,long-term expected reward"
"Standardization of Extended Reality (XR) over 5G and 5G-Advanced 3GPP
  New Radio","Extended Reality (XR) is one of the major innovations to be introduced in
5G/5G-Advanced communication systems. A combination of augmented reality,
virtual reality, and mixed reality, supplemented by cloud gaming, revisits the
way how humans interact with computers, networks, and each other. However,
efficient support of XR services imposes new challenges for existing and future
wireless networks. This article presents a tutorial on integrating support for
the XR into the 3GPP New Radio (NR), summarizing a range of activities handled
within various 3GPP Service and Systems Aspects (SA) and Radio Access Networks
(RAN) groups. The article also delivers a case study evaluating the performance
of different XR services in state-of-the-art NR Release 17. The paper concludes
with a vision of further enhancements to better support XR in future NR
releases and outlines open problems in this area.","['Margarita Gapeyenko', 'Vitaly Petrov', 'Stefano Paris', 'Andrea Marcano', 'Klaus I. Pedersen']",2022-03-04T11:17:34Z,http://arxiv.org/abs/2203.02242v3,['cs.NI'],"Extended Reality,XR,5G,5G-Advanced,3GPP,New Radio,augmented reality,virtual reality,mixed reality,wireless networks"
"Mixed Reality Depth Contour Occlusion Using Binocular Similarity
  Matching and Three-dimensional Contour Optimisation","Mixed reality applications often require virtual objects that are partly
occluded by real objects. However, previous research and commercial products
have limitations in terms of performance and efficiency. To address these
challenges, we propose a novel depth contour occlusion (DCO) algorithm. The
proposed method is based on the sensitivity of contour occlusion and a
binocular stereoscopic vision device. In this method, a depth contour map is
combined with a sparse depth map obtained from a two-stage adaptive filter area
stereo matching algorithm and the depth contour information of the objects
extracted by a digital image stabilisation optical flow method. We also propose
a quadratic optimisation model with three constraints to generate an accurate
dense map of the depth contour for high-quality real-virtual occlusion. The
whole process is accelerated by GPU. To evaluate the effectiveness of the
algorithm, we demonstrate a time con-sumption statistical analysis for each
stage of the DCO algorithm execution. To verify the relia-bility of the
real-virtual occlusion effect, we conduct an experimental analysis on
single-sided, enclosed, and complex occlusions; subsequently, we compare it
with the occlusion method without quadratic optimisation. With our GPU
implementation for real-time DCO, the evaluation indicates that applying the
presented DCO algorithm can enhance the real-time performance and the visual
quality of real-virtual occlusion.","['Naye Ji', 'Fan Zhang', 'Haoxiang Zhang', 'Youbing Zhao', 'Dingguo Yu']",2022-03-04T13:16:40Z,http://arxiv.org/abs/2203.02300v1,"['cs.CV', 'cs.LG']","Mixed reality,Depth contour occlusion,Binocular similarity matching,Three-dimensional contour optimisation,Stereoscopic vision,Depth contour map,Sparse depth map,Stereo matching algorithm,Quadratic optimisation model,GPU acceleration"
"HI-DWA: Human-Influenced Dynamic Window Approach for Shared Control of a
  Telepresence Robot","This paper considers the problem of enabling the user to modify the path of a
telepresence robot. The robot is capable of autonomously navigating to a goal
predefined by the user, but the user might still want to modify the path, for
example, to go further away from other people, or to go closer to landmarks she
wants to see on the way. We propose Human-Influenced Dynamic Window Approach
(HI-DWA), a shared control method aimed for telepresence robots based on
Dynamic Window Approach (DWA) that allows the user to influence the control
input given to the robot. To verify the proposed method, we performed a user
study (N=32) in Virtual Reality (VR) to compare HI-DWA with switching between
autonomous navigation and manual control for controlling a simulated
telepresence robot moving in a virtual environment. Results showed that users
reached their goal faster using HI-DWA controller and found it easier to use.
Preference between the two methods was split equally. Qualitative analysis
revealed that a major reason for the participants that preferred switching
between two modes was the feeling of control. We also analyzed the effect of
different input methods, joystick and gesture, on the preference and perceived
workload.","['Juho Kalliokoski', 'Basak Sakcak', 'Markku Suomalainen', 'Katherine J. Mimnaugh', 'Alexis P. Chambers', 'Timo Ojala', 'Steven M. LaValle']",2022-03-05T11:02:07Z,http://arxiv.org/abs/2203.02703v2,"['cs.RO', 'cs.HC', 'cs.MM']","Telepresence robot,Shared control,Dynamic Window Approach,Human-influenced,User study,Virtual Reality (VR),Autonomous navigation,Control input,Manual control,Input methods"
Privacy Leakage in Proactive VR Streaming: Modeling and Tradeoff,"Proactive tile-based virtual reality (VR) video streaming employs the
viewpoint of a user to predict the tiles to be requested, renders and delivers
the predicted tiles before playback. Recently, it has been found that the
identity and preference of the user can be inferred from the trace of viewpoint
uploaded for proactive streaming, which indicates that viewpoint leakage incurs
privacy leakage. In this paper, we strive to answer the following questions
regarding viewpoint leakage during proactive VR video streaming. When is the
viewpoint leaked? Can privacy-preserving approaches (e.g., federated or
individual training, using predictors with no need for training, or predicting
locally) avoid viewpoint leakage? We find that if the prediction error or the
quality of experience (QoE) metric is uploaded for adaptive streaming, the real
viewpoint can be inferred even with the privacy-preserving approaches. Then, we
define viewpoint leakage probability to characterize the accuracy of the
inferred viewpoint, and respectively derive the probability when uploading
prediction error and QoE metric. We find that the viewpoint leakage probability
can be reduced by sacrificing QoE or increasing resources. Simulation with the
state-of-the-art predictor over a real dataset shows that such a tradeoff does
not exist only in rare cases.","['Xing Wei', 'Chenyang Yang', 'Chengjian Sun']",2022-03-07T02:52:21Z,http://arxiv.org/abs/2203.03107v2,"['cs.MM', 'cs.NI', 'cs.SY', 'eess.SY']","Proactive VR streaming,Privacy leakage,Viewpoint leakage,Privacy-preserving approaches,Quality of experience (QoE),Prediction error,Federated learning,Individual training,Predictors,Simulation"
"Distributed On-Sensor Compute System for AR/VR Devices: A
  Semi-Analytical Simulation Framework for Power Estimation","Augmented Reality/Virtual Reality (AR/VR) glasses are widely foreseen as the
next generation computing platform. AR/VR glasses are a complex ""system of
systems"" which must satisfy stringent form factor, computing-, power- and
thermal- requirements. In this paper, we will show that a novel distributed
on-sensor compute architecture, coupled with new semiconductor technologies
(such as dense 3D-IC interconnects and Spin-Transfer Torque Magneto Random
Access Memory, STT-MRAM) and, most importantly, a full hardware-software
co-optimization are the solutions to achieve attractive and socially acceptable
AR/VR glasses. To this end, we developed a semi-analytical simulation framework
to estimate the power consumption of novel AR/VR distributed on-sensor
computing architectures. The model allows the optimization of the main
technological features of the system modules, as well as the computer-vision
algorithm partition strategy across the distributed compute architecture. We
show that, in the case of the compute-intensive machine learning based Hand
Tracking algorithm, the distributed on-sensor compute architecture can reduce
the system power consumption compared to a centralized system, with the
additional benefits in terms of latency and privacy.","['Jorge Gomez', 'Saavan Patel', 'Syed Shakib Sarwar', 'Ziyun Li', 'Raffaele Capoccia', 'Zhao Wang', 'Reid Pinkham', 'Andrew Berkovich', 'Tsung-Hsun Tsai', 'Barbara De Salvo', 'Chiao Liu']",2022-03-14T20:18:24Z,http://arxiv.org/abs/2203.07474v1,"['cs.AR', 'cs.LG']","Distributed On-Sensor Compute System,AR/VR Devices,Semi-Analytical Simulation Framework,Power Estimation,Semiconductor Technologies,3D-IC Interconnects,Spin-Transfer Torque MRAM,Hardware-Software Co-optimization,Computer-Vision Algorithm,Machine Learning"
"Spot the Difference: A Cooperative Object-Referring Game in
  Non-Perfectly Co-Observable Scene","Visual dialog has witnessed great progress after introducing various
vision-oriented goals into the conversation, especially such as GuessWhich and
GuessWhat, where the only image is visible by either and both of the questioner
and the answerer, respectively. Researchers explore more on visual dialog tasks
in such kind of single- or perfectly co-observable visual scene, while somewhat
neglect the exploration on tasks of non perfectly co-observable visual scene,
where the images accessed by two agents may not be exactly the same, often
occurred in practice. Although building common ground in non-perfectly
co-observable visual scene through conversation is significant for advanced
dialog agents, the lack of such dialog task and corresponding large-scale
dataset makes it impossible to carry out in-depth research. To break this
limitation, we propose an object-referring game in non-perfectly co-observable
visual scene, where the goal is to spot the difference between the similar
visual scenes through conversing in natural language. The task addresses
challenges of the dialog strategy in non-perfectly co-observable visual scene
and the ability of categorizing objects. Correspondingly, we construct a
large-scale multimodal dataset, named SpotDiff, which contains 87k Virtual
Reality images and 97k dialogs generated by self-play. Finally, we give
benchmark models for this task, and conduct extensive experiments to evaluate
its performance as well as analyze its main challenges.","['Duo Zheng', 'Fandong Meng', 'Qingyi Si', 'Hairun Fan', 'Zipeng Xu', 'Jie Zhou', 'Fangxiang Feng', 'Xiaojie Wang']",2022-03-16T02:55:33Z,http://arxiv.org/abs/2203.08362v1,"['cs.CV', 'cs.CL']","visual dialog,object-referring game,non-perfectly co-observable scene,dialog strategy,categorizing objects,multimodal dataset,Virtual Reality images,benchmark models,natural language,large-scale dataset"
"MIDAS: Multi-sensorial Immersive Dynamic Autonomous System Improves
  Motivation of Stroke Affected Patients for Hand Rehabilitation","Majority of stroke survivors are left with poorly functioning paretic hands.
Current rehabilitation devices have failed to motivate the patients enough to
continue rehabilitation exercises. The objective of this project, MIDAS
(Multi-sensorial Immersive Dynamic Autonomous System) is a proof of concept by
using an immersive system to improve motivation of stroke patients for hand
rehabilitation. MIDAS is intended for stroke patients who suffer from light to
mild stroke. MIDAS is lightweight and portable. It consists of a hand
exoskeleton subsystem, a Virtual Reality (VR) subsystem, and an olfactory
subsystem. Altogether, MIDAS engages four out of five senses during
rehabilitation. To evaluate the efficacy of MIDAS a pilot study consisting of
three sessions is carried out on five stroke affected patients. Subsystems of
MIDAS are added progressively in each session. The game environment, sonic
effects, and scent released is carefully chosen to enhance the immersive
experience. 60% of the scores of user experience are above 40 (out of 56). 96%
Self Rehabilitation Motivation Scale (SRMS) rating shows that the participants
are motivated to use MIDAS and 87% rating shows that MIDAS is exciting for
rehabilitation. Participants experienced elevated motivation to continue stroke
rehabilitation using MIDAS and no undesired side effects were reported.","['Fok-Chi-Seng Fok Kow', 'Anoop Kumar Sinha', 'Zhang Jin Ming', 'Bao Songyu', 'Jake Tan Jun Kang', 'Hong Yan Jack Jeffrey', 'Galina Mihaleva', 'Nadia Magnenat Thalmann', 'Yiyu Cai']",2022-03-20T12:00:05Z,http://arxiv.org/abs/2203.10536v1,"['cs.RO', 'cs.HC']","Multi-sensorial,Immersive,Autonomous System,Stroke,Rehabilitation,Exoskeleton,Virtual Reality,Olfactory,Pilot Study,User Experience"
"Energy Efficient VM Placement in a Heterogeneous Fog Computing
  Architecture","Recent years have witnessed a remarkable development in communication and
computing systems, mainly driven by the increasing demands of data and
processing intensive applications such as virtual reality, M2M, connected
vehicles, IoT services, to name a few. Massive amounts of data will be
collected by various mobile and fixed terminals that will need to be processed
in order to extract knowledge from the data. Traditionally, a centralized
approach is taken for processing the collected data using large data centers
connected to a core network. However, due to the scale of the
Internet-connected things, transporting raw data all the way to the core
network is costly in terms of the power consumption, delay, and privacy. This
has compelled researchers to propose different decentralized computing
paradigms such as fog computing to process collected data at the network edge
close to the terminals and users. In this paper, we study, in a Passive Optical
Network (PON)-based collaborative-fog computing system, the impact of the
heterogeneity of the fog units capacity and energy-efficiency on the overall
energy-efficiency of the fog system. We optimized the virtual machine (VM)
placement in this fog system with three fog cells and formulated the problem as
a mixed integer linear programming (MILP) optimization model with the objective
of minimizing the networking and processing power consumption of the fog
system. The results indicate that in our proposed architecture, the processing
power consumption is the crucial element to achieve energy efficient VMs
placement.","['Abdullah M. Alqahtani', 'Barzan Yosuf', 'Sanaa H. Mohamed', 'Taisir E. H. El-Gorashi', 'Jaafar M. H. Elmirghani']",2022-03-27T00:51:28Z,http://arxiv.org/abs/2203.14178v1,"['cs.NI', 'eess.SP']","energy efficient,VM placement,heterogeneous,fog computing,architecture,virtual machine,optimization model,processing power consumption,networking,fog system"
"Human Instance Segmentation and Tracking via Data Association and
  Single-stage Detector","Human video instance segmentation plays an important role in computer
understanding of human activities and is widely used in video processing, video
surveillance, and human modeling in virtual reality. Most current VIS methods
are based on Mask-RCNN framework, where the target appearance and motion
information for data matching will increase computational cost and have an
impact on segmentation real-time performance; on the other hand, the existing
datasets for VIS focus less on all the people appearing in the video. In this
paper, to solve the problems, we develop a new method for human video instance
segmentation based on single-stage detector. To tracking the instance across
the video, we have adopted data association strategy for matching the same
instance in the video sequence, where we jointly learn target instance
appearances and their affinities in a pair of video frames in an end-to-end
fashion. We have also adopted the centroid sampling strategy for enhancing the
embedding extraction ability of instance, which is to bias the instance
position to the inside of each instance mask with heavy overlap condition. As a
result, even there exists a sudden change in the character activity, the
instance position will not move out of the mask, so that the problem that the
same instance is represented by two different instances can be alleviated.
Finally, we collect PVIS dataset by assembling several video instance
segmentation datasets to fill the gap of the current lack of datasets dedicated
to human video segmentation. Extensive simulations based on such dataset has
been conduct. Simulation results verify the effectiveness and efficiency of the
proposed work.","['Lu Cheng', 'Mingbo Zhao']",2022-03-31T11:36:09Z,http://arxiv.org/abs/2203.16966v1,"['cs.CV', 'cs.AI']","Instance Segmentation,Data Association,Single-stage Detector,Video Processing,Human Modeling,Virtual Reality,Affinities,Centroid Sampling Strategy,Embedding Extraction,PVIS Dataset"
"Collaborative Learning for Hand and Object Reconstruction with
  Attention-guided Graph Convolution","Estimating the pose and shape of hands and objects under interaction finds
numerous applications including augmented and virtual reality. Existing
approaches for hand and object reconstruction require explicitly defined
physical constraints and known objects, which limits its application domains.
Our algorithm is agnostic to object models, and it learns the physical rules
governing hand-object interaction. This requires automatically inferring the
shapes and physical interaction of hands and (potentially unknown) objects. We
seek to approach this challenging problem by proposing a collaborative learning
strategy where two-branches of deep networks are learning from each other.
Specifically, we transfer hand mesh information to the object branch and vice
versa for the hand branch. The resulting optimisation (training) problem can be
unstable, and we address this via two strategies: (i) attention-guided graph
convolution which helps identify and focus on mutual occlusion and (ii)
unsupervised associative loss which facilitates the transfer of information
between the branches. Experiments using four widely-used benchmarks show that
our framework achieves beyond state-of-the-art accuracy in 3D pose estimation,
as well as recovers dense 3D hand and object shapes. Each technical component
above contributes meaningfully in the ablation study.","['Tze Ho Elden Tse', 'Kwang In Kim', 'Ales Leonardis', 'Hyung Jin Chang']",2022-04-27T17:00:54Z,http://arxiv.org/abs/2204.13062v1,['cs.CV'],"Collaborative Learning,Hand Reconstruction,Object Reconstruction,Attention-guided Graph Convolution,Deep Networks,Hand Mesh,Optimization,Graph Convolution,3D Pose Estimation,Ablation Study"
"Virtual and Augmented Reality-Based Assistive Interfaces for Upper-limb
  Prosthesis Control and Rehabilitation","Functional upper-limb prosthetic training can improve users performance in
controlling prostheses and has been incorporated into occupational therapy for
individuals in need. In recent years, virtual reality (VR) and augmented
reality (AR) technologies have been shown to be promising avenues to improve
the convenience of rehabilitative prosthesis training systems. However, it is
uncertain if the comprehensive efficacy and effectiveness of VR or AR assistive
tools are adequate compared to conventional prosthetic tools and if not,
whether enhancements can be made through incorporation of other technical
paradigms.
  This work first presents a mixed reality system we developed for prosthesis
control and training. Five able-bodied subjects are involved to perform
three-dimensional object manipulation tasks in analogous AR and VR
environments. Multiple evaluation metrics are applied to assess subjects
performances within the two paradigms. Based on the comparative analysis, we
find that VR-based environment promotes more efficient motion along with higher
task completion rate and path efficiency while AR paradigm allows subjects to
perform motor tasks with shorter time consumed. Another study is conducted to
evaluate the efficiency and feasibility of AR-facilitated prosthesis control
system compared to that in real-world and if any technical additions can be
applied to improve the AR-based system. Three able-bodied subjects were engaged
in the experiment to perform object manipulation tasks in a) physical
environment, b) AR-without-bypass environment, and c) AR-with-bypass
environment. Based on the results obtained from the assessment, we conclude
that while our AR-based system modestly lags behind the effectiveness of
physical systems, the study conducted using a bypass prosthesis suggests that
AR system has the potential to improve the efficacy of prosthesis control.",['Yinghe Sun'],2022-04-28T03:26:12Z,http://arxiv.org/abs/2205.02227v1,['cs.HC'],"Virtual reality,augmented reality,upper-limb prosthesis,assistive interfaces,rehabilitation,training systems,mixed reality,three-dimensional object manipulation,evaluation metrics"
"Metaversal Learning Environments: Measuring, predicting and improving
  interpersonal effectiveness","Experiential learning has been known to be an engaging and effective modality
for personal and professional development. The Metaverse provides ample
opportunities for the creation of environments in which such experiential
learning can occur. In this work, we introduce a novel architecture that
combines Artificial intelligence and Virtual Reality to create a highly
immersive and efficient learning experience using avatars. The framework allows
us to measure the interpersonal effectiveness of an individual interacting with
the avatar. We first present a small pilot study and its results which were
used to enhance the framework. We then present a larger study using the
enhanced framework to measure, assess, and predict the interpersonal
effectiveness of individuals interacting with an avatar. Results reveal that
individuals with deficits in their interpersonal effectiveness show a
significant improvement in performance after multiple interactions with an
avatar. The results also reveal that individuals interact naturally with
avatars within this framework, and exhibit similar behavioral traits as they
would in the real world. We use this as a basis to analyze the underlying audio
and video data streams of individuals during these interactions. Finally, we
extract relevant features from these data and present a machine-learning based
approach to predict interpersonal effectiveness during human-avatar
conversation. We conclude by discussing the implications of these findings to
build beneficial applications for the real world.","['Arjun Nagendran', 'Scott Compton', 'William Follette', 'Artem Golenchenko', 'Anna Compton', 'Jonathan Grizou']",2022-05-05T18:22:27Z,http://arxiv.org/abs/2205.02875v1,"['cs.AI', 'cs.GR', 'I.2; I.3; J.4']","Metaversal Learning Environments,Experiential learning,Metaverse,Artificial intelligence,Virtual Reality,Avatars,Interpersonal effectiveness,Pilot study,Machine-learning,Human-avatar conversation"
"PointDistiller: Structured Knowledge Distillation Towards Efficient and
  Compact 3D Detection","The remarkable breakthroughs in point cloud representation learning have
boosted their usage in real-world applications such as self-driving cars and
virtual reality. However, these applications usually have an urgent requirement
for not only accurate but also efficient 3D object detection. Recently,
knowledge distillation has been proposed as an effective model compression
technique, which transfers the knowledge from an over-parameterized teacher to
a lightweight student and achieves consistent effectiveness in 2D vision.
However, due to point clouds' sparsity and irregularity, directly applying
previous image-based knowledge distillation methods to point cloud detectors
usually leads to unsatisfactory performance. To fill the gap, this paper
proposes PointDistiller, a structured knowledge distillation framework for
point clouds-based 3D detection. Concretely, PointDistiller includes local
distillation which extracts and distills the local geometric structure of point
clouds with dynamic graph convolution and reweighted learning strategy, which
highlights student learning on the crucial points or voxels to improve
knowledge distillation efficiency. Extensive experiments on both voxels-based
and raw points-based detectors have demonstrated the effectiveness of our
method over seven previous knowledge distillation methods. For instance, our 4X
compressed PointPillars student achieves 2.8 and 3.4 mAP improvements on BEV
and 3D object detection, outperforming its teacher by 0.9 and 1.8 mAP,
respectively. Codes have been released at
https://github.com/RunpeiDong/PointDistiller.","['Linfeng Zhang', 'Runpei Dong', 'Hung-Shuo Tai', 'Kaisheng Ma']",2022-05-23T07:40:07Z,http://arxiv.org/abs/2205.11098v1,"['cs.CV', 'cs.LG']","point cloud representation learning,3D object detection,knowledge distillation,model compression,point clouds,structured knowledge distillation,dynamic graph convolution,reweighted learning strategy,voxels-based detectors,raw points-based detectors"
Dynamic Control of Data-Intensive Services over Edge Computing Networks,"Next-generation distributed computing networks (e.g., edge and fog computing)
enable the efficient delivery of delay-sensitive, compute-intensive
applications by facilitating access to computation resources in close proximity
to end users. Many of these applications (e.g., augmented/virtual reality) are
also data-intensive: in addition to user-specific (live) data streams, they
require access to (static) digital objects (e.g., image database) to complete
the required processing tasks. When required objects are not available at the
servers hosting the associated service functions, they must be fetched from
other edge locations, incurring additional communication cost and latency. In
such settings, overall service delivery performance shall benefit from jointly
optimized decisions around (i) routing paths and processing locations for live
data streams, together with (ii) cache selection and distribution paths for
associated digital objects. In this paper, we address the problem of dynamic
control of data-intensive services over edge cloud networks. We characterize
the network stability region and design the first throughput-optimal control
policy that coordinates processing and routing decisions for both live and
static data-streams. Numerical results demonstrate the superior performance
(e.g., throughput, delay, and resource consumption) obtained via the novel
multi-pipeline flow control mechanism of the proposed policy, compared with
state-of-the-art algorithms that lack integrated stream processing and data
distribution control.","['Yang Cai', 'Jaime Llorca', 'Antonia M. Tulino', 'Andreas F. Molisch']",2022-05-29T18:42:00Z,http://arxiv.org/abs/2205.14735v1,"['cs.NI', 'cs.SY', 'eess.SY']","Edge Computing,Data-Intensive Services,Distributed Computing Networks,Delay-Sensitive Applications,Computation Resources,Digital Objects,Routing Paths,Cache Selection,Throughput-Optimal Control Policy"
Insertion of real agents behaviors in CARLA autonomous driving simulator,"The role of simulation in autonomous driving is becoming increasingly
important due to the need for rapid prototyping and extensive testing. The use
of physics-based simulation involves multiple benefits and advantages at a
reasonable cost while eliminating risks to prototypes, drivers and vulnerable
road users. However, there are two main limitations. First, the well-known
reality gap which refers to the discrepancy between reality and simulation that
prevents simulated autonomous driving experience from enabling effective
real-world performance. Second, the lack of empirical knowledge about the
behavior of real agents, including backup drivers or passengers and other road
users such as vehicles, pedestrians or cyclists. Agent simulation is usually
pre-programmed deterministically, randomized probabilistically or generated
based on real data, but it does not represent behaviors from real agents
interacting with the specific simulated scenario. In this paper we present a
preliminary framework to enable real-time interaction between real agents and
the simulated environment (including autonomous vehicles) and generate
synthetic sequences from simulated sensor data from multiple views that can be
used for training predictive systems that rely on behavioral models. Our
approach integrates immersive virtual reality and human motion capture systems
with the CARLA simulator for autonomous driving. We describe the proposed
hardware and software architecture, and discuss about the so-called behavioural
gap or presence. We present preliminary, but promising, results that support
the potential of this methodology and discuss about future steps.","['Sergio Martín Serrano', 'David Fernández Llorca', 'Iván García Daza', 'Miguel Ángel Sotelo']",2022-06-01T09:03:05Z,http://arxiv.org/abs/2206.00337v2,['cs.RO'],"autonomous driving,simulator,simulation,real agents,behavior,agent simulation,CARLA,sensor data,predictive systems,virtual reality"
"6G Survey on Challenges, Requirements, Applications, Key Enabling
  Technologies, Use Cases, AI integration issues and Security aspects","The fifth-generation (5G) network is likely to bring in high data rates, more
reliability, and low delays for mobile, personal and local area networks.
Alongside the rapid growth of smart wireless sensing and communication
technologies, data traffic has significantly risen, and existing 5G networks
are not fully capable of supporting future massive data traffic in terms of
services, storage, and processing. To meet the forthcoming challenges, the
research community is investigating the Terahertz-based sixth-generation (6G)
wireless network which is supposed to be offered for industrial usage in around
10 years. This is the right time to explore and learn about various 6G aspects
that will play a key role in the successful execution and implementation of 6G
networks in the future. This survey provides a review of specifications,
requirements, applications, enabling technologies including disruptive and
innovative, integration of 6G with advanced architectures and networks like
software-defined networks (SDN), network functions virtualization (NFV),
cloud/fog computing, etc, artificial intelligence (AI) oriented technologies,
privacy and security issues and solutions, and potential futuristic use cases:
virtual reality, smart healthcare and Industry 5.0. Furthermore, based on the
conducted review, challenges and future research directions are highlighted to
aid the deployment of 6G networks.","['Muhammad Sajjad Akbar', 'Zawar Hussain', 'Quan Z. Sheng', 'Subhas Mukhopadhyay']",2022-06-02T04:34:57Z,http://arxiv.org/abs/2206.00868v1,['cs.NI'],"5G,6G,Terahertz,wireless network,AI,security,virtual reality,smart healthcare,Industry 5.0,network functions virtualization (NFV)"
Volumetric Disentanglement for 3D Scene Manipulation,"Recently, advances in differential volumetric rendering enabled significant
breakthroughs in the photo-realistic and fine-detailed reconstruction of
complex 3D scenes, which is key for many virtual reality applications. However,
in the context of augmented reality, one may also wish to effect semantic
manipulations or augmentations of objects within a scene. To this end, we
propose a volumetric framework for (i) disentangling or separating, the
volumetric representation of a given foreground object from the background, and
(ii) semantically manipulating the foreground object, as well as the
background. Our framework takes as input a set of 2D masks specifying the
desired foreground object for training views, together with the associated 2D
views and poses, and produces a foreground-background disentanglement that
respects the surrounding illumination, reflections, and partial occlusions,
which can be applied to both training and novel views. Our method enables the
separate control of pixel color and depth as well as 3D similarity
transformations of both the foreground and background objects. We subsequently
demonstrate the applicability of our framework on a number of downstream
manipulation tasks including object camouflage, non-negative 3D object
inpainting, 3D object translation, 3D object inpainting, and 3D text-based
object manipulation. Full results are given in our project webpage at
https://sagiebenaim.github.io/volumetric-disentanglement/","['Sagie Benaim', 'Frederik Warburg', 'Peter Ebert Christensen', 'Serge Belongie']",2022-06-06T17:57:07Z,http://arxiv.org/abs/2206.02776v1,['cs.CV'],"volumetric rendering,3D scene manipulation,semantic manipulations,foreground object,background,2D masks,pixel color,depth,3D object inpainting,3D object translation"
"Vehicle-To-Pedestrian Communication Feedback Module: A Study on
  Increasing Legibility, Public Acceptance and Trust","Vehicle pedestrian communication is extremely important when developing
autonomy for an autonomous vehicle. Enabling bidirectional nonverbal
communication between pedestrians and autonomous vehicles will lead to an
improvement of pedestrians' safety in autonomous driving. If a pedestrian wants
to communicate, the autonomous vehicle should provide feedback to the human
about what it is about to do. The user study presented in this paper
investigated several possible options for an external vehicle display for
effective nonverbal communication between an autonomous vehicle and a human.
The result of this study will guide the development of the feedback module in
future studies, optimizing for public acceptance and trust in the autonomous
vehicle's decision while being legible to the widest range of potential users.
The results of this study show that participants prefer symbols over text,
lights and road projection. Additionally, participants prefer the combination
of symbols and text as interaction modes to be displayed if the autonomous
vehicle is not driving. Further, the results show that the text interaction
mode option ""Safe to cross"" should be used combined with the symbol interaction
mode option that displays a symbol of a walking person. We plan to elaborate
and focus on the selected interaction modes via Virtual Reality and in the real
world in ongoing and future studies.","['Melanie Schmidt-Wolf', 'David Feil-Seifer']",2022-06-10T18:18:09Z,http://arxiv.org/abs/2206.05312v2,['cs.HC'],"Vehicle-to-pedestrian communication,feedback module,autonomy,nonverbal communication,pedestrians' safety,user study,external vehicle display,symbols,text interaction mode"
"The Effects of Spatial Configuration on Relative Translation Gain
  Thresholds in Redirected Walking","In this study, we explore how spatial configurations can be reflected in
determining the threshold range of Relative Translation Gains (RTGs), a
translation gain-based Redirected Walking (RDW) technique that scales the
user's movement in Virtual Reality (VR) in different ratios for width and
depth. While previous works have shown that various cognitive factors or
individual differences influence the RDW threshold, constructive studies
investigating the impact of the environmental composition on the RDW threshold
with regard to the user's visual perception were lacking. Therefore, we
examined the effect of spatial configurations on the RTG threshold by analyzing
the participant's responses and gaze distribution data in two user studies. The
first study concerned the size of the virtual room and the existence of objects
within it, and the second study focused on the combined impact of room size and
the spatial layout. Our results show that three compositions of spatial
configuration (size, object existence, spatial layout) significantly affect the
RTG threshold range. Based on our findings, we proposed virtual space rescaling
guidelines to increase the range of adjustable movable space with RTGs for
developers: placing distractors in the room, setting the perceived movable
space to be larger than the adjusted movable space if it's an empty room, and
avoid placing objects together as centered layout. Our findings can be used to
adaptively rescale VR users' space according to the target virtual space's
configuration with a unified coordinate system that enables the utilization of
physical objects in a virtual scene.","['Dooyoung Kim', 'Seonji Kim', 'Jae-eun Shin', 'Boram Yoon', 'Jinwook Kim', 'Jeongmi Lee', 'Woontack Woo']",2022-06-11T13:08:59Z,http://arxiv.org/abs/2206.05522v2,['cs.HC'],"spatial configuration,relative translation gain,redirected walking,virtual reality,threshold range,user study,gaze distribution,virtual space,rescaling,adjustable movable space"
"Visual Guidance for User Placement in Avatar-Mediated Telepresence
  between Dissimilar Spaces","Rapid advances in technology gradually realize immersive mixed-reality (MR)
telepresence between distant spaces. This paper presents a novel visual
guidance system for avatar-mediated telepresence, directing users to optimal
placements that facilitate the clear transfer of gaze and pointing contexts
through remote avatars in dissimilar spaces, where the spatial relationship
between the remote avatar and the interaction targets may differ from that of
the local user. Representing the spatial relationship between the user/avatar
and interaction targets with angle-based interaction features, we assign
recommendation scores of sampled local placements as their maximum feature
similarity with remote placements. These scores are visualized as color-coded
2D sectors to inform the users of better placements for interaction with
selected targets. In addition, virtual objects of the remote space are
overlapped with the local space for the user to better understand the
recommendations. We examine whether the proposed score measure agrees with the
actual user perception of the partner's interaction context and find a score
threshold for recommendation through user experiments in virtual reality (VR).
A subsequent user study in VR investigates the effectiveness and perceptual
overload of different combinations of visualizations. Finally, we conduct a
user study in an MR telepresence scenario to evaluate the effectiveness of our
method in real-world applications.","['Dongseok Yang', 'Jiho Kang', 'Taehei Kim', 'Sung-Hee Lee']",2022-06-20T02:41:39Z,http://arxiv.org/abs/2206.09542v3,"['cs.HC', 'cs.GR']","visual guidance,user placement,avatar-mediated telepresence,mixed-reality,remote avatars,interaction targets,angle-based interaction features,recommendation scores,virtual objects,user study"
"Learn to Predict How Humans Manipulate Large-sized Objects from
  Interactive Motions","Understanding human intentions during interactions has been a long-lasting
theme, that has applications in human-robot interaction, virtual reality and
surveillance. In this study, we focus on full-body human interactions with
large-sized daily objects and aim to predict the future states of objects and
humans given a sequential observation of human-object interaction. As there is
no such dataset dedicated to full-body human interactions with large-sized
daily objects, we collected a large-scale dataset containing thousands of
interactions for training and evaluation purposes. We also observe that an
object's intrinsic physical properties are useful for the object motion
prediction, and thus design a set of object dynamic descriptors to encode such
intrinsic properties. We treat the object dynamic descriptors as a new modality
and propose a graph neural network, HO-GCN, to fuse motion data and dynamic
descriptors for the prediction task. We show the proposed network that consumes
dynamic descriptors can achieve state-of-the-art prediction results and help
the network better generalize to unseen objects. We also demonstrate the
predicted results are useful for human-robot collaborations.","['Weilin Wan', 'Lei Yang', 'Lingjie Liu', 'Zhuoying Zhang', 'Ruixing Jia', 'Yi-King Choi', 'Jia Pan', 'Christian Theobalt', 'Taku Komura', 'Wenping Wang']",2022-06-25T09:55:39Z,http://arxiv.org/abs/2206.12612v1,"['cs.CV', 'cs.HC']","human-robot interaction,virtual reality,surveillance,full-body interaction,large-scale dataset,object motion prediction,dynamic descriptors,graph neural network"
"Immersive and Interactive Visualization of 3D Spatio-Temporal Data using
  a Space Time Hypercube","We propose an extension of the well-known Space-Time Cube (STC) visualization
technique in order to visualize time-varying 3D spatial data, taking advantage
of the interaction capabilities of Virtual Reality (VR). The analysis of
multidimensional time-varying datasets, which size grows as recording and
simulating techniques advance, faces challenges on the representation and
visualization of dense data, as well as on the study of temporal variations.
First, we propose the Space-Time Hypercube (STH) as an abstraction for 3D
temporal data, extended from the STC concept. Second, through the example of
embryo development imaging dataset, we detail the construction and
visualization of a STC based on a user-driven projection of the spatial and
temporal information. This projection yields a 3D STC visualization, which can
also encode additional numerical and categorical data. Additionally, we propose
a set of tools allowing the user to filter and manipulate the 3D STC which
benefits from the visualization, exploration and interaction possibilities
offered by VR. Finally, we evaluated the proposed visualization method in the
context of the visualization of spatio-temporal biological data. Several
biology experts accompanied the application design to provide insight on how
the STC visualization could be used to explore such data. We report a user
study (n=12) using non-expert users performing a set of exploration and query
tasks to evaluate the system.","['Gwendal Fouché', 'Ferran Argelaguet', 'Emmanuel Faure', 'Charles Kervrann']",2022-06-27T12:01:17Z,http://arxiv.org/abs/2206.13213v1,"['cs.HC', 'cs.GR']","Immersive visualization,Interactive visualization,3D data,Spatio-temporal data,Space Time Hypercube,Virtual Reality,Visualization technique,Time-varying datasets,Multidimensional data,Embryo development."
Interactive Physically-Based Simulation of Roadheader Robot,"Roadheader is an engineering robot widely used in underground engineering and
mining industry. Interactive dynamics simulation of roadheader is a fundamental
problem in unmanned excavation and virtual reality training. However, current
research is only based on traditional animation techniques or commercial game
engines. There are few studies that apply real-time physical simulation of
computer graphics to the field of roadheader robot. This paper aims to present
an interactive physically-based simulation system of roadheader robot. To this
end, an improved multibody simulation method based on generalized coordinates
is proposed. First, our simulation method describes robot dynamics based on
generalized coordinates. Compared to state-of-the-art methods, our method is
more stable and accurate. Numerical simulation results showed that our method
has significantly less error than the game engine in the same number of
iterations. Second, we adopt the symplectic Euler integrator instead of the
conventional fourth-order Runge-Kutta (RK4) method for dynamics iteration.
Compared with other integrators, our method is more stable in energy drift
during long-term simulation. The test results showed that our system achieved
real-time interaction performance of 60 frames per second (fps). Furthermore,
we propose a model format for geometric and robotics modeling of roadheaders to
implement the system. Our interactive simulation system of roadheader meets the
requirements of interactivity, accuracy and stability.","['Shengzhe Hou', 'Xinming Lu', 'Wenli Gao', 'Shuai Jiang', 'Xingli Zhang']",2022-06-29T14:33:50Z,http://arxiv.org/abs/2206.15429v1,"['cs.RO', 'cs.GR']","physically-based simulation,roadheader robot,multibody simulation,generalized coordinates,symplectic Euler integrator,dynamics iteration,real-time interaction,geometric modeling,robotics modeling"
Visual-Assisted Sound Source Depth Estimation in the Wild,"Depth estimation enables a wide variety of 3D applications, such as robotics,
autonomous driving, and virtual reality. Despite significant work in this area,
it remains open how to enable accurate, low-cost, high-resolution, and
large-range depth estimation. Inspired by the flash-to-bang phenomenon (i.e.
hearing the thunder after seeing the lightning), this paper develops FBDepth,
the first audio-visual depth estimation framework. It takes the difference
between the time-of-flight (ToF) of the light and the sound to infer the sound
source depth. FBDepth is the first to incorporate video and audio with both
semantic features and spatial hints for range estimation. It first aligns
correspondence between the video track and audio track to locate the target
object and target sound in a coarse granularity. Based on the observation of
moving objects' trajectories, FBDepth proposes to estimate the intersection of
optical flow before and after the sound production to locate video events in
time. FBDepth feeds the estimated timestamp of the video event and the audio
clip for the final depth estimation. We use a mobile phone to collect 3000+
video clips with 20 different objects at up to $50m$. FBDepth decreases the
Absolute Relative error (AbsRel) by 55\% compared to RGB-based methods.","['Wei Sun', 'Lili Qiu']",2022-07-07T03:58:19Z,http://arxiv.org/abs/2207.03074v2,"['cs.SD', 'eess.AS', 'eess.IV']","depth estimation,audio-visual,time-of-flight,ToF,optical flow,spatial hints,semantic features,mobile phone,Absolute Relative error,RGB-based methods"
"Technical Report: Comparative Evaluation of AR-based, VR-based, and
  Traditional Basic Life Support Training","Basic life support (BLS) is crucial in the emergency response system as
sudden cardiac arrest is still a major cause of death worldwide. In the
majority of cases, cardiac arrest is witnessed out-of-hospital where execution
of BLS including resuscitation through by-standers gets indispensable. However,
survival rates of cardiac arrest victims could majorly increase if BLS skills
would be trained regularly. In this context, technology-enhanced BLS training
approaches utilizing augmented (AR) and virtual reality (VR) have been proposed
in recent works. However, these approaches are not compliant with the medical
BLS guidelines or focus only on specific steps of BLS training such as
resuscitation. Furthermore, most of the existing training approaches do not
focus on automated assessment to enhance efficiency and effectiveness through
fine-grained real-time feedback. To overcome these issues, we present a novel
AR- and VR-based training environment which supports a comprehensive BLS
training compliant with the medical guidelines. Our training environment
combines AR-/VR-based BLS training with an interactive haptic manikin that
supports automated assessment, real-time feedback, and debriefing in an
integrated environment. We have conducted a usability evaluation where we
analyze the efficiency, effectiveness, and user satisfaction of BLS training
based on our AR and VR environment against traditional BLS training. Results of
the evaluation indicate that AR and VR technology have the potential to
increase engagement in BLS training, improve high-quality resuscitation, and
reduce the cognitive workload compared to traditional training.","['Enes Yigitbas', 'Sebastian Krois', 'Timo Renzelmann', 'Gregor Engels']",2022-07-07T14:07:12Z,http://arxiv.org/abs/2207.03306v1,['cs.HC'],"AR-based,VR-based,traditional,basic life support training,resuscitation,automated assessment,real-time feedback,medical guidelines,usability evaluation"
The SPEC-RG Reference Architecture for the Compute Continuum,"As the next generation of diverse workloads like autonomous driving and
augmented/virtual reality evolves, computation is shifting from cloud-based
services to the edge, leading to the emergence of a cloud-edge compute
continuum. This continuum promises a wide spectrum of deployment opportunities
for workloads that can leverage the strengths of cloud (scalable
infrastructure, high reliability) and edge (energy efficient, low latencies).
Despite its promises, the continuum has only been studied in silos of various
computing models, thus lacking strong end-to-end theoretical and engineering
foundations for computing and resource management across the continuum.
Consequently, developers resort to ad hoc approaches to reason about
performance and resource utilization of workloads in the continuum. In this
work, we conduct a first-of-its-kind systematic study of various computing
models, identify salient properties, and make a case to unify them under a
compute continuum reference architecture. This architecture provides an
end-to-end analysis framework for developers to reason about resource
management, workload distribution, and performance analysis. We demonstrate the
utility of the reference architecture by analyzing two popular continuum
workloads, deep learning and industrial IoT. We have developed an accompanying
deployment and benchmarking framework and first-order analytical model for
quantitative reasoning of continuum workloads. The framework is open-sourced
and available at https://github.com/atlarge-research/continuum.","['Matthijs Jansen', 'Auday Al-Dulaimy', 'Alessandro V. Papadopoulos', 'Animesh Trivedi', 'Alexandru Iosup']",2022-07-09T00:05:16Z,http://arxiv.org/abs/2207.04159v3,['cs.DC'],"SPEC-RG,Reference Architecture,Compute Continuum,Cloud-edge,Workloads,Resource Management,Deployment,Benchmarking,Analytical Model,Continuum Workloads."
Mixed vine copula flows for flexible modelling of neural dependencies,"Recordings of complex neural population responses provide a unique
opportunity for advancing our understanding of neural information processing at
multiple scales and improving performance of brain computer interfaces.
However, most existing analytical techniques fall short of capturing the
complexity of interactions within the concerted population activity. Vine
copula-based approaches have shown to be successful at addressing complex
high-order dependencies within the population, disentangled from the
single-neuron statistics. However, most applications have focused on parametric
copulas which bear the risk of misspecifying dependence structures. In order to
avoid this risk, we adopted a fully non-parametric approach for the
single-neuron margins and copulas by using Neural Spline Flows (NSF). We
validated the NSF framework on simulated data of continuous and discrete type
with various forms of dependency structures and with different dimensionality.
Overall, NSFs performed similarly to existing non-parametric estimators, while
allowing for considerably faster and more flexible sampling which also enables
faster Monte Carlo estimation of copula entropy. Moreover, our framework was
able to capture low and higher order heavy tail dependencies in neuronal
responses recorded in the mouse primary visual cortex during a visual learning
task while the animal was navigating a virtual reality environment. These
findings highlight an often ignored aspect of complexity in coordinated
neuronal activity which can be important for understanding and deciphering
collective neural dynamics for neurotechnological applications.","['Lazaros Mitskopoulos', 'Theoklitos Amvrosiadis', 'Arno Onken']",2022-07-11T12:59:13Z,http://arxiv.org/abs/2207.04832v1,['q-bio.NC'],"neural population responses,brain computer interfaces,vine copula,single-neuron statistics,non-parametric approach,Neural Spline Flows (NSF),simulated data,dependency structures,Monte Carlo estimation,visual learning task"
"AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
  Sensing","Today's Mixed Reality head-mounted displays track the user's head pose in
world space as well as the user's hands for interaction in both Augmented
Reality and Virtual Reality scenarios. While this is adequate to support user
input, it unfortunately limits users' virtual representations to just their
upper bodies. Current systems thus resort to floating avatars, whose limitation
is particularly evident in collaborative settings. To estimate full-body poses
from the sparse input sources, prior work has incorporated additional trackers
and sensors at the pelvis or lower body, which increases setup complexity and
limits practical application in mobile settings. In this paper, we present
AvatarPoser, the first learning-based method that predicts full-body poses in
world coordinates using only motion input from the user's head and hands. Our
method builds on a Transformer encoder to extract deep features from the input
signals and decouples global motion from the learned local joint orientations
to guide pose estimation. To obtain accurate full-body motions that resemble
motion capture animations, we refine the arm joints' positions using an
optimization routine with inverse kinematics to match the original tracking
input. In our evaluation, AvatarPoser achieved new state-of-the-art results in
evaluations on large motion capture datasets (AMASS). At the same time, our
method's inference speed supports real-time operation, providing a practical
interface to support holistic avatar control and representation for Metaverse
applications.","['Jiaxi Jiang', 'Paul Streli', 'Huajian Qiu', 'Andreas Fender', 'Larissa Laich', 'Patrick Snape', 'Christian Holz']",2022-07-27T20:52:39Z,http://arxiv.org/abs/2207.13784v1,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.HC', '68T07, 68T45, 68U01', 'I.2; I.3; I.4; I.5']","Mixed Reality,Pose Tracking,Motion Sensing,Avatar,Full-Body Poses,Transformer Encoder,Inverse Kinematics,Motion Capture,Metaverse,Sparse Input"
"Benchmarking Visual-Inertial Deep Multimodal Fusion for Relative Pose
  Regression and Odometry-aided Absolute Pose Regression","Visual-inertial localization is a key problem in computer vision and robotics
applications such as virtual reality, self-driving cars, and aerial vehicles.
The goal is to estimate an accurate pose of an object when either the
environment or the dynamics are known. Absolute pose regression (APR)
techniques directly regress the absolute pose from an image input in a known
scene using convolutional and spatio-temporal networks. Odometry methods
perform relative pose regression (RPR) that predicts the relative pose from a
known object dynamic (visual or inertial inputs). The localization task can be
improved by retrieving information from both data sources for a cross-modal
setup, which is a challenging problem due to contradictory tasks. In this work,
we conduct a benchmark to evaluate deep multimodal fusion based on pose graph
optimization and attention networks. Auxiliary and Bayesian learning are
utilized for the APR task. We show accuracy improvements for the APR-RPR task
and for the RPR-RPR task for aerial vehicles and hand-held devices. We conduct
experiments on the EuRoC MAV and PennCOSYVIO datasets and record and evaluate a
novel industry dataset.","['Felix Ott', 'Nisha Lakshmana Raichur', 'David Rügamer', 'Tobias Feigl', 'Heiko Neumann', 'Bernd Bischl', 'Christopher Mutschler']",2022-08-01T15:05:26Z,http://arxiv.org/abs/2208.00919v3,"['cs.CV', '68T40, 65D19', 'I.4; I.5.1']","visual-inertial,deep multimodal fusion,relative pose regression,odometry,absolute pose regression,convolutional networks,spatio-temporal networks,pose graph optimization,attention networks,Bayesian learning"
Segmented Learning for Class-of-Service Network Traffic Classification,"Class-of-service (CoS) network traffic classification (NTC) classifies a
group of similar traffic applications. The CoS classification is advantageous
in resource scheduling for Internet service providers and avoids the necessity
of remodelling. Our goal is to find a robust, lightweight, and fast-converging
CoS classifier that uses fewer data in modelling and does not require
specialized tools in feature extraction. The commonality of statistical
features among the network flow segments motivates us to propose novel
segmented learning that includes essential vector representation and a
simple-segment method of classification. We represent the segmented traffic in
the vector form using the EVR. Then, the segmented traffic is modelled for
classification using random forest. Our solution's success relies on finding
the optimal segment size and a minimum number of segments required in
modelling. The solution is validated on multiple datasets for various CoS
services, including virtual reality (VR). Significant findings of the research
work are i) Synchronous services that require acknowledgment and request to
continue communication are classified with 99% accuracy, ii) Initial 1,000
packets in any session are good enough to model a CoS traffic for promising
results, and we therefore can quickly deploy a CoS classifier, and iii) Test
results remain consistent even when trained on one dataset and tested on a
different dataset. In summary, our solution is the first to propose
segmentation learning NTC that uses fewer features to classify most CoS traffic
with an accuracy of 99%. The implementation of our solution is available on
GitHub.","['Yoga Suhas Kuruba Manjunath', 'Sihao Zhao', 'Hatem Abou-zeid', 'Akram Bin Sediq', 'Ramy Atawia', 'Xiao-Ping Zhang']",2022-08-03T00:26:56Z,http://arxiv.org/abs/2208.01793v1,['eess.SP'],"Class-of-service,Network traffic classification,Segmented learning,Feature extraction,Statistical features,Vector representation,Random forest,Segment size,Segments,CoS classifier"
RAZE: Region Guided Self-Supervised Gaze Representation Learning,"Automatic eye gaze estimation is an important problem in vision based
assistive technology with use cases in different emerging topics such as
augmented reality, virtual reality and human-computer interaction. Over the
past few years, there has been an increasing interest in unsupervised and
self-supervised learning paradigms as it overcomes the requirement of large
scale annotated data. In this paper, we propose RAZE, a Region guided
self-supervised gAZE representation learning framework which leverage from
non-annotated facial image data. RAZE learns gaze representation via auxiliary
supervision i.e. pseudo-gaze zone classification where the objective is to
classify visual field into different gaze zones (i.e. left, right and center)
by leveraging the relative position of pupil-centers. Thus, we automatically
annotate pseudo gaze zone labels of 154K web-crawled images and learn feature
representations via `Ize-Net' framework. `Ize-Net' is a capsule layer based CNN
architecture which can efficiently capture rich eye representation. The
discriminative behaviour of the feature representation is evaluated on four
benchmark datasets: CAVE, TabletGaze, MPII and RT-GENE. Additionally, we
evaluate the generalizability of the proposed network on two other downstream
task (i.e. driver gaze estimation and visual attention estimation) which
demonstrate the effectiveness of the learnt eye gaze representation.","['Neeru Dubey', 'Shreya Ghosh', 'Abhinav Dhall']",2022-08-04T06:23:49Z,http://arxiv.org/abs/2208.02485v2,['cs.CV'],"eye gaze estimation,self-supervised learning,representation learning,pseudo-gaze zone classification,pupil-centers,feature representations,capsule layer,CNN architecture,benchmark datasets,generalizability"
"CreatureShop: Interactive 3D Character Modeling and Texturing from a
  Single Color Drawing","Creating 3D shapes from 2D drawings is an important problem with applications
in content creation for computer animation and virtual reality. We introduce a
new sketch-based system, CreatureShop, that enables amateurs to create
high-quality textured 3D character models from 2D drawings with ease and
efficiency. CreatureShop takes an input bitmap drawing of a character (such as
an animal or other creature), depicted from an arbitrary descriptive pose and
viewpoint, and creates a 3D shape with plausible geometric details and textures
from a small number of user annotations on the 2D drawing. Our key
contributions are a novel oblique view modeling method, a set of systematic
approaches for producing plausible textures on the invisible or occluded parts
of the 3D character (as viewed from the direction of the input drawing), and a
user-friendly interactive system. We validate our system and methods by
creating numerous 3D characters from various drawings, and compare our results
with related works to show the advantages of our method. We perform a user
study to evaluate the usability of our system, which demonstrates that our
system is a practical and efficient approach to create fully-textured 3D
character models for novice users.","['Congyi Zhang', 'Lei Yang', 'Nenglun Chen', 'Nicholas Vining', 'Alla Sheffer', 'Francis C. M. Lau', 'Guoping Wang', 'Wenping Wang']",2022-08-10T21:48:56Z,http://arxiv.org/abs/2208.05572v1,['cs.GR'],"3D character modeling,Texturing,Sketch-based system,Bitmap drawing,Geometric details,Oblique view modeling,User annotations,Plausible textures,User study"
"Going Incognito in the Metaverse: Achieving Theoretically Optimal
  Privacy-Usability Tradeoffs in VR","Virtual reality (VR) telepresence applications and the so-called ""metaverse""
promise to be the next major medium of human-computer interaction. However,
with recent studies demonstrating the ease at which VR users can be profiled
and deanonymized, metaverse platforms carry many of the privacy risks of the
conventional internet (and more) while at present offering few of the defensive
utilities that users are accustomed to having access to. To remedy this, we
present the first known method of implementing an ""incognito mode"" for VR. Our
technique leverages local differential privacy to quantifiably obscure
sensitive user data attributes, with a focus on intelligently adding noise when
and where it is needed most to maximize privacy while minimizing usability
impact. Our system is capable of flexibly adapting to the unique needs of each
VR application to further optimize this trade-off. We implement our solution as
a universal Unity (C#) plugin that we then evaluate using several popular VR
applications. Upon faithfully replicating the most well-known VR privacy attack
studies, we show a significant degradation of attacker capabilities when using
our solution.","['Vivek Nair', 'Gonzalo Munilla Garrido', 'Dawn Song']",2022-08-11T01:55:45Z,http://arxiv.org/abs/2208.05604v5,['cs.CR'],"Virtual reality,Metaverse,Privacy,Usability,Differential privacy,Noise,Unity,C#,Privacy attack,Telepresence"
"Theoretical and real-time study of uniaxial nematic liquid crystal phase
  transitions using Fresnel diffraction","Liquid crystals (LCs) play a fundamental and significant role in modern
technology. Recently, they have also been used in active switching, adaptive
optics, and next-generation displays for augmented and virtual reality. This is
due to the diverse properties of their various phases and the growing physical
understanding of LCs. Our goal is to examine the applicability of a new method
in determining these quantities for thermotropic uniaxial nematic liquid
crystals (NLCs), even though nearly all theoretical and experimental efforts
are focused on a deeper understanding of the temperature-dependent free energy
behavior and other quantities related to it, especially in the vicinity of the
first- and second-order phase transitions of LCs. The method that is being
discussed is based on Fresnel diffraction (FD) from phase objects, which has
found a wide range of precise metrological applications over the past two
decades. Diffractometry is a very sensitive, accurate, and immune technique
that can convert any change in the order of LCs as a function of temperature
into a change in the optical phase and, as a result, a recordable change in the
visibility of the light diffraction pattern from phase steps. This contrasts
with interferometry, which is very sensitive to environmental changes.
Theoretical investigations, numerical calculations, and comparisons of the
results with experimental observations in turn demonstrate very high compliance
with the output of other existing methods. As we will see, this method has the
potential to not only strengthen existing approaches by addressing some of
their flaws and shortcomings but also to take its place next to them.","['N. Madadi', 'M. Amiri']",2022-08-10T17:27:48Z,http://arxiv.org/abs/2208.07944v1,['physics.optics'],"nematic liquid crystal,phase transitions,Fresnel diffraction,thermotropic,free energy,first-order,second-order,diffractometry,interferometry,metrological"
"Providing High Capacity for AR/VR traffic in 5G Systems with
  Multi-Connectivity","Augmented and Virtual Reality (AR/VR) is often called a ""killer"" application
of 5G systems because it imposes very strict Quality of Service (QoS)
requirements related to throughput, latency, and reliability. A high-resolution
AR/VR flow requires a bandwidth of dozens of MHz. Since the existing
low-frequency bands (i.e., below 6 GHz) have limited bandwidth and are
overpopulated, one of the ways to satisfy high AR/VR demands is to use wide
frequency channels available in the millimeter-Wave (mmWave) band. However,
transmission in the mmWave band suffers from high throughput fluctuation and
even blockage, which leads to violation of strict AR/VR latency and reliability
requirements. To address this problem, 5G specifications introduce a
Multi-Connectivity (MC) feature that allows a mobile user to connect
simultaneously to several base stations. The paper considers a scenario with
two base stations: the first base station operates in the low-frequency band to
provide reliable data delivery, while the second one operates in the mmWave
band and offers high data rates when the channel conditions are favorable. An
open question that falls out of the scope of specifications is how to balance
AR/VR traffic between two links with different characteristics. The paper
proposes a Delay-Based Traffic Balancing (DBTB) algorithm that minimizes
resource consumption of the low-frequency link while satisfying strict AR/VR
QoS requirements. With extensive simulations, DBTB is shown to double the
network capacity for AR/VR traffic compared with the state-of-the-art traffic
balancing algorithms.","['Maxim Susloparov', 'Artem Krasilov', 'Evgeny Khorov']",2022-08-17T13:17:54Z,http://arxiv.org/abs/2208.08277v1,['cs.NI'],"5G systems,AR/VR traffic,Multi-Connectivity,Quality of Service,mmWave,bandwidth,latency,reliability,base stations,traffic balancing"
"Joint Scheduling and Coding for Reliable, Latency-bounded Transmission
  over Parallel Wireless Links","Several novel industrial applications involve human control of vehicles,
cranes, or mobile robots through various high-throughput feedback systems, such
as Virtual Reality (VR) and tactile/haptic signals. The near real-time
interaction between the system and the operator requires strict latency
constraints in packet exchange, which is difficult to guarantee over wireless
communication links. In this work, we advocate that packet-level coding and
packet scheduling over multiple parallel (unreliable) links have the potential
to provide reliable, latency-bounded communication for applications with
periodic data generation patterns. However, this goal can be reached only
through a careful joint design of such mechanisms, whose interactions can be
subtle and difficult to predict. In this paper we first discuss these aspects
in general terms, and then present a Markov Decision Process (MDP) model that
can be used to find a scheme that optimally exploits the multichannel wireless
access in order to maximize the fraction of data blocks delivered within
deadline. Our illustrative example is then used to show the optimal
coding/scheduling strategies under different combinations of wireless links,
also showing that the common solution of backing up a high bitrate unreliable
mmWave link with a low bitrate more stable sub-6 GHz link can actually be
ineffective in the considered scenario","['Andrea Bedin', 'Federico Chiariotti', 'Andrea Zanella']",2022-08-25T10:15:48Z,http://arxiv.org/abs/2208.11978v1,['cs.NI'],"scheduling,coding,transmission,reliable,latency,packet exchange,wireless links,Markov Decision Process,multichannel wireless access"
"Semantics and Non-Fungible Tokens for Copyright Management on the
  Metaverse and Beyond","Recent initiatives related to the Metaverse focus on better visualisation,
like augmented or virtual reality, but also persistent digital objects. To
guarantee real ownership of these digital objects, open systems based on public
blockchains and Non-Fungible Tokens (NFTs) are emerging together with a nascent
decentralized and open creator economy. To manage this emerging economy in a
more organised way, and fight the so common NFT plagiarism, we propose
CopyrightLY, a decentralized application for authorship and copyright
management. It provides means to claim content authorship, including supporting
evidence. Content and metadata are stored in decentralized storage and
registered on the blockchain. A token is used to curate these claims, and
potential complaints, by staking it on them. Staking is incentivized by the
fact that the token is minted using a bonding curve. The tokenomics include the
resolution of complaints and enabling the monetization of curated claims.
Monetization is achieved through licensing NFTs with metadata enhanced by
semantic technologies. Semantic data makes explicit the reuse conditions
transferred with the token while keeping the connection to the underlying
copyright claims to improve the trustability of the NFTs. Moreover, the
semantic metadata is flexible enough to enable licensing not just in the real
world. Licenses can refer to reuses in specific locations in a metaverse, thus
facilitating the emergence of creative economies in them.","['Roberto García', 'Ana Cediel', 'Mercè Teixidó', 'Rosa Gil']",2022-08-25T05:59:38Z,http://arxiv.org/abs/2208.14174v1,"['cs.CR', 'cs.DB', 'cs.DC', 'cs.MM']","Semantics,Non-Fungible Tokens,Copyright Management,Metaverse,Public Blockchains,Decentralized Application,Blockchain,Tokenomics,Semantic Technologies,Licensing"
"Metaverse for Healthcare: A Survey on Potential Applications, Challenges
  and Future Directions","The rapid progress in digitalization and automation have led to an
accelerated growth in healthcare, generating novel models that are creating new
channels for rendering treatment with reduced cost. The Metaverse is an
emerging technology in the digital space which has huge potential in
healthcare, enabling realistic experiences to the patients as well as the
medical practitioners. The Metaverse is a confluence of multiple enabling
technologies such as artificial intelligence, virtual reality, augmented
reality, internet of medical devices, robotics, quantum computing, etc. through
which new directions for providing quality healthcare treatment and services
can be explored. The amalgamation of these technologies ensures immersive,
intimate and personalized patient care. It also provides adaptive intelligent
solutions that eliminates the barriers between healthcare providers and
receivers. This article provides a comprehensive review of the Metaverse for
healthcare, emphasizing on the state of the art, the enabling technologies for
adopting the Metaverse for healthcare, the potential applications and the
related projects. The issues in the adaptation of the Metaverse for healthcare
applications are also identified and the plausible solutions are highlighted as
part of future research directions.","['Rajeswari Chengoden', 'Nancy Victor', 'Thien Huynh-The', 'Gokul Yenduri', 'Rutvij H. Jhaveri', 'Mamoun Alazab', 'Sweta Bhattacharya', 'Pawan Hegde', 'Praveen Kumar Reddy Maddikunta', 'Thippa Reddy Gadekallu']",2022-09-09T07:40:11Z,http://arxiv.org/abs/2209.04160v1,['cs.AI'],"Metaverse,Healthcare,Digitalization,Automation,Artificial Intelligence,Virtual Reality,Augmented Reality,Internet of Medical Devices,Robotics,Quantum Computing"
"TruVR: Trustworthy Cybersickness Detection using Explainable Machine
  Learning","Cybersickness can be characterized by nausea, vertigo, headache, eye strain,
and other discomforts when using virtual reality (VR) systems. The previously
reported machine learning (ML) and deep learning (DL) algorithms for detecting
(classification) and predicting (regression) VR cybersickness use black-box
models; thus, they lack explainability. Moreover, VR sensors generate a massive
amount of data, resulting in complex and large models. Therefore, having
inherent explainability in cybersickness detection models can significantly
improve the model's trustworthiness and provide insight into why and how the
ML/DL model arrived at a specific decision. To address this issue, we present
three explainable machine learning (xML) models to detect and predict
cybersickness: 1) explainable boosting machine (EBM), 2) decision tree (DT),
and 3) logistic regression (LR). We evaluate xML-based models with publicly
available physiological and gameplay datasets for cybersickness. The results
show that the EBM can detect cybersickness with an accuracy of 99.75% and
94.10% for the physiological and gameplay datasets, respectively. On the other
hand, while predicting the cybersickness, EBM resulted in a Root Mean Square
Error (RMSE) of 0.071 for the physiological dataset and 0.27 for the gameplay
dataset. Furthermore, the EBM-based global explanation reveals exposure length,
rotation, and acceleration as key features causing cybersickness in the
gameplay dataset. In contrast, galvanic skin responses and heart rate are most
significant in the physiological dataset. Our results also suggest that
EBM-based local explanation can identify cybersickness-causing factors for
individual samples. We believe the proposed xML-based cybersickness detection
method can help future researchers understand, analyze, and design simpler
cybersickness detection and reduction models.","['Ripan Kumar Kundu', 'Rifatul Islam', 'Prasad Calyam', 'Khaza Anuarul Hoque']",2022-09-12T13:55:13Z,http://arxiv.org/abs/2209.05257v1,"['cs.HC', 'cs.LG']","Trustworthy Cybersickness Detection,Explainable Machine Learning,Virtual Reality,Cybersickness,Machine Learning,Deep Learning,Black-box Models,Explainability,Physiological Data,Gameplay Data"
"A Survey on Mobile Edge Computing for Video Streaming: Opportunities and
  Challenges","5G communication brings substantial improvements in the quality of service
provided to various applications by achieving higher throughput and lower
latency. However, interactive multimedia applications (e.g., ultra high
definition video conferencing, 3D and multiview video streaming, crowd-sourced
video streaming, cloud gaming, virtual and augmented reality) are becoming more
ambitious with high volume and low latency video streams putting strict demands
on the already congested networks. Mobile Edge Computing (MEC) is an emerging
paradigm that extends cloud computing capabilities to the edge of the network
i.e., at the base station level. To meet the latency requirements and avoid the
end-to-end communication with remote cloud data centers, MEC allows to store
and process video content (e.g., caching, transcoding, pre-processing) at the
base stations. Both video on demand and live video streaming can utilize MEC to
improve existing services and develop novel use cases, such as video analytics,
and targeted advertisements. MEC is expected to reshape the future of video
streaming by providing ultra-reliable and low latency streaming (e.g., in
augmented reality, virtual reality, and autonomous vehicles), pervasive
computing (e.g., in real-time video analytics), and blockchain-enabled
architecture for secure live streaming. This paper presents a comprehensive
survey of recent developments in MEC-enabled video streaming bringing
unprecedented improvement to enable novel use cases. A detailed review of the
state-of-the-art is presented covering novel caching schemes, optimal
computation offloading, cooperative caching and offloading and the use of
artificial intelligence (i.e., machine learning, deep learning, and
reinforcement learning) in MEC-assisted video streaming services.","['Muhammad Asif Khan', 'Emna Baccour', 'Zina Chkirbene', 'Aiman Erbad', 'Ridha Hamila', 'Mounir Hamdi', 'Moncef Gabbouj']",2022-09-13T06:53:32Z,http://arxiv.org/abs/2209.05761v1,"['cs.MM', 'cs.NI']","Mobile Edge Computing,Video Streaming,5G communication,Latency,Cloud Computing,Base Stations,Caching,Transcoding,Artificial Intelligence"
"Haptic Feedback Relocation from the Fingertips to the Wrist for
  Two-Finger Manipulation in Virtual Reality","Relocation of haptic feedback from the fingertips to the wrist has been
considered as a way to enable haptic interaction with mixed reality virtual
environments while leaving the fingers free for other tasks. We present a pair
of wrist-worn tactile haptic devices and a virtual environment to study how
various mappings between fingers and tactors affect task performance. The
haptic feedback rendered to the wrist reflects the interaction forces occurring
between a virtual object and virtual avatars controlled by the index finger and
thumb. We performed a user study comparing four different finger-to-tactor
haptic feedback mappings and one no-feedback condition as a control. We
evaluated users' ability to perform a simple pick-and-place task via the
metrics of task completion time, path length of the fingers and virtual cube,
and magnitudes of normal and shear forces at the fingertips. We found that
multiple mappings were effective, and there was a greater impact when visual
cues were limited. We discuss the limitations of our approach and describe next
steps toward multi-degree-of-freedom haptic rendering for wrist-worn devices to
improve task performance in virtual environments.","['Jasmin E. Palmer', 'Mine Sarac', 'Aaron A. Garza', 'Allison M. Okamura']",2022-09-15T22:46:25Z,http://arxiv.org/abs/2209.07640v2,"['cs.HC', 'cs.RO']","Haptic feedback,Wrist,Two-finger manipulation,Virtual reality,Tactile haptic devices,Virtual environment,Task performance,Finger-to-tactor mappings,User study,Virtual object"
"Progressive tearing and cutting of soft-bodies in high-performance
  virtual reality","We present an algorithm that allows a user within a virtual environment to
perform real-time unconstrained cuts or consecutive tears, i.e., progressive,
continuous fractures on a deformable rigged and soft-body mesh model in
high-performance 10ms. In order to recreate realistic results for different
physically-principled materials such as sponges, hard or soft tissues, we
incorporate a novel soft-body deformation, via a particle system layered on-top
of a linear-blend skinning model. Our framework allows the simulation of
realistic, surgical-grade cuts and continuous tears, especially valuable in the
context of medical VR training. In order to achieve high performance in VR, our
algorithms are based on Euclidean geometric predicates on the rigged mesh,
without requiring any specific model pre-processing. The contribution of this
work lies on the fact that current frameworks supporting similar kinds of model
tearing, either do not operate in high-performance real-time or only apply to
predefined tears. The framework presented allows the user to freely cut or tear
a 3D mesh model in a consecutive way, under 10ms, while preserving its
soft-body behaviour and/or allowing further animation.","['Manos Kamarianakis', 'Antonis Protopsaltis', 'Dimitris Angelis', 'Michail Tamiolakis', 'George Papagiannakis']",2022-09-18T10:45:05Z,http://arxiv.org/abs/2209.08531v1,"['cs.GR', '68U05', 'I.3.8']","high-performance,virtual reality,cuts,tears,soft-body,mesh model,algorithm,simulation,deformation"
FoVolNet: Fast Volume Rendering using Foveated Deep Neural Networks,"Volume data is found in many important scientific and engineering
applications. Rendering this data for visualization at high quality and
interactive rates for demanding applications such as virtual reality is still
not easily achievable even using professional-grade hardware. We introduce
FoVolNet -- a method to significantly increase the performance of volume data
visualization. We develop a cost-effective foveated rendering pipeline that
sparsely samples a volume around a focal point and reconstructs the full-frame
using a deep neural network. Foveated rendering is a technique that prioritizes
rendering computations around the user's focal point. This approach leverages
properties of the human visual system, thereby saving computational resources
when rendering data in the periphery of the user's field of vision. Our
reconstruction network combines direct and kernel prediction methods to produce
fast, stable, and perceptually convincing output. With a slim design and the
use of quantization, our method outperforms state-of-the-art neural
reconstruction techniques in both end-to-end frame times and visual quality. We
conduct extensive evaluations of the system's rendering performance, inference
speed, and perceptual properties, and we provide comparisons to competing
neural image reconstruction techniques. Our test results show that FoVolNet
consistently achieves significant time saving over conventional rendering while
preserving perceptual quality.","['David Bauer', 'Qi Wu', 'Kwan-Liu Ma']",2022-09-20T19:48:56Z,http://arxiv.org/abs/2209.09965v1,"['cs.GR', 'cs.LG']","Volume rendering,Foveated rendering,Deep neural network,Focal point,Computational resources,Perceptual quality,Quantization,Neural reconstruction,Inference speed,Visualization"
"Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and
  Restoration","Compression plays an important role on the efficient transmission and storage
of images and videos through band-limited systems such as streaming services,
virtual reality or videogames. However, compression unavoidably leads to
artifacts and the loss of the original information, which may severely degrade
the visual quality. For these reasons, quality enhancement of compressed images
has become a popular research topic. While most state-of-the-art image
restoration methods are based on convolutional neural networks, other
transformers-based methods such as SwinIR, show impressive performance on these
tasks.
  In this paper, we explore the novel Swin Transformer V2, to improve SwinIR
for image super-resolution, and in particular, the compressed input scenario.
Using this method we can tackle the major issues in training transformer vision
models, such as training instability, resolution gaps between pre-training and
fine-tuning, and hunger on data. We conduct experiments on three representative
tasks: JPEG compression artifacts removal, image super-resolution (classical
and lightweight), and compressed image super-resolution. Experimental results
demonstrate that our method, Swin2SR, can improve the training convergence and
performance of SwinIR, and is a top-5 solution at the ""AIM 2022 Challenge on
Super-Resolution of Compressed Image and Video"".","['Marcos V. Conde', 'Ui-Jin Choi', 'Maxime Burchi', 'Radu Timofte']",2022-09-22T23:25:08Z,http://arxiv.org/abs/2209.11345v1,"['cs.CV', 'eess.IV']","Swin2SR,SwinV2 Transformer,Compressed Image,Super-Resolution,Restoration,Compression,Artifacts,Convolutional Neural Networks,Transformers-based methods,SwinIR"
Realistic Hair Synthesis with Generative Adversarial Networks,"Recent successes in generative modeling have accelerated studies on this
subject and attracted the attention of researchers. One of the most important
methods used to achieve this success is Generative Adversarial Networks (GANs).
It has many application areas such as; virtual reality (VR), augmented reality
(AR), super resolution, image enhancement. Despite the recent advances in hair
synthesis and style transfer using deep learning and generative modelling, due
to the complex nature of hair still contains unsolved challenges. The methods
proposed in the literature to solve this problem generally focus on making
high-quality hair edits on images. In this thesis, a generative adversarial
network method is proposed to solve the hair synthesis problem. While
developing this method, it is aimed to achieve real-time hair synthesis while
achieving visual outputs that compete with the best methods in the literature.
The proposed method was trained with the FFHQ dataset and then its results in
hair style transfer and hair reconstruction tasks were evaluated. The results
obtained in these tasks and the operating time of the method were compared with
MichiGAN, one of the best methods in the literature. The comparison was made at
a resolution of 128x128. As a result of the comparison, it has been shown that
the proposed method achieves competitive results with MichiGAN in terms of
realistic hair synthesis, and performs better in terms of operating time.","['Muhammed Pektas', 'Aybars Ugur']",2022-09-13T11:48:26Z,http://arxiv.org/abs/2209.12875v1,"['cs.GR', 'cs.CV']","Generative Adversarial Networks,Hair Synthesis,Deep Learning,Generative Modelling,Real-time,Style Transfer,Image Enhancement,Resolution,FFHQ Dataset,MichiGAN"
"Social VR and multi-party holographic communications: Opportunities,
  Challenges and Impact in the Education and Training Sectors","Technological advances can bring many benefits to our daily lives, and this
includes the education and training sectors. In the last years, online
education, teaching and training models are becoming increasingly adopted, in
part influenced by major circumstances like the pandemic. The use of
videoconferencing tools in such sectors has become fundamental, but recent
research has shown their multiple limitations in terms of relevant aspects,
like comfort, interaction quality, situational awareness, (co-)presence, etc.
This study elaborates on a new communication, interaction and collaboration
medium that becomes a promising candidate to overcome such limitations, by
adopting immersive technologies: Social Virtual Reality (VR). First, this
article provides a comprehensive review of studies having provided initial
evidence on (potential) benefits provided by Social VR in relevant use cases
related to education, such as online classes, training and co-design
activities, virtual conferences and interactive visits to virtual spaces, many
of them including comparisons with classical tools like 2D conferencing.
Likewise, the potential benefits of integrating realistic and volumetric users'
representations to enable multi-party holographic communications in Social VR
is also discussed. Next, this article identifies and elaborates on key
limitations of existing studies in this field, including both technological and
methodological aspects. Finally, it discusses key remaining challenges to be
addressed to fully exploit the potential of Social VR in the education sector.","['Mario Montagud', 'Gianluca Cernigliaro', 'Miguel Arevalillo-Herráez', 'Miguel García-Pineda', 'Jaume Segura-Garcia', 'Sergi Fernández']",2022-10-01T17:55:43Z,http://arxiv.org/abs/2210.00330v1,['cs.MM'],"Social VR,multi-party holographic communications,education,training sectors,immersive technologies,online education,videoconferencing tools,situational awareness,(co-)presence"
"Comparison of Data Representations and Machine Learning Architectures
  for User Identification on Arbitrary Motion Sequences","Reliable and robust user identification and authentication are important and
often necessary requirements for many digital services. It becomes paramount in
social virtual reality (VR) to ensure trust, specifically in digital encounters
with lifelike realistic-looking avatars as faithful replications of real
persons. Recent research has shown that the movements of users in extended
reality (XR) systems carry user-specific information and can thus be used to
verify their identities. This article compares three different potential
encodings of the motion data from head and hands (scene-relative,
body-relative, and body-relative velocities), and the performances of five
different machine learning architectures (random forest, multi-layer
perceptron, fully recurrent neural network, long-short term memory, gated
recurrent unit). We use the publicly available dataset ""Talking with Hands"" and
publish all code to allow reproducibility and to provide baselines for future
work. After hyperparameter optimization, the combination of a long-short term
memory architecture and body-relative data outperformed competing combinations:
the model correctly identifies any of the 34 subjects with an accuracy of 100%
within 150 seconds. Altogether, our approach provides an effective foundation
for behaviometric-based identification and authentication to guide researchers
and practitioners. Data and code are published under
https://go.uniwue.de/58w1r.","['Christian Schell', 'Andreas Hotho', 'Marc Erich Latoschik']",2022-10-02T14:12:10Z,http://arxiv.org/abs/2210.00527v2,"['cs.LG', 'cs.HC']","data representations,machine learning architectures,user identification,motion sequences,extended reality (XR) systems,encodings,random forest,multi-layer perceptron,fully recurrent neural network,long-short term memory"
"Brain structure can mediate or moderate the relationship of behavior to
  brain function and transcriptome. A preliminary study","Abnormalities in motor-control behavior, which have been with concussion and
head acceleration events (HAE), can be quantified using virtual reality (VR)
technologies. Motor-control behavior has been consistently mapped to the
brain's somatomotor network (SM) using both structural (sMRI) and functional
MRI (fMRI). However, no studies habe integrated HAE, motor-control behavior,
sMRI and fMRI measures. Here, brain networks important for motor-control were
hypothesized to show changes in tractography-based diffusion weighted imaging
[difference in fractional anisotropy (dFA)] and resting-state fMRI (rs-fMRI)
measures in collegiate American football players across the season, and that
these measures would relate to VR-based motor-control. We firther tested if
nine inflammation-related miRNAs were associated with
behavior-structure-function variables. Using permutation-based mediation and
moderation methods, we found that across-season dFA from the SM structural
connectome (SM-dFA) mediated the relationship between across-season VR-based
Sensory-motor Reactivity (dSR) and rs-fMRI SM fingerprint similarity (p = 0.007
and Teff = 47%). The interaction between dSR and SM-dFA also predicted (pF =
0.036, pbeta3 = 0.058) across-season levels of dmiRNA-30d through
permutation-based moderation analysis. These results suggest (1) that
motor-control is in a feedback relationship with brain structure and function,
(2) behavior-structure-function can be connected to HAE, and (3)
behavior-structure might predict molecular biology measures.","['Sumra Bari', 'Nicole L Vike', 'Khrystyna Stetsiv', 'Anne J Blood', 'Eric A Nauman', 'Thomas M Talavage', 'Semyon Slobounov', 'Hans C Breiter']",2022-10-06T20:26:26Z,http://arxiv.org/abs/2210.03195v1,['q-bio.QM'],"behavior,brain structure,brain function,transcriptome,motor-control behavior,sMRI,fMRI,diffusion weighted imaging,resting-state fMRI,miRNAs"
"Integrating Digital Twin and Advanced Intelligent Technologies to
  Realize the Metaverse","The advances in Artificial Intelligence (AI) have led to technological
advancements in a plethora of domains. Healthcare, education, and smart city
services are now enriched with AI capabilities. These technological
advancements would not have been realized without the assistance of fast,
secure, and fault-tolerant communication media. Traditional processing,
communication and storage technologies cannot maintain high levels of
scalability and user experience for immersive services. The metaverse is an
immersive three-dimensional (3D) virtual world that integrates fantasy and
reality into a virtual environment using advanced virtual reality (VR) and
augmented reality (AR) devices. Such an environment is still being developed
and requires extensive research in order for it to be realized to its highest
attainable levels. In this article, we discuss some of the key issues required
in order to attain realization of metaverse services. We propose a framework
that integrates digital twin (DT) with other advanced technologies such as the
sixth generation (6G) communication network, blockchain, and AI, to maintain
continuous end-to-end metaverse services. This article also outlines
requirements for an integrated, DT-enabled metaverse framework and provides a
look ahead into the evolving topic.","['Moayad Aloqaily', 'Ouns Bouachir', 'Fakhri Karray', 'Ismaeel Al Ridhawi', 'Abdulmotaleb El Saddik']",2022-10-03T17:02:58Z,http://arxiv.org/abs/2210.04606v1,"['cs.HC', 'cs.AI']","Artificial Intelligence,Healthcare,Education,Smart City Services,Communication Media,Scalability,User Experience,Metaverse,Virtual Reality,Augmented Reality"
"LACV-Net: Semantic Segmentation of Large-Scale Point Cloud Scene via
  Local Adaptive and Comprehensive VLAD","Large-scale point cloud semantic segmentation is an important task in 3D
computer vision, which is widely applied in autonomous driving, robotics, and
virtual reality. Current large-scale point cloud semantic segmentation methods
usually use down-sampling operations to improve computation efficiency and
acquire point clouds with multi-resolution. However, this may cause the problem
of missing local information. Meanwhile, it is difficult for networks to
capture global information in large-scale distributed contexts. To capture
local and global information effectively, we propose an end-to-end deep neural
network called LACV-Net for large-scale point cloud semantic segmentation. The
proposed network contains three main components: 1) a local adaptive feature
augmentation module (LAFA) to adaptively learn the similarity of centroids and
neighboring points to augment the local context; 2) a comprehensive VLAD module
(C-VLAD) that fuses local features with multi-layer, multi-scale, and
multi-resolution to represent a comprehensive global description vector; and 3)
an aggregation loss function to effectively optimize the segmentation
boundaries by constraining the adaptive weight from the LAFA module. Compared
to state-of-the-art networks on several large-scale benchmark datasets,
including S3DIS, Toronto3D, and SensatUrban, we demonstrated the effectiveness
of the proposed network.","['Ziyin Zeng', 'Yongyang Xu', 'Zhong Xie', 'Wei Tang', 'Jie Wan', 'Weichao Wu']",2022-10-12T02:11:00Z,http://arxiv.org/abs/2210.05870v1,['cs.CV'],"Large-scale point cloud,Semantic segmentation,Local adaptive,Comprehensive VLAD,End-to-end,Deep neural network,Feature augmentation,VLAD module,Aggregation loss,Benchmark datasets"
Towards Immersive Collaborative Sensemaking,"When collaborating face-to-face, people commonly use the surfaces and spaces
around them to perform sensemaking tasks, such as spatially organising
documents, notes or images. However, when people collaborate remotely using
desktop interfaces they no longer feel like they are sharing the same space.
This limitation may be overcome through collaboration in immersive
environments, which simulate the physical in-person experience. In this paper,
we report on a between-groups study comparing collaborations on image
organisation tasks, in an immersive Virtual Reality (VR) environment to more
conventional desktop conferencing. Collecting data from 40 subjects in groups
of four, we measured task performance, user behaviours, collaboration
engagement and awareness. Overall, the VR and desktop interface resulted in
similar speed, accuracy and social presence rating, but we observed more
conversations and interaction with objects, and more equal contributions to the
interaction from participants within groups in VR. We also identified
differences in coordination and collaborative awareness behaviours between VR
and desktop platforms. We report on a set of systematic measures for assessing
VR collaborative experience and a new analysis tool that we have developed to
capture user behaviours in collaborative setting. Finally, we provide design
considerations and directions for future work.","['Ying Yang', 'Tim Dwyer', 'Michael Wybrow', 'Benjamin Lee', 'Maxime Cordeil', 'Mark Billinghurst', 'Bruce H. Thomas']",2022-10-14T13:13:00Z,http://arxiv.org/abs/2210.07784v1,['cs.HC'],"Immersive environments,Sensemaking tasks,Virtual Reality (VR),Desktop interfaces,Collaboration,Spatial organization,User behaviors,Collaboration engagement,Collaborative awareness,Design considerations"
An Efficient FPGA Accelerator for Point Cloud,"Deep learning-based point cloud processing plays an important role in various
vision tasks, such as autonomous driving, virtual reality (VR), and augmented
reality (AR). The submanifold sparse convolutional network (SSCN) has been
widely used for the point cloud due to its unique advantages in terms of visual
results. However, existing convolutional neural network accelerators suffer
from non-trivial performance degradation when employed to accelerate SSCN
because of the extreme and unstructured sparsity, and the complex computational
dependency between the sparsity of the central activation and the neighborhood
ones. In this paper, we propose a high performance FPGA-based accelerator for
SSCN. Firstly, we develop a zero removing strategy to remove the coarse-grained
redundant regions, thus significantly improving computational efficiency.
Secondly, we propose a concise encoding scheme to obtain the matching
information for efficient point-wise multiplications. Thirdly, we develop a
sparse data matching unit and a computing core based on the proposed encoding
scheme, which can convert the irregular sparse operations into regular
multiply-accumulate operations. Finally, an efficient hardware architecture for
the submanifold sparse convolutional layer is developed and implemented on the
Xilinx ZCU102 field-programmable gate array board, where the 3D submanifold
sparse U-Net is taken as the benchmark. The experimental results demonstrate
that our design drastically improves computational efficiency, and can
dramatically improve the power efficiency by 51 times compared to GPU.","['Zilun Wang', 'Wendong Mao', 'Peixiang Yang', 'Zhongfeng Wang', 'Jun Lin']",2022-10-14T13:34:00Z,http://arxiv.org/abs/2210.07803v1,"['eess.SP', 'cs.AR', 'eess.IV']","FPGA Accelerator,Point Cloud,Deep learning,Sparse Convolutional Network,Computational Efficiency,Encoding Scheme,Multiply-Accumulate Operations,Hardware Architecture,Field-Programmable Gate Array,Power Efficiency"
"Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR)
  for Metaverses","Metaverse is expected to emerge as a new paradigm for the next-generation
Internet, providing fully immersive and personalised experiences to socialize,
work, and play in self-sustaining and hyper-spatio-temporal virtual world(s).
The advancements in different technologies like augmented reality, virtual
reality, extended reality (XR), artificial intelligence (AI), and 5G/6G
communication will be the key enablers behind the realization of AI-XR
metaverse applications. While AI itself has many potential applications in the
aforementioned technologies (e.g., avatar generation, network optimization,
etc.), ensuring the security of AI in critical applications like AI-XR
metaverse applications is profoundly crucial to avoid undesirable actions that
could undermine users' privacy and safety, consequently putting their lives in
danger. To this end, we attempt to analyze the security, privacy, and
trustworthiness aspects associated with the use of various AI techniques in
AI-XR metaverse applications. Specifically, we discuss numerous such challenges
and present a taxonomy of potential solutions that could be leveraged to
develop secure, private, robust, and trustworthy AI-XR applications. To
highlight the real implications of AI-associated adversarial threats, we
designed a metaverse-specific case study and analyzed it through the
adversarial lens. Finally, we elaborate upon various open issues that require
further research interest from the community.","['Adnan Qayyum', 'Muhammad Atif Butt', 'Hassan Ali', 'Muhammad Usman', 'Osama Halabi', 'Ala Al-Fuqaha', 'Qammer H. Abbasi', 'Muhammad Ali Imran', 'Junaid Qadir']",2022-10-24T14:26:59Z,http://arxiv.org/abs/2210.13289v1,"['cs.AI', 'cs.CR', 'cs.CY']","Artificial Intelligence,Extended Reality,Metaverse,Security,Privacy,Trustworthiness,AI-XR,5G,6G"
An Effective Deep Network for Head Pose Estimation without Keypoints,"Human head pose estimation is an essential problem in facial analysis in
recent years that has a lot of computer vision applications such as gaze
estimation, virtual reality, and driver assistance. Because of the importance
of the head pose estimation problem, it is necessary to design a compact model
to resolve this task in order to reduce the computational cost when deploying
on facial analysis-based applications such as large camera surveillance
systems, AI cameras while maintaining accuracy. In this work, we propose a
lightweight model that effectively addresses the head pose estimation problem.
Our approach has two main steps. 1) We first train many teacher models on the
synthesis dataset - 300W-LPA to get the head pose pseudo labels. 2) We design
an architecture with the ResNet18 backbone and train our proposed model with
the ensemble of these pseudo labels via the knowledge distillation process. To
evaluate the effectiveness of our model, we use AFLW-2000 and BIWI - two
real-world head pose datasets. Experimental results show that our proposed
model significantly improves the accuracy in comparison with the
state-of-the-art head pose estimation methods. Furthermore, our model has the
real-time speed of $\sim$300 FPS when inferring on Tesla V100.","['Chien Thai', 'Viet Tran', 'Minh Bui', 'Huong Ninh', 'Hai Tran']",2022-10-25T01:57:04Z,http://arxiv.org/abs/2210.13705v1,['cs.CV'],"head pose estimation,deep network,keypoints,computer vision,facial analysis,virtual reality,driver assistance,ResNet18,knowledge distillation,accuracy"
Learning Variational Motion Prior for Video-based Motion Capture,"Motion capture from a monocular video is fundamental and crucial for us
humans to naturally experience and interact with each other in Virtual Reality
(VR) and Augmented Reality (AR). However, existing methods still struggle with
challenging cases involving self-occlusion and complex poses due to the lack of
effective motion prior modeling. In this paper, we present a novel variational
motion prior (VMP) learning approach for video-based motion capture to resolve
the above issue. Instead of directly building the correspondence between the
video and motion domain, We propose to learn a generic latent space for
capturing the prior distribution of all natural motions, which serve as the
basis for subsequent video-based motion capture tasks. To improve the
generalization capacity of prior space, we propose a transformer-based
variational autoencoder pretrained over marker-based 3D mocap data, with a
novel style-mapping block to boost the generation quality. Afterward, a
separate video encoder is attached to the pretrained motion generator for
end-to-end fine-tuning over task-specific video datasets. Compared to existing
motion prior models, our VMP model serves as a motion rectifier that can
effectively reduce temporal jittering and failure modes in frame-wise pose
estimation, leading to temporally stable and visually realistic motion capture
results. Furthermore, our VMP-based framework models motion at sequence level
and can directly generate motion clips in the forward pass, achieving real-time
motion capture during inference. Extensive experiments over both public
datasets and in-the-wild videos have demonstrated the efficacy and
generalization capability of our framework.","['Xin Chen', 'Zhuo Su', 'Lingbo Yang', 'Pei Cheng', 'Lan Xu', 'Bin Fu', 'Gang Yu']",2022-10-27T02:45:48Z,http://arxiv.org/abs/2210.15134v2,"['cs.CV', 'I.4.8']","Variational Motion Prior,Video-based Motion Capture,Motion Capture,Virtual Reality,Augmented Reality,Transformer-based Variational Autoencoder,Style-mapping Block,Motion Generator,Video Encoder"
Big Data Meets Metaverse: A Survey,"We are living in the era of big data. The Metaverse is an emerging technology
in the future, and it has a combination of big data, AI (artificial
intelligence), VR (Virtual Reality), AR (Augmented Reality), MR (mixed
reality), and other technologies that will diminish the difference between
online and real-life interaction. It has the goal of becoming a platform where
we can work, go shopping, play around, and socialize. Each user who enters the
Metaverse interacts with the virtual world in a data way. With the development
and application of the Metaverse, the data will continue to grow, thus forming
a big data network, which will bring huge data processing pressure to the
digital world. Therefore, big data processing technology is one of the key
technologies to implement the Metaverse. In this survey, we provide a
comprehensive review of how Metaverse is changing big data. Moreover, we
discuss the key security and privacy of Metaverse big data in detail. Finally,
we summarize the open problems and opportunities of Metaverse, as well as the
future of Metaverse with big data. We hope that this survey will provide
researchers with the research direction and prospects of applying big data in
the Metaverse.","['Jiayi Sun', 'Wensheng Gan', 'Zefeng Chen', 'Junhui Li', 'Philip S. Yu']",2022-10-28T17:22:20Z,http://arxiv.org/abs/2210.16282v1,"['cs.DB', 'cs.CY']","big data,Metaverse,AI,VR,AR,MR,data processing technology,security,privacy,research direction"
Foveated Rendering: a State-of-the-Art Survey,"Recently, virtual reality (VR) technology has been widely used in medical,
military, manufacturing, entertainment, and other fields.
  These applications must simulate different complex material surfaces, various
dynamic objects, and complex physical phenomena, increasing the complexity of
VR scenes. Current computing devices cannot efficiently render these complex
scenes in real time, and delayed rendering makes the content observed by the
user inconsistent with the user's interaction, causing discomfort.
  Foveated rendering is a promising technique that can accelerate rendering. It
takes advantage of human eyes' inherent features and renders different regions
with different qualities without sacrificing perceived visual quality.
  Foveated rendering research has a history of 31 years and is mainly focused
on solving the following three problems.
  The first is to apply perceptual models of the human visual system into
foveated rendering. The second is to render the image with different qualities
according to foveation principles. The third is to integrate foveated rendering
into existing rendering paradigms to improve rendering performance.
  In this survey, we review foveated rendering research from 1990 to 2021.
  We first revisit the visual perceptual models related to foveated rendering.
  Subsequently, we propose a new foveated rendering taxonomy and then classify
and review the research on this basis. Finally, we discuss potential
opportunities and open questions in the foveated rendering field.
  We anticipate that this survey will provide new researchers with a high-level
overview of the state of the art in this field, furnish experts with up-to-date
information and offer ideas alongside a framework to VR display software and
hardware designers and engineers.","['Lili Wang', 'Xuehuai Shi', 'Yi Liu']",2022-11-15T08:12:37Z,http://arxiv.org/abs/2211.07969v1,['cs.GR'],"Foveated Rendering,Virtual Reality,Rendering Performance,Perceptual Models,Human Visual System,Rendering Paradigms,Taxonomy,State of the Art,Survey,VR Display"
"Rate-Distortion Modeling for Bit Rate Constrained Point Cloud
  Compression","As being one of the main representation formats of 3D real world and
well-suited for virtual reality and augmented reality applications, point
clouds have gained a lot of popularity. In order to reduce the huge amount of
data, a considerable amount of research on point cloud compression has been
done. However, given a target bit rate, how to properly choose the color and
geometry quantization parameters for compressing point clouds is still an open
issue. In this paper, we propose a rate-distortion model based quantization
parameter selection scheme for bit rate constrained point cloud compression.
Firstly, to overcome the measurement uncertainty in evaluating the distortion
of the point clouds, we propose a unified model to combine the geometry
distortion and color distortion. In this model, we take into account the
correlation between geometry and color variables of point clouds and derive a
dimensionless quantity to represent the overall quality degradation. Then, we
derive the relationships of overall distortion and bit rate with the
quantization parameters. Finally, we formulate the bit rate constrained point
cloud compression as a constrained minimization problem using the derived
polynomial models and deduce the solution via an iterative numerical method.
Experimental results show that the proposed algorithm can achieve optimal
decoded point cloud quality at various target bit rates, and substantially
outperform the video-rate-distortion model based point cloud compression
scheme.","['Pan Gao', 'Shengzhou Luo', 'Manoranjan Paul']",2022-11-19T10:21:06Z,http://arxiv.org/abs/2211.10646v1,"['cs.MM', 'cs.IT', 'eess.IV', 'math.IT']","point cloud,compression,rate-distortion modeling,bit rate,quantization parameters,geometry distortion,color distortion,iterative numerical method,virtual reality"
"A Lightweight Domain Adaptive Absolute Pose Regressor Using Barlow Twins
  Objective","Identifying the camera pose for a given image is a challenging problem with
applications in robotics, autonomous vehicles, and augmented/virtual reality.
Lately, learning-based methods have shown to be effective for absolute camera
pose estimation. However, these methods are not accurate when generalizing to
different domains. In this paper, a domain adaptive training framework for
absolute pose regression is introduced. In the proposed framework, the scene
image is augmented for different domains by using generative methods to train
parallel branches using Barlow Twins objective. The parallel branches leverage
a lightweight CNN-based absolute pose regressor architecture. Further, the
efficacy of incorporating spatial and channel-wise attention in the regression
head for rotation prediction is investigated. Our method is evaluated with two
datasets, Cambridge landmarks and 7Scenes. The results demonstrate that, even
with using roughly 24 times fewer FLOPs, 12 times fewer activations, and 5
times fewer parameters than MS-Transformer, our approach outperforms all the
CNN-based architectures and achieves performance comparable to
transformer-based architectures. Our method ranks 2nd and 4th with the
Cambridge Landmarks and 7Scenes datasets, respectively. In addition, for
augmented domains not encountered during training, our approach significantly
outperforms the MS-transformer. Furthermore, it is shown that our domain
adaptive framework achieves better performance than the single branch model
trained with the identical CNN backbone with all instances of the unseen
distribution.","['Praveen Kumar Rajendran', 'Quoc-Vinh Lai-Dang', 'Luiz Felipe Vecchietti', 'Dongsoo Har']",2022-11-20T12:18:53Z,http://arxiv.org/abs/2211.10963v1,"['cs.CV', 'cs.AI', 'cs.LG']","camera pose estimation,domain adaptive,absolute pose regression,Barlow Twins objective,lightweight CNN,spatial attention,channel-wise attention,FLOPs,activations,parameters"
"AI Enabled Maneuver Identification via the Maneuver Identification
  Challenge","Artificial intelligence (AI) has enormous potential to improve Air Force
pilot training by providing actionable feedback to pilot trainees on the
quality of their maneuvers and enabling instructor-less flying familiarization
for early-stage trainees in low-cost simulators. Historically, AI challenges
consisting of data, problem descriptions, and example code have been critical
to fueling AI breakthroughs. The Department of the Air Force-Massachusetts
Institute of Technology AI Accelerator (DAF-MIT AI Accelerator) developed such
an AI challenge using real-world Air Force flight simulator data. The Maneuver
ID challenge assembled thousands of virtual reality simulator flight recordings
collected by actual Air Force student pilots at Pilot Training Next (PTN). This
dataset has been publicly released at Maneuver-ID.mit.edu and represents the
first of its kind public release of USAF flight training data. Using this
dataset, we have applied a variety of AI methods to separate ""good"" vs ""bad""
simulator data and categorize and characterize maneuvers. These data,
algorithms, and software are being released as baselines of model performance
for others to build upon to enable the AI ecosystem for flight simulator
training.","['Kaira Samuel', 'Matthew LaRosa', 'Kyle McAlpin', 'Morgan Schaefer', 'Brandon Swenson', 'Devin Wasilefsky', 'Yan Wu', 'Dan Zhao', 'Jeremy Kepner']",2022-11-28T16:55:32Z,http://arxiv.org/abs/2211.15552v1,['cs.AI'],"Artificial intelligence,Maneuver Identification,Air Force,Pilot training,Maneuvers,Flight simulator,AI methods,Dataset,USAF,Model performance"
"NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with
  360° Views","Virtual reality and augmented reality (XR) bring increasing demand for 3D
content. However, creating high-quality 3D content requires tedious work that a
human expert must do. In this work, we study the challenging task of lifting a
single image to a 3D object and, for the first time, demonstrate the ability to
generate a plausible 3D object with 360{\deg} views that correspond well with
the given reference image. By conditioning on the reference image, our model
can fulfill the everlasting curiosity for synthesizing novel views of objects
from images. Our technique sheds light on a promising direction of easing the
workflows for 3D artists and XR designers. We propose a novel framework, dubbed
NeuralLift-360, that utilizes a depth-aware neural radiance representation
(NeRF) and learns to craft the scene guided by denoising diffusion models. By
introducing a ranking loss, our NeuralLift-360 can be guided with rough depth
estimation in the wild. We also adopt a CLIP-guided sampling strategy for the
diffusion prior to provide coherent guidance. Extensive experiments demonstrate
that our NeuralLift-360 significantly outperforms existing state-of-the-art
baselines. Project page: https://vita-group.github.io/NeuralLift-360/","['Dejia Xu', 'Yifan Jiang', 'Peihao Wang', 'Zhiwen Fan', 'Yi Wang', 'Zhangyang Wang']",2022-11-29T17:59:06Z,http://arxiv.org/abs/2211.16431v2,['cs.CV'],"3D content,neural radiance representation,denoising diffusion models,XR designers,NeuralLift-360,CLIP-guided sampling,virtual reality,augmented reality,depth estimation,novel views"
Self-Adaptive Digital Assistance Systems for Work 4.0,"In the era of digital transformation, new technological foundations and
possibilities for collaboration, production as well as organization open up
many opportunities to work differently in the future. The digitization of
workflows results in new forms of working which is denoted by the term Work
4.0. In the context of Work 4.0, digital assistance systems play an important
role as they give users additional situation-specific information about a
workflow or a product via displays, mobile devices such as tablets and
smartphones, or data glasses. Furthermore, such digital assistance systems can
be used to provide instructions and technical support in the working process as
well as for training purposes. However, existing digital assistance systems are
mostly created focusing on the ""design for all"" paradigm neglecting the
situation-specific tasks, skills, preferences, or environments of an individual
human worker. To overcome this issue, we present a monitoring and adaptation
framework for supporting self-adaptive digital assistance systems for Work 4.0.
Our framework supports context monitoring as well as UI adaptation for
augmented (AR) and virtual reality (VR)-based digital assistance systems. The
benefit of our framework is shown based on exemplary case studies from
different domains, e.g. context-aware maintenance application in AR or
warehouse management training in VR.","['Enes Yigitbas', 'Stefan Sauer', 'Gregor Engels']",2022-11-30T10:47:40Z,http://arxiv.org/abs/2211.16895v1,"['cs.HC', 'cs.SE']","digital assistance systems,Work 4.0,workflows,context monitoring,UI adaptation,augmented reality,virtual reality,digital transformation,training purposes,self-adaptive."
"NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and
  Animation","The capture and animation of human hair are two of the major challenges in
the creation of realistic avatars for the virtual reality. Both problems are
highly challenging, because hair has complex geometry and appearance, as well
as exhibits challenging motion. In this paper, we present a two-stage approach
that models hair independently from the head to address these challenges in a
data-driven manner. The first stage, state compression, learns a
low-dimensional latent space of 3D hair states containing motion and
appearance, via a novel autoencoder-as-a-tracker strategy. To better
disentangle the hair and head in appearance learning, we employ multi-view hair
segmentation masks in combination with a differentiable volumetric renderer.
The second stage learns a novel hair dynamics model that performs temporal hair
transfer based on the discovered latent codes. To enforce higher stability
while driving our dynamics model, we employ the 3D point-cloud autoencoder from
the compression stage for de-noising of the hair state. Our model outperforms
the state of the art in novel view synthesis and is capable of creating novel
hair animations without having to rely on hair observations as a driving
signal. Project page is here https://ziyanw1.github.io/neuwigs/.","['Ziyan Wang', 'Giljoo Nam', 'Tuur Stuyck', 'Stephen Lombardi', 'Chen Cao', 'Jason Saragih', 'Michael Zollhoefer', 'Jessica Hodgins', 'Christoph Lassner']",2022-12-01T16:09:54Z,http://arxiv.org/abs/2212.00613v3,"['cs.CV', 'cs.GR']","Neural Dynamic Model,Volumetric Hair Capture,Animation,Data-driven,Autoencoder,Multi-view Hair Segmentation,Volumetric Renderer,Hair Dynamics Model,3D Point-cloud Autoencoder,Novel View Synthesis"
"A Graph Neural Networks based Framework for Topology-Aware Proactive SLA
  Management in a Latency Critical NFV Application Use-case","Recent advancements in the rollout of 5G and 6G have led to the emergence of
a new range of latency-critical applications delivered via a Network Function
Virtualization (NFV) enabled paradigm of flexible and softwarized communication
networks. Evolving verticals like telecommunications, smart grid, virtual
reality (VR), industry 4.0, automated vehicles, etc. are driven by the vision
of low latency and high reliability, and there is a wide gap to efficiently
bridge the Quality of Service (QoS) constraints for both the service providers
and the end-user. In this work, we look to tackle the over-provisioning of
latency-critical services by proposing a proactive SLA management framework
leveraging Graph Neural Networks (GNN) and Deep Reinforcement Learning (DRL) to
balance the trade-off between efficiency and reliability. To summarize our key
contributions: 1) we compose a graph-based spatio-temporal multivariate
time-series forecasting model with multiple time-step predictions in a
multi-output scenario, delivering 74.62% improved performance over the
established baseline state-of-art model on the use-case; and 2) we leverage
realistic SLA definitions for the use-case to achieve a dynamic SLA-aware
oversight for scaling policy management with DRL.","['Nikita Jalodia', 'Mohit Taneja', 'Alan Davy']",2022-11-10T23:22:05Z,http://arxiv.org/abs/2212.00714v1,"['cs.DC', 'cs.LG', 'cs.NI']","Graph Neural Networks,Topology-Aware,Proactive SLA Management,Latency Critical,NFV Application,Quality of Service (QoS) constraints,Deep Reinforcement Learning (DRL),Time-Series Forecasting,Multi-Output Scenario,Scaling Policy Management"
Pretrained Diffusion Models for Unified Human Motion Synthesis,"Generative modeling of human motion has broad applications in computer
animation, virtual reality, and robotics. Conventional approaches develop
separate models for different motion synthesis tasks, and typically use a model
of a small size to avoid overfitting the scarce data available in each setting.
It remains an open question whether developing a single unified model is
feasible, which may 1) benefit the acquirement of novel skills by combining
skills learned from multiple tasks, and 2) help in increasing the model
capacity without overfitting by combining multiple data sources. Unification is
challenging because 1) it involves diverse control signals as well as targets
of varying granularity, and 2) motion datasets may use different skeletons and
default poses. In this paper, we present MoFusion, a framework for unified
motion synthesis. MoFusion employs a Transformer backbone to ease the inclusion
of diverse control signals via cross attention, and pretrains the backbone as a
diffusion model to support multi-granularity synthesis ranging from motion
completion of a body part to whole-body motion generation. It uses a learnable
adapter to accommodate the differences between the default skeletons used by
the pretraining and the fine-tuning data. Empirical results show that
pretraining is vital for scaling the model size without overfitting, and
demonstrate MoFusion's potential in various tasks, e.g., text-to-motion, motion
completion, and zero-shot mixing of multiple control signals. Project page:
\url{https://ofa-sys.github.io/MoFusion/}.","['Jianxin Ma', 'Shuai Bai', 'Chang Zhou']",2022-12-06T09:19:21Z,http://arxiv.org/abs/2212.02837v1,"['cs.CV', 'cs.GR', 'cs.LG', 'cs.RO']","Generative modeling,Human motion,Unified model,Diffusion model,Transformer backbone,Motion synthesis,Control signals,Pretraining,Skeletons,Adapter."
"Objective Surgical Skills Assessment and Tool Localization: Results from
  the MICCAI 2021 SimSurgSkill Challenge","Timely and effective feedback within surgical training plays a critical role
in developing the skills required to perform safe and efficient surgery.
Feedback from expert surgeons, while especially valuable in this regard, is
challenging to acquire due to their typically busy schedules, and may be
subject to biases. Formal assessment procedures like OSATS and GEARS attempt to
provide objective measures of skill, but remain time-consuming. With advances
in machine learning there is an opportunity for fast and objective automated
feedback on technical skills. The SimSurgSkill 2021 challenge (hosted as a
sub-challenge of EndoVis at MICCAI 2021) aimed to promote and foster work in
this endeavor. Using virtual reality (VR) surgical tasks, competitors were
tasked with localizing instruments and predicting surgical skill. Here we
summarize the winning approaches and how they performed. Using this publicly
available dataset and results as a springboard, future work may enable more
efficient training of surgeons with advances in surgical data science. The
dataset can be accessed from
https://console.cloud.google.com/storage/browser/isi-simsurgskill-2021.","['Aneeq Zia', 'Kiran Bhattacharyya', 'Xi Liu', 'Ziheng Wang', 'Max Berniker', 'Satoshi Kondo', 'Emanuele Colleoni', 'Dimitris Psychogyios', 'Yueming Jin', 'Jinfan Zhou', 'Evangelos Mazomenos', 'Lena Maier-Hein', 'Danail Stoyanov', 'Stefanie Speidel', 'Anthony Jarc']",2022-12-08T18:14:52Z,http://arxiv.org/abs/2212.04448v1,['cs.CV'],"surgical skills assessment,tool localization,MICCAI 2021,SimSurgSkill Challenge,machine learning,automated feedback,virtual reality (VR),surgical data science,objective measures,technical skills"
Design-time Fashion Popularity Forecasting in VR Environments,"Being able to forecast the popularity of new garment designs is very
important in an industry as fast paced as fashion, both in terms of
profitability and reducing the problem of unsold inventory. Here, we attempt to
address this task in order to provide informative forecasts to fashion
designers within a virtual reality designer application that will allow them to
fine tune their creations based on current consumer preferences within an
interactive and immersive environment. To achieve this we have to deal with the
following central challenges: (1) the proposed method should not hinder the
creative process and thus it has to rely only on the garment's visual
characteristics, (2) the new garment lacks historical data from which to
extrapolate their future popularity and (3) fashion trends in general are
highly dynamical. To this end, we develop a computer vision pipeline fine tuned
on fashion imagery in order to extract relevant visual features along with the
category and attributes of the garment. We propose a hierarchical label sharing
(HLS) pipeline for automatically capturing hierarchical relations among fashion
categories and attributes. Moreover, we propose MuQAR, a Multimodal
Quasi-AutoRegressive neural network that forecasts the popularity of new
garments by combining their visual features and categorical features while an
autoregressive neural network is modelling the popularity time series of the
garment's category and attributes. Both the proposed HLS and MuQAR prove
capable of surpassing the current state-of-the-art in key benchmark datasets,
DeepFashion for image classification and VISUELLE for new garment sales
forecasting.","['Stefanos-Iordanis Papadopoulos', 'Christos Koutlis', 'Anastasios Papazoglou-Chalikias', 'Symeon Papadopoulos', 'Spiros Nikolopoulos']",2022-12-14T12:30:03Z,http://arxiv.org/abs/2212.07187v1,['cs.CV'],"Design-time,Fashion,Popularity Forecasting,VR Environments,Computer Vision,Hierarchical Label Sharing,Multimodal Quasi-AutoRegressive,Neural Network,Forecasting,Garment's Category"
"Human-centric telerobotics: investigating users' performance and
  workload via VR-based eye-tracking measures","Virtual Reality (VR) is gaining ground in the robotics and teleoperation
industry, opening new prospects as a novel computerized methodology to make
humans interact with robots. In contrast with more conventional button-based
teleoperations, VR allows users to use their physical movements to drive
robotic systems in the virtual environment. The latest VR devices are also
equipped with integrated eye-tracking, which constitutes an exceptional
opportunity for monitoring users' workload online. However, such devices are
fairly recent, and human factors have been consistently marginalized so far in
telerobotics research. We thus covered these aspects by analyzing extensive
behavioral data generated by 24 participants driving a simulated industrial
robot in VR through a pick-and-place task. Users drove the robot via
button-based and action-based controls and under low (single-task) and high
(dual-task) mental demands. We collected self-reports, performance and
eye-tracking data. Specifically, we asked i) how the interactive features of VR
affect users' performance and workload, and additionally tested ii) the
sensibility of diverse eye parameters in monitoring users' vigilance and
workload throughout the task. Users performed faster and more accurately, while
also showing a lower mental workload, when using an action-based VR control.
Among the eye parameters, pupil size was the most resilient indicator of
workload, as it was highly correlated with the self-reports and was not
affected by the user's degree of physical motion in VR. Our results thus bring
a fresh human-centric overview of human-robot interactions in VR, and
systematically demonstrate the potential of VR devices for monitoring human
factors in telerobotics contexts.","['Federica Nenna', 'Davide Zanardi', 'Luciano Gamberini']",2022-12-14T17:13:40Z,http://arxiv.org/abs/2212.07345v1,['cs.HC'],"telerobotics,VR,eye-tracking,human factors,performance,workload,virtual reality,robotics,teleoperation,human-robot interactions"
"Estimating Cloth Elasticity Parameters Using Position-Based Simulation
  of Compliant Constrained Dynamics","Clothing plays a vital role in real life and hence, is also important for
virtual realities and virtual applications, such as online retail, virtual
try-on, and real-time digital avatar interactions. However, choosing the
correct parameters to generate realistic clothing requires expert knowledge and
is often an arduous manual process. To alleviate this issue, we develop a
pipeline for automatically determining the static material parameters required
to simulate clothing of a particular material based on easily captured
real-world fabrics. We use differentiable simulation to find an optimal set of
parameters that minimizes the difference between simulated cloth and deformed
target cloth. Our novel well-suited loss function is optimized through
non-linear least squares. We designed our objective function to capture
material-specific behavior, resulting in similar values for different wrinkle
configurations of the same material. While existing methods carefully design
experiments to isolate stretch parameters from bending modes, we embrace that
stretching fabrics causes wrinkling. We estimate bending first, given that
membrane stiffness has little effect on bending. Furthermore, our pipeline
decouples the capture method from the optimization by registering a template
mesh to the scanned data. These choices simplify the capture system and allow
for wrinkles in scanned fabrics. We use a differentiable extended
position-based dynamics (XPBD) cloth simulator, which is capable of real-time
simulation. We demonstrate our method on captured data of three different
real-world fabrics and on three digital fabrics produced by a third-party
simulator.","['Egor Larionov', 'Marie-Lena Eckert', 'Katja Wolff', 'Tuur Stuyck']",2022-12-17T03:43:45Z,http://arxiv.org/abs/2212.08790v1,['cs.GR'],"Position-based simulation,Elasticity parameters,Cloth simulator,Compliant constrained dynamics,Differentiable simulation,Non-linear least squares,Wrinkle configurations,Membrane stiffness,Capture system,Template mesh"
"Multi-Projection Fusion and Refinement Network for Salient Object
  Detection in 360° Omnidirectional Image","Salient object detection (SOD) aims to determine the most visually attractive
objects in an image. With the development of virtual reality technology,
360{\deg} omnidirectional image has been widely used, but the SOD task in
360{\deg} omnidirectional image is seldom studied due to its severe distortions
and complex scenes. In this paper, we propose a Multi-Projection Fusion and
Refinement Network (MPFR-Net) to detect the salient objects in 360{\deg}
omnidirectional image. Different from the existing methods, the equirectangular
projection image and four corresponding cube-unfolding images are embedded into
the network simultaneously as inputs, where the cube-unfolding images not only
provide supplementary information for equirectangular projection image, but
also ensure the object integrity of the cube-map projection. In order to make
full use of these two projection modes, a Dynamic Weighting Fusion (DWF) module
is designed to adaptively integrate the features of different projections in a
complementary and dynamic manner from the perspective of inter and intra
features. Furthermore, in order to fully explore the way of interaction between
encoder and decoder features, a Filtration and Refinement (FR) module is
designed to suppress the redundant information between the feature itself and
the feature. Experimental results on two omnidirectional datasets demonstrate
that the proposed approach outperforms the state-of-the-art methods both
qualitatively and quantitatively.","['Runmin Cong', 'Ke Huang', 'Jianjun Lei', 'Yao Zhao', 'Qingming Huang', 'Sam Kwong']",2022-12-23T14:50:40Z,http://arxiv.org/abs/2212.12378v1,['cs.CV'],"Salient object detection,360° omnidirectional image,Multi-Projection Fusion,Refinement Network,Equirectangular projection,Cube-unfolding images,Dynamic Weighting Fusion,Filtration and Refinement,Encoder,Decoder"
"Development and Evaluation of a Learning-based Model for Real-time
  Haptic Texture Rendering","Current Virtual Reality (VR) environments lack the rich haptic signals that
humans experience during real-life interactions, such as the sensation of
texture during lateral movement on a surface. Adding realistic haptic textures
to VR environments requires a model that generalizes to variations of a user's
interaction and to the wide variety of existing textures in the world. Current
methodologies for haptic texture rendering exist, but they usually develop one
model per texture, resulting in low scalability. We present a deep
learning-based action-conditional model for haptic texture rendering and
evaluate its perceptual performance in rendering realistic texture vibrations
through a multi part human user study. This model is unified over all materials
and uses data from a vision-based tactile sensor (GelSight) to render the
appropriate surface conditioned on the user's action in real time. For
rendering texture, we use a high-bandwidth vibrotactile transducer attached to
a 3D Systems Touch device. The result of our user study shows that our
learning-based method creates high-frequency texture renderings with comparable
or better quality than state-of-the-art methods without the need for learning a
separate model per texture. Furthermore, we show that the method is capable of
rendering previously unseen textures using a single GelSight image of their
surface.","['Negin Heravi', 'Heather Culbertson', 'Allison M. Okamura', 'Jeannette Bohg']",2022-12-27T01:06:26Z,http://arxiv.org/abs/2212.13332v3,"['cs.RO', 'cs.HC', 'cs.LG']","Virtual Reality,Haptic Texture Rendering,Deep Learning,Perceptual Performance,Realistic Texture Vibrations,Vision-based Tactile Sensor,Vibrotactile Transducer,User Study,GelSight"
"Metaverse Communications, Networking, Security, and Applications:
  Research Issues, State-of-the-Art, and Future Directions","Metaverse is an evolving orchestrator of the next-generation Internet
architecture that produces an immersive and self-adapting virtual world in
which humans perform activities similar to those in the real world, such as
playing sports, doing work, and socializing. It is becoming a reality and is
driven by ever-evolving advanced technologies such as extended reality,
artificial intelligence, and blockchain. In this context, Metaverse will play
an essential role in developing smart cities, which becomes more evident in the
post COVID 19 pandemic metropolitan setting. However, the new paradigm imposes
new challenges, such as developing novel privacy and security threats that can
emerge in the digital Metaverse ecosystem. Moreover, it requires the
convergence of several media types with the capability to quickly process
massive amounts of data to keep the residents safe and well-informed, which can
raise issues related to scalability and interoperability. In light of this,
this research study aims to review the literature on the state of the art of
integrating the Metaverse architecture concepts in smart cities. First, this
paper presents the theoretical architecture of Metaverse and discusses
international companies interest in this emerging technology. It also examines
the notion of Metaverse relevant to virtual reality, identifies the prevalent
threats, and determines the importance of communication infrastructure in
information gathering for efficient Metaverse operation. Next, the notion of
blockchain technologies is discussed regarding privacy preservation and how it
can provide tamper-proof content sharing among Metaverse users. Finally, the
application of distributed Metaverse for social good is highlighted.","['Mansoor Ali', 'Faisal Naeem', 'Georges Kaddoum', 'Ekram Hossain']",2022-12-25T03:37:35Z,http://arxiv.org/abs/2212.13993v2,"['cs.CR', 'cs.SY', 'eess.SY']","Metaverse Communications,Networking,Security,Applications,Internet architecture,Extended reality,Artificial intelligence,Blockchain,Smart cities"
"Pensieve 5G: Implementation of RL-based ABR Algorithm for UHD 4K/8K
  Content Delivery on Commercial 5G SA/NR-DC Network","While the rollout of the fifth-generation mobile network (5G) is underway
across the globe with the intention to deliver 4K/8K UHD videos, Augmented
Reality (AR), and Virtual Reality (VR) content to the mass amounts of users,
the coverage and throughput are still one of the most significant issues,
especially in the rural areas, where only 5G in the low-frequency band are
being deployed. This called for a high-performance adaptive bitrate (ABR)
algorithm that can maximize the user quality of experience given 5G network
characteristics and data rate of UHD contents.
  Recently, many of the newly proposed ABR techniques were machine-learning
based. Among that, Pensieve is one of the state-of-the-art techniques, which
utilized reinforcement-learning to generate an ABR algorithm based on
observation of past decision performance. By incorporating the context of the
5G network and UHD content, Pensieve has been optimized into Pensieve 5G. New
QoE metrics that more accurately represent the QoE of UHD video streaming on
the different types of devices were proposed and used to evaluate Pensieve 5G
against other ABR techniques including the original Pensieve. The results from
the simulation based on the real 5G Standalone (SA) network throughput shows
that Pensieve 5G outperforms both conventional algorithms and Pensieve with the
average QoE improvement of 8.8% and 14.2%, respectively. Additionally, Pensieve
5G also performed well on the commercial 5G NR-NR Dual Connectivity (NR-DC)
Network, despite the training being done solely using the data from the 5G
Standalone (SA) network.","['Kasidis Arunruangsirilert', 'Bo Wei', 'Hang Song', 'Jiro Katto']",2022-12-29T23:03:47Z,http://arxiv.org/abs/2212.14479v1,"['cs.NI', 'cs.LG']","5G,RL-based,ABR Algorithm,UHD,4K,8K,Content Delivery,Network,SA/NR-DC."
Imitator: Personalized Speech-driven 3D Facial Animation,"Speech-driven 3D facial animation has been widely explored, with applications
in gaming, character animation, virtual reality, and telepresence systems.
State-of-the-art methods deform the face topology of the target actor to sync
the input audio without considering the identity-specific speaking style and
facial idiosyncrasies of the target actor, thus, resulting in unrealistic and
inaccurate lip movements. To address this, we present Imitator, a speech-driven
facial expression synthesis method, which learns identity-specific details from
a short input video and produces novel facial expressions matching the
identity-specific speaking style and facial idiosyncrasies of the target actor.
Specifically, we train a style-agnostic transformer on a large facial
expression dataset which we use as a prior for audio-driven facial expressions.
Based on this prior, we optimize for identity-specific speaking style based on
a short reference video. To train the prior, we introduce a novel loss function
based on detected bilabial consonants to ensure plausible lip closures and
consequently improve the realism of the generated expressions. Through detailed
experiments and a user study, we show that our approach produces temporally
coherent facial expressions from input audio while preserving the speaking
style of the target actors.","['Balamurugan Thambiraja', 'Ikhsanul Habibie', 'Sadegh Aliakbarian', 'Darren Cosker', 'Christian Theobalt', 'Justus Thies']",2022-12-30T19:00:02Z,http://arxiv.org/abs/2301.00023v1,['cs.CV'],"Speech-driven,3D facial animation,facial expression synthesis,identity-specific,style-agnostic transformer,facial expression dataset,bilabial consonants,user study"
"Towards a Pipeline for Real-Time Visualization of Faces for VR-based
  Telepresence and Live Broadcasting Utilizing Neural Rendering","While head-mounted displays (HMDs) for Virtual Reality (VR) have become
widely available in the consumer market, they pose a considerable obstacle for
a realistic face-to-face conversation in VR since HMDs hide a significant
portion of the participants faces. Even with image streams from cameras
directly attached to an HMD, stitching together a convincing image of an entire
face remains a challenging task because of extreme capture angles and strong
lens distortions due to a wide field of view. Compared to the long line of
research in VR, reconstruction of faces hidden beneath an HMD is a very recent
topic of research. While the current state-of-the-art solutions demonstrate
photo-realistic 3D reconstruction results, they require high-cost laboratory
equipment and large computational costs. We present an approach that focuses on
low-cost hardware and can be used on a commodity gaming computer with a single
GPU. We leverage the benefits of an end-to-end pipeline by means of Generative
Adversarial Networks (GAN). Our GAN produces a frontal-facing 2.5D point cloud
based on a training dataset captured with an RGBD camera. In our approach, the
training process is offline, while the reconstruction runs in real-time. Our
results show adequate reconstruction quality within the 'learned' expressions.
Expressions not learned by the network produce artifacts and can trigger the
Uncanny Valley effect.","['Philipp Ladwig', 'Rene Ebertowski', 'Alexander Pech', 'Ralf Dörner', 'Christian Geiger']",2023-01-04T08:49:51Z,http://arxiv.org/abs/2301.01490v1,['cs.CV'],"Real-Time Visualization,Faces,VR,Telepresence,Neural Rendering,Head-Mounted Displays,Image Streams,Cameras,Lens Distortions,Generative Adversarial Networks"
"Neuromorphic High-Frequency 3D Dancing Pose Estimation in Dynamic
  Environment","As a beloved sport worldwide, dancing is getting integrated into traditional
and virtual reality-based gaming platforms nowadays. It opens up new
opportunities in the technology-mediated dancing space. These platforms
primarily rely on passive and continuous human pose estimation as an input
capture mechanism. Existing solutions are mainly based on RGB or RGB-Depth
cameras for dance games. The former suffers in low-lighting conditions due to
the motion blur and low sensitivity, while the latter is too power-hungry, has
a low frame rate, and has limited working distance. With ultra-low latency,
energy efficiency, and wide dynamic range characteristics, the event camera is
a promising solution to overcome these shortcomings. We propose YeLan, an event
camera-based 3-dimensional high-frequency human pose estimation(HPE) system
that survives low-lighting conditions and dynamic backgrounds. We collected the
world's first event camera dance dataset and developed a fully customizable
motion-to-event physics-aware simulator. YeLan outperforms the baseline models
in these challenging conditions and demonstrated robustness against different
types of clothing, background motion, viewing angle, occlusion, and lighting
fluctuations.","['Zhongyang Zhang', 'Kaidong Chai', 'Haowen Yu', 'Ramzi Majaj', 'Francesca Walsh', 'Edward Wang', 'Upal Mahbub', 'Hava Siegelmann', 'Donghyun Kim', 'Tauhidur Rahman']",2023-01-17T00:55:12Z,http://arxiv.org/abs/2301.06648v2,"['cs.CV', 'cs.AI']","Neuromorphic,High-Frequency,3D,Dancing,Pose Estimation,Dynamic Environment,Event Camera,Human Pose Estimation,Simulator"
DIGITOUR: Automatic Digital Tours for Real-Estate Properties,"A virtual or digital tour is a form of virtual reality technology which
allows a user to experience a specific location remotely. Currently, these
virtual tours are created by following a 2-step strategy. First, a photographer
clicks a 360 degree equirectangular image; then, a team of annotators manually
links these images for the ""walkthrough"" user experience. The major challenge
in the mass adoption of virtual tours is the time and cost involved in manual
annotation/linking of images. Therefore, this paper presents an end-to-end
pipeline to automate the generation of 3D virtual tours using equirectangular
images for real-estate properties. We propose a novel HSV-based coloring scheme
for paper tags that need to be placed at different locations before clicking
the equirectangular images using 360 degree cameras. These tags have two
characteristics: i) they are numbered to help the photographer for placement of
tags in sequence and; ii) bi-colored, which allows better learning of tag
detection (using YOLOv5 architecture) in an image and digit recognition (using
custom MobileNet architecture) tasks. Finally, we link/connect all the
equirectangular images based on detected tags. We show the efficiency of the
proposed pipeline on a real-world equirectangular image dataset collected from
the Housing.com database.","['Prateek Chhikara', 'Harshul Kuhar', 'Anil Goyal', 'Chirag Sharma']",2023-01-17T03:43:34Z,http://arxiv.org/abs/2301.06680v1,"['cs.CV', 'cs.GR', 'cs.LG']","virtual reality technology,360 degree equirectangular image,annotation,3D virtual tours,HSV-based coloring scheme,paper tags,equirectangular images,360 degree cameras,YOLOv5 architecture,MobileNet architecture"
"Real-Time Viewport-Aware Optical Flow Estimation in 360-degree Videos
  for Visually-Induced Motion Sickness Mitigation","Visually-induced motion sickness (VIMS), a side effect of perceived motion
caused by visual stimulation, is a major obstacle to the widespread use of
Virtual Reality (VR). Along with scene object information, visual stimulation
can be primarily indicated by optical flow, which characterizes the motion
pattern, such as the intensity and direction of the moving image. We estimated
the real time optical flow in 360-degree videos targeted at immersive user
interactive visualization based on the user's current viewport. The proposed
method allows the estimation of customized visual flow for each experience of
dynamic 360-degree videos and is an improvement over previous methods that
consider a single optical flow value for the entire equirectangular frame. We
applied our method to modulate the opacity of granulated rest frames (GRFs), a
technique consisting of visual noise-like randomly distributed visual
references that are stable to the user's body during immersive pre-recorded
360-degree video experience. We report the results of a pilot one-session
between-subject study with 18 participants, where users watched a 2-minute
high-intensity 360-degree video. The results show that our proposed method
successfully estimates optical flow, with pilot data showing that GRFs combined
with real-time optical flow estimation may improve user comfort when watching
360-degree videos. However, more data are needed for statistically significant
results.","['Zekun Cao', 'Regis Kopper']",2023-01-18T17:24:30Z,http://arxiv.org/abs/2301.07669v2,"['cs.HC', 'cs.GR']","Real-Time,Viewport-Aware,Optical Flow Estimation,360-degree Videos,Visually-Induced Motion Sickness,Virtual Reality,User Interactive Visualization,Equirectangular Frame,Granulated Rest Frames,Pilot Study"
PhysGraph: Physics-Based Integration Using Graph Neural Networks,"Physics-based simulation of mesh based domains remains a challenging task.
State-of-the-art techniques can produce realistic results but require expert
knowledge. A major bottleneck in many approaches is the step of integrating a
potential energy in order to compute velocities or displacements. Recently,
learning based method for physics-based simulation have sparked interest with
graph based approaches being a promising research direction. One of the
challenges for these methods is to generate models that are mesh independent
and generalize to different material properties. Moreover, the model should
also be able to react to unforeseen external forces like ubiquitous collisions.
Our contribution is based on a simple observation: evaluating forces is
computationally relatively cheap for traditional simulation methods and can be
computed in parallel in contrast to their integration. If we learn how a system
reacts to forces in general, irrespective of their origin, we can learn an
integrator that can predict state changes due to the total forces with high
generalization power. We effectively factor out the physical model behind
resulting forces by relying on an opaque force module. We demonstrate that this
idea leads to a learnable module that can be trained on basic internal forces
of small mesh patches and generalizes to different mesh typologies,
resolutions, material parameters and unseen forces like collisions at inference
time. Our proposed paradigm is general and can be used to model a variety of
physical phenomena. We focus our exposition on the detail enhancement of coarse
clothing geometry which has many applications including computer games, virtual
reality and virtual try-on.","['Oshri Halimi', 'Egor Larionov', 'Zohar Barzelay', 'Philipp Herholz', 'Tuur Stuyck']",2023-01-27T16:47:10Z,http://arxiv.org/abs/2301.11841v2,"['cs.GR', 'cs.LG']","Physics-based simulation,Mesh domains,Graph neural networks,Integrating potential energy,Learning-based method,Mesh independent,Generalization,External forces,Forces evaluation,Physical model"
"MED1stMR: Mixed Reality to Enhance Training of Medical First
  Responder]{MED1stMR: Mixed Reality to Enhance the Training of Medical First
  Responders for Challenging Contexts","Mass-casualty incidents with a large number of injured persons caused by
human-made or by natural disasters are increasing globally. In such situations,
medical first responders (MFRs) need to perform diagnosis, basic life support,
or other first aid to help stabilize victims and keep them alive to wait for
the arrival of further support. Situational awareness and effective coping with
acute stressors is essential to enable first responders to take appropriate
action that saves lives.
  Virtual Reality (VR) has been demonstrated in several domains to be a serious
alternative, and in some areas also a significant improvement to conventional
learning and training. Especially for the challenges in the training of MFRs,
it can be highly useful for practicing and learning domains where the context
of the training is not easily available. VR training offers controlled,
easy-to-create environments that can be created and trained repeatedly under
the same conditions.
  As an advanced alternative to VR, Mixed Reality (MR) environments have the
potential to augment current VR training by providing a dynamic simulation of
an environment and hands-on practice on injured victims. Building on this
interpretation of MR, the main aim of MED1stMR is to develop a new generation
of MR training with haptic feedback for enhanced realism. in this workshop
paper, we will present the vision of the project and suggest questions for
discussion.","['Helmut Schrom-Feiertag', 'Georg Regal', 'Markus Murtinger']",2023-01-30T18:01:32Z,http://arxiv.org/abs/2301.13124v1,"['cs.CY', 'cs.HC']","Mixed Reality,Training,Medical First Responder,Virtual Reality,Situational awareness,Acute stressors,Haptic feedback,Mass-casualty incidents,Simulation,Hands-on practice"
"Passively Addressed Robotic Morphing Surface (PARMS) Based on Machine
  Learning","Reconfigurable morphing surfaces provide new opportunities for advanced
human-machine interfaces and bio-inspired robotics. Morphing into arbitrary
surfaces on demand requires a device with a sufficiently large number of
actuators and an inverse control strategy that can calculate the actuator
stimulation necessary to achieve a target surface. The programmability of a
morphing surface can be improved by increasing the number of independent
actuators, but this increases the complexity of the control system. Thus,
developing compact and efficient control interfaces and control algorithms is a
crucial knowledge gap for the adoption of morphing surfaces in broad
applications. In this work, we describe a passively addressed robotic morphing
surface (PARMS) composed of matrix-arranged ionic actuators. To reduce the
complexity of the physical control interface, we introduce passive matrix
addressing. Matrix addressing allows the control of independent actuators using
only 2N control inputs, which is significantly lower than control inputs
required for traditional direct addressing. Our control algorithm is based on
machine learning using finite element simulations as the training data. This
machine learning approach allows both forward and inverse control with high
precision in real time. Inverse control demonstrations show that the PARMS can
dynamically morph into arbitrary pre-defined surfaces on demand. These
innovations in actuator matrix control may enable future implementation of
PARMS in wearables, haptics, and augmented reality/virtual reality (AR/VR).","['Jue Wang', 'Michael Sotzing', 'Mina Lee', 'Alex Chortos']",2023-01-30T20:51:14Z,http://arxiv.org/abs/2301.13284v2,['cs.RO'],"reconfigurable morphing surfaces,actuators,inverse control,programmability,compact control interfaces,matrix addressing,machine learning,finite element simulations"
"Scheduling Inference Workloads on Distributed Edge Clusters with
  Reinforcement Learning","Many real-time applications (e.g., Augmented/Virtual Reality, cognitive
assistance) rely on Deep Neural Networks (DNNs) to process inference tasks.
Edge computing is considered a key infrastructure to deploy such applications,
as moving computation close to the data sources enables us to meet stringent
latency and throughput requirements. However, the constrained nature of edge
networks poses several additional challenges to the management of inference
workloads: edge clusters can not provide unlimited processing power to DNN
models, and often a trade-off between network and processing time should be
considered when it comes to end-to-end delay requirements. In this paper, we
focus on the problem of scheduling inference queries on DNN models in edge
networks at short timescales (i.e., few milliseconds). By means of simulations,
we analyze several policies in the realistic network settings and workloads of
a large ISP, highlighting the need for a dynamic scheduling policy that can
adapt to network conditions and workloads. We therefore design ASET, a
Reinforcement Learning based scheduling algorithm able to adapt its decisions
according to the system conditions. Our results show that ASET effectively
provides the best performance compared to static policies when scheduling over
a distributed pool of edge resources.","['Gabriele Castellano', 'Juan-José Nieto', 'Jordi Luque', 'Ferrán Diego', 'Carlos Segura', 'Diego Perino', 'Flavio Esposito', 'Fulvio Risso', 'Aravindh Raman']",2023-01-31T13:23:34Z,http://arxiv.org/abs/2301.13618v1,"['cs.LG', 'cs.NI', 'cs.SY', 'eess.SY']","Deep Neural Networks (DNNs),Edge computing,Inference workloads,Reinforcement Learning,Scheduling,Edge clusters,Network conditions,Processing time,Latency,Throughput"
"Design and Implementation of A Soccer Ball Detection System with
  Multiple Cameras","The detection of small and medium-sized objects in three dimensions has
always been a frontier exploration problem. This technology has a very wide
application in sports analysis, games, virtual reality, human animation and
other fields. The traditional three-dimensional small target detection
technology has the disadvantages of high cost, low precision and inconvenience,
so it is difficult to apply in practice. With the development of machine
learning and deep learning, the technology of computer vision algorithms is
becoming more mature. Creating an immersive media experience is considered to
be a very important research work in sports.
  The main work is to explore and solve the problem of football detection under
the multiple cameras, aiming at the research and implementation of the live
broadcast system of football matches. Using multi cameras detects a target ball
and determines its position in three dimension with the occlusion, motion, low
illumination of the target object.
  This paper designed and implemented football detection system under multiple
cameras for the detection and capture of targets in real-time matches. The main
work mainly consists of three parts, football detector, single camera
detection, and multi-cameras detection. The system used bundle adjustment to
obtain the three-dimensional position of the target, and the GPU to accelerates
data pre-processing and achieve accurate real-time capture of the target. By
testing the system, it shows that the system can accurately detect and capture
the moving targets in 3D.
  In addition, the solution in this paper is reusable for large-scale
competitions, like basketball and soccer. The system framework can be well
transplanted into other similar engineering project systems. It has been put
into the market.","['Lei Li', 'Tianfang Zhang', 'Zhongfeng Kang', 'Wenhan Zhang']",2023-01-31T22:04:53Z,http://arxiv.org/abs/2302.00123v1,"['cs.CV', 'cs.AI']","Soccer ball detection system,Multiple cameras,Three-dimensional,Machine learning,Deep learning,Computer vision algorithms,Bundle adjustment,GPU,Real-time capture"
"Towards an Understanding of Distributed Asymmetric Collaborative
  Visualization on Problem-solving","This paper provided empirical knowledge of the user experience for using
collaborative visualization in a distributed asymmetrical setting through
controlled user studies. With the ability to access various computing devices,
such as Virtual Reality (VR) head-mounted displays, scenarios emerge when
collaborators have to or prefer to use different computing environments in
different places. However, we still lack an understanding of using VR in an
asymmetric setting for collaborative visualization. To get an initial
understanding and better inform the designs for asymmetric systems, we first
conducted a formative study with 12 pairs of participants. All participants
collaborated in asymmetric (PC-VR) and symmetric settings (PC-PC and VR-VR). We
then improved our asymmetric design based on the key findings and observations
from the first study. Another ten pairs of participants collaborated with
enhanced PC-VR and PC-PC conditions in a follow-up study. We found that a
well-designed asymmetric collaboration system could be as effective as a
symmetric system. Surprisingly, participants using PC perceived less mental
demand and effort in the asymmetric setting (PC-VR) compared to the symmetric
setting (PC-PC). We provided fine-grained discussions about the trade-offs
between different collaboration settings.","['Wai Tong', 'Meng Xia', 'Kam Kwai Wong', 'Doug A. Bowman', 'Ting-Chuen Pong', 'Huamin Qu', 'Yalong Yang']",2023-02-03T19:23:15Z,http://arxiv.org/abs/2302.01966v1,['cs.HC'],"Distributed,Asymmetric,Collaborative Visualization,User Experience,Controlled User Studies,Virtual Reality (VR),Computing Environments,Formative Study,Symmetric Settings,Asymmetric Design"
"You Only Train Once: Multi-Identity Free-Viewpoint Neural Human
  Rendering from Monocular Videos","We introduce You Only Train Once (YOTO), a dynamic human generation
framework, which performs free-viewpoint rendering of different human
identities with distinct motions, via only one-time training from monocular
videos. Most prior works for the task require individualized optimization for
each input video that contains a distinct human identity, leading to a
significant amount of time and resources for the deployment, thereby impeding
the scalability and the overall application potential of the system. In this
paper, we tackle this problem by proposing a set of learnable identity codes to
expand the capability of the framework for multi-identity free-viewpoint
rendering, and an effective pose-conditioned code query mechanism to finely
model the pose-dependent non-rigid motions. YOTO optimizes neural radiance
fields (NeRF) by utilizing designed identity codes to condition the model for
learning various canonical T-pose appearances in a single shared volumetric
representation. Besides, our joint learning of multiple identities within a
unified model incidentally enables flexible motion transfer in high-quality
photo-realistic renderings for all learned appearances. This capability expands
its potential use in important applications, including Virtual Reality. We
present extensive experimental results on ZJU-MoCap and PeopleSnapshot to
clearly demonstrate the effectiveness of our proposed model. YOTO shows
state-of-the-art performance on all evaluation metrics while showing
significant benefits in training and inference efficiency as well as rendering
quality. The code and model will be made publicly available soon.","['Jaehyeok Kim', 'Dongyoon Wee', 'Dan Xu']",2023-03-10T10:23:17Z,http://arxiv.org/abs/2303.05835v1,['cs.CV'],"neural rendering,free-viewpoint rendering,monocular videos,identity codes,neural radiance fields,non-rigid motions,motion transfer,photo-realistic renderings,virtual reality,training efficiency"
Toward Super-Resolution for Appearance-Based Gaze Estimation,"Gaze tracking is a valuable tool with a broad range of applications in
various fields, including medicine, psychology, virtual reality, marketing, and
safety. Therefore, it is essential to have gaze tracking software that is
cost-efficient and high-performing. Accurately predicting gaze remains a
difficult task, particularly in real-world situations where images are affected
by motion blur, video compression, and noise. Super-resolution has been shown
to improve image quality from a visual perspective. This work examines the
usefulness of super-resolution for improving appearance-based gaze tracking. We
show that not all SR models preserve the gaze direction. We propose a two-step
framework based on SwinIR super-resolution model. The proposed method
consistently outperforms the state-of-the-art, particularly in scenarios
involving low-resolution or degraded images. Furthermore, we examine the use of
super-resolution through the lens of self-supervised learning for gaze
prediction. Self-supervised learning aims to learn from unlabelled data to
reduce the amount of required labeled data for downstream tasks. We propose a
novel architecture called SuperVision by fusing an SR backbone network to a
ResNet18 (with some skip connections). The proposed SuperVision method uses 5x
less labeled data and yet outperforms, by 15%, the state-of-the-art method of
GazeTR which uses 100% of training data.","[""Galen O'Shea"", 'Majid Komeili']",2023-03-17T17:40:32Z,http://arxiv.org/abs/2303.10151v1,['cs.CV'],"Super-Resolution,Appearance-Based,Gaze Estimation,Gaze Tracking,SwinIR,Self-Supervised Learning,Image Quality,SuperVision,ResNet18"
"LFACon: Introducing Anglewise Attention to No-Reference Quality
  Assessment in Light Field Space","Light field imaging can capture both the intensity information and the
direction information of light rays. It naturally enables a
six-degrees-of-freedom viewing experience and deep user engagement in virtual
reality. Compared to 2D image assessment, light field image quality assessment
(LFIQA) needs to consider not only the image quality in the spatial domain but
also the quality consistency in the angular domain. However, there is a lack of
metrics to effectively reflect the angular consistency and thus the angular
quality of a light field image (LFI). Furthermore, the existing LFIQA metrics
suffer from high computational costs due to the excessive data volume of LFIs.
In this paper, we propose a novel concept of ""anglewise attention"" by
introducing a multihead self-attention mechanism to the angular domain of an
LFI. This mechanism better reflects the LFI quality. In particular, we propose
three new attention kernels, including anglewise self-attention, anglewise grid
attention, and anglewise central attention. These attention kernels can realize
angular self-attention, extract multiangled features globally or selectively,
and reduce the computational cost of feature extraction. By effectively
incorporating the proposed kernels, we further propose our light field
attentional convolutional neural network (LFACon) as an LFIQA metric. Our
experimental results show that the proposed LFACon metric significantly
outperforms the state-of-the-art LFIQA metrics. For the majority of distortion
types, LFACon attains the best performance with lower complexity and less
computational time.","['Qiang Qu', 'Xiaoming Chen', 'Yuk Ying Chung', 'Weidong Cai']",2023-03-20T09:37:41Z,http://arxiv.org/abs/2303.10961v1,"['eess.IV', 'cs.CV', 'cs.MM']","Light field imaging,Quality assessment,Anglewise attention,No-reference,Self-attention mechanism,Multiangled features,Convolutional neural network,Computational cost,Feature extraction,Distortion types"
"Recommendation Systems in Libraries: an Application with Heterogeneous
  Data Sources","The Reading&Machine project exploits the support of digitalization to
increase the attractiveness of libraries and improve the users' experience. The
project implements an application that helps the users in their decision-making
process, providing recommendation system (RecSys)-generated lists of books the
users might be interested in, and showing them through an interactive Virtual
Reality (VR)-based Graphical User Interface (GUI). In this paper, we focus on
the design and testing of the recommendation system, employing data about all
users' loans over the past 9 years from the network of libraries located in
Turin, Italy. In addition, we use data collected by the Anobii online social
community of readers, who share their feedback and additional information about
books they read. Armed with this heterogeneous data, we build and evaluate
Content Based (CB) and Collaborative Filtering (CF) approaches. Our results
show that the CF outperforms the CB approach, improving by up to 47\% the
relevant recommendations provided to a reader. However, the performance of the
CB approach is heavily dependent on the number of books the reader has already
read, and it can work even better than CF for users with a large history.
Finally, our evaluations highlight that the performances of both approaches are
significantly improved if the system integrates and leverages the information
from the Anobii dataset, which allows us to include more user readings (for CF)
and richer book metadata (for CB).","['Alessandro Speciale', 'Greta Vallero', 'Luca Vassio', 'Marco Mellia']",2023-03-21T11:13:01Z,http://arxiv.org/abs/2303.11746v1,"['cs.IR', 'cs.CY', 'cs.LG']","Recommendation Systems,Heterogeneous Data Sources,RecSys,Virtual Reality,Graphical User Interface,Content Based,Collaborative Filtering,Loans,Readers,Metadata"
FER-former: Multi-modal Transformer for Facial Expression Recognition,"The ever-increasing demands for intuitive interactions in Virtual Reality has
triggered a boom in the realm of Facial Expression Recognition (FER). To
address the limitations in existing approaches (e.g., narrow receptive fields
and homogenous supervisory signals) and further cement the capacity of FER
tools, a novel multifarious supervision-steering Transformer for FER in the
wild is proposed in this paper. Referred as FER-former, our approach features
multi-granularity embedding integration, hybrid self-attention scheme, and
heterogeneous domain-steering supervision. In specific, to dig deep into the
merits of the combination of features provided by prevailing CNNs and
Transformers, a hybrid stem is designed to cascade two types of learning
paradigms simultaneously. Wherein, a FER-specific transformer mechanism is
devised to characterize conventional hard one-hot label-focusing and CLIP-based
text-oriented tokens in parallel for final classification. To ease the issue of
annotation ambiguity, a heterogeneous domains-steering supervision module is
proposed to make image features also have text-space semantic correlations by
supervising the similarity between image features and text features. On top of
the collaboration of multifarious token heads, diverse global receptive fields
with multi-modal semantic cues are captured, thereby delivering superb learning
capability. Extensive experiments on popular benchmarks demonstrate the
superiority of the proposed FER-former over the existing state-of-the-arts.","['Yande Li', 'Mingjie Wang', 'Minglun Gong', 'Yonggang Lu', 'Li Liu']",2023-03-23T02:29:53Z,http://arxiv.org/abs/2303.12997v1,['cs.CV'],"Facial Expression Recognition,Multi-modal Transformer,Supervision,Hybrid Self-attention,Receptive Fields,Supervisory Signals,Transformer Mechanism,Image Features,Text Features,Benchmark"
"GP-PCS: One-shot Feature-Preserving Point Cloud Simplification with
  Gaussian Processes on Riemannian Manifolds","The processing, storage and transmission of large-scale point clouds is an
ongoing challenge in the computer vision community which hinders progress in
the application of 3D models to real-world settings, such as autonomous
driving, virtual reality and remote sensing. We propose a novel, one-shot point
cloud simplification method which preserves both the salient structural
features and the overall shape of a point cloud without any prior surface
reconstruction step. Our method employs Gaussian processes suitable for
functions defined on Riemannian manifolds, allowing us to model the surface
variation function across any given point cloud. A simplified version of the
original cloud is obtained by sequentially selecting points using a greedy
sparsification scheme. The selection criterion used for this scheme ensures
that the simplified cloud best represents the surface variation of the original
point cloud. We evaluate our method on several benchmark and self-acquired
point clouds, compare it to a range of existing methods, demonstrate its
application in downstream tasks of registration and surface reconstruction, and
show that our method is competitive both in terms of empirical performance and
computational efficiency.","['Stuti Pathak', 'Thomas M. McDonald', 'Seppe Sels', 'Rudi Penne']",2023-03-27T14:05:34Z,http://arxiv.org/abs/2303.15225v3,['cs.CV'],"point cloud,simplification,Gaussian processes,Riemannian manifolds,surface variation,sparsification,registration,surface reconstruction,computational efficiency"
"A Generative Framework for Low-Cost Result Validation of Machine
  Learning-as-a-Service Inference","The growing popularity of Machine Learning (ML) has led to its deployment in
various sensitive domains, which has resulted in significant research focused
on ML security and privacy. However, in some applications, such as
Augmented/Virtual Reality, integrity verification of the outsourced ML tasks is
more critical--a facet that has not received much attention. Existing
solutions, such as multi-party computation and proof-based systems, impose
significant computation overhead, which makes them unfit for real-time
applications. We propose Fides, a novel framework for real-time integrity
validation of ML-as-a-Service (MLaaS) inference. Fides features a novel and
efficient distillation technique--Greedy Distillation Transfer Learning--that
dynamically distills and fine-tunes a space and compute-efficient verification
model for verifying the corresponding service model while running inside a
trusted execution environment. Fides features a client-side attack detection
model that uses statistical analysis and divergence measurements to identify,
with a high likelihood, if the service model is under attack. Fides also offers
a re-classification functionality that predicts the original class whenever an
attack is identified. We devised a generative adversarial network framework for
training the attack detection and re-classification models. The evaluation
shows that Fides achieves an accuracy of up to 98% for attack detection and 94%
for re-classification.","['Abhinav Kumar', 'Miguel A. Guirao Aguilera', 'Reza Tourani', 'Satyajayant Misra']",2023-03-31T19:17:30Z,http://arxiv.org/abs/2304.00083v4,"['cs.CR', 'cs.LG']","Machine Learning,ML security,privacy,Augmented Reality,Virtual Reality,real-time applications,verification model,attack detection,generative adversarial network"
"Human-Centric Resource Allocation in the Metaverse over Wireless
  Communications","The Metaverse will provide numerous immersive applications for human users,
by consolidating technologies like extended reality (XR), video streaming, and
cellular networks. Optimizing wireless communications to enable the
human-centric Metaverse is important to satisfy the demands of mobile users. In
this paper, we formulate the optimization of the system utility-cost ratio
(UCR) for the Metaverse over wireless networks. Our human-centric utility
measure for virtual reality (VR) applications of the Metaverse represents
users' perceptual assessment of the VR video quality as a function of the data
rate and the video resolution, and is learnt from real datasets. The variables
jointly optimized in our problem include the allocation of both communication
and computation resources as well as VR video resolutions. The system cost in
our problem comprises the energy consumption and delay, and is non-convex with
respect to the optimization variables due to fractions in the mathematical
expressions. To solve the non-convex optimization, we develop a novel
fractional programming technique, which contributes to optimization theory and
has broad applicability beyond our paper. Our proposed algorithm for the system
UCR optimization is computationally efficient and finds a stationary point to
the constrained optimization. Through extensive simulations, our algorithm is
demonstrated to outperform other approaches.","['Jun Zhao', 'Liangxin Qian', 'Wenhan Yu']",2023-04-01T16:31:54Z,http://arxiv.org/abs/2304.00355v2,"['cs.SI', 'cs.NI']","Metaverse,Wireless Communications,Extended Reality (XR),Video Streaming,Cellular Networks,Virtual Reality (VR),Optimization,Fractional Programming,Energy Consumption,Delay"
Impact of XR on Mental Health: Are we Playing with Fire?,"Extended reality (XR) technology has the incredible potential to
revolutionize mental health treatment and support, bringing a whole new
dimension to the field. Through the use of immersive virtual and augmented
reality experiences, individuals can enter entirely new worlds and realities
that provide a safe and controlled space for therapy and self-exploration.
Whether it's stepping into a calming natural environment, practicing social
interactions or confronting past traumas in a controlled environment, extended
reality offers endless possibilities. Engaging these virtual realities,
individuals can gain a deeper understanding of themselves and their emotions,
learn coping strategies, and practice important life skills in a way that is
both engaging and effective. The wonders of extended reality for mental health
are truly awe-inspiring and offer a powerful tool for improving the well-being
of individuals around the world. However, we should remember, everything has
its disadvantages, and XR is no different. While XR is a revolution, the human
brain is very complex, fragile and unique (like with fingerprints, no two
people have the same brain anatomy), leading to varying conditions, results,
experiences and consequences. This article presents insights and information on
how immersive interactive digital experiences can shape our minds and
behaviors. Research to date suggests that XR experiences can change regions of
the brain responsible for attention and visuospatial skills.",['Benjamin Kenwright'],2023-04-04T09:06:18Z,http://arxiv.org/abs/2304.01648v1,"['cs.HC', 'cs.CY']","XR,mental health,treatment,immersive virtual reality,augmented reality,therapy,self-exploration,coping strategies,well-being,brain anatomy"
MonoHuman: Animatable Human Neural Field from Monocular Video,"Animating virtual avatars with free-view control is crucial for various
applications like virtual reality and digital entertainment. Previous studies
have attempted to utilize the representation power of the neural radiance field
(NeRF) to reconstruct the human body from monocular videos. Recent works
propose to graft a deformation network into the NeRF to further model the
dynamics of the human neural field for animating vivid human motions. However,
such pipelines either rely on pose-dependent representations or fall short of
motion coherency due to frame-independent optimization, making it difficult to
generalize to unseen pose sequences realistically. In this paper, we propose a
novel framework MonoHuman, which robustly renders view-consistent and
high-fidelity avatars under arbitrary novel poses. Our key insight is to model
the deformation field with bi-directional constraints and explicitly leverage
the off-the-peg keyframe information to reason the feature correlations for
coherent results. Specifically, we first propose a Shared Bidirectional
Deformation module, which creates a pose-independent generalizable deformation
field by disentangling backward and forward deformation correspondences into
shared skeletal motion weight and separate non-rigid motions. Then, we devise a
Forward Correspondence Search module, which queries the correspondence feature
of keyframes to guide the rendering network. The rendered results are thus
multi-view consistent with high fidelity, even under challenging novel pose
settings. Extensive experiments demonstrate the superiority of our proposed
MonoHuman over state-of-the-art methods.","['Zhengming Yu', 'Wei Cheng', 'Xian Liu', 'Wayne Wu', 'Kwan-Yee Lin']",2023-04-04T17:55:03Z,http://arxiv.org/abs/2304.02001v1,"['cs.CV', 'cs.GR']","MonoHuman,animatable,neural radiance field,deformation network,human neural field,virtual avatars,view-consistent,high-fidelity,keyframe information,bi-directional constraints"
Level generation for rhythm VR games,"Ragnarock is a virtual reality (VR) rhythm game in which you play a Viking
captain competing in a longship race. With two hammers, the task is to crush
the incoming runes in sync with epic Viking music. The runes are defined by a
beat map which the player can manually create. The creation of beat maps takes
hours. This work aims to automate the process of beat map creation, also known
as the task of learning to choreograph. The assignment is broken down into
three parts: determining the timing of the beats (action placement),
determining where in space the runes connected with the chosen beats should be
placed (action selection) and web-application creation. For the first task of
action placement, extraction of predominant local pulse (PLP) information from
music recordings is used. This approach allows to learn where and how many
beats are supposed to be placed. For the second task of action selection,
Recurrent Neural Networks (RNN) are used, specifically Gated recurrent unit
(GRU) to learn sequences of beats, and their patterns to be able to recreate
those rules and receive completely new levels. Then the last task was to build
a solution for non-technical players, the task was to combine the results of
the first and the second parts into a web application for easy use. For this
task the frontend was built using JavaScript and React and the backend - python
and FastAPI.",['Mariia Rizhko'],2023-04-13T20:24:51Z,http://arxiv.org/abs/2304.06809v1,"['cs.SD', 'cs.LG', 'eess.AS']","rhythm games,level generation,VR,beat map,action placement,action selection,local pulse,Recurrent Neural Networks,Gated recurrent unit,web application"
"Revival of the Silk Road using the applications of AR/VR and its role on
  cultural tourism","This research project seeks to investigate the incorporation of augmented
reality (AR) and virtual reality (VR) technology with human-computer
interaction (HCI) in order to revitalize the Silk Road - specifically in
Kermanshah, Iran - and its effect on cultural tourism. Kermanshah has
underexplored the rich historical significance of the Silk Road, despite the
presence of 24 UNESCO World Heritage sites. From the 2nd century BCE to the
18th century CE, the Silk Road was a vital trade route connecting the West and
the East and had enormous cultural, economic, religious, and political effects.
The purpose of this study is to examine the application of AR/VR technologies
in HCI for the preservation, interpretation, and promotion of the Silk Road's
tangible and intangible cultural heritage in Kermanshah, as well as their
impact on cultural tourism development. The study also investigates how these
innovative technologies can enhance visitors' experiences through immersive and
interactive approaches, promote sustainable tourism practices, and contribute
to the region's broader socioeconomic benefits. The research will analyze the
challenges and opportunities of implementing AR/VR technology in HCI within the
context of cultural heritage and tourism in Kermanshah and the Silk Road region
more broadly. By combining HCI, AR/VR, and cultural tourism, this research
seeks to provide valuable insights into the development of user-centered,
immersive experiences that promote a deeper understanding and appreciation of
the Silk Road's distinctive cultural heritage.",['Sahar Zandi'],2023-04-13T11:31:53Z,http://arxiv.org/abs/2304.10545v1,['cs.HC'],"Silk Road,AR,VR,human-computer interaction,cultural tourism,Kermanshah,UNESCO World Heritage sites,cultural heritage,immersive experiences"
"Privacy Computing Meets Metaverse: Necessity, Taxonomy and Challenges","Metaverse, the core of the next-generation Internet, is a computer-generated
holographic digital environment that simultaneously combines spatio-temporal,
immersive, real-time, sustainable, interoperable, and data-sensitive
characteristics. It cleverly blends the virtual and real worlds, allowing users
to create, communicate, and transact in virtual form. With the rapid
development of emerging technologies including augmented reality, virtual
reality and blockchain, the metaverse system is becoming more and more
sophisticated and widely used in various fields such as social, tourism,
industry and economy. However, the high level of interaction with the real
world also means a huge risk of privacy leakage both for individuals and
enterprises, which has hindered the wide deployment of metaverse. Then, it is
inevitable to apply privacy computing techniques in the framework of metaverse,
which is a current research hotspot. In this paper, we conduct comprehensive
research on the necessity, taxonomy and challenges when privacy computing meets
metaverse. Specifically, we first introduce the underlying technologies and
various applications of metaverse, on which we analyze the challenges of data
usage in metaverse, especially data privacy. Next, we review and summarize
state-of-the-art solutions based on federated learning, differential privacy,
homomorphic encryption, and zero-knowledge proofs for different privacy
problems in metaverse. Finally, we show the current security and privacy
challenges in the development of metaverse and provide open directions for
building a well-established privacy-preserving metaverse system. For easy
access and reference, we integrate the related publications and their codes
into a GitHub repository:
https://github.com/6lyc/Awesome-Privacy-Computing-in-Metaverse.git.","['Chuan Chen', 'Yuecheng Li', 'Zhenpeng Wu', 'Chengyuan Mai', 'Youming Liu', 'Yanming Hu', 'Zibin Zheng', 'Jiawen Kang']",2023-04-23T13:05:58Z,http://arxiv.org/abs/2304.11643v2,"['cs.CR', 'cs.CY']","Privacy Computing,Metaverse,Taxonomy,Challenges,Augmented Reality,Virtual Reality,Blockchain,Federated Learning,Differential Privacy,Homomorphic Encryption"
"Gen-NeRF: Efficient and Generalizable Neural Radiance Fields via
  Algorithm-Hardware Co-Design","Novel view synthesis is an essential functionality for enabling immersive
experiences in various Augmented- and Virtual-Reality (AR/VR) applications, for
which generalizable Neural Radiance Fields (NeRFs) have gained increasing
popularity thanks to their cross-scene generalization capability. Despite their
promise, the real-device deployment of generalizable NeRFs is bottlenecked by
their prohibitive complexity due to the required massive memory accesses to
acquire scene features, causing their ray marching process to be
memory-bounded. To this end, we propose Gen-NeRF, an algorithm-hardware
co-design framework dedicated to generalizable NeRF acceleration, which for the
first time enables real-time generalizable NeRFs. On the algorithm side,
Gen-NeRF integrates a coarse-then-focus sampling strategy, leveraging the fact
that different regions of a 3D scene contribute differently to the rendered
pixel, to enable sparse yet effective sampling. On the hardware side, Gen-NeRF
highlights an accelerator micro-architecture to maximize the data reuse
opportunities among different rays by making use of their epipolar geometric
relationship. Furthermore, our Gen-NeRF accelerator features a customized
dataflow to enhance data locality during point-to-hardware mapping and an
optimized scene feature storage strategy to minimize memory bank conflicts.
Extensive experiments validate the effectiveness of our proposed Gen-NeRF
framework in enabling real-time and generalizable novel view synthesis.","['Yonggan Fu', 'Zhifan Ye', 'Jiayi Yuan', 'Shunyao Zhang', 'Sixu Li', 'Haoran You', 'Yingyan Lin']",2023-04-24T06:22:06Z,http://arxiv.org/abs/2304.11842v3,"['cs.CV', 'cs.AR']","Neural Radiance Fields,View Synthesis,Augmented Reality,Virtual Reality,Algorithm-Hardware Co-Design,Memory Accesses,Ray Marching,Accelerator Micro-Architecture,Data Reuse,Data Locality"
Microgravity induces overconfidence in perceptual decision-making,"Does gravity affect decision-making? This question comes into sharp focus as
plans for interplanetary human space missions solidify. In the framework of
Bayesian brain theories, gravity encapsulates a strong prior, anchoring agents
to a reference frame via the vestibular system, informing their decisions and
possibly their integration of uncertainty. What happens when such a strong
prior is altered? We address this question using a self-motion estimation task
in a space analog environment under conditions of altered gravity. Two
participants were cast as remote drone operators orbiting Mars in a virtual
reality environment on board a parabolic flight, where both hyper- and
microgravity conditions were induced. From a first-person perspective,
participants viewed a drone exiting a cave and had to first predict a collision
and then provide a confidence estimate of their response. We evoked uncertainty
in the task by manipulating the motion's trajectory angle. Post-decision
subjective confidence reports were negatively predicted by stimulus
uncertainty, as expected. Uncertainty alone did not impact overt behavioral
responses (performance, choice) differentially across gravity conditions.
However microgravity predicted higher subjective confidence, especially in
interaction with stimulus uncertainty. These results suggest that variables
relating to uncertainty affect decision-making distinctly in microgravity,
highlighting the possible need for automatized, compensatory mechanisms when
considering human factors in space research.","['Leyla Loued-Khenissi', 'Christian Pfeiffer', 'Rupal Saxena', 'Shivam Adarsh', 'Davide Scaramuzza']",2023-04-24T14:37:55Z,http://arxiv.org/abs/2304.12133v3,"['cs.RO', 'cs.HC']","microgravity,decision-making,Bayesian brain theories,vestibular system,uncertainty,self-motion estimation,virtual reality,parabolic flight,subjective confidence,human factors"
"Instant-3D: Instant Neural Radiance Field Training Towards On-Device
  AR/VR 3D Reconstruction","Neural Radiance Field (NeRF) based 3D reconstruction is highly desirable for
immersive Augmented and Virtual Reality (AR/VR) applications, but achieving
instant (i.e., < 5 seconds) on-device NeRF training remains a challenge. In
this work, we first identify the inefficiency bottleneck: the need to
interpolate NeRF embeddings up to 200,000 times from a 3D embedding grid during
each training iteration. To alleviate this, we propose Instant-3D, an
algorithm-hardware co-design acceleration framework that achieves instant
on-device NeRF training. Our algorithm decomposes the embedding grid
representation in terms of color and density, enabling computational redundancy
to be squeezed out by adopting different (1) grid sizes and (2) update
frequencies for the color and density branches. Our hardware accelerator
further reduces the dominant memory accesses for embedding grid interpolation
by (1) mapping multiple nearby points' memory read requests into one during the
feed-forward process, (2) merging embedding grid updates from the same sliding
time window during back-propagation, and (3) fusing different computation cores
to support the different grid sizes needed by the color and density branches of
Instant-3D algorithm. Extensive experiments validate the effectiveness of
Instant-3D, achieving a large training time reduction of 41x - 248x while
maintaining the same reconstruction quality. Excitingly, Instant-3D has enabled
instant 3D reconstruction for AR/VR, requiring a reconstruction time of only
1.6 seconds per scene and meeting the AR/VR power consumption constraint of 1.9
W.","['Sixu Li', 'Chaojian Li', 'Wenbo Zhu', 'Boyang', 'Yu', 'Yang', 'Zhao', 'Cheng Wan', 'Haoran You', 'Huihong Shi', 'Yingyan', 'Lin']",2023-04-24T21:53:58Z,http://arxiv.org/abs/2304.12467v2,['cs.AR'],"Neural Radiance Field,3D reconstruction,On-Device,AR/VR,Training,Embedding grid,Acceleration framework,Memory accesses,Computation cores"
"Uncertainty-aware Self-supervised Learning for Cross-domain Technical
  Skill Assessment in Robot-assisted Surgery","Objective technical skill assessment is crucial for effective training of new
surgeons in robot-assisted surgery. With advancements in surgical training
programs in both physical and virtual environments, it is imperative to develop
generalizable methods for automatically assessing skills. In this paper, we
propose a novel approach for skill assessment by transferring domain knowledge
from labeled kinematic data to unlabeled data. Our approach leverages labeled
data from common surgical training tasks such as Suturing, Needle Passing, and
Knot Tying to jointly train a model with both labeled and unlabeled data.
Pseudo labels are generated for the unlabeled data through an iterative manner
that incorporates uncertainty estimation to ensure accurate labeling. We
evaluate our method on a virtual reality simulated training task (Ring
Transfer) using data from the da Vinci Research Kit (dVRK). The results show
that trainees with robotic assistance have significantly higher expert
probability compared to these without any assistance, p < 0.05, which aligns
with previous studies showing the benefits of robotic assistance in improving
training proficiency. Our method offers a significant advantage over other
existing works as it does not require manual labeling or prior knowledge of the
surgical training task for robot-assisted surgery.","['Ziheng Wang', 'Andrea Mariani', 'Arianna Menciassi', 'Elena De Momi', 'Ann Majewicz Fey']",2023-04-28T01:52:18Z,http://arxiv.org/abs/2304.14589v1,"['cs.RO', 'cs.AI', 'cs.CV']","Self-supervised learning,Uncertainty-aware,Cross-domain,Technical skill assessment,Robot-assisted surgery,Kinematic data,Pseudo labels,Virtual reality simulation,Da Vinci Research Kit,Robotic assistance"
"Improved Static Hand Gesture Classification on Deep Convolutional Neural
  Networks using Novel Sterile Training Technique","In this paper, we investigate novel data collection and training techniques
towards improving classification accuracy of non-moving (static) hand gestures
using a convolutional neural network (CNN) and
frequency-modulated-continuous-wave (FMCW) millimeter-wave (mmWave) radars.
Recently, non-contact hand pose and static gesture recognition have received
considerable attention in many applications ranging from human-computer
interaction (HCI), augmented/virtual reality (AR/VR), and even therapeutic
range of motion for medical applications. While most current solutions rely on
optical or depth cameras, these methods require ideal lighting and temperature
conditions. mmWave radar devices have recently emerged as a promising
alternative offering low-cost system-on-chip sensors whose output signals
contain precise spatial information even in non-ideal imaging conditions.
Additionally, deep convolutional neural networks have been employed extensively
in image recognition by learning both feature extraction and classification
simultaneously. However, little work has been done towards static gesture
recognition using mmWave radars and CNNs due to the difficulty involved in
extracting meaningful features from the radar return signal, and the results
are inferior compared with dynamic gesture classification. This article
presents an efficient data collection approach and a novel technique for deep
CNN training by introducing ``sterile'' images which aid in distinguishing
distinct features among the static gestures and subsequently improve the
classification accuracy. Applying the proposed data collection and training
methods yields an increase in classification rate of static hand gestures from
$85\%$ to $93\%$ and $90\%$ to $95\%$ for range and range-angle profiles,
respectively.","['Josiah Smith', 'Shiva Thiagarajan', 'Richard Willis', 'Yiorgos Makris', 'Murat Torlak']",2023-05-03T11:10:50Z,http://arxiv.org/abs/2305.02039v1,"['cs.CV', 'cs.AI', 'eess.SP']","deep convolutional neural networks,static hand gestures,classification accuracy,data collection,training techniques,mmWave radar,feature extraction,classification,sterile training,static gesture recognition"
"Examining the Factors of Place Sameness: A Classroom Re-creation Task in
  a Virtual Environment","Virtual re-creations of real-world places are attractive and becoming more
popular. Attractiveness of re-created place is commonly determined by the
degree of visual similarity to the original places. Even if the re-creation has
poor similarity to the original, there are cases in which people recognize and
allow the re-creation to have similarity as the same entity. However, the
mechanisms and factors behind this contradiction have not yet been studied.
This is the first study focusing on the activity and
meaningofpeoplewhentheyfeelsamenesstoare-createdplace. Thispaper investigates
the concept of place sameness, which is defined as the degree of similarity
between an original and a virtual re-creation that represents the original
place. Further, the factors of place sameness are explored using a classroom
re-creation task with virtual reality (VR) technology. The participants were
instructed to recreate two classrooms within virtual
environmentsusingvirtualfixtures. Asresults, thedisplaydevicesinRoom B, the
student desks in Room A at the memory index, display devices in
RoomA,andstudentdesksandothersinRoomBattheaveragedistancehad moderate or large
correlations with the place sameness index. The results suggest that the
reproducibility of the number of objects related to activities in a place, and
the inaccuracy of the positions of objects are factors of place sameness. Our
own interpretation of the uncanny valley effect of a place was also partially
observed. The main contributions of this study are the proposal of the concept
of place sameness as a new perspective for virtual re-creation research and the
finding of promising factors for that.","['Saizo Aoyagi', 'Satoshi Fukumori', 'Kenji Hirose', 'Takayoshi Kitamura']",2023-05-08T04:09:14Z,http://arxiv.org/abs/2305.04450v1,['cs.HC'],"place sameness,virtual environment,virtual reality (VR) technology,virtual re-creation,visual similarity,memory index,display devices,student desks,reproducibility,uncanny valley effect"
"A sensor fusion approach for improving implementation speed and accuracy
  of RTAB-Map algorithm based indoor 3D mapping","In recent years, 3D mapping for indoor environments has undergone
considerable research and improvement because of its effective applications in
various fields, including robotics, autonomous navigation, and virtual reality.
Building an accurate 3D map for indoor environment is challenging due to the
complex nature of the indoor space, the problem of real-time embedding and
positioning errors of the robot system. This study proposes a method to improve
the accuracy, speed, and quality of 3D indoor mapping by fusing data from the
Inertial Measurement System (IMU) of the Intel Realsense D435i camera, the
Ultrasonic-based Indoor Positioning System (IPS), and the encoder of the
robot's wheel using the extended Kalman filter (EKF) algorithm. The merged data
is processed using a Real-time Image Based Mapping algorithm (RTAB-Map), with
the processing frequency updated in synch with the position frequency of the
IPS device. The results suggest that fusing IMU and IPS data significantly
improves the accuracy, mapping time, and quality of 3D maps. Our study
highlights the proposed method's potential to improve indoor mapping in various
fields, indicating that the fusion of multiple data sources can be a valuable
tool in creating high-quality 3D indoor maps.","['Hoang-Anh Phan', 'Phuc Vinh Nguyen', 'Thu Hang Thi Khuat', 'Hieu Dang Van', 'Dong Huu Quoc Tran', 'Bao Lam Dang', 'Tung Thanh Bui', 'Van Nguyen Thi Thanh', 'Trinh Chu Duc']",2023-05-08T10:08:55Z,http://arxiv.org/abs/2305.04594v1,['cs.RO'],"sensor fusion,implementation speed,accuracy,RTAB-Map algorithm,indoor 3D mapping,Inertial Measurement System,IMU,Ultrasonic-based Indoor Positioning System,IPS,extended Kalman filter,EKF,Real-time Image Based Mapping algorithm"
Active Huygens' metasurface based on in-situ grown conductive polymer,"Active metasurfaces provide unique advantages for on-demand light
manipulation at a subwavelength scale for emerging applications of 3D displays,
augmented/virtual reality (AR/VR) glasses, holographic projectors and light
detection and ranging (LiDAR). These applications put stringent requirements on
switching speed, cycling duration, controllability over intermediate states,
modulation contrast, optical efficiency and operation voltages. However,
previous demonstrations focus only on particular subsets of these key
performance requirements for device implementation, while the other performance
metrics have remained too low for any practical use. Here, we demonstrate an
active Huygens' metasurface based on in-situ grown conductive polymer with
holistic switching performance, including switching speed of 60 frames per
second (fps), switching duration of more than 2000 switching cycles without
noticeable degradation, hysteresis-free controllability over intermediate
states, modulation contrast of over 1400%, optical efficiency of 28% and
operation voltage range within 1 V. Our active metasurface design meets all
foundational requirements for display applications and can be readily
incorporated into other metasurface concepts to deliver high-reliability
electrical control over its optical response, paving the way for compact and
robust electro-optic metadevices.","['Wenzheng Lu', 'Leonarde de S. Menezes', 'Andreas Tittl', 'Haoran Ren', 'Stefan A. Maier']",2023-05-12T10:07:14Z,http://arxiv.org/abs/2305.07356v1,"['physics.optics', 'physics.app-ph']","active metasurface,conductive polymer,light manipulation,3D displays,augmented/virtual reality,holographic projectors,LiDAR,switching speed,modulation contrast,optical efficiency"
"Streaming 360-degree VR Video with Statistical QoS Provisioning in
  mmWave Networks from Delay and Rate Perspectives","Millimeter-wave(mmWave) technology has emerged as a promising enabler for
unleashing the full potential of 360-degree virtual reality (VR). However, the
explosive growth of VR services, coupled with the reliability issues of mmWave
communications, poses enormous challenges in terms of wireless resource and
quality-of-service (QoS) provisioning for mmWave-enabled 360-degree VR. In this
paper, we propose an innovative 360-degree VR streaming architecture that
addresses three under-exploited issues: overlapping field-of-views (FoVs),
statistical QoS provisioning (SQP), and loss-tolerant active data discarding.
Specifically, an overlapping FoV-based optimal joint unicast and multicast
(JUM) task assignment scheme is designed to implement the non-redundant task
assignments, thereby conserving wireless resources remarkably. Furthermore,
leveraging stochastic network calculus, we develop a comprehensive SQP
theoretical framework that encompasses two SQP schemes from delay and rate
perspectives. Additionally, a corresponding optimal adaptive joint time-slot
allocation and active-discarding (ADAPT-JTAAT) transmission scheme is proposed
to minimize resource consumption while guaranteeing diverse statistical QoS
requirements under loss-intolerant and loss-tolerant scenarios from delay and
rate perspectives, respectively. Extensive simulations demonstrate the
effectiveness of the designed overlapping FoV-based JUM optimal task assignment
scheme. Comparisons with six baseline schemes validate that the proposed
optimal ADAPTJTAAT transmission scheme can achieve superior SQP performance in
resource utilization, flexible rate control, and robust queue behaviors.","['Yuang Chen', 'Hancheng Lu', 'Langtian Qin', 'Chang Wu', 'Chang Wen Chen']",2023-05-13T14:57:27Z,http://arxiv.org/abs/2305.07935v1,"['cs.IT', 'eess.IV', 'math.IT']","Streaming,360-degree,VR,mmWave Networks,QoS,Provisioning,Delay,Rate,Statistical,Architecture"
"Assessor360: Multi-sequence Network for Blind Omnidirectional Image
  Quality Assessment","Blind Omnidirectional Image Quality Assessment (BOIQA) aims to objectively
assess the human perceptual quality of omnidirectional images (ODIs) without
relying on pristine-quality image information. It is becoming more significant
with the increasing advancement of virtual reality (VR) technology. However,
the quality assessment of ODIs is severely hampered by the fact that the
existing BOIQA pipeline lacks the modeling of the observer's browsing process.
To tackle this issue, we propose a novel multi-sequence network for BOIQA
called Assessor360, which is derived from the realistic multi-assessor ODI
quality assessment procedure. Specifically, we propose a generalized Recursive
Probability Sampling (RPS) method for the BOIQA task, combining content and
details information to generate multiple pseudo-viewport sequences from a given
starting point. Additionally, we design a Multi-scale Feature Aggregation (MFA)
module with a Distortion-aware Block (DAB) to fuse distorted and semantic
features of each viewport. We also devise Temporal Modeling Module (TMM) to
learn the viewport transition in the temporal domain. Extensive experimental
results demonstrate that Assessor360 outperforms state-of-the-art methods on
multiple OIQA datasets. The code and models are available at
https://github.com/TianheWu/Assessor360.","['Tianhe Wu', 'Shuwei Shi', 'Haoming Cai', 'Mingdeng Cao', 'Jing Xiao', 'Yinqiang Zheng', 'Yujiu Yang']",2023-05-18T13:55:28Z,http://arxiv.org/abs/2305.10983v3,"['cs.CV', 'eess.IV']","Blind Omnidirectional Image Quality Assessment,Assessor360,Multi-sequence Network,Virtual Reality,Recursive Probability Sampling,Multi-scale Feature Aggregation,Distortion-aware Block,Temporal Modeling Module,OIQA datasets"
Few-shot 3D Shape Generation,"Realistic and diverse 3D shape generation is helpful for a wide variety of
applications such as virtual reality, gaming, and animation. Modern generative
models, such as GANs and diffusion models, learn from large-scale datasets and
generate new samples following similar data distributions. However, when
training data is limited, deep neural generative networks overfit and tend to
replicate training samples. Prior works focus on few-shot image generation to
produce high-quality and diverse results using a few target images.
Unfortunately, abundant 3D shape data is typically hard to obtain as well. In
this work, we make the first attempt to realize few-shot 3D shape generation by
adapting generative models pre-trained on large source domains to target
domains using limited data. To relieve overfitting and keep considerable
diversity, we propose to maintain the probability distributions of the pairwise
relative distances between adapted samples at feature-level and shape-level
during domain adaptation. Our approach only needs the silhouettes of few-shot
target samples as training data to learn target geometry distributions and
achieve generated shapes with diverse topology and textures. Moreover, we
introduce several metrics to evaluate the quality and diversity of few-shot 3D
shape generation. The effectiveness of our approach is demonstrated
qualitatively and quantitatively under a series of few-shot 3D shape adaptation
setups.","['Jingyuan Zhu', 'Huimin Ma', 'Jiansheng Chen', 'Jian Yuan']",2023-05-19T13:30:10Z,http://arxiv.org/abs/2305.11664v1,['cs.CV'],"few-shot,3D shape generation,generative models,GANs,diffusion models,deep neural networks,domain adaptation,pairwise relative distances,silhouette,metrics"
"Learning from demonstrations: An intuitive VR environment for imitation
  learning of construction robots","Construction robots are challenging the traditional paradigm of labor
intensive and repetitive construction tasks. Present concerns regarding
construction robots are focused on their abilities in performing complex tasks
consisting of several subtasks and their adaptability to work in unstructured
and dynamic construction environments. Imitation learning (IL) has shown
advantages in training a robot to imitate expert actions in complex tasks and
the policy thereafter generated by reinforcement learning (RL) is more adaptive
in comparison with pre-programmed robots. In this paper, we proposed a
framework composed of two modules for imitation learning of construction
robots. The first module provides an intuitive expert demonstration collection
Virtual Reality (VR) platform where a robot will automatically follow the
position, rotation, and actions of the expert's hand in real-time, instead of
requiring an expert to control the robot via controllers. The second module
provides a template for imitation learning using observations and actions
recorded in the first module. In the second module, Behavior Cloning (BC) is
utilized for pre-training, Generative Adversarial Imitation Learning (GAIL) and
Proximal Policy Optimization (PPO) are combined to achieve a trade-off between
the strength of imitation vs. exploration. Results show that imitation
learning, especially when combined with PPO, could significantly accelerate
training in limited training steps and improve policy performance.","['Kangkang Duan', 'Zhengbo Zou']",2023-05-23T23:46:57Z,http://arxiv.org/abs/2305.14584v1,['cs.RO'],"Construction robots,Imitation learning,Virtual Reality (VR),Expert demonstration,Reinforcement learning (RL),Behavior Cloning (BC),Generative Adversarial Imitation Learning (GAIL),Proximal Policy Optimization (PPO),Training,Policy performance."
"Exploring Human Response Times to Combinations of Audio, Haptic, and
  Visual Stimuli from a Mobile Device","Auditory, haptic, and visual stimuli provide alerts, notifications, and
information for a wide variety of applications ranging from virtual reality to
wearable and hand-held devices. Response times to these stimuli have been used
to assess motor control and design human-computer interaction systems. In this
study, we investigate human response times to 26 combinations of auditory,
haptic, and visual stimuli at three levels (high, low, and off). We developed
an iOS app that presents these stimuli in random intervals and records response
times on an iPhone 11. We conducted a user study with 20 participants and found
that response time decreased with more types and higher levels of stimuli. The
low visual condition had the slowest mean response time (mean +/- standard
deviation, 528 +/- 105 ms) and the condition with high levels of audio, haptic,
and visual stimuli had the fastest mean response time (320 +/- 43 ms). This
work quantifies response times to multi-modal stimuli, identifies interactions
between different stimuli types and levels, and introduces an app-based method
that can be widely distributed to measure response time. Understanding
preferences and response times for stimuli can provide insight into designing
devices for human-machine interaction.","['Kyle T. Yoshida', 'Joel X. Kiernan', 'Allison M. Okamura', 'Cara M. Nunez']",2023-05-26T18:08:58Z,http://arxiv.org/abs/2305.17180v1,['cs.HC'],"audio,haptic,visual stimuli,response times,combinations,mobile device,human-computer interaction,iOS app,user study"
Controllable Motion Diffusion Model,"Generating realistic and controllable motions for virtual characters is a
challenging task in computer animation, and its implications extend to games,
simulations, and virtual reality. Recent studies have drawn inspiration from
the success of diffusion models in image generation, demonstrating the
potential for addressing this task. However, the majority of these studies have
been limited to offline applications that target at sequence-level generation
that generates all steps simultaneously. To enable real-time motion synthesis
with diffusion models in response to time-varying control signals, we propose
the framework of the Controllable Motion Diffusion Model (COMODO). Our
framework begins with an auto-regressive motion diffusion model (A-MDM), which
generates motion sequences step by step. In this way, simply using the standard
DDPM algorithm without any additional complexity, our framework is able to
generate high-fidelity motion sequences over extended periods with different
types of control signals. Then, we propose our reinforcement learning-based
controller and controlling strategies on top of the A-MDM model, so that our
framework can steer the motion synthesis process across multiple tasks,
including target reaching, joystick-based control, goal-oriented control, and
trajectory following. The proposed framework enables the real-time generation
of diverse motions that react adaptively to user commands on-the-fly, thereby
enhancing the overall user experience. Besides, it is compatible with the
inpainting-based editing methods and can predict much more diverse motions
without additional fine-tuning of the basic motion generation models. We
conduct comprehensive experiments to evaluate the effectiveness of our
framework in performing various tasks and compare its performance against
state-of-the-art methods.","['Yi Shi', 'Jingbo Wang', 'Xuekun Jiang', 'Bo Dai']",2023-06-01T07:48:34Z,http://arxiv.org/abs/2306.00416v1,"['cs.CV', 'cs.AI', 'cs.GR']","controllable motion,diffusion model,computer animation,real-time motion synthesis,auto-regressive,reinforcement learning,controller,trajectory following,inpainting-based editing,motion generation"
"PanoGRF: Generalizable Spherical Radiance Fields for Wide-baseline
  Panoramas","Achieving an immersive experience enabling users to explore virtual
environments with six degrees of freedom (6DoF) is essential for various
applications such as virtual reality (VR). Wide-baseline panoramas are commonly
used in these applications to reduce network bandwidth and storage
requirements. However, synthesizing novel views from these panoramas remains a
key challenge. Although existing neural radiance field methods can produce
photorealistic views under narrow-baseline and dense image captures, they tend
to overfit the training views when dealing with \emph{wide-baseline} panoramas
due to the difficulty in learning accurate geometry from sparse $360^{\circ}$
views. To address this problem, we propose PanoGRF, Generalizable Spherical
Radiance Fields for Wide-baseline Panoramas, which construct spherical radiance
fields incorporating $360^{\circ}$ scene priors. Unlike generalizable radiance
fields trained on perspective images, PanoGRF avoids the information loss from
panorama-to-perspective conversion and directly aggregates geometry and
appearance features of 3D sample points from each panoramic view based on
spherical projection. Moreover, as some regions of the panorama are only
visible from one view while invisible from others under wide baseline settings,
PanoGRF incorporates $360^{\circ}$ monocular depth priors into spherical depth
estimation to improve the geometry features. Experimental results on multiple
panoramic datasets demonstrate that PanoGRF significantly outperforms
state-of-the-art generalizable view synthesis methods for wide-baseline
panoramas (e.g., OmniSyn) and perspective images (e.g., IBRNet, NeuRay).","['Zheng Chen', 'Yan-Pei Cao', 'Yuan-Chen Guo', 'Chen Wang', 'Ying Shan', 'Song-Hai Zhang']",2023-06-02T13:35:07Z,http://arxiv.org/abs/2306.01531v2,"['cs.CV', 'cs.GR']","Generalizable Spherical Radiance Fields,Wide-baseline Panoramas,Neural Radiance Field Methods,6DoF,Virtual Reality,Geometry Features,Appearance Features,Monocular Depth Priors,Perspective Images"
"A Survey on Multi-AP Coordination Approaches over Emerging WLANs: Future
  Directions and Open Challenges","Recent advancements in wireless local area network (WLAN) technology include
IEEE 802.11be and 802.11ay, often known as Wi-Fi 7 and WiGig, respectively. The
goal of these developments is to provide Extremely High Throughput (EHT) and
low latency to meet the demands of future applications like as 8K videos,
augmented and virtual reality, the Internet of Things, telesurgery, and other
developing technologies. IEEE 802.11be includes new features such as 320 MHz
bandwidth, multi-link operation, Multi-user Multi-Input Multi-Output,
orthogonal frequency-division multiple access, and Multiple-Access Point
(multi-AP) coordination (MAP-Co) to achieve EHT. With the increase in the
number of overlapping APs and inter-AP interference, researchers have focused
on studying MAP-Co approaches for coordinated transmission in IEEE 802.11be,
making MAP-Co a key feature of future WLANs. Moreover, similar issues may arise
in EHF bands WLAN, particularly for standards beyond IEEE 802.11ay. This has
prompted researchers to investigate the implementation of MAP-Co over future
802.11ay WLANs. Thus, in this article, we provide a comprehensive review of the
state-of-the-art MAP-Co features and their shortcomings concerning emerging
WLAN. Finally, we discuss several novel future directions and open challenges
for MAP-Co.","['Shikhar Verma', 'Tiago Koketsu Rodrigues', 'Yuichi Kawamoto', 'Mostafa M. Fouda', 'Nei Kato']",2023-06-07T05:34:21Z,http://arxiv.org/abs/2306.04164v2,['cs.NI'],"WLAN,IEEE 802.11be,802.11ay,Multi-AP coordination,EHT,latency,8K videos,IoT,telesurgery"
Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields,"The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored ""fog"" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF","['Qianqiu Tan', 'Tao Liu', 'Yinling Xie', 'Shuwan Yu', 'Baohua Zhang']",2023-06-08T15:49:30Z,http://arxiv.org/abs/2306.05303v1,['cs.CV'],"Neural Radiance Fields,3D reconstruction,virtual reality,augmented reality,color balance,decoding architecture,performance evaluation,recognition,outdoor scenes,plug-and-play"
"HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection","An individualised head-related transfer function (HRTF) is very important for
creating realistic virtual reality (VR) and augmented reality (AR)
environments. However, acoustically measuring high-quality HRTFs requires
expensive equipment and an acoustic lab setting. To overcome these limitations
and to make this measurement more efficient HRTF upsampling has been exploited
in the past where a high-resolution HRTF is created from a low-resolution one.
This paper demonstrates how generative adversarial networks (GANs) can be
applied to HRTF upsampling. We propose a novel approach that transforms the
HRTF data for direct use with a convolutional super-resolution generative
adversarial network (SRGAN). This new approach is benchmarked against three
baselines: barycentric upsampling, spherical harmonic (SH) upsampling and an
HRTF selection approach. Experimental results show that the proposed method
outperforms all three baselines in terms of log-spectral distortion (LSD) and
localisation performance using perceptual models when the input HRTF is sparse
(less than 20 measured positions).","['Aidan O. T. Hogg', 'Mads Jenkins', 'He Liu', 'Isaac Squires', 'Samuel J. Cooper', 'Lorenzo Picinali']",2023-06-09T11:05:09Z,http://arxiv.org/abs/2306.05812v2,"['eess.AS', 'cs.CV', 'cs.HC', 'cs.LG', 'cs.SD', 'eess.SP']","HRTF,upsampling,generative adversarial network,gnomonic equiangular projection,virtual reality,augmented reality,convolutional super-resolution,log-spectral distortion,perceptual models"
UniG3D: A Unified 3D Object Generation Dataset,"The field of generative AI has a transformative impact on various areas,
including virtual reality, autonomous driving, the metaverse, gaming, and
robotics. Among these applications, 3D object generation techniques are of
utmost importance. This technique has unlocked fresh avenues in the realm of
creating, customizing, and exploring 3D objects. However, the quality and
diversity of existing 3D object generation methods are constrained by the
inadequacies of existing 3D object datasets, including issues related to text
quality, the incompleteness of multi-modal data representation encompassing 2D
rendered images and 3D assets, as well as the size of the dataset. In order to
resolve these issues, we present UniG3D, a unified 3D object generation dataset
constructed by employing a universal data transformation pipeline on Objaverse
and ShapeNet datasets. This pipeline converts each raw 3D model into
comprehensive multi-modal data representation <text, image, point cloud, mesh>
by employing rendering engines and multi-modal models. These modules ensure the
richness of textual information and the comprehensiveness of data
representation. Remarkably, the universality of our pipeline refers to its
ability to be applied to any 3D dataset, as it only requires raw 3D data. The
selection of data sources for our dataset is based on their scale and quality.
Subsequently, we assess the effectiveness of our dataset by employing Point-E
and SDFusion, two widely recognized methods for object generation, tailored to
the prevalent 3D representations of point clouds and signed distance functions.
Our dataset is available at: https://unig3d.github.io.","['Qinghong Sun', 'Yangguang Li', 'ZeXiang Liu', 'Xiaoshui Huang', 'Fenggang Liu', 'Xihui Liu', 'Wanli Ouyang', 'Jing Shao']",2023-06-19T07:03:45Z,http://arxiv.org/abs/2306.10730v1,['cs.CV'],"generative AI,3D object generation,dataset,Objaverse,ShapeNet,multi-modal data,rendering engines,Point-E,SDFusion"
Facial Expression Re-targeting from a Single Character,"Video retargeting for digital face animation is used in virtual reality,
social media, gaming, movies, and video conference, aiming to animate avatars'
facial expressions based on videos of human faces. The standard method to
represent facial expressions for 3D characters is by blendshapes, a vector of
weights representing the avatar's neutral shape and its variations under facial
expressions, e.g., smile, puff, blinking. Datasets of paired frames with
blendshape vectors are rare, and labeling can be laborious, time-consuming, and
subjective. In this work, we developed an approach that handles the lack of
appropriate datasets. Instead, we used a synthetic dataset of only one
character. To generalize various characters, we re-represented each frame to
face landmarks. We developed a unique deep-learning architecture that groups
landmarks for each facial organ and connects them to relevant blendshape
weights. Additionally, we incorporated complementary methods for facial
expressions that landmarks did not represent well and gave special attention to
eye expressions. We have demonstrated the superiority of our approach to
previous research in qualitative and quantitative metrics. Our approach
achieved a higher MOS of 68% and a lower MSE of 44.2% when tested on videos
with various users and expressions.","['Ariel Larey', 'Omri Asraf', 'Adam Kelder', 'Itzik Wilf', 'Ofer Kruzel', 'Nati Daniel']",2023-06-21T11:35:22Z,http://arxiv.org/abs/2306.12188v1,"['cs.GR', 'cs.AI', 'cs.CV']","Facial Expression,Re-targeting,Character,Video Retargeting,Blendshapes,Deep Learning,Landmarks,Facial Organ,Eye Expressions,Dataset"
"Supporting Construction and Architectural Visualization through BIM and
  AR/VR: A Systematic Literature Review","The Architecture, Engineering, Construction, and Facility Management (AEC/FM)
industry deals with the design, construction, and operation of complex
buildings. Today, Building Information Modeling (BIM) is used to represent
information about a building in a single, non-redundant representation. Here,
Augmented Reality (AR) and Virtual Reality (VR) can improve the visualization
and interaction with the resulting model by augmenting the real world with
information from the BIM model or allowing a user to immerse in a virtual world
generated from the BIM model. This can improve the design, construction, and
operation of buildings. While an increasing number of studies in HCI,
construction, or engineering have shown the potential of using AR and VR
technology together with BIM, often research remains focused on individual
explorations and key design strategies. In addition to that, a systematic
overview and discussion of recent works combining AR/VR with BIM are not yet
fully covered. Therefore, this paper systematically reviews recent approaches
combining AR/VR with BIM and categorizes the literature by the building's
lifecycle phase while systematically describing relevant use cases. In total,
32 out of 447 papers between 2017 and 2022 were categorized. The categorization
shows that most approaches focus on the construction phase and the use case of
review and quality assurance. In the design phase, most approaches use VR,
while in the construction and operation phases, AR is prevalent.","['Enes Yigitbas', 'Alexander Nowosad', 'Gregor Engels']",2023-06-21T13:52:37Z,http://arxiv.org/abs/2306.12274v1,['cs.HC'],"Construction,Architectural Visualization,BIM,AR,VR,Building Information Modeling,Augmented Reality,Virtual Reality,HCI,Facility Management"
"Let's Resonate! How to Elicit Improvisation and Letting Go in
  Interactive Digital Art","Participatory art allows for the spectator to be a participant or a viewer
able to engage actively with interactive art. Real-time technologies offer new
ways to create participative artworks. We hereby investigate how to engage
participation through movement in interactive digital art, and what this
engagement can awaken, focusing on the ways to elicit improvisation and letting
go. We analyze two Virtual Reality installations, ''InterACTE'' and ''Eve,
dance is an unplaceable place,'' involving body movement, dance, creativity and
the presence of an observing audience. We evaluate the premises, the setup, and
the feedback of the spectators in the two installations. We propose a model
following three different perspectives of resonance: 1. Inter Resonance between
Spectator and Artwork, which involves curiosity, imitation, playfulness and
improvisation. 2. Inner Resonance of Spectator him/herself, where embodiment
and creativity contribute to the sense of being present and letting go. 3.
Collective Resonance between Spectator/Artwork and Audience, which is
stimulated by curiosity, and triggers motor contagion, engagement and
gathering. The two analyzed examples seek to awaken open-minded communicative
possibilities through the use of interactive digital artworks. Moreover, the
need to recognize and develop the idea of resonance becomes increasingly
important in this time of urgency to communicate, understand and support
collectivity.","['Jean-François Jégo', 'Margherita Bergamo Meneghini']",2023-06-22T12:09:05Z,http://arxiv.org/abs/2306.12832v1,['cs.GR'],"interactive digital art,participatory art,real-time technologies,movement,improvisation,letting go,Virtual Reality,body movement,dance,creativity"
"PaRUS: A Virtual Reality Shopping Method Focusing on Context between
  Products and Real Usage Scenes","The development of AR and VR technologies is enhancing users' online shopping
experiences in various ways. However, in existing VR shopping applications,
shopping contexts merely refer to the products and virtual malls or
metaphorical scenes where users select products. This leads to the defect that
users can only imagine rather than intuitively feel whether the selected
products are suitable for their real usage scenes, resulting in a significant
discrepancy between their expectations before and after the purchase. To
address this issue, we propose PaRUS, a VR shopping approach that focuses on
the context between products and their real usage scenes. PaRUS begins by
rebuilding the virtual scenario of the products' real usage scene through a new
semantic scene reconstruction pipeline, which preserves both the structured
scene and textured object models in the scene. Afterwards, intuitive
visualization of how the selected products fit the reconstructed virtual scene
is provided. We conducted two user studies to evaluate how PaRUS impacts user
experience, behavior, and satisfaction with their purchase. The results
indicated that PaRUS significantly reduced the perceived performance risk and
improved users' trust and satisfaction with their purchase results.","['Weitao You', 'Yinyu Lu', 'Ziqing Zheng', 'Yizhan Shao', 'Changyuan Yang', 'Zhibin Zhou', 'Lingyun Sun']",2023-06-25T11:19:46Z,http://arxiv.org/abs/2306.14208v2,['cs.HC'],"Virtual reality,Shopping,Context,Products,Real usage scenes,AR,VR technologies,User experience,Semantic scene reconstruction,User studies"
VibHead: An Authentication Scheme for Smart Headsets through Vibration,"Recent years have witnessed the fast penetration of Virtual Reality (VR) and
Augmented Reality (AR) systems into our daily life, the security and privacy
issues of the VR/AR applications have been attracting considerable attention.
Most VR/AR systems adopt head-mounted devices (i.e., smart headsets) to
interact with users and the devices usually store the users' private data.
Hence, authentication schemes are desired for the head-mounted devices.
Traditional knowledge-based authentication schemes for general personal devices
have been proved vulnerable to shoulder-surfing attacks, especially considering
the headsets may block the sight of the users. Although the robustness of the
knowledge-based authentication can be improved by designing complicated secret
codes in virtual space, this approach induces a compromise of usability.
Another choice is to leverage the users' biometrics; however, it either relies
on highly advanced equipments which may not always be available in commercial
headsets or introduce heavy cognitive load to users.
  In this paper, we propose a vibration-based authentication scheme, VibHead,
for smart headsets. Since the propagation of vibration signals through human
heads presents unique patterns for different individuals, VibHead employs a
CNN-based model to classify registered legitimate users based the features
extracted from the vibration signals. We also design a two-step authentication
scheme where the above user classifiers are utilized to distinguish the
legitimate user from illegitimate ones. We implement VibHead on a Microsoft
HoloLens equipped with a linear motor and an IMU sensor which are commonly used
in off-the-shelf personal smart devices. According to the results of our
extensive experiments, with short vibration signals ($\leq 1s$), VibHead has an
outstanding authentication accuracy; both FAR and FRR are around 5%.","['Feng Li', 'Jiayi Zhao', 'Huan Yang', 'Dongxiao Yu', 'Yuanfeng Zhou', 'Yiran Shen']",2023-06-29T15:00:32Z,http://arxiv.org/abs/2306.17002v1,['cs.CR'],"Smart headsets,Authentication scheme,Vibration,Virtual Reality,Augmented Reality,Biometrics,CNN-based model,Propagation of vibration signals,IMU sensor,Authentication accuracy"
"A Mixed Reality System for Interaction with Heterogeneous Robotic
  Systems","The growing spread of robots for service and industrial purposes calls for
versatile, intuitive and portable interaction approaches. In particular, in
industrial environments, operators should be able to interact with robots in a
fast, effective, and possibly effortless manner. To this end, reality
enhancement techniques have been used to achieve efficient management and
simplify interactions, in particular in manufacturing and logistics processes.
Building upon this, in this paper we propose a system based on mixed reality
that allows a ubiquitous interface for heterogeneous robotic systems in dynamic
scenarios, where users are involved in different tasks and need to interact
with different robots. By means of mixed reality, users can interact with a
robot through manipulation of its virtual replica, which is always colocated
with the user and is extracted when interaction is needed. The system has been
tested in a simulated intralogistics setting, where different robots are
present and require sporadic intervention by human operators, who are involved
in other tasks. In our setting we consider the presence of drones and AGVs with
different levels of autonomy, calling for different user interventions. The
proposed approach has been validated in virtual reality, considering
quantitative and qualitative assessment of performance and user's feedback.","['Valeria Villani', 'Beatrice Capelli', 'Lorenzo Sabattini']",2023-07-11T14:18:25Z,http://arxiv.org/abs/2307.05280v2,['cs.RO'],"Mixed reality,Interaction,Heterogeneous robotic systems,Reality enhancement techniques,Manufacturing,Logistics processes,Ubiquitous interface,Virtual replica,Drones,AGVs"
Efficient 3D Articulated Human Generation with Layered Surface Volumes,"Access to high-quality and diverse 3D articulated digital human assets is
crucial in various applications, ranging from virtual reality to social
platforms. Generative approaches, such as 3D generative adversarial networks
(GANs), are rapidly replacing laborious manual content creation tools. However,
existing 3D GAN frameworks typically rely on scene representations that
leverage either template meshes, which are fast but offer limited quality, or
volumes, which offer high capacity but are slow to render, thereby limiting the
3D fidelity in GAN settings. In this work, we introduce layered surface volumes
(LSVs) as a new 3D object representation for articulated digital humans. LSVs
represent a human body using multiple textured mesh layers around a
conventional template. These layers are rendered using alpha compositing with
fast differentiable rasterization, and they can be interpreted as a volumetric
representation that allocates its capacity to a manifold of finite thickness
around the template. Unlike conventional single-layer templates that struggle
with representing fine off-surface details like hair or accessories, our
surface volumes naturally capture such details. LSVs can be articulated, and
they exhibit exceptional efficiency in GAN settings, where a 2D generator
learns to synthesize the RGBA textures for the individual layers. Trained on
unstructured, single-view 2D image datasets, our LSV-GAN generates high-quality
and view-consistent 3D articulated digital humans without the need for
view-inconsistent 2D upsampling networks.","['Yinghao Xu', 'Wang Yifan', 'Alexander W. Bergman', 'Menglei Chai', 'Bolei Zhou', 'Gordon Wetzstein']",2023-07-11T17:50:02Z,http://arxiv.org/abs/2307.05462v1,['cs.CV'],"3D,Articulated,Human Generation,Layered Surface Volumes,GANs,Meshes,Volumes,Rasterization,RGBA textures,LSV-GAN"
"Agilicious: Open-Source and Open-Hardware Agile Quadrotor for
  Vision-Based Flight","Autonomous, agile quadrotor flight raises fundamental challenges for robotics
research in terms of perception, planning, learning, and control. A versatile
and standardized platform is needed to accelerate research and let
practitioners focus on the core problems. To this end, we present Agilicious, a
co-designed hardware and software framework tailored to autonomous, agile
quadrotor flight. It is completely open-source and open-hardware and supports
both model-based and neural-network--based controllers. Also, it provides high
thrust-to-weight and torque-to-inertia ratios for agility, onboard vision
sensors, GPU-accelerated compute hardware for real-time perception and
neural-network inference, a real-time flight controller, and a versatile
software stack. In contrast to existing frameworks, Agilicious offers a unique
combination of flexible software stack and high-performance hardware. We
compare Agilicious with prior works and demonstrate it on different agile
tasks, using both model-based and neural-network--based controllers. Our
demonstrators include trajectory tracking at up to 5g and 70 km/h in a
motion-capture system, and vision-based acrobatic flight and obstacle avoidance
in both structured and unstructured environments using solely onboard
perception. Finally, we demonstrate its use for hardware-in-the-loop simulation
in virtual-reality environments. Thanks to its versatility, we believe that
Agilicious supports the next generation of scientific and industrial quadrotor
research.","['Philipp Foehn', 'Elia Kaufmann', 'Angel Romero', 'Robert Penicka', 'Sihao Sun', 'Leonard Bauersfeld', 'Thomas Laengle', 'Giovanni Cioffi', 'Yunlong Song', 'Antonio Loquercio', 'Davide Scaramuzza']",2023-07-12T11:48:16Z,http://arxiv.org/abs/2307.06100v1,['cs.RO'],"open-source,open-hardware,agile,quadrotor,vision-based flight,perception,planning,learning,control,neural-network"
"Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities","In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.","['Kai Li', 'Billy Pik Lik Lau', 'Xin Yuan', 'Wei Ni', 'Mohsen Guizani', 'Chau Yuen']",2023-07-13T11:14:46Z,http://arxiv.org/abs/2307.06687v2,"['cs.HC', 'cs.AI', 'cs.NI']","ubiquitous,semantic,Metaverse,augmented reality,virtual reality,artificial intelligence,spatio-temporal data representation,Internet of Things,digital twin,context-aware interactions"
"Magic NeRF Lens: Interactive Fusion of Neural Radiance Fields for
  Virtual Facility Inspection","Large industrial facilities such as particle accelerators and nuclear power
plants are critical infrastructures for scientific research and industrial
processes. These facilities are complex systems that not only require regular
maintenance and upgrades but are often inaccessible to humans due to various
safety hazards. Therefore, a virtual reality (VR) system that can quickly
replicate real-world remote environments to provide users with a high level of
spatial and situational awareness is crucial for facility maintenance planning.
However, the exact 3D shapes of these facilities are often too complex to be
accurately modeled with geometric primitives through the traditional
rasterization pipeline.
  In this work, we develop Magic NeRF Lens, an interactive framework to support
facility inspection in immersive VR using neural radiance fields (NeRF) and
volumetric rendering. We introduce a novel data fusion approach that combines
the complementary strengths of volumetric rendering and geometric
rasterization, allowing a NeRF model to be merged with other conventional 3D
data, such as a computer-aided design model. We develop two novel 3D magic lens
effects to optimize NeRF rendering by exploiting the properties of human vision
and context-aware visualization. We demonstrate the high usability of our
framework and methods through a technical benchmark, a visual search user
study, and expert reviews. In addition, the source code of our VR NeRF
framework is made publicly available for future research and development.","['Ke Li', 'Susanne Schmidt', 'Tim Rolff', 'Reinhard Bacher', 'Wim Leemans', 'Frank Steinicke']",2023-07-19T09:43:47Z,http://arxiv.org/abs/2307.09860v1,"['cs.GR', 'cs.HC']","Neural Radiance Fields,Virtual Reality,Facility Inspection,Volumetric Rendering,3D Modeling,Geometric Primitives,Magic Lens Effects,Data Fusion,Computer-Aided Design,Human Vision"
"Contextual Beamforming: Exploiting Location and AI for Enhanced Wireless
  Telecommunication Performance","The pervasive nature of wireless telecommunication has made it the foundation
for mainstream technologies like automation, smart vehicles, virtual reality,
and unmanned aerial vehicles. As these technologies experience widespread
adoption in our daily lives, ensuring the reliable performance of cellular
networks in mobile scenarios has become a paramount challenge. Beamforming, an
integral component of modern mobile networks, enables spatial selectivity and
improves network quality. However, many beamforming techniques are iterative,
introducing unwanted latency to the system. In recent times, there has been a
growing interest in leveraging mobile users' location information to expedite
beamforming processes. This paper explores the concept of contextual
beamforming, discussing its advantages, disadvantages and implications.
Notably, the study presents an impressive 53% improvement in signal-to-noise
ratio (SNR) by implementing the adaptive beamforming (MRT) algorithm compared
to scenarios without beamforming. It further elucidates how MRT contributes to
contextual beamforming. The importance of localization in implementing
contextual beamforming is also examined. Additionally, the paper delves into
the use of artificial intelligence schemes, including machine learning and deep
learning, in implementing contextual beamforming techniques that leverage user
location information. Based on the comprehensive review, the results suggest
that the combination of MRT and Zero forcing (ZF) techniques, alongside deep
neural networks (DNN) employing Bayesian Optimization (BO), represents the most
promising approach for contextual beamforming. Furthermore, the study discusses
the future potential of programmable switches, such as Tofino, in enabling
location-aware beamforming.","['Jaspreet Kaur', 'Satyam Bhatti', 'Olaoluwa R Popoola', 'Muhammad Ali Imran', 'Rami Ghannam', 'Qammer H Abbasi', 'Hasan T Abbas']",2023-07-02T11:56:04Z,http://arxiv.org/abs/2307.10183v1,"['cs.IT', 'cs.SY', 'eess.SY', 'math.IT']","beamforming,wireless telecommunication,AI,location information,cellular networks,spatial selectivity,signal-to-noise ratio (SNR),machine learning,deep learning,neural networks"
PHYFU: Fuzzing Modern Physics Simulation Engines,"A physical simulation engine (PSE) is a software system that simulates
physical environments and objects. Modern PSEs feature both forward and
backward simulations, where the forward phase predicts the behavior of a
simulated system, and the backward phase provides gradients (guidance) for
learning-based control tasks, such as a robot arm learning to fetch items. This
way, modern PSEs show promising support for learning-based control methods. To
date, PSEs have been largely used in various high-profitable, commercial
applications, such as games, movies, virtual reality (VR), and robotics.
Despite the prosperous development and usage of PSEs by academia and industrial
manufacturers such as Google and NVIDIA, PSEs may produce incorrect
simulations, which may lead to negative results, from poor user experience in
entertainment to accidents in robotics-involved manufacturing and surgical
operations.
  This paper introduces PHYFU, a fuzzing framework designed specifically for
PSEs to uncover errors in both forward and backward simulation phases. PHYFU
mutates initial states and asserts if the PSE under test behaves consistently
with respect to basic Physics Laws (PLs). We further use feedback-driven test
input scheduling to guide and accelerate the search for errors. Our study of
four PSEs covers mainstream industrial vendors (Google and NVIDIA) as well as
academic products. We successfully uncover over 5K error-triggering inputs that
generate incorrect simulation results spanning across the whole software stack
of PSEs.","['Dongwei Xiao', 'Zhibo Liu', 'Shuai Wang']",2023-07-20T12:26:50Z,http://arxiv.org/abs/2307.10818v2,['cs.SE'],"physics simulation engine,modern,forward simulation,backward simulation,gradients,learning-based control,fuzzing,errors,physics laws,feedback-driven"
"Systematic Adaptation of Communication-focused Machine Learning Models
  from Real to Virtual Environments for Human-Robot Collaboration","Virtual reality has proved to be useful in applications in several fields
ranging from gaming, medicine, and training to development of interfaces that
enable human-robot collaboration. It empowers designers to explore applications
outside of the constraints posed by the real world environment and develop
innovative solutions and experiences. Hand gestures recognition which has been
a topic of much research and subsequent commercialization in the real world has
been possible because of the creation of large, labelled datasets. In order to
utilize the power of natural and intuitive hand gestures in the virtual domain
for enabling embodied teleoperation of collaborative robots, similarly large
datasets must be created so as to keep the working interface easy to learn and
flexible enough to add more gestures. Depending on the application, this may be
computationally or economically prohibitive. Thus, the adaptation of trained
deep learning models that perform well in the real environment to the virtual
may be a solution to this challenge. This paper presents a systematic framework
for the real to virtual adaptation using limited size of virtual dataset along
with guidelines for creating a curated dataset. Finally, while hand gestures
have been considered as the communication mode, the guidelines and
recommendations presented are generic. These are applicable to other modes such
as body poses and facial expressions which have large datasets available in the
real domain which must be adapted to the virtual one.","['Debasmita Mukherjee', 'Ritwik Singhai', 'Homayoun Najjaran']",2023-07-21T03:24:55Z,http://arxiv.org/abs/2307.11327v1,"['cs.HC', 'cs.LG', 'cs.RO']","machine learning,virtual environments,human-robot collaboration,hand gestures recognition,deep learning models,virtual dataset,communication mode,body poses,facial expressions"
"ExWarp: Extrapolation and Warping-based Temporal Supersampling for
  High-frequency Displays","High-frequency displays are gaining immense popularity because of their
increasing use in video games and virtual reality applications. However, the
issue is that the underlying GPUs cannot continuously generate frames at this
high rate -- this results in a less smooth and responsive experience.
Furthermore, if the frame rate is not synchronized with the refresh rate, the
user may experience screen tearing and stuttering. Previous works propose
increasing the frame rate to provide a smooth experience on modern displays by
predicting new frames based on past or future frames. Interpolation and
extrapolation are two widely used algorithms that predict new frames.
Interpolation requires waiting for the future frame to make a prediction, which
adds additional latency. On the other hand, extrapolation provides a better
quality of experience because it relies solely on past frames -- it does not
incur any additional latency. The simplest method to extrapolate a frame is to
warp the previous frame using motion vectors; however, the warped frame may
contain improperly rendered visual artifacts due to dynamic objects -- this
makes it very challenging to design such a scheme. Past work has used DNNs to
get good accuracy, however, these approaches are slow. This paper proposes
Exwarp -- an approach based on reinforcement learning (RL) to intelligently
choose between the slower DNN-based extrapolation and faster warping-based
methods to increase the frame rate by 4x with an almost negligible reduction in
the perceived image quality.","['Akanksha Dixit', 'Yashashwee Chakrabarty', 'Smruti R. Sarangi']",2023-07-24T08:32:27Z,http://arxiv.org/abs/2307.12607v1,"['cs.GR', 'cs.AR', 'cs.CV', 'cs.LG']","High-frequency displays,GPUs,frames,refresh rate,screen tearing,interpolation,extrapolation,latency,motion vectors,reinforcement learning"
"A Large-Scale Feasibility Study of Screen-based 3D Visualization and
  Augmented Reality Tools for Human Anatomy Education: Exploring Gender
  Perspectives in Learning Experience","Anatomy education is an indispensable part of medical training, but
traditional methods face challenges like limited resources for dissection in
large classes and difficulties understanding 2D anatomy in textbooks. Advanced
technologies, such as 3D visualization and augmented reality (AR), are
transforming anatomy learning. This paper presents two in-house solutions that
use handheld tablets or screen-based AR to visualize 3D anatomy models with
informative labels and in-situ visualizations of the muscle anatomy. To assess
these tools, a user study of muscle anatomy education involved 236 premedical
students in dyadic teams, with results showing that the tablet-based 3D
visualization and screen-based AR tools led to significantly higher learning
experience scores than traditional textbook. While knowledge retention didn't
differ significantly, ethnographic and gender analysis showed that male
students generally reported more positive learning experiences than female
students. This study discusses the implications for anatomy and medical
education, highlighting the potential of these innovative learning tools
considering gender and team dynamics in body painting anatomy learning
interventions.","['Roghayeh Leila Barmaki', 'Kangsoo Kim', 'Zhang Guo', 'Qile Wang', 'Kevin Yu', 'Rebecca Pearlman', 'Nassir Navab']",2023-07-26T01:28:58Z,http://arxiv.org/abs/2307.14383v2,"['cs.HC', 'cs.GR']","3D visualization,Augmented Reality,Human anatomy education,Gender perspectives,Learning experience,Anatomy learning,Muscle anatomy,Tablets,Screen-based AR,Medical education"
Dynamic PlenOctree for Adaptive Sampling Refinement in Explicit NeRF,"The explicit neural radiance field (NeRF) has gained considerable interest
for its efficient training and fast inference capabilities, making it a
promising direction such as virtual reality and gaming. In particular,
PlenOctree (POT)[1], an explicit hierarchical multi-scale octree
representation, has emerged as a structural and influential framework. However,
POT's fixed structure for direct optimization is sub-optimal as the scene
complexity evolves continuously with updates to cached color and density,
necessitating refining the sampling distribution to capture signal complexity
accordingly. To address this issue, we propose the dynamic PlenOctree DOT,
which adaptively refines the sample distribution to adjust to changing scene
complexity. Specifically, DOT proposes a concise yet novel hierarchical feature
fusion strategy during the iterative rendering process. Firstly, it identifies
the regions of interest through training signals to ensure adaptive and
efficient refinement. Next, rather than directly filtering out valueless nodes,
DOT introduces the sampling and pruning operations for octrees to aggregate
features, enabling rapid parameter learning. Compared with POT, our DOT
outperforms it by enhancing visual quality, reducing over $55.15$/$68.84\%$
parameters, and providing 1.7/1.9 times FPS for NeRF-synthetic and Tanks $\&$
Temples, respectively. Project homepage:https://vlislab22.github.io/DOT.
  [1] Yu, Alex, et al. ""Plenoctrees for real-time rendering of neural radiance
fields."" Proceedings of the IEEE/CVF International Conference on Computer
Vision. 2021.","['Haotian Bai', 'Yiqi Lin', 'Yize Chen', 'Lin Wang']",2023-07-28T06:21:42Z,http://arxiv.org/abs/2307.15333v1,['cs.CV'],"Dynamic PlenOctree,Adaptive Sampling Refinement,Explicit NeRF,hierarchical,multi-scale,octree representation,sampling distribution,iterative rendering,hierarchical feature fusion,parameter learning,visual quality"
"From Talent Shortage to Workforce Excellence in the CHIPS Act Era:
  Harnessing Industry 4.0 Paradigms for a Sustainable Future in Domestic Chip
  Production","The CHIPS Act is driving the U.S. towards a self-sustainable future in
domestic chip production. Decades of outsourced manufacturing, assembly,
testing, and packaging has diminished the workforce ecosystem, imposing major
limitations on semiconductor companies racing to build new fabrication sites as
part of the CHIPS Act. In response, a systemic alliance between academic
institutions, the industry, government, various consortiums, and organizations
has emerged to establish a pipeline to educate and onboard the next generation
of talent. Establishing a stable and continuous flow of talent requires
significant time investments and comes with no guarantees, particularly
factoring in the low workplace desirability in current fabrication houses for
U.S workforce. This paper will explore the feasibility of two paradigms of
Industry 4.0, automation and Augmented Reality(AR)/Virtual Reality(VR), to
complement ongoing workforce development efforts and optimize workplace
desirability by catalyzing core manufacturing processes and effectively
enhancing the education, onboarding, and professional realms-all with promising
capabilities amid the ongoing talent shortage and trajectory towards advanced
packaging.","['Aida Damanpak Rizi', 'Antika Roy', 'Rouhan Noor', 'Hyo Kang', 'Nitin Varshney', 'Katja Jacob', 'Sindia Rivera-Jimenez', 'Nathan Edwards', 'Volker J. Sorger', 'Hamed Dalir', 'Navid Asadizanjani']",2023-08-01T01:15:51Z,http://arxiv.org/abs/2308.00215v1,['cs.CY'],"CHIPS Act,Industry 4.0,semiconductor companies,fabrication sites,workforce ecosystem,workforce development,automation,Augmented Reality,Virtual Reality,talent shortage"
"Hand tracking for clinical applications: validation of the Google
  MediaPipe Hand (GMH) and the depth-enhanced GMH-D frameworks","Accurate 3D tracking of hand and fingers movements poses significant
challenges in computer vision. The potential applications span across multiple
domains, including human-computer interaction, virtual reality, industry, and
medicine. While gesture recognition has achieved remarkable accuracy,
quantifying fine movements remains a hurdle, particularly in clinical
applications where the assessment of hand dysfunctions and rehabilitation
training outcomes necessitate precise measurements. Several novel and
lightweight frameworks based on Deep Learning have emerged to address this
issue; however, their performance in accurately and reliably measuring fingers
movements requires validation against well-established gold standard systems.
In this paper, the aim is to validate the handtracking framework implemented by
Google MediaPipe Hand (GMH) and an innovative enhanced version, GMH-D, that
exploits the depth estimation of an RGB-Depth camera to achieve more accurate
tracking of 3D movements. Three dynamic exercises commonly administered by
clinicians to assess hand dysfunctions, namely Hand Opening-Closing, Single
Finger Tapping and Multiple Finger Tapping are considered. Results demonstrate
high temporal and spectral consistency of both frameworks with the gold
standard. However, the enhanced GMH-D framework exhibits superior accuracy in
spatial measurements compared to the baseline GMH, for both slow and fast
movements. Overall, our study contributes to the advancement of hand tracking
technology, the establishment of a validation procedure as a good-practice to
prove efficacy of deep-learning-based hand-tracking, and proves the
effectiveness of GMH-D as a reliable framework for assessing 3D hand movements
in clinical applications.","['Gianluca Amprimo', 'Giulia Masi', 'Giuseppe Pettiti', 'Gabriella Olmo', 'Lorenzo Priano', 'Claudia Ferraris']",2023-08-02T11:44:49Z,http://arxiv.org/abs/2308.01088v1,"['cs.CV', 'cs.AI']","hand tracking,validation,Google MediaPipe Hand,GMH,depth-enhanced GMH-D,computer vision,fine movements,clinical applications,rehabilitation training,Deep Learning"
Source-free Domain Adaptive Human Pose Estimation,"Human Pose Estimation (HPE) is widely used in various fields, including
motion analysis, healthcare, and virtual reality. However, the great expenses
of labeled real-world datasets present a significant challenge for HPE. To
overcome this, one approach is to train HPE models on synthetic datasets and
then perform domain adaptation (DA) on real-world data. Unfortunately, existing
DA methods for HPE neglect data privacy and security by using both source and
target data in the adaptation process. To this end, we propose a new task,
named source-free domain adaptive HPE, which aims to address the challenges of
cross-domain learning of HPE without access to source data during the
adaptation process. We further propose a novel framework that consists of three
models: source model, intermediate model, and target model, which explores the
task from both source-protect and target-relevant perspectives. The
source-protect module preserves source information more effectively while
resisting noise, and the target-relevant module reduces the sparsity of spatial
representations by building a novel spatial probability space, and
pose-specific contrastive learning and information maximization are proposed on
the basis of this space. Comprehensive experiments on several domain adaptive
HPE benchmarks show that the proposed method outperforms existing approaches by
a considerable margin. The codes are available at
https://github.com/davidpengucf/SFDAHPE.","['Qucheng Peng', 'Ce Zheng', 'Chen Chen']",2023-08-06T20:19:06Z,http://arxiv.org/abs/2308.03202v4,"['cs.CV', 'cs.AI', 'cs.LG']","Human Pose Estimation,Domain Adaptation,Synthetic Datasets,Source-free,Adaptation Process,Privacy,Security,Framework,Spatial Representations,Benchmarks."
VR-based body tracking to stimulate musculoskeletal training,"Training helps to maintain and improve sufficient muscle function, body
control, and body coordination. These are important to reduce the risk of
fracture incidents caused by falls, especially for the elderly or people
recovering from injury. Virtual reality training can offer a cost-effective and
individualized training experience. We present an application for the HoloLens
2 to enable musculoskeletal training for elderly and impaired persons to allow
for autonomous training and automatic progress evaluation. We designed a
virtual downhill skiing scenario that is controlled by body movement to
stimulate balance and body control. By adapting the parameters of the ski
slope, we can tailor the intensity of the training to individual users. In this
work, we evaluate whether the movement data of the HoloLens 2 alone is
sufficient to control and predict body movement and joint angles during
musculoskeletal training. We record the movements of 10 healthy volunteers with
external tracking cameras and track a set of body and joint angles of the
participant during training. We estimate correlation coefficients and
systematically analyze whether whole body movement can be derived from the
movement data of the HoloLens 2. No participant reports movement sickness
effects and all were able to quickly interact and control their movement during
skiing. Our results show a high correlation between HoloLens 2 movement data
and the external tracking of the upper body movement and joint angles of the
lower limbs.","['M. Neidhardt', 'S. Gerlach F. N. Schmidt', 'I. A. K. Fiedler', 'S. Grube', 'B. Busse', 'A. Schlaefer']",2023-08-07T07:54:32Z,http://arxiv.org/abs/2308.03375v1,['cs.CV'],"body tracking,musculoskeletal training,virtual reality,HoloLens 2,body movement,joint angles,training intensity,movement data,balance,musculoskeletal training"
"A data-driven approach to predict decision point choice during normal
  and evacuation wayfinding in multi-story buildings","Understanding pedestrian route choice behavior in complex buildings is
important to ensure pedestrian safety. Previous studies have mostly used
traditional data collection methods and discrete choice modeling to understand
the influence of different factors on pedestrian route and exit choice,
particularly in simple indoor environments. However, research on pedestrian
route choice in complex buildings is still limited. This paper presents a
data-driven approach for understanding and predicting the pedestrian decision
point choice during normal and emergency wayfinding in a multi-story building.
For this, we first built an indoor network representation and proposed a data
mapping technique to map VR coordinates to the indoor representation. We then
used a well-established machine learning algorithm, namely the random forest
(RF) model to predict pedestrian decision point choice along a route during
four wayfinding tasks in a multi-story building. Pedestrian behavioral data in
a multi-story building was collected by a Virtual Reality experiment. The
results show a much higher prediction accuracy of decision points using the RF
model (i.e., 93% on average) compared to the logistic regression model. The
highest prediction accuracy was 96% for task 3. Additionally, we tested the
model performance combining personal characteristics and we found that personal
characteristics did not affect decision point choice. This paper demonstrates
the potential of applying a machine learning algorithm to study pedestrian
route choice behavior in complex indoor buildings.","['Yan Feng', 'Panchamy Krishnakumari']",2023-08-07T12:05:55Z,http://arxiv.org/abs/2308.03511v1,['cs.LG'],"data-driven approach,decision point choice,wayfinding,multi-story buildings,indoor network representation,machine learning algorithm,random forest model,pedestrian behavioral data,Virtual Reality experiment,prediction accuracy"
"Energy-Efficient Deadline-Aware Edge Computing: Bandit Learning with
  Partial Observations in Multi-Channel Systems","In this paper, we consider a task offloading problem in a multi-access edge
computing (MEC) network, in which edge users can either use their local
processing unit to compute their tasks or offload their tasks to a nearby edge
server through multiple communication channels each with different
characteristics. The main objective is to maximize the energy efficiency of the
edge users while meeting computing tasks deadlines. In the multi-user
multi-channel offloading scenario, users are distributed with partial
observations of the system states. We formulate this problem as a stochastic
optimization problem and leverage \emph{contextual neural multi-armed bandit}
models to develop an energy-efficient deadline-aware solution, dubbed E2DA. The
proposed E2DA framework only relies on partial state information (i.e.,
computation task features) to make offloading decisions. Through extensive
numerical analysis, we demonstrate that the E2DA algorithm can efficiently
learn an offloading policy and achieve close-to-optimal performance in
comparison with several baseline policies that optimize energy consumption
and/or response time. Furthermore, we provide a comprehensive set of results on
the MEC system performance for various applications such as augmented reality
(AR) and virtual reality (VR).","['Babak Badnava', 'Keenan Roach', 'Kenny Cheung', 'Morteza Hashemi', 'Ness B Shroff']",2023-08-12T21:48:04Z,http://arxiv.org/abs/2308.06647v1,"['cs.NI', 'cs.DC', 'cs.IT', 'math.IT']","Edge Computing,Energy Efficiency,Deadline-Aware,Bandit Learning,Multi-Channel Systems,Stochastic Optimization,Contextual Neural Multi-Armed Bandit,Offloading Policy,Energy Consumption,Response Time"
"Neural Super-Resolution for Real-time Rendering with Radiance
  Demodulation","It is time-consuming to render high-resolution images in applications such as
video games and virtual reality, and thus super-resolution technologies become
increasingly popular for real-time rendering. However, it is challenging to
preserve sharp texture details, keep the temporal stability and avoid the
ghosting artifacts in real-time super-resolution rendering. To address this
issue, we introduce radiance demodulation to separate the rendered image or
radiance into a lighting component and a material component, considering the
fact that the light component is smoother than the rendered image so that the
high-resolution material component with detailed textures can be easily
obtained. We perform the super-resolution on the lighting component only and
re-modulate it with the high-resolution material component to obtain the final
super-resolution image with more texture details. A reliable warping module is
proposed by explicitly marking the occluded regions to avoid the ghosting
artifacts. To further enhance the temporal stability, we design a
frame-recurrent neural network and a temporal loss to aggregate the previous
and current frames, which can better capture the spatial-temporal consistency
among reconstructed frames. As a result, our method is able to produce
temporally stable results in real-time rendering with high-quality details,
even in the challenging 4 $\times$ 4 super-resolution scenarios.","['Jia Li', 'Ziling Chen', 'Xiaolong Wu', 'Lu Wang', 'Beibei Wang', 'Lei Zhang']",2023-08-13T06:40:41Z,http://arxiv.org/abs/2308.06699v2,['cs.GR'],"Neural Super-Resolution,Real-time Rendering,Radiance Demodulation,Textures,Ghosting Artifacts,Warping Module,Frame-Recurrent Neural Network,Temporal Stability,Spatial-temporal Consistency"
BehaVR: User Identification Based on VR Sensor Data,"Virtual reality (VR) platforms enable a wide range of applications, however
pose unique privacy risks. In particular, VR devices are equipped with a rich
set of sensors that collect personal and sensitive information (e.g., body
motion, eye gaze, hand joints, and facial expression), which can be used to
uniquely identify a user, even without explicit identifiers. In this paper, we
are interested in understanding the extent to which a user can be identified
based on data collected by different VR sensors. We consider adversaries with
capabilities that range from observing APIs available within a single VR app
(app adversary) to observing all, or selected, sensor measurements across all
apps on the VR device (device adversary). To that end, we introduce BEHAVR, a
framework for collecting and analyzing data from all sensor groups collected by
all apps running on a VR device. We use BEHAVR to perform a user study and
collect data from real users that interact with popular real-world apps. We use
that data to build machine learning models for user identification, with
features extracted from sensor data available within and across apps. We show
that these models can identify users with an accuracy of up to 100%, and we
reveal the most important features and sensor groups, depending on the
functionality of the app and the strength of the adversary, as well as the
minimum time needed for user identification. To the best of our knowledge,
BEHAVR is the first to analyze user identification in VR comprehensively, i.e.,
considering jointly all sensor measurements available on a VR device (whether
within an app or across multiple apps), collected by real-world, as opposed to
custom-made, apps.","['Ismat Jarin', 'Yu Duan', 'Rahmadi Trimananda', 'Hao Cui', 'Salma Elmalaki', 'Athina Markopoulou']",2023-08-14T17:43:42Z,http://arxiv.org/abs/2308.07304v1,"['cs.HC', 'cs.CR']","VR,User identification,Sensor data,Privacy risks,Machine learning,API,Adversary,User study,Sensor groups,VR device"
OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution,"Omnidirectional images (ODIs) have become increasingly popular, as their
large field-of-view (FoV) can offer viewers the chance to freely choose the
view directions in immersive environments such as virtual reality. The M\""obius
transformation is typically employed to further provide the opportunity for
movement and zoom on ODIs, but applying it to the image level often results in
blurry effect and aliasing problem. In this paper, we propose a novel deep
learning-based approach, called \textbf{OmniZoomer}, to incorporate the
M\""obius transformation into the network for movement and zoom on ODIs. By
learning various transformed feature maps under different conditions, the
network is enhanced to handle the increasing edge curvatures, which alleviates
the blurry effect. Moreover, to address the aliasing problem, we propose two
key components. Firstly, to compensate for the lack of pixels for describing
curves, we enhance the feature maps in the high-resolution (HR) space and
calculate the transformed index map with a spatial index generation module.
Secondly, considering that ODIs are inherently represented in the spherical
space, we propose a spherical resampling module that combines the index map and
HR feature maps to transform the feature maps for better spherical correlation.
The transformed feature maps are decoded to output a zoomed ODI. Experiments
show that our method can produce HR and high-quality ODIs with the flexibility
to move and zoom in to the object of interest. Project page is available at
http://vlislab22.github.io/OmniZoomer/.","['Zidong Cao', 'Hao Ai', 'Yan-Pei Cao', 'Ying Shan', 'Xiaohu Qie', 'Lin Wang']",2023-08-16T02:58:43Z,http://arxiv.org/abs/2308.08114v2,"['cs.CV', 'cs.AI']","OmniZoomer,Omnidirectional images,M\""obius transformation,deep learning,high-resolution,feature maps,aliasing,spherical space,spherical correlation,virtual reality."
"Coordinate Transformer: Achieving Single-stage Multi-person Mesh
  Recovery from Videos","Multi-person 3D mesh recovery from videos is a critical first step towards
automatic perception of group behavior in virtual reality, physical therapy and
beyond. However, existing approaches rely on multi-stage paradigms, where the
person detection and tracking stages are performed in a multi-person setting,
while temporal dynamics are only modeled for one person at a time.
Consequently, their performance is severely limited by the lack of inter-person
interactions in the spatial-temporal mesh recovery, as well as by detection and
tracking defects. To address these challenges, we propose the Coordinate
transFormer (CoordFormer) that directly models multi-person spatial-temporal
relations and simultaneously performs multi-mesh recovery in an end-to-end
manner. Instead of partitioning the feature map into coarse-scale patch-wise
tokens, CoordFormer leverages a novel Coordinate-Aware Attention to preserve
pixel-level spatial-temporal coordinate information. Additionally, we propose a
simple, yet effective Body Center Attention mechanism to fuse position
information. Extensive experiments on the 3DPW dataset demonstrate that
CoordFormer significantly improves the state-of-the-art, outperforming the
previously best results by 4.2%, 8.8% and 4.7% according to the MPJPE, PAMPJPE,
and PVE metrics, respectively, while being 40% faster than recent video-based
approaches. The released code can be found at
https://github.com/Li-Hao-yuan/CoordFormer.","['Haoyuan Li', 'Haoye Dong', 'Hanchao Jia', 'Dong Huang', 'Michael C. Kampffmeyer', 'Liang Lin', 'Xiaodan Liang']",2023-08-20T18:23:07Z,http://arxiv.org/abs/2308.10334v1,['cs.CV'],"Coordinate Transformer,Multi-person,Mesh Recovery,Spatial-temporal Relations,Coordinate-Aware Attention,Body Center Attention,3DPW Dataset,MPJPE,PAMPJPE"
"Metaverse: A Vision, Architectural Elements, and Future Directions for
  Scalable and Realtime Virtual Worlds","With the emergence of Cloud computing, Internet of Things-enabled
Human-Computer Interfaces, Generative Artificial Intelligence, and
high-accurate Machine and Deep-learning recognition and predictive models,
along with the Post Covid-19 proliferation of social networking, and remote
communications, the Metaverse gained a lot of popularity. Metaverse has the
prospective to extend the physical world using virtual and augmented reality so
the users can interact seamlessly with the real and virtual worlds using
avatars and holograms. It has the potential to impact people in the way they
interact on social media, collaborate in their work, perform marketing and
business, teach, learn, and even access personalized healthcare. Several works
in the literature examine Metaverse in terms of hardware wearable devices, and
virtual reality gaming applications. However, the requirements of realizing the
Metaverse in realtime and at a large-scale need yet to be examined for the
technology to be usable. To address this limitation, this paper presents the
temporal evolution of Metaverse definitions and captures its evolving
requirements. Consequently, we provide insights into Metaverse requirements. In
addition to enabling technologies, we lay out architectural elements for
scalable, reliable, and efficient Metaverse systems, and a classification of
existing Metaverse applications along with proposing required future research
directions.","['Leila Ismail', 'Rajkumar Buyya']",2023-08-21T08:23:10Z,http://arxiv.org/abs/2308.10559v2,"['cs.HC', 'cs.AI', 'cs.HC, cs.DC']","Metaverse,Architectural Elements,Scalable,Realtime,Virtual Reality,Cloud Computing,Internet of Things,Artificial Intelligence,Machine Learning,Deep Learning"
"Enhancing Perception and Immersion in Pre-Captured Environments through
  Learning-Based Eye Height Adaptation","Pre-captured immersive environments using omnidirectional cameras provide a
wide range of virtual reality applications. Previous research has shown that
manipulating the eye height in egocentric virtual environments can
significantly affect distance perception and immersion. However, the influence
of eye height in pre-captured real environments has received less attention due
to the difficulty of altering the perspective after finishing the capture
process. To explore this influence, we first propose a pilot study that
captures real environments with multiple eye heights and asks participants to
judge the egocentric distances and immersion. If a significant influence is
confirmed, an effective image-based approach to adapt pre-captured real-world
environments to the user's eye height would be desirable. Motivated by the
study, we propose a learning-based approach for synthesizing novel views for
omnidirectional images with altered eye heights. This approach employs a
multitask architecture that learns depth and semantic segmentation in two
formats, and generates high-quality depth and semantic segmentation to
facilitate the inpainting stage. With the improved omnidirectional-aware
layered depth image, our approach synthesizes natural and realistic visuals for
eye height adaptation. Quantitative and qualitative evaluation shows favorable
results against state-of-the-art methods, and an extensive user study verifies
improved perception and immersion for pre-captured real-world environments.","['Qi Feng', 'Hubert P. H. Shum', 'Shigeo Morishima']",2023-08-24T19:14:28Z,http://arxiv.org/abs/2308.13042v1,"['cs.CV', 'cs.HC']","omnidirectional cameras,virtual reality,distance perception,immersion,eye height,real environments,image-based approach,learning-based approach,omnidirectional images,depth and semantic segmentation"
"Exploring the Effects of VR Activities on Stress Relief: A Comparison of
  Sitting-in-Silence, VR Meditation, and VR Smash Room","In our lives, we encounter various stressors that may cause negative mental
and bodily reactions to make us feel frustrated, angry, or irritated. Effective
methods to manage or reduce stress and anxiety are essential for a healthy
life, and several stress-management approaches are found to be useful for
stress relief, such as meditation, taking a rest, walking around nature, or
even breaking things in a smash room. Previous research has revealed that
certain experiences in virtual reality (VR) are effective for reducing stress
as traditional real-world methods. However, it is still unclear how the stress
relief effects are associated with other factors like individual user profile
in terms of different treatment activities. In this paper, we report our
findings from a formal user study that investigates the effects of two virtual
activities: (1) VR Meditation and (2) VR Smash Room experience, compared with a
traditional Sitting-in-Silence method. Our results show that VR Meditation has
a better stress relief effect compared to VR Smash Room and Sitting-in-Silence,
and the effects of the treatments are correlated with the participants'
personalities. We discuss the findings and implications addressing potential
benefits/impacts of different stress-relief activities in VR.","['Dongyun Han', 'Donghoon Kim', 'Kangsoo Kim', 'Isaac Cho']",2023-08-26T20:24:03Z,http://arxiv.org/abs/2308.13952v1,['cs.HC'],"VR activities,Stress relief,VR Meditation,VR Smash Room,Sitting-in-Silence,Stress management,Virtual reality,User study,User profile,Stressors"
"Synergizing Contrastive Learning and Optimal Transport for 3D Point
  Cloud Domain Adaptation","Recently, the fundamental problem of unsupervised domain adaptation (UDA) on
3D point clouds has been motivated by a wide variety of applications in
robotics, virtual reality, and scene understanding, to name a few. The point
cloud data acquisition procedures manifest themselves as significant domain
discrepancies and geometric variations among both similar and dissimilar
classes. The standard domain adaptation methods developed for images do not
directly translate to point cloud data because of their complex geometric
nature. To address this challenge, we leverage the idea of multimodality and
alignment between distributions. We propose a new UDA architecture for point
cloud classification that benefits from multimodal contrastive learning to get
better class separation in both domains individually. Further, the use of
optimal transport (OT) aims at learning source and target data distributions
jointly to reduce the cross-domain shift and provide a better alignment. We
conduct a comprehensive empirical study on PointDA-10 and GraspNetPC-10 and
show that our method achieves state-of-the-art performance on GraspNetPC-10
(with approx 4-12% margin) and best average performance on PointDA-10. Our
ablation studies and decision boundary analysis also validate the significance
of our contrastive learning module and OT alignment.","['Siddharth Katageri', 'Arkadipta De', 'Chaitanya Devaguptapu', 'VSSV Prasad', 'Charu Sharma', 'Manohar Kaul']",2023-08-27T15:03:10Z,http://arxiv.org/abs/2308.14126v1,['cs.CV'],"unsupervised domain adaptation,3D point clouds,domain discrepancies,geometric variations,multimodality,contrastive learning,optimal transport,UDA architecture,point cloud classification,cross-domain shift"
"Utilizing Task-Generic Motion Prior to Recover Full-Body Motion from
  Very Sparse Signals","The most popular type of devices used to track a user's posture in a virtual
reality experience consists of a head-mounted display and two controllers held
in both hands. However, due to the limited number of tracking sensors (three in
total), faithfully recovering the user in full-body is challenging, limiting
the potential for interactions among simulated user avatars within the virtual
world. Therefore, recent studies have attempted to reconstruct full-body poses
using neural networks that utilize previously learned human poses or accept a
series of past poses over a short period. In this paper, we propose a method
that utilizes information from a neural motion prior to improve the accuracy of
reconstructed user's motions. Our approach aims to reconstruct user's full-body
poses by predicting the latent representation of the user's overall motion from
limited input signals and integrating this information with tracking sensor
inputs. This is based on the premise that the ultimate goal of pose
reconstruction is to reconstruct the motion, which is a series of poses. Our
results show that this integration enables more accurate reconstruction of the
user's full-body motion, particularly enhancing the robustness of lower body
motion reconstruction from impoverished signals. Web:
https://https://mjsh34.github.io/mp-sspe/","['Myungjin Shin', 'Dohae Lee', 'In-Kwon Lee']",2023-08-30T08:21:52Z,http://arxiv.org/abs/2308.15839v1,['cs.CV'],"motion prior,full-body motion,sparse signals,tracking sensors,neural networks,human poses,pose reconstruction,latent representation,user's motions,lower body motion"
Deadline Aware Two-Timescale Resource Allocation for VR Video Streaming,"In this paper, we investigate resource allocation problem in the context of
multiple virtual reality (VR) video flows sharing a certain link, considering
specific deadline of each video frame and the impact of different frames on
video quality. Firstly, we establish a queuing delay bound estimation model,
enabling link node to proactively discard frames that will exceed the deadline.
Secondly, we model the importance of different frames based on viewport feature
of VR video and encoding method. Accordingly, the frames of each flow are
sorted. Then we formulate a problem of minimizing long-term quality loss caused
by frame dropping subject to per-flow quality guarantee and bandwidth
constraints. Since the frequency of frame dropping and network fluctuation are
not on the same time scale, we propose a two-timescale resource allocation
scheme. On the long timescale, a queuing theory based resource allocation
method is proposed to satisfy quality requirement, utilizing frame queuing
delay bound to obtain minimum resource demand for each flow. On the short
timescale, in order to quickly fine-tune allocation results to cope with the
unstable network state, we propose a low-complexity heuristic algorithm,
scheduling available resources based on the importance of frames in each flow.
Extensive experimental results demonstrate that the proposed scheme can
efficiently improve quality and fairness of VR video flows under various
network conditions.","['Qingxuan Feng', 'Peng Yang', 'Zhixuan Huang', 'Jiayin Chen', 'Ning Zhang']",2023-08-31T03:12:27Z,http://arxiv.org/abs/2308.16419v1,['cs.NI'],"resource allocation,VR video streaming,deadline,frame dropping,queuing delay bound,VR video,encoding method,viewport feature,bandwidth constraints,network fluctuation"
"Terrain Diffusion Network: Climatic-Aware Terrain Generation with
  Geological Sketch Guidance","Sketch-based terrain generation seeks to create realistic landscapes for
virtual environments in various applications such as computer games, animation
and virtual reality. Recently, deep learning based terrain generation has
emerged, notably the ones based on generative adversarial networks (GAN).
However, these methods often struggle to fulfill the requirements of flexible
user control and maintain generative diversity for realistic terrain.
Therefore, we propose a novel diffusion-based method, namely terrain diffusion
network (TDN), which actively incorporates user guidance for enhanced
controllability, taking into account terrain features like rivers, ridges,
basins, and peaks. Instead of adhering to a conventional monolithic denoising
process, which often compromises the fidelity of terrain details or the
alignment with user control, a multi-level denoising scheme is proposed to
generate more realistic terrains by taking into account fine-grained details,
particularly those related to climatic patterns influenced by erosion and
tectonic activities. Specifically, three terrain synthesisers are designed for
structural, intermediate, and fine-grained level denoising purposes, which
allow each synthesiser concentrate on a distinct terrain aspect. Moreover, to
maximise the efficiency of our TDN, we further introduce terrain and sketch
latent spaces for the synthesizers with pre-trained terrain autoencoders.
Comprehensive experiments on a new dataset constructed from NASA Topology
Images clearly demonstrate the effectiveness of our proposed method, achieving
the state-of-the-art performance. Our code and dataset will be publicly
available.","['Zexin Hu', 'Kun Hu', 'Clinton Mo', 'Lei Pan', 'Zhiyong Wang']",2023-08-31T13:41:34Z,http://arxiv.org/abs/2308.16725v1,"['cs.CV', 'cs.AI', 'cs.MM']","Terrain Diffusion Network,Terrain Generation,Geological Sketch,Deep Learning,Generative Adversarial Networks,User Guidance,Climatic Patterns,Terrain Synthesisers,Terrain Autoencoders,NASA Topology Images"
"How Does Forecasting Affect the Convergence of DRL Techniques in O-RAN
  Slicing?","The success of immersive applications such as virtual reality (VR) gaming and
metaverse services depends on low latency and reliable connectivity. To provide
seamless user experiences, the open radio access network (O-RAN) architecture
and 6G networks are expected to play a crucial role. RAN slicing, a critical
component of the O-RAN paradigm, enables network resources to be allocated
based on the needs of immersive services, creating multiple virtual networks on
a single physical infrastructure. In the O-RAN literature, deep reinforcement
learning (DRL) algorithms are commonly used to optimize resource allocation.
However, the practical adoption of DRL in live deployments has been sluggish.
This is primarily due to the slow convergence and performance instabilities
suffered by the DRL agents both upon initial deployment and when there are
significant changes in network conditions. In this paper, we investigate the
impact of time series forecasting of traffic demands on the convergence of the
DRL-based slicing agents. For that, we conduct an exhaustive experiment that
supports multiple services including real VR gaming traffic. We then propose a
novel forecasting-aided DRL approach and its respective O-RAN practical
deployment workflow to enhance DRL convergence. Our approach shows up to 22.8%,
86.3%, and 300% improvements in the average initial reward value, convergence
rate, and number of converged scenarios respectively, enhancing the
generalizability of the DRL agents compared with the implemented baselines. The
results also indicate that our approach is robust against forecasting errors
and that forecasting models do not have to be ideal.","['Ahmad M. Nagib', 'Hatem Abou-Zeid', 'Hossam S. Hassanein']",2023-09-01T14:30:04Z,http://arxiv.org/abs/2309.00489v1,"['cs.NI', 'cs.LG']","forecasting,convergence,DRL techniques,O-RAN slicing,6G networks,RAN slicing,immersive applications,virtual reality gaming,metaverse services,deep reinforcement learning,resource allocation"
"DualStream: Spatially Sharing Selves and Surroundings using Mobile
  Devices and Augmented Reality","In-person human interaction relies on our spatial perception of each other
and our surroundings. Current remote communication tools partially address each
of these aspects. Video calls convey real user representations but without
spatial interactions. Augmented and Virtual Reality (AR/VR) experiences are
immersive and spatial but often use virtual environments and characters instead
of real-life representations. Bridging these gaps, we introduce DualStream, a
system for synchronous mobile AR remote communication that captures, streams,
and displays spatial representations of users and their surroundings.
DualStream supports transitions between user and environment representations
with different levels of visuospatial fidelity, as well as the creation of
persistent shared spaces using environment snapshots. We demonstrate how
DualStream can enable spatial communication in real-world contexts, and support
the creation of blended spaces for collaboration. A formative evaluation of
DualStream revealed that users valued the ability to interact spatially and
move between representations, and could see DualStream fitting into their own
remote communication practices in the near future. Drawing from these findings,
we discuss new opportunities for designing more widely accessible spatial
communication tools, centered around the mobile phone.","['Rishi Vanukuru', 'Suibi Che-Chuan Weng', 'Krithik Ranjan', 'Torin Hopkins', 'Amy Banic', 'Mark D. Gross', 'Ellen Yi-Luen Do']",2023-09-02T06:38:33Z,http://arxiv.org/abs/2309.00842v1,['cs.HC'],"Spatial perception,Remote communication,Augmented Reality,Virtual Reality,Synchronous communication,Visuospatial fidelity,Shared spaces,Collaboration,Spatial communication,Mobile phone"
"Immersive Technologies in Virtual Companions: A Systematic Literature
  Review","The emergence of virtual companions is transforming the evolution of
intelligent systems that effortlessly cater to the unique requirements of
users. These advanced systems not only take into account the user present
capabilities, preferences, and needs but also possess the capability to adapt
dynamically to changes in the environment, as well as fluctuations in the users
emotional state or behavior. A virtual companion is an intelligent software or
application that offers support, assistance, and companionship across various
aspects of users lives. Various enabling technologies are involved in building
virtual companion, among these, Augmented Reality (AR), and Virtual Reality
(VR) are emerging as transformative tools. While their potential for use in
virtual companions or digital assistants is promising, their applications in
these domains remain relatively unexplored. To address this gap, a systematic
review was conducted to investigate the applications of VR, AR, and MR
immersive technologies in the development of virtual companions. A
comprehensive search across PubMed, Scopus, and Google Scholar yielded 28
relevant articles out of a pool of 644. The review revealed that immersive
technologies, particularly VR and AR, play a significant role in creating
digital assistants, offering a wide range of applications that brings various
facilities in the individuals life in areas such as addressing social
isolation, enhancing cognitive abilities and dementia care, facilitating
education, and more. Additionally, AR and MR hold potential for enhancing
Quality of life (QoL) within the context of virtual companion technology. The
findings of this review provide a valuable foundation for further research in
this evolving field.","['Ziaullah Momand', 'Jonathan H. Chan', 'Pornchai Mongkolnam']",2023-09-03T16:39:22Z,http://arxiv.org/abs/2309.01214v1,['cs.HC'],"Immersive Technologies,Virtual Companions,Systematic Literature Review,Augmented Reality (AR),Virtual Reality (VR),Mixed Reality (MR),Intelligent Systems,Digital Assistants,Quality of Life (QoL)"
"Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree
  Image Generation","A 360-degree (omni-directional) image provides an all-encompassing spherical
view of a scene. Recently, there has been an increasing interest in
synthesising 360-degree images from conventional narrow field of view (NFoV)
images captured by digital cameras and smartphones, for providing immersive
experiences in various scenarios such as virtual reality. Yet, existing methods
typically fall short in synthesizing intricate visual details or ensure the
generated images align consistently with user-provided prompts. In this study,
autoregressive omni-aware generative network (AOG-Net) is proposed for
360-degree image generation by out-painting an incomplete 360-degree image
progressively with NFoV and text guidances joinly or individually. This
autoregressive scheme not only allows for deriving finer-grained and
text-consistent patterns by dynamically generating and adjusting the process
but also offers users greater flexibility to edit their conditions throughout
the generation process. A global-local conditioning mechanism is devised to
comprehensively formulate the outpainting guidance in each autoregressive step.
Text guidances, omni-visual cues, NFoV inputs and omni-geometry are encoded and
further formulated with cross-attention based transformers into a global stream
and a local stream into a conditioned generative backbone model. As AOG-Net is
compatible to leverage large-scale models for the conditional encoder and the
generative prior, it enables the generation to use extensive open-vocabulary
text guidances. Comprehensive experiments on two commonly used 360-degree image
datasets for both indoor and outdoor settings demonstrate the state-of-the-art
performance of our proposed method. Our code will be made publicly available.","['Zhuqiang Lu', 'Kun Hu', 'Chaoyue Wang', 'Lei Bai', 'Zhiyong Wang']",2023-09-07T03:22:59Z,http://arxiv.org/abs/2309.03467v2,"['cs.CV', 'cs.AI', 'I.4.0']","Autoregressive,Omni-directional,360-degree,Image Generation,NFoV,Generative Network,Outpainting,Text Guidance,Cross-attention,Transformers"
Phase-change nonlocal metasurfaces for dynamic wavefront manipulation,"Recent advances in nonlocal metasurfaces have enabled unprecedented success
in shaping the wavefront of light with spectral selectivity, offering new
solutions for many emerging nanophotonics applications. The ability to tune
both the spectral and spatial properties of such a novel class of metasurfaces
is highly desirable, but the dynamic nonvolatile control remains elusive. Here,
we demonstrate active narrowband wavefront manipulation by harnessing
quasi-bound states in the continuum (quasi-BICs) in phase-change nonlocal
metasurfaces. The proof-of-principle metasurfaces made of Sb$_2$S$_3$ allow for
nonvolatile, reversible, and tunable spectral control over wavefront and
switchable spatial response at a given wavelength. The design principle mainly
builds upon the combination of the geometry phase of quasi-BICs and the dynamic
tunability of phase-change meta-atoms to tailor the spatial response of light
at distinct resonant wavelengths. By tuning the crystallization level of
Sb$_2$S$_3$ meta-atoms, the dynamic nonlocal wavefront-shaping functionalities
of beam steering, 1D, and 2D focusing are achieved. Furthermore, we demonstrate
tunable holographic imaging with active spectral selectivity using our
phase-change nonlocal metasurface. This work represents a critical advance
towards developing integrated dynamic nonlocal metasurface for future augmented
and virtual reality wearables.","['Tingting Liu', 'Dandan Zhang', 'Wenxing Liu', 'Tianbao Yu', 'Feng Wu', 'Shuyuan Xiao', 'Lujun Huang', 'Andrey E. Miroshnichenko']",2023-09-07T10:46:14Z,http://arxiv.org/abs/2309.03626v2,"['physics.optics', 'physics.app-ph']","nonlocal metasurfaces,wavefront manipulation,phase-change,quasi-bound states in the continuum,spectral control,spatial response,tunability,crystallization level,beam steering,focusing"
"Haptic Guidance and Haptic Error Amplification in a Virtual Surgical
  Robotic Training Environment","Teleoperated robotic systems have introduced more intuitive control for
minimally invasive surgery, but the optimal method for training remains
unknown. Recent motor learning studies have demonstrated that exaggeration of
errors helps trainees learn to perform tasks with greater speed and accuracy.
We hypothesized that training in a force field that pushes the operator away
from a desired path would improve their performance on a virtual reality
ring-on-wire task.
  Forty surgical novices trained under a no-force, guidance, or
error-amplifying force field over five days. Completion time, translational and
rotational path error, and combined error-time were evaluated under no force
field on the final day. The groups significantly differed in combined
error-time, with the guidance group performing the worst. Error-amplifying
field participants showed the most improvement and did not plateau in their
performance during training, suggesting that learning was still ongoing.
Guidance field participants had the worst performance on the final day,
confirming the guidance hypothesis. Participants with high initial path error
benefited more from guidance. Participants with high initial combined
error-time benefited more from guidance and error-amplifying force field
training. Our results suggest that error-amplifying and error-reducing haptic
training for robot-assisted telesurgery benefits trainees of different
abilities differently.","['Yousi A. Oquendo', 'Margaret M. Coad', 'Sherry M. Wren', 'Thomas S. Lendvay', 'Ilana Nisky', 'Anthony M. Jarc', 'Allison M. Okamura', 'Zonghe Chua']",2023-09-11T01:25:34Z,http://arxiv.org/abs/2309.05187v1,"['cs.RO', 'cs.HC']","Haptic guidance,Haptic error amplification,Virtual surgical training,Robotic systems,Minimally invasive surgery,Motor learning,Force field,Performance evaluation,Teleoperated systems,Robot-assisted surgery."
Digital Twin System for Home Service Robot Based on Motion Simulation,"In order to improve the task execution capability of home service robot, and
to cope with the problem that purely physical robot platforms cannot sense the
environment and make decisions online, a method for building digital twin
system for home service robot based on motion simulation is proposed. A
reliable mapping of the home service robot and its working environment from
physical space to digital space is achieved in three dimensions: geometric,
physical and functional. In this system, a digital space-oriented URDF file
parser is designed and implemented for the automatic construction of the robot
geometric model. Next, the physical model is constructed from the kinematic
equations of the robot and an improved particle swarm optimization algorithm is
proposed for the inverse kinematic solution. In addition, to adapt to the home
environment, functional attributes are used to describe household objects, thus
improving the semantic description of the digital space for the real home
environment. Finally, through geometric model consistency verification,
physical model validity verification and virtual-reality consistency
verification, it shows that the digital twin system designed in this paper can
construct the robot geometric model accurately and completely, complete the
operation of household objects successfully, and the digital twin system is
effective and practical.","['Zhengsong Jiang', 'Guohui Tian', 'Yongcheng Cui', 'Tiantian Liu', 'Yu Gu', 'Yifei Wang']",2023-09-12T06:48:30Z,http://arxiv.org/abs/2309.05993v1,['cs.RO'],"digital twin system,home service robot,motion simulation,URDF file parser,kinematic equations,particle swarm optimization algorithm,inverse kinematic solution,geometric model,physical model,virtual-reality consistency"
"A Health Monitoring System Based on Flexible Triboelectric Sensors for
  Intelligence Medical Internet of Things and its Applications in Virtual
  Reality","The Internet of Medical Things (IoMT) is a platform that combines Internet of
Things (IoT) technology with medical applications, enabling the realization of
precision medicine, intelligent healthcare, and telemedicine in the era of
digitalization and intelligence. However, the IoMT faces various challenges,
including sustainable power supply, human adaptability of sensors and the
intelligence of sensors. In this study, we designed a robust and intelligent
IoMT system through the synergistic integration of flexible wearable
triboelectric sensors and deep learning-assisted data analytics. We embedded
four triboelectric sensors into a wristband to detect and analyze limb
movements in patients suffering from Parkinson's Disease (PD). By further
integrating deep learning-assisted data analytics, we actualized an intelligent
healthcare monitoring system for the surveillance and interaction of PD
patients, which includes location/trajectory tracking, heart monitoring and
identity recognition. This innovative approach enabled us to accurately capture
and scrutinize the subtle movements and fine motor of PD patients, thus
providing insightful feedback and comprehensive assessment of the patients
conditions. This monitoring system is cost-effective, easily fabricated, highly
sensitive, and intelligent, consequently underscores the immense potential of
human body sensing technology in a Health 4.0 society.","['Junqi Mao', 'Puen Zhou', 'Xiaoyao Wang', 'Hongbo Yao', 'Liuyang Liang', 'Yiqiao Zhao', 'Jiawei Zhang', 'Dayan Ban', 'Haiwu Zheng']",2023-09-13T01:01:16Z,http://arxiv.org/abs/2309.07185v1,"['eess.SP', 'cs.AI', 'cs.HC']","Health Monitoring System,Triboelectric Sensors,Internet of Things,IoMT,Virtual Reality,Precision Medicine,Telemedicine,Deep Learning,Wearable Sensors,Parkinson's Disease"
Unified Human-Scene Interaction via Prompted Chain-of-Contacts,"Human-Scene Interaction (HSI) is a vital component of fields like embodied AI
and virtual reality. Despite advancements in motion quality and physical
plausibility, two pivotal factors, versatile interaction control and the
development of a user-friendly interface, require further exploration before
the practical application of HSI. This paper presents a unified HSI framework,
UniHSI, which supports unified control of diverse interactions through language
commands. This framework is built upon the definition of interaction as Chain
of Contacts (CoC): steps of human joint-object part pairs, which is inspired by
the strong correlation between interaction types and human-object contact
regions. Based on the definition, UniHSI constitutes a Large Language Model
(LLM) Planner to translate language prompts into task plans in the form of CoC,
and a Unified Controller that turns CoC into uniform task execution. To
facilitate training and evaluation, we collect a new dataset named ScenePlan
that encompasses thousands of task plans generated by LLMs based on diverse
scenarios. Comprehensive experiments demonstrate the effectiveness of our
framework in versatile task execution and generalizability to real scanned
scenes. The project page is at https://github.com/OpenRobotLab/UniHSI .","['Zeqi Xiao', 'Tai Wang', 'Jingbo Wang', 'Jinkun Cao', 'Wenwei Zhang', 'Bo Dai', 'Dahua Lin', 'Jiangmiao Pang']",2023-09-14T17:59:49Z,http://arxiv.org/abs/2309.07918v3,['cs.CV'],"Human-Scene Interaction,Embodied AI,Virtual reality,Interaction control,User-friendly interface,Framework,Chain of Contacts,Large Language Model,Unified Controller,Dataset"
"Hand Gesture Recognition with Two Stage Approach Using Transfer Learning
  and Deep Ensemble Learning","Human-Computer Interaction (HCI) has been the subject of research for many
years, and recent studies have focused on improving its performance through
various techniques. In the past decade, deep learning studies have shown high
performance in various research areas, leading researchers to explore their
application to HCI. Convolutional neural networks can be used to recognize hand
gestures from images using deep architectures. In this study, we evaluated
pre-trained high-performance deep architectures on the HG14 dataset, which
consists of 14 different hand gesture classes. Among 22 different models,
versions of the VGGNet and MobileNet models attained the highest accuracy
rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of
94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models
achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand
gesture recognition on the dataset using an ensemble learning technique, which
combined the four most successful models. By utilizing these models as base
learners and applying the Dirichlet ensemble technique, we achieved an accuracy
rate of 98.88%. These results demonstrate the effectiveness of the deep
ensemble learning technique for HCI and its potential applications in areas
such as augmented reality, virtual reality, and game technologies.","['Serkan SavaŁE, 'Atilla Ergüzen']",2023-09-20T19:53:05Z,http://arxiv.org/abs/2309.11610v1,"['cs.CV', 'cs.AI', 'cs.HC']","Hand Gesture Recognition,Two Stage Approach,Transfer Learning,Deep Ensemble Learning,Human-Computer Interaction,Convolutional Neural Networks,VGGNet,MobileNet,Ensemble Learning,Dirichlet ensemble technique"
"A Multi-label Classification Approach to Increase Expressivity of
  EMG-based Gesture Recognition","Objective: The objective of the study is to efficiently increase the
expressivity of surface electromyography-based (sEMG) gesture recognition
systems. Approach: We use a problem transformation approach, in which actions
were subset into two biomechanically independent components - a set of wrist
directions and a set of finger modifiers. To maintain fast calibration time, we
train models for each component using only individual gestures, and extrapolate
to the full product space of combination gestures by generating synthetic data.
We collected a supervised dataset with high-confidence ground truth labels in
which subjects performed combination gestures while holding a joystick, and
conducted experiments to analyze the impact of model architectures, classifier
algorithms, and synthetic data generation strategies on the performance of the
proposed approach. Main Results: We found that a problem transformation
approach using a parallel model architecture in combination with a non-linear
classifier, along with restricted synthetic data generation, shows promise in
increasing the expressivity of sEMG-based gestures with a short calibration
time. Significance: sEMG-based gesture recognition has applications in
human-computer interaction, virtual reality, and the control of robotic and
prosthetic devices. Existing approaches require exhaustive model calibration.
The proposed approach increases expressivity without requiring users to
demonstrate all combination gesture classes. Our results may be extended to
larger gesture vocabularies and more complicated model architectures.","['Niklas Smedemark-Margulies', 'Yunus Bicer', 'Elifnur Sunger', 'Stephanie Naufel', 'Tales Imbiriba', 'Eugene Tunik', 'Deniz ErdoğmuŁE, 'Mathew Yarossi']",2023-09-13T20:21:41Z,http://arxiv.org/abs/2309.12217v1,"['eess.SP', 'cs.HC', 'cs.LG']","Multi-label classification,EMG,Gesture recognition,sEMG,Biomechanically independent components,Synthetic data generation,Model architectures,Classifier algorithms,Calibration time,Human-computer interaction"
"PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for
  Video Segmentation","Panoramic videos contain richer spatial information and have attracted
tremendous amounts of attention due to their exceptional experience in some
fields such as autonomous driving and virtual reality. However, existing
datasets for video segmentation only focus on conventional planar images. To
address the challenge, in this paper, we present a panoramic video dataset,
PanoVOS. The dataset provides 150 videos with high video resolutions and
diverse motions. To quantify the domain gap between 2D planar videos and
panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)
models on PanoVOS. Through error analysis, we found that all of them fail to
tackle pixel-level content discontinues of panoramic videos. Thus, we present a
Panoramic Space Consistency Transformer (PSCFormer), which can effectively
utilize the semantic boundary information of the previous frame for pixel-level
matching with the current frame. Extensive experiments demonstrate that
compared with the previous SOTA models, our PSCFormer network exhibits a great
advantage in terms of segmentation results under the panoramic setting. Our
dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can
advance the development of panoramic segmentation/tracking.","['Shilin Yan', 'Xiaohao Xu', 'Renrui Zhang', 'Lingyi Hong', 'Wenchao Chen', 'Wenqiang Zhang', 'Wei Zhang']",2023-09-21T17:59:02Z,http://arxiv.org/abs/2309.12303v3,['cs.CV'],"Transformer,Video Segmentation,Panoramic Views,Panoramic Videos,Dataset,Pixel-level,Semantic Boundary,Segmentation Results,Panoramic Segmentation,Tracking"
"Embodied Cognition Guides Virtual-Real Interaction Design to Help
  Yicheng Flower Drum Intangible Cultural Heritage Dissemination","In order to make the non-heritage culture of Yicheng Flower Drum more
relevant to the trend of the digital era and promote its dissemination and
inheritance, the design and application of gesture recognition and virtual
reality technologies guided by embodied cognition theory in the process of
non-heritage culture dissemination is studied. At the same time, it will
enhance the interaction between people and NRM culture, stimulate the
audience's interest in understanding NRM and spreading NRM, and create
awareness of preserving NRM culture. Using embodied cognition as a theoretical
guide, expanding the unidirectional communication mode through human-computer
interaction close to natural behavior and cooperating with multisensory
information reception channels, so as to construct an embodied and immersive
interactive atmosphere for the participants and enable them to naturally form
the cognition and understanding of the traditional culture in the process of
interaction. The dissemination of the non-heritage culture Yicheng Flower Drum
can take the theory of embodied cognition as an entry point, and through the
virtual and real scenes of Yicheng Flower Drum and the immersive experience, we
can empower the interaction design of non-heritage culture dissemination of the
virtual and real, and provide a new method for the research of digital design
of non-heritage culture.","['Yuhan Ma', 'Weiran Zhao', 'Xiaolin Zhang', 'Ze Gao']",2023-10-07T10:36:41Z,http://arxiv.org/abs/2310.04771v1,"['cs.HC', 'cs.MM', '14J60 (Primary) 14F05, 14J26 (Secondary)', 'F.2.2; I.2.7']","Embodied Cognition,Virtual Reality,Gesture Recognition,Yicheng Flower Drum,Intangible Cultural Heritage,Dissemination,Interaction Design,Human-Computer Interaction,Multisensory Information,Digital Design"
"Privacy Preservation in Artificial Intelligence and Extended Reality
  (AI-XR) Metaverses: A Survey","The metaverse is a nascent concept that envisions a virtual universe, a
collaborative space where individuals can interact, create, and participate in
a wide range of activities. Privacy in the metaverse is a critical concern as
the concept evolves and immersive virtual experiences become more prevalent.
The metaverse privacy problem refers to the challenges and concerns surrounding
the privacy of personal information and data within Virtual Reality (VR)
environments as the concept of a shared VR space becomes more accessible.
Metaverse will harness advancements from various technologies such as
Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and
5G/6G-based communication to provide personalized and immersive services to its
users. Moreover, to enable more personalized experiences, the metaverse relies
on the collection of fine-grained user data that leads to various privacy
issues. Therefore, before the potential of the metaverse can be fully realized,
privacy concerns related to personal information and data within VR
environments must be addressed. This includes safeguarding users' control over
their data, ensuring the security of their personal information, and protecting
in-world actions and interactions from unauthorized sharing. In this paper, we
explore various privacy challenges that future metaverses are expected to face,
given their reliance on AI for tracking users, creating XR and MR experiences,
and facilitating interactions. Moreover, we thoroughly analyze technical
solutions such as differential privacy, Homomorphic Encryption (HE), and
Federated Learning (FL) and discuss related sociotechnical issues regarding
privacy.","['Mahdi Alkaeed', 'Adnan Qayyum', 'Junaid Qadir']",2023-09-19T11:56:12Z,http://arxiv.org/abs/2310.10665v1,"['cs.CR', 'cs.AI', 'cs.LG']","Privacy Preservation,Artificial Intelligence,Extended Reality,Metaverses,Virtual Reality,Mixed Reality,5G/6G-based communication,Fine-grained user data,Differential Privacy,Homomorphic Encryption"
"Towards Inferring Users' Impressions of Robot Performance in Navigation
  Scenarios","Human impressions of robot performance are often measured through surveys. As
a more scalable and cost-effective alternative, we study the possibility of
predicting people's impressions of robot behavior using non-verbal behavioral
cues and machine learning techniques. To this end, we first contribute the SEAN
TOGETHER Dataset consisting of observations of an interaction between a person
and a mobile robot in a Virtual Reality simulation, together with impressions
of robot performance provided by users on a 5-point scale. Second, we
contribute analyses of how well humans and supervised learning techniques can
predict perceived robot performance based on different combinations of
observation types (e.g., facial, spatial, and map features). Our results show
that facial expressions alone provide useful information about human
impressions of robot performance; but in the navigation scenarios we tested,
spatial features are the most critical piece of information for this inference
task. Also, when evaluating results as binary classification (rather than
multiclass classification), the F1-Score of human predictions and machine
learning models more than doubles, showing that both are better at telling the
directionality of robot performance than predicting exact performance ratings.
Based on our findings, we provide guidelines for implementing these predictions
models in real-world navigation scenarios.","['Qiping Zhang', 'Nathan Tsoi', 'Booyeon Choi', 'Jie Tan', 'Hao-Tien Lewis Chiang', 'Marynel Vázquez']",2023-10-17T21:12:32Z,http://arxiv.org/abs/2310.11590v1,"['cs.RO', 'cs.LG']","Impressions,Robot performance,Navigation,Behavioral cues,Machine learning,SEAN TOGETHER Dataset,Facial expressions,Spatial features,Binary classification,F1-Score"
"Video Quality Assessment and Coding Complexity of the Versatile Video
  Coding Standard","In recent years, the proliferation of multimedia applications and formats,
such as IPTV, Virtual Reality (VR, 360-degree), and point cloud videos, has
presented new challenges to the video compression research community.
Simultaneously, there has been a growing demand from users for higher
resolutions and improved visual quality. To further enhance coding efficiency,
a new video coding standard, Versatile Video Coding (VVC), was introduced in
July 2020. This paper conducts a comprehensive analysis of coding performance
and complexity for the latest VVC standard in comparison to its predecessor,
High Efficiency Video Coding (HEVC). The study employs a diverse set of test
sequences, covering both High Definition (HD) and Ultra High Definition (UHD)
resolutions, and spans a wide range of bit-rates. These sequences are encoded
using the reference software encoders of HEVC (HM) and VVC (VTM). The results
consistently demonstrate that VVC outperforms HEVC, achieving bit-rate savings
of up to 40% on the subjective quality scale, particularly at realistic
bit-rates and quality levels. Objective quality metrics, including PSNR, SSIM,
and VMAF, support these findings, revealing bit-rate savings ranging from 31%
to 40%, depending on the video content, spatial resolution, and the selected
quality metric. However, these improvements in coding efficiency come at the
cost of significantly increased computational complexity. On average, our
results indicate that the VVC decoding process is 1.5 times more complex, while
the encoding process becomes at least eight times more complex than that of the
HEVC reference encoder. Our simultaneous profiling of the two standards sheds
light on the primary evolutionary differences between them and highlights the
specific stages responsible for the observed increase in complexity.","['Thomas Amestoy', 'Naty Sidaty', 'Wassim Hamidouche', 'Pierrick Philippe', 'Daniel Menard']",2023-10-19T18:48:02Z,http://arxiv.org/abs/2310.13093v1,['eess.IV'],"Video Quality Assessment,Coding Complexity,Versatile Video Coding Standard,VVC,High Efficiency Video Coding,HEVC,Bit-rate,PSNR,SSIM,VMAF"
"TexFusion: Synthesizing 3D Textures with Text-Guided Image Diffusion
  Models","We present TexFusion (Texture Diffusion), a new method to synthesize textures
for given 3D geometries, using large-scale text-guided image diffusion models.
In contrast to recent works that leverage 2D text-to-image diffusion models to
distill 3D objects using a slow and fragile optimization process, TexFusion
introduces a new 3D-consistent generation technique specifically designed for
texture synthesis that employs regular diffusion model sampling on different 2D
rendered views. Specifically, we leverage latent diffusion models, apply the
diffusion model's denoiser on a set of 2D renders of the 3D object, and
aggregate the different denoising predictions on a shared latent texture map.
Final output RGB textures are produced by optimizing an intermediate neural
color field on the decodings of 2D renders of the latent texture. We thoroughly
validate TexFusion and show that we can efficiently generate diverse, high
quality and globally coherent textures. We achieve state-of-the-art text-guided
texture synthesis performance using only image diffusion models, while avoiding
the pitfalls of previous distillation-based methods. The text-conditioning
offers detailed control and we also do not rely on any ground truth 3D textures
for training. This makes our method versatile and applicable to a broad range
of geometry and texture types. We hope that TexFusion will advance AI-based
texturing of 3D assets for applications in virtual reality, game design,
simulation, and more.","['Tianshi Cao', 'Karsten Kreis', 'Sanja Fidler', 'Nicholas Sharp', 'Kangxue Yin']",2023-10-20T19:15:29Z,http://arxiv.org/abs/2310.13772v1,"['cs.CV', 'cs.LG', 'I.3.3']","3D textures,Text-guided,Image diffusion models,Texture synthesis,Latent diffusion models,Neural color field,Texture map,Texturing,Virtual reality,Game design"
"Examination of Cybersickness in Virtual Reality: The Role of Individual
  Differences, Effects on Cognitive Functions & Motor Skills, and Intensity
  Differences During and After Immersion","Background: Given that VR is applied in multiple domains, understanding the
effects of cyber-sickness on human cognition and motor skills and the factors
contributing to cybersickness gains urgency. This study aimed to explore the
predictors of cybersickness and its interplay with cognitive and motor skills.
Methods: 30 participants, 20-45 years old, completed the MSSQ and the CSQ-VR,
and were immersed in VR. During immersion, they were exposed to a roller
coaster ride. Before and after the ride, participants responded to CSQ-VR and
performed VR-based cognitive and psychomotor tasks. Post-VR session,
participants completed the CSQ-VR again. Results: Motion sickness
susceptibility, during adulthood, was the most prominent predictor of
cybersickness. Pupil dilation emerged as a significant predictor of
cybersickness. Experience in videogaming was a significant predictor of both
cybersickness and cognitive/motor functions. Cybersickness negatively affected
visuospatial working memory and psychomotor skills. Overall cybersickness',
nausea and vestibular symptoms' intensities significantly decreased after
removing the VR headset. Conclusions: In order of importance, motion sickness
susceptibility and gaming experience are significant predictors of
cybersickness. Pupil dilation appears as a cybersickness' biomarker.
Cybersickness negatively affects visuospatial working memory and psychomotor
skills. Cybersickness and its effects on performance should be examined during
and not after immersion.","['Panagiotis Kourtesis', 'Agapi Papadopoulou', 'Petros Roussos']",2023-10-26T12:23:08Z,http://arxiv.org/abs/2310.17344v1,"['cs.HC', 'I.3.6; I.3.7; H.5.1; H.5.2; J.4; J.7']","Cybersickness,Individual Differences,Cognitive Functions,Motor Skills,Intensity Differences,Virtual Reality,Immersion,Motion Sickness Susceptibility,Pupil Dilation"
Neural Stress Fields for Reduced-order Elastoplasticity and Fracture,"We propose a hybrid neural network and physics framework for reduced-order
modeling of elastoplasticity and fracture. State-of-the-art scientific
computing models like the Material Point Method (MPM) faithfully simulate
large-deformation elastoplasticity and fracture mechanics. However, their long
runtime and large memory consumption render them unsuitable for applications
constrained by computation time and memory usage, e.g., virtual reality. To
overcome these barriers, we propose a reduced-order framework. Our key
innovation is training a low-dimensional manifold for the Kirchhoff stress
field via an implicit neural representation. This low-dimensional neural stress
field (NSF) enables efficient evaluations of stress values and,
correspondingly, internal forces at arbitrary spatial locations. In addition,
we also train neural deformation and affine fields to build low-dimensional
manifolds for the deformation and affine momentum fields. These neural stress,
deformation, and affine fields share the same low-dimensional latent space,
which uniquely embeds the high-dimensional simulation state. After training, we
run new simulations by evolving in this single latent space, which drastically
reduces the computation time and memory consumption. Our general
continuum-mechanics-based reduced-order framework is applicable to any
phenomena governed by the elastodynamics equation. To showcase the versatility
of our framework, we simulate a wide range of material behaviors, including
elastica, sand, metal, non-Newtonian fluids, fracture, contact, and collision.
We demonstrate dimension reduction by up to 100,000X and time savings by up to
10X.","['Zeshun Zong', 'Xuan Li', 'Minchen Li', 'Maurizio M. Chiaramonte', 'Wojciech Matusik', 'Eitan Grinspun', 'Kevin Carlberg', 'Chenfanfu Jiang', 'Peter Yichen Chen']",2023-10-26T21:37:32Z,http://arxiv.org/abs/2310.17790v1,"['cs.GR', 'cs.CE', 'cs.LG', 'cs.NA', 'math.NA']","neural network,reduced-order modeling,elastoplasticity,fracture,Material Point Method (MPM),neural representation,latent space,continuum mechanics,dimension reduction,time savings"
"MolecularWebXR: Multiuser discussions about chemistry and biology in
  immersive and inclusive VR","MolecularWebXR is our new website for education, science communication and
scientific peer discussion in chemistry and biology built on WebXR. It
democratizes multi-user, inclusive virtual reality (VR) experiences that are
deeply immersive for users wearing high-end headsets, yet allow participation
by users with consumer devices such as smartphones, possibly inserted into
cardboard goggles for immersivity, or even computers or tablets. With no
installs as it is all web-served, MolecularWebXR enables multiple users to
simultaneously explore, communicate and discuss chemistry and biology concepts
in immersive 3D environments, manipulating objects with their bare hands,
either present in the same real space or scattered throughout the globe thanks
to built-in audio features. A series of preset rooms cover educational material
on chemistry and structural biology, and an empty room can be populated with
material prepared ad hoc using moleculARweb's VMD-based PDB2AR tool. We
verified ease of use and versatility by users aged 12-80 in entirely virtual
sessions or mixed real-virtual sessions at science outreach events, student
instruction, scientific collaborations, and conference lectures. MolecularWebXR
is available for free use without registration at https://molecularwebxr.org,
and a blog post version of this preprint with embedded videos is available at
https://go.epfl.ch/molecularwebxr-blog-post.","['Fabio J. Cortes Rodriguez', 'Gianfranco Frattini', 'Fernando Teixeira Pinto Meireles', 'Danae A. Terrien', 'Sergio Cruz-Leon', 'Matteo Dal Peraro', 'Eva Schier', 'Diego M. Moreno', 'Luciano A. Abriata']",2023-11-01T09:22:08Z,http://arxiv.org/abs/2311.00385v1,"['cs.HC', 'cs.GR', 'q-bio.BM', '68U05 (Primary)', 'I.3.7']","MolecularWebXR,chemistry,biology,WebXR,virtual reality,immersive,multi-user,3D environments,structural biology,VMD-based PDB2AR"
Fixation-based Self-calibration for Eye Tracking in VR Headsets,"This study proposes a novel self-calibration method for eye tracking in a
virtual reality (VR) headset. The proposed method is based on the assumptions
that the user's viewpoint can freely move and that the points of regard (PoRs)
from different viewpoints are distributed within a small area on an object
surface during visual fixation. In the method, fixations are first detected
from the time-series data of uncalibrated gaze directions using an extension of
the I-VDT (velocity and dispersion threshold identification) algorithm to a
three-dimensional (3D) scene. Then, the calibration parameters are optimized by
minimizing the sum of a dispersion metrics of the PoRs. The proposed method can
potentially identify the optimal calibration parameters representing the
user-dependent offset from the optical axis to the visual axis without explicit
user calibration, image processing, or marker-substitute objects. For the gaze
data of 18 participants walking in two VR environments with many occlusions,
the proposed method achieved an accuracy of 2.1$^\circ$, which was
significantly lower than the average offset. Our method is the first
self-calibration method with an average error lower than 3$^\circ$ in 3D
environments. Further, the accuracy of the proposed method can be improved by
up to 1.2$^\circ$ by refining the fixation detection or optimization algorithm.","['Ryusei Uramune', 'Sei Ikeda', 'Hiroki Ishizuka', 'Osamu Oshiro']",2023-11-01T09:34:15Z,http://arxiv.org/abs/2311.00391v2,"['cs.CV', 'cs.HC']","eye tracking,VR headset,self-calibration,fixations,gaze directions,3D scene,calibration parameters,dispersion metrics,optical axis,visual axis"
Collaboration in Immersive Environments: Challenges and Solutions,"Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.",['Shahin Doroudian'],2023-11-01T17:45:22Z,http://arxiv.org/abs/2311.00689v3,"['cs.HC', 'cs.CV']","Immersive environments,Collaboration,Virtual Reality,Augmented Reality,Shared Cooperative Activities,Communication,Coordination,Social presence,Usability,Research"
VR-NeRF: High-Fidelity Virtualized Walkable Spaces,"We present an end-to-end system for the high-fidelity capture, model
reconstruction, and real-time rendering of walkable spaces in virtual reality
using neural radiance fields. To this end, we designed and built a custom
multi-camera rig to densely capture walkable spaces in high fidelity and with
multi-view high dynamic range images in unprecedented quality and density. We
extend instant neural graphics primitives with a novel perceptual color space
for learning accurate HDR appearance, and an efficient mip-mapping mechanism
for level-of-detail rendering with anti-aliasing, while carefully optimizing
the trade-off between quality and speed. Our multi-GPU renderer enables
high-fidelity volume rendering of our neural radiance field model at the full
VR resolution of dual 2K$\times$2K at 36 Hz on our custom demo machine. We
demonstrate the quality of our results on our challenging high-fidelity
datasets, and compare our method and datasets to existing baselines. We release
our dataset on our project website.","['Linning Xu', 'Vasu Agrawal', 'William Laney', 'Tony Garcia', 'Aayush Bansal', 'Changil Kim', 'Samuel Rota Bulò', 'Lorenzo Porzi', 'Peter Kontschieder', 'Aljaž BožiāE, 'Dahua Lin', 'Michael Zollhöfer', 'Christian Richardt']",2023-11-05T02:03:14Z,http://arxiv.org/abs/2311.02542v1,"['cs.CV', 'cs.GR']","virtual reality,neural radiance fields,multi-camera rig,high dynamic range,neural graphics,HDR appearance,mip-mapping,level-of-detail rendering,anti-aliasing,multi-GPU renderer"
"Multimodal extended reality applications offer benefits for volumetric
  biomedical image analysis in research and medicine","3D data from high-resolution volumetric imaging is a central resource for
diagnosis and treatment in modern medicine. While the fast development of AI
enhances imaging and analysis, commonly used visualization methods lag far
behind. Recent research used extended reality (XR) for perceiving 3D images
with visual depth perception and touch, but used restricting haptic devices.
While unrestricted touch is beneficial for volumetric data examination,
implementing natural haptic interaction with XR is challenging. The research
question is whether a multimodal XR application with intutitive haptic
interaction adds value and should be pursued. In a study, 24 expterts for
biomedical images in research and medicine. explored 3D anatomical medical
shapes with 3 applications: a multimodal virtual reality (VR) prototype using
haptic gloves, a simple VR prototype using VR controllers, and a commonly used
standard PC application. Results of the standardized questionnaires showed no
significant differences between the three application types regarding usability
and no significant difference between both VR applications regarding presence.
Participants agreed to statements that VR visualizations provide better depth
information, that using the hands instead of controllers simplifies data
exploration, that the multimodal VR prototype allows intuitive data
exploration, and that it is beneficial over traditional data examination
methods. While most participants mentioned the manual interaction as best
aspect, they also found it the most improvable. We conclude that a multimodal
XR application with improved manual interaction adds value for volumetric
biomedical data examination. We will proceed with our open-source research
project ISH3DE (Intuitive Stereoptic Haptic 3D Data Exploration) to serve
medical education, therapeutic decisions, surgery preparations, or research
data analysis.","['Kathrin Krieger', 'Jan Egger', 'Jens Kleesiek', 'Matthias Gunzer', 'Jianxu Chen']",2023-11-07T13:37:47Z,http://arxiv.org/abs/2311.03986v1,"['cs.SE', 'cs.GR', 'cs.HC']","volumetric imaging,extended reality,XR,haptic interaction,virtual reality,VR,3D data,biomedical images,data exploration,medical shapes"
"DualTalker: A Cross-Modal Dual Learning Approach for Speech-Driven 3D
  Facial Animation","In recent years, audio-driven 3D facial animation has gained significant
attention, particularly in applications such as virtual reality, gaming, and
video conferencing. However, accurately modeling the intricate and subtle
dynamics of facial expressions remains a challenge. Most existing studies
approach the facial animation task as a single regression problem, which often
fail to capture the intrinsic inter-modal relationship between speech signals
and 3D facial animation and overlook their inherent consistency. Moreover, due
to the limited availability of 3D-audio-visual datasets, approaches learning
with small-size samples have poor generalizability that decreases the
performance. To address these issues, in this study, we propose a cross-modal
dual-learning framework, termed DualTalker, aiming at improving data usage
efficiency as well as relating cross-modal dependencies. The framework is
trained jointly with the primary task (audio-driven facial animation) and its
dual task (lip reading) and shares common audio/motion encoder components. Our
joint training framework facilitates more efficient data usage by leveraging
information from both tasks and explicitly capitalizing on the complementary
relationship between facial motion and audio to improve performance.
Furthermore, we introduce an auxiliary cross-modal consistency loss to mitigate
the potential over-smoothing underlying the cross-modal complementary
representations, enhancing the mapping of subtle facial expression dynamics.
Through extensive experiments and a perceptual user study conducted on the VOCA
and BIWI datasets, we demonstrate that our approach outperforms current
state-of-the-art methods both qualitatively and quantitatively. We have made
our code and video demonstrations available at
https://github.com/sabrina-su/iadf.git.","['Guinan Su', 'Yanwu Yang', 'Zhifeng Li']",2023-11-08T15:39:56Z,http://arxiv.org/abs/2311.04766v2,['cs.CV'],"Cross-Modal Learning,Speech-Driven,3D Facial Animation,Audio-Driven,Data Usage Efficiency,Cross-Modal Dependencies,Lip Reading,Joint Training Framework,Facial Expression Dynamics,State-of-the-Art Methods."
"3DFusion, A real-time 3D object reconstruction pipeline based on
  streamed instance segmented data","This paper presents a real-time segmentation and reconstruction system that
utilizes RGB-D images to generate accurate and detailed individual 3D models of
objects within a captured scene. Leveraging state-of-the-art instance
segmentation techniques, the system performs pixel-level segmentation on RGB-D
data, effectively separating foreground objects from the background. The
segmented objects are then reconstructed into distinct 3D models in a
high-performance computation platform. The real-time 3D modelling can be
applied across various domains, including augmented/virtual reality, interior
design, urban planning, road assistance, security systems, and more. To achieve
real-time performance, the paper proposes a method that effectively samples
consecutive frames to reduce network load while ensuring reconstruction
quality. Additionally, a multi-process SLAM pipeline is adopted for parallel 3D
reconstruction, enabling efficient cutting of the clustering objects into
individuals. This system employs the industry-leading framework YOLO for
instance segmentation. To improve YOLO's performance and accuracy,
modifications were made to resolve duplicated or false detection of similar
objects, ensuring the reconstructed models align with the targets. Overall,
this work establishes a robust real-time system with a significant enhancement
for object segmentation and reconstruction in the indoor environment. It can
potentially be extended to the outdoor scenario, opening up numerous
opportunities for real-world applications.","['Xi Sun', 'Derek Jacoby', 'Yvonne Coady']",2023-11-11T20:11:58Z,http://arxiv.org/abs/2311.06659v1,['cs.CV'],"3DFusion,real-time,3D object reconstruction,pipeline,streamed instance segmented data"
"Mapping Eye Vergence Angle to the Depth of Real and Virtual Objects as
  an Objective Measure of Depth Perception","Recently, extended reality (XR) displays including augmented reality (AR) and
virtual reality (VR) have integrated eye tracking capabilities, which could
enable novel ways of interacting with XR content. The vergence angle of the
eyes constantly changes according to the distance of fixated objects. Here we
measured vergence angle for eye fixations on real and simulated target objects
in three different environments: real objects in the real-world (real), virtual
objects in the real-world (AR), and virtual objects in the virtual world (VR)
using gaze data from an eye-tracking device. In a repeated-measures design with
13 participants, Gaze-measured Vergence Angle (GVA) was measured while
participants fixated on targets at varying distances. As expected, results
showed a significant main effect of target depth such that increasing GVA was
associated with closer targets. However, there were consistent individual
differences in baseline GVA. When these individual differences were controlled
for, there was a small but statistically-significant main effect of environment
(real, AR, VR). Importantly, GVA was stable with respect to the starting depth
of previously fixated targets and invariant to directionality (convergence vs.
divergence). In addition, GVA proved to be a more veridical depth estimate than
subjective depth judgements.","['Mohammed Safayet Arefin', 'J. Edward Swan II', 'Russell Cohen Hoffing', 'Steven Thurman']",2023-11-12T06:50:32Z,http://arxiv.org/abs/2311.09242v2,['cs.HC'],"eye vergence angle,depth perception,extended reality,eye tracking,vergence angle,real-world,augmented reality,virtual reality,gaze data,depth estimate"
"EvaSurf: Efficient View-Aware Implicit Textured Surface Reconstruction
  on Mobile Devices","Reconstructing real-world 3D objects has numerous applications in computer
vision, such as virtual reality, video games, and animations. Ideally, 3D
reconstruction methods should generate high-fidelity results with 3D
consistency in real-time. Traditional methods match pixels between images using
photo-consistency constraints or learned features, while differentiable
rendering methods like Neural Radiance Fields (NeRF) use differentiable volume
rendering or surface-based representation to generate high-fidelity scenes.
However, these methods require excessive runtime for rendering, making them
impractical for daily applications. To address these challenges, we present
$\textbf{EvaSurf}$, an $\textbf{E}$fficient $\textbf{V}$iew-$\textbf{A}$ware
implicit textured $\textbf{Surf}$ace reconstruction method on mobile devices.
In our method, we first employ an efficient surface-based model with a
multi-view supervision module to ensure accurate mesh reconstruction. To enable
high-fidelity rendering, we learn an implicit texture embedded with a set of
Gaussian lobes to capture view-dependent information. Furthermore, with the
explicit geometry and the implicit texture, we can employ a lightweight neural
shader to reduce the expense of computation and further support real-time
rendering on common mobile devices. Extensive experiments demonstrate that our
method can reconstruct high-quality appearance and accurate mesh on both
synthetic and real-world datasets. Moreover, our method can be trained in just
1-2 hours using a single GPU and run on mobile devices at over 40 FPS (Frames
Per Second), with a final package required for rendering taking up only 40-50
MB.","['Jingnan Gao', 'Zhuo Chen', 'Yichao Yan', 'Bowen Pan', 'Zhe Wang', 'Jiangjing Lyu', 'Xiaokang Yang']",2023-11-16T11:30:56Z,http://arxiv.org/abs/2311.09806v2,['cs.CV'],"Efficient View-Aware,Implicit Textured Surface Reconstruction,3D reconstruction,Neural Radiance Fields,differentiable rendering methods,mobile devices,real-time rendering,neural shader,multi-view supervision"
"GaussianEditor: Swift and Controllable 3D Editing with Gaussian
  Splatting","3D editing plays a crucial role in many areas such as gaming and virtual
reality. Traditional 3D editing methods, which rely on representations like
meshes and point clouds, often fall short in realistically depicting complex
scenes. On the other hand, methods based on implicit 3D representations, like
Neural Radiance Field (NeRF), render complex scenes effectively but suffer from
slow processing speeds and limited control over specific scene areas. In
response to these challenges, our paper presents GaussianEditor, an innovative
and efficient 3D editing algorithm based on Gaussian Splatting (GS), a novel 3D
representation. GaussianEditor enhances precision and control in editing
through our proposed Gaussian semantic tracing, which traces the editing target
throughout the training process. Additionally, we propose Hierarchical Gaussian
splatting (HGS) to achieve stabilized and fine results under stochastic
generative guidance from 2D diffusion models. We also develop editing
strategies for efficient object removal and integration, a challenging task for
existing methods. Our comprehensive experiments demonstrate GaussianEditor's
superior control, efficacy, and rapid performance, marking a significant
advancement in 3D editing. Project Page:
https://buaacyw.github.io/gaussian-editor/","['Yiwen Chen', 'Zilong Chen', 'Chi Zhang', 'Feng Wang', 'Xiaofeng Yang', 'Yikai Wang', 'Zhongang Cai', 'Lei Yang', 'Huaping Liu', 'Guosheng Lin']",2023-11-24T14:46:59Z,http://arxiv.org/abs/2311.14521v4,['cs.CV'],"3D editing,Gaussian Splatting,Gaussian semantic tracing,Hierarchical Gaussian splatting,Neural Radiance Field (NeRF),2D diffusion models,object removal,integration,3D representation,editing algorithm"
"Comparing Feature Engineering and End-to-End Deep Learning for Autism
  Spectrum Disorder Assessment based on Fullbody-Tracking","Autism Spectrum Disorder (ASD) is characterized by challenges in social
communication and restricted patterns, with motor abnormalities gaining
traction for early detection. However, kinematic analysis in ASD is limited,
often lacking robust validation and relying on hand-crafted features for single
tasks, leading to inconsistencies across studies. Thus, end-to-end models have
become promising methods to overcome the need for feature engineering. Our aim
is to assess both approaches across various kinematic tasks to measure the
efficacy of commonly used features in ASD assessment, while comparing them to
end-to-end models. Specifically, we developed a virtual reality environment
with multiple motor tasks and trained models using both classification
approaches. We prioritized a reliable validation framework with repeated
cross-validation. Our comparative analysis revealed that hand-crafted features
outperformed our deep learning approach in specific tasks, achieving a
state-of-the-art area under the curve (AUC) of 0.90$\pm$0.06. Conversely,
end-to-end models provided more consistent results with less variability across
all VR tasks, demonstrating domain generalization and reliability, with a
maximum task AUC of 0.89$\pm$0.06. These findings show that end-to-end models
enable less variable and context-independent ASD assessments without requiring
domain knowledge or task specificity. However, they also recognize the
effectiveness of hand-crafted features in specific task scenarios.","['Alberto Altozano', 'Maria Eleonora Minissi', 'Mariano Alcañiz', 'Javier Marín-Morales']",2023-11-24T14:56:36Z,http://arxiv.org/abs/2311.14533v1,['cs.LG'],"Autism Spectrum Disorder,Feature Engineering,End-to-End Deep Learning,Fullbody-Tracking,Kinematic Analysis,Validation,Classification,Virtual Reality,Area under the Curve,Domain Generalization"
Visualizing Plasma Physics Simulations in Immersive Environments,"Plasma physics simulations create complex datasets for which researchers need
state-of-the-art visualization tools to gain insights. These datasets are 3D in
nature but are commonly depicted and analyzed using 2D idioms displayed on 2D
screens. These offer limited understandability in a domain where spatial
awareness is key. Virtual reality (VR) can be used as an alternative to
conventional means for analyzing such datasets. VR has been known to improve
depth and spatial relationship perception, which are fundamental for obtaining
insights into 3D plasma morphology. Likewise, VR can potentially increase user
engagement by offering more immersive and enjoyable experiences. Methods This
study presents PlasmaVR, a proof-of-concept VR tool for visualizing datasets
resulting from plasma physics simulations. It enables immersive
multidimensional data visualization of particles, scalar, and vector fields and
uses a more natural interface. The study includes user evaluation with domain
experts where PlasmaVR was employed to assess the possible benefits of
immersive environments in plasma physics visualization. The experimental group
comprised five plasma physics researchers who were asked to perform tasks
designed to represent their typical analysis workflow. To assess the
suitability of the prototype for the different types of tasks, a set of
objective metrics, such as completion time and number of errors, were measured.
The prototype's usability was also evaluated using a standard System Usability
Survey questionnaire.","['Nuno Verdelho Trindade', 'Oscar Amaro', 'David Bras', 'Daniel Goncalves', 'João Madeiras Pereira', 'Alfredo Ferreira']",2023-11-24T16:32:06Z,http://arxiv.org/abs/2311.14593v1,['cs.GR'],"Plasma physics simulations,visualization tools,3D datasets,2D screens,virtual reality,immersive environments,multidimensional data visualization,scalar fields,vector fields,user evaluation"
"StreamFunnel: Facilitating Communication Between a VR Streamer and Many
  Spectators","The increasing adoption of Virtual Reality (VR) systems in different domains
have led to a need to support interaction between many spectators and a VR
user. This is common in game streaming, live performances, and webinars. Prior
CSCW systems for VR environments are limited to small groups of users. In this
work, we identify problems associated with interaction carried out with large
groups of users. To address this, we introduce an additional user role: the
co-host. They mediate communications between the VR user and many spectators.
To facilitate this mediation, we present StreamFunnel, which allows the co-host
to be part of the VR application's space and interact with it. The design of
StreamFunnel was informed by formative interviews with six experts.
StreamFunnel uses a cloud-based streaming solution to enable remote co-host and
many spectators to view and interact through standard web browsers, without
requiring any custom software. We present results of informal user testing
which provides insights into StreamFunnel's ability to facilitate these
scalable interactions. Our participants, who took the role of a co-host, found
that StreamFunnel enables them to add value in presenting the VR experience to
the spectators and relaying useful information from the live chat to the VR
user.","['Haohua Lyu', 'Cyrus Vachha', 'Qianyi Chen', 'Balasaravanan Thoravi Kumaravel', 'Bjöern Hartmann']",2023-11-25T04:43:57Z,http://arxiv.org/abs/2311.14930v1,['cs.HC'],"Virtual Reality,VR systems,interaction,co-host,StreamFunnel,cloud-based streaming,web browsers,informal user testing,scalable interactions"
"Move or Push? Studying Pseudo-Haptic Perceptions Obtained with Motion or
  Force Input","Pseudo-haptics techniques are interesting alternatives for generating haptic
perceptions, which entails the manipulation of haptic perception through the
appropriate alteration of primarily visual feedback in response to body
movements. However, the use of pseudo-haptics techniques with a motion-input
system can sometimes be limited. This paper investigates a novel approach for
extending the potential of pseudo-haptics techniques in virtual reality (VR).
The proposed approach utilizes a reaction force from force-input as a
substitution of haptic cue for the pseudo-haptic perception. The paper
introduced a manipulation method in which the vertical acceleration of the
virtual hand is controlled by the extent of push-in of a force sensor. Such a
force-input manipulation of a virtual body can not only present pseudo-haptics
with less physical spaces and be used by more various users including
physically handicapped people, but also can present the reaction force
proportional to the user's input to the user. We hypothesized that such a
haptic force cue would contribute to the pseudo-haptic perception. Therefore,
the paper endeavors to investigate the force-input pseudo-haptic perception in
a comparison with the motion-input pseudo-haptics. The paper compared
force-input and motion-input manipulation in a point of achievable range and
resolution of pseudo-haptic weight. The experimental results suggest that the
force-input manipulation successfully extends the range of perceptible
pseudo-weight by 80\% in comparison to the motion-input manipulation. On the
other hand, it is revealed that the motion-input manipulation has 1 step larger
number of distinguishable weight levels and is easier to operate than the
force-input manipulation.","['Yutaro Hirao', 'Takuji Narumi', 'Ferran Argelaguet', 'Anatole Lecuyer']",2023-11-27T05:22:03Z,http://arxiv.org/abs/2311.15546v1,['cs.HC'],"Pseudo-haptics,Haptic perceptions,Motion input,Force input,Virtual reality,Reaction force,Manipulation method,Pseudo-haptic weight,Physically handicapped,Perceptible pseudo-weight"
"Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information","Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.","['Jie Li', 'Zhixin Li', 'Zhi Liu', 'Pengyuan Zhou', 'Richang Hong', 'Qiyue Li', 'Han Hu']",2023-11-28T03:45:29Z,http://arxiv.org/abs/2311.16462v1,"['cs.CV', 'cs.MM']","1. Volumetric video
2. Viewport prediction
3. Video saliency
4. Trajectory information
5. Virtual Reality (VR)
6. Augmented Reality (AR)
7. Mixed Reality (MR)
8. Wireless communication
9. Saliency detection
10. Computational complexity"
"Implicit-explicit Integrated Representations for Multi-view Video
  Compression","With the increasing consumption of 3D displays and virtual reality,
multi-view video has become a promising format. However, its high resolution
and multi-camera shooting result in a substantial increase in data volume,
making storage and transmission a challenging task. To tackle these
difficulties, we propose an implicit-explicit integrated representation for
multi-view video compression. Specifically, we first use the explicit
representation-based 2D video codec to encode one of the source views.
Subsequently, we propose employing the implicit neural representation
(INR)-based codec to encode the remaining views. The implicit codec takes the
time and view index of multi-view video as coordinate inputs and generates the
corresponding implicit reconstruction frames.To enhance the compressibility, we
introduce a multi-level feature grid embedding and a fully convolutional
architecture into the implicit codec. These components facilitate
coordinate-feature and feature-RGB mapping, respectively. To further enhance
the reconstruction quality from the INR codec, we leverage the high-quality
reconstructed frames from the explicit codec to achieve inter-view
compensation. Finally, the compensated results are fused with the implicit
reconstructions from the INR to obtain the final reconstructed frames. Our
proposed framework combines the strengths of both implicit neural
representation and explicit 2D codec. Extensive experiments conducted on public
datasets demonstrate that the proposed framework can achieve comparable or even
superior performance to the latest multi-view video compression standard MIV
and other INR-based schemes in terms of view compression and scene modeling.","['Chen Zhu', 'Guo Lu', 'Bing He', 'Rong Xie', 'Li Song']",2023-11-29T04:15:57Z,http://arxiv.org/abs/2311.17350v1,"['cs.CV', 'cs.MM']","multi-view video,compression,implicit-explicit integrated representations,2D video codec,implicit neural representation,coordinate input,reconstruction frames,feature grid embedding,convolutional architecture,inter-view compensation."
Gaussian Shell Maps for Efficient 3D Human Generation,"Efficient generation of 3D digital humans is important in several industries,
including virtual reality, social media, and cinematic production. 3D
generative adversarial networks (GANs) have demonstrated state-of-the-art
(SOTA) quality and diversity for generated assets. Current 3D GAN
architectures, however, typically rely on volume representations, which are
slow to render, thereby hampering the GAN training and requiring
multi-view-inconsistent 2D upsamplers. Here, we introduce Gaussian Shell Maps
(GSMs) as a framework that connects SOTA generator network architectures with
emerging 3D Gaussian rendering primitives using an articulable multi
shell--based scaffold. In this setting, a CNN generates a 3D texture stack with
features that are mapped to the shells. The latter represent inflated and
deflated versions of a template surface of a digital human in a canonical body
pose. Instead of rasterizing the shells directly, we sample 3D Gaussians on the
shells whose attributes are encoded in the texture features. These Gaussians
are efficiently and differentiably rendered. The ability to articulate the
shells is important during GAN training and, at inference time, to deform a
body into arbitrary user-defined poses. Our efficient rendering scheme bypasses
the need for view-inconsistent upsamplers and achieves high-quality multi-view
consistent renderings at a native resolution of $512 \times 512$ pixels. We
demonstrate that GSMs successfully generate 3D humans when trained on
single-view datasets, including SHHQ and DeepFashion.","['Rameen Abdal', 'Wang Yifan', 'Zifan Shi', 'Yinghao Xu', 'Ryan Po', 'Zhengfei Kuang', 'Qifeng Chen', 'Dit-Yan Yeung', 'Gordon Wetzstein']",2023-11-29T18:04:07Z,http://arxiv.org/abs/2311.17857v1,"['cs.CV', 'cs.GR']","3D digital humans,Gaussian Shell Maps,3D generative adversarial networks,rendering,CNN,texture stack,3D Gaussians,GAN training,multi-view consistent,native resolution"
HybridNeRF: Efficient Neural Rendering via Adaptive Volumetric Surfaces,"Neural radiance fields provide state-of-the-art view synthesis quality but
tend to be slow to render. One reason is that they make use of volume
rendering, thus requiring many samples (and model queries) per ray at render
time. Although this representation is flexible and easy to optimize, most
real-world objects can be modeled more efficiently with surfaces instead of
volumes, requiring far fewer samples per ray. This observation has spurred
considerable progress in surface representations such as signed distance
functions, but these may struggle to model semi-opaque and thin structures. We
propose a method, HybridNeRF, that leverages the strengths of both
representations by rendering most objects as surfaces while modeling the
(typically) small fraction of challenging regions volumetrically. We evaluate
HybridNeRF against the challenging Eyeful Tower dataset along with other
commonly used view synthesis datasets. When comparing to state-of-the-art
baselines, including recent rasterization-based approaches, we improve error
rates by 15-30% while achieving real-time framerates (at least 36 FPS) for
virtual-reality resolutions (2Kx2K).","['Haithem Turki', 'Vasu Agrawal', 'Samuel Rota Bulò', 'Lorenzo Porzi', 'Peter Kontschieder', 'Deva Ramanan', 'Michael Zollhöfer', 'Christian Richardt']",2023-12-05T22:04:49Z,http://arxiv.org/abs/2312.03160v2,"['cs.CV', 'cs.GR', 'cs.LG']","Neural radiance fields,HybridNeRF,rendering,volumetric surfaces,view synthesis,volume rendering,model queries,surface representations,signed distance functions,real-time."
"Personalized Face Inpainting with Diffusion Models by Parallel Visual
  Attention","Face inpainting is important in various applications, such as photo
restoration, image editing, and virtual reality. Despite the significant
advances in face generative models, ensuring that a person's unique facial
identity is maintained during the inpainting process is still an elusive goal.
Current state-of-the-art techniques, exemplified by MyStyle, necessitate
resource-intensive fine-tuning and a substantial number of images for each new
identity. Furthermore, existing methods often fall short in accommodating
user-specified semantic attributes, such as beard or expression. To improve
inpainting results, and reduce the computational complexity during inference,
this paper proposes the use of Parallel Visual Attention (PVA) in conjunction
with diffusion models. Specifically, we insert parallel attention matrices to
each cross-attention module in the denoising network, which attends to features
extracted from reference images by an identity encoder. We train the added
attention modules and identity encoder on CelebAHQ-IDI, a dataset proposed for
identity-preserving face inpainting. Experiments demonstrate that PVA attains
unparalleled identity resemblance in both face inpainting and face inpainting
with language guidance tasks, in comparison to various benchmarks, including
MyStyle, Paint by Example, and Custom Diffusion. Our findings reveal that PVA
ensures good identity preservation while offering effective
language-controllability. Additionally, in contrast to Custom Diffusion, PVA
requires just 40 fine-tuning steps for each new identity, which translates to a
significant speed increase of over 20 times.","['Jianjin Xu', 'Saman Motamed', 'Praneetha Vaddamanu', 'Chen Henry Wu', 'Christian Haene', 'Jean-Charles Bazin', 'Fernando de la Torre']",2023-12-06T15:39:03Z,http://arxiv.org/abs/2312.03556v1,"['cs.CV', 'cs.LG']","Face inpainting,Diffusion models,Parallel Visual Attention,Identity preservation,Image editing,Virtual reality,Semantic attributes,Denoising network,Attention modules,Computational complexity"
Facial Emotion Recognition in VR Games,"Emotion detection is a crucial component of Games User Research (GUR), as it
allows game developers to gain insights into players' emotional experiences and
tailor their games accordingly. However, detecting emotions in Virtual Reality
(VR) games is challenging due to the Head-Mounted Display (HMD) that covers the
top part of the player's face, namely, their eyes and eyebrows, which provide
crucial information for recognizing the impression. To tackle this we used a
Convolutional Neural Network (CNN) to train a model to predict emotions in
full-face images where the eyes and eyebrows are covered. We used the FER2013
dataset, which we modified to cover eyes and eyebrows in images. The model in
these images can accurately recognize seven different emotions which are anger,
happiness, disgust, fear, impartiality, sadness and surprise.
  We assessed the model's performance by testing it on two VR games and using
it to detect players' emotions. We collected self-reported emotion data from
the players after the gameplay sessions. We analyzed the data collected from
our experiment to understand which emotions players experience during the
gameplay. We found that our approach has the potential to enhance gameplay
analysis by enabling the detection of players' emotions in VR games, which can
help game developers create more engaging and immersive game experiences.","['Fatemeh Dehghani', 'Loutfouz Zaman']",2023-12-12T01:40:14Z,http://arxiv.org/abs/2312.06925v1,"['cs.HC', 'cs.CV', 'cs.LG']","Facial Emotion Recognition,VR Games,Emotion Detection,Games User Research (GUR),Virtual Reality (VR),Head-Mounted Display (HMD),Convolutional Neural Network (CNN),FER2013 dataset,Emotions"
"Robotics Applications in Neurology: A Review of Recent Advancements and
  Future Directions","Robotic technology has the potential to revolutionize the field of neurology
by providing new methods for diagnosis, treatment, and rehabilitation of
neurological disorders. In recent years, there has been an increasing interest
in the development of robotics applications for neurology, driven by advances
in sensing, actuation, and control systems. This review paper provides a
comprehensive overview of the recent advancements in robotics technology for
neurology, with a focus on three main areas: diagnosis, treatment, and
rehabilitation. In the area of diagnosis, robotics has been used for developing
new imaging techniques and tools for more accurate and non-invasive mapping of
brain structures and functions. For treatment, robotics has been used for
developing minimally invasive surgical procedures, including stereotactic and
endoscopic approaches, as well as for the delivery of therapeutic agents to
specific targets in the brain. In rehabilitation, robotics has been used for
developing assistive devices and platforms for motor and cognitive training of
patients with neurological disorders. The paper also discusses the challenges
and limitations of current robotics technology for neurology, including the
need for more reliable and precise sensing and actuation systems, the
development of better control algorithms, and the ethical implications of
robotic interventions in the human brain. Finally, the paper outlines future
directions and opportunities for robotics applications in neurology, including
the integration of robotics with other emerging technologies, such as
neuroprosthetics, artificial intelligence, and virtual reality. Overall, this
review highlights the potential of robotics technology to transform the field
of neurology and improve the lives of patients with neurological disorders.","['Retnaningsih Retnaningsih', 'Agus Budiyono', 'Rifky Ismail', 'Dodik Tugasworo', 'Rivan Danuaji', 'Syahrul Syahrul', 'Hendry Gunawan']",2023-12-12T03:32:34Z,http://arxiv.org/abs/2312.06956v1,['cs.RO'],"Robotics,Neurology,Diagnosis,Treatment,Rehabilitation,Sensing,Actuation,Control systems,Imaging techniques,Surgical procedures"
"Exploring Large Language Models to Facilitate Variable Autonomy for
  Human-Robot Teaming","In a rapidly evolving digital landscape autonomous tools and robots are
becoming commonplace. Recognizing the significance of this development, this
paper explores the integration of Large Language Models (LLMs) like Generative
pre-trained transformer (GPT) into human-robot teaming environments to
facilitate variable autonomy through the means of verbal human-robot
communication. In this paper, we introduce a novel framework for such a
GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality
(VR) setting. This system allows users to interact with robot agents through
natural language, each powered by individual GPT cores. By means of OpenAI's
function calling, we bridge the gap between unstructured natural language input
and structure robot actions. A user study with 12 participants explores the
effectiveness of GPT-4 and, more importantly, user strategies when being given
the opportunity to converse in natural language within a multi-robot
environment. Our findings suggest that users may have preconceived expectations
on how to converse with robots and seldom try to explore the actual language
and cognitive capabilities of their robot collaborators. Still, those users who
did explore where able to benefit from a much more natural flow of
communication and human-like back-and-forth. We provide a set of lessons
learned for future research and technical implementations of similar systems.","['Younes Lakhnati', 'Max Pascher', 'Jens Gerken']",2023-12-12T12:26:48Z,http://arxiv.org/abs/2312.07214v3,"['cs.HC', 'cs.AI', 'cs.RO']","Large Language Models,Variable Autonomy,Human-Robot Teaming,Generative pre-trained transformer,GPT,Unity Virtual Reality,OpenAI,Natural Language,Multi-robot environment,User study"
"CERN for AGI: A Theoretical Framework for Autonomous Simulation-Based
  Artificial Intelligence Testing and Alignment","This paper explores the potential of a multidisciplinary approach to testing
and aligning artificial general intelligence (AGI) and LLMs. Due to the rapid
development and wide application of LLMs, challenges such as ethical alignment,
controllability, and predictability of these models have become important
research topics. This study investigates an innovative simulation-based
multi-agent system within a virtual reality framework that replicates the
real-world environment. The framework is populated by automated 'digital
citizens,' simulating complex social structures and interactions to examine and
optimize AGI. Application of various theories from the fields of sociology,
social psychology, computer science, physics, biology, and economics
demonstrates the possibility of a more human-aligned and socially responsible
AGI. The purpose of such a digital environment is to provide a dynamic platform
where advanced AI agents can interact and make independent decisions, thereby
mimicking realistic scenarios. The actors in this digital city, operated by the
LLMs, serve as the primary agents, exhibiting high degrees of autonomy. While
this approach shows immense potential, there are notable challenges and
limitations, most significantly the unpredictable nature of real-world social
dynamics. This research endeavors to contribute to the development and
refinement of AGI, emphasizing the integration of social, ethical, and
theoretical dimensions for future research.","['Ljubisa Bojic', 'Matteo Cinelli', 'Dubravko Culibrk', 'Boris Delibasic']",2023-12-14T23:48:51Z,http://arxiv.org/abs/2312.09402v1,"['cs.CY', 'cs.AI', 'cs.GT']","Artificial general intelligence,LLMs,simulation-based,multi-agent system,virtual reality framework,digital citizens,social structures,social psychology,computer science,ethics"
"Attention-Based VR Facial Animation with Visual Mouth Camera Guidance
  for Immersive Telepresence Avatars","Facial animation in virtual reality environments is essential for
applications that necessitate clear visibility of the user's face and the
ability to convey emotional signals. In our scenario, we animate the face of an
operator who controls a robotic Avatar system. The use of facial animation is
particularly valuable when the perception of interacting with a specific
individual, rather than just a robot, is intended. Purely keypoint-driven
animation approaches struggle with the complexity of facial movements. We
present a hybrid method that uses both keypoints and direct visual guidance
from a mouth camera. Our method generalizes to unseen operators and requires
only a quick enrolment step with capture of two short videos. Multiple source
images are selected with the intention to cover different facial expressions.
Given a mouth camera frame from the HMD, we dynamically construct the target
keypoints and apply an attention mechanism to determine the importance of each
source image. To resolve keypoint ambiguities and animate a broader range of
mouth expressions, we propose to inject visual mouth camera information into
the latent space. We enable training on large-scale speaking head datasets by
simulating the mouth camera input with its perspective differences and facial
deformations. Our method outperforms a baseline in quality, capability, and
temporal consistency. In addition, we highlight how the facial animation
contributed to our victory at the ANA Avatar XPRIZE Finals.","['Andre Rochow', 'Max Schwarz', 'Sven Behnke']",2023-12-15T12:45:11Z,http://arxiv.org/abs/2312.09750v1,"['cs.CV', 'cs.RO']","virtual reality,facial animation,attention mechanism,mouth camera,immersive telepresence,avatar,keypoints,facial expressions,latent space,speaking head dataset"
"Measuring the Sense of Presence and Learning Efficacy in Immersive
  Virtual Assembly Training","With the rapid progress in virtual reality (VR) technology, the scope of VR
applications has greatly expanded across various domains. However, the
superiority of VR training over traditional methods and its impact on learning
efficacy are still uncertain. To investigate whether VR training is more
effective than traditional methods, we designed virtual training systems for
mechanical assembly on both VR and desktop platforms, subsequently conducting
pre-test and post-test experiments. A cohort of 53 students, all enrolled in
engineering drawing course without prior knowledge distinctions, was randomly
divided into three groups: physical training, desktop virtual training, and
immersive VR training. Our investigation utilized analysis of covariance
(ANCOVA) to examine the differences in post-test scores among the three groups
while controlling for pre-test scores. The group that received VR training
showed the highest scores on the post-test. Another facet of our study delved
into the presence of the virtual system. We developed a specialized scale to
assess this aspect for our research objectives. Our findings indicate that VR
training can enhance the sense of presence, particularly in terms of sensory
factors and realism factors. Moreover, correlation analysis uncovers
connections between the various dimensions of presence. This study confirms
that using VR training can improve learning efficacy and the presence in the
context of mechanical assembly, surpassing traditional training methods.
Furthermore, it provides empirical evidence supporting the integration of VR
technology in higher education and engineering training. This serves as a
reference for the practical application of VR technology in different fields.","['Weichao Lin', 'Liang Chen', 'Wei Xiong', 'Kang Ran', 'Anlan Fan']",2023-12-16T08:54:04Z,http://arxiv.org/abs/2312.10387v1,['cs.HC'],"virtual reality,VR technology,immersive training,learning efficacy,presence,mechanical assembly,traditional training methods,engineering training,empirical evidence"
"A Computationally Efficient Neural Video Compression Accelerator Based
  on a Sparse CNN-Transformer Hybrid Network","Video compression is widely used in digital television, surveillance systems,
and virtual reality. Real-time video decoding is crucial in practical
scenarios. Recently, neural video compression (NVC) combines traditional coding
with deep learning, achieving impressive compression efficiency. Nevertheless,
the NVC models involve high computational costs and complex memory access
patterns, challenging real-time hardware implementations. To relieve this
burden, we propose an algorithm and hardware co-design framework named NVCA for
video decoding on resource-limited devices. Firstly, a CNN-Transformer hybrid
network is developed to improve compression performance by capturing
multi-scale non-local features. In addition, we propose a fast algorithm-based
sparse strategy that leverages the dual advantages of pruning and fast
algorithms, sufficiently reducing computational complexity while maintaining
video compression efficiency. Secondly, a reconfigurable sparse computing core
is designed to flexibly support sparse convolutions and deconvolutions based on
the fast algorithm-based sparse strategy. Furthermore, a novel heterogeneous
layer chaining dataflow is incorporated to reduce off-chip memory traffic
stemming from extensive inter-frame motion and residual information. Thirdly,
the overall architecture of NVCA is designed and synthesized in TSMC 28nm CMOS
technology. Extensive experiments demonstrate that our design provides superior
coding quality and up to 22.7x decoding speed improvements over other video
compression designs. Meanwhile, our design achieves up to 2.2x improvements in
energy efficiency compared to prior accelerators.","['Siyu Zhang', 'Wendong Mao', 'Huihong Shi', 'Zhongfeng Wang']",2023-12-17T13:31:23Z,http://arxiv.org/abs/2312.10716v2,"['eess.IV', 'cs.AR']","neural video compression,CNN-Transformer hybrid network,sparse strategy,computational complexity,sparse computing core,reconfigurable,heterogeneous layer chaining,TSMC 28nm CMOS technology,decoding speed improvements,energy efficiency"
"AE-NeRF: Audio Enhanced Neural Radiance Field for Few Shot Talking Head
  Synthesis","Audio-driven talking head synthesis is a promising topic with wide
applications in digital human, film making and virtual reality. Recent
NeRF-based approaches have shown superiority in quality and fidelity compared
to previous studies. However, when it comes to few-shot talking head
generation, a practical scenario where only few seconds of talking video is
available for one identity, two limitations emerge: 1) they either have no base
model, which serves as a facial prior for fast convergence, or ignore the
importance of audio when building the prior; 2) most of them overlook the
degree of correlation between different face regions and audio, e.g., mouth is
audio related, while ear is audio independent. In this paper, we present Audio
Enhanced Neural Radiance Field (AE-NeRF) to tackle the above issues, which can
generate realistic portraits of a new speaker with fewshot dataset.
Specifically, we introduce an Audio Aware Aggregation module into the feature
fusion stage of the reference scheme, where the weight is determined by the
similarity of audio between reference and target image. Then, an Audio-Aligned
Face Generation strategy is proposed to model the audio related and audio
independent regions respectively, with a dual-NeRF framework. Extensive
experiments have shown AE-NeRF surpasses the state-of-the-art on image
fidelity, audio-lip synchronization, and generalization ability, even in
limited training set or training iterations.","['Dongze Li', 'Kang Zhao', 'Wei Wang', 'Bo Peng', 'Yingya Zhang', 'Jing Dong', 'Tieniu Tan']",2023-12-18T04:14:38Z,http://arxiv.org/abs/2312.10921v1,"['cs.CV', 'cs.SD', 'eess.AS']","Neural Radiance Field,Few-shot,Talking Head Synthesis,Audio Enhancemnt,Audio-Aware Aggregation,Audio-Aligned Face Generation,Image Fidelity,Audio-lip Synchronization,Generalization Ability"
"Pose2Gaze: Generating Realistic Human Gaze Behaviour from Full-body
  Poses using an Eye-body Coordination Model","While generating realistic body movements, e.g., for avatars in virtual
reality, is widely studied in computer vision and graphics, the generation of
eye movements that exhibit realistic coordination with the body remains
under-explored. We first report a comprehensive analysis of the coordination of
human eye and full-body movements during everyday activities based on data from
the MoGaze and GIMO datasets. We show that eye gaze has strong correlations
with head directions and also full-body motions and there exists a noticeable
time delay between body and eye movements. Inspired by the analyses, we then
present Pose2Gaze -- a novel eye-body coordination model that first uses a
convolutional neural network and a spatio-temporal graph convolutional neural
network to extract features from head directions and full-body poses
respectively and then applies a convolutional neural network to generate
realistic eye movements. We compare our method with state-of-the-art methods
that predict eye gaze only from head movements for three different generation
tasks and demonstrate that Pose2Gaze significantly outperforms these baselines
on both datasets with an average improvement of 26.4% and 21.6% in mean angular
error, respectively. Our findings underline the significant potential of
cross-modal human gaze behaviour analysis and modelling.","['Zhiming Hu', 'Jiahui Xu', 'Syn Schmitt', 'Andreas Bulling']",2023-12-19T10:55:46Z,http://arxiv.org/abs/2312.12042v1,['cs.CV'],"computer vision,graphics,eye movements,body movements,coordination,dataset,convolutional neural network,spatio-temporal graph convolutional neural network,human gaze behavior"
"GazeMoDiff: Gaze-guided Diffusion Model for Stochastic Human Motion
  Prediction","Human motion prediction is important for virtual reality (VR) applications,
e.g., for realistic avatar animation. Existing methods have synthesised body
motion only from observed past motion, despite the fact that human gaze is
known to correlate strongly with body movements and is readily available in
recent VR headsets. We present GazeMoDiff -- a novel gaze-guided denoising
diffusion model to generate stochastic human motions. Our method first uses a
graph attention network to learn the spatio-temporal correlations between eye
gaze and human movements and to fuse them into cross-modal gaze-motion
features. These cross-modal features are injected into a noise prediction
network via a cross-attention mechanism and progressively denoised to generate
realistic human full-body motions. Experimental results on the MoGaze and GIMO
datasets demonstrate that our method outperforms the state-of-the-art methods
by a large margin in terms of average displacement error (15.03% on MoGaze and
9.20% on GIMO). We further conducted an online user study to compare our method
with state-of-the-art methods and the responses from 23 participants validate
that the motions generated by our method are more realistic than those from
other methods. Taken together, our work makes a first important step towards
gaze-guided stochastic human motion prediction and guides future work on this
important topic in VR research.","['Haodong Yan', 'Zhiming Hu', 'Syn Schmitt', 'Andreas Bulling']",2023-12-19T12:10:12Z,http://arxiv.org/abs/2312.12090v1,['cs.CV'],"Stochastic human motion prediction,Virtual reality,Diffusion model,Gaze,Graph attention network,Cross-modal features,Noise prediction network,Average displacement error,MoGaze dataset,GIMO dataset"
"Exploring the current applications and potential of extended reality for
  environmental sustainability in manufacturing","In response to the transformation towards Industry 5.0, there is a growing
call for manufacturing systems that prioritize environmental sustainability,
alongside the emerging application of digital tools. Extended Reality (XR) -
including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) -
is one of the technologies identified as an enabler for Industry 5.0. XR could
potentially also be a driver for more sustainable manufacturing: however, its
potential environmental benefits have received limited attention. This paper
aims to explore the current manufacturing applications and research within the
field of XR technology connected to the environmental sustainability principle.
The objectives of this paper are two-fold: (1) Identify the currently explored
use cases of XR technology in literature and research, addressing environmental
sustainability in manufacturing; (2) Provide guidance and references for
industry and companies to use cases, toolboxes, methodologies, and workflows
for implementing XR in environmental sustainable manufacturing practices. Based
on the categorization of sustainability indicators, developed by the National
Institute of Standards and Technology (NIST), the authors analyzed and mapped
the current literature, with criteria of pragmatic XR use cases for
manufacturing. The exploration resulted in a mapping of the current
applications and use cases of XR technology within manufacturing that has the
potential to drive environmental sustainability. The results are presented as
stated use-cases with reference to the literature, contributing as guidance and
inspiration for future researchers or implementations in industry, using XR as
a driver for environmental sustainability. Furthermore, the authors open up the
discussion for future work and research to increase the attention of XR as a
driver for environmental sustainability.","['Huizhong Cao', 'Henrik Söderlund', 'Mélanie Derspeisse', 'Björn Johansson']",2023-12-29T13:18:01Z,http://arxiv.org/abs/2312.17595v1,"['cs.CY', 'cs.HC']","extended reality,environmental sustainability,manufacturing,virtual reality,augmented reality,mixed reality,Industry 5.0,use cases,sustainability indicators,National Institute of Standards and Technology"
"A Novel Approach for Defect Detection of Wind Turbine Blade Using
  Virtual Reality and Deep Learning","Wind turbines are subjected to continuous rotational stresses and unusual
external forces such as storms, lightning, strikes by flying objects, etc.,
which may cause defects in turbine blades. Hence, it requires a periodical
inspection to ensure proper functionality and avoid catastrophic failure. The
task of inspection is challenging due to the remote location and inconvenient
reachability by human inspection. Researchers used images with cropped defects
from the wind turbine in the literature. They neglected possible background
biases, which may hinder real-time and autonomous defect detection using aerial
vehicles such as drones or others. To overcome such challenges, in this paper,
we experiment with defect detection accuracy by having the defects with the
background using a two-step deep-learning methodology. In the first step, we
develop virtual models of wind turbines to synthesize the near-reality images
for four types of common defects - cracks, leading edge erosion, bending, and
light striking damage. The Unity perception package is used to generate wind
turbine blade defects images with variations in background, randomness, camera
angle, and light effects. In the second step, a customized U-Net architecture
is trained to classify and segment the defect in turbine blades. The outcomes
of U-Net architecture have been thoroughly tested and compared with 5-fold
validation datasets. The proposed methodology provides reasonable defect
detection accuracy, making it suitable for autonomous and remote inspection
through aerial vehicles.","['Md Fazle Rabbi', 'Solayman Hossain Emon', 'Ehtesham Mahmud Nishat', 'Tzu-Liang', 'Tseng', 'Atira Ferdoushi', 'Chun-Che Huang', 'Md Fashiar Rahman']",2023-12-30T13:58:50Z,http://arxiv.org/abs/2401.00237v1,"['cs.CV', 'eess.IV']","defect detection,wind turbine blade,virtual reality,deep learning,aerial vehicles,U-Net architecture,autonomous inspection,remote inspection,cracks,leading edge erosion"
Federated Multi-View Synthesizing for Metaverse,"The metaverse is expected to provide immersive entertainment, education, and
business applications. However, virtual reality (VR) transmission over wireless
networks is data- and computation-intensive, making it critical to introduce
novel solutions that meet stringent quality-of-service requirements. With
recent advances in edge intelligence and deep learning, we have developed a
novel multi-view synthesizing framework that can efficiently provide
computation, storage, and communication resources for wireless content delivery
in the metaverse. We propose a three-dimensional (3D)-aware generative model
that uses collections of single-view images. These single-view images are
transmitted to a group of users with overlapping fields of view, which avoids
massive content transmission compared to transmitting tiles or whole 3D models.
We then present a federated learning approach to guarantee an efficient
learning process. The training performance can be improved by characterizing
the vertical and horizontal data samples with a large latent feature space,
while low-latency communication can be achieved with a reduced number of
transmitted parameters during federated learning. We also propose a federated
transfer learning framework to enable fast domain adaptation to different
target domains. Simulation results have demonstrated the effectiveness of our
proposed federated multi-view synthesizing framework for VR content delivery.","['Yiyu Guo', 'Zhijin Qin', 'Xiaoming Tao', 'Geoffrey Ye Li']",2023-12-18T13:51:56Z,http://arxiv.org/abs/2401.00859v1,"['eess.IV', 'cs.CV', 'cs.LG']","metaverse,virtual reality,wireless networks,edge intelligence,deep learning,multi-view synthesizing,federated learning,generative model,federated transfer learning,VR content delivery"
Teaching with a companion: the case of gravity,"Virtual Reality (VR) has repeatedly proven its effectiveness in student
learning. However, despite its benefits, the student equipped with a personal
headset remains isolated from the real world while immersed in a virtual space
and the classic student-teacher model of learning is difficult to transpose in
such a situation. This study aims to bring the teacher back into the learning
process when students use a VR headset. We describe the benefits of using a
companion for educational purposes, taking as a test case the concept of
gravity. We present an experimental setup designed to compare three different
teaching contexts: with a physically present real teacher, using a live video
of the teacher, and with a VR avatar of the teacher. We designed and evaluated
three scenarios to teach the concept of gravity: an introduction to the concept
of free fall, a parabolic trajectory workshop and a final exercise combining
both approaches. Due to sanitary conditions, only pre-tests are reported. The
results showed that the effectiveness of using the VR simulations for learning
and the self-confidence level of the students increased as well. The interviews
show that the students ranked the teaching modes in this order: VR companion
mode, video communication and real teacher.","['Iuliia Zhurakovskaia', 'Jeanne Vezien', 'Patrick Bourdot']",2024-01-03T16:49:30Z,http://arxiv.org/abs/2401.01832v1,"['cs.GR', 'cs.HC']","Virtual Reality,companion,gravity,educational purposes,experimental setup,teaching contexts,VR avatar,concept of free fall,parabolic trajectory workshop"
"How Do Pedestrians' Perception Change toward Autonomous Vehicles during
  Unmarked Midblock Multilane Crossings: Role of AV Operation and Signal
  Indication","One of the primary impediments hindering the widespread acceptance of
autonomous vehicles (AVs) among pedestrians is their limited comprehension of
AVs. This study employs virtual reality (VR) to provide pedestrians with an
immersive environment for engaging with and comprehending AVs during unmarked
midblock multilane crossings. Diverse AV driving behaviors were modeled to
exhibit negotiation behavior with a yellow signal indication or non-yielding
behavior with a blue signal indication. This paper aims to investigate the
impact of various factors, such as AV behavior and signaling, pedestrian past
behavior, etc., on pedestrians' perception change of AVs. Before and after the
VR experiment, participants completed surveys assessing their perception of
AVs, focusing on two main aspects: ""Attitude"" and ""System Effectiveness."" The
Wilcoxon signed-rank test results demonstrated that both pedestrians' overall
attitude score toward AVs and trust in the effectiveness of AV systems
significantly increased following the VR experiment. Notably, individuals who
exhibited a greater trust in the yellow signals were more inclined to display a
higher attitude score toward AVs and to augment their trust in the
effectiveness of AV systems. This indicates that the design of the yellow
signal instills pedestrians with greater confidence in their interactions with
AVs. Further, pedestrians who exhibit more aggressive crossing behavior are
less likely to change their perception towards AVs as compared to those
pedestrians with more positive crossing behaviors. It is concluded that
integrating this paper's devised AV behavior and signaling within an immersive
VR setting facilitated pedestrian engagement with AVs, thereby changing their
perception of AVs.","['Fengjiao Zou', 'Jennifer Harper Ogle', 'Patrick Gerard', 'Weimin Jin']",2024-01-04T16:26:21Z,http://arxiv.org/abs/2401.02339v1,['cs.RO'],"autonomous vehicles,perception,unmarked crossings,virtual reality,AV behavior,signal indication,pedestrian behavior,trust,immersive environment,VR experiment"
A Survey on 3D Gaussian Splatting,"3D Gaussian splatting (GS) has recently emerged as a transformative technique
in the realm of explicit radiance field and computer graphics. This innovative
approach, characterized by the utilization of millions of learnable 3D
Gaussians, represents a significant departure from mainstream neural radiance
field approaches, which predominantly use implicit, coordinate-based models to
map spatial coordinates to pixel values. 3D GS, with its explicit scene
representation and differentiable rendering algorithm, not only promises
real-time rendering capability but also introduces unprecedented levels of
editability. This positions 3D GS as a potential game-changer for the next
generation of 3D reconstruction and representation. In the present paper, we
provide the first systematic overview of the recent developments and critical
contributions in the domain of 3D GS. We begin with a detailed exploration of
the underlying principles and the driving forces behind the emergence of 3D GS,
laying the groundwork for understanding its significance. A focal point of our
discussion is the practical applicability of 3D GS. By enabling unprecedented
rendering speed, 3D GS opens up a plethora of applications, ranging from
virtual reality to interactive media and beyond. This is complemented by a
comparative analysis of leading 3D GS models, evaluated across various
benchmark tasks to highlight their performance and practical utility. The
survey concludes by identifying current challenges and suggesting potential
avenues for future research in this domain. Through this survey, we aim to
provide a valuable resource for both newcomers and seasoned researchers,
fostering further exploration and advancement in applicable and explicit
radiance field representation.","['Guikun Chen', 'Wenguan Wang']",2024-01-08T13:42:59Z,http://arxiv.org/abs/2401.03890v2,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.MM']","3D Gaussian splatting,Radiance field,Computer graphics,Neural radiance field,Differentiable rendering algorithm,Real-time rendering,Editability,3D reconstruction,Rendering speed,Benchmark tasks"
Enabling Technologies for Web 3.0: A Comprehensive Survey,"Web 3.0 represents the next stage of Internet evolution, aiming to empower
users with increased autonomy, efficiency, quality, security, and privacy. This
evolution can potentially democratize content access by utilizing the latest
developments in enabling technologies. In this paper, we conduct an in-depth
survey of enabling technologies in the context of Web 3.0, such as blockchain,
semantic web, 3D interactive web, Metaverse, Virtual reality/Augmented reality,
Internet of Things technology, and their roles in shaping Web 3.0. We commence
by providing a comprehensive background of Web 3.0, including its concept,
basic architecture, potential applications, and industry adoption.
Subsequently, we examine recent breakthroughs in IoT, 5G, and blockchain
technologies that are pivotal to Web 3.0 development. Following that, other
enabling technologies, including AI, semantic web, and 3D interactive web, are
discussed. Utilizing these technologies can effectively address the critical
challenges in realizing Web 3.0, such as ensuring decentralized identity,
platform interoperability, data transparency, reducing latency, and enhancing
the system's scalability. Finally, we highlight significant challenges
associated with Web 3.0 implementation, emphasizing potential solutions and
providing insights into future research directions in this field.","['Md Arif Hassan', 'Mohammad Behdad Jamshidi', 'Bui Duc Manh', 'Nam H. Chu', 'Chi-Hieu Nguyen', 'Nguyen Quang Hieu', 'Cong T. Nguyen', 'Dinh Thai Hoang', 'Diep N. Nguyen', 'Nguyen Van Huynh', 'Mohammad Abu Alsheikh', 'Eryk Dutkiewicz']",2023-12-29T10:22:18Z,http://arxiv.org/abs/2401.10901v1,['cs.CY'],"Web 3.0,Enabling technologies,Blockchain,Semantic web,3D interactive web,Metaverse,Virtual reality,Augmented reality,Internet of Things,AI"
"A VR Serious Game to Increase Empathy towards Students with Phonological
  Dyslexia","Dyslexia is a neurodevelopmental disorder that is estimated to affect about
5-10% of the population. In particular, phonological dyslexia causes problems
in connecting the sounds of words with their written forms. This results in
difficulties such as slow reading speed, inaccurate reading, and difficulty
decoding unfamiliar words. Moreover, dyslexia can also be a challenging and
frustrating experience for students as they may feel misunderstood or
stigmatized by their peers or educators. For these reasons, the use of
compensatory tools and strategies is of crucial importance for dyslexic
students to have the same opportunities as non-dyslexic ones. However,
generally, people underestimate the problem and are not aware of the importance
of support methodologies. In the light of this, the main purpose of this paper
is to propose a virtual reality (VR) serious game through which teachers,
students and, in general, non-dyslexic people could understand which are some
of the issues of student with dyslexia and the fundamental utility of offering
support to them. In the game, players must create a potion by following a
recipe written in an alphabet that is specifically designed to replicate the
reading difficulties experienced by individuals with dyslexia. The task must be
solved first without any help and then by receiving supporting tools and
strategies with the idea that the player can put himself in the place of the
dyslexic person and understand the real need for support methodologies.","['José M. Alcalde-Llergo', 'Enrique Yeguas-Bolívar', 'Pilar Aparicio-Martínez', 'Andrea Zingoni', 'Juri Taborri', 'Sara Pinzi']",2024-01-15T23:47:23Z,http://arxiv.org/abs/2401.10926v1,"['cs.HC', 'cs.CV', 'cs.GR']","VR,Serious Game,Empathy,Phonological Dyslexia,Neurodevelopmental Disorder,Compensatory Tools,Support Methodologies,Virtual Reality,Dyslexic Students"
Fast Registration of Photorealistic Avatars for VR Facial Animation,"Virtual Reality (VR) bares promise of social interactions that can feel more
immersive than other media. Key to this is the ability to accurately animate a
photorealistic avatar of one's likeness while wearing a VR headset. Although
high quality registration of person-specific avatars to headset-mounted camera
(HMC) images is possible in an offline setting, the performance of generic
realtime models are significantly degraded. Online registration is also
challenging due to oblique camera views and differences in modality. In this
work, we first show that the domain gap between the avatar and headset-camera
images is one of the primary sources of difficulty, where a transformer-based
architecture achieves high accuracy on domain-consistent data, but degrades
when the domain-gap is re-introduced. Building on this finding, we develop a
system design that decouples the problem into two parts: 1) an iterative
refinement module that takes in-domain inputs, and 2) a generic avatar-guided
image-to-image style transfer module that is conditioned on current estimation
of expression and head pose. These two modules reinforce each other, as image
style transfer becomes easier when close-to-ground-truth examples are shown,
and better domain-gap removal helps registration. Our system produces
high-quality results efficiently, obviating the need for costly offline
registration to generate personalized labels. We validate the accuracy and
efficiency of our approach through extensive experiments on a commodity
headset, demonstrating significant improvements over direct regression methods
as well as offline registration.","['Chaitanya Patel', 'Shaojie Bai', 'Te-Li Wang', 'Jason Saragih', 'Shih-En Wei']",2024-01-19T19:42:38Z,http://arxiv.org/abs/2401.11002v1,"['cs.CV', 'cs.AI']","photorealistic avatars,VR,facial animation,registration,headset-mounted camera,realtime models,transformer-based architecture,domain gap,image-to-image style transfer,expression"
"Full-Body Motion Reconstruction with Sparse Sensing from Graph
  Perspective","Estimating 3D full-body pose from sparse sensor data is a pivotal technique
employed for the reconstruction of realistic human motions in Augmented Reality
and Virtual Reality. However, translating sparse sensor signals into
comprehensive human motion remains a challenge since the sparsely distributed
sensors in common VR systems fail to capture the motion of full human body. In
this paper, we use well-designed Body Pose Graph (BPG) to represent the human
body and translate the challenge into a prediction problem of graph missing
nodes. Then, we propose a novel full-body motion reconstruction framework based
on BPG. To establish BPG, nodes are initially endowed with features extracted
from sparse sensor signals. Features from identifiable joint nodes across
diverse sensors are amalgamated and processed from both temporal and spatial
perspectives. Temporal dynamics are captured using the Temporal Pyramid
Structure, while spatial relations in joint movements inform the spatial
attributes. The resultant features serve as the foundational elements of the
BPG nodes. To further refine the BPG, node features are updated through a graph
neural network that incorporates edge reflecting varying joint relations. Our
method's effectiveness is evidenced by the attained state-of-the-art
performance, particularly in lower body motion, outperforming other baseline
methods. Additionally, an ablation study validates the efficacy of each module
in our proposed framework.","['Feiyu Yao', 'Zongkai Wu', 'Li Yi']",2024-01-22T09:29:42Z,http://arxiv.org/abs/2401.11783v1,['cs.CV'],"full-body motion reconstruction,sparse sensing,graph perspective,body pose graph,prediction problem,temporal dynamics,spatial relations,graph neural network,lower body motion,ablation study"
"VRMN-bD: A Multi-modal Natural Behavior Dataset of Immersive Human Fear
  Responses in VR Stand-up Interactive Games","Understanding and recognizing emotions are important and challenging issues
in the metaverse era. Understanding, identifying, and predicting fear, which is
one of the fundamental human emotions, in virtual reality (VR) environments
plays an essential role in immersive game development, scene development, and
next-generation virtual human-computer interaction applications. In this
article, we used VR horror games as a medium to analyze fear emotions by
collecting multi-modal data (posture, audio, and physiological signals) from 23
players. We used an LSTM-based model to predict fear with accuracies of 65.31%
and 90.47% under 6-level classification (no fear and five different levels of
fear) and 2-level classification (no fear and fear), respectively. We
constructed a multi-modal natural behavior dataset of immersive human fear
responses (VRMN-bD) and compared it with existing relevant advanced datasets.
The results show that our dataset has fewer limitations in terms of collection
method, data scale and audience scope. We are unique and advanced in targeting
multi-modal datasets of fear and behavior in VR stand-up interactive
environments. Moreover, we discussed the implications of this work for
communities and applications. The dataset and pre-trained model are available
at https://github.com/KindOPSTAR/VRMN-bD.","['He Zhang', 'Xinyang Li', 'Yuanxi Sun', 'Xinyi Fu', 'Christine Qiu', 'John M. Carroll']",2024-01-22T17:15:02Z,http://arxiv.org/abs/2401.12133v1,"['cs.HC', 'cs.CV', 'cs.LG']","multi-modal data,fear,virtual reality,LSTM-based model,immersive games,human-computer interaction,behavior dataset,interactive environments,physiological signals,emotion recognition"
Style-Consistent 3D Indoor Scene Synthesis with Decoupled Objects,"Controllable 3D indoor scene synthesis stands at the forefront of
technological progress, offering various applications like gaming, film, and
augmented/virtual reality. The capability to stylize and de-couple objects
within these scenarios is a crucial factor, providing an advanced level of
control throughout the editing process. This control extends not just to
manipulating geometric attributes like translation and scaling but also
includes managing appearances, such as stylization. Current methods for scene
stylization are limited to applying styles to the entire scene, without the
ability to separate and customize individual objects. Addressing the
intricacies of this challenge, we introduce a unique pipeline designed for
synthesis 3D indoor scenes. Our approach involves strategically placing objects
within the scene, utilizing information from professionally designed bounding
boxes. Significantly, our pipeline prioritizes maintaining style consistency
across multiple objects within the scene, ensuring a cohesive and visually
appealing result aligned with the desired aesthetic. The core strength of our
pipeline lies in its ability to generate 3D scenes that are not only visually
impressive but also exhibit features like photorealism, multi-view consistency,
and diversity. These scenes are crafted in response to various natural language
prompts, demonstrating the versatility and adaptability of our model.","['Yunfan Zhang', 'Hong Huang', 'Zhiwei Xiong', 'Zhiqi Shen', 'Guosheng Lin', 'Hao Wang', 'Nicholas Vun']",2024-01-24T03:10:36Z,http://arxiv.org/abs/2401.13203v1,['cs.CV'],"3D indoor scene synthesis,Style-consistent,Decoupled objects,Geometric attributes,Stylization,Pipeline,Bounding boxes,Style consistency,Photorealism,Multi-view consistency"
"How does Simulation-based Testing for Self-driving Cars match Human
  Perception?","Software metrics such as coverage and mutation scores have been extensively
explored for the automated quality assessment of test suites. While traditional
tools rely on such quantifiable software metrics, the field of self-driving
cars (SDCs) has primarily focused on simulation-based test case generation
using quality metrics such as the out-of-bound (OOB) parameter to determine if
a test case fails or passes. However, it remains unclear to what extent this
quality metric aligns with the human perception of the safety and realism of
SDCs, which are critical aspects in assessing SDC behavior. To address this
gap, we conducted an empirical study involving 50 participants to investigate
the factors that determine how humans perceive SDC test cases as safe, unsafe,
realistic, or unrealistic. To this aim, we developed a framework leveraging
virtual reality (VR) technologies, called SDC-Alabaster, to immerse the study
participants into the virtual environment of SDC simulators. Our findings
indicate that the human assessment of the safety and realism of failing and
passing test cases can vary based on different factors, such as the test's
complexity and the possibility of interacting with the SDC. Especially for the
assessment of realism, the participants' age as a confounding factor leads to a
different perception. This study highlights the need for more research on SDC
simulation testing quality metrics and the importance of human perception in
evaluating SDC behavior.","['Christian Birchler', 'Tanzil Kombarabettu Mohammed', 'Pooja Rani', 'Teodora Nechita', 'Timo Kehrer', 'Sebastiano Panichella']",2024-01-26T09:58:12Z,http://arxiv.org/abs/2401.14736v1,['cs.SE'],"Simulation-based testing,Self-driving cars,Human perception,Quality metrics,Test cases,Software metrics,Virtual reality,Empirical study,Safety,Realism"
"LIV-GaussMap: LiDAR-Inertial-Visual Fusion for Real-time 3D Radiance
  Field Map Rendering","We introduce an integrated precise LiDAR, Inertial, and Visual (LIV)
multimodal sensor fused mapping system that builds on the differentiable
\pre{surface splatting }\now{Gaussians} to improve the mapping fidelity,
quality, and structural accuracy. Notably, this is also a novel form of tightly
coupled map for LiDAR-visual-inertial sensor fusion.
  This system leverages the complementary characteristics of LiDAR and visual
data to capture the geometric structures of large-scale 3D scenes and restore
their visual surface information with high fidelity. The initialization for the
scene's surface Gaussians and the sensor's poses of each frame are obtained
using a LiDAR-inertial system with the feature of size-adaptive voxels. Then,
we optimized and refined the Gaussians using visual-derived photometric
gradients to optimize their quality and density.
  Our method is compatible with various types of LiDAR, including solid-state
and mechanical LiDAR, supporting both repetitive and non-repetitive scanning
modes. Bolstering structure construction through LiDAR and facilitating
real-time generation of photorealistic renderings across diverse LIV datasets.
It showcases notable resilience and versatility in generating real-time
photorealistic scenes potentially for digital twins and virtual reality, while
also holding potential applicability in real-time SLAM and robotics domains.
  We release our software and hardware and self-collected datasets to benefit
the community.","['Sheng Hong', 'Junjie He', 'Xinhu Zheng', 'Chunran Zheng', 'Shaojie Shen']",2024-01-26T13:36:46Z,http://arxiv.org/abs/2401.14857v2,['cs.RO'],"LiDAR,Inertial,Visual fusion,3D rendering,Gaussians,Mapping system,Photorealistic rendering,SLAM,Robotics,Digital twins"
On the Emergence of Symmetrical Reality,"Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.","['Zhenliang Zhang', 'Zeyu Zhang', 'Ziyuan Jiao', 'Yao Su', 'Hangxin Liu', 'Wei Wang', 'Song-Chun Zhu']",2024-01-26T16:09:39Z,http://arxiv.org/abs/2401.15132v1,"['cs.HC', 'cs.AI']","artificial intelligence,symmetrical reality,virtual reality,mixed reality,augmented reality,AI agents,physical-virtual amalgamations,human-AI coexistence,cognitive abilities,perception"
Saccade-Contingent Rendering,"Battery-constrained power consumption, compute limitations, and high frame
rate requirements in head-mounted displays present unique challenges in the
drive to present increasingly immersive and comfortable imagery in virtual
reality. However, humans are not equally sensitive to all regions of the visual
field, and perceptually-optimized rendering techniques are increasingly
utilized to address these bottlenecks. Many of these techniques are
gaze-contingent and often render reduced detail away from a user's fixation.
Such techniques are dependent on spatio-temporally-accurate gaze tracking and
can result in obvious visual artifacts when eye tracking is inaccurate. In this
work we present a gaze-contingent rendering technique which only requires
saccade detection, bypassing the need for highly-accurate eye tracking. In our
first experiment, we show that visual acuity is reduced for several hundred
milliseconds after a saccade. In our second experiment, we use these results to
reduce the rendered image resolution after saccades in a controlled
psychophysical setup, and find that observers cannot discriminate between
saccade-contingent reduced-resolution rendering and full-resolution rendering.
Finally, in our third experiment, we introduce a 90 pixels per degree headset
and validate our saccade-contingent rendering method under typical VR viewing
conditions.","['Yuna Kwak', 'Eric Penner', 'Xuan Wang', 'Mohammad R. Saeedpour-Parizi', 'Olivier Mercier', 'Xiuyun Wu', 'T. Scott Murdison', 'Phillip Guan']",2024-01-29T20:17:51Z,http://arxiv.org/abs/2401.16536v1,"['cs.GR', 'cs.HC']","saccade-contingent,rendering,virtual reality,gaze tracking,visual acuity,eye tracking,high frame rate,gaze-contingent,immersive imagery"
"VR-based generation of photorealistic synthetic data for training
  hand-object tracking models","Supervised learning models for precise tracking of hand-object interactions
(HOI) in 3D require large amounts of annotated data for training. Moreover, it
is not intuitive for non-experts to label 3D ground truth (e.g. 6DoF object
pose) on 2D images. To address these issues, we present ""blender-hoisynth"", an
interactive synthetic data generator based on the Blender software.
Blender-hoisynth can scalably generate and automatically annotate visual HOI
training data. Other competing approaches usually generate synthetic HOI data
compeletely without human input. While this may be beneficial in some
scenarios, HOI applications inherently necessitate direct control over the HOIs
as an expression of human intent. With blender-hoisynth, it is possible for
users to interact with objects via virtual hands using standard Virtual Reality
hardware. The synthetically generated data are characterized by a high degree
of photorealism and contain visually plausible and physically realistic videos
of hands grasping objects and moving them around in 3D. To demonstrate the
efficacy of our data generation, we replace large parts of the training data in
the well-known DexYCB dataset with hoisynth data and train a state-of-the-art
HOI reconstruction model with it. We show that there is no significant
degradation in the model performance despite the data replacement.","['Chengyan Zhang', 'Rahul Chaudhari']",2024-01-31T14:32:56Z,http://arxiv.org/abs/2401.17874v2,['cs.CV'],"virtual reality,synthetic data generation,hand-object tracking,3D interactions,Blender software,photorealism,annotated data,training models,human input"
"Reimagining TaxiVis through an Immersive Space-Time Cube metaphor and
  reflecting on potential benefits of Immersive Analytics for urban data
  exploration","Current visualization research has identified the potential of more immersive
settings for data exploration, leveraging VR and AR technologies. To explore
how a traditional visualization system could be adapted into an immersive
framework, and how it could benefit from this, we decided to revisit a landmark
paper presented ten years ago at IEEE VIS. TaxiVis, by Ferreira et al., enabled
interactive spatio-temporal querying of a large dataset of taxi trips in New
York City. Here, we reimagine how TaxiVis' functionalities could be implemented
and extended in a 3D immersive environment. Among the unique features we
identify as being enabled by the Immersive TaxiVis prototype are alternative
uses of the additional visual dimension, a fully visual 3D spatio-temporal
query framework, and the opportunity to explore the data at different scales
and frames of reference. By revisiting the case studies from the original
paper, we demonstrate workflows that can benefit from this immersive
perspective. Through reporting on our experience, and on the vision and
reasoning behind our design decisions, we hope to contribute to the debate on
how conventional and immersive visualization paradigms can complement each
other and on how the exploration of urban datasets can be facilitated in the
coming years.","['Jorge Wagner', 'Claudio T. Silva', 'Wolfgang Stuerzlinger', 'Luciana Nedel']",2024-02-01T05:19:15Z,http://arxiv.org/abs/2402.00344v2,"['cs.HC', 'H.5.1']","Immersive Space-Time Cube,Immersive Analytics,Visualization,VR,AR,Spatio-temporal querying,3D immersive environment,TaxiVis,Urban data"
"Human Emotions Analysis and Recognition Using EEG Signals in Response to
  360$^\circ$ Videos","Emotion recognition (ER) technology is an integral part for developing
innovative applications such as drowsiness detection and health monitoring that
plays a pivotal role in contemporary society. This study delves into ER using
electroencephalography (EEG), within immersive virtual reality (VR)
environments. There are four main stages in our proposed methodology including
data acquisition, pre-processing, feature extraction, and emotion
classification. Acknowledging the limitations of existing 2D datasets, we
introduce a groundbreaking 3D VR dataset to elevate the precision of emotion
elicitation. Leveraging the Interaxon Muse headband for EEG recording and
Oculus Quest 2 for VR stimuli, we meticulously recorded data from 40
participants, prioritizing subjects without reported mental illnesses.
Pre-processing entails rigorous cleaning, uniform truncation, and the
application of a Savitzky-Golay filter to the EEG data. Feature extraction
encompasses a comprehensive analysis of metrics such as power spectral density,
correlation, rational and divisional asymmetry, and power spectrum. To ensure
the robustness of our model, we employed a 10-fold cross-validation, revealing
an average validation accuracy of 85.54\%, with a noteworthy maximum accuracy
of 90.20\% in the best fold. Subsequently, the trained model demonstrated a
commendable test accuracy of 82.03\%, promising favorable outcomes.","['Haseeb ur Rahman Abbasi', 'Zeeshan Rashid', 'Muhammad Majid', 'Syed Muhammad Anwar']",2024-02-06T16:48:58Z,http://arxiv.org/abs/2402.04142v1,['cs.HC'],"ER technology,Electroencephalography (EEG),Virtual reality (VR),Data acquisition,Pre-processing,Feature extraction,Emotion classification,Power spectral density,Cross-validation,Validation accuracy"
"A Framework For Gait-Based User Demography Estimation Using Inertial
  Sensors","Human gait has been shown to provide crucial motion cues for various
applications. Recognizing patterns in human gait has been widely adopted in
various application areas such as security, virtual reality gaming, medical
rehabilitation, and ailment identification. Furthermore, wearable inertial
sensors have been widely used for not only recording gait but also to predict
users' demography. Machine Learning techniques such as deep learning, combined
with inertial sensor signals, have shown promising results in recognizing
patterns in human gait and estimate users' demography. However, the black-box
nature of such deep learning models hinders the researchers from uncovering the
reasons behind the model's predictions. Therefore, we propose leveraging deep
learning and Layer-Wise Relevance Propagation (LRP) to identify the important
variables that play a vital role in identifying the users' demography such as
age and gender. To assess the efficacy of this approach we train a deep neural
network model on a large sensor-based gait dataset consisting of 745 subjects
to identify users' age and gender. Using LRP we identify the variables relevant
for characterizing the gait patterns. Thus, we enable interpretation of
non-linear ML models which are experts in identifying the users' demography
based on inertial signals. We believe this approach can not only provide
clinicians information about the gait parameters relevant to age and gender but
also can be expanded to analyze and diagnose gait disorders.",['Chinmay Prakash Swami'],2024-02-15T07:23:34Z,http://arxiv.org/abs/2402.09761v1,"['cs.HC', 'cs.LG', 'eess.SP']","Gait,User Demography Estimation,Inertial Sensors,Machine Learning,Deep Learning,Layer-Wise Relevance Propagation (LRP),Sensor-Based Gait Dataset,Age,Gender,Gait Disorders"
"Analysis of Neural Video Compression Networks for 360-Degree Video
  Coding","With the increasing efforts of bringing high-quality virtual reality
technologies into the market, efficient 360-degree video compression gains in
importance. As such, the state-of-the-art H.266/VVC video coding standard
integrates dedicated tools for 360-degree video, and considerable efforts have
been put into designing 360-degree projection formats with improved compression
efficiency. For the fast-evolving field of neural video compression networks
(NVCs), the effects of different 360-degree projection formats on the overall
compression performance have not yet been investigated. It is thus unclear,
whether a resampling from the conventional equirectangular projection (ERP) to
other projection formats yields similar gains for NVCs as for hybrid video
codecs, and which formats perform best. In this paper, we analyze several
generations of NVCs and an extensive set of 360-degree projection formats with
respect to their compression performance for 360-degree video. Based on our
analysis, we find that projection format resampling yields significant
improvements in compression performance also for NVCs. The adjusted cubemap
projection (ACP) and equatorial cylindrical projection (ECP) show to perform
best and achieve rate savings of more than 55% compared to ERP based on WS-PSNR
for the most recent NVC. Remarkably, the observed rate savings are higher than
for H.266/VVC, emphasizing the importance of projection format resampling for
NVCs.","['Andy Regensky', 'Fabian Brand', 'André Kaup']",2024-02-15T17:15:54Z,http://arxiv.org/abs/2402.10257v1,['eess.IV'],"Neural Video Compression Networks,360-Degree Video,Video Coding,H.266/VVC,Compression Efficiency,Projection Formats,Equirectangular Projection,Cubemap Projection,Cylindrical Projection"
"Radar-Based Recognition of Static Hand Gestures in American Sign
  Language","In the fast-paced field of human-computer interaction (HCI) and virtual
reality (VR), automatic gesture recognition has become increasingly essential.
This is particularly true for the recognition of hand signs, providing an
intuitive way to effortlessly navigate and control VR and HCI applications.
Considering increased privacy requirements, radar sensors emerge as a
compelling alternative to cameras. They operate effectively in low-light
conditions without capturing identifiable human details, thanks to their lower
resolution and distinct wavelength compared to visible light.
  While previous works predominantly deploy radar sensors for dynamic hand
gesture recognition based on Doppler information, our approach prioritizes
classification using an imaging radar that operates on spatial information,
e.g. image-like data. However, generating large training datasets required for
neural networks (NN) is a time-consuming and challenging process, often falling
short of covering all potential scenarios. Acknowledging these challenges, this
study explores the efficacy of synthetic data generated by an advanced radar
ray-tracing simulator. This simulator employs an intuitive material model that
can be adjusted to introduce data diversity.
  Despite exclusively training the NN on synthetic data, it demonstrates
promising performance when put to the test with real measurement data. This
emphasizes the practicality of our methodology in overcoming data scarcity
challenges and advancing the field of automatic gesture recognition in VR and
HCI applications.","['Christian Schuessler', 'Wenxuan Zhang', 'Johanna Bräunig', 'Marcel Hoffmann', 'Michael Stelzig', 'Martin Vossiek']",2024-02-20T08:19:30Z,http://arxiv.org/abs/2402.12800v1,"['cs.CV', 'eess.SP']","radar-based,hand gestures,American Sign Language,gesture recognition,radar sensors,Doppler information,imaging radar,neural networks,synthetic data,ray-tracing simulator"
"Solving the decision-making differential equations from eye fixation
  data in Unity software by using Hermite Long-Short-Term Memory neural network","Cognitive decision-making processes are crucial aspects of human behavior,
influencing various personal and professional domains. This research delves
into the application of differential equations in analyzing decision-making
accuracy by leveraging eye-tracking data within a virtual industrial town
setting. The study unveils a systematic approach to transforming raw data into
a differential equation, essential for deciphering the relationship between eye
movements during decision-making processes.
  Mathematical relationship extraction and variable-parameter definition pave
the way for deriving a differential equation that encapsulates the growth of
fixations on characters. The key factors in this equation encompass the
fixation rate $(\lambda)$ and separation rate $(\mu)$, reflecting user
interaction dynamics and their impact on decision-making complexities tied to
user engagement with virtual characters.
  For a comprehensive grasp of decision dynamics, solving this differential
equation requires initial fixation counts, fixation rate, and separation rate.
The formulation of differential equations incorporates various considerations
such as engagement duration, character-player distance, relative speed, and
character attributes, enabling the representation of fixation changes, speed
dynamics, distance variations, and the effects of character attributes.
  This comprehensive analysis not only enhances our comprehension of
decision-making processes but also provides a foundational framework for
predictive modeling and data-driven insights for future research and
applications in cognitive science and virtual reality environments.","['Kourosh Parand', 'Saeed Setayeshi', 'Mir Mohsen Pedram', 'Ali Yoonesi', 'Aida Pakniyat']",2024-02-20T14:09:28Z,http://arxiv.org/abs/2402.13027v2,"['cs.HC', 'cs.NA', 'math.NA']","decision-making,eye fixation,Unity software,Hermite Long-Short-Term Memory neural network,differential equations,eye-tracking data,virtual reality,neural network,cognitive science,predictive modeling"
A Disruptive Research Playbook for Studying Disruptive Innovations,"As researchers, we are now witnessing a fundamental change in our
technologically-enabled world due to the advent and diffusion of highly
disruptive technologies such as generative AI, Augmented Reality (AR) and
Virtual Reality (VR). In particular, software engineering has been profoundly
affected by the transformative power of disruptive innovations for decades,
with a significant impact of technical advancements on social dynamics due to
its the socio-technical nature. In this paper, we reflect on the importance of
formulating and addressing research in software engineering through a
socio-technical lens, thus ensuring a holistic understanding of the complex
phenomena in this field. We propose a research playbook with the goal of
providing a guide to formulate compelling and socially relevant research
questions and to identify the appropriate research strategies for empirical
investigations, with an eye on the long-term implications of technologies or
their use. We showcase how to apply the research playbook. Firstly, we show how
it can be used retrospectively to reflect on a prior disruptive technology,
Stack Overflow, and its impact on software development. Secondly, we show it
can be used to question the impact of two current disruptive technologies: AI
and AR/VR. Finally, we introduce a specialized GPT model to support the
researcher in framing future investigations. We conclude by discussing the
broader implications of adopting the playbook for both researchers and
practitioners in software engineering and beyond.","['Margaret-Anne Storey', 'Daniel Russo', 'Nicole Novielli', 'Takashi Kobayashi', 'Dong Wang']",2024-02-20T19:13:36Z,http://arxiv.org/abs/2402.13329v1,['cs.SE'],"disruptive innovations,generative AI,Augmented Reality,Virtual Reality,software engineering,socio-technical,research playbook,empirical investigations,GPT model"
"Bring Your Own Character: A Holistic Solution for Automatic Facial
  Animation Generation of Customized Characters","Animating virtual characters has always been a fundamental research problem
in virtual reality (VR). Facial animations play a crucial role as they
effectively convey emotions and attitudes of virtual humans. However, creating
such facial animations can be challenging, as current methods often involve
utilization of expensive motion capture devices or significant investments of
time and effort from human animators in tuning animation parameters. In this
paper, we propose a holistic solution to automatically animate virtual human
faces. In our solution, a deep learning model was first trained to retarget the
facial expression from input face images to virtual human faces by estimating
the blendshape coefficients. This method offers the flexibility of generating
animations with characters of different appearances and blendshape topologies.
Second, a practical toolkit was developed using Unity 3D, making it compatible
with the most popular VR applications. The toolkit accepts both image and video
as input to animate the target virtual human faces and enables users to
manipulate the animation results. Furthermore, inspired by the spirit of
Human-in-the-loop (HITL), we leveraged user feedback to further improve the
performance of the model and toolkit, thereby increasing the customization
properties to suit user preferences. The whole solution, for which we will make
the code public, has the potential to accelerate the generation of facial
animations for use in VR applications.","['Zechen Bai', 'Peng Chen', 'Xiaolan Peng', 'Lu Liu', 'Hui Chen', 'Mike Zheng Shou', 'Feng Tian']",2024-02-21T11:35:20Z,http://arxiv.org/abs/2402.13724v1,"['cs.HC', 'cs.CV']","facial animation,virtual characters,deep learning,Unity 3D,blendshape coefficients,VR applications,human-in-the-loop,animation generation,customization,facial expression"
"Decoding Driver Takeover Behaviour in Conditional Automation with
  Immersive Virtual Reality","The safe transition from conditional automation to manual driving control is
significantly intertwined with the vehicle's lateral and longitudinal dynamics.
The transition may occur as a result of a system-initiated mandatory takeover
(MTOR) or as a driver-initiated discretionary takeover (DTOR). In either
condition, the takeover process entails differing cognitive demands and may
affect the driving behaviour differently. This study analyzes driving stability
and perceived mental workload in 304 takeover attempts recorded from 104
participants within virtual and immersive reality environments. Adopting an
exploratory approach, this dynamic simulator study employs a mixed factorial
design. Utilizing a deep neural network-based survival analysis with SHAP
interpretability, the study investigated the influence of covariates on
perception-reaction time (PRT), distinguishing between safe and unsafe control
transition and offering insights into the temporal dynamics of these shifts.
The distributions of key parameters in experimental groups were analyzed and
factors influencing the perceived mental workload were estimated using
multivariate linear regression. The findings indicate a notable decrease in the
risk of unsafe takeovers (described by a longer PRT) when drivers have prior
control-transition experience and familiarity with Automated Vehicles (AVs).
However, driver's prior familiarity and experience with AVs only decreased the
perceived mental workload associated with DTOR, with an insignificant impact on
the cognitive demand of MTOR. Furthermore, multitasking during automated
driving significantly elevated the cognitive demand linked to DTOR and led to
longer PRT in MTOR situations.","['Muhammad Sajjad Ansar', 'Bilal Farooq']",2024-02-25T10:00:21Z,http://arxiv.org/abs/2402.16046v1,['cs.HC'],"conditional automation,driver takeover,virtual reality,dynamic simulator study,cognitive demands,mental workload,perception-reaction time,automated vehicles,control transition,multitasking"
"ODVista: An Omnidirectional Video Dataset for super-resolution and
  Quality Enhancement Tasks","Omnidirectional or 360-degree video is being increasingly deployed, largely
due to the latest advancements in immersive virtual reality (VR) and extended
reality (XR) technology. However, the adoption of these videos in streaming
encounters challenges related to bandwidth and latency, particularly in
mobility conditions such as with unmanned aerial vehicles (UAVs). Adaptive
resolution and compression aim to preserve quality while maintaining low
latency under these constraints, yet downscaling and encoding can still degrade
quality and introduce artifacts. Machine learning (ML)-based super-resolution
(SR) and quality enhancement techniques offer a promising solution by enhancing
detail recovery and reducing compression artifacts. However, current publicly
available 360-degree video SR datasets lack compression artifacts, which limit
research in this field. To bridge this gap, this paper introduces
omnidirectional video streaming dataset (ODVista), which comprises 200
high-resolution and high quality videos downscaled and encoded at four bitrate
ranges using the high-efficiency video coding (HEVC)/H.265 standard.
Evaluations show that the dataset not only features a wide variety of scenes
but also spans different levels of content complexity, which is crucial for
robust solutions that perform well in real-world scenarios and generalize
across diverse visual environments. Additionally, we evaluate the performance,
considering both quality enhancement and runtime, of two handcrafted and two
ML-based SR models on the validation and testing sets of ODVista.","['Ahmed Telili', 'Ibrahim Farhat', 'Wassim Hamidouche', 'Hadi Amirpour']",2024-03-01T15:30:04Z,http://arxiv.org/abs/2403.00604v2,['eess.IV'],"omnidirectional video,dataset,super-resolution,quality enhancement,adaptive resolution,compression,machine learning,HEVC,content complexity,visual environments"
"Polarization-Encoded Lenticular Nano-Printing with Single-Layer
  Metasurfaces","Metasurface-based nano-printing has enabled ultrahigh-resolution grayscale or
color image display. However, the maximum number of independent nano-printing
images allowed by one single-layer metasurface is still limited despite many
multiplexing methods that have been proposed to increase the design degree of
freedom. In this work, we substantially push the multiplexing limit of
nano-printing by transforming images at different observation angles into
mapping the corresponding images to different positions in the Fourier space,
and simultaneously controlling the complex electric field across multiple
polarization channels. Our proposed Polarization-Encoded Lenticular
Nano-Printing (Pollen), aided by a modified evolutionary algorithm, allows the
display of several images based on the viewing angle, similar to traditional
lenticular printing but without requiring a lenticular layer. In addition, it
extends the display capability to encompass multiple polarization states.
Empowered by the ability to control the complex amplitude of three polarization
channels, we numerically and experimentally demonstrate the generation of 13
distinguished gray-scale Chinese ink wash painting images, 49 binary patterns,
and three sets of 3D nano-printing images, totaling 25 unique visuals. These
results present the largest number of recorded images with ultra-high
resolution to date. Our innovative Pollen technique is expected to benefit the
development of modern optical applications, including but not limited to
optical encryption, optical data storage, lightweight display, and augmented
reality and virtual reality.","['Lin Deng', 'Ziqiang Cai', 'Yongmin Liu']",2024-03-05T03:21:21Z,http://arxiv.org/abs/2403.02620v1,"['physics.optics', 'physics.app-ph']","Metasurface,Nano-printing,Polarization encoding,Lenticular printing,Multiplexing methods,Fourier space,Electric field,Evolutionary algorithm,Grayscale images,Binary patterns"
"IMU Tracking of Kinematic Chains in the Absence of Gravitational and
  Magnetic Fields","Tracking kinematic chains has many uses from healthcare to virtual reality.
Inertial measurement units, IMUs, are well-recognised for their body tracking
capabilities, however, existing solutions rely on gravity and often magnetic
fields for drift correction. As humanity's presence in space increases, systems
that don't rely on gravity or magnetism are required. We aim to demonstrate the
viability of IMU body tracking in a microgravity environment by showing that
gravity and magnetism are not necessary for correcting gyroscope-based
dead-reckoning drift. We aim to build and evaluate an end-to-end solution
accomplishing this. A novel algorithm is developed that compensates for drift
using local accelerations alone, without needing gravity or magnetism. Custom
PCB sensor, IMU, nodes are created and combined into a body-sensor-network to
implement the algorithm and the system is evaluated to determine its strengths
and weaknesses. Dead-reckoning alone is accurate to within 1 degree for 30s.
The drift correction solution can correct large drifts in yaw within 4 seconds
of lateral accelerations to within 3.3 degrees RMSE. Correction accuracy when
drift-free and under motion is 1.1 degrees RSME. We demonstrate that gyroscopic
drift can be compensated for in a kinematic chain by making use of local
acceleration information and often-discarded centripetal and tangential
acceleration information, even in the absence of gravitational and magnetic
fields. Therefore, IMU body tracking is a viable technology for use in
microgravity environments.","['Greg K. Stretton', 'George Alex Koulieris']",2024-03-07T09:39:15Z,http://arxiv.org/abs/2403.04357v1,"['cs.HC', 'H.5.2; G.1.0; C.3']","IMU,tracking,kinematic chains,microgravity,drift correction,gyroscope-based,dead-reckoning,local accelerations,body-sensor-network,yaw"
"ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering
  Perceived Texture Roughness in Mixed Reality","Extensive research has been done in haptic feedback for texture simulation in
virtual reality (VR). However, it is challenging to modify the perceived
tactile texture of existing physical objects which usually serve as anchors for
virtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a
finger-worn haptic device that uses vibratory-pneumatic feedback to modulate
(i.e., increase and decrease) the perceived roughness of the material surface
contacted by the user's fingerpad while supporting the perceived sensation of
other haptic properties (e.g., temperature or stickiness) in MR. Our device
includes a silicone-based pneumatic actuator that can lift the user's fingerpad
on the physical surface to reduce the contact area for roughness decreasing,
and an on-finger vibrator for roughness increasing. Our user-perception
experimental results showed that the participants could perceive changes in
roughness, both increasing and decreasing, compared to the original material
surface. We also observed the overlapping roughness ratings among certain
haptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived
roughness of some materials without any haptic feedback. This suggests the
potential to alter the perceived texture of one type of material to another in
terms of roughness (e.g., modifying the perceived texture of ceramics as
glass). Lastly, a user study of MR experience showed that ViboPneumo could
significantly improve the MR user experience, particularly for visual-haptic
matching, compared to the condition of a bare finger. We also demonstrated a
few application scenarios for ViboPneumo.","['Shaoyu Cai', 'Zhenlin Chen', 'Haichen Gao', 'Ya Huang', 'Qi Zhang', 'Xinge Yu', 'Kening Zhu']",2024-03-08T09:48:05Z,http://arxiv.org/abs/2403.05182v1,"['cs.HC', 'cs.GR']","haptic feedback,texture simulation,virtual reality,mixed reality,pneumatic actuator,vibrotactile,user study,roughness,haptic device"
"Enabling Developers, Protecting Users: Investigating Harassment and
  Safety in VR","Virtual Reality (VR) has witnessed a rising issue of harassment, prompting
the integration of safety controls like muting and blocking in VR applications.
However, the lack of standardized safety measures across VR applications
hinders their universal effectiveness, especially across contexts like
socializing, gaming, and streaming. While prior research has studied safety
controls in social VR applications, our user study (n = 27) takes a
multi-perspective approach, examining both users' perceptions of safety control
usability and effectiveness as well as the challenges that developers face in
designing and deploying VR safety controls. We identify challenges VR users
face while employing safety controls, such as finding users in crowded virtual
spaces to block them. VR users also find controls ineffective in addressing
harassment; for instance, they fail to eliminate the harassers' presence from
the environment. Further, VR users find the current methods of submitting
evidence for reports time-consuming and cumbersome. Improvements desired by
users include live moderation and behavior tracking across VR apps; however,
developers cite technological, financial, and legal obstacles to implementing
such solutions, often due to a lack of awareness and high development costs. We
emphasize the importance of establishing technical and legal guidelines to
enhance user safety in virtual environments.","['Abhinaya S. B.', 'Aafaq Sabir', 'Anupam Das']",2024-03-08T18:15:53Z,http://arxiv.org/abs/2403.05499v1,"['cs.HC', 'cs.CR', 'cs.CY', 'cs.ET']","harassment,safety controls,VR applications,social VR,usability,effectiveness,developers,user study,user safety,virtual environments"
"Interest-Aware Joint Caching, Computing, and Communication Optimization
  for Mobile VR Delivery in MEC Networks","In the upcoming B5G/6G era, virtual reality (VR) over wireless has become a
typical application, which is an inevitable trend in the development of video.
However, in immersive and interactive VR experiences, VR services typically
exhibit high delay, while simultaneously posing challenges for the energy
consumption of local devices. To address these issues, this paper aims to
improve the performance of the VR service in the edge-terminal cooperative
system. Specifically, we formulate a problem of joint caching, computing, and
communication VR service policy, by optimizing the weighted sum of overall VR
delivery delay and energy consumption of local devices. For the purpose of
designing the optimal VR service policy, the optimization problem is decoupled
into three independent subproblems to be solved separately. To enhance the
caching efficiency within the network, a bidirectional encoder representations
from transformers (Bert)-based user interest analysis method is first proposed
to characterize the content requesting behavior accurately. On the basis of
this, a service cost minimum-maximization problem is formulated with
consideration of performance fairness among users. Thereafter, the joint
caching and computing scheme is derived for each user with given allocation of
communication resources while a bisection-based communication scheme is
acquired with the given information on joint caching and computing policy. With
alternative optimization, an optimal policy for joint caching, computing and
communication based on user interest can be finally obtained. Simulation
results are presented to demonstrate the superiority of the proposed user
interest-aware caching scheme and the effective of the joint caching, computing
and communication optimization policy with consideration of user fairness.","['Baojie Fu', 'Tong Tang', 'Dapeng Wu', 'Ruyan Wang']",2024-03-09T09:33:43Z,http://arxiv.org/abs/2403.05851v1,"['cs.MM', 'cs.ET']","Joint caching,computing,communication optimization,mobile VR delivery,MEC networks,VR service policy,VR delivery delay,energy consumption,bidirectional encoder representations,transformers"
"TeleMoMa: A Modular and Versatile Teleoperation System for Mobile
  Manipulation","A critical bottleneck limiting imitation learning in robotics is the lack of
data. This problem is more severe in mobile manipulation, where collecting
demonstrations is harder than in stationary manipulation due to the lack of
available and easy-to-use teleoperation interfaces. In this work, we
demonstrate TeleMoMa, a general and modular interface for whole-body
teleoperation of mobile manipulators. TeleMoMa unifies multiple human
interfaces including RGB and depth cameras, virtual reality controllers,
keyboard, joysticks, etc., and any combination thereof. In its more accessible
version, TeleMoMa works using simply vision (e.g., an RGB-D camera), lowering
the entry bar for humans to provide mobile manipulation demonstrations. We
demonstrate the versatility of TeleMoMa by teleoperating several existing
mobile manipulators - PAL Tiago++, Toyota HSR, and Fetch - in simulation and
the real world. We demonstrate the quality of the demonstrations collected with
TeleMoMa by training imitation learning policies for mobile manipulation tasks
involving synchronized whole-body motion. Finally, we also show that TeleMoMa's
teleoperation channel enables teleoperation on site, looking at the robot, or
remote, sending commands and observations through a computer network, and
perform user studies to evaluate how easy it is for novice users to learn to
collect demonstrations with different combinations of human interfaces enabled
by our system. We hope TeleMoMa becomes a helpful tool for the community
enabling researchers to collect whole-body mobile manipulation demonstrations.
For more information and video results,
https://robin-lab.cs.utexas.edu/telemoma-web.","['Shivin Dass', 'Wensi Ai', 'Yuqian Jiang', 'Samik Singh', 'Jiaheng Hu', 'Ruohan Zhang', 'Peter Stone', 'Ben Abbatematteo', 'Roberto Martín-Martín']",2024-03-12T17:58:01Z,http://arxiv.org/abs/2403.07869v2,"['cs.RO', 'cs.AI', 'cs.LG']","teleoperation,mobile manipulation,demonstrations,interfaces,RGB-D camera,imitation learning,whole-body motion,user studies,modular,versatile"
Semi-Transparent Image Sensors for Eye-Tracking Applications,"Image sensors hold a pivotal role in society due to their ability to capture
vast amounts of information. Traditionally, image sensors are opaque due to
light absorption in both the pixels and the read-out electronics that are
stacked on top of each other. Making image sensors visibly transparent would
have a far-reaching impact in numerous areas such as human-computer interfaces,
smart displays, and both augmented and virtual reality. In this paper, we
present the development and analysis of the first semi-transparent image sensor
and its applicability as an eye-tracking device. The device consists of an 8x8
array of semi-transparent photodetectors and electrodes disposed on a fully
transparent substrate. Each pixel of the array has a size of 60 x 140 {\mu}m
and an optical transparency of 85-95%. Pixels have a high sensitivity, with
more than 90% of them showing a noise equivalent irradiance < 10-4 W/m2 for
wavelengths of 637 nm. As the semi-transparent photodetectors have a large
amount of built-in gain, the opaque read-out electronics can be placed far away
from the detector array to ensure maximum transparency and fill factor. Indeed,
the operation and appearance of transparent image sensors present a fundamental
shift in how we think about cameras and imaging, as these devices can be
concealed in plain sight.","['Gabriel Mercier', 'Emre O. Polat', 'Shengtai Shi', 'Shuchi Gupta', 'Gerasimos Konstantatos', 'Stijn Goossens', 'Frank H. L. Koppens']",2024-03-13T07:08:54Z,http://arxiv.org/abs/2403.08297v1,"['physics.optics', 'cs.HC']","semi-transparent image sensors,eye-tracking applications,photodetectors,electrodes,optical transparency,sensitivity,noise equivalent irradiance,wavelengths,fill factor,transparent image sensors"
"Constrained Reinforcement Learning for Adaptive Controller
  Synchronization in Distributed SDN","In software-defined networking (SDN), the implementation of distributed SDN
controllers, with each controller responsible for managing a specific
sub-network or domain, plays a critical role in achieving a balance between
centralized control, scalability, reliability, and network efficiency. These
controllers must be synchronized to maintain a logically centralized view of
the entire network. While there are various approaches for synchronizing
distributed SDN controllers, most tend to prioritize goals such as optimization
of communication latency or load balancing, often neglecting to address both
the aspects simultaneously. This limitation becomes particularly significant
when considering applications like Augmented and Virtual Reality (AR/VR), which
demand constrained network latencies and substantial computational resources.
Additionally, many existing studies in this field predominantly rely on
value-based reinforcement learning (RL) methods, overlooking the potential
advantages offered by state-of-the-art policy-based RL algorithms. To bridge
this gap, our work focuses on examining deep reinforcement learning (DRL)
techniques, encompassing both value-based and policy-based methods, to
guarantee an upper latency threshold for AR/VR task offloading within SDN
environments, while selecting the most cost-effective servers for AR/VR task
offloading. Our evaluation results indicate that while value-based methods
excel in optimizing individual network metrics such as latency or load
balancing, policy-based approaches exhibit greater robustness in adapting to
sudden network changes or reconfiguration.","['Ioannis Panitsas', 'Akrit Mudvari', 'Leandros Tassiulas']",2024-01-21T21:57:22Z,http://arxiv.org/abs/2403.08775v1,"['cs.NI', 'cs.AI']","Constrained Reinforcement Learning,Adaptive Controller,Distributed SDN,Synchronization,Software-defined Networking,Latency,Augmented Reality,Virtual Reality,Deep Reinforcement Learning,Task Offloading"
"A Virtual Environment for Collaborative Inspection in Additive
  Manufacturing","Additive manufacturing (AM) techniques have been used to enhance the design
and fabrication of complex components for various applications in the medical,
aerospace, energy, and consumer products industries. A defining feature for
many AM parts is the complex internal geometry enabled by the printing process.
However, inspecting these internal structures requires volumetric imaging,
i.e., X-ray CT, leading to the well-known challenge of visualizing complex 3D
geometries using 2D desktop interfaces. Furthermore, existing tools are limited
to single-user systems making it difficult to jointly discuss or share findings
with a larger team, i.e., the designers, manufacturing experts, and evaluation
team. In this work, we present a collaborative virtual reality (VR) for the
exploration and inspection of AM parts. Geographically separated experts can
virtually inspect and jointly discuss data. It also supports VR and non-VR
users, who can be spectators in the VR environment. Various features for data
exploration and inspection are developed and enhanced via real-time
synchronization. We followed usability and interface verification guidelines
using Nielsen's heuristics approach. Furthermore, we conducted exploratory and
semi-structured interviews with domain experts to collect qualitative feedback.
Results reveal potential benefits, applicability, and current limitations. The
proposed collaborative VR environment provides a new basis and opens new
research directions for virtual inspection and team collaboration in AM
settings.","['Vuthea Chheang', 'Brian Thomas Weston', 'Robert William Cerda', 'Brian Au', 'Brian Giera', 'Peer-Timo Bremer', 'Haichao Miao']",2024-03-13T20:16:16Z,http://arxiv.org/abs/2403.08940v1,"['cs.HC', 'cs.DC']","Additive Manufacturing,Virtual Environment,Collaborative Inspection,Virtual Reality,Volumetric Imaging,3D Geometries,Data Exploration,Real-time Synchronization,Usability,Interface Verification"
"Den-SOFT: Dense Space-Oriented Light Field DataseT for 6-DOF Immersive
  Experience","We have built a custom mobile multi-camera large-space dense light field
capture system, which provides a series of high-quality and sufficiently dense
light field images for various scenarios. Our aim is to contribute to the
development of popular 3D scene reconstruction algorithms such as IBRnet, NeRF,
and 3D Gaussian splitting. More importantly, the collected dataset, which is
much denser than existing datasets, may also inspire space-oriented light field
reconstruction, which is potentially different from object-centric 3D
reconstruction, for immersive VR/AR experiences. We utilized a total of 40
GoPro 10 cameras, capturing images of 5k resolution. The number of photos
captured for each scene is no less than 1000, and the average density (view
number within a unit sphere) is 134.68. It is also worth noting that our system
is capable of efficiently capturing large outdoor scenes. Addressing the
current lack of large-space and dense light field datasets, we made efforts to
include elements such as sky, reflections, lights and shadows that are of
interest to researchers in the field of 3D reconstruction during the data
capture process. Finally, we validated the effectiveness of our provided
dataset on three popular algorithms and also integrated the reconstructed 3DGS
results into the Unity engine, demonstrating the potential of utilizing our
datasets to enhance the realism of virtual reality (VR) and create feasible
interactive spaces. The dataset is available at our project website.","['Xiaohang Yu', 'Zhengxian Yang', 'Shi Pan', 'Yuqi Han', 'Haoxiang Wang', 'Jun Zhang', 'Shi Yan', 'Borong Lin', 'Lei Yang', 'Tao Yu', 'Lu Fang']",2024-03-15T02:39:44Z,http://arxiv.org/abs/2403.09973v1,['cs.CV'],"mobile multi-camera system,dense light field,3D scene reconstruction,IBRnet,NeRF,3D Gaussian splitting,VR,AR,large-space dataset"
"MSI-NeRF: Linking Omni-Depth with View Synthesis through Multi-Sphere
  Image aided Generalizable Neural Radiance Field","Panoramic observation using fisheye cameras is significant in robot
perception, reconstruction, and remote operation. However, panoramic images
synthesized by traditional methods lack depth information and can only provide
three degrees-of-freedom (3DoF) rotation rendering in virtual reality
applications. To fully preserve and exploit the parallax information within the
original fisheye cameras, we introduce MSI-NeRF, which combines deep learning
omnidirectional depth estimation and novel view rendering. We first construct a
multi-sphere image as a cost volume through feature extraction and warping of
the input images. It is then processed by geometry and appearance decoders,
respectively. Unlike methods that regress depth maps directly, we further build
an implicit radiance field using spatial points and interpolated 3D feature
vectors as input. In this way, we can simultaneously realize omnidirectional
depth estimation and 6DoF view synthesis. Our method is trained in a
semi-self-supervised manner. It does not require target view images and only
uses depth data for supervision. Our network has the generalization ability to
reconstruct unknown scenes efficiently using only four images. Experimental
results show that our method outperforms existing methods in depth estimation
and novel view synthesis tasks.","['Dongyu Yan', 'Guanyu Huang', 'Fengyu Quan', 'Haoyao Chen']",2024-03-16T07:26:50Z,http://arxiv.org/abs/2403.10840v1,"['cs.RO', 'cs.CV']","fisheye cameras,depth estimation,neural radiance field,view synthesis,deep learning,omnidirectional,semi-self-supervised,generalization,feature extraction,radiance field"
"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a
  Multi-Objective Genetic Algorithm","Augmented Reality (AR) and Virtual Reality (VR) systems involve
computationally intensive image processing algorithms that can burden
end-devices with limited resources, leading to poor performance in providing
low latency services. Edge-to-cloud computing overcomes the limitations of
end-devices by offloading their computations to nearby edge devices or remote
cloud servers. Although this proves to be sufficient for many applications,
optimal placement of latency sensitive AR/VR services in edge-to-cloud
infrastructures (to provide desirable service response times and reliability)
remain a formidable challenging. To address this challenge, this paper develops
a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of
AR/VR-based services in multi-tier edge-to-cloud environments. The primary
objective of the proposed MOGA is to minimize the response time of all running
services, while maximizing the reliability of the underlying system from both
software and hardware perspectives. To evaluate its performance, we
mathematically modeled all components and developed a tailor-made simulator to
assess its effectiveness on various scales. MOGA was compared with several
heuristics to prove that intuitive solutions, which are usually assumed
sufficient, are not efficient enough for the stated problem. The experimental
results indicated that MOGA can significantly reduce the response time of
deployed services by an average of 67\% on different scales, compared to other
heuristic methods. MOGA also ensures reliability of the 97\% infrastructure
(hardware) and 95\% services (software).","['Mohammadsadeq Garshasbi Herabad', 'Javid Taheri', 'Bestoun S. Ahmed', 'Calin Curescu']",2024-03-19T15:54:56Z,http://arxiv.org/abs/2403.12849v1,['cs.DC'],"Service Placement,Edge-to-Cloud,AR,VR,Multi-Objective Genetic Algorithm,Response Time,Reliability,Edge Devices,Cloud Servers"
"RHINO-VR Experience: Teaching Mobile Robotics Concepts in an Interactive
  Museum Exhibit","In 1997, the very first tour guide robot RHINO was deployed in a museum in
Germany. With the ability to navigate autonomously through the environment, the
robot gave tours to over 2,000 visitors. Today, RHINO itself has become an
exhibit and is no longer operational. In this paper, we present RHINO-VR, an
interactive museum exhibit using virtual reality (VR) that allows museum
visitors to experience the historical robot RHINO in operation in a virtual
museum. RHINO-VR, unlike static exhibits, enables users to familiarize
themselves with basic mobile robotics concepts without the fear of damaging the
exhibit. In the virtual environment, the user is able to interact with RHINO in
VR by pointing to a location to which the robot should navigate and observing
the corresponding actions of the robot. To include other visitors who cannot
use the VR, we provide an external observation view to make RHINO visible to
them. We evaluated our system by measuring the frame rate of the VR simulation,
comparing the generated virtual 3D models with the originals, and conducting a
user study. The user-study showed that RHINO-VR improved the visitors'
understanding of the robot's functionality and that they would recommend
experiencing the VR exhibit to others.","['Erik Schlachhoff', 'Nils Dengler', 'Leif Van Holland', 'Patrick Stotko', 'Jorge de Heuvel', 'Reinhard Klein', 'Maren Bennewitz']",2024-03-22T12:07:03Z,http://arxiv.org/abs/2403.15151v1,['cs.RO'],"mobile robotics,interactive exhibit,virtual reality,museum,RHINO,autonomous navigation,user study,virtual environment,3D models"
"An International and Multidisciplinary Teaching Experience with Real
  Industrial Team Project Development","This paper presents the design, objectives, experiences, and results of an
international cooperation project funded by the European Commission in the
context of the Erasmus Intensive Programme (IP, for short) designed to improve
students' curricula. An IP is a short programme of study (minimum 2 weeks) that
brings together university students and staff from at least three countries in
order to encourage efficient and multinational teaching of specialist topics,
which might otherwise not be taught at all. This project lasted for 6 years,
covering two different editions, each one with three year duration. This
project lasted for 6 years, covering two different editions, each one with
three year duration. The first edition, named SAVRO (Simulation and Virtual
Reality in Robotics for Industrial Assembly Processes) was held in the period
2008-2010, with the participation of three Universities, namely the Universitat
Politecnica de Valencia (Spain), acting as IP coordinator, the Technische
Universitat Kaiserslautern (Germany), and the Universita degli Studi di Salerno
(Italy). The Universite de Reims Champagne-Ardenne (France) participated as a
new partner in the subsequent edition (2011-2013) of the IP, renamed as HUMAIN
(Human-Machine Interaction). Both editions of the teaching project were
characterized by the same objectives and organizational aspects, aiming to
provide educational initiatives based on active teaching through collaborative
works between international institutions, involving industrial partners too.
The aim of the paper is to illustrate the best practices that characterized the
organization of our experience as well as to present some general
recommendations and suggestions on how to devise computing academic curricula.","['Martin Mellado', 'Eduardo Vendrell', 'Filomena Ferrucci', 'Andrea Abate', 'Detlef Zuhlke', 'Bernard Riera']",2024-02-18T00:14:21Z,http://arxiv.org/abs/2403.15398v1,['cs.CY'],"international cooperation,European Commission,Erasmus Intensive Programme,specialist topics,multinational teaching,Simulation,Virtual Reality,Robotics,Human-Machine Interaction,computing academic curricula"
"The Correlations of Scene Complexity, Workload, Presence, and
  Cybersickness in a Task-Based VR Game","This investigation examined the relationships among scene complexity,
workload, presence, and cybersickness in virtual reality (VR) environments.
Numerous factors can influence the overall VR experience, and existing research
on this matter is not yet conclusive, warranting further investigation. In this
between-subjects experimental setup, 44 participants engaged in the Pendulum
Chair game, with half exposed to a simple scene with lower optic flow and lower
familiarity, and the remaining half to a complex scene characterized by higher
optic flow and greater familiarity. The study measured the dependent variables
workload, presence, and cybersickness and analyzed their correlations.
Equivalence testing was also used to compare the simple and complex
environments. Results revealed that despite the visible differences between the
environments, within the 10% boundaries of the maximum possible value for
workload and presence, and 13.6% of the maximum SSQ value, a statistically
significant equivalence was observed between the simple and complex scenes.
Additionally, a moderate, negative correlation emerged between workload and SSQ
scores. The findings suggest two key points: (1) the nature of the task can
mitigate the impact of scene complexity factors such as optic flow and
familiarity, and (2) the correlation between workload and cybersickness may
vary, showing either a positive or negative relationship.","['Mohammadamin Sanaei', 'Stephen B. Gilbert', 'Nikoo Javadpour', 'Hila Sabouni', 'Michael C. Dorneich', 'Jonathan W. Kelly']",2024-03-27T21:21:58Z,http://arxiv.org/abs/2403.19019v1,['cs.HC'],"scene complexity,workload,presence,cybersickness,virtual reality,VR environments,Pendulum Chair game,optic flow,familiarity"
Non-Exponential Reverberation Modeling Using Dark Velvet Noise,"Previous research on late-reverberation modeling has mainly focused on
exponentially decaying room impulse responses, whereas methods for accurately
modeling non-exponential reverberation remain challenging. This paper extends
the previously proposed basic dark-velvet-noise reverberation algorithm and
proposes a parametrization scheme for modeling late reverberation with
arbitrary temporal energy decay. Each pulse in the velvet-noise sequence is
routed to a single dictionary filter that is selected from a set of filters
based on weighted probabilities. The probabilities control the spectral
evolution of the late-reverberation model and are optimized to fit a target
impulse response via non-negative least-squares optimization. In this way, the
frequency-dependent energy decay of a target late-reverberation impulse
response can be fitted with mean and maximum T60 errors of 4% and 8%,
respectively, requiring about 50% less coloration filters than a previously
proposed filtered velvet-noise algorithm. Furthermore, the extended
dark-velvet-noise reverberation algorithm allows the modeled impulse response
to be gated, the frequency-dependent reverberation time to be modified, and the
model's spectral evolution and broadband decay to be decoupled. The proposed
method is suitable for the parametric late-reverberation synthesis of various
acoustic environments, especially spaces that exhibit a non-exponential energy
decay, motivating its use in musical audio and virtual reality.","['Jon Fagerström', 'Sebastian J. Schlecht', 'Vesa Välimäki']",2024-03-29T09:54:37Z,http://arxiv.org/abs/2403.20090v1,['eess.AS'],"late-reverberation modeling,dark velvet noise,parametrization scheme,dictionary filter,spectral evolution,energy decay,impulse response,non-negative least-squares optimization,reverberation time,acoustic environments"
"Efficient 3D Implicit Head Avatar with Mesh-anchored Hash Table
  Blendshapes","3D head avatars built with neural implicit volumetric representations have
achieved unprecedented levels of photorealism. However, the computational cost
of these methods remains a significant barrier to their widespread adoption,
particularly in real-time applications such as virtual reality and
teleconferencing. While attempts have been made to develop fast neural
rendering approaches for static scenes, these methods cannot be simply employed
to support realistic facial expressions, such as in the case of a dynamic
facial performance. To address these challenges, we propose a novel fast 3D
neural implicit head avatar model that achieves real-time rendering while
maintaining fine-grained controllability and high rendering quality. Our key
idea lies in the introduction of local hash table blendshapes, which are
learned and attached to the vertices of an underlying face parametric model.
These per-vertex hash-tables are linearly merged with weights predicted via a
CNN, resulting in expression dependent embeddings. Our novel representation
enables efficient density and color predictions using a lightweight MLP, which
is further accelerated by a hierarchical nearest neighbor search method.
Extensive experiments show that our approach runs in real-time while achieving
comparable rendering quality to state-of-the-arts and decent results on
challenging expressions.","['Ziqian Bai', 'Feitong Tan', 'Sean Fanello', 'Rohit Pandey', 'Mingsong Dou', 'Shichen Liu', 'Ping Tan', 'Yinda Zhang']",2024-04-02T00:55:50Z,http://arxiv.org/abs/2404.01543v1,"['cs.CV', 'cs.GR']","3D Implicit,Head Avatar,Mesh-anchored Hash Table,Blendshapes,Neural Implicit,Volumetric Representations,Real-time Rendering,Facial Expressions,Hash-tables,Hierarchical Nearest Neighbor"
"Exploring Emotions in Multi-componential Space using Interactive VR
  Games","Emotion understanding is a complex process that involves multiple components.
The ability to recognise emotions not only leads to new context awareness
methods but also enhances system interaction's effectiveness by perceiving and
expressing emotions. Despite the attention to discrete and dimensional models,
neuroscientific evidence supports those emotions as being complex and
multi-faceted. One framework that resonated well with such findings is the
Component Process Model (CPM), a theory that considers the complexity of
emotions with five interconnected components: appraisal, expression,
motivation, physiology and feeling. However, the relationship between CPM and
discrete emotions has not yet been fully explored. Therefore, to better
understand emotions underlying processes, we operationalised a data-driven
approach using interactive Virtual Reality (VR) games and collected multimodal
measures (self-reports, physiological and facial signals) from 39 participants.
We used Machine Learning (ML) methods to identify the unique contributions of
each component to emotion differentiation. Our results showed the role of
different components in emotion differentiation, with the model including all
components demonstrating the most significant contribution. Moreover, we found
that at least five dimensions are needed to represent the variation of emotions
in our dataset. These findings also have implications for using VR environments
in emotion research and highlight the role of physiological signals in emotion
recognition within such environments.","['Rukshani Somarathna', 'Gelareh Mohammadi']",2024-04-04T06:54:44Z,http://arxiv.org/abs/2404.03239v1,"['cs.HC', 'cs.AI', 'cs.LG']","emotions,interactive VR,Component Process Model (CPM),neuroscientific evidence,multimodal measures,Machine Learning (ML),physiological signals,facial signals,emotion recognition,Virtual Reality (VR)"
High-Frequency Capacitive Sensing for Electrohydraulic Soft Actuators,"The need for compliant and proprioceptive actuators has grown more evident in
pursuing more adaptable and versatile robotic systems. Hydraulically Amplified
Self-Healing Electrostatic (HASEL) actuators offer distinctive advantages with
their inherent softness and flexibility, making them promising candidates for
various robotic tasks, including delicate interactions with humans and animals,
biomimetic locomotion, prosthetics, and exoskeletons. This has resulted in a
growing interest in the capacitive self-sensing capabilities of HASEL actuators
to create miniature displacement estimation circuitry that does not require
external sensors. However, achieving HASEL self-sensing for actuation
frequencies above 1 Hz and with miniature high-voltage power supplies has
remained limited. In this paper, we introduce the F-HASEL actuator, which adds
an additional electrode pair used exclusively for capacitive sensing to a
Peano-HASEL actuator. We demonstrate displacement estimation of the F-HASEL
during high-frequency actuation up to 20 Hz and during external loading using
miniaturized circuitry comprised of low-cost off-the-shelf components and a
miniature high-voltage power supply. Finally, we propose a circuitry to
estimate the displacement of multiple F-HASELs and demonstrate it in a wearable
application to track joint rotations of a virtual reality user in real-time.","['Michel R. Vogt', 'Maximilian Eberlein', 'Clemens C. Christoph', 'Felix Baumann', 'Fabrice Bourquin', 'Wim Wende', 'Fabio Schaub', 'Amirhossein Kazemipour', 'Robert K. Katzschmann']",2024-04-05T12:55:04Z,http://arxiv.org/abs/2404.04071v2,['cs.RO'],"High-frequency,Capacitive sensing,Electrohydraulic,Soft actuators,HASEL actuators,Displacement estimation,Miniaturized circuitry,High-voltage power supply,Wearable application"
"Reduction of Forgetting by Contextual Variation During Encoding Using
  360-Degree Video-Based Immersive Virtual Environments","Recall impairment in a different environmental context from learning is
called context-dependent forgetting. Two learning methods have been proposed to
prevent context-dependent forgetting: reinstatement and decontextualization.
Reinstatement matches the environmental context between learning and retrieval,
whereas decontextualization involves repeated learning in various environmental
contexts and eliminates the context dependency of memory. Conventionally, these
methods have been validated by switching between physical rooms. However, in
this study, we use immersive virtual environments (IVEs) as the environmental
context assisted by virtual reality (VR), which is known for its low cost and
high reproducibility compared to traditional manipulation. Whereas most
existing studies using VR have failed to reveal the reinstatement effect, we
test its occurrence using a 360-degree video-based IVE with improved
familiarity and realism instead of a computer graphics-based IVE. Furthermore,
we are the first to address decontextualization using VR. Our experiment showed
that repeated learning in the same constant IVE as retrieval did not
significantly reduce forgetting compared to repeated learning in different
constant IVEs. Conversely, repeated learning in various IVEs significantly
reduced forgetting than repeated learning in constant IVEs. These findings
contribute to the design of IVEs for VR-based applications, particularly in
educational settings.","['Takato Mizuho', 'Takuji Narumi', 'Hideaki Kuzuoka']",2024-04-07T16:16:30Z,http://arxiv.org/abs/2404.05007v2,['cs.HC'],"context-dependent forgetting,reinstatement,decontextualization,immersive virtual environments,virtual reality,360-degree video,memory,environmental context,forgetting,learning"
Nanomolecular OLED Pixelization Enabling Electroluminescent Metasurfaces,"Miniaturization of light-emitting diodes (LEDs) can enable high-resolution
augmented and virtual reality displays and on-chip light sources for
ultra-broadband chiplet communication. However, unlike silicon scaling in
electronic integrated circuits, patterning of inorganic III-V semiconductors in
LEDs considerably compromises device efficiencies at submicrometer scales.
Here, we present the scalable fabrication of nanoscale organic LEDs
(nano-OLEDs), with the highest array density (>84,000 pixels per inch) and the
smallest pixel size (~100 nm) ever reported to date. Direct nanomolecular
patterning of organic semiconductors is realized by self-aligned evaporation
through nanoapertures fabricated on a free-standing silicon nitride film
adhering to the substrate. The average external quantum efficiencies (EQEs)
extracted from a nano-OLED device of more than 4 megapixels reach up to 10%. At
the subwavelength scale, individual pixels act as electroluminescent meta-atoms
forming metasurfaces that directly convert electricity into modulated light.
The diffractive coupling between nano-pixels enables control over the far-field
emission properties, including directionality and polarization. The results
presented here lay the foundation for bright surface light sources of dimension
smaller than the Abbe diffraction limit, offering new technological platforms
for super-resolution imaging, spectroscopy, sensing, and hybrid integrated
photonics.","['Tommaso Marcato', 'Jiwoo Oh', 'Zhan-Hong Lin', 'Sunil B. Shivarudraiah', 'Sudhir Kumar', 'Shuangshuang Zeng', 'Chih-Jen Shih']",2024-04-08T09:25:11Z,http://arxiv.org/abs/2404.05336v1,"['physics.optics', 'cond-mat.mtrl-sci']","Nanomolecular OLED,Pixelization,Electroluminescent,Metasurfaces,III-V semiconductors,Nanoapertures,External quantum efficiencies (EQEs),Meta-atoms,Far-field emission,Abbe diffraction limit"
Harnessing the Power of AI-Generated Content for Semantic Communication,"Semantic Communication (SemCom) is envisaged as the next-generation paradigm
to address challenges stemming from the conflicts between the increasing volume
of transmission data and the scarcity of spectrum resources. However, existing
SemCom systems face drawbacks, such as low explainability, modality rigidity,
and inadequate reconstruction functionality. Recognizing the transformative
capabilities of AI-generated content (AIGC) technologies in content generation,
this paper explores a pioneering approach by integrating them into SemCom to
address the aforementioned challenges. We employ a three-layer model to
illustrate the proposed AIGC-assisted SemCom (AIGC-SCM) architecture,
emphasizing its clear deviation from existing SemCom. Grounded in this model,
we investigate various AIGC technologies with the potential to augment SemCom's
performance. In alignment with SemCom's goal of conveying semantic meanings, we
also introduce the new evaluation methods for our AIGC-SCM system.
Subsequently, we explore communication scenarios where our proposed AIGC-SCM
can realize its potential. For practical implementation, we construct a
detailed integration workflow and conduct a case study in a virtual reality
image transmission scenario. The results demonstrate our ability to maintain a
high degree of alignment between the reconstructed content and the original
source information, while substantially minimizing the data volume required for
transmission. These findings pave the way for further enhancements in
communication efficiency and the improvement of Quality of Service. At last, we
present future directions for AIGC-SCM studies.","['Yiru Wang', 'Wanting Yang', 'Zehui Xiong', 'Yuping Zhao', 'Tony Q. S. Quek', 'Zhu Han']",2024-04-10T06:13:45Z,http://arxiv.org/abs/2404.06765v1,['eess.SP'],"Semantic Communication,AI-generated content,SemCom,spectrum resources,content generation,AIGC-SCM architecture,communication scenarios,evaluation methods,virtual reality,Quality of Service"
"DreamScene360: Unconstrained Text-to-3D Scene Generation with Panoramic
  Gaussian Splatting","The increasing demand for virtual reality applications has highlighted the
significance of crafting immersive 3D assets. We present a text-to-3D
360$^{\circ}$ scene generation pipeline that facilitates the creation of
comprehensive 360$^{\circ}$ scenes for in-the-wild environments in a matter of
minutes. Our approach utilizes the generative power of a 2D diffusion model and
prompt self-refinement to create a high-quality and globally coherent panoramic
image. This image acts as a preliminary ""flat"" (2D) scene representation.
Subsequently, it is lifted into 3D Gaussians, employing splatting techniques to
enable real-time exploration. To produce consistent 3D geometry, our pipeline
constructs a spatially coherent structure by aligning the 2D monocular depth
into a globally optimized point cloud. This point cloud serves as the initial
state for the centroids of 3D Gaussians. In order to address invisible issues
inherent in single-view inputs, we impose semantic and geometric constraints on
both synthesized and input camera views as regularizations. These guide the
optimization of Gaussians, aiding in the reconstruction of unseen regions. In
summary, our method offers a globally consistent 3D scene within a
360$^{\circ}$ perspective, providing an enhanced immersive experience over
existing techniques. Project website at: http://dreamscene360.github.io/","['Shijie Zhou', 'Zhiwen Fan', 'Dejia Xu', 'Haoran Chang', 'Pradyumna Chari', 'Tejas Bharadwaj', 'Suya You', 'Zhangyang Wang', 'Achuta Kadambi']",2024-04-10T10:46:59Z,http://arxiv.org/abs/2404.06903v1,"['cs.CV', 'cs.AI']","3D scene generation,Text-to-3D,Panoramic image,Gaussian splatting,2D diffusion model,Monocular depth,Point cloud,Semantic constraints,Geometric constraints,Immersive experience"
OccGaussian: 3D Gaussian Splatting for Occluded Human Rendering,"Rendering dynamic 3D human from monocular videos is crucial for various
applications such as virtual reality and digital entertainment. Most methods
assume the people is in an unobstructed scene, while various objects may cause
the occlusion of body parts in real-life scenarios. Previous method utilizing
NeRF for surface rendering to recover the occluded areas, but it requiring more
than one day to train and several seconds to render, failing to meet the
requirements of real-time interactive applications. To address these issues, we
propose OccGaussian based on 3D Gaussian Splatting, which can be trained within
6 minutes and produces high-quality human renderings up to 160 FPS with
occluded input. OccGaussian initializes 3D Gaussian distributions in the
canonical space, and we perform occlusion feature query at occluded regions,
the aggregated pixel-align feature is extracted to compensate for the missing
information. Then we use Gaussian Feature MLP to further process the feature
along with the occlusion-aware loss functions to better perceive the occluded
area. Extensive experiments both in simulated and real-world occlusions,
demonstrate that our method achieves comparable or even superior performance
compared to the state-of-the-art method. And we improving training and
inference speeds by 250x and 800x, respectively. Our code will be available for
research purposes.","['Jingrui Ye', 'Zongkai Zhang', 'Yujiao Jiang', 'Qingmin Liao', 'Wenming Yang', 'Zongqing Lu']",2024-04-12T13:00:06Z,http://arxiv.org/abs/2404.08449v2,['cs.CV'],"3D Gaussian Splatting,Occluded Human Rendering,NeRF,Occlusion,Surface Rendering,Real-time Interactive Applications,OccGaussian,Gaussian Feature MLP,Occlusion-aware Loss Functions"
3D Human Scan With A Moving Event Camera,"Capturing a 3D human body is one of the important tasks in computer vision
with a wide range of applications such as virtual reality and sports analysis.
However, conventional frame cameras are limited by their temporal resolution
and dynamic range, which imposes constraints in real-world application setups.
Event cameras have the advantages of high temporal resolution and high dynamic
range (HDR), but the development of event-based methods is necessary to handle
data with different characteristics. This paper proposes a novel event-based
method for 3D pose estimation and human mesh recovery. Prior work on
event-based human mesh recovery require frames (images) as well as event data.
The proposed method solely relies on events; it carves 3D voxels by moving the
event camera around a stationary body, reconstructs the human pose and mesh by
attenuated rays, and fit statistical body models, preserving high-frequency
details. The experimental results show that the proposed method outperforms
conventional frame-based methods in the estimation accuracy of both pose and
body mesh. We also demonstrate results in challenging situations where a
conventional camera has motion blur. This is the first to demonstrate
event-only human mesh recovery, and we hope that it is the first step toward
achieving robust and accurate 3D human body scanning from vision sensors.
https://florpeng.github.io/event-based-human-scan/","['Kai Kohyama', 'Shintaro Shiba', 'Yoshimitsu Aoki']",2024-04-12T14:34:24Z,http://arxiv.org/abs/2404.08504v2,['cs.CV'],"3D human scan,event camera,temporal resolution,dynamic range,pose estimation,human mesh recovery,event-based method,3D voxels,statistical body models"
"Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation:
  Do We Need Artificial Augmentation?","We present ANYU, a new virtually augmented version of the NYU depth v2
dataset, designed for monocular depth estimation. In contrast to the well-known
approach where full 3D scenes of a virtual world are utilized to generate
artificial datasets, ANYU was created by incorporating RGB-D representations of
virtual reality objects into the original NYU depth v2 images. We specifically
did not match each generated virtual object with an appropriate texture and a
suitable location within the real-world image. Instead, an assignment of
texture, location, lighting, and other rendering parameters was randomized to
maximize a diversity of the training data, and to show that it is randomness
that can improve the generalizing ability of a dataset. By conducting extensive
experiments with our virtually modified dataset and validating on the original
NYU depth v2 and iBims-1 benchmarks, we show that ANYU improves the monocular
depth estimation performance and generalization of deep neural networks with
considerably different architectures, especially for the current
state-of-the-art VPD model. To the best of our knowledge, this is the first
work that augments a real-world dataset with randomly generated virtual 3D
objects for monocular depth estimation. We make our ANYU dataset publicly
available in two training configurations with 10% and 100% additional
synthetically enriched RGB-D pairs of training images, respectively, for
efficient training and empirical exploration of virtual augmentation at
https://github.com/ABrain-One/ANYU","['Dmitry Ignatov', 'Andrey Ignatov', 'Radu Timofte']",2024-04-15T05:44:03Z,http://arxiv.org/abs/2404.09469v1,"['cs.CV', 'cs.LG']","NYU Depth V2 Dataset,Monocular Depth Estimation,Virtual Augmentation,RGB-D,Virtual Reality Objects,Deep Neural Networks,VPD model,Dataset Augmentation,Training Data,Synthetic Enrichment"
"AAM-VDT: Vehicle Digital Twin for Tele-Operations in Advanced Air
  Mobility","This study advanced tele-operations in Advanced Air Mobility (AAM) through
the creation of a Vehicle Digital Twin (VDT) system for eVTOL aircraft,
tailored to enhance remote control safety and efficiency, especially for Beyond
Visual Line of Sight (BVLOS) operations. By synergizing digital twin technology
with immersive Virtual Reality (VR) interfaces, we notably elevate situational
awareness and control precision for remote operators. Our VDT framework
integrates immersive tele-operation with a high-fidelity aerodynamic database,
essential for authentically simulating flight dynamics and control tactics. At
the heart of our methodology lies an eVTOL's high-fidelity digital replica,
placed within a simulated reality that accurately reflects physical laws,
enabling operators to manage the aircraft via a master-slave dynamic,
substantially outperforming traditional 2D interfaces. The architecture of the
designed system ensures seamless interaction between the operator, the digital
twin, and the actual aircraft, facilitating exact, instantaneous feedback.
Experimental assessments, involving propulsion data gathering, simulation
database fidelity verification, and tele-operation testing, verify the system's
capability in precise control command transmission and maintaining the
digital-physical eVTOL synchronization. Our findings underscore the VDT
system's potential in augmenting AAM efficiency and safety, paving the way for
broader digital twin application in autonomous aerial vehicles.","['Tuan Anh Nguyen', 'Taeho Kwag', 'Vinh Pham', 'Viet Nghia Nguyen', 'Jeongseok Hyun', 'Minseok Jang', 'Jae-Woo Lee']",2024-04-15T09:49:17Z,http://arxiv.org/abs/2404.09621v1,"['eess.SY', 'cs.ET', 'cs.HC', 'cs.RO', 'cs.SY']","Advanced Air Mobility,Tele-Operations,Vehicle Digital Twin,eVTOL aircraft,Beyond Visual Line of Sight (BVLOS) operations,Virtual Reality (VR) interfaces,aerodynamic database,flight dynamics,control tactics,digital-physical synchronization"
"Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic
  Displays","Recently, deep learning-based computer-generated holography (CGH) has
demonstrated tremendous potential in three-dimensional (3D) displays and
yielded impressive display quality. However, most existing deep learning-based
CGH techniques can only generate holograms of 1080p resolution, which is far
from the ultra-high resolution (16K+) required for practical virtual reality
(VR) and augmented reality (AR) applications to support a wide field of view
and large eye box. One of the major obstacles in current CGH frameworks lies in
the limited memory available on consumer-grade GPUs which could not facilitate
the generation of higher-definition holograms. To overcome the aforementioned
challenge, we proposed a divide-conquer-and-merge strategy to address the
memory and computational capacity scarcity in ultra-high-definition CGH
generation. This algorithm empowers existing CGH frameworks to synthesize
higher-definition holograms at a faster speed while maintaining high-fidelity
image display quality. Both simulations and experiments were conducted to
demonstrate the capabilities of the proposed framework. By integrating our
strategy into HoloNet and CCNNs, we achieved significant reductions in GPU
memory usage during the training period by 64.3\% and 12.9\%, respectively.
Furthermore, we observed substantial speed improvements in hologram generation,
with an acceleration of up to 3$\times$ and 2 $\times$, respectively.
Particularly, we successfully trained and inferred 8K definition holograms on
an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore,
we conducted full-color optical experiments to verify the effectiveness of our
method. We believe our strategy can provide a novel approach for memory- and
time-efficient holographic displays.","['Zhenxing Dong', 'Jidong Jia', 'Yan Li', 'Yuye Ling']",2024-02-25T13:58:03Z,http://arxiv.org/abs/2404.10777v1,"['eess.IV', 'cs.GR', 'physics.optics']","computer-generated holography,deep learning,CGH techniques,ultra-high resolution,GPU memory,hologram generation,HoloNet,CCNNs,image display quality,holographic displays"
"Comparing Continuous and Retrospective Emotion Ratings in Remote VR
  Study","This study investigates the feasibility of remote virtual reality (VR)
studies conducted at home using VR headsets and video conferencing by deploying
an experiment on emotion ratings. 20 participants used head-mounted displays to
immerse themselves in 360{\deg} videos selected to evoke emotional responses.
The research compares continuous ratings using a graphical interface to
retrospective questionnaires on a digitized Likert Scale for measuring arousal
and valence, both based on the self-assessment manikin (SAM). It was
hypothesized that the two different rating methods would lead to significantly
different values for both valence and arousal. The goal was to investigate
whether continuous ratings during the experience would better reflect users'
emotions compared to the post-questionnaire by mitigating biases such as the
peak-end rule. The results show significant differences with moderate to strong
effect sizes for valence and no significant differences for arousal with low to
moderate effect sizes. This indicates the need for further investigation of the
methods used to assess emotion ratings in VR studies. Overall, this study is an
example of a remotely conducted VR experiment, offering insights into methods
for emotion elicitation in VR by varying the timing and interface of the
rating.","['Maximilian Warsinke', 'Tanja KojiāE, 'Maurizio Vergari', 'Robert Spang', 'Jan-Niklas Voigt-Antons', 'Sebastian Möller']",2024-04-25T10:19:44Z,http://arxiv.org/abs/2404.16487v1,['cs.HC'],"remote virtual reality,VR headsets,video conferencing,emotion ratings,360° videos,Likert Scale,self-assessment manikin,continuous ratings,retrospective questionnaires,arousal"
ResVR: Joint Rescaling and Viewport Rendering of Omnidirectional Images,"With the advent of virtual reality technology, omnidirectional image (ODI)
rescaling techniques are increasingly embraced for reducing transmitted and
stored file sizes while preserving high image quality. Despite this progress,
current ODI rescaling methods predominantly focus on enhancing the quality of
images in equirectangular projection (ERP) format, which overlooks the fact
that the content viewed on head mounted displays (HMDs) is actually a rendered
viewport instead of an ERP image. In this work, we emphasize that focusing
solely on ERP quality results in inferior viewport visual experiences for
users. Thus, we propose ResVR, which is the first comprehensive framework for
the joint Rescaling and Viewport Rendering of ODIs. ResVR allows obtaining LR
ERP images for transmission while rendering high-quality viewports for users to
watch on HMDs. In our ResVR, a novel discrete pixel sampling strategy is
developed to tackle the complex mapping between the viewport and ERP, enabling
end-to-end training of ResVR pipeline. Furthermore, a spherical pixel shape
representation technique is innovatively derived from spherical differentiation
to significantly improve the visual quality of rendered viewports. Extensive
experiments demonstrate that our ResVR outperforms existing methods in viewport
rendering tasks across different fields of view, resolutions, and view
directions while keeping a low transmission overhead.","['Weiqi Li', 'Shijie Zhao', 'Bin Chen', 'Xinhua Cheng', 'Junlin Li', 'Li Zhang', 'Jian Zhang']",2024-04-25T17:59:46Z,http://arxiv.org/abs/2404.16825v1,"['cs.CV', 'eess.IV']","Omnidirectional image,Rescaling,Viewport rendering,Equirectangular projection,Head mounted displays,Pixel sampling,Spherical pixel shape representation,Transmission overhead"
Motor Focus: Ego-Motion Prediction with All-Pixel Matching,"Motion analysis plays a critical role in various applications, from virtual
reality and augmented reality to assistive visual navigation. Traditional
self-driving technologies, while advanced, typically do not translate directly
to pedestrian applications due to their reliance on extensive sensor arrays and
non-feasible computational frameworks. This highlights a significant gap in
applying these solutions to human users since human navigation introduces
unique challenges, including the unpredictable nature of human movement,
limited processing capabilities of portable devices, and the need for
directional responsiveness due to the limited perception range of humans. In
this project, we introduce an image-only method that applies motion analysis
using optical flow with ego-motion compensation to predict Motor Focus-where
and how humans or machines focus their movement intentions. Meanwhile, this
paper addresses the camera shaking issue in handheld and body-mounted devices
which can severely degrade performance and accuracy, by applying a Gaussian
aggregation to stabilize the predicted motor focus area and enhance the
prediction accuracy of movement direction. This also provides a robust,
real-time solution that adapts to the user's immediate environment.
Furthermore, in the experiments part, we show the qualitative analysis of motor
focus estimation between the conventional dense optical flow-based method and
the proposed method. In quantitative tests, we show the performance of the
proposed method on a collected small dataset that is specialized for motor
focus estimation tasks.","['Hao Wang', 'Jiayou Qin', 'Xiwen Chen', 'Ashish Bastola', 'John Suchanek', 'Zihao Gong', 'Abolfazl Razi']",2024-04-25T20:45:39Z,http://arxiv.org/abs/2404.17031v1,['cs.CV'],"motion analysis,ego-motion prediction,optical flow,motor focus,image-only method,Gaussian aggregation,prediction accuracy,real-time solution,qualitative analysis,quantitative tests"
"Sports Analysis and VR Viewing System Based on Player Tracking and Pose
  Estimation with Multimodal and Multiview Sensors","Sports analysis and viewing play a pivotal role in the current sports domain,
offering significant value not only to coaches and athletes but also to fans
and the media. In recent years, the rapid development of virtual reality (VR)
and augmented reality (AR) technologies have introduced a new platform for
watching games. Visualization of sports competitions in VR/AR represents a
revolutionary technology, providing audiences with a novel immersive viewing
experience. However, there is still a lack of related research in this area. In
this work, we present for the first time a comprehensive system for sports
competition analysis and real-time visualization on VR/AR platforms. First, we
utilize multiview LiDARs and cameras to collect multimodal game data.
Subsequently, we propose a framework for multi-player tracking and pose
estimation based on a limited amount of supervised data, which extracts precise
player positions and movements from point clouds and images. Moreover, we
perform avatar modeling of players to obtain their 3D models. Ultimately, using
these 3D player data, we conduct competition analysis and real-time
visualization on VR/AR. Extensive quantitative experiments demonstrate the
accuracy and robustness of our multi-player tracking and pose estimation
framework. The visualization results showcase the immense potential of our
sports visualization system on the domain of watching games on VR/AR devices.
The multimodal competition dataset we collected and all related code will be
released soon.","['Wenxuan Guo', 'Zhiyu Pan', 'Ziheng Xi', 'Alapati Tuerxun', 'Jianjiang Feng', 'Jie Zhou']",2024-05-02T09:19:43Z,http://arxiv.org/abs/2405.01112v1,['cs.CV'],"Sports analysis,VR viewing system,Player tracking,Pose estimation,Multimodal sensors,Multiview sensors,Virtual reality,Augmented reality,LiDAR,Avatar modeling"
"Effects of Realism and Representation on Self-Embodied Avatars in
  Immersive Virtual Environments","Virtual Reality (VR) has recently gained traction with many new and ever more
affordable devices being released. The increase in popularity of this paradigm
of interaction has given birth to new applications and has attracted casual
consumers to experience VR. Providing a self-embodied representation (avatar)
of users' full bodies inside shared virtual spaces can improve the VR
experience and make it more engaging to both new and experienced users . This
is especially important in fully immersive systems, where the equipment
completely occludes the real world making self awareness problematic. Indeed,
the feeling of presence of the user is highly influenced by their virtual
representations, even though small flaws could lead to uncanny valley
side-effects. Following previous research, we would like to assess whether
using a third-person perspective could also benefit the VR experience, via an
improved spatial awareness of the user's virtual surroundings. In this paper we
investigate realism and perspective of self-embodied representation in VR
setups in natural tasks, such as walking and avoiding obstacles. We compare
both First and Third-Person perspectives with three different levels of realism
in avatar representation. These range from a stylized abstract avatar, to a
""realistic"" mesh-based humanoid representation and a point-cloud rendering. The
latter uses data captured via depth-sensors and mapped into a virtual self
inside the Virtual Environment. We present a throughout evaluation and
comparison of these different representations, describing a series of
guidelines for self-embodied VR applications. The effects of the uncanny valley
are also discussed in the context of navigation and reflex-based tasks.","['Rafael Kuffner dos Anjos', 'João Madeiras Pereira']",2024-05-04T14:02:21Z,http://arxiv.org/abs/2405.02672v1,"['cs.HC', 'cs.GR']","Virtual Reality,Self-Embodied Avatars,Immersive Virtual Environments,Realism,Representation,Third-Person Perspective,First-Person Perspective,Avatar Representation,Depth-Sensors,Uncanny Valley"
Considering Avatar Crossing as Harm or Help for Adolescents in Social VR,"People leverage avatars to communicate nonverbal behaviors in immersive
virtual reality (VR), like interpersonal distance [2, 6] and virtual touch [5].
However, violations of appropriate physical distancing and unsolicited intimate
touching behavior in social virtual worlds represent potential social and
psychological virtual harm to older adolescent users [4, 8]. Obtaining peer
acceptance and social rewards, while avoiding social rejection can drive older
adolescent behavior even in simulated virtual spaces [1, 3], and while ""the
beginning of adolescence is largely defined by a biological event, [...] the
end of adolescence is often defined socially"" [3] (p.912). Avatar crossing, the
phenomenon of avatars walking through each other in virtual environments, is a
unique capability of virtual embodiment, and others intriguing possibilities
and ethical concerns for older adolescents experiencing social virtual spaces.
For example, the ability to cross through and share positions with other
avatars in a virtual classroom helps students concentrate on accessing and
comprehending information without concerns about blocking others when
navigating for better viewpoints [10]. However, the ability to cross through
others in virtual spaces has been associated with a reduction in perceived
presence and avatar realism, coupled with a greater level of discomfort and
intimidation in comparison to avatar collisions [12]. In this article, we
consider the potential benefits and harms of utilizing avatar crossing with
adolescent users.","['Jakki O. Bailey', 'Xinyue', 'You']",2024-04-23T15:39:37Z,http://arxiv.org/abs/2405.05933v1,['cs.HC'],"avatars,immersive virtual reality,interpersonal distance,virtual touch,physical distancing,social virtual worlds,older adolescents,peer acceptance,social rejection,virtual environments"
"A Performance Analysis Modeling Framework for Extended Reality
  Applications in Edge-Assisted Wireless Networks","Extended reality (XR) is at the center of attraction in the research
community due to the emergence of augmented, mixed, and virtual reality
applications. The performance of such applications needs to be uptight to
maintain the requirements of latency, energy consumption, and freshness of
data. Therefore, a comprehensive performance analysis model is required to
assess the effectiveness of an XR application but is challenging to design due
to the dependence of the performance metrics on several difficult-to-model
parameters, such as computing resources and hardware utilization of XR and edge
devices, which are controlled by both their operating systems and the
application itself. Moreover, the heterogeneity in devices and wireless access
networks brings additional challenges in modeling. In this paper, we propose a
novel modeling framework for performance analysis of XR applications
considering edge-assisted wireless networks and validate the model with
experimental data collected from testbeds designed specifically for XR
applications. In addition, we present the challenges associated with
performance analysis modeling and present methods to overcome them in detail.
Finally, the performance evaluation shows that the proposed analytical model
can analyze XR applications' performance with high accuracy compared to the
state-of-the-art analytical models.","['Anik Mallik', 'Jiang Xie', 'Zhu Han']",2024-05-11T15:16:12Z,http://arxiv.org/abs/2405.07033v1,"['cs.NI', 'cs.CV', 'cs.DC', 'eess.IV']","Performance Analysis,Modeling Framework,Extended Reality,Edge-Assisted,Wireless Networks,Latency,Energy Consumption,Freshness of Data,Heterogeneity,Experimental Data"
3D Hand Mesh Recovery from Monocular RGB in Camera Space,"With the rapid advancement of technologies such as virtual reality, augmented
reality, and gesture control, users expect interactions with computer
interfaces to be more natural and intuitive. Existing visual algorithms often
struggle to accomplish advanced human-computer interaction tasks, necessitating
accurate and reliable absolute spatial prediction methods. Moreover, dealing
with complex scenes and occlusions in monocular images poses entirely new
challenges. This study proposes a network model that performs parallel
processing of root-relative grids and root recovery tasks. The model enables
the recovery of 3D hand meshes in camera space from monocular RGB images. To
facilitate end-to-end training, we utilize an implicit learning approach for 2D
heatmaps, enhancing the compatibility of 2D cues across different subtasks.
Incorporate the Inception concept into spectral graph convolutional network to
explore relative mesh of root, and integrate it with the locally detailed and
globally attentive method designed for root recovery exploration. This approach
improves the model's predictive performance in complex environments and
self-occluded scenes. Through evaluation on the large-scale hand dataset
FreiHAND, we have demonstrated that our proposed model is comparable with
state-of-the-art models. This study contributes to the advancement of
techniques for accurate and reliable absolute spatial prediction in various
human-computer interaction applications.","['Haonan Li', 'Patrick P. K. Chen', 'Yitong Zhou']",2024-05-12T05:36:37Z,http://arxiv.org/abs/2405.07167v1,['cs.CV'],"3D hand mesh recovery,monocular RGB,camera space,network model,root-relative grids,root recovery,implicit learning,spectral graph convolutional network,Inception concept,FreiHAND"
"Improving the Real-Data Driven Network Evaluation Model for Digital Twin
  Networks","With the emergence and proliferation of new forms of large-scale services
such as smart homes, virtual reality/augmented reality, the increasingly
complex networks are raising concerns about significant operational costs. As a
result, the need for network management automation is emphasized, and Digital
Twin Networks (DTN) technology is expected to become the foundation technology
for autonomous networks. DTN has the advantage of being able to operate and
system networks based on real-time collected data in a closed-loop system, and
currently it is mainly designed for optimization scenarios. To improve network
performance in optimization scenarios, it is necessary to select appropriate
configurations and perform accurate performance evaluation based on real data.
However, most network evaluation models currently use simulation data.
Meanwhile, according to DTN standards documents, artificial intelligence (AI)
models can ensure scalability, real-time performance, and accuracy in
large-scale networks. Various AI research and standardization work is ongoing
to optimize the use of DTN. When designing AI models, it is crucial to consider
the characteristics of the data. This paper presents an autoencoder-based skip
connected message passing neural network (AE-SMPN) as a network evaluation
model using real network data. The model is created by utilizing graph neural
network (GNN) with recurrent neural network (RNN) models to capture the
spatiotemporal features of network data. Additionally, an AutoEncoder (AE) is
employed to extract initial features. The neural network was trained using the
real DTN dataset provided by the Barcelona Neural Networking Center (BNN-UPC),
and the paper presents the analysis of the model structure along with
experimental results.","['Hyeju Shin', 'Ibrahim Aliyu', 'Abubakar Isah', 'Jinsul Kim']",2024-05-14T09:55:03Z,http://arxiv.org/abs/2405.08473v1,['cs.LG'],"Real-Data,Network Evaluation Model,Digital Twin Networks,Autonomous Networks,Artificial Intelligence,Graph Neural Network,Recurrent Neural Network,AutoEncoder,Spatiotemporal Features"
"Motion Prediction with Gaussian Processes for Safe Human-Robot
  Interaction in Virtual Environments","Humans use collaborative robots as tools for accomplishing various tasks. The
interaction between humans and robots happens in tight shared workspaces.
However, these machines must be safe to operate alongside humans to minimize
the risk of accidental collisions. Ensuring safety imposes many constraints,
such as reduced torque and velocity limits during operation, thus increasing
the time to accomplish many tasks. However, for applications such as using
collaborative robots as haptic interfaces with intermittent contacts for
virtual reality applications, speed limitations result in poor user
experiences. This research aims to improve the efficiency of a collaborative
robot while improving the safety of the human user. We used Gaussian process
models to predict human hand motion and developed strategies for human
intention detection based on hand motion and gaze to improve the time for the
robot and human security in a virtual environment. We then studied the effect
of prediction. Results from comparisons show that the prediction models
improved the robot time by 3\% and safety by 17\%. When used alongside gaze,
prediction with Gaussian process models resulted in an improvement of the robot
time by 2\% and the safety by 13\%.","['Stanley Mugisha', 'Vamsi Krishna Guda', 'Christine Chevallereau', 'Damien Chablat', 'Matteo Zoppi']",2024-05-15T05:51:41Z,http://arxiv.org/abs/2405.09109v2,"['cs.RO', 'cs.AI', 'cs.LG', 'I.2.6; I.2.9; I.3.2; H.5.2']","Gaussian Processes,Human-Robot Interaction,Virtual Environments,Safety,Motion Prediction,Collaborative Robots,Human Intention Detection,Haptic Interfaces,Virtual Reality,Prediction Models"
A Survey On Text-to-3D Contents Generation In The Wild,"3D content creation plays a vital role in various applications, such as
gaming, robotics simulation, and virtual reality. However, the process is
labor-intensive and time-consuming, requiring skilled designers to invest
considerable effort in creating a single 3D asset. To address this challenge,
text-to-3D generation technologies have emerged as a promising solution for
automating 3D creation. Leveraging the success of large vision language models,
these techniques aim to generate 3D content based on textual descriptions.
Despite recent advancements in this area, existing solutions still face
significant limitations in terms of generation quality and efficiency. In this
survey, we conduct an in-depth investigation of the latest text-to-3D creation
methods. We provide a comprehensive background on text-to-3D creation,
including discussions on datasets employed in training and evaluation metrics
used to assess the quality of generated 3D models. Then, we delve into the
various 3D representations that serve as the foundation for the 3D generation
process. Furthermore, we present a thorough comparison of the rapidly growing
literature on generative pipelines, categorizing them into feedforward
generators, optimization-based generation, and view reconstruction approaches.
By examining the strengths and weaknesses of these methods, we aim to shed
light on their respective capabilities and limitations. Lastly, we point out
several promising avenues for future research. With this survey, we hope to
inspire researchers further to explore the potential of open-vocabulary
text-conditioned 3D content creation.",['Chenhan Jiang'],2024-05-15T15:23:22Z,http://arxiv.org/abs/2405.09431v1,"['cs.CV', 'cs.GR']","text-to-3D generation,3D content creation,vision language models,generative pipelines,feedforward generators,optimization-based generation,view reconstruction approaches,datasets,evaluation metrics,3D representations."
"Sparse Attention-driven Quality Prediction for Production Process
  Optimization in Digital Twins","In the process industry, optimizing production lines for long-term efficiency
requires real-time monitoring and analysis of operation states to fine-tune
production line parameters. However, the complexity in operational logic and
the intricate coupling of production process parameters make it difficult to
develop an accurate mathematical model for the entire process, thus hindering
the deployment of efficient optimization mechanisms. In view of these
difficulties, we propose to deploy a digital twin of the production line by
digitally abstracting its physical layout and operational logic. By iteratively
mapping the real-world data reflecting equipment operation status and product
quality inspection in the digital twin, we adopt a quality prediction model for
production process based on self-attention-enabled temporal convolutional
neural networks. This model enables the data-driven state evolution of the
digital twin. The digital twin takes a role of aggregating the information of
actual operating conditions and the results of quality-sensitive analysis,
which facilitates the optimization of process production quality with
virtual-reality evolution under multi-dimensional constraints. Leveraging the
digital twin model as an information-flow carrier, we extract temporal features
from key process indicators and establish a production process quality
prediction model based on the proposed composite neural network. Our operation
experiments on a specific tobacco shredding line demonstrate that the proposed
digital twin-based production process optimization method fosters seamless
integration between virtual and real production lines. This integration
achieves an average operating status prediction accuracy of over 98\% and
near-optimal production process control.","['Yanlei Yin', 'Lihua Wang', 'Wenbo Wang', 'Dinh Thai Hoang']",2024-05-20T09:28:23Z,http://arxiv.org/abs/2405.11895v1,"['cs.LG', 'cs.SY', 'eess.SY']","Sparse Attention-driven Quality Prediction,Production Process Optimization,Digital Twins,Real-time Monitoring,Operation States,Quality Prediction Model,Temporal Convolutional Neural Networks,Digital Twin Model,Production Line Parameters"
MOSS: Motion-based 3D Clothed Human Synthesis from Monocular Video,"Single-view clothed human reconstruction holds a central position in virtual
reality applications, especially in contexts involving intricate human motions.
It presents notable challenges in achieving realistic clothing deformation.
Current methodologies often overlook the influence of motion on surface
deformation, resulting in surfaces lacking the constraints imposed by global
motion. To overcome these limitations, we introduce an innovative framework,
Motion-Based 3D Clothed Humans Synthesis (MOSS), which employs kinematic
information to achieve motion-aware Gaussian split on the human surface. Our
framework consists of two modules: Kinematic Gaussian Locating Splatting (KGAS)
and Surface Deformation Detector (UID). KGAS incorporates matrix-Fisher
distribution to propagate global motion across the body surface. The density
and rotation factors of this distribution explicitly control the Gaussians,
thereby enhancing the realism of the reconstructed surface. Additionally, to
address local occlusions in single-view, based on KGAS, UID identifies
significant surfaces, and geometric reconstruction is performed to compensate
for these deformations. Experimental results demonstrate that MOSS achieves
state-of-the-art visual quality in 3D clothed human synthesis from monocular
videos. Notably, we improve the Human NeRF and the Gaussian Splatting by 33.94%
and 16.75% in LPIPS* respectively. Codes are available at
https://wanghongsheng01.github.io/MOSS/.","['Hongsheng Wang', 'Xiang Cai', 'Xi Sun', 'Jinhong Yue', 'Shengyu Zhang', 'Feng Lin', 'Fei Wu']",2024-05-21T13:57:53Z,http://arxiv.org/abs/2405.12806v1,['cs.CV'],"Motion-based,3D,Clothed human synthesis,Monocular video,Kinematic information,Gaussian split,Surface deformation,Matrix-Fisher distribution,Global motion,Local occlusions."
Remote Keylogging Attacks in Multi-user VR Applications,"As Virtual Reality (VR) applications grow in popularity, they have bridged
distances and brought users closer together. However, with this growth, there
have been increasing concerns about security and privacy, especially related to
the motion data used to create immersive experiences. In this study, we
highlight a significant security threat in multi-user VR applications, which
are applications that allow multiple users to interact with each other in the
same virtual space. Specifically, we propose a remote attack that utilizes the
avatar rendering information collected from an adversary's game clients to
extract user-typed secrets like credit card information, passwords, or private
conversations. We do this by (1) extracting motion data from network packets,
and (2) mapping motion data to keystroke entries. We conducted a user study to
verify the attack's effectiveness, in which our attack successfully inferred
97.62% of the keystrokes. Besides, we performed an additional experiment to
underline that our attack is practical, confirming its effectiveness even when
(1) there are multiple users in a room, and (2) the attacker cannot see the
victims. Moreover, we replicated our proposed attack on four applications to
demonstrate the generalizability of the attack. These results underscore the
severity of the vulnerability and its potential impact on millions of VR social
platform users.","['Zihao Su', 'Kunlin Cai', 'Reuben Beeler', 'Lukas Dresel', 'Allan Garcia', 'Ilya Grishchenko', 'Yuan Tian', 'Christopher Kruegel', 'Giovanni Vigna']",2024-05-22T22:10:40Z,http://arxiv.org/abs/2405.14036v1,['cs.CR'],"Remote Keylogging,Multi-user VR Applications,Security Threat,Motion Data,Avatar Rendering,User Study,Keystroke Entries,Network Packets,Attack Effectiveness,VR Social Platform"
Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis,"Accurate reconstruction of complex dynamic scenes from just a single
viewpoint continues to be a challenging task in computer vision. Current
dynamic novel view synthesis methods typically require videos from many
different camera viewpoints, necessitating careful recording setups, and
significantly restricting their utility in the wild as well as in terms of
embodied AI applications. In this paper, we propose $\textbf{GCD}$, a
controllable monocular dynamic view synthesis pipeline that leverages
large-scale diffusion priors to, given a video of any scene, generate a
synchronous video from any other chosen perspective, conditioned on a set of
relative camera pose parameters. Our model does not require depth as input, and
does not explicitly model 3D scene geometry, instead performing end-to-end
video-to-video translation in order to achieve its goal efficiently. Despite
being trained on synthetic multi-view video data only, zero-shot real-world
generalization experiments show promising results in multiple domains,
including robotics, object permanence, and driving environments. We believe our
framework can potentially unlock powerful applications in rich dynamic scene
understanding, perception for robotics, and interactive 3D video viewing
experiences for virtual reality.","['Basile Van Hoorick', 'Rundi Wu', 'Ege Ozguroglu', 'Kyle Sargent', 'Ruoshi Liu', 'Pavel Tokmakov', 'Achal Dave', 'Changxi Zheng', 'Carl Vondrick']",2024-05-23T17:59:52Z,http://arxiv.org/abs/2405.14868v1,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']","generative camera dolly,monocular,dynamic view synthesis,computer vision,diffusion priors,camera pose parameters,video-to-video translation,3D scene geometry,embodied AI applications"
PanoTree: Autonomous Photo-Spot Explorer in Virtual Reality Scenes,"Social VR platforms enable social, economic, and creative activities by
allowing users to create and share their own virtual spaces. In social VR,
photography within a VR scene is an important indicator of visitors'
activities. Although automatic identification of photo spots within a VR scene
can facilitate the process of creating a VR scene and enhance the visitor
experience, there are challenges in quantitatively evaluating photos taken in
the VR scene and efficiently exploring the large VR scene. We propose PanoTree,
an automated photo-spot explorer in VR scenes. To assess the aesthetics of
images captured in VR scenes, a deep scoring network is trained on a large
dataset of photos collected by a social VR platform to determine whether humans
are likely to take similar photos. Furthermore, we propose a Hierarchical
Optimistic Optimization (HOO)-based search algorithm to efficiently explore 3D
VR spaces with the reward from the scoring network. Our user study shows that
the scoring network achieves human-level performance in distinguishing randomly
taken images from those taken by humans. In addition, we show applications
using the explored photo spots, such as automatic thumbnail generation, support
for VR world creation, and visitor flow planning within a VR scene.","['Tomohiro Hayase', 'Braun Sacha', 'Hikari Yanagawa', 'Itsuki Orito', 'Yuichi Hiroi']",2024-05-27T12:54:05Z,http://arxiv.org/abs/2405.17136v1,"['cs.CV', 'cs.GR', 'I.3; I.4']","Autonomous,Photo-spot,Explorer,Virtual reality,Social VR,Photography,VR scene,Deep scoring network,Hierarchical Optimistic Optimization,3D VR spaces"
"4Doodle: Two-handed Gestures for Immersive Sketching of Architectural
  Models","Three-dimensional immersive sketching for content creation and modeling has
been studied for some time. However, research in this domain mainly focused on
CAVE-like scenarios. These setups can be expensive and offer a narrow
interaction space. Building more affordable setups using head-mounted displays
is possible, allowing greater immersion and a larger space for user physical
movements. This paper presents a fully immersive environment using bi-manual
gestures to sketch and create content freely in the virtual world. This
approach can be applied to many scenarios, allowing people to express their
ideas or review existing designs. To cope with known motor difficulties and
inaccuracy of freehand 3D sketching, we explore proxy geometry and a laser-like
metaphor to draw content directly from models and create content surfaces. Our
current prototype offers 24 cubic meters for movement, limited by the room
size. It features infinite virtual drawing space through pan and scale
techniques and is larger than the typical 6-sided cave at a fraction of the
cost. In a preliminary study conducted with architects and engineers, our
system showed a clear promise as a tool for sketching and 3D content creation
in virtual reality with a great emphasis on bi-manual gestures.","['Fernando Fonseca', 'Maurício Sousa', 'Daniel Mendes', 'Alfredo Ferreira', 'Joaquim Jorge']",2024-05-29T08:42:49Z,http://arxiv.org/abs/2405.18887v1,"['cs.HC', 'H.5.2, I.3.4, I.3.7', 'H.5.2; I.3.4; I.3.7']","immersive sketching,architectural models,bi-manual gestures,virtual reality,proxy geometry,laser-like metaphor,3D sketching,content creation,head-mounted displays,virtual drawing space"
Are Game Platforms suitable for Parkinson Disease patients?,"Parkinson's Disease (PD) is a progressive neurodegenerative movement disorder
that affects more that 6 million people worldwide. Motor dysfunction gradually
increases as the disease progress. It is usually mild in the early stages of
the disease but it relentlessly progresses to a severe or very severe
disability that is characterized by increasing degrees of bradykinesia,
hypokinesia, muscle rigidity, loss of postural reflexes and balance control as
well as freezing of gait. In addition to a line of treatment based on
dopaminergic PD-specific drugs, attending neurologists strongly recommend
regular exercise combined with physiotherapy. However, the routine of
traditional rehabilitation often create boredom and loss of interest.
Opportunities to liven up a daily exercise schedule may well take the form of
character-based virtual reality games which engage the player to physically
train in a non-linear and looser fashion, providing an experience that varies
from one game loop the next. Such ""exergames"", a word that results from the
amalgamation of the words ""exercise"" and ""game"" challenge patients into
performing movements of varying complexity in a playful and immersive virtual
environment. In fact, today's game consoles using controllers like Nintendo's
Wii, Sony PlayStation Eye and the Microsoft Kinect sensor present new
opportunities to infuse motivation and variety to an otherwise mundane
physiotherapy routine. But are these controllers and the games built for them
appropriate for PD patients? In this paper we present some of these approaches
and discuss their suitability for these patients mainly on the basis of demands
made on balance, agility and gesture precision.","['Ioannis Pachoulakis', 'Nikolaos Papadopoulos', 'Cleanthe Spanaki']",2015-11-11T20:13:56Z,http://arxiv.org/abs/1511.03638v1,['cs.CY'],"Parkinson's Disease,neurodegenerative,movement disorder,bradykinesia,hypokinesia,muscle rigidity,postural reflexes,balance control,freezing of gait,exergames"
CanvoX: High-resolution VR Painting in Large Volumetric Canvas,"With virtual reality, digital painting on 2D canvases is now being extended
to 3D spaces. Tilt Brush and Oculus Quill are widely accepted among artists as
tools that pave the way to a new form of art - 3D emmersive painting. Current
3D painting systems are only a start, emitting textured triangular geometries.
In this paper, we advance this new art of 3D painting to 3D volumetric painting
that enables an artist to draw a huge scene with full control of spatial color
fields. Inspired by the fact that 2D paintings often use vast space to paint
background and small but detailed space for foreground, we claim that
supporting a large canvas in varying detail is essential for 3D painting. In
order to help artists focus and audiences to navigate the large canvas space,
we provide small artist-defined areas, called rooms, that serve as beacons for
artist-suggested scales, spaces, locations for intended appreciation view of
the painting. Artists and audiences can easily transport themselves between
different rooms. Technically, our canvas is represented as an array of deep
octrees of depth 24 or higher, built on CPU for volume painting and on GPU for
volume rendering using accurate ray casting. In CPU side, we design an
efficient iterative algorithm to refine or coarsen octree, as a result of
volumetric painting strokes, at highly interactive rates, and update the
corresponding GPU textures. Then we use GPU-based ray casting algorithms to
render the volumetric painting result. We explore precision issues stemming
from ray-casting the octree of high depth, and provide a new analysis and
verification. From our experimental results as well as the positive feedback
from the participating artists, we strongly believe that our new 3D volume
painting system can open up a new possibility for VR-driven digital art medium
to professional artists as well as to novice users.","['Yeojin Kim', 'Byungmoon Kim', 'Jiyang Kim', 'Young J. Kim']",2017-04-10T06:40:56Z,http://arxiv.org/abs/1704.02724v1,['cs.GR'],"virtual reality,VR painting,3D painting,volumetric canvas,octrees,GPU rendering,ray casting,iterative algorithm,digital art,professional artists"
"First Data Release of the COSMOS Lyman-Alpha Mapping And Tomography
  Observations: 3D Lyman-$α$ Forest Tomography at 2.05 < z < 2.55","Faint star-forming galaxies at $z\sim 2-3$ can be used as alternative
background sources to probe the Lyman-$\alpha$ forest in addition to quasars,
yielding high sightline densities that enable 3D tomographic reconstruction of
the foreground absorption field. Here, we present the first data release from
the COSMOS Lyman-Alpha Mapping And Mapping Observations (CLAMATO) Survey, which
was conducted with the LRIS spectrograph on the Keck-I telescope. Over an
observational footprint of 0.157$\mathrm{deg}^2$ within the COSMOS field, we
used 240 galaxies and quasars at $2.17<z<3.00$, with a mean comoving transverse
separation of $2.37\,h^{-1}\,\mathrm{Mpc^3}$, as background sources probing the
foreground Lyman-$\alpha$ forest absorption at $2.05<z<2.55$. The
Lyman-$\alpha$ forest data was then used to create a Wiener-filtered
tomographic reconstruction over a comoving volume of $3.15\,\times
10^5\,h^{-3}\,\mathrm{Mpc^3}$ with an effective smoothing scale of
$2.5\,h^{-1}\,\mathrm{Mpc}$. In addition to traditional figures, this map is
also presented as a virtual-reality YouTube360 video visualization and
manipulable interactive figure. We see large overdensities and underdensities
that visually agree with the distribution of coeval galaxies from spectroscopic
redshift surveys in the same field, including overdensities associated with
several recently-discovered galaxy protoclusters in the volume. This data
release includes the redshift catalog, reduced spectra, extracted
Lyman-$\alpha$ forest pixel data, and tomographic map of the absorption.","['Khee-Gan Lee', 'Alex Krolewski', 'Martin White', 'David Schlegel', 'Peter E. Nugent', 'Joseph F. Hennawi', 'Thomas Müller', 'Richard Pan', 'J. Xavier Prochaska', 'Andreu Font-Ribera', 'Nao Suzuki', 'Karl Glazebrook', 'Glenn G. Kacprzak', 'Jeyhan S. Kartaltepe', 'Anton M. Koekemoer', 'Olivier Le Févre', 'Brian C. Lemaux', 'Christian Maier', 'Themiya Nanayakkara', 'R. Michael Rich', 'D. B. Sanders', 'Mara Salvato', 'Lidia Tasca', 'Kim-Vy H. Tran']",2017-10-08T22:33:01Z,http://arxiv.org/abs/1710.02894v3,"['astro-ph.CO', 'astro-ph.GA']","Lyman-alpha forest,Tomography,Spectrograph,Quasars,Transverse separation,Comoving volume,Overdensities,Underdensities,Protoclusters,Redshift catalog"
"LookinGood: Enhancing Performance Capture with Real-time Neural
  Re-Rendering","Motivated by augmented and virtual reality applications such as telepresence,
there has been a recent focus in real-time performance capture of humans under
motion. However, given the real-time constraint, these systems often suffer
from artifacts in geometry and texture such as holes and noise in the final
rendering, poor lighting, and low-resolution textures. We take the novel
approach to augment such real-time performance capture systems with a deep
architecture that takes a rendering from an arbitrary viewpoint, and jointly
performs completion, super resolution, and denoising of the imagery in
real-time. We call this approach neural (re-)rendering, and our live system
""LookinGood"". Our deep architecture is trained to produce high resolution and
high quality images from a coarse rendering in real-time. First, we propose a
self-supervised training method that does not require manual ground-truth
annotation. We contribute a specialized reconstruction error that uses semantic
information to focus on relevant parts of the subject, e.g. the face. We also
introduce a salient reweighing scheme of the loss function that is able to
discard outliers. We specifically design the system for virtual and augmented
reality headsets where the consistency between the left and right eye plays a
crucial role in the final user experience. Finally, we generate temporally
stable results by explicitly minimizing the difference between two consecutive
frames. We tested the proposed system in two different scenarios: one involving
a single RGB-D sensor, and upper body reconstruction of an actor, the second
consisting of full body 360 degree capture. Through extensive experimentation,
we demonstrate how our system generalizes across unseen sequences and subjects.
The supplementary video is available at http://youtu.be/Md3tdAKoLGU.","['Ricardo Martin-Brualla', 'Rohit Pandey', 'Shuoran Yang', 'Pavel Pidlypenskyi', 'Jonathan Taylor', 'Julien Valentin', 'Sameh Khamis', 'Philip Davidson', 'Anastasia Tkach', 'Peter Lincoln', 'Adarsh Kowdle', 'Christoph Rhemann', 'Dan B Goldman', 'Cem Keskin', 'Steve Seitz', 'Shahram Izadi', 'Sean Fanello']",2018-11-12T22:51:19Z,http://arxiv.org/abs/1811.05029v1,['cs.CV'],"real-time performance capture,neural re-rendering,deep architecture,completion,super resolution,denoising,self-supervised training,reconstruction error,semantic information,virtual reality,augmented reality"
"Taming the latency in multi-user VR 360$^\circ$: A QoE-aware deep
  learning-aided multicast framework","Immersive virtual reality (VR) applications require ultra-high data rate and
low-latency for smooth operation. Hence in this paper, aiming to improve VR
experience in multi-user VR wireless video streaming, a deep-learning aided
scheme for maximizing the quality of the delivered video chunks with
low-latency is proposed. Therein the correlations in the predicted field of
view (FoV) and locations of viewers watching 360$^\circ$ HD VR videos are
capitalized on to realize a proactive FoV-centric millimeter wave (mmWave)
physical-layer multicast transmission. The problem is cast as a frame quality
maximization problem subject to tight latency constraints and network
stability. The problem is then decoupled into an HD frame request admission and
scheduling subproblems and a matching theory game is formulated to solve the
scheduling subproblem by associating requests from clusters of users to mmWave
small cell base stations (SBSs) for their unicast/multicast transmission.
Furthermore, for realistic modeling and simulation purposes, a real VR
head-tracking dataset and a deep recurrent neural network (DRNN) based on gated
recurrent units (GRUs) are leveraged. Extensive simulation results show how the
content-reuse for clusters of users with highly overlapping FoVs brought in by
multicasting reduces the VR frame delay in 12\%. This reduction is further
boosted by proactiveness that cuts by half the average delays of both reactive
unicast and multicast baselines while preserving HD delivery rates above 98\%.
Finally, enforcing tight latency bounds shortens the delay-tail as evinced by
13\% lower delays in the 99th percentile.","['Cristina Perfecto', 'Mohammed S. Elbamby', 'Javier Del Ser', 'Mehdi Bennis']",2018-11-18T19:43:19Z,http://arxiv.org/abs/1811.07388v2,"['cs.IT', 'math.IT']","latency,VR,deep learning,multicast,mmWave,HD,scheduling,neural network,simulation,QoE"
"Hierarchical Policy Design for Sample-Efficient Learning of Robot Table
  Tennis Through Self-Play","Training robots with physical bodies requires developing new methods and
action representations that allow the learning agents to explore the space of
policies efficiently. This work studies sample-efficient learning of complex
policies in the context of robot table tennis. It incorporates learning into a
hierarchical control framework using a model-free strategy layer (which
requires complex reasoning about opponents that is difficult to do in a
model-based way), model-based prediction of external objects (which are
difficult to control directly with analytic control methods, but governed by
learnable and relatively simple laws of physics), and analytic controllers for
the robot itself. Human demonstrations are used to train dynamics models, which
together with the analytic controller allow any robot that is physically
capable to play table tennis without training episodes. Using only about 7,000
demonstrated trajectories, a striking policy can hit ball targets with about 20
cm error. Self-play is used to train cooperative and adversarial strategies on
top of model-based striking skills trained from human demonstrations. After
only about 24,000 strikes in self-play the agent learns to best exploit the
human dynamics models for longer cooperative games. Further experiments
demonstrate that more flexible variants of the policy can discover new strikes
not demonstrated by humans and achieve higher performance at the expense of
lower sample-efficiency. Experiments are carried out in a virtual reality
environment using sensory observations that are obtainable in the real world.
The high sample-efficiency demonstrated in the evaluations show that the
proposed method is suitable for learning directly on physical robots without
transfer of models or policies from simulation.
  Supplementary material available at
https://sites.google.com/view/robottabletennis","['Reza Mahjourian', 'Risto Miikkulainen', 'Nevena Lazic', 'Sergey Levine', 'Navdeep Jaitly']",2018-11-30T18:27:41Z,http://arxiv.org/abs/1811.12927v2,['cs.RO'],"Hierarchical Policy Design,Sample-Efficient Learning,Robot Table Tennis,Self-Play,Model-Free Strategy,Model-Based Prediction,Analytic Controllers,Human Demonstrations,Self-Play Training,Virtual Reality Environment"
"XR: Enabling training mode in the human brain XR: Enabling training mode
  in the human brain","The face of simulation-based training has greatly evolved, with the most
recent tools giving the ability to create virtual environments that rival
realism. At first glance, it might appear that what the training sector needs
is the most realistic simulators possible, but traditional simulators are not
necessarily the most efficient or practical training tools. With all that these
new technologies have to offer; the challenge is to go back to the core of
training needs and identify the right vector of sensory cues that will most
effectively enable training mode in the human brain. Bigger and Pricier doesn't
necessarily mean better. Simulation with cross-reality content (XR), which by
definition encompasses virtual reality (VR), mixed reality (MR), and augmented
reality (AR), is the most practical solution for deploying any kind of
simulation-based training. The authors of this paper (a teacher and a
technology expert) share their experiences and expose XR-specific best
practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :
Starting his career in the modeling and simulation community more than 15 years
ago, S{\'e}bastien has focused on learning about the latest simulation
innovations and sharing information on how experts have solved their
challenges. He worked on the COTS integration at CAE and the Presagis focusing
on Simulation and Visualization products. More recently, Sebastien put together
simulation and training teams and strategies for emerging companies like CM
Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games,
focusing on helping companies develop real-time solutions for simulation-based
training. Philippe Lepinard: Former military helicopter pilot and simulation
officer, Philippe L{\'e}pinard is now an associate professor at the University
of Paris-Est Cr{\'e}teil (UPEC). His research is focusing on playful learning
and training through simulation. He is one of the founding members of the
French simulation association.","['Philippe Lépinard', 'Sébastien Lozé']",2019-04-26T07:55:39Z,http://arxiv.org/abs/1904.11704v1,"['cs.GR', 'cs.HC']","XR,training mode,simulation-based training,virtual reality,mixed reality,augmented reality,cross-reality content,sensory cues,learning transfer"
"OO-VR: NUMA Friendly Object-Oriented VR Rendering Framework For Future
  NUMA-Based Multi-GPU Systems","With the strong computation capability, NUMA-based multi-GPU system is a
promising candidate to provide sustainable and scalable performance for Virtual
Reality. However, the entire multi-GPU system is viewed as a single GPU which
ignores the data locality in VR rendering during the workload distribution,
leading to tremendous remote memory accesses among GPU models. By conducting
comprehensive characterizations on different kinds of parallel rendering
frameworks, we observe that distributing the rendering object along with its
required data per GPM can reduce the inter-GPM memory accesses. However, this
object-level rendering still faces two major challenges in NUMA-based multi-GPU
system: (1) the large data locality between the left and right views of the
same object and the data sharing among different objects and (2) the unbalanced
workloads induced by the software-level distribution and composition
mechanisms. To tackle these challenges, we propose object-oriented VR rendering
framework (OO-VR) that conducts the software and hardware co-optimization to
provide a NUMA friendly solution for VR multi-view rendering in NUMA-based
multi-GPU systems. We first propose an object-oriented VR programming model to
exploit the data sharing between two views of the same object and group objects
into batches based on their texture sharing levels. Then, we design an object
aware runtime batch distribution engine and distributed hardware composition
unit to achieve the balanced workloads among GPMs. Finally, evaluations on our
VR featured simulator show that OO-VR provides 1.58x overall performance
improvement and 76% inter-GPM memory traffic reduction over the
state-of-the-art multi-GPU systems. In addition, OO-VR provides NUMA friendly
performance scalability for the future larger multi-GPU scenarios with ever
increasing asymmetric bandwidth between local and remote memory.","['Chenhao Xie', 'Xin Fu', 'Mingsong Chen', 'Shuaiwen Leon Song']",2020-01-08T19:44:51Z,http://arxiv.org/abs/2001.03537v1,"['cs.DC', 'cs.GR']","NUMA,Object-Oriented,VR,Rendering Framework,Multi-GPU Systems,Memory Accesses,Data Locality,Workload Distribution,Hardware Composition,Performance Scalability"
"Optimal Wireless Streaming of Multi-Quality 360 VR Video by Exploiting
  Natural, Relative Smoothness-enabled and Transcoding-enabled Multicast
  Opportunities","In this paper, we would like to investigate optimal wireless streaming of a
multi-quality tiled 360 virtual reality (VR) video from a server to multiple
users. To this end, we propose to maximally exploit potential multicast
opportunities by effectively utilizing characteristics of multi-quality tiled
360 VR videos and computation resources at the users' side. In particular, we
consider two requirements for quality variation in one field-of-view (FoV),
i.e., the absolute smoothness requirement and the relative smoothness
requirement, and two video playback modes, i.e., the direct-playback mode
(without user transcoding) and transcode-playback mode (with user transcoding).
Besides natural multicast opportunities, we introduce two new types of
multicast opportunities, namely, relative smoothness-enabled multicast
opportunities, which allow flexible tradeoff between viewing quality and
communications resource consumption, and transcoding-enabled multicast
opportunities, which allow flexible tradeoff between computation and
communications resource consumptions. Then, we establish a novel mathematical
model that reflects the impacts of natural, relative smoothness-enabled and
transcoding-enabled multicast opportunities on the average transmission energy
and transcoding energy. Based on this model, we optimize the transmission
resource allocation, playback quality level selection and transmission quality
level selection to minimize the energy consumption in the four cases with
different requirements for quality variation and video playback modes. By
comparing the optimal values in the four cases, we prove that the energy
consumption reduces when more multicast opportunities can be utilized. Finally,
numerical results show substantial gains of the proposed solutions over
existing schemes, and demonstrate the importance of effective exploitation of
the three types of multicast opportunities.","['Kaixuan Long', 'Ying Cui', 'Chencheng Ye', 'Zhi Liu']",2020-09-02T15:34:33Z,http://arxiv.org/abs/2009.01632v1,"['cs.IT', 'math.IT']","Wireless streaming,Multi-quality,360 VR video,Multicast opportunities,Relative smoothness-enabled,Transcoding-enabled,Quality variation,Field-of-view,Transmission resource allocation"
"How to Improve Your Virtual Experience -- Exploring the Obstacles of
  Mainstream VR","What is Virtual Reality? A professional tool, made to facilitate our everyday
tasks? A conceptual mistake, accompanied by cybersickness and unsolved
locomotion issues since the very beginning? Or just another source of
entertainment that helps us escape from our deteriorating world? The public and
scientific opinions in this respect are diverse. Furthermore, as researchers,
we sometimes ask ourselves whether our work in this area is really ""worth it"",
given the ambiguous prognosis regarding the future of VR. To tackle this
question, we explore three different areas of VR research in this dissertation,
namely locomotion, interaction, and perception. We begin our journey by
structuring VR locomotion and by introducing a novel locomotion concept for
large distance traveling via virtual body resizing. In the second part, we
focus on our interaction possibilities in VR. We learn how to represent virtual
objects via self-transforming controllers and how to store our items in VR
inventories. We design comprehensive 3D gestures for the audience and provide
an I/O abstraction layer to facilitate the realization and usage of such
diverse interaction modalities. The third part is dedicated to the exploration
of perceptual phenomena in VR. In contrast to locomotion and interaction, our
contributions in the field of perception emphasize the strong points of
immersive setups. We utilize VR to transfer the illusion of virtual body
ownership to nonhumanoid avatars and exploit this phenomenon for novel gaming
experiences with animals in the leading role. As one of our contributions, we
demonstrate how to repurpose the dichoptic presentation capability of immersive
setups for preattentive zero-overhead highlighting in visualizations. We round
off the dissertation by coming back to VR research in general, providing a
critical assessment of our contributions and sharing our lessons learned along
the way.",['Andrey Krekhov'],2020-09-09T12:52:27Z,http://arxiv.org/abs/2009.04272v1,['cs.HC'],"Virtual Reality,Obstacles,Mainstream,VR research,Locomotion,Interaction,Perception,Virtual objects,Immersive setups"
3D Reconstruction & Assessment Framework based on affordable 2D Lidar,"Lidar is extensively used in the industry and mass-market. Due to its
measurement accuracy and insensitivity to illumination compared to cameras, It
is applied onto a broad range of applications, like geodetic engineering, self
driving cars or virtual reality. But the 3D Lidar with multi-beam is very
expensive, and the massive measurements data can not be fully leveraged on some
constrained platforms. The purpose of this paper is to explore the possibility
of using cheap 2D Lidar off-the-shelf, to preform complex 3D Reconstruction,
moreover, the generated 3D map quality is evaluated by our proposed metrics at
the end. The 3D map is constructed in two ways, one way in which the scan is
performed at known positions with an external rotary axis at another plane. The
other way, in which the 2D Lidar for mapping and another 2D Lidar for
localization are placed on a trolley, the trolley is pushed on the ground
arbitrarily. The generated maps by different approaches are converted to
octomaps uniformly before the evaluation. The similarity and difference between
two maps will be evaluated by the proposed metrics thoroughly. The whole
mapping system is composed of several modular components. A 3D bracket was made
for assembling of the Lidar with a long range, the driver and the motor
together. A cover platform made for the IMU and 2D Lidar with a shorter range
but high accuracy. The software is stacked up in different ROS packages.","['Xueyang Kang', 'Shengjiong Yin', 'Yinglong Fen']",2018-03-24T22:09:08Z,http://arxiv.org/abs/1803.09167v3,"['cs.RO', '28-06', 'I.2.9, C.0, C.1, C.5']","3D Reconstruction,Assessment Framework,2D Lidar,Lidar,3D Map,Multi-beam,Geodetic Engineering,Virtual Reality,Octomaps,ROS packages"
Unsupervised Predictive Memory in a Goal-Directed Agent,"Animals execute goal-directed behaviours despite the limited range and scope
of their sensors. To cope, they explore environments and store memories
maintaining estimates of important information that is not presently available.
Recently, progress has been made with artificial intelligence (AI) agents that
learn to perform tasks from sensory input, even at a human level, by merging
reinforcement learning (RL) algorithms with deep neural networks, and the
excitement surrounding these results has led to the pursuit of related ideas as
explanations of non-human animal learning. However, we demonstrate that
contemporary RL algorithms struggle to solve simple tasks when enough
information is concealed from the sensors of the agent, a property called
""partial observability"". An obvious requirement for handling partially observed
tasks is access to extensive memory, but we show memory is not enough; it is
critical that the right information be stored in the right format. We develop a
model, the Memory, RL, and Inference Network (MERLIN), in which memory
formation is guided by a process of predictive modeling. MERLIN facilitates the
solution of tasks in 3D virtual reality environments for which partial
observability is severe and memories must be maintained over long durations.
Our model demonstrates a single learning agent architecture that can solve
canonical behavioural tasks in psychology and neurobiology without strong
simplifying assumptions about the dimensionality of sensory input or the
duration of experiences.","['Greg Wayne', 'Chia-Chun Hung', 'David Amos', 'Mehdi Mirza', 'Arun Ahuja', 'Agnieszka Grabska-Barwinska', 'Jack Rae', 'Piotr Mirowski', 'Joel Z. Leibo', 'Adam Santoro', 'Mevlana Gemici', 'Malcolm Reynolds', 'Tim Harley', 'Josh Abramson', 'Shakir Mohamed', 'Danilo Rezende', 'David Saxton', 'Adam Cain', 'Chloe Hillier', 'David Silver', 'Koray Kavukcuoglu', 'Matt Botvinick', 'Demis Hassabis', 'Timothy Lillicrap']",2018-03-28T17:54:01Z,http://arxiv.org/abs/1803.10760v1,"['cs.LG', 'stat.ML']","unsupervised learning,predictive memory,goal-directed agent,reinforcement learning,deep neural networks,partial observability,extensive memory,Memory-RL-Inference Network (MERLIN),virtual reality environments,canonical behavioral tasks"
"CAVE-AR: A VR Authoring System to Interactively Design, Simulate, and
  Debug Multi-user AR Experiences","Despite advances in augmented reality (AR), the process of creating
meaningful experiences with this technology is still extremely challenging. Due
to different tracking implementations and hardware constraints, developing AR
applications either requires low-level programming skills, or is done through
specific authoring tools that largely sacrifice the possibility of customizing
the AR experience. Existing development workflows also do not support
previewing or simulating the AR experience, requiring a lengthy process of
trial and error by which content creators deploy and physically test
applications in each iteration. To mitigate these limitations, we propose
CAVE-AR, a novel virtual reality system for authoring, simulating and debugging
custom AR experiences. Available both as a standalone or a plug-in tool,
CAVE-AR is based on the concept of representing in the same global reference
system both in AR content and tracking information, mixing geographical
information, architectural features, and sensor data to simulate the context of
an AR experience. Thanks to its novel abstraction of existing tracking
technologies, CAVE-AR operates independently of users' devices, and integrates
with existing programming tools to provide maximum flexibility. Our VR
application provides designers with ways to create and modify an AR
application, even while others are in the midst of using it. CAVE-AR further
allows the designer to track how users are behaving, preview what they are
currently seeing, and interact with them through several different channels. To
illustrate our proposed development workflow and demonstrate the advantages of
our authoring system, we introduce two CAVEAR use cases in which an augmented
reality application is created and tested. We compare the CAVE-AR workflow to
traditional development methods and demonstrate the importance of simulation
and live application debugging.","['Marco Cavallo', 'Angus G. Forbes']",2018-09-14T16:52:06Z,http://arxiv.org/abs/1809.05500v2,['cs.HC'],"virtual reality,authoring system,augmented reality,tracking,debugging,AR experiences,simulation,customizing,tracking technologies,development workflow"
"Effects of Different Hand-Grounding Locations on Haptic Performance With
  a Wearable Kinesthetic Haptic Device","Grounding of kinesthetic feedback against a user's hand can increase the
portability and wearability of a haptic device. However, the effects of
different hand-grounding locations on haptic perception of a user are unknown.
In this letter, we investigate the effects of three different hand-grounding
locations-back of the hand, proximal phalanx of the index finger, and middle
phalanx of the index finger-on haptic perception using a newly designed
wearable haptic device. The novel device can provide kinesthetic feedback to
the user's index finger in two directions: along the finger-axis and in the
finger's flexion-extension movement direction. We measure users' haptic
perception for each grounding location through a psychophysical experiment for
each of the two feedback directions. Results show that among the studied
locations, grounding at proximal phalanx has a smaller average just noticeable
difference for both feedback directions, indicating a more sensitive haptic
perception. The realism of the haptic feedback, based on user ratings, was the
highest with grounding at the middle phalanx for feedback along the finger
axis, and at the proximal phalanx for feedback in the flexion-extension
direction. Users identified the haptic feedback as most comfortable with
grounding at the back of the hand for feedback along the finger axis and at the
proximal phalanx for feedback in the flexion-extension direction. These
findings show that the choice of grounding location has a significant impact on
the user's haptic perception and qualitative experience. The results provide
insights for designing next-generation wearable hand-grounded kinesthetic
devices to achieve better haptic performance and user experience in virtual
reality and teleoperated robotic applications.","['Sajid Nisar', 'Melisa Orta Martinez', 'Takahiro Endo', 'Fumitoshi Matsuno', 'Allison M. Okamura']",2019-06-02T15:58:59Z,http://arxiv.org/abs/1906.00430v1,"['cs.RO', 'physics.app-ph']","hand-grounding,haptic performance,wearable device,kinesthetic feedback,proximal phalanx,middle phalanx,haptic perception,psychophysical experiment,just noticeable difference,virtual reality"
"A Mixed VR and Physical Framework to Evaluate Impacts of Virtual Legs
  and Elevated Narrow Working Space on Construction Workers Gait Pattern","It is difficult to conduct training and evaluate workers' postural
performance by using the actual job site environment due to safety concerns.
Virtual reality (VR) provides an alternative to create immersive working
environments without significant safety concerns. Working on elevated surfaces
is a dangerous scenario, which may lead to gait and postural instability and,
consequently, a serious fall. Previous studies showed that VR is a promising
tool for measuring the impact of height on the postural sway. However, most of
these studies used the treadmill as the walking locomotion apparatus in a
virtual environment (VE). This paper was focused on natural walking locomotion
to reduce the inherent postural perturbations of VR devices. To investigate the
impact of virtual height on gait characteristics and keep the level of realism
and feeling of presence at their highest, we enhanced the
first-person-character model with ""virtual legs"". Afterward, we investigated
its effect on the gait parameters of the participants with and without the
presence of height. To that end, twelve healthy adults were asked to walk on a
virtual loop path once at the ground level and once at the 17th floor of an
unfinished structure. By quantitatively comparing the participants' gait
pattern results, we observed a decrease in the stride length and increase in
the gait duration of the participants exposed to height. At the ground level,
the use of the enhanced model reduced participants' average stride length and
height. The results of this study help us understand users' behaviors when they
were exposed to elevated surfaces and establish a firm ground for gait
stability analysis for the future height-related VR studies. We expect this
developed VR platform can generate reliable results of VR application in more
construction safety studies.","['Mahmoud Habibnezhad', 'Jay Puckett', 'Mohammad Sadra Fardhosseini', 'Lucky Agung Pratama']",2019-06-20T14:47:49Z,http://arxiv.org/abs/1906.08670v1,['cs.HC'],"virtual reality,framework,construction workers,gait pattern,immersive environments,postural performance,elevated surfaces,gait characteristics,virtual legs,gait stability"
"Experiencing Extreme Height for The First Time: The Influence of Height,
  Self-Judgment of Fear and a Moving Structural Beam on the Heart Rate and
  Postural Sway During the Quiet Stance","Falling from elevated surfaces is the main cause of death and injury at
construction sites. Based on the Bureau of Labor Statistics (BLS) reports, an
average of nearly three workers per day suffer fatal injuries from falling.
Studies show that postural instability is the foremost cause of this
disproportional falling rate. To study what affects the postural stability of
construction workers, we conducted a series of experiments in the virtual
reality (VR). Twelve healthy adults, all students at the University of Nebraska
were recruited for this study. During each trial, participants heart rates and
postural sways were measured as the dependent factors. The independent factors
included a moving structural beam (MB) coming directly at the participants, the
presence of VR, height, the participants self-judgment of fear, and their level
of acrophobia. The former was designed in an attempt to simulate some part of
the steel erection procedure, which is one of the key tasks of ironworkers. The
results of this study indicate that height increase the postural sway.
Self-judged fear significantly was found to decrease postural sway, more
specifically the normalized total excursion of the center of pressure (TE),
both in the presence and absence of height. Also, participants heart rates
significantly increase once they are confronted by a moving beam in the virtual
environment (VE), even though they are informed that the beam will not hit
them. The findings of this study can be useful for training novice ironworkers
that will be subjected to height and steel erection for the first time.","['Mahmoud Habibnezhad', 'Jay Puckett', 'Mohammad Sadra Fardhosseini', 'Houtan Jebelli', 'Terry Stentz', 'Lucky Agung Pratama']",2019-06-20T15:08:45Z,http://arxiv.org/abs/1906.08682v1,['cs.HC'],"postural instability,construction workers,virtual reality,heart rate,postural sway,acrophobia,steel erection,ironworkers,center of pressure,height"
"Ultrareliable and Low-Latency Communication Techniques for Tactile
  Internet Services","This paper presents novel ultrareliable and low-latency communication (URLLC)
techniques for URLLC services, such as Tactile Internet services. Among typical
use-cases of URLLC services are tele-operation, immersive virtual reality,
cooperative automated driving, and so on. In such URLLC services, new kinds of
traffic such as haptic information including kinesthetic information and
tactile information need to be delivered in addition to high-quality video and
audio traffic in traditional multimedia services. Further, such a variety of
traffic has various characteristics in terms of packet sizes and data rates
with a variety of requirements of latency and reliability. Furthermore, some
traffic may occur in a sporadic manner but require reliable delivery of packets
of medium to large sizes within a low latency, which is not supported by
current state-of-the-art wireless communication systems and is very challenging
for future wireless communication systems. Thus, to meet such a variety of
tight traffic requirements in a wireless communication system, novel
technologies from the physical layer to the network layer need to be devised.
In this paper, some novel physical layer technologies such as waveform
multiplexing, multiple access scheme, channel code design, synchronization, and
full-duplex transmission for spectrally-efficient URLLC are introduced. In
addition, a novel performance evaluation approach, which combines a ray-tracing
tool and system-level simulation, is suggested for evaluating the performance
of the proposed schemes. Simulation results show the feasibility of the
proposed schemes providing realistic URLLC services in realistic geographical
environments, which encourages further efforts to substantiate the proposed
work.","['Kwang Soon Kim', 'Dong Ku Kim', 'Chan-Byoung Chae', 'Sunghyun Choi', 'Young-Chai Ko', 'Jonghyun Kim', 'Yeon-Geun Lim', 'Minho Yang', 'Sundo Kim', 'Byungju Lim', 'Kwanghoon Lee', 'Kyung Lin Ryu']",2019-07-10T01:11:28Z,http://arxiv.org/abs/1907.04474v1,"['cs.IT', 'eess.SP', 'math.IT']","Ultrareliable communication,Low-latency communication,Tactile Internet services,Haptic information,Kinesthetic information,Tactile information,Waveform multiplexing,Multiple access scheme,Channel code design,Full-duplex transmission"
"Dense 3D Reconstruction for Visual Tunnel Inspection using Unmanned
  Aerial Vehicle","Advances in Unmanned Aerial Vehicle (UAV) opens venues for application such
as tunnel inspection. Owing to its versatility to fly inside the tunnels, it
can quickly identify defects and potential problems related to safety. However,
long tunnels, especially with repetitive or uniform structures pose a
significant problem for UAV navigation. Furthermore, post-processing visual
data from the camera mounted on the UAV is required to generate useful
information for the inspection task. In this work, we design a UAV with a
single rotating camera to accomplish the task. Compared to other platforms, our
solution can fit the stringent requirement for tunnel inspection, in terms of
battery life, size and weight. While the current state-of-the-art can estimate
camera pose and 3D geometry from a sequence of images, they assume large
overlap, small rotational motion, and many distinct matching points between
images. These assumptions severely limit their effectiveness in tunnel-like
scenarios where the camera has erratic or large rotational motion, such as the
one mounted on the UAV. This paper presents a novel solution which exploits
Structure-from-Motion, Bundle Adjustment, and available geometry priors to
robustly estimate camera pose and automatically reconstruct a fully-dense 3D
scene using the least possible number of images in various challenging
tunnel-like environments. We validate our system with both Virtual Reality
application and experimentation with a real dataset. The results demonstrate
that the proposed reconstruction along with texture mapping allows for remote
navigation and inspection of tunnel-like environments, even those which are
inaccessible for humans.","['Ramanpreet Singh Pahwa', 'Kennard Yanting Chan', 'Jiamin Bai', 'Vincensius Billy Saputra', 'Minh N. Do', 'Shaohui Foong']",2019-11-09T03:22:10Z,http://arxiv.org/abs/1911.03603v1,"['cs.RO', 'cs.CV']","Dense 3D Reconstruction,Visual Tunnel Inspection,Unmanned Aerial Vehicle,Structure-from-Motion,Bundle Adjustment,Camera Pose,3D Geometry,Tunnel-Like Environments,Texture Mapping,Virtual Reality"
"Emerging Natural User Interfaces in Mobile Computing: A Bottoms-Up
  Survey","Mobile and wearable interfaces and interaction paradigms are highly
constrained by the available screen real estate, and the computational and
power resources. Although there exist many ways of displaying information to
mobile users, inputting data to a mobile device is, usually, limited to a
conventional touch based interaction, that distracts users from their ongoing
activities. Furthermore, emerging applications, like augmented, mixed and
virtual reality (AR/MR/VR), require new types of input methods in order to
interact with complex virtual worlds, challenging the traditional techniques of
Human-Computer Interaction (HCI). Leveraging of Natural User Interfaces (NUIs),
as a paradigm of using natural intuitive actions to interact with computing
systems, is one of many ways to meet these challenges in mobile computing and
its modern applications. Brain-Machine Interfaces that enable thought-only
hands-free interaction, Myoelectric input methods that track body gestures and
gaze-tracking input interfaces - are the examples of NUIs applicable to mobile
and wearable interactions. The wide adoption of wearable devices and the
penetration of mobile technologies, alongside with the growing market of
AR/MR/VR, motivates the exploration and implementation of new interaction
paradigms. The concurrent development of bio-signal acquisition techniques and
accompanying ecosystems offers a useful toolbox to address open challenges. In
this survey, we present state-of-the-art bio-signal acquisition methods,
summarize and evaluate recent developments in the area of NUIs and outline
potential application in mobile scenarios. The survey will provide a bottoms-up
overview starting from (i) underlying biological aspects and signal acquisition
techniques, (ii) portable NUI hardware solutions, (iii) NUI-enabled
applications, as well as (iv) research challenges and open problems.","['Kirill A. Shatilov', 'Dimitris Chatzopoulos', 'Lik-Hang Lee', 'Pan Hui']",2019-11-12T11:13:59Z,http://arxiv.org/abs/1911.04794v2,['cs.HC'],"Natural User Interfaces,Mobile Computing,Wearable Interfaces,Human-Computer Interaction,Augmented Reality,Virtual Reality,Mixed Reality,Brain-Machine Interfaces,Myoelectric input,Gaze-tracking"
"On the assesment of functional connectivity in an immersive
  brain-computer interface during motor imagery","New trends on brain-computer interface (BCI) design are aiming to combine
this technology with immersive virtual reality in order to provide a sense of
realism to its users. In this study, we propose an experimental BCI to control
an immersive telepresence system using motor imagery (MI). The system is
immersive in the sense that the users can control the movement of a NAO
humanoid robot in a first person perspective (1PP), i.e., as if the movement of
the robot was his/her own. We analyze functional brain connectivity between 1PP
and 3PP during the control of our BCI using graph theory properties such as
degree, betweenness centrality, and efficiency. Changes in these metrics are
obtained for the case of the 1PP, as well as for the traditional third person
perspective (3PP) in which the user can see the movement of the robot as
feedback. As proof-of-concept, electroencephalography (EEG) signals were
recorded from two subjects while they performed MI to control the movement of
the robot. The graph theoretical analysis was applied to the binary directed
networks obtained through the partial directed coherence (PDC). In our
preliminary assessment we found that the efficiency in the alpha brain rhythm
is greater in 1PP condition in comparison to the 3PP at the prefrontal cortex.
Also, a stronger influence of signals measured at EEG channel C3 (primary motor
cortex) to other regions was found in 1PP condition. Furthermore, our
preliminary results seem to indicate that alpha and beta brain rhythms have a
high indegree at prefrontal cortex in 1PP condition, and this could be possibly
related to the experience of sense of agency. Therefore, using the PDC combined
with graph theory while controlling a telepresence robot in an immersive system
may contribute to understand the organization and behavior of brain networks in
these environments.","['Myriam Alanis-Espinosa', 'David Gutiérrez']",2019-12-05T16:58:21Z,http://arxiv.org/abs/1912.02723v1,"['eess.SP', 'q-bio.NC']","brain-computer interface,functional connectivity,immersive,motor imagery,virtual reality,graph theory,electroencephalography,alpha brain rhythm,beta brain rhythm,sense of agency"
"Toward Better Understanding of Saliency Prediction in Augmented 360
  Degree Videos","Augmented reality (AR) overlays digital content onto the reality. In AR
system, correct and precise estimations of user's visual fixations and head
movements can enhance the quality of experience by allocating more computation
resources on the areas of interest. However, there is inadequate research about
understanding the visual exploration of users when using an AR system or
modeling AR visual attention. To bridge the gap between the saliency prediction
on real-world scene and on scene augmented by virtual information, we construct
the ARVR saliency dataset with 12 diverse videos viewed by 20 people. The
virtual reality (VR) technique is employed to simulate the real-world.
Annotations of object recognition and tracking as augmented contents are
blended into the omnidirectional videos. The saliency annotations of head and
eye movements for both original and augmented videos are collected and together
constitute the ARVR dataset. We also design a model which is capable of solving
the saliency prediction problem in AR. Local block images are extracted to
simulate the viewport and offset the projection distortion. Conspicuous visual
cues in local viewports are extracted to constitute the spatial features. The
optical flow information is estimated as the important temporal feature. We
also consider the interplay between virtual information and reality. The
composition of the augmentation information is distinguished, and the joint
effects of adversarial augmentation and complementary augmentation are
estimated. We generate a graph by taking each block image as one node. Both the
visual saliency mechanism and the characteristics of viewing behaviors are
considered in the computation of edge weights on the graph which are
interpreted as Markov chains. The fraction of the visual attention that is
diverted to each block image is estimated through equilibrium distribution on
of this chain.","['Yucheng Zhu', 'Xiongkuo Min', 'DanDan Zhu', 'Ke Gu', 'Jiantao Zhou', 'Guangtao Zhai', 'Xiaokang Yang', 'Wenjun Zhang']",2019-12-12T14:16:05Z,http://arxiv.org/abs/1912.05971v2,"['eess.IV', 'cs.CV', 'cs.HC']","saliency prediction,augmented reality,visual fixations,head movements,AR system,AR visual attention,ARVR dataset,virtual reality technique,object recognition,tracking,optical flow information"
"Operation of a Brain-Computer Interface Walking Simulator by Users with
  Spinal Cord Injury","Background: Spinal cord injury (SCI) can leave the affected individuals
unable to ambulate. Since there are no restorative treatments for SCI, novel
approaches such as brain-controlled prostheses have been sought. Our recent
studies show that a brain-computer interface (BCI) can be used to control
ambulation within a virtual reality environment (VRE), suggesting that a
BCI-controlled lower extremity prosthesis for ambulation may be feasible.
However, the operability of our BCI has not been tested in a SCI population.
  Methods: Five subjects with paraplegia or tetraplegia due to SCI underwent a
10-min training session in which they alternated between kinesthetic motor
imagery (KMI) of idling and walking while their electroencephalogram (EEG) were
recorded. Subjects then performed a goal-oriented online task, where they
utilized KMI to control the linear ambulation of an avatar and make 10
sequential stops at designated points within the VRE. Multiple online trials
were performed over 5 experimental days.
  Results: Classification accuracy of idling and walking was estimated offline
and ranged from 60.5% (p=0.0176) to 92.3% (p=1.36*10^-20) across subjects and
days. In the online task, all subjects achieved purposeful control with an
average performance of 7.4 +/- 2.3 successful stops in 273 +/- 51 sec (p<0.01).
All subjects maintained purposeful control throughout the study, and their
online performances improved over time.
  Conclusions: The results demonstrate that SCI subjects can purposefully
operate a self-paced BCI walking simulator to complete a goal-oriented
ambulation task. The operation of this BCI system requires short training, is
intuitive, and robust against subject-to-subject and day-to-day
neurophysiological variations. These findings indicate that BCI-controlled
lower extremity prostheses for gait rehabilitation or restoration after SCI may
be feasible in the future.","['Christine E. King', 'Po T. Wang', 'Luis A. Chui', 'An H. Do', 'Zoran Nenadic']",2012-09-10T00:01:23Z,http://arxiv.org/abs/1209.1859v1,"['cs.HC', 'q-bio.NC']","Brain-computer interface,Spinal cord injury,Ambulation,Virtual reality environment,Kinesthetic motor imagery,Electroencephalogram,Classification accuracy,Purposeful control,Self-paced,Gait rehabilitation"
"Human-in-the-Loop Wireless Communications: Machine Learning and
  Brain-Aware Resource Management","Human-centric applications such as virtual reality and immersive gaming will
be central to the future wireless networks. Common features of such services
include: a) their dependence on the human user's behavior and state, and b)
their need for more network resources compared to conventional cellular
applications. To successfully deploy such applications over wireless and
cellular systems, the network must be made cognizant of not only the
quality-of-service (QoS) needs of the applications, but also of the perceptions
of the human users on this QoS. In this paper, by explicitly modeling the
limitations of the human brain, a concrete measure for the delay perception of
human users in a wireless network is introduced. Then, a novel learning method,
called probability distribution identification, is proposed to find a
probabilistic model for this delay perception based on the brain features of a
human user. The proposed learning method uses both supervised and unsupervised
learning techniques to build a Gaussian mixture model of the human brain
features. Given a model for the delay perception of the human brain, a novel
brain-aware resource management algorithm based on Lyapunov optimization is
proposed for allocating radio resources to human users while minimizing the
transmit power and taking into account the reliability of both machine type
devices and human users. The proposed algorithm is shown to have a low
complexity. Moreover, a closed-form relationship between the reliability
measure and wireless physical layer metrics of the network is derived.
Simulation results using real data from actual human users show that a
brain-aware approach can yield savings of up to 78% in power compared to the
system","['Ali Taleb Zadeh Kasgari', 'Walid Saad', 'Merouane Debbah']",2018-03-31T20:07:37Z,http://arxiv.org/abs/1804.00209v2,"['cs.IT', 'eess.SP', 'math.IT']","wireless communications,machine learning,brain-aware resource management,quality-of-service (QoS),delay perception,probability distribution identification,Gaussian mixture model,Lyapunov optimization,transmit power,reliability measure"
"Echo-Liquid State Deep Learning for $360^\circ$ Content Transmission and
  Caching in Wireless VR Networks with Cellular-Connected UAVs","In this paper, the problem of content caching and transmission is studied for
a wireless virtual reality (VR) network in which unmanned aerial vehicles
(UAVs) capture videos on live games or sceneries and transmit them to small
base stations (SBSs) that service the VR users. However, due to its limited
capacity, the wireless network may not be able to meet the delay requirements
of such 360 content transmissions. To meet the VR delay requirements, the UAVs
can extract specific visible content (e.g., user field of view) from the
original 360 data and send this visible content to the users so as to reduce
the traffic load over backhaul and radio access links. To further alleviate the
UAV-SBS backhaul traffic, the SBSs can also cache the popular contents that
users request. This joint content caching and transmission problem is
formulated as an optimization problem whose goal is to maximize the users'
reliability, defined as the probability that the content transmission delay of
each user satisfies the instantaneous VR delay target. To address this problem,
a distributed deep learning algorithm that brings together new neural network
ideas from liquid state machine (LSM) and echo state networks (ESNs) is
proposed. The proposed algorithm enables each SBS to predict the users'
reliability so as to find the optimal contents to cache and content
transmission format for each UAV. Analytical results are derived to expose the
various network factors that impact content caching and content transmission
format selection. Simulation results show that the proposed algorithm yields
25.4% gain of reliability compared to Q-learning. The results also show that
the proposed algorithm can achieve 14.7% gain of reliability due to the
reduction of traffic load over backhaul compared to the proposed algorithm with
random caching.","['Mingzhe Chen', 'Walid Saad', 'Changchuan Yin']",2018-04-10T00:25:37Z,http://arxiv.org/abs/1804.03284v1,"['cs.IT', 'math.IT']","Echo-Liquid State Deep Learning,Content Caching,Transmission,Wireless VR Networks,Cellular,UAVs,Neural Network,Liquid State Machine,Echo State Networks"
"All Reality: Virtual, Augmented, Mixed (X), Mediated (X,Y), and
  Multimediated Reality","The contributions of this paper are: (1) a taxonomy of the ""Realities""
(Virtual, Augmented, Mixed, Mediated, etc.), and (2) some new kinds of
""reality"" that come from nature itself, i.e. that expand our notion beyond
synthetic realities to include also phenomenological realities.
  VR (Virtual Reality) replaces the real world with a simulated experience
(virtual world). AR (Augmented Reality) allows a virtual world to be
experienced while also experiencing the real world at the same time. Mixed
Reality provides blends that interpolate between real and virtual worlds in
various proportions, along a ""Virtuality"" axis, and extrapolate to an ""X-axis"".
Mediated Reality goes a step further by mixing/blending and also modifying
reality. This modifying of reality introduces a second axis. Mediated Reality
is useful as a seeing aid (e.g. modifying reality to make it easier to
understand), and for psychology experiments like Stratton's 1896 upside-down
eyeglasses experiment.
  We propose Multimediated Reality as a multidimensional multisensory mediated
reality that includes not just interactive multimedia-based reality for our
five senses, but also includes additional senses (like sensory sonar, sensory
radar, etc.), as well as our human actions/actuators. These extra senses are
mapped to our human senses using synthetic synesthesia. This allows us to
directly experience real (but otherwise invisible) phenomena, such as wave
propagation and wave interference patterns, so that we can see radio waves and
sound waves and how they interact with objects and each other. Multimediated
reality is multidimensional, multimodal, multisensory, and multiscale. It is
also multidisciplinary, in that we must consider not just the user, but also
how the technology affects others, e.g. how its physical appearance affects
social situations.","['Steve Mann', 'Tom Furness', 'Yu Yuan', 'Jay Iorio', 'Zixin Wang']",2018-04-20T15:40:39Z,http://arxiv.org/abs/1804.08386v1,['cs.HC'],"Virtual Reality,Augmented Reality,Mixed Reality,Mediated Reality,Multimediated Reality,Synthetic Reality,Phenomenological Reality,Virtuality axis,X-axis,Multisensory"
Visual Distortions in 360-degree Videos,"Omnidirectional (or 360-degree) images and videos are emergent signals in
many areas such as robotics and virtual/augmented reality. In particular, for
virtual reality, they allow an immersive experience in which the user is
provided with a 360-degree field of view and can navigate throughout a scene,
e.g., through the use of Head Mounted Displays. Since it represents the full
360-degree field of view from one point of the scene, omnidirectional content
is naturally represented as spherical visual signals. Current approaches for
capturing, processing, delivering, and displaying 360-degree content, however,
present many open technical challenges and introduce several types of
distortions in these visual signals. Some of the distortions are specific to
the nature of 360-degree images, and often different from those encountered in
the classical image communication framework. This paper provides a first
comprehensive review of the most common visual distortions that alter
360-degree signals undergoing state of the art processing in common
applications. While their impact on viewers' visual perception and on the
immersive experience at large is still unknown ---thus, it stays an open
research topic--- this review serves the purpose of identifying the main causes
of visual distortions in the end-to-end 360-degree content distribution
pipeline. It is essential as a basis for benchmarking different processing
techniques, allowing the effective design of new algorithms and applications.
It is also necessary to the deployment of proper psychovisual studies to
characterise the human perception of these new images in interactive and
immersive applications.","['Roberto G. de A. Azevedo', 'Neil Birkbeck', 'Francesca De Simone', 'Ivan Janatra', 'Balu Adsumilli', 'Pascal Frossard']",2019-01-07T14:52:28Z,http://arxiv.org/abs/1901.01848v1,['cs.MM'],"360-degree videos,visual distortions,omnidirectional images,virtual reality,immersive experience,Head Mounted Displays,spherical visual signals,processing techniques,psychovisual studies,human perception"
"Deep Coarse-to-fine Dense Light Field Reconstruction with Flexible
  Sampling and Geometry-aware Fusion","A densely-sampled light field (LF) is highly desirable in various
applications, such as 3-D reconstruction, post-capture refocusing and virtual
reality. However, it is costly to acquire such data. Although many
computational methods have been proposed to reconstruct a densely-sampled LF
from a sparsely-sampled one, they still suffer from either low reconstruction
quality, low computational efficiency, or the restriction on the regularity of
the sampling pattern. To this end, we propose a novel learning-based method,
which accepts sparsely-sampled LFs with irregular structures, and produces
densely-sampled LFs with arbitrary angular resolution accurately and
efficiently. We also propose a simple yet effective method for optimizing the
sampling pattern. Our proposed method, an end-to-end trainable network,
reconstructs a densely-sampled LF in a coarse-to-fine manner. Specifically, the
coarse sub-aperture image (SAI) synthesis module first explores the scene
geometry from an unstructured sparsely-sampled LF and leverages it to
independently synthesize novel SAIs, in which a confidence-based blending
strategy is proposed to fuse the information from different input SAIs, giving
an intermediate densely-sampled LF. Then, the efficient LF refinement module
learns the angular relationship within the intermediate result to recover the
LF parallax structure. Comprehensive experimental evaluations demonstrate the
superiority of our method on both real-world and synthetic LF images when
compared with state-of-the-art methods. In addition, we illustrate the benefits
and advantages of the proposed approach when applied in various LF-based
applications, including image-based rendering and depth estimation enhancement.","['Jing Jin', 'Junhui Hou', 'Jie Chen', 'Huanqiang Zeng', 'Sam Kwong', 'Jingyi Yu']",2019-08-31T05:16:21Z,http://arxiv.org/abs/1909.01341v3,"['eess.IV', 'cs.CV']","deep learning,light field reconstruction,sampling,geometry-aware fusion,coarse-to-fine,computational efficiency,angular resolution,end-to-end trainable network,parallax structure"
Heterogeneous Dataflow Accelerators for Multi-DNN Workloads,"Emerging AI-enabled applications such as augmented/virtual reality (AR/VR)
leverage multiple deep neural network (DNN) models for sub-tasks such as object
detection, hand tracking, and so on. Because of the diversity of the sub-tasks,
the layers within and across the DNN models are highly heterogeneous in
operation and shape. Such layer heterogeneity is a challenge for a fixed
dataflow accelerator (FDA) that employs a fixed dataflow on a single
accelerator substrate since each layer prefers different dataflows (computation
order and parallelization) and tile sizes. Reconfigurable DNN accelerators
(RDAs) have been proposed to adapt their dataflows to diverse layers to address
the challenge. However, the dataflow flexibility in RDAs is enabled at the area
and energy costs of expensive hardware structures (switches, controller, etc.)
and per-layer reconfiguration.
  Alternatively, this work proposes a new class of accelerators, heterogeneous
dataflow accelerators (HDAs), which deploys multiple sub-accelerators each
supporting a different dataflow. HDAs enable coarser-grained dataflow
flexibility than RDAs with higher energy efficiency and lower area cost
comparable to FDAs. To exploit such benefits, hardware resource partitioning
across sub-accelerators and layer execution schedule need to be carefully
optimized. Therefore, we also present Herald, which co-optimizes hardware
partitioning and layer execution schedule. Using Herald on a suite of AR/VR and
MLPerf workloads, we identify a promising HDA architecture, Maelstrom, which
demonstrates 65.3% lower latency and 5.0% lower energy than the best FDAs and
22.0% lower energy at the cost of 20.7% higher latency than a state-of-the-art
RDA. The results suggest that HDA is an alternative class of Pareto-optimal
accelerators to RDA with strength in energy, which can be a better choice than
RDAs depending on the use cases.","['Hyoukjun Kwon', 'Liangzhen Lai', 'Michael Pellauer', 'Tushar Krishna', 'Yu-Hsin Chen', 'Vikas Chandra']",2019-09-13T17:46:13Z,http://arxiv.org/abs/1909.07437v4,['cs.DC'],"Heterogeneous Dataflow Accelerators,Multi-DNN Workloads,Deep Neural Network (DNN) Models,Fixed Dataflow Accelerator (FDA),Reconfigurable DNN Accelerators (RDAs),Hardware Resource Partitioning,Layer Execution Schedule,Energy Efficiency,Latency."
"Increasing the Quality of 360° Video Streaming by Transitioning
  between Viewport Quality Adaptation Mechanisms","Virtual reality has been gaining popularity in recent years caused by the
proliferation of affordable consumer-grade devices such as Oculus Rift, HTC
Vive, and Samsung VR. Amongst the various VR applications, 360{\deg} video
streaming is currently one of the most popular ones. It allows user to change
their field-of-view (FoV) based on head movement, which enables them to freely
select an area anywhere from the sphere the video is (virtually) projected to.
While 360{\deg} video streaming offers new exciting ways of consuming content
for viewers, it poses a series of challenges to the systems that are
responsible for the distribution of such content from the origin to the viewer.
One challenge is the significantly increased bandwidth requirement for
streaming such content in real time. Recent research has shown that only
streaming the content that is in the user's FoV in high quality can lead to
strong bandwidth savings. This can be achieved by analyzing the viewers head
orientation and movement based on sensor information. Alternatively, historic
information from users that watched the content in the past can be taken into
account to prefetch 360{\deg} video data in high quality assuming the viewer
will direct the FoV to these areas. In this paper, we present a 360{\deg} video
streaming system that transitions between sensor- and content-based predictive
mechanisms. We evaluate the effects of this transition-based approach on the
Quality of Experience (QoE) of such a VR streaming system and show that the
perceived quality can be increased between 50\% and 80\% compared to systems
that only apply either one of the two approaches.","['Christian Koch', 'Arne-Tobias Rak', 'Michael Zink', 'Ralf Steinmetz', 'Amr Rizk']",2019-10-06T08:36:28Z,http://arxiv.org/abs/1910.02397v1,['cs.NI'],"Quality of Experience,360° video streaming,Virtual Reality,Viewport,Adaptation Mechanisms,Bandwidth,Sensor Information,Predictive Mechanisms,Head Movement,Field-of-View"
"Channel load aware AP / Extender selection in Home WiFi networks using
  IEEE 802.11k/v","Next-generation Home WiFi networks have to step forward in terms of
performance. New applications such as on-line games, virtual reality or high
quality video contents will further demand higher throughput levels, as well as
low latency. Beyond physical (PHY) and medium access control (MAC)
improvements, deploying multiple access points (APs) in a given area may
significantly contribute to achieve those performance goals by simply improving
average coverage and data rates. However, it opens a new challenge: to
determine the best AP for each given station (STA).
  This article studies the achievable performance gains of using secondary APs,
also called Extenders, in Home WiFi networks in terms of throughput and delay.
To do that, we introduce a centralized, easily implementable channel load aware
selection mechanism for WiFi networks that takes full advantage of IEEE
802.11k/v capabilities to collect data from STAs, and distribute association
decisions accordingly. These decisions are completely computed in the AP (or,
alternatively, in an external network controller) based on an AP selection
decision metric that, in addition to RSSI, also takes into account the load of
both access and backhaul wireless links for each potential STA-AP/Extender
connection.
  Performance evaluation of the proposed channel load aware AP and Extender
selection mechanism has been first conducted in a purpose-built simulator,
resulting in an overall improvement of the main analyzed metrics (throughput
and delay) and the ability to serve, at least, 35% more traffic while keeping
the network uncongested when compared to the traditional RSSI-based WiFi
association. This trend was confirmed when the channel load aware mechanism was
tested in a real deployment, where STAs were associated to the indicated
AP/Extender and total throughput was increased by 77.12%.","['Toni Adame', 'Marc Carrascosa', 'Boris Bellalta', 'Iván Pretel', 'Iñaki Etxebarria']",2020-04-17T08:34:17Z,http://arxiv.org/abs/2004.08110v2,['cs.NI'],"Channel load aware,AP selection,Extender selection,Home WiFi networks,IEEE 802.11k/v,throughput,latency,WiFi association,RSSI,performance evaluation"
Human and Machine Action Prediction Independent of Object Information,"Predicting other people's action is key to successful social interactions,
enabling us to adjust our own behavior to the consequence of the others' future
actions. Studies on action recognition have focused on the importance of
individual visual features of objects involved in an action and its context.
Humans, however, recognize actions on unknown objects or even when objects are
imagined (pantomime). Other cues must thus compensate the lack of recognizable
visual object features. Here, we focus on the role of inter-object relations
that change during an action. We designed a virtual reality setup and tested
recognition speed for 10 different manipulation actions on 50 subjects. All
objects were abstracted by emulated cubes so the actions could not be inferred
using object information. Instead, subjects had to rely only on the information
that comes from the changes in the spatial relations that occur between those
cubes. In spite of these constraints, our results show the subjects were able
to predict actions in, on average, less than 64% of the action's duration. We
employed a computational model -an enriched Semantic Event Chain (eSEC)-
incorporating the information of spatial relations, specifically (a) objects'
touching/untouching, (b) static spatial relations between objects and (c)
dynamic spatial relations between objects. Trained on the same actions as those
observed by subjects, the model successfully predicted actions even better than
humans. Information theoretical analysis shows that eSECs optimally use
individual cues, whereas humans presumably mostly rely on a mixed-cue strategy,
which takes longer until recognition. Providing a better cognitive basis of
action recognition may, on one hand improve our understanding of related human
pathologies and, on the other hand, also help to build robots for conflict-free
human-robot cooperation. Our results open new avenues here.","['Fatemeh Ziaeetabar', 'Jennifer Pomp', 'Stefan Pfeiffer', 'Nadiya El-Sourani', 'Ricarda I. Schubotz', 'Minija Tamosiunaite', 'Florentin Wörgötter']",2020-04-22T12:13:25Z,http://arxiv.org/abs/2004.10518v1,"['cs.AI', 'cs.CV']","action prediction,object information,inter-object relations,virtual reality,manipulation actions,computational model,Semantic Event Chain,spatial relations,information theoretical analysis,human-robot cooperation"
"Can Terahertz Provide High-Rate Reliable Low Latency Communications for
  Wireless VR?","Wireless virtual reality (VR) imposes new visual and haptic requirements that
are directly linked to the quality-of-experience (QoE) of VR users. These QoE
requirements can only be met by wireless connectivity that offers high-rate and
high-reliability low latency communications (HRLLC), unlike the low rates
usually considered in vanilla ultra-reliable low latency communication
scenarios. The high rates for VR over short distances can only be supported by
an enormous bandwidth, which is available in terahertz (THz) frequency bands.
Guaranteeing HRLLC requires dealing with the uncertainty that is specific to
the THz channel. To explore the potential of THz for meeting HRLLC
requirements, a quantification of the risk for an unreliable VR performance is
conducted through a novel and rigorous characterization of the tail of the
end-to-end (E2E) delay. Then, a thorough analysis of the tail-value-atrisk
(TVaR) is performed to concretely characterize the behavior of extreme wireless
events crucial to the real-time VR experience. System reliability for scenarios
with guaranteed line-of-sight (LoS) is then derived as a function of THz
network parameters after deriving a novel expression for the probability
distribution function of the THz transmission delay. Numerical results show
that abundant bandwidth and low molecular absorption are necessary to improve
the reliability. However, their effect remains secondary compared to the
availability of LoS, which significantly affects the THz HRLLC performance. In
particular, for scenarios with guaranteed LoS, a reliability of 99.999% (with
an E2E delay threshold of 20 ms) for a bandwidth of 15 GHz along with data
rates of 18.3 Gbps can be achieved by the THz network (operating at a frequency
of 1 THz), compared to a reliability of 96% for twice the bandwidth, when
blockages are considered.","['Christina Chaccour', 'Mehdi Naderi Soorki', 'Walid Saad', 'Mehdi Bennis', 'Petar Popovski']",2020-05-01T03:15:46Z,http://arxiv.org/abs/2005.00536v2,"['cs.IT', 'math.IT']","Terahertz,High-rate,Reliable,Low latency communications,Wireless VR,Quality-of-experience,Bandwidth,Line-of-sight,Molecular absorption,End-to-end delay"
"Towards Fine-grained Human Pose Transfer with Detail Replenishing
  Network","Human pose transfer (HPT) is an emerging research topic with huge potential
in fashion design, media production, online advertising and virtual reality.
For these applications, the visual realism of fine-grained appearance details
is crucial for production quality and user engagement. However, existing HPT
methods often suffer from three fundamental issues: detail deficiency, content
ambiguity and style inconsistency, which severely degrade the visual quality
and realism of generated images. Aiming towards real-world applications, we
develop a more challenging yet practical HPT setting, termed as Fine-grained
Human Pose Transfer (FHPT), with a higher focus on semantic fidelity and detail
replenishment. Concretely, we analyze the potential design flaws of existing
methods via an illustrative example, and establish the core FHPT methodology by
combing the idea of content synthesis and feature transfer together in a
mutually-guided fashion. Thereafter, we substantiate the proposed methodology
with a Detail Replenishing Network (DRN) and a corresponding coarse-to-fine
model training scheme. Moreover, we build up a complete suite of fine-grained
evaluation protocols to address the challenges of FHPT in a comprehensive
manner, including semantic analysis, structural detection and perceptual
quality assessment. Extensive experiments on the DeepFashion benchmark dataset
have verified the power of proposed benchmark against start-of-the-art works,
with 12\%-14\% gain on top-10 retrieval recall, 5\% higher joint localization
accuracy, and near 40\% gain on face identity preservation. Moreover, the
evaluation results offer further insights to the subject matter, which could
inspire many promising future works along this direction.","['Lingbo Yang', 'Pan Wang', 'Chang Liu', 'Zhanning Gao', 'Peiran Ren', 'Xinfeng Zhang', 'Shanshe Wang', 'Siwei Ma', 'Xiansheng Hua', 'Wen Gao']",2020-05-26T03:05:23Z,http://arxiv.org/abs/2005.12494v2,['cs.CV'],"Human pose transfer,Detail replenishing network,Fine-grained,Semantic fidelity,Feature transfer,Fine-grained evaluation protocols,DeepFashion benchmark dataset,Joint localization accuracy,Face identity preservation."
"A micromirror array with annular partitioning for high-speed
  random-access axial focusing","Dynamic axial focusing functionality has recently experienced widespread
incorporation in microscopy, augmented/virtual reality (AR/VR), adaptive
optics, and material processing. However, the limitations of existing varifocal
tools continue to beset the performance capabilities and operating overhead of
the optical systems that mobilize such functionality. The varifocal tools that
are the least burdensome to drive (ex: liquid crystal, elastomeric or
optofluidic lenses) suffer from low (~ 100 Hz) refresh rates. Conversely, the
fastest devices sacrifice either critical capabilities such as their dwelling
capacity (ex: acoustic gradient lenses or monolithic micromechanical mirrors)
or low operating overhead (e.g., deformable mirrors). Here, we present a
general-purpose random-access axial focusing device that bridges these
previously conflicting features of high speed, dwelling capacity and
lightweight drive by employing low-rigidity micromirrors that exploit the
robustness of defocusing phase profiles. Geometrically, the device consists of
an 8.2 mm diameter array of piston-motion and 48 um-pitch micromirror pixels
that provide 2pi phase shifting for wavelengths shorter than 1 100 nm with
10-90 % settling in 64.8 us (i.e., 15.44 kHz refresh rate). The pixels are
electrically partitioned into 32 rings for a driving scheme that enables
phase-wrapped operation with circular symmetry and requires less than 30 V per
channel. Optical experiments demonstrated the array's wide focusing range with
a measured ability to target 29 distinct, resolvable depth planes. Overall, the
features of the proposed array offer the potential for compact, straightforward
methods of tackling bottlenecked applications including high-throughput
single-cell targeting in neurobiology and the delivery of dense 3D visual
information in AR/VR.","['Nathan Tessema Ersumo', 'Cem Yalcin', 'Nick Antipa', 'Nicolas Pegard', 'Laura Waller', 'Daniel Lopez', 'Rikky Muller']",2020-06-14T03:10:23Z,http://arxiv.org/abs/2006.07779v3,"['physics.optics', 'physics.app-ph']","micromirror array,axial focusing,random-access,varifocal tools,refresh rates,micromechanical mirrors,phase shifting,settling time,partitioned,driving scheme"
"Metabolomic measures of altered energy metabolism mediate the
  relationship of inflammatory miRNAs to motor control in collegiate football
  athletes","Recent research has shown there can be detrimental neurological effects of
short- and long-term exposure to contact sports. In the present study,
metabolomic profiling was combined with inflammatory miRNA quantification,
computational behavior with virtual reality (VR) testing of motor control, and
head collision event monitoring to explore trans-omic and collision effects on
human behavior across a season of players on a collegiate American football
team. We integrated permutation-based statistics with mediation analyses to
test complex, directional relationships between miRNAs, metabolites, and VR
task performance. Fourteen significant mediations (metabolite = mediator; miRNA
= independent variable; VR score = dependent variable) were discovered at
preseason (N=6) and across season (N=8) with Sobel p-values less than or equal
to 0.05 and with total effects at or exceeding 50%. The majority of mediation
findings involved long to medium chain fatty acids (2-HG, 8-HOA, UND, sebacate,
suberate, and heptanoate). In parallel, TCA metabolites were found to be
significantly decreased at postseason relative to preseason. HAEs were
associated with metabolomic measures and miRNA levels across-season. Together,
these observations suggest a state of chronic HAE-induced neuroinflammation (as
evidence by elevated miRNAs) and mitochondrial dysfunction (as observed by
abnormal FAs and TCA metabolites) that together produce subtle changes in
neurological function (as observed by impaired motor control behavior). These
findings point to a shift in mitochondrial metabolism, away from mitochondria
function, consistent with other illnesses classified as mitochondrial
disorders, suggesting a plausible mechanism underlying HAEs in contact sports
and potential avenue for treatment intervention.","['Nicole L. Vike', 'Sumra Bari', 'Khrystyna Stetsiv', 'Linda Papa', 'Eric A. Nauman', 'Thomas M. Talavage', 'Semyon Slobounov', 'Hans C. Breiter']",2020-06-23T18:43:24Z,http://arxiv.org/abs/2006.13264v1,['q-bio.QM'],"metabolomic profiling,inflammatory miRNA,motor control,virtual reality testing,head collision events,fatty acids,TCA metabolites,neuroinflammation,mitochondrial dysfunction,mitochondrial disorders"
"A metabolomic measure of energy metabolism moderates how an inflammatory
  miRNA relates to rs-fMRI network and motor control in football athletes","Collision sports athletes experience many head acceleration events (HAEs) per
season. The effects of these subconcussive events are largely understudied
since HAEs may produce no overt symptoms, and are likely to diffusely manifest
across multiple scales of study (e.g., molecular, cellular network, and
behavior). This study integrated resting-state fMRI with metabolome,
transcriptome and computational virtual reality (VR) behavior measures to
assess the effects of exposure to HAEs on players in a collegiate American
football team. Permutation-based mediation and moderation analysis was used to
investigate relationships between network fingerprint, changes in omic measures
and VR metrics over the season. Change in an energy cycle fatty acid,
tridecenedioate, moderated the relationship between 1) miR-505 and DMN
fingerprint and 2) the relationship between DMN fingerprint and worsening VR
Balance measures (all p less than or equal to 0.05). In addition, the
similarity in DMN over the season was negatively related to cumulative number
of HAEs above 80G, and DMN fingerprint was less similar across the season in
athletes relative to age-matched non-athletes. miR-505 was also positively
related to average number of HAEs above 25G per session. It is important to
note that tridecenedioate has a double bond making it a candidate for ROS
scavenging. These findings between a candidate ROS-related metabolite,
inflammatory miRNA, altered brain imaging and diminished behavioral performance
suggests that impact athletes may experience chronic neuroinflammation. The
rigorous permutation-based mediation/moderation may provide a methodology for
investigating complex multi-scale biological data within humans alone and thus
assist study of other functional brain problems.","['Sumra Bari', 'Nicole L. Vike', 'Khrystyna Stetsiv', 'Linda Papa', 'Eric A. Nauman', 'Thomas M. Talavage', 'Semyon Slobounov', 'Hans C. Breiter']",2020-06-23T18:44:02Z,http://arxiv.org/abs/2006.14930v1,['q-bio.QM'],"metabolomic,energy metabolism,miRNA,rs-fMRI,network,motor control,collision sports,head acceleration events,inflammatory,virtual reality,neuroscience"
"Assistive VR Gym: Interactions with Real People to Improve Virtual
  Assistive Robots","Versatile robotic caregivers could benefit millions of people worldwide,
including older adults and people with disabilities. Recent work has explored
how robotic caregivers can learn to interact with people through physics
simulations, yet transferring what has been learned to real robots remains
challenging. Virtual reality (VR) has the potential to help bridge the gap
between simulations and the real world. We present Assistive VR Gym (AVR Gym),
which enables real people to interact with virtual assistive robots. We also
provide evidence that AVR Gym can help researchers improve the performance of
simulation-trained assistive robots with real people. Prior to AVR Gym, we
trained robot control policies (Original Policies) solely in simulation for
four robotic caregiving tasks (robot-assisted feeding, drinking, itch
scratching, and bed bathing) with two simulated robots (PR2 from Willow Garage
and Jaco from Kinova). With AVR Gym, we developed Revised Policies based on
insights gained from testing the Original policies with real people. Through a
formal study with eight participants in AVR Gym, we found that the Original
policies performed poorly, the Revised policies performed significantly better,
and that improvements to the biomechanical models used to train the Revised
policies resulted in simulated people that better match real participants.
Notably, participants significantly disagreed that the Original policies were
successful at assistance, but significantly agreed that the Revised policies
were successful at assistance. Overall, our results suggest that VR can be used
to improve the performance of simulation-trained control policies with real
people without putting people at risk, thereby serving as a valuable stepping
stone to real robotic assistance.","['Zackory Erickson', 'Yijun Gu', 'Charles C. Kemp']",2020-07-09T17:42:06Z,http://arxiv.org/abs/2007.04959v2,"['cs.RO', 'cs.HC', 'cs.LG']","Assistive VR Gym,Virtual Reality,Robotics,Simulation,Robot Control Policies,Robotic Caregiving Tasks,Biomechanical Models,Real People,Performance Evaluation"
One Shot 3D Photography,"3D photography is a new medium that allows viewers to more fully experience a
captured moment. In this work, we refer to a 3D photo as one that displays
parallax induced by moving the viewpoint (as opposed to a stereo pair with a
fixed viewpoint). 3D photos are static in time, like traditional photos, but
are displayed with interactive parallax on mobile or desktop screens, as well
as on Virtual Reality devices, where viewing it also includes stereo. We
present an end-to-end system for creating and viewing 3D photos, and the
algorithmic and design choices therein. Our 3D photos are captured in a single
shot and processed directly on a mobile device. The method starts by estimating
depth from the 2D input image using a new monocular depth estimation network
that is optimized for mobile devices. It performs competitively to the
state-of-the-art, but has lower latency and peak memory consumption and uses an
order of magnitude fewer parameters. The resulting depth is lifted to a layered
depth image, and new geometry is synthesized in parallax regions. We synthesize
color texture and structures in the parallax regions as well, using an
inpainting network, also optimized for mobile devices, on the LDI directly.
Finally, we convert the result into a mesh-based representation that can be
efficiently transmitted and rendered even on low-end devices and over poor
network connections. Altogether, the processing takes just a few seconds on a
mobile device, and the result can be instantly viewed and shared. We perform
extensive quantitative evaluation to validate our system and compare its new
components against the current state-of-the-art.","['Johannes Kopf', 'Kevin Matzen', 'Suhib Alsisan', 'Ocean Quigley', 'Francis Ge', 'Yangming Chong', 'Josh Patterson', 'Jan-Michael Frahm', 'Shu Wu', 'Matthew Yu', 'Peizhao Zhang', 'Zijian He', 'Peter Vajda', 'Ayush Saraf', 'Michael Cohen']",2020-08-27T17:59:31Z,http://arxiv.org/abs/2008.12298v2,"['cs.CV', 'cs.GR']","3D photography,parallax,viewpoint,virtual reality,mobile device,depth estimation,monocular,inpainting,mesh-based representation,quantitative evaluation"
"An XR rapid prototyping framework for interoperability across the
  reality spectrum","Applications of the Extended Reality (XR) spectrum, a superset of Mixed,
Augmented and Virtual Reality, are gaining prominence and can be employed in a
variety of areas, such as virtual museums. Examples can be found in the areas
of education, cultural heritage, health/treatment, entertainment, marketing,
and more. The majority of computer graphics applications nowadays are used to
operate only in one of the above realities. The lack of applications across the
XR spectrum is a real shortcoming. There are many advantages resulting from
this problem's solution. Firstly, releasing an application across the XR
spectrum could contribute in discovering its most suitable reality. Moreover,
an application could be more immersive within a particular reality, depending
on its context. Furthermore, its availability increases to a broader range of
users. For instance, if an application is released both in Virtual and
Augmented Reality, it is accessible to users that may lack the possession of a
VR headset, but not of a mobile AR device. The question that arises at this
point, would be ""Is it possible for a full s/w application stack to be
converted across XR without sacrificing UI/UX in a semi-automatic way?"". It may
be quite difficult, depending on the architecture and application
implementation. Most companies nowadays support only one reality, due to their
lack of UI/UX software architecture or resources to support the complete XR
spectrum. In this work, we present an ""automatic reality transition"" in the
context of virtual museum applications. We propose a development framework,
which will automatically allow this XR transition. This framework transforms
any XR project into different realities such as Augmented or Virtual. It also
reduces the development time while increasing the XR availability of 3D
applications, encouraging developers to release applications across the XR
spectrum.","['Efstratios Geronikolakis', 'George Papagiannakis']",2021-01-05T20:27:47Z,http://arxiv.org/abs/2101.01771v2,"['cs.GR', 'cs.HC', '68U99', 'D.2.2; D.2.12']","Extended Reality (XR),Mixed Reality,Augmented Reality,Virtual Reality,interoperability,computer graphics,immersive,UI/UX,software architecture,development framework"
"No-Reference Quality Assessment for 360-degree Images by Analysis of
  Multi-frequency Information and Local-global Naturalness","360-degree/omnidirectional images (OIs) have achieved remarkable attentions
due to the increasing applications of virtual reality (VR). Compared to
conventional 2D images, OIs can provide more immersive experience to consumers,
benefitting from the higher resolution and plentiful field of views (FoVs).
Moreover, observing OIs is usually in the head mounted display (HMD) without
references. Therefore, an efficient blind quality assessment method, which is
specifically designed for 360-degree images, is urgently desired. In this
paper, motivated by the characteristics of the human visual system (HVS) and
the viewing process of VR visual contents, we propose a novel and effective
no-reference omnidirectional image quality assessment (NR OIQA) algorithm by
Multi-Frequency Information and Local-Global Naturalness (MFILGN).
Specifically, inspired by the frequency-dependent property of visual cortex, we
first decompose the projected equirectangular projection (ERP) maps into
wavelet subbands. Then, the entropy intensities of low and high frequency
subbands are exploited to measure the multi-frequency information of OIs.
Besides, except for considering the global naturalness of ERP maps, owing to
the browsed FoVs, we extract the natural scene statistics features from each
viewport image as the measure of local naturalness. With the proposed
multi-frequency information measurement and local-global naturalness
measurement, we utilize support vector regression as the final image quality
regressor to train the quality evaluation model from visual quality-related
features to human ratings. To our knowledge, the proposed model is the first
no-reference quality assessment method for 360-degreee images that combines
multi-frequency information and image naturalness. Experimental results on two
publicly available OIQA databases demonstrate that our proposed MFILGN
outperforms state-of-the-art approaches.","['Wei Zhou', 'Jiahua Xu', 'Qiuping Jiang', 'Zhibo Chen']",2021-02-22T22:52:35Z,http://arxiv.org/abs/2102.11393v1,"['cs.CV', 'cs.CG', 'cs.MM', 'eess.IV']","no-reference quality assessment,360-degree images,multi-frequency information,local-global naturalness,omnidirectional images,virtual reality,field of views,head mounted display,human visual system,wavelet subbands."
"Meta-Reinforcement Learning for Reliable Communication in THz/VLC
  Wireless VR Networks","In this paper, the problem of enhancing the quality of virtual reality (VR)
services is studied for an indoor terahertz (THz)/visible light communication
(VLC) wireless network. In the studied model, small base stations (SBSs)
transmit high-quality VR images to VR users over THz bands and light-emitting
diodes (LEDs) provide accurate indoor positioning services for them using VLC.
Here, VR users move in real time and their movement patterns change over time
according to their applications, where both THz and VLC links can be blocked by
the bodies of VR users. To control the energy consumption of the studied
THz/VLC wireless VR network, VLC access points (VAPs) must be selectively
turned on so as to ensure accurate and extensive positioning for VR users.
Based on the user positions, each SBS must generate corresponding VR images and
establish THz links without body blockage to transmit the VR content. The
problem is formulated as an optimization problem whose goal is to maximize the
reliability of the VR network by selecting the appropriate VAPs to be turned on
and controlling the user association with SBSs. To solve this problem, a policy
gradient-based reinforcement learning (RL) algorithm that adopts a
meta-learning approach is proposed. The proposed meta policy gradient (MPG)
algorithm enables the trained policy to quickly adapt to new user movement
patterns. In order to solve the problem of maximizing the average number of
successfully served users for VR scenarios with a large number of users, a dual
method based MPG algorithm (D-MPG) with a low complexity is proposed.
Simulation results demonstrate that, compared to the trust region policy
optimization algorithm (TRPO), the proposed MPG and D-MPG algorithms yield up
to 26.8% and 21.9% improvement in the reliability as well as 81.2% and 87.5%
gains in the convergence speed, respectively.","['Yining Wang', 'Mingzhe Chen', 'Zhaohui Yang', 'Walid Saad', 'Tao luo', 'Shuguang Cui', 'H. Vincent Poor']",2021-01-29T15:57:25Z,http://arxiv.org/abs/2102.12277v2,"['cs.NI', 'cs.LG']","Meta-reinforcement learning,Communication,THz,VLC,Wireless networks,Virtual reality,Base stations,Light-emitting diodes,Optimization,Policy gradient-based"
"DONeRF: Towards Real-Time Rendering of Compact Neural Radiance Fields
  using Depth Oracle Networks","The recent research explosion around implicit neural representations, such as
NeRF, shows that there is immense potential for implicitly storing high-quality
scene and lighting information in compact neural networks. However, one major
limitation preventing the use of NeRF in real-time rendering applications is
the prohibitive computational cost of excessive network evaluations along each
view ray, requiring dozens of petaFLOPS. In this work, we bring compact neural
representations closer to practical rendering of synthetic content in real-time
applications, such as games and virtual reality. We show that the number of
samples required for each view ray can be significantly reduced when samples
are placed around surfaces in the scene without compromising image quality. To
this end, we propose a depth oracle network that predicts ray sample locations
for each view ray with a single network evaluation. We show that using a
classification network around logarithmically discretized and spherically
warped depth values is essential to encode surface locations rather than
directly estimating depth. The combination of these techniques leads to DONeRF,
our compact dual network design with a depth oracle network as its first step
and a locally sampled shading network for ray accumulation. With DONeRF, we
reduce the inference costs by up to 48x compared to NeRF when conditioning on
available ground truth depth information. Compared to concurrent acceleration
methods for raymarching-based neural representations, DONeRF does not require
additional memory for explicit caching or acceleration structures, and can
render interactively (20 frames per second) on a single GPU.","['Thomas Neff', 'Pascal Stadlbauer', 'Mathias Parger', 'Andreas Kurz', 'Joerg H. Mueller', 'Chakravarty R. Alla Chaitanya', 'Anton Kaplanyan', 'Markus Steinberger']",2021-03-04T18:55:09Z,http://arxiv.org/abs/2103.03231v4,"['cs.CV', 'cs.GR']","implicit neural representations,NeRF,real-time rendering,neural networks,depth oracle network,ray sampling,surface locations,rendering applications,compact neural networks,raymarching-based neural representations"
"F-CAD: A Framework to Explore Hardware Accelerators for Codec Avatar
  Decoding","Creating virtual avatars with realistic rendering is one of the most
essential and challenging tasks to provide highly immersive virtual reality
(VR) experiences. It requires not only sophisticated deep neural network (DNN)
based codec avatar decoders to ensure high visual quality and precise motion
expression, but also efficient hardware accelerators to guarantee smooth
real-time rendering using lightweight edge devices, like untethered VR
headsets. Existing hardware accelerators, however, fail to deliver sufficient
performance and efficiency targeting such decoders which consist of
multi-branch DNNs and require demanding compute and memory resources. To
address these problems, we propose an automation framework, called F-CAD
(Facebook Codec avatar Accelerator Design), to explore and deliver optimized
hardware accelerators for codec avatar decoding. Novel technologies include 1)
a new accelerator architecture to efficiently handle multi-branch DNNs; 2) a
multi-branch dynamic design space to enable fine-grained architecture
configurations; and 3) an efficient architecture search for picking the
optimized hardware design based on both application-specific demands and
hardware resource constraints. To the best of our knowledge, F-CAD is the first
automation tool that supports the whole design flow of hardware acceleration of
codec avatar decoders, allowing joint optimization on decoder designs in
popular machine learning frameworks and corresponding customized accelerator
design with cycle-accurate evaluation. Results show that the accelerators
generated by F-CAD can deliver up to 122.1 frames per second (FPS) and 91.6%
hardware efficiency when running the latest codec avatar decoder. Compared to
the state-of-the-art designs, F-CAD achieves 4.0X and 2.8X higher throughput,
62.5% and 21.2% higher efficiency than DNNBuilder and HybridDNN by targeting
the same hardware device.","['Xiaofan Zhang', 'Dawei Wang', 'Pierce Chuang', 'Shugao Ma', 'Deming Chen', 'Yuecheng Li']",2021-03-08T18:28:53Z,http://arxiv.org/abs/2103.04958v1,"['cs.AR', 'cs.CV']","Framework,Hardware accelerators,Codec avatar decoding,Deep neural network (DNN),Real-time rendering,Architecture search,Accelerator design,Automation tool,Machine learning frameworks,Cycle-accurate evaluation"
"Enhancing Human-in-the-Loop Adaptive Systems through Digital Twins and
  VR Interfaces","Self-adaptation approaches usually rely on closed-loop controllers that avoid
human intervention from adaptation. While such fully automated approaches have
proven successful in many application domains, there are situations where human
involvement in the adaptation process is beneficial or even necessary. For such
""human-in-the-loop"" adaptive systems, two major challenges, namely transparency
and controllability, have to be addressed to include the human in the
self-adaptation loop. Transparency means that relevant context information
about the adaptive systems and its context is represented based on a digital
twin enabling the human an immersive and realistic view. Concerning
controllability, the decision-making and adaptation operations should be
managed in a natural and interactive way. As existing human-in-the-loop
adaptation approaches do not fully cover these aspects, we investigate
alternative human-in-the-loop strategies by using a combination of digital
twins and virtual reality (VR) interfaces. Based on the concept of the digital
twin, we represent a self-adaptive system and its respective context in a
virtual environment. With the help of a VR interface, we support an immersive
and realistic human involvement in the self-adaptation loop by mirroring the
physical entities of the real world to the VR interface. For integrating the
human in the decision-making and adaptation process, we have implemented and
analyzed two different human-in-the-loop strategies in VR: a procedural control
where the human can control the decision making-process and adaptations through
VR interactions (human-controlled) and a declarative control where the human
specifies the goal state and the configuration is delegated to an AI planner
(mixed-initiative). We illustrate and evaluate our approach based on an
autonomic robot system that is accessible and controlled through a VR
interface.","['Enes Yigitbas', 'Kadiray Karakaya', 'Ivan Jovanovikj', 'Gregor Engels']",2021-03-19T13:48:25Z,http://arxiv.org/abs/2103.10804v1,"['cs.HC', 'cs.SE']","Digital Twins,VR Interfaces,Human-in-the-Loop,Self-adaptation,Transparency,Controllability,Virtual Reality,Immersive,Realistic,Decision-making."
"eXtended Artificial Intelligence: New Prospects of Human-AI Interaction
  Research","Artificial Intelligence (AI) covers a broad spectrum of computational
problems and use cases. Many of those implicate profound and sometimes
intricate questions of how humans interact or should interact with AIs.
Moreover, many users or future users do have abstract ideas of what AI is,
significantly depending on the specific embodiment of AI applications.
Human-centered-design approaches would suggest evaluating the impact of
different embodiments on human perception of and interaction with AI. An
approach that is difficult to realize due to the sheer complexity of
application fields and embodiments in reality. However, here XR opens new
possibilities to research human-AI interactions. The article's contribution is
twofold: First, it provides a theoretical treatment and model of human-AI
interaction based on an XR-AI continuum as a framework for and a perspective of
different approaches of XR-AI combinations. It motivates XR-AI combinations as
a method to learn about the effects of prospective human-AI interfaces and
shows why the combination of XR and AI fruitfully contributes to a valid and
systematic investigation of human-AI interactions and interfaces. Second, the
article provides two exemplary experiments investigating the aforementioned
approach for two distinct AI-systems. The first experiment reveals an
interesting gender effect in human-robot interaction, while the second
experiment reveals an Eliza effect of a recommender system. Here the article
introduces two paradigmatic implementations of the proposed XR testbed for
human-AI interactions and interfaces and shows how a valid and systematic
investigation can be conducted. In sum, the article opens new perspectives on
how XR benefits human-centered AI design and development.","['Carolin Wienrich', 'Marc Erich Latoschik']",2021-03-27T22:12:06Z,http://arxiv.org/abs/2103.15004v3,"['cs.AI', 'cs.HC']","Artificial Intelligence,Human-AI interaction,XR,Human-centered-design,Embodiments,XR-AI continuum,Interfaces,Human perception,Experiment,Human-centered AI design."
3D Human Body Reshaping with Anthropometric Modeling,"Reshaping accurate and realistic 3D human bodies from anthropometric
parameters (e.g., height, chest size, etc.) poses a fundamental challenge for
person identification, online shopping and virtual reality. Existing approaches
for creating such 3D shapes often suffer from complex measurement by range
cameras or high-end scanners, which either involve heavy expense cost or result
in low quality. However, these high-quality equipments limit existing
approaches in real applications, because the equipments are not easily
accessible for common users. In this paper, we have designed a 3D human body
reshaping system by proposing a novel feature-selection-based local mapping
technique, which enables automatic anthropometric parameter modeling for each
body facet. Note that the proposed approach can leverage limited anthropometric
parameters (i.e., 3-5 measurements) as input, which avoids complex measurement,
and thus better user-friendly experience can be achieved in real scenarios.
Specifically, the proposed reshaping model consists of three steps. First, we
calculate full-body anthropometric parameters from limited user inputs by
imputation technique, and thus essential anthropometric parameters for 3D body
reshaping can be obtained. Second, we select the most relevant anthropometric
parameters for each facet by adopting relevance masks, which are learned
offline by the proposed local mapping technique. Third, we generate the 3D body
meshes by mapping matrices, which are learned by linear regression from the
selected parameters to mesh-based body representation. We conduct experiments
by anthropomorphic evaluation and a user study from 68 volunteers. Experiments
show the superior results of the proposed system in terms of mean
reconstruction error against the state-of-the-art approaches.","['Yanhong Zeng', 'Jianlong Fu', 'Hongyang Chao']",2021-04-05T04:09:39Z,http://arxiv.org/abs/2104.01762v1,"['cs.CV', 'cs.GR']","3D human body,reshaping,anthropometric modeling,feature selection,local mapping technique,anthropometric parameters,imputation technique,relevance masks,linear regression,mesh-based representation"
Privacy-aware VR streaming,"Proactive tile-based virtual reality (VR) video streaming employs the current
tracking data of a user to predict future requested tiles, then renders and
delivers the predicted tiles to be requested before playback. The quality of
experience (QoE) depends on the overall performance of prediction, computing
(i.e., rendering) and communication. All prior works neglect that users may
have privacy requirement, i.e., not all the current tracking data are allowed
to be uploaded. In this paper, we investigate the privacy-aware VR streaming.
We first establish a dataset that collects the privacy requirement of 66 users
among 18 panoramic videos. The dataset shows that the privacy requirements of
360$^{\circ}$ videos are heterogeneous. Only 41\% of the total watched videos
have no privacy requirement. Based on these findings, we formulate the privacy
requirement as the \textit{degree of privacy} (DoP), and investigate the impact
of DoP on the proactive VR streaming. First, we find that with DoP, the length
of the observation window and prediction window of a tile predictor should be
variable. Then, we jointly optimize the durations for computing and
transmitting the selected tiles as well as the computing and communication
capability, aimed at maximizing the QoE given arbitrary predictor and
configured resources. From the obtained optimal closed-form solution, we find a
resource-saturated region where DoP has no impact on the QoE and a
resource-unsaturated region where the two-fold impacts of DoP are
contradictory. On the one hand, the increase of DoP will degrade the prediction
performance and thus degrade the QoE. On the other hand, the increase of DoP
will improve the capability of computing and communication and thus improve the
QoE. Simulation results using two predictors and a real dataset validate the
analysis and demonstrate the overall impact of DoP on the QoE.","['Xing Wei', 'Chenyang Yang']",2021-04-20T06:28:31Z,http://arxiv.org/abs/2104.09779v1,['cs.MM'],"privacy-aware,VR streaming,tracking data,tiles,quality of experience (QoE),prediction,computing,communication,dataset,degree of privacy (DoP)"
"Electrotactile feedback applications for hand and arm interactions: A
  systematic review, meta-analysis, and future directions","Haptic feedback is critical in a broad range of
human-machine/computer-interaction applications. However, the high cost and low
portability/wearability of haptic devices remain unresolved issues, severely
limiting the adoption of this otherwise promising technology. Electrotactile
interfaces have the advantage of being more portable and wearable due to their
reduced actuators' size, as well as their lower power consumption and
manufacturing cost. The applications of electrotactile feedback have been
explored in human-computer interaction and human-machine-interaction for
facilitating hand-based interactions in applications such as prosthetics,
virtual reality, robotic teleoperation, surface haptics, portable devices, and
rehabilitation. This paper presents a technological overview of electrotactile
feedback, as well a systematic review and meta-analysis of its applications for
hand-based interactions. We discuss the different electrotactile systems
according to the type of application. We also discuss over a quantitative
congregation of the findings, to offer a high-level overview into the
state-of-art and suggest future directions. Electrotactile feedback systems
showed increased portability/wearability, and they were successful in rendering
and/or augmenting most tactile sensations, eliciting perceptual processes, and
improving performance in many scenarios. However, knowledge gaps (e.g.,
embodiment), technical (e.g., recurrent calibration, electrodes' durability)
and methodological (e.g., sample size) drawbacks were detected, which should be
addressed in future studies.","['Panagiotis Kourtesis', 'Ferran Argelaguet', 'Sebastian Vizcay', 'Maud Marchal', 'Claudio Pacchierotti']",2021-05-11T21:03:20Z,http://arxiv.org/abs/2105.05343v3,"['cs.HC', 'cs.RO', 'I.3.7; H.5.2; J.3; J.7']","electrotactile feedback,hand-based interactions,human-computer interaction,human-machine interaction,prosthetics,virtual reality,robotic teleoperation,surface haptics,rehabilitation"
"Dissolving yourself in connection to others: shared experiences of ego
  attenuation and connectedness during group VR experiences can be comparable
  to psychedelics","With a growing body of research highlighting the therapeutic potential of
experiential phenomenology which diminishes egoic identity and increases one's
sense of connectedness, there is significant interest in how to elicit such
'self-transcendent experiences' (STEs) in laboratory contexts. Psychedelic
drugs (YDs) have proven particularly effective in this respect, producing
subjective phenomenology which reliably elicits intense STEs. With virtual
reality (VR) emerging as a powerful tool for constructing new perceptual
environments, we describe a VR framework called 'Isness-distributed' (Isness-D)
which harnesses the unique affordances of distributed multi-person VR to blur
conventional self-other boundaries. Within Isness-D, groups of participants
co-habit a shared virtual space, collectively experiencing their bodies as
luminous energetic essences with diffuse spatial boundaries. It enables moments
of 'energetic coalescence', a new class of embodied phenomenological
intersubjective experience where bodies can fluidly merge, enabling
participants to have an experience of including multiple others within their
self-representation. To evaluate Isness-D, we adopted a citizen science
approach, coordinating an international network of Isness-D 'nodes'. We
analyzed the results (N = 58) using 4 different self-report scales previously
applied to analyze subjective YD phenomenology (the inclusion of community in
self scale, ego-dissolution inventory, communitas scale, and the MEQ30 mystical
experience questionnaire). Despite the complexities associated with a
distributed experiment like this, the Isness-D scores on all 4 scales were
statistically indistinguishable from recently published YD studies,
demonstrating that distributed VR can be used to design intersubjective STEs
where people dissolve their sense of self in the connection to others.","['David R. Glowacki', 'Rhoslyn Roebuck Williams', 'Olivia M. Maynard', 'James E. Pike', 'Rachel Freire', 'Mark D. Wonnacott', 'Mike Chatziapostolou']",2021-05-17T13:07:32Z,http://arxiv.org/abs/2105.07796v1,['cs.HC'],"experiential phenomenology,ego attenuation,connectedness,psychedelics,virtual reality,Isness-distributed,self-transcendent experiences,citizen science,intersubjective experience,ego-dissolution"
"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad
  Virtual Y Conocimiento Experto para generación de entornos de aprendizaje
  Hombre-Máquina","This work presents the results of project CONECT4, which addresses the
research and development of new non-intrusive communication methods for the
generation of a human-machine learning ecosystem oriented to predictive
maintenance in the automotive industry. Through the use of innovative
technologies such as Augmented Reality, Virtual Reality, Digital Twin and
expert knowledge, CONECT4 implements methodologies that allow improving the
efficiency of training techniques and knowledge management in industrial
companies. The research has been supported by the development of content and
systems with a low level of technological maturity that address solutions for
the industrial sector applied in training and assistance to the operator. The
results have been analyzed in companies in the automotive sector, however, they
are exportable to any other type of industrial sector. -- --
  En esta publicaci\'on se presentan los resultados del proyecto CONECT4, que
aborda la investigaci\'on y desarrollo de nuevos m\'etodos de comunicaci\'on no
intrusivos para la generaci\'on de un ecosistema de aprendizaje
hombre-m\'aquina orientado al mantenimiento predictivo en la industria de
automoci\'on. A trav\'es del uso de tecnolog\'ias innovadoras como la Realidad
Aumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,
CONECT4 implementa metodolog\'ias que permiten mejorar la eficiencia de las
t\'ecnicas de formaci\'on y gesti\'on de conocimiento en las empresas
industriales. La investigaci\'on se ha apoyado en el desarrollo de contenidos y
sistemas con un nivel de madurez tecnol\'ogico bajo que abordan soluciones para
el sector industrial aplicadas en la formaci\'on y asistencia al operario. Los
resultados han sido analizados en empresas del sector de automoci\'on, no
obstante, son exportables a cualquier otro tipo de sector industrial.","['Santiago González', 'Alvaro García', 'Ana Núñez']",2021-05-24T11:51:46Z,http://arxiv.org/abs/2105.11216v1,['cs.HC'],"Realidad Mixta,Realidad Virtual,Conocimiento Experto,Aprendizaje Hombre-Máquina,Mantenimiento Predictivo,Industria Automotriz,Realidad Aumentada,Gemelo Digital"
OVRseen: Auditing Network Traffic and Privacy Policies in Oculus VR,"Virtual reality (VR) is an emerging technology that enables new applications
but also introduces privacy risks. In this paper, we focus on Oculus VR (OVR),
the leading platform in the VR space and we provide the first comprehensive
analysis of personal data exposed by OVR apps and the platform itself, from a
combined networking and privacy policy perspective. We experimented with the
Quest 2 headset and tested the most popular VR apps available on the official
Oculus and the SideQuest app stores. We developed OVRseen, a methodology and
system for collecting, analyzing, and comparing network traffic and privacy
policies on OVR. On the networking side, we captured and decrypted network
traffic of VR apps, which was previously not possible on OVR, and we extracted
data flows, defined as <app, data type, destination>. Compared to the mobile
and other app ecosystems, we found OVR to be more centralized and driven by
tracking and analytics, rather than by third-party advertising. We show that
the data types exposed by VR apps include personally identifiable information
(PII), device information that can be used for fingerprinting, and VR-specific
data types. By comparing the data flows found in the network traffic with
statements made in the apps' privacy policies, we found that approximately 70%
of OVR data flows were not properly disclosed. Furthermore, we extracted
additional context from the privacy policies, and we observed that 69% of the
data flows were used for purposes unrelated to the core functionality of apps.","['Rahmadi Trimananda', 'Hieu Le', 'Hao Cui', 'Janice Tran Ho', 'Anastasia Shuba', 'Athina Markopoulou']",2021-06-09T21:52:13Z,http://arxiv.org/abs/2106.05407v4,"['cs.CR', 'cs.NI']","network traffic,privacy policies,Oculus VR,personal data,VR apps,privacy risks,data flows,personally identifiable information (PII),device information,VR-specific data types"
"Applying VertexShuffle Toward 360-Degree Video Super-Resolution on
  Focused-Icosahedral-Mesh","With the emerging of 360-degree image/video, augmented reality (AR) and
virtual reality (VR), the demand for analysing and processing spherical signals
get tremendous increase. However, plenty of effort paid on planar signals that
projected from spherical signals, which leading to some problems, e.g. waste of
pixels, distortion. Recent advances in spherical CNN have opened up the
possibility of directly analysing spherical signals. However, they pay
attention to the full mesh which makes it infeasible to deal with situations in
real-world application due to the extremely large bandwidth requirement. To
address the bandwidth waste problem associated with 360-degree video streaming
and save computation, we exploit Focused Icosahedral Mesh to represent a small
area and construct matrices to rotate spherical content to the focused mesh
area. We also proposed a novel VertexShuffle operation that can significantly
improve both the performance and the efficiency compared to the original
MeshConv Transpose operation introduced in UGSCNN. We further apply our
proposed methods on super resolution model, which is the first to propose a
spherical super-resolution model that directly operates on a mesh
representation of spherical pixels of 360-degree data. To evaluate our model,
we also collect a set of high-resolution 360-degree videos to generate a
spherical image dataset. Our experiments indicate that our proposed spherical
super-resolution model achieves significant benefits in terms of both
performance and inference time compared to the baseline spherical
super-resolution model that uses the simple MeshConv Transpose operation. In
summary, our model achieves great super-resolution performance on 360-degree
inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices
on the mesh.","['Na Li', 'Yao Liu']",2021-06-21T16:53:57Z,http://arxiv.org/abs/2106.11253v1,"['eess.IV', 'cs.CV', 'cs.MM']","VertexShuffle,360-degree video,Super-Resolution,Focused-Icosahedral-Mesh,Spherical signals,Spherical CNN,Bandwidth,Mesh representation,MeshConv Transpose,Augmented reality (AR)"
"Effect of acoustic scene complexity and visual scene representation on
  auditory perception in virtual audio-visual environments","In daily life, social interaction and acoustic communication often take place
in complex acoustic environments (CAE) with a variety of interfering sounds and
reverberation. For hearing research and the evaluation of hearing systems,
simulated CAEs using virtual reality techniques have gained interest in the
context of ecological validity. In the current study, the effect of scene
complexity and visual representation of the scene on psychoacoustic measures
like sound source location, distance perception, loudness, speech
intelligibility, and listening effort in a virtual audio-visual environment was
investigated. A 3-dimensional, 86-channel loudspeaker array was used to render
the sound field in combination with or without a head-mounted display (HMD) to
create an immersive stereoscopic visual representation of the scene. The scene
consisted of a ring of eight (virtual) loudspeakers which played a target
speech stimulus and nonsense speech interferers in several spatial conditions.
Either an anechoic (snowy outdoor scenery) or echoic environment (loft
apartment) with a reverberation time (T60) of about 1.5 s was simulated. In
addition to varying the number of interferers, scene complexity was varied by
assessing the psychoacoustic measures in isolated consecutive measurements
orcsimultaneously. Results showed no significant effect of wearing the HMD on
the data. Loudness and distance perception showed significantly different
results when they were measured simultaneously instead of consecutively in
isolation. The advantage of the suggested setup is that it can be directly
transferred to a corresponding real room, enabling a 1:1 comparison and
verification of the perception experiments in the real and virtual environment.","['Stefan Fichna', 'Thomas Biberger', 'Bernhard U. Seeber', 'Stephan D. Ewert']",2021-06-30T08:53:48Z,http://arxiv.org/abs/2106.15909v2,"['eess.AS', 'cs.SD']","acoustic scene complexity,visual scene representation,auditory perception,virtual audio-visual environments,reverberation,sound source location,distance perception,loudness,speech intelligibility,listening effort"
"iGibson 2.0: Object-Centric Simulation for Robot Learning of Everyday
  Household Tasks","Recent research in embodied AI has been boosted by the use of simulation
environments to develop and train robot learning approaches. However, the use
of simulation has skewed the attention to tasks that only require what robotics
simulators can simulate: motion and physical contact. We present iGibson 2.0,
an open-source simulation environment that supports the simulation of a more
diverse set of household tasks through three key innovations. First, iGibson
2.0 supports object states, including temperature, wetness level, cleanliness
level, and toggled and sliced states, necessary to cover a wider range of
tasks. Second, iGibson 2.0 implements a set of predicate logic functions that
map the simulator states to logic states like Cooked or Soaked. Additionally,
given a logic state, iGibson 2.0 can sample valid physical states that satisfy
it. This functionality can generate potentially infinite instances of tasks
with minimal effort from the users. The sampling mechanism allows our scenes to
be more densely populated with small objects in semantically meaningful
locations. Third, iGibson 2.0 includes a virtual reality (VR) interface to
immerse humans in its scenes to collect demonstrations. As a result, we can
collect demonstrations from humans on these new types of tasks, and use them
for imitation learning. We evaluate the new capabilities of iGibson 2.0 to
enable robot learning of novel tasks, in the hope of demonstrating the
potential of this new simulator to support new research in embodied AI. iGibson
2.0 and its new dataset are publicly available at
http://svl.stanford.edu/igibson/.","['Chengshu Li', 'Fei Xia', 'Roberto Martín-Martín', 'Michael Lingelbach', 'Sanjana Srivastava', 'Bokui Shen', 'Kent Vainio', 'Cem Gokmen', 'Gokul Dharan', 'Tanish Jain', 'Andrey Kurenkov', 'C. Karen Liu', 'Hyowon Gweon', 'Jiajun Wu', 'Li Fei-Fei', 'Silvio Savarese']",2021-08-06T18:41:39Z,http://arxiv.org/abs/2108.03272v4,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.LG']","simulation environment,robot learning,household tasks,object states,predicate logic functions,virtual reality interface,demonstrations,imitation learning,embodied AI,dataset"
"To VR or not to VR: Is virtual reality suitable to understand software
  development metrics?","Background/Context: Currently, the usual interface for visualizing data is
based on 2-D screens. Recently, devices capable of visualizing data while
immersed in VR scenes are becoming common. However, it has not been studied in
detail to which extent these devices are suitable for interacting with data
visualizations in the specific case of data about software development.
Objective/Aim: In this registered report, we propose to answer the following
question: ""Is comprehension of software development processes, via the
visualization of their metrics, better when presented in VR scenes than in 2D
screens?"" In particular, we will study if answers obtained after interacting
with visualizations presented as VR scenes are more or less correct than those
obtained from traditional screens, and if it takes more or less time to produce
those answers. Method: We will run an experiment with volunteer subjects from
several backgrounds. We will have two setups: an on-screen application, and a
VR scene. Both will be designed to be as much equivalent as possible in terms
of the information they provide. For the former, we use a commercial-grade set
of \kibana-based interactive dashboards that stakeholders currently use to get
insights. For the latter, we use a set of visualizations similar to those in
the on-screen case, prepared to provide the same set of data using the museum
metaphor in a VR room. The field of analysis will be related to modern code
review, in particular pull request activity. The subjects will try to answer
some questions in both setups (some will work first in VR, some on-screen),
which will be presented to them in random order. To draw results, we will
compare and statistically analyze both the correctness of their answers, and
the time spent until they are produced.","['David Moreno-Lumbreras', 'Gregorio Robles', 'Daniel Izquierdo-Cortázar', 'Jesus M. Gonzalez-Barahona']",2021-09-28T14:43:41Z,http://arxiv.org/abs/2109.13768v1,['cs.SE'],"virtual reality,software development,metrics,data visualization,2D screens,VR scenes,code review,pull request,interactive dashboards,commercial-grade."
"3D Hand Pose and Shape Estimation from RGB Images for Keypoint-Based
  Hand Gesture Recognition","Estimating the 3D pose of a hand from a 2D image is a well-studied problem
and a requirement for several real-life applications such as virtual reality,
augmented reality, and hand gesture recognition. Currently, reasonable
estimations can be computed from single RGB images, especially when a
multi-task learning approach is used to force the system to consider the shape
of the hand when its pose is determined. However, depending on the method used
to represent the hand, the performance can drop considerably in real-life
tasks, suggesting that stable descriptions are required to achieve satisfactory
results. In this paper, we present a keypoint-based end-to-end framework for 3D
hand and pose estimation and successfully apply it to the task of hand gesture
recognition as a study case. Specifically, after a pre-processing step in which
the images are normalized, the proposed pipeline uses a multi-task semantic
feature extractor generating 2D heatmaps and hand silhouettes from RGB images,
a viewpoint encoder to predict the hand and camera view parameters, a stable
hand estimator to produce the 3D hand pose and shape, and a loss function to
guide all of the components jointly during the learning phase. Tests were
performed on a 3D pose and shape estimation benchmark dataset to assess the
proposed framework, which obtained state-of-the-art performance. Our system was
also evaluated on two hand-gesture recognition benchmark datasets and
significantly outperformed other keypoint-based approaches, indicating that it
is an effective solution that is able to generate stable 3D estimates for hand
pose and shape.","['Danilo Avola', 'Luigi Cinque', 'Alessio Fagioli', 'Gian Luca Foresti', 'Adriano Fragomeni', 'Daniele Pannone']",2021-09-28T17:07:43Z,http://arxiv.org/abs/2109.13879v2,['cs.CV'],"3D hand pose,shape estimation,RGB images,keypoint-based,hand gesture recognition,multi-task learning,semantic feature extractor,viewpoint encoder,stable hand estimator,loss function"
"Pavlovian Signalling with General Value Functions in Agent-Agent
  Temporal Decision Making","In this paper, we contribute a multi-faceted study into Pavlovian signalling
-- a process by which learned, temporally extended predictions made by one
agent inform decision-making by another agent. Signalling is intimately
connected to time and timing. In service of generating and receiving signals,
humans and other animals are known to represent time, determine time since past
events, predict the time until a future stimulus, and both recognize and
generate patterns that unfold in time. We investigate how different temporal
processes impact coordination and signalling between learning agents by
introducing a partially observable decision-making domain we call the Frost
Hollow. In this domain, a prediction learning agent and a reinforcement
learning agent are coupled into a two-part decision-making system that works to
acquire sparse reward while avoiding time-conditional hazards. We evaluate two
domain variations: machine agents interacting in a seven-state linear walk, and
human-machine interaction in a virtual-reality environment. Our results
showcase the speed of learning for Pavlovian signalling, the impact that
different temporal representations do (and do not) have on agent-agent
coordination, and how temporal aliasing impacts agent-agent and human-agent
interactions differently. As a main contribution, we establish Pavlovian
signalling as a natural bridge between fixed signalling paradigms and fully
adaptive communication learning between two agents. We further show how to
computationally build this adaptive signalling process out of a fixed
signalling process, characterized by fast continual prediction learning and
minimal constraints on the nature of the agent receiving signals. Our results
therefore suggest an actionable, constructivist path towards communication
learning between reinforcement learning agents.","['Andrew Butcher', 'Michael Bradley Johanson', 'Elnaz Davoodi', 'Dylan J. A. Brenneis', 'Leslie Acker', 'Adam S. R. Parker', 'Adam White', 'Joseph Modayil', 'Patrick M. Pilarski']",2022-01-11T00:14:04Z,http://arxiv.org/abs/2201.03709v1,"['cs.AI', 'cs.LG', 'cs.MA']","Pavlovian signalling,General value functions,Temporal decision making,Prediction learning,Reinforcement learning,Sparse reward,Time representation,Coordination,Signal generation,Communication learning."
"An Optimization Framework for General Rate Splitting for General
  Multicast","Immersive video, such as virtual reality (VR) and multi-view videos, is
growing in popularity. Its wireless streaming is an instance of general
multicast, extending conventional unicast and multicast, whose effective design
is still open. This paper investigates general rate splitting for general
multicast. Specifically, we consider a multi-carrier single-cell wireless
network where a multi-antenna base station (BS) communicates to multiple
single-antenna users via general multicast. We consider linear beamforming at
the BS and joint decoding at each user in the slow fading and fast fading
scenarios. In the slow fading scenario, we consider the maximization of the
weighted sum average rate, which is a challenging nonconvex stochastic problem
with numerous variables. To reduce computational complexity, we decouple the
original nonconvex stochastic problem into multiple nonconvex deterministic
problems, one for each system channel state. Then, we propose an iterative
algorithm for each deterministic problem to obtain a Karush-Kuhn-Tucker (KKT)
point using the concave-convex procedure (CCCP). In the fast fading scenario,
we consider the maximization of the weighted sum ergodic rate. This problem is
more challenging than the one for the slow fading scenario, as it is not
separable. First, we propose a stochastic iterative algorithm to obtain a KKT
point using stochastic successive convex approximation (SSCA) and the exact
penalty method. Then, we propose two low-complexity iterative algorithms to
obtain feasible points with promising performance for two cases of channel
distributions using approximation and CCCP. The proposed optimization framework
generalizes the existing ones for rate splitting for various types of services.
Finally, we numerically show substantial gains of the proposed solutions over
existing schemes in both scenarios.","['Lingzhi Zhao', 'Ying Cui', 'Sheng Yang', 'Shlomo Shamai']",2022-01-19T02:22:52Z,http://arxiv.org/abs/2201.07386v2,"['cs.IT', 'math.IT']","general rate splitting,general multicast,immersive video,virtual reality (VR),multi-view videos,multi-carrier,linear beamforming,joint decoding,slow fading,fast fading"
"Trajectory planning in Dynamics Environment : Application for Haptic
  Perception in Safe HumanRobot Interaction","In a human-robot interaction system, the most important thing to consider is
the safety of the user. This must be guaranteed in order to implement a
reliable system. The main objective of this paper is to generate a safe motion
scheme that takes into account the obstacles present in a virtual reality (VR)
environment. The work is developed using the MoveIt software in ROS to control
an industrial robot UR5. Thanks to this, we will be able to set up the planning
group, which is realized by the UR5 robot with a 6-sided prop and the base of
the manipulator, in order to plan feasible trajectories that it will be able to
execute in the environment. The latter is based on the interior of a vehicle,
containing a person (which would be the user in this case) for which the
configuration will also be made to be taken into account in the system. To do
this, we first investigated the software's capabilities and options for path
planning, as well as the different ways to execute the movements. We also
compared the different trajectory planning algorithms that the software is
capable of using in order to determine which one is best suited for the task.
Finally, we proposed different mobility schemes to be executed by the robot
depending on the situation it is facing. The first one is used when the robot
has to plan trajectories in a safe space, where the only obstacle to avoid is
the user's workspace. The second one is used when the robot has to interact
with the user, where a dummy model represents the user's position as a function
of time, which is the one to be avoided.","['A Gutierrez', 'V Guda', 'S Mugisha', 'C Chevallereau', 'Damien Chablat']",2022-02-23T07:48:10Z,http://arxiv.org/abs/2202.11336v1,['cs.RO'],"Trajectory planning,Dynamics Environment,Haptic Perception,Safe Human-Robot Interaction,MoveIt software,ROS,Industrial robot,UR5,Planning group,Virtual reality,Path planning"
WEMAC: Women and Emotion Multi-modal Affective Computing dataset,"Among the seventeen Sustainable Development Goals (SDGs) proposed within the
2030 Agenda and adopted by all the United Nations member states, the Fifth SDG
is a call for action to turn Gender Equality into a fundamental human right and
an essential foundation for a better world. It includes the eradication of all
types of violence against women. Within this context, the UC3M4Safety research
team aims to develop Bindi. This is a cyber-physical system which includes
embedded Artificial Intelligence algorithms, for user real-time monitoring
towards the detection of affective states, with the ultimate goal of achieving
the early detection of risk situations for women. On this basis, we make use of
wearable affective computing including smart sensors, data encryption for
secure and accurate collection of presumed crime evidence, as well as the
remote connection to protecting agents. Towards the development of such system,
the recordings of different laboratory and into-the-wild datasets are in
process. These are contained within the UC3M4Safety Database. Thus, this paper
presents and details the first release of WEMAC, a novel multi-modal dataset,
which comprises a laboratory-based experiment for 47 women volunteers that were
exposed to validated audio-visual stimuli to induce real emotions by using a
virtual reality headset while physiological, speech signals and self-reports
were acquired and collected. We believe this dataset will serve and assist
research on multi-modal affective computing using physiological and speech
information.","['Jose A. Miranda', 'Esther Rituerto-González', 'Laura Gutiérrez-Martín', 'Clara Luis-Mingueza', 'Manuel F. Canabal', 'Alberto Ramírez Bárcenas', 'Jose M. Lanza-Gutiérrez', 'Carmen Peláez-Moreno', 'Celia López-Ongil']",2022-03-01T13:39:54Z,http://arxiv.org/abs/2203.00456v4,"['cs.HC', 'eess.SP']","Women,Emotion,Multi-modal,Affective Computing,Dataset,Sustainable Development Goals,Gender Equality,Artificial Intelligence,Wearable computing,Cyber-physical system"
"The Frost Hollow Experiments: Pavlovian Signalling as a Path to
  Coordination and Communication Between Agents","Learned communication between agents is a powerful tool when approaching
decision-making problems that are hard to overcome by any single agent in
isolation. However, continual coordination and communication learning between
machine agents or human-machine partnerships remains a challenging open
problem. As a stepping stone toward solving the continual communication
learning problem, in this paper we contribute a multi-faceted study into what
we term Pavlovian signalling -- a process by which learned, temporally extended
predictions made by one agent inform decision-making by another agent with
different perceptual access to their shared environment. We seek to establish
how different temporal processes and representational choices impact Pavlovian
signalling between learning agents. To do so, we introduce a partially
observable decision-making domain we call the Frost Hollow. In this domain a
prediction learning agent and a reinforcement learning agent are coupled into a
two-part decision-making system that seeks to acquire sparse reward while
avoiding time-conditional hazards. We evaluate two domain variations: 1)
machine prediction and control learning in a linear walk, and 2) a prediction
learning machine interacting with a human participant in a virtual reality
environment. Our results showcase the speed of learning for Pavlovian
signalling, the impact that different temporal representations do (and do not)
have on agent-agent coordination, and how temporal aliasing impacts agent-agent
and human-agent interactions differently. As a main contribution, we establish
Pavlovian signalling as a natural bridge between fixed signalling paradigms and
fully adaptive communication learning. Our results therefore point to an
actionable, constructivist path towards continual communication learning
between reinforcement learning agents, with potential impact in a range of
real-world settings.","['Patrick M. Pilarski', 'Andrew Butcher', 'Elnaz Davoodi', 'Michael Bradley Johanson', 'Dylan J. A. Brenneis', 'Adam S. R. Parker', 'Leslie Acker', 'Matthew M. Botvinick', 'Joseph Modayil', 'Adam White']",2022-03-17T17:49:45Z,http://arxiv.org/abs/2203.09498v1,"['cs.AI', 'cs.CL', 'cs.LG', 'cs.MA']","Pavlovian signalling,Coordination,Communication,Machine agents,Decision-making,Prediction learning,Reinforcement learning,Temporal processes,Representational choices,Virtual reality"
Extended Reality for Mental Health Evaluation -A Scoping Review,"Mental health disorders are the leading cause of health-related problems
globally. It is projected that mental health disorders will be the leading
cause of morbidity among adults as the incidence rates of anxiety and
depression grows globally. Recently, extended reality (XR), a general term
covering virtual reality (VR), augmented reality (AR) and mixed reality (MR),
is paving a new way to deliver mental health care. In this paper, we conduct a
scoping review on the development and application of XR in the area of mental
disorders. We performed a scoping database search to identify the relevant
studies indexed in Google Scholar, PubMed, and the ACM Digital Library. A
search period between August 2016 and December 2023 was defined to select
articles related to the usage of VR, AR, and MR in a mental health context. We
identified a total of 85 studies from 27 countries across the globe. By
performing data analysis, we found that most of the studies focused on
developed countries such as the US (16.47%) and Germany (12.94%). None of the
studies were for African countries. The majority of the articles reported that
XR techniques led to a significant reduction in symptoms of anxiety or
depression. More studies were published in the year 2021, i.e., 31.76% (n =
31). This could indicate that mental disorder intervention received a higher
attention when COVID-19 emerged. Most studies (n = 65) focused on a population
between 18 and 65 years old, only a few studies focused on teenagers (n = 2).
Also, more studies were done experimentally (n = 67, 78.82%) rather than by
analytical and modeling approaches (n = 8, 9.41%). This shows that there is a
rapid development of XR technology for mental health care. Furthermore, these
studies showed that XR technology can effectively be used for evaluating mental
disorders in similar or better way as the conventional approaches.","['Omisore Olatunji', 'Ifeanyi Odenigbo', 'Joseph Orji', 'Amelia Beltran', 'Nilufar Baghaei', 'Meier Sandra', 'Rita Orji']",2022-04-04T09:46:30Z,http://arxiv.org/abs/2204.01348v2,"['cs.HC', 'cs.CV']","Extended reality,Mental health evaluation,Scoping review,Virtual reality,Augmented reality,Mixed reality,Mental disorders,Data analysis,Population,XR technology"
"Modeling the Noticeability of User-Avatar Movement Inconsistency for
  Sense of Body Ownership Intervention","An avatar mirroring the user's movement is commonly adopted in Virtual
Reality(VR). Maintaining the user-avatar movement consistency provides the user
a sense of body ownership and thus an immersive experience. However, breaking
this consistency can enable new interaction functionalities, such as pseudo
haptic feedback or input augmentation, at the expense of immersion. We propose
to quantify the probability of users noticing the movement inconsistency while
the inconsistency amplitude is being enlarged, which aims to guide the
intervention of the users' sense of body ownership in VR. We applied angular
offsets to the avatar's shoulder and elbow joints and recorded whether the user
identified the inconsistency through a series of three user studies and built a
statistical model based on the results. Results show that the noticeability of
movement inconsistency increases roughly quadratically with the enlargement of
offsets and the offsets at two joints negatively affect the probability
distributions of each other. Leveraging the model, we implemented a technique
that amplifies the user's arm movements with unnoticeable offsets and then
evaluated implementations with different parameters(offset strength, offset
distribution). Results show that the technique with medium-level and
balanced-distributed offsets achieves the best overall performance. Finally, we
demonstrated our model's extendability in interventions in the sense of body
ownership with three VR applications including stroke rehabilitation, action
game and widget arrangement.","['Zhipeng Li', 'Yu Jiang', 'Yihao Zhu', 'Ruijia Chen', 'Ruolin Wang', 'Yuntao Wang', 'Yukang Yan', 'Yuanchun Shi']",2022-04-26T04:43:35Z,http://arxiv.org/abs/2204.12071v3,['cs.HC'],"user-avatar movement,noticeability,body ownership,intervention,virtual reality,movement consistency,angular offsets,statistical model,pseudo haptic feedback,input augmentation"
"Two are not always better than one: Role specialization is an important
  determinant of collaborative task performance","Collaboration frequently yields better results in decision making, learning,
and haptic interactions than when these actions are performed individually.
However, is collaboration always superior to solo actions, or do its benefits
depend on whether collaborating individuals have different or the same roles?
To answer this question, we asked human subjects to perform virtual-reality
collaborative and individual beam transportation tasks. These tasks were
simulated in real-time by coupling the motion of a pair of hand-held robotic
manipulanda to the virtual beam using virtual spring-dampers. For the task to
be considered successful, participants had to complete it within temporal and
spatial constraints. While the visual feedback remained the same, the
underlying dynamics of the beam were altered to create two distinctive task
contexts which were determined by a moving pivot constraint. When the pivot was
placed at the center of the beam, two hands contribute to the task with
symmetric mechanical leverage (symmetric context). When the pivot was placed at
the left side of the beam, two hands contribute to the task with asymmetric
mechanical leverage (asymmetric context). Participants performed these task
contexts either individually with both hands (solo), or collaboratively by
pairing one hand with another one (dyads). We found that dyads in the
asymmetric context performed better than solos. In contrast, solos performed
the symmetric context better than dyads. Importantly, we found that two hands
took different roles in the asymmetric context for both solos and dyads. In
contrast, the contribution from each hand was statistically indistinguishable
in the symmetric context. Our findings suggest that better performance in dyads
than solos is not a general phenomenon, but rather that collaboration yields
better performance only when role specialization emerges in dyadic
interactions.","['Asuka Takai', 'Qiushi Fu', 'Yuzuru Doibata', 'Giuseppe Lisi', 'Toshiki Tsuchiya', 'Keivan Mojtahedi', 'Toshinori Yoshioka', 'Mitsuo Kawato', 'Jun Morimoto', 'Marco Santello']",2022-05-12T16:35:06Z,http://arxiv.org/abs/2205.06196v1,['q-bio.NC'],"collaborative task,role specialization,virtual reality,robotic manipulanda,virtual spring-dampers,spatial constraints,mechanical leverage,symmetric context,asymmetric context,dyadic interactions"
"BinauralGrad: A Two-Stage Conditional Diffusion Probabilistic Model for
  Binaural Audio Synthesis","Binaural audio plays a significant role in constructing immersive augmented
and virtual realities. As it is expensive to record binaural audio from the
real world, synthesizing them from mono audio has attracted increasing
attention. This synthesis process involves not only the basic physical warping
of the mono audio, but also room reverberations and head/ear related
filtrations, which, however, are difficult to accurately simulate in
traditional digital signal processing. In this paper, we formulate the
synthesis process from a different perspective by decomposing the binaural
audio into a common part that shared by the left and right channels as well as
a specific part that differs in each channel. Accordingly, we propose
BinauralGrad, a novel two-stage framework equipped with diffusion models to
synthesize them respectively. Specifically, in the first stage, the common
information of the binaural audio is generated with a single-channel diffusion
model conditioned on the mono audio, based on which the binaural audio is
generated by a two-channel diffusion model in the second stage. Combining this
novel perspective of two-stage synthesis with advanced generative models (i.e.,
the diffusion models),the proposed BinauralGrad is able to generate accurate
and high-fidelity binaural audio samples. Experiment results show that on a
benchmark dataset, BinauralGrad outperforms the existing baselines by a large
margin in terms of both object and subject evaluation metrics (Wave L2: 0.128
vs. 0.157, MOS: 3.80 vs. 3.61). The generated audio samples
(https://speechresearch.github.io/binauralgrad) and code
(https://github.com/microsoft/NeuralSpeech/tree/master/BinauralGrad) are
available online.","['Yichong Leng', 'Zehua Chen', 'Junliang Guo', 'Haohe Liu', 'Jiawei Chen', 'Xu Tan', 'Danilo Mandic', 'Lei He', 'Xiang-Yang Li', 'Tao Qin', 'Sheng Zhao', 'Tie-Yan Liu']",2022-05-30T02:09:26Z,http://arxiv.org/abs/2205.14807v2,"['eess.AS', 'cs.LG', 'cs.SD']","Conditional Diffusion,Probabilistic Model,Binaural Audio,Synthesis,Mono Audio,Room Reverberations,Head/Ear Filtrations,Generative Models,Benchmark Dataset,Audio Samples"
DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes,"Modeling dynamic scenes is important for many applications such as virtual
reality and telepresence. Despite achieving unprecedented fidelity for novel
view synthesis in dynamic scenes, existing methods based on Neural Radiance
Fields (NeRF) suffer from slow convergence (i.e., model training time measured
in days). In this paper, we present DeVRF, a novel representation to accelerate
learning dynamic radiance fields. The core of DeVRF is to model both the 3D
canonical space and 4D deformation field of a dynamic, non-rigid scene with
explicit and discrete voxel-based representations. However, it is quite
challenging to train such a representation which has a large number of model
parameters, often resulting in overfitting issues. To overcome this challenge,
we devise a novel static-to-dynamic learning paradigm together with a new data
capture setup that is convenient to deploy in practice. This paradigm unlocks
efficient learning of deformable radiance fields via utilizing the 3D
volumetric canonical space learnt from multi-view static images to ease the
learning of 4D voxel deformation field with only few-view dynamic sequences. To
further improve the efficiency of our DeVRF and its synthesized novel view's
quality, we conduct thorough explorations and identify a set of strategies. We
evaluate DeVRF on both synthetic and real-world dynamic scenes with different
types of deformation. Experiments demonstrate that DeVRF achieves two orders of
magnitude speedup (100x faster) with on-par high-fidelity results compared to
the previous state-of-the-art approaches. The code and dataset will be released
in https://github.com/showlab/DeVRF.","['Jia-Wei Liu', 'Yan-Pei Cao', 'Weijia Mao', 'Wenqiao Zhang', 'David Junhao Zhang', 'Jussi Keppo', 'Ying Shan', 'Xiaohu Qie', 'Mike Zheng Shou']",2022-05-31T12:13:54Z,http://arxiv.org/abs/2205.15723v2,['cs.CV'],"DeVRF,Deformable Voxel Radiance Fields,Dynamic Scenes,Neural Radiance Fields,3D Canonical Space,4D Deformation Field,Voxel-based Representations,Model Parameters,Novel View Synthesis,Data Capture Setup"
Quantifying the Effects of Working in VR for One Week,"Virtual Reality (VR) provides new possibilities for modern knowledge work.
However, the potential advantages of virtual work environments can only be used
if it is feasible to work in them for an extended period of time. Until now,
there are limited studies of long-term effects when working in VR. This paper
addresses the need for understanding such long-term effects. Specifically, we
report on a comparative study (n=16), in which participants were working in VR
for an entire week -- for five days, eight hours each day -- as well as in a
baseline physical desktop environment. This study aims to quantify the effects
of exchanging a desktop-based work environment with a VR-based environment.
Hence, during this study, we do not present the participants with the best
possible VR system but rather a setup delivering a comparable experience to
working in the physical desktop environment. The study reveals that, as
expected, VR results in significantly worse ratings across most measures. Among
other results, we found concerning levels of simulator sickness, below average
usability ratings and two participants dropped out on the first day using VR,
due to migraine, nausea and anxiety. Nevertheless, there is some indication
that participants gradually overcame negative first impressions and initial
discomfort. Overall, this study helps lay the groundwork for subsequent
research, by clearly highlighting current shortcomings and identifying
opportunities for improving the experience of working in VR.","['Verena Biener', 'Snehanjali Kalamkar', 'Negar Nouri', 'Eyal Ofek', 'Michel Pahud', 'John J. Dudley', 'Jinghui Hu', 'Per Ola Kristensson', 'Maheshya Weerasinghe', 'Klen ČopiāEPucihar', 'Matjaž Kljun', 'Stephan Streuber', 'Jens Grubert']",2022-06-07T11:21:18Z,http://arxiv.org/abs/2206.03189v2,"['cs.HC', 'I.3.7']","Virtual Reality,Knowledge work,Long-term effects,Comparative study,Simulator sickness,Usability ratings,Migraine,Nausea,Anxiety,Work environment"
"Timeline Design Space for Immersive Exploration of Time-Varying Spatial
  3D Data","Timelines are common visualizations to represent and manipulate temporal
data, from historical events storytelling to animation authoring. However,
timeline visualizations rarely consider spatio-temporal 3D data (e.g. mesh or
volumetric models) directly, which are typically explored using 3D visualizers
only displaying one time-step at a time. In this paper, leveraging the
increased workspace and 3D interaction capabilities of virtual reality, we
propose to use timelines for the visualization of 3D temporal data to support
exploration and analysis. First, we propose a timeline design space for 3D
temporal data extending the timeline design space proposed by Brehmer et al.
The proposed design space adapts the scale, layout and representation
dimensions to account for the depth dimension and how 3D temporal data can be
partitioned and structured. In our approach, an additional dimension is
introduced, the support, which further characterizes the 3D dimension of the
visualization. To complement the design space and the interaction capabilities
of VR systems, we discuss the interaction methods required for the efficient
visualization of 3D timelines. Then, to evaluate the benefits of 3D timelines,
we conducted a formal evaluation with two main objectives: comparing the
proposed visualization with a traditional visualization method; exploring how
users interact with different 3D timeline designs. Our results showed that
time-related tasks can be achieved more comfortably using timelines, and more
efficiently for specific tasks requiring the analysis of the surrounding
temporal context. Though the comparison between the different timeline designs
were inconclusive, participants reported a clear preference towards the
timeline design that did not occupy the vertical space. Finally, we illustrate
the use of the 3D timelines to a real use-case on the analysis of biological 3D
temporal datasets.","['Gwendal Fouché', 'Ferran Argelaguet', 'Emmanuel Faure', 'Charles Kervrann']",2022-06-20T17:26:36Z,http://arxiv.org/abs/2206.09910v1,"['cs.HC', 'cs.GR']","timeline,immersive exploration,time-varying spatial data,3D,spatio-temporal,virtual reality,visualization,interaction methods,evaluation,biological dataset"
"Slicing4Meta: An Intelligent Integration Framework with
  Multi-dimensional Network Resources for Metaverse-as-a-Service in Web 3.0","As the next-generation Internet paradigm, Web 3.0 encapsulates the
expectations of user-centric immersion and interaction experiences in a
decentralized manner. Metaverse, a virtual world interacting with the physical
world, is becoming one of the most potential technology to push forward with
Web 3.0. In the Metaverse, users expect to tailor immersive and interactive
experiences by customizing real-time Metaverse services (e.g.,
augmented/virtual reality and digital twin) in the three-dimensional virtual
world. Nevertheless, there are still no unified solutions for the Metaverse
services in terms of management and orchestration. It is calling for a
continuous evolution of scalable solutions and mobile systems to support
Metaverse services, and thus bring Metaverse into reality. In this paper, to
provide scalable subscription solutions for Metaverse services, we propose a
new concept, named Metaverse-as-a-Service (MaaS), in which various
physical-virtual components and technologies in the Metaverse can be delivered
as services. Furthermore, to unify the management and orchestration of MaaS, we
propose a novel framework, called Slicing4Meta, to customize Metaverse services
by intelligently integrating MaaS models and the associated multi-dimensional
resources on the components and technologies. Additionally, we propose the
classification for typical Metaverse services based on the quality of
experience (QoE) requirements and illustrate how to fulfill the QoE
requirements under the Slicing4Meta framework. We then illustrate a virtual
traveling study case, in which we examine the relationship between the QoE and
the multi-dimensional resources by quantitatively modeling the QoE of Metaverse
users. Finally, we discuss some open challenges of Slicing4Meta and propose
potential solutions to address the challenges.","['Yi-Jing Liu', 'Hongyang Du', 'Dusit Niyato', 'Gang Feng', 'Jiawen Kang', 'Zehui Xiong']",2022-08-12T01:45:52Z,http://arxiv.org/abs/2208.06081v3,['cs.NI'],"Web 3.0,Metaverse,Metaverse-as-a-Service (MaaS),Slicing4Meta,multi-dimensional network resources,virtual reality,digital twin,quality of experience (QoE),management and orchestration,mobile systems"
"Temporal View Synthesis of Dynamic Scenes through 3D Object Motion
  Estimation with Multi-Plane Images","The challenge of graphically rendering high frame-rate videos on low compute
devices can be addressed through periodic prediction of future frames to
enhance the user experience in virtual reality applications. This is studied
through the problem of temporal view synthesis (TVS), where the goal is to
predict the next frames of a video given the previous frames and the head poses
of the previous and the next frames. In this work, we consider the TVS of
dynamic scenes in which both the user and objects are moving. We design a
framework that decouples the motion into user and object motion to effectively
use the available user motion while predicting the next frames. We predict the
motion of objects by isolating and estimating the 3D object motion in the past
frames and then extrapolating it. We employ multi-plane images (MPI) as a 3D
representation of the scenes and model the object motion as the 3D displacement
between the corresponding points in the MPI representation. In order to handle
the sparsity in MPIs while estimating the motion, we incorporate partial
convolutions and masked correlation layers to estimate corresponding points.
The predicted object motion is then integrated with the given user or camera
motion to generate the next frame. Using a disocclusion infilling module, we
synthesize the regions uncovered due to the camera and object motion. We
develop a new synthetic dataset for TVS of dynamic scenes consisting of 800
videos at full HD resolution. We show through experiments on our dataset and
the MPI Sintel dataset that our model outperforms all the competing methods in
the literature.","['Nagabhushan Somraj', 'Pranali Sancheti', 'Rajiv Soundararajan']",2022-08-19T17:40:13Z,http://arxiv.org/abs/2208.09463v1,['cs.CV'],"Temporal View Synthesis,Dynamic Scenes,3D Object Motion Estimation,Multi-Plane Images,User Motion,Object Motion,MPI,Partial Convolutions,Masked Correlation Layers,Disocclusion Infilling Module"
"Defensive Distillation based Adversarial Attacks Mitigation Method for
  Channel Estimation using Deep Learning Models in Next-Generation Wireless
  Networks","Future wireless networks (5G and beyond) are the vision of forthcoming
cellular systems, connecting billions of devices and people together. In the
last decades, cellular networks have been dramatically growth with advanced
telecommunication technologies for high-speed data transmission, high cell
capacity, and low latency. The main goal of those technologies is to support a
wide range of new applications, such as virtual reality, metaverse, telehealth,
online education, autonomous and flying vehicles, smart cities, smart grids,
advanced manufacturing, and many more. The key motivation of NextG networks is
to meet the high demand for those applications by improving and optimizing
network functions. Artificial Intelligence (AI) has a high potential to achieve
these requirements by being integrated in applications throughout all layers of
the network. However, the security concerns on network functions of NextG using
AI-based models, i.e., model poising, have not been investigated deeply.
Therefore, it needs to design efficient mitigation techniques and secure
solutions for NextG networks using AI-based methods. This paper proposes a
comprehensive vulnerability analysis of deep learning (DL)-based channel
estimation models trained with the dataset obtained from MATLAB's 5G toolbox
for adversarial attacks and defensive distillation-based mitigation methods.
The adversarial attacks produce faulty results by manipulating trained DL-based
models for channel estimation in NextG networks, while making models more
robust against any attacks through mitigation methods. This paper also presents
the performance of the proposed defensive distillation mitigation method for
each adversarial attack against the channel estimation model. The results
indicated that the proposed mitigation method can defend the DL-based channel
estimation models against adversarial attacks in NextG networks.","['Ferhat Ozgur Catak', 'Murat Kuzlu', 'Evren Catak', 'Umit Cali', 'Ozgur Guler']",2022-08-12T08:35:36Z,http://arxiv.org/abs/2208.10279v1,"['cs.CR', 'cs.LG']","Defensive distillation,Adversarial attacks,Channel estimation,Deep learning models,Next-generation wireless networks"
Leaning-Based Control of an Immersive-Telepresence Robot,"In this paper, we present an implementation of a leaning-based control of a
differential drive telepresence robot and a user study in simulation, with the
goal of bringing the same functionality to a real telepresence robot. The
participants used a balance board to control the robot and viewed the virtual
environment through a head-mounted display. The main motivation for using a
balance board as the control device stems from Virtual Reality (VR) sickness;
even small movements of your own body matching the motions seen on the screen
decrease the sensory conflict between vision and vestibular organs, which lies
at the heart of most theories regarding the onset of VR sickness. To test the
hypothesis that the balance board as a control method would be less sickening
than using joysticks, we designed a user study (N=32, 15 women) in which the
participants drove a simulated differential drive robot in a virtual
environment with either a Nintendo Wii Balance Board or joysticks. However, our
pre-registered main hypotheses were not supported; the joystick did not cause
any more VR sickness on the participants than the balance board, and the board
proved to be statistically significantly more difficult to use, both
subjectively and objectively. Analyzing the open-ended questions revealed these
results to be likely connected, meaning that the difficulty of use seemed to
affect sickness; even unlimited training time before the test did not make the
use as easy as the familiar joystick. Thus, making the board easier to use is a
key to enable its potential; we present a few possibilities towards this goal.","['Joona Halkola', 'Markku Suomalainen', 'Basak Sakcak', 'Katherine J. Mimnaugh', 'Juho Kalliokoski', 'Alexis P. Chambers', 'Timo Ojala', 'Steven M. LaValle']",2022-08-22T21:37:49Z,http://arxiv.org/abs/2208.10613v1,"['cs.RO', 'cs.HC', 'cs.MM']","Leaning-based control,Immersive-telepresence robot,Differential drive,Balance board,Head-mounted display,Virtual Reality sickness,User study,Joysticks,Virtual environment"
Virtual ALMA Tour in VRChat: A Whole New Experience,"Many forefront observatories are located in remote areas and are difficult to
visit, and the global pandemic made visits even harder. Several virtual tours
have been executed on YouTube or Facebook Live, however, it is difficult to
feel a sense of immersion and these are far from the actual experience of
visiting a site. To solve this problem, we pursued an astronomy outreach event
on the virtual reality social platform VRChat. To provide an experience similar
to visiting the site, we performed a virtual tour of the ALMA Observatory in
VRChat guided by an ALMA staff member. 47 guests participated in the tour. The
post-event survey showed that the overall lecture and guided tour were very
positively accepted by the participants. Respondents answered that the
communication in the VRChat was more intensive than in other online outreach
events or on-site public talks. The ratio of respondents who answered that they
were able to communicate well with the guide was higher for those who used head
mounted displays than for those who participated in other ways. 40 answered
that the tour increased their interest in astronomy, and this did not show a
clear difference depending on how they participated. In the free descriptions
in the responses, there were noticeable mentions of the physical sensations
received from the realistic 3D space, which left a positive and strong
impression on the participants. The responses show that VRChat has the
potential to be a strong tool for astronomy communication in the pandemic and
post-pandemic eras.","['Masaaki Hiramatsu', 'S_Asagiri', 'Stella. G. Amano', 'Naohiro Takanashi', 'Shio K. Kawagoe', 'Kazuhisa Kamegai']",2022-08-23T05:27:03Z,http://arxiv.org/abs/2208.10740v1,"['astro-ph.IM', 'physics.ed-ph']","virtual reality,VRChat,ALMA Observatory,astronomy outreach,immersive experience,head mounted displays,online outreach events,pandemic,astronomy communication"
"AI and 6G into the Metaverse: Fundamentals, Challenges and Future
  Research Trends","Since Facebook was renamed Meta, a lot of attention, debate, and exploration
have intensified about what the Metaverse is, how it works, and the possible
ways to exploit it. It is anticipated that Metaverse will be a continuum of
rapidly emerging technologies, usecases, capabilities, and experiences that
will make it up for the next evolution of the Internet. Several researchers
have already surveyed the literature on artificial intelligence (AI) and
wireless communications in realizing the Metaverse. However, due to the rapid
emergence and continuous evolution of technologies, there is a need for a
comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both
in realizing the immersive experiences of Metaverse. Therefore, in this survey,
we first introduce the background and ongoing progress in augmented reality
(AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed
by the technical aspects of AI and 6G. Then, we survey the role of AI in the
Metaverse by reviewing the state-of-the-art in deep learning, computer vision,
and Edge AI to extract the requirements of 6G in Metaverse. Next, we
investigate the promising services of B5G/6G towards Metaverse, followed by
identifying the role of AI in 6G networks and 6G networks for AI in support of
Metaverse applications, and the need for sustainability in Metaverse. Finally,
we enlist the existing and potential applications, usecases, and projects to
highlight the importance of progress in the Metaverse. Moreover, in order to
provide potential research directions to researchers, we underline the
challenges, research gaps, and lessons learned identified from the literature
review of the aforementioned technologies.","['Muhammad Zawish', 'Fayaz Ali Dharejo', 'Sunder Ali Khowaja', 'Kapal Dev', 'Steven Davy', 'Nawab Muhammad Faseeh Qureshi', 'Paolo Bellavista']",2022-08-23T12:48:53Z,http://arxiv.org/abs/2208.10921v2,"['cs.AI', 'cs.HC', 'cs.NI', 'cs.SI']","AI,6G,Metaverse,augmented reality,virtual reality,mixed reality,spatial computing,deep learning,computer vision,Edge AI"
Unsupervised Learning of 3D Scene Flow with 3D Odometry Assistance,"Scene flow represents the 3D motion of each point in the scene, which
explicitly describes the distance and the direction of each point's movement.
Scene flow estimation is used in various applications such as autonomous
driving fields, activity recognition, and virtual reality fields. As it is
challenging to annotate scene flow with ground truth for real-world data, this
leaves no real-world dataset available to provide a large amount of data with
ground truth for scene flow estimation. Therefore, many works use synthesized
data to pre-train their network and real-world LiDAR data to finetune. Unlike
the previous unsupervised learning of scene flow in point clouds, we propose to
use odometry information to assist the unsupervised learning of scene flow and
use real-world LiDAR data to train our network. Supervised odometry provides
more accurate shared cost volume for scene flow. In addition, the proposed
network has mask-weighted warp layers to get a more accurate predicted point
cloud. The warp operation means applying an estimated pose transformation or
scene flow to a source point cloud to obtain a predicted point cloud and is the
key to refining scene flow from coarse to fine. When performing warp
operations, the points in different states use different weights for the pose
transformation and scene flow transformation. We classify the states of points
as static, dynamic, and occluded, where the static masks are used to divide
static and dynamic points, and the occlusion masks are used to divide occluded
points. The mask-weighted warp layer indicates that static masks and occlusion
masks are used as weights when performing warp operations. Our designs are
proved to be effective in ablation experiments. The experiment results show the
promising prospect of an odometry-assisted unsupervised learning method for 3D
scene flow in real-world data.","['Guangming Wang', 'Zhiheng Feng', 'Chaokang Jiang', 'Hesheng Wang']",2022-09-11T21:53:43Z,http://arxiv.org/abs/2209.04945v1,['cs.CV'],"3D Scene Flow,Unsupervised Learning,3D Odometry Assistance,Point Clouds,LiDAR Data,Network Training,Pose Transformation,Scene Flow Estimation,Mask-Weighted Warp Layers,Ablation Experiments"
Color-Perception-Guided Display Power Reduction for Virtual Reality,"Battery life is an increasingly urgent challenge for today's untethered VR
and AR devices. However, the power efficiency of head-mounted displays is
naturally at odds with growing computational requirements driven by better
resolution, refresh rate, and dynamic ranges, all of which reduce the sustained
usage time of untethered AR/VR devices. For instance, the Oculus Quest 2, under
a fully-charged battery, can sustain only 2 to 3 hours of operation time. Prior
display power reduction techniques mostly target smartphone displays. Directly
applying smartphone display power reduction techniques, however, degrades the
visual perception in AR/VR with noticeable artifacts. For instance, the
""power-saving mode"" on smartphones uniformly lowers the pixel luminance across
the display and, as a result, presents an overall darkened visual perception to
users if directly applied to VR content.
  Our key insight is that VR display power reduction must be cognizant of the
gaze-contingent nature of high field-of-view VR displays. To that end, we
present a gaze-contingent system that, without degrading luminance, minimizes
the display power consumption while preserving high visual fidelity when users
actively view immersive video sequences. This is enabled by constructing a
gaze-contingent color discrimination model through psychophysical studies, and
a display power model (with respect to pixel color) through real-device
measurements. Critically, due to the careful design decisions made in
constructing the two models, our algorithm is cast as a constrained
optimization problem with a closed-form solution, which can be implemented as a
real-time, image-space shader. We evaluate our system using a series of
psychophysical studies and large-scale analyses on natural images. Experiment
results show that our system reduces the display power by as much as 24% with
little to no perceptual fidelity degradation.","['Budmonde Duinkharjav', 'Kenneth Chen', 'Abhishek Tyagi', 'Jiayi He', 'Yuhao Zhu', 'Qi Sun']",2022-09-15T21:12:38Z,http://arxiv.org/abs/2209.07610v2,['cs.HC'],"Color perception,Display power reduction,Virtual reality,Battery life,Power efficiency,Head-mounted displays,Gaze-contingent system,Visual perception,Pixel luminance,Psychophysical studies"
A Tagging Solution to Discover IoT Devices in Apartments,"The number of IoT devices in smart homes is increasing. This broad adoption
facilitates users' lives, but it also brings problems. One such issue is that
some IoT devices may invade users' privacy. Some reasons for this invasion can
stem from obscure data collection practices or hidden devices. Specific IoT
devices can exist out of sight and still collect user data to send to third
parties via the Internet. Owners can easily forget the location or even the
existence of these devices, especially if the owner is a landlord who manages
several properties. The landlord-owner scenario creates multi-user problems as
designers build machines for single users. We developed tags that use wireless
protocols, buzzers, and LED lighting to lead users to solve the issue of device
discovery in shared spaces and accommodate multi-user scenarios. They are
attached to IoT devices inside a unit during their installation to be later
discovered by a tenant. These tags have similar functionalities as the popular
Tile models or Airtag, but our tags have different features based on our
privacy use case. Our tags do not require pairing; multiple users can interact
with them through our Android application. Although researchers developed
several other tools, such as thermal cameras or virtual reality (VR), for
discovering devices in environments, they have not used wireless protocols as a
solution. We measured specific performance metrics of our tags to analyze their
feasibility for this problem. We also conducted a user study to measure the
participants' comfort levels while finding objects with our tags attached. Our
results indicate that wireless tags can be viable for device tracking in
residential properties.","['Berkay Kaplan', 'Jingyu Qian', 'Israel J Lopez-Toledo', 'Carl A. Gunter']",2022-10-13T02:23:08Z,http://arxiv.org/abs/2210.06676v3,['cs.CR'],"IoT devices,tagging solution,wireless protocols,privacy,multi-user scenarios,device discovery,Android application,user study,residential properties"
"Rate-Splitting for Intelligent Reflecting Surface-Aided Multiuser VR
  Streaming","The growing demand for virtual reality (VR) applications requires wireless
systems to provide a high transmission rate to support 360-degree video
streaming to multiple users simultaneously. In this paper, we propose an
intelligent reflecting surface (IRS)-aided rate-splitting (RS) VR streaming
system. In the proposed system, RS facilitates the exploitation of the shared
interests of the users in VR streaming, and IRS creates additional propagation
channels to support the transmission of high-resolution 360-degree videos. IRS
also enhances the capability to mitigate the performance bottleneck caused by
the requirement that all RS users have to be able to decode the common message.
We formulate an optimization problem for maximization of the achievable bitrate
of the 360-degree video subject to the quality-of-service (QoS) constraints of
the users. We propose a deep deterministic policy gradient with imitation
learning (Deep-GRAIL) algorithm, in which we leverage deep reinforcement
learning (DRL) and the hidden convexity of the formulated problem to optimize
the IRS phase shifts, RS parameters, beamforming vectors, and bitrate selection
of the 360-degree video tiles. We also propose RavNet, which is a deep neural
network customized for the policy learning in our Deep-GRAIL algorithm.
Performance evaluation based on a real-world VR streaming dataset shows that
the proposed IRS-aided RS VR streaming system outperforms several baseline
schemes in terms of system sum-rate, achievable bitrate of the 360-degree
videos, and online execution runtime. Our results also reveal the respective
performance gains obtained from RS and IRS for improving the QoS in multiuser
VR streaming systems.","['Rui Huang', 'Vincent W. S. Wong', 'Robert Schober']",2022-10-21T18:38:41Z,http://arxiv.org/abs/2210.12191v3,['eess.SP'],"Rate-splitting,Intelligent reflecting surface,Multiuser,Virtual reality,Streaming,360-degree video,Quality-of-service,Deep reinforcement learning,Beamforming,Bitrate."
"Enhanced Visual Feedback with Decoupled Viewpoint Control in Immersive
  Humanoid Robot Teleoperation using SLAM","In immersive humanoid robot teleoperation, there are three main shortcomings
that can alter the transparency of the visual feedback: the lag between the
motion of the operator's and robot's head due to network communication delays
or slow robot joint motion. This latency could cause a noticeable delay in the
visual feedback, which jeopardizes the embodiment quality, can cause dizziness,
and affects the interactivity resulting in operator frequent motion pauses for
the visual feedback to settle; (ii) the mismatch between the camera's and the
headset's field-of-views (FOV), the former having generally a lower FOV; and
(iii) a mismatch between human's and robot's range of motions of the neck, the
latter being also generally lower. In order to leverage these drawbacks, we
developed a decoupled viewpoint control solution for a humanoid platform which
allows visual feedback with low-latency and artificially increases the camera's
FOV range to match that of the operator's headset. Our novel solution uses SLAM
technology to enhance the visual feedback from a reconstructed mesh,
complementing the areas that are not covered by the visual feedback from the
robot. The visual feedback is presented as a point cloud in real-time to the
operator. As a result, the operator is fed with real-time vision from the
robot's head orientation by observing the pose of the point cloud. Balancing
this kind of awareness and immersion is important in virtual reality based
teleoperation, considering the safety and robustness of the control system. An
experiment shows the effectiveness of our solution.","['Yang Chen', 'Leyuan Sun', 'Mehdi Benallegue', 'Rafael Cisneros', 'Rohan P. Singh', 'Kenji Kaneko', 'Arnaud Tanguy', 'Guillaume Caron', 'Kenji Suzuki', 'Abderrahmane Kheddar', 'Fumio Kanehiro']",2022-11-03T12:29:14Z,http://arxiv.org/abs/2211.01749v1,['cs.RO'],"immersive,humanoid robot,teleoperation,SLAM,visual feedback,viewpoint control,latency,field-of-view (FOV),range of motions,point cloud"
"Simultaneous Estimation of Hand Configurations and Finger Joint Angles
  using Forearm Ultrasound","With the advancement in computing and robotics, it is necessary to develop
fluent and intuitive methods for interacting with digital systems,
augmented/virtual reality (AR/VR) interfaces, and physical robotic systems.
Hand motion recognition is widely used to enable these interactions. Hand
configuration classification and MCP joint angle detection is important for a
comprehensive reconstruction of hand motion. sEMG and other technologies have
been used for the detection of hand motions. Forearm ultrasound images provide
a musculoskeletal visualization that can be used to understand hand motion.
Recent work has shown that these ultrasound images can be classified using
machine learning to estimate discrete hand configurations. Estimating both hand
configuration and MCP joint angles based on forearm ultrasound has not been
addressed in the literature. In this paper, we propose a CNN based deep
learning pipeline for predicting the MCP joint angles. The results for the hand
configuration classification were compared by using different machine learning
algorithms. SVC with different kernels, MLP, and the proposed CNN have been
used to classify the ultrasound images into 11 hand configurations based on
activities of daily living. Forearm ultrasound images were acquired from 6
subjects instructed to move their hands according to predefined hand
configurations. Motion capture data was acquired to get the finger angles
corresponding to the hand movements at different speeds. Average classification
accuracy of 82.7% for the proposed CNN and over 80% for SVC for different
kernels was observed on a subset of the dataset. An average RMSE of 7.35
degrees was obtained between the predicted and the true MCP joint angles. A low
latency (6.25 - 9.1 Hz) pipeline has been proposed for estimating both MCP
joint angles and hand configuration aimed at real-time control of human-machine
interfaces.","['Keshav Bimbraw', 'Christopher J. Nycz', 'Matt Schueler', 'Ziming Zhang', 'Haichong K. Zhang']",2022-11-29T02:06:19Z,http://arxiv.org/abs/2211.15871v1,"['cs.RO', 'cs.CV', 'cs.HC']","hand configurations,finger joint angles,forearm ultrasound,deep learning,machine learning algorithms,CNN,SVC,MLP,musculoskeletal visualization,motion capture"
"RT-NeRF: Real-Time On-Device Neural Radiance Fields Towards Immersive
  AR/VR Rendering","Neural Radiance Field (NeRF) based rendering has attracted growing attention
thanks to its state-of-the-art (SOTA) rendering quality and wide applications
in Augmented and Virtual Reality (AR/VR). However, immersive real-time (> 30
FPS) NeRF based rendering enabled interactions are still limited due to the low
achievable throughput on AR/VR devices. To this end, we first profile SOTA
efficient NeRF algorithms on commercial devices and identify two primary causes
of the aforementioned inefficiency: (1) the uniform point sampling and (2) the
dense accesses and computations of the required embeddings in NeRF.
Furthermore, we propose RT-NeRF, which to the best of our knowledge is the
first algorithm-hardware co-design acceleration of NeRF. Specifically, on the
algorithm level, RT-NeRF integrates an efficient rendering pipeline for largely
alleviating the inefficiency due to the commonly adopted uniform point sampling
method in NeRF by directly computing the geometry of pre-existing points.
Additionally, RT-NeRF leverages a coarse-grained view-dependent computing
ordering scheme for eliminating the (unnecessary) processing of invisible
points. On the hardware level, our proposed RT-NeRF accelerator (1) adopts a
hybrid encoding scheme to adaptively switch between a bitmap- or
coordinate-based sparsity encoding format for NeRF's sparse embeddings, aiming
to maximize the storage savings and thus reduce the required DRAM accesses
while supporting efficient NeRF decoding; and (2) integrates both a
dual-purpose bi-direction adder & search tree and a high-density sparse search
unit to coordinate the two aforementioned encoding formats. Extensive
experiments on eight datasets consistently validate the effectiveness of
RT-NeRF, achieving a large throughput improvement (e.g., 9.7x - 3,201x) while
maintaining the rendering quality as compared with SOTA efficient NeRF
solutions.","['Chaojian Li', 'Sixu Li', 'Yang Zhao', 'Wenbo Zhu', 'Yingyan Lin']",2022-12-02T12:08:42Z,http://arxiv.org/abs/2212.01120v1,['cs.AR'],"NeRF,Real-Time,On-Device,Neural Radiance Fields,AR,VR,Rendering,Algorithm-Hardware co-design,Acceleration"
"When Brain-Computer Interfaces Meet the Metaverse: Landscape,
  Demonstrator, Trends, Challenges, and Concerns","The metaverse has gained tremendous popularity in recent years, allowing the
interconnection of users worldwide. However, current systems in metaverse
scenarios, such as virtual reality glasses, offer a partial immersive
experience. In this context, Brain-Computer Interfaces (BCIs) can introduce a
revolution in the metaverse, although a study of the applicability and
implications of BCIs in these virtual scenarios is required. Based on the
absence of literature, this work reviews, for the first time, the applicability
of BCIs in the metaverse, analyzing the current status of this integration
based on different categories related to virtual worlds and the evolution of
BCIs in these scenarios in the medium and long term. This work also proposes
the design and implementation of a general framework that integrates BCIs with
different data sources from sensors and actuators (e.g., VR glasses) based on a
modular design to be easily extended. This manuscript also validates the
framework in a demonstrator consisting of driving a car within a metaverse,
using a BCI for neural data acquisition, a VR headset to provide realism, and a
steering wheel and pedals. Four use cases (UCs) are selected, focusing on
cognitive and emotional assessment of the driver, detection of drowsiness, and
driver authentication while using the vehicle. Moreover, this manuscript offers
an analysis of BCI trends in the metaverse, also identifying future challenges
that the intersection of these technologies will face. Finally, it reviews the
concerns that using BCIs in virtual world applications could generate according
to different categories: accessibility, user inclusion, privacy, cybersecurity,
physical safety, and ethics.","['Sergio López Bernal', 'Mario Quiles Pérez', 'Enrique Tomás Martínez Beltrán', 'Gregorio Martínez Pérez', 'Alberto Huertas Celdrán']",2022-12-06T17:44:03Z,http://arxiv.org/abs/2212.03169v2,['cs.HC'],"Brain-Computer Interfaces,Metaverse,Virtual reality,Immersive experience,Framework,Data sources,Sensors,Actuators,Use cases,Trends"
"Robust and Resource-efficient Machine Learning Aided Viewport Prediction
  in Virtual Reality","360-degree panoramic videos have gained considerable attention in recent
years due to the rapid development of head-mounted displays (HMDs) and
panoramic cameras. One major problem in streaming panoramic videos is that
panoramic videos are much larger in size compared to traditional ones.
Moreover, the user devices are often in a wireless environment, with limited
battery, computation power, and bandwidth. To reduce resource consumption,
researchers have proposed ways to predict the users' viewports so that only
part of the entire video needs to be transmitted from the server. However, the
robustness of such prediction approaches has been overlooked in the literature:
it is usually assumed that only a few models, pre-trained on past users'
experiences, are applied for prediction to all users. We observe that those
pre-trained models can perform poorly for some users because they might have
drastically different behaviors from the majority, and the pre-trained models
cannot capture the features in unseen videos. In this work, we propose a novel
meta learning based viewport prediction paradigm to alleviate the worst
prediction performance and ensure the robustness of viewport prediction. This
paradigm uses two machine learning models, where the first model predicts the
viewing direction, and the second model predicts the minimum video prefetch
size that can include the actual viewport. We first train two meta models so
that they are sensitive to new training data, and then quickly adapt them to
users while they are watching the videos. Evaluation results reveal that the
meta models can adapt quickly to each user, and can significantly increase the
prediction accuracy, especially for the worst-performing predictions.","['Yuang Jiang', 'Konstantinos Poularakis', 'Diego Kiedanski', 'Sastry Kompella', 'Leandros Tassiulas']",2022-12-20T01:46:18Z,http://arxiv.org/abs/2212.09945v1,"['cs.CV', 'cs.AI']","Machine Learning,Viewport Prediction,Virtual Reality,Panoramic Videos,Resource-efficient,Robustness,Meta Learning,Prefetch,Wireless Environment"
"Needs, trends, and advances in scintillators for radiographic imaging
  and tomography","Scintillators are important materials for radiographic imaging and tomography
(RadIT), when ionizing radiations are used to reveal internal structures of
materials. Since its invention by R\""ontgen, RadIT now come in many modalities
such as absorption-based X-ray radiography, phase contrast X-ray imaging,
coherent X-ray diffractive imaging, high-energy X- and $\gamma-$ray radiography
at above 1 MeV, X-ray computed tomography (CT), proton imaging and tomography
(IT), neutron IT, positron emission tomography (PET), high-energy electron
radiography, muon tomography, etc. Spatial, temporal resolution, sensitivity,
and radiation hardness, among others, are common metrics for RadIT performance,
which are enabled by, in addition to scintillators, advances in high-luminosity
accelerators and high-power lasers, photodetectors especially CMOS pixelated
sensor arrays, and lately data science. Medical imaging, nondestructive
testing, nuclear safety and safeguards are traditional RadIT applications.
Examples of growing or emerging applications include space, additive
manufacturing, machine vision, and virtual reality or `metaverse'. Scintillator
metrics such as light yield and decay time are correlated to RadIT metrics.
More than 160 kinds of scintillators and applications are presented during the
SCINT22 conference. New trends include inorganic and organic scintillator
heterostructures, liquid phase synthesis of perovskites and $\mu$m-thick films,
use of multiphysics models and data science to guide scintillator development,
structural innovations such as photonic crystals, nanoscintillators enhanced by
the Purcell effect, novel scintillator fibers, and multilayer configurations.
Opportunities exist through optimization of RadIT with reduced radiation dose,
data-driven measurements, photon/particle counting and tracking methods
supplementing time-integrated measurements, and multimodal RadIT.","['Zhehui Wang', 'Christophe Dujardin', 'Matthew S. Freeman', 'Amanda E. Gehring', 'James F. Hunter', 'Paul Lecoq', 'Wei Liu', 'Charles L. Melcher', 'C. L. Morris', 'Martin Nikl', 'Ghanshyam Pilania', 'Reeju Pokharel', 'Daniel G. Robertson', 'Daniel J. Rutstrom', 'Sky K. Sjue', 'Anton S. Tremsin', 'S. A. Watson', 'Brenden W. Wiggins', 'Nicola M. Winch', 'Mariya Zhuravleva']",2022-12-20T15:14:25Z,http://arxiv.org/abs/2212.10322v1,['physics.ins-det'],"scintillators,radiographic imaging,tomography,ionizing radiations,X-ray radiography,computed tomography,photodetectors,data science,high-luminosity accelerators,high-power lasers"
ML-powered KQI estimation for XR services. A case study on 360-Video,"The arise of cutting-edge technologies and services such as XR promise to
change the concepts of how day-to-day things are done. At the same time, the
appearance of modern and decentralized architectures approaches has given birth
to a new generation of mobile networks such as 5G, as well as outlining the
roadmap for B5G and posterior. These networks are expected to be the enablers
for bringing to life the Metaverse and other futuristic approaches. In this
sense, this work presents an ML-based (Machine Learning) framework that allows
the estimation of service Key Quality Indicators (KQIs). For this, only
information reachable to operators is required, such as statistics and
configuration parameters from these networks. This strategy prevents operators
from avoiding intrusion into the user data and guaranteeing privacy. To test
this proposal, 360-Video has been selected as a use case of Virtual Reality
(VR), from which specific KQIs are estimated such as video resolution, frame
rate, initial startup time, throughput, and latency, among others. To select
the best model for each KQI, a search grid with a cross-validation strategy has
been used to determine the best hyperparameter tuning. To boost the creation of
each KQI model, feature engineering techniques together with cross-validation
strategies have been used. The performance is assessed using MAE (Mean Average
Error) and the prediction time. The outcomes point out that KNR (K-Near
Neighbors) and RF (Random Forest) are the best algorithms in combination with
Feature Selection techniques. Likewise, this work will help as a baseline for
E2E-Quality-of-Experience-based network management working in conjunction with
network slicing, virtualization, and MEC, among other enabler technologies.","['O. S. Peñaherrera-Pulla', 'Carlos Baena', 'Sergio Fortes', 'Raquel Barco']",2022-12-08T17:30:23Z,http://arxiv.org/abs/2212.12002v1,"['cs.NI', 'cs.LG']","ML-powered,KQI estimation,XR services,360-Video,Machine Learning,Key Quality Indicators,5G,B5G,Metaverse,Virtual Reality,MAE"
Cross-Resolution Flow Propagation for Foveated Video Super-Resolution,"The demand of high-resolution video contents has grown over the years.
However, the delivery of high-resolution video is constrained by either
computational resources required for rendering or network bandwidth for remote
transmission. To remedy this limitation, we leverage the eye trackers found
alongside existing augmented and virtual reality headsets. We propose the
application of video super-resolution (VSR) technique to fuse low-resolution
context with regional high-resolution context for resource-constrained
consumption of high-resolution content without perceivable drop in quality. Eye
trackers provide us the gaze direction of a user, aiding us in the extraction
of the regional high-resolution context. As only pixels that falls within the
gaze region can be resolved by the human eye, a large amount of the delivered
content is redundant as we can't perceive the difference in quality of the
region beyond the observed region. To generate a visually pleasing frame from
the fusion of high-resolution region and low-resolution region, we study the
capability of a deep neural network of transferring the context of the observed
region to other regions (low-resolution) of the current and future frames. We
label this task a Foveated Video Super-Resolution (FVSR), as we need to
super-resolve the low-resolution regions of current and future frames through
the fusion of pixels from the gaze region. We propose Cross-Resolution Flow
Propagation (CRFP) for FVSR. We train and evaluate CRFP on REDS dataset on the
task of 8x FVSR, i.e. a combination of 8x VSR and the fusion of foveated
region. Departing from the conventional evaluation of per frame quality using
SSIM or PSNR, we propose the evaluation of past foveated region, measuring the
capability of a model to leverage the noise present in eye trackers during
FVSR. Code is made available at https://github.com/eugenelet/CRFP.","['Eugene Lee', 'Lien-Feng Hsu', 'Evan Chen', 'Chen-Yi Lee']",2022-12-27T15:38:38Z,http://arxiv.org/abs/2212.13525v1,"['cs.CV', 'cs.AI']","video super-resolution,foveated video,eye trackers,deep neural network,cross-resolution flow propagation,foveated region,CRFP,VSR,high-resolution content,low-resolution region"
"A Reconfigurable Data Glove for Reconstructing Physical and Virtual
  Grasps","In this work, we present a reconfigurable data glove design to capture
different modes of human hand-object interactions, which are critical in
training embodied artificial intelligence (AI) agents for fine manipulation
tasks. To achieve various downstream tasks with distinct features, our
reconfigurable data glove operates in three modes sharing a unified backbone
design that reconstructs hand gestures in real time. In the tactile-sensing
mode, the glove system aggregates manipulation force via customized force
sensors made from a soft and thin piezoresistive material; this design
minimizes interference during complex hand movements. The virtual reality (VR)
mode enables real-time interaction in a physically plausible fashion: A
caging-based approach is devised to determine stable grasps by detecting
collision events. Leveraging a state-of-the-art finite element method (FEM),
the simulation mode collects data on fine-grained 4D manipulation events
comprising hand and object motions in 3D space and how the object's physical
properties (e.g., stress and energy) change in accordance with manipulation
over time. Notably, the glove system presented here is the first to use
high-fidelity simulation to investigate the unobservable physical and causal
factors behind manipulation actions. In a series of experiments, we
characterize our data glove in terms of individual sensors and the overall
system. More specifically, we evaluate the system's three modes by (i)
recording hand gestures and associated forces, (ii) improving manipulation
fluency in VR, and (iii) producing realistic simulation effects of various tool
uses, respectively. Based on these three modes, our reconfigurable data glove
collects and reconstructs fine-grained human grasp data in both physical and
virtual environments, thereby opening up new avenues for the learning of
manipulation skills for embodied AI agents.","['Hangxin Liu', 'Zeyu Zhang', 'Ziyuan Jiao', 'Zhenliang Zhang', 'Minchen Li', 'Chenfanfu Jiang', 'Yixin Zhu', 'Song-Chun Zhu']",2023-01-14T05:35:50Z,http://arxiv.org/abs/2301.05821v4,"['cs.RO', 'cs.AI', 'cs.HC']","reconfigurable data glove,hand-object interactions,artificial intelligence agents,force sensors,virtual reality,finite element method,simulation mode,manipulation events,high-fidelity simulation,embodied AI agents"
"Is Embodied Interaction Beneficial? A Study on Navigating Network
  Visualizations","Network visualizations are commonly used to analyze relationships in various
contexts. To efficiently explore a network visualization, the user needs to
quickly navigate to different parts of the network and analyze local details.
Recent advancements in display and interaction technologies inspire new visions
for improved visualization and interaction design. Past research into network
design has identified some key benefits to visualizing networks in 3D versus
2D. However, little work has been done to study the impact of varying levels of
embodied interaction on network analysis. We present a controlled user study
that compared four environments featuring conditions and hardware that
leveraged different amounts of embodiment and visual perception ranging from a
2D visualization desktop environment with a standard mouse to a 3D
visualization virtual reality environment. We measured the accuracy, speed,
perceived workload, and preferences of 20 participants as they completed three
network analytic tasks, each of which required unique navigation and
substantial effort. For the task that required participants to iterate over the
entire visualization rather than focus on a specific area, we found that
participants were more accurate using a VR and a trackball mouse than
conventional desktop settings. From a workload perspective, VR was generally
considered the least mentally demanding and least frustrating in two of our
three tasks. It was also preferred and ranked as the most effective and
visually appealing condition overall. However, using VR to compare two
side-by-side networks was difficult, and it was similar to or slower than other
conditions in two of the three tasks. Overall, the accuracy and workload
advantages of conditions with greater embodiment in specific tasks suggest
promising opportunities to create more effective environments in which to
analyze network visualizations.","['Helen H. Huang', 'Hanspeter Pfister', 'Yalong Yang']",2023-01-27T03:32:19Z,http://arxiv.org/abs/2301.11516v1,"['cs.HC', 'cs.GR']","Network visualizations,Embodied interaction,3D visualization,2D visualization,Virtual reality,User study,Visual perception,Navigation,Network analysis"
"Listen2Scene: Interactive material-aware binaural sound propagation for
  reconstructed 3D scenes","We present an end-to-end binaural audio rendering approach (Listen2Scene) for
virtual reality (VR) and augmented reality (AR) applications. We propose a
novel neural-network-based binaural sound propagation method to generate
acoustic effects for indoor 3D models of real environments. Any clean audio or
dry audio can be convolved with the generated acoustic effects to render audio
corresponding to the real environment. We propose a graph neural network that
uses both the material and the topology information of the 3D scenes and
generates a scene latent vector. Moreover, we use a conditional generative
adversarial network (CGAN) to generate acoustic effects from the scene latent
vector. Our network can handle holes or other artifacts in the reconstructed 3D
mesh model. We present an efficient cost function for the generator network to
incorporate spatial audio effects. Given the source and the listener position,
our learning-based binaural sound propagation approach can generate an acoustic
effect in 0.1 milliseconds on an NVIDIA GeForce RTX 2080 Ti GPU. We have
evaluated the accuracy of our approach with binaural acoustic effects generated
using an interactive geometric sound propagation algorithm and captured real
acoustic effects / real-world recordings. We also performed a perceptual
evaluation and observed that the audio rendered by our approach is more
plausible than audio rendered using prior learning-based and geometric-based
sound propagation algorithms. We quantitatively evaluated the accuracy of our
approach using statistical acoustic parameters, and energy decay curves. The
demo videos, code and dataset are available online
(https://anton-jeran.github.io/Listen2Scene/).","['Anton Ratnarajah', 'Dinesh Manocha']",2023-02-02T04:09:23Z,http://arxiv.org/abs/2302.02809v4,"['eess.AS', 'cs.CV', 'cs.LG', 'cs.MM', 'cs.SD']","binaural sound propagation,neural network,3D scenes,acoustic effects,virtual reality (VR),augmented reality (AR),graph neural network,conditional generative adversarial network (CGAN),spatial audio effects,acoustic parameters"
"LiteVR: Interpretable and Lightweight Cybersickness Detection using
  Explainable AI","Cybersickness is a common ailment associated with virtual reality (VR) user
experiences. Several automated methods exist based on machine learning (ML) and
deep learning (DL) to detect cybersickness. However, most of these
cybersickness detection methods are perceived as computationally intensive and
black-box methods. Thus, those techniques are neither trustworthy nor practical
for deploying on standalone energy-constrained VR head-mounted devices (HMDs).
In this work, we present an explainable artificial intelligence (XAI)-based
framework, LiteVR, for cybersickness detection, explaining the model's outcome
and reducing the feature dimensions and overall computational costs. First, we
develop three cybersickness DL models based on long-term short-term memory
(LSTM), gated recurrent unit (GRU), and multilayer perceptron (MLP). Then, we
employed a post-hoc explanation, such as SHapley Additive Explanations (SHAP),
to explain the results and extract the most dominant features of cybersickness.
Finally, we retrain the DL models with the reduced number of features. Our
results show that eye-tracking features are the most dominant for cybersickness
detection. Furthermore, based on the XAI-based feature ranking and
dimensionality reduction, we significantly reduce the model's size by up to
4.3x, training time by up to 5.6x, and its inference time by up to 3.8x, with
higher cybersickness detection accuracy and low regression error (i.e., on Fast
Motion Scale (FMS)). Our proposed lite LSTM model obtained an accuracy of 94%
in classifying cybersickness and regressing (i.e., FMS 1-10) with a Root Mean
Square Error (RMSE) of 0.30, which outperforms the state-of-the-art. Our
proposed LiteVR framework can help researchers and practitioners analyze,
detect, and deploy their DL-based cybersickness detection models in standalone
VR HMDs.","['Ripan Kumar Kundu', 'Rifatul Islam', 'John Quarles', 'Khaza Anuarul Hoque']",2023-02-05T21:51:12Z,http://arxiv.org/abs/2302.03037v1,"['cs.HC', 'cs.AI', 'cs.LG']","virtual reality,cybersickness,machine learning,deep learning,explainable artificial intelligence,LSTM,GRU,MLP,SHapley Additive Explanations,eye-tracking"
"A simple cognitive model explains movement decisions during schooling in
  zebrafish","While moving, animals must frequently make decisions about their future
travel direction, whether they are alone or in a group. Here we investigate
this process for zebrafish (Danio rerio), which naturally move in cohesive
groups. Employing state-of-the-art virtual reality, we study how real fish
follow one or several moving, virtual conspecifics. These data are used to
inform, and test, a model of social response that includes a process of
explicit decision-making, whereby the fish can decide which of the virtual
conspecifics to follow, or to follow some average direction. This approach is
in contrast with previous models where the direction of motion was based on a
continuous computation, such as directional averaging. Building upon a
simplified version of this model [Sridhar et al. 2021], which was limited to a
one-dimensional projection of the fish motion, we present here a model that
describes the motion of the real fish as it swims freely in two-dimensions.
Motivated by experimental observations, the swim speed of the fish in this
model uses a burst-and-coast swimming pattern, with the burst frequency being
dependent on the distance of the fish from the followed conspecific(s). We
demonstrate that this model is able to explain the observed spatial
distribution of the real fish behind the virtual conspecifics in the
experiments. In particular, the model naturally explains the observed critical
bifurcations for a freely swimming fish, whenever the fish makes a decision to
follow only one of the virtual conspecifics, instead of following them as an
averaged group. This model can provide the foundation for modeling a cohesive
shoal of swimming fish, while explicitly describing their directional
decision-making process at the individual level","['Lital Oscar', 'Liang Li', 'Dan Gorbonos', 'Iain D. Couzin', 'Nir S. Gov']",2023-02-07T10:30:07Z,http://arxiv.org/abs/2302.03374v2,['physics.bio-ph'],"cognitive model,movement decisions,schooling,zebrafish,virtual reality,social response,decision-making,swim speed,spatial distribution,directional decision-making"
"Virtual Therapy Exergame for Upper Extremity Rehabilitation Using Smart
  Wearable Sensors","Virtual Reality (VR) has been utilized for several applications and has shown
great potential for rehabilitation, especially for home therapy. However, these
systems solely rely on information from VR hand controllers, which do not fully
capture the individual movement of the joints. In this paper, we propose a
creative VR therapy exergame for upper extremity rehabilitation using
multi-dimensional reaching tasks while simultaneously capturing hand movement
from the VR controllers and elbow joint movement from a flexible carbon
nanotube sleeve. We conducted a preliminary study with non-clinical
participants (n = 12, 7 F). In a 2x2 within-subjects study (orientation
(vertical, horizontal) x configuration (flat, curved)), we evaluated the
effectiveness and enjoyment of the exergame in different study conditions. The
results show that there was a statistically significant difference in terms of
task completion time between the two orientations. However, no significant
differences were found in the number of mistakes in both orientation and
configuration of the virtual exergame. This can lead to customizing therapy
while maintaining the same level of intensity. That is, if a patient has
restricted lower limb mobility and requires to be seated, they can use the
orientations interchangeably. The results of resistance change generated from
the carbon nanotube sleeve revealed that the flat configuration in the vertical
orientation induced more elbow stretches than the other conditions. Finally, we
reported the subjective measures based on questionnaires for usability and user
experience in different study conditions. In conclusion, the proposed VR
exergame has the potential as a multimodal sensory tool for personalized upper
extremity home-based therapy and telerehabilitation.","['Lauren Baron', 'Vuthea Chheang', 'Amit Chaudhari', 'Arooj Liaqat', 'Aishwarya Chandrasekaran', 'Yufan Wang', 'Joshua Cashaback', 'Erik Thostenson', 'Roghayeh Leila Barmaki']",2023-02-16T20:38:17Z,http://arxiv.org/abs/2302.08573v1,['cs.HC'],"Virtual Reality,Exergame,Rehabilitation,Wearable Sensors,Upper Extremity,Therapy,Virtual Reality Controllers,Carbon Nanotube Sleeve,Telerehabilitation"
"Improving Surgical Situational Awareness with Signed Distance Field: A
  Pilot Study in Virtual Reality","The introduction of image-guided surgical navigation (IGSN) has greatly
benefited technically demanding surgical procedures by providing real-time
support and guidance to the surgeon during surgery. \hi{To develop effective
IGSN, a careful selection of the surgical information and the medium to present
this information to the surgeon is needed. However, this is not a trivial task
due to the broad array of available options.} To address this problem, we have
developed an open-source library that facilitates the development of multimodal
navigation systems in a wide range of surgical procedures relying on medical
imaging data. To provide guidance, our system calculates the minimum distance
between the surgical instrument and the anatomy and then presents this
information to the user through different mechanisms. The real-time performance
of our approach is achieved by calculating Signed Distance Fields at
initialization from segmented anatomical volumes. Using this framework, we
developed a multimodal surgical navigation system to help surgeons navigate
anatomical variability in a skull base surgery simulation environment. Three
different feedback modalities were explored: visual, auditory, and haptic. To
evaluate the proposed system, a pilot user study was conducted in which four
clinicians performed mastoidectomy procedures with and without guidance. Each
condition was assessed using objective performance and subjective workload
metrics. This pilot user study showed improvements in procedural safety without
additional time or workload. These results demonstrate our pipeline's
successful use case in the context of mastoidectomy.","['Hisashi Ishida', 'Juan Antonio Barragan', 'Adnan Munawar', 'Zhaoshuo Li', 'Andy Ding', 'Peter Kazanzides', 'Danielle Trakimas', 'Francis X. Creighton', 'Russell H. Taylor']",2023-03-03T06:27:38Z,http://arxiv.org/abs/2303.01733v2,"['cs.HC', 'cs.RO']","image-guided surgical navigation,multimodal navigation system,surgical instrument,anatomy,Signed Distance Fields,surgical navigation system,skull base surgery,feedback modalities,pilot user study,mastoidectomy"
"adaPARL: Adaptive Privacy-Aware Reinforcement Learning for
  Sequential-Decision Making Human-in-the-Loop Systems","Reinforcement learning (RL) presents numerous benefits compared to rule-based
approaches in various applications. Privacy concerns have grown with the
widespread use of RL trained with privacy-sensitive data in IoT devices,
especially for human-in-the-loop systems. On the one hand, RL methods enhance
the user experience by trying to adapt to the highly dynamic nature of humans.
On the other hand, trained policies can leak the user's private information.
Recent attention has been drawn to designing privacy-aware RL algorithms while
maintaining an acceptable system utility. A central challenge in designing
privacy-aware RL, especially for human-in-the-loop systems, is that humans have
intrinsic variability and their preferences and behavior evolve. The effect of
one privacy leak mitigation can be different for the same human or across
different humans over time. Hence, we can not design one fixed model for
privacy-aware RL that fits all. To that end, we propose adaPARL, an adaptive
approach for privacy-aware RL, especially for human-in-the-loop IoT systems.
adaPARL provides a personalized privacy-utility trade-off depending on human
behavior and preference. We validate the proposed adaPARL on two IoT
applications, namely (i) Human-in-the-Loop Smart Home and (ii)
Human-in-the-Loop Virtual Reality (VR) Smart Classroom. Results obtained on
these two applications validate the generality of adaPARL and its ability to
provide a personalized privacy-utility trade-off. On average, for the first
application, adaPARL improves the utility by $57\%$ over the baseline and by
$43\%$ over randomization. adaPARL also reduces the privacy leak by $23\%$ on
average. For the second application, adaPARL decreases the privacy leak to
$44\%$ before the utility drops by $15\%$.","['Mojtaba Taherisadr', 'Stelios Andrew Stavroulakis', 'Salma Elmalaki']",2023-03-07T21:55:22Z,http://arxiv.org/abs/2303.04257v1,"['cs.LG', 'cs.AI', 'cs.CR']","Reinforcement learning,Privacy-aware,Sequential-decision making,Human-in-the-loop systems,IoT devices,Privacy leak mitigation,System utility,Personalized privacy-utility trade-off,Human behavior,Privacy leak,IoT applications"
"HOOV: Hand Out-Of-View Tracking for Proprioceptive Interaction using
  Inertial Sensing","Current Virtual Reality systems are designed for interaction under visual
control. Using built-in cameras, headsets track the user's hands or hand-held
controllers while they are inside the field of view. Current systems thus
ignore the user's interaction with off-screen content -- virtual objects that
the user could quickly access through proprioception without requiring
laborious head motions to bring them into focus. In this paper, we present
HOOV, a wrist-worn sensing method that allows VR users to interact with objects
outside their field of view. Based on the signals of a single wrist-worn
inertial sensor, HOOV continuously estimates the user's hand position in
3-space to complement the headset's tracking as the hands leave the tracking
range. Our novel data-driven method predicts hand positions and trajectories
from just the continuous estimation of hand orientation, which by itself is
stable based solely on inertial observations. Our inertial sensing
simultaneously detects finger pinching to register off-screen selection events,
confirms them using a haptic actuator inside our wrist device, and thus allows
users to select, grab, and drop virtual content. We compared HOOV's performance
with a camera-based optical motion capture system in two folds. In the first
evaluation, participants interacted based on tracking information from the
motion capture system to assess the accuracy of their proprioceptive input,
whereas in the second, they interacted based on HOOV's real-time estimations.
We found that HOOV's target-agnostic estimations had a mean tracking error of
7.7 cm, which allowed participants to reliably access virtual objects around
their body without first bringing them into focus. We demonstrate several
applications that leverage the larger input space HOOV opens up for quick
proprioceptive interaction, and conclude by discussing the potential of our
technique.","['Paul Streli', 'Rayan Armani', 'Yi Fei Cheng', 'Christian Holz']",2023-03-13T11:25:32Z,http://arxiv.org/abs/2303.07016v2,"['cs.HC', 'cs.CV', 'I.2; I.5; H.5']","Virtual Reality,Proprioceptive Interaction,Inertial Sensing,Hand Tracking,Wrist-worn Sensing,Data-driven Method,Hand Orientation,Haptic Feedback,Motion Capture System,Tracking Error"
"Implementation of communication media around a mixed reality experience
  with HoloLens headset, as part of a digitalization of a nutrition workshop","The release of Microsoft's HoloLens headset addresses new types of issues
that would have been difficult to design without such a hardware. This
semi-transparent visor headset allows the user who wears it to view the
projection of 3D virtual objects placed in its real environment. The user can
also interact with these 3D objects, which can interact with each other. The
framework of this new technology is called mixed reality. We had the
opportunity to numerically transform a conventional human nutrition workshop
for patients waiting for bariatric surgery by developing a software called
HOLO_NUTRI using the HoloLens headset. Unlike our experience of user and
conventional programmer specialized in the development of interactive 3D
graphics applications, we realized that such a mixed reality experience
required specific programming concepts quite different from those of
conventional software or those of virtual reality applications, but above all
required a thorough reflection about communication for users. In this article,
we propose to explain our design of communication (graphic supports, tutorials
of use of material, explanatory videos), a step which was crucial for the good
progress of our project. The software was used by thirty patients from Le
Puy-en-Velay Hospital during 10 sessions of one hour and a half during which
patients had to take in hand the headset and software HOLO_NUTRI. We also
proposed a series of questions to patients to have an assessment of both the
adequacy and the importance of this communication approach for such experience.
As the mixed reality technology is very recent but the number of applications
based on it significantly increases, the reflection on the implementation of
the elements of communication described in this article (videos, exercise of
learning for the use of the headset, communication leaflet, etc.) can help
developers of such applications.","['Owen Kevin Appadoo', 'Hugo Rositi', 'Sylvie Valarier', 'Marie-Claire Ombret', 'Émilie Gadéa', 'Christine Barret-Grimault', 'Christophe Lohou']",2023-03-23T07:15:28Z,http://arxiv.org/abs/2303.13079v1,"['cs.HC', 'cs.MM', 'H.5.1; J.3.2']","communication media,mixed reality,HoloLens headset,digitalization,nutrition workshop,3D virtual objects,interactive,software,programming concepts,graphic supports"
"Pedestrian Behavior Interacting with Autonomous Vehicles during Unmarked
  Midblock Multilane Crossings: Role of Infrastructure Design, AV Operations
  and Signaling","One of the main challenges autonomous vehicles (AVs) will face is interacting
with pedestrians, especially at unmarked midblock locations where the
right-of-way is unspecified. This study investigates pedestrian crossing
behavior given different roadway centerline features (i.e., undivided, two-way
left-turn lane (TWLTL), and median) and various AV operational schemes
portrayed to pedestrians through on-vehicle signals (i.e., no signal, yellow
negotiating indication, and yellow/blue negotiating/no-yield indications). This
study employs virtual reality to simulate an urban unmarked midblock
environment where pedestrians interact with AVs. Results demonstrate that both
roadway centerline design features and AV operations and signaling
significantly impact pedestrian unmarked midblock crossing behavior, including
the waiting time at the curb, waiting time in the middle of the road, and the
total crossing time. But only the roadway centerline features significantly
impact the walking time. Participants in the undivided scene spent a longer
time waiting at the curb and walking on the road than in the median and TWLTL
scenes, but they spent a shorter time waiting in the middle. Compared to the AV
without a signal, the design of yellow signal significantly reduced pedestrian
waiting time at the curb and in the middle. But yellow/blue significantly
increased the pedestrian waiting time. Interaction effects between roadway
centerline design features and AV operations and signaling are significant only
for waiting time in the middle. For middle waiting time, yellow/blue signals
had the most impact on the median road type and the least on the undivided
road. Demographics, past behaviors, and walking exposure are also explored.
Older individuals tend to wait longer, and pedestrian past crossing behaviors
and past walking exposures do not significantly impact pedestrian walking
behavior.","['Fengjiao Zou', 'Jennifer Ogle', 'Weimin Jin', 'Patrick Gerard', 'Daniel Petty', 'Andrew Robb']",2023-03-30T21:36:51Z,http://arxiv.org/abs/2303.17717v1,['cs.RO'],"pedestrian behavior,autonomous vehicles,infrastructure design,AV operations,signaling,unmarked midblock crossings,roadway centerline features,virtual reality,waiting time,crossing behavior"
"The Metaverse: Survey, Trends, Novel Pipeline Ecosystem & Future
  Directions","The Metaverse offers a second world beyond reality, where boundaries are
non-existent, and possibilities are endless through engagement and immersive
experiences using the virtual reality (VR) technology. Many disciplines can
benefit from the advancement of the Metaverse when accurately developed,
including the fields of technology, gaming, education, art, and culture.
Nevertheless, developing the Metaverse environment to its full potential is an
ambiguous task that needs proper guidance and directions. Existing surveys on
the Metaverse focus only on a specific aspect and discipline of the Metaverse
and lack a holistic view of the entire process. To this end, a more holistic,
multi-disciplinary, in-depth, and academic and industry-oriented review is
required to provide a thorough study of the Metaverse development pipeline. To
address these issues, we present in this survey a novel multi-layered pipeline
ecosystem composed of (1) the Metaverse computing, networking, communications
and hardware infrastructure, (2) environment digitization, and (3) user
interactions. For every layer, we discuss the components that detail the steps
of its development. Also, for each of these components, we examine the impact
of a set of enabling technologies and empowering domains (e.g., Artificial
Intelligence, Security & Privacy, Blockchain, Business, Ethics, and Social) on
its advancement. In addition, we explain the importance of these technologies
to support decentralization, interoperability, user experiences, interactions,
and monetization. Our presented study highlights the existing challenges for
each component, followed by research directions and potential solutions. To the
best of our knowledge, this survey is the most comprehensive and allows users,
scholars, and entrepreneurs to get an in-depth understanding of the Metaverse
ecosystem to find their opportunities and potentials for contribution.","['Hani Sami', 'Ahmad Hammoud', 'Mouhamad Arafeh', 'Mohamad Wazzeh', 'Sarhad Arisdakessian', 'Mario Chahoud', 'Osama Wehbi', 'Mohamad Ajaj', 'Azzam Mourad', 'Hadi Otrok', 'Omar Abdel Wahab', 'Rabeb Mizouni', 'Jamal Bentahar', 'Chamseddine Talhi', 'Zbigniew Dziong', 'Ernesto Damiani', 'Mohsen Guizani']",2023-04-18T18:58:14Z,http://arxiv.org/abs/2304.09240v1,"['cs.CY', 'cs.AI', 'cs.CV', 'cs.NI']","Metaverse,Virtual reality,Pipeline ecosystem,Virtual worlds,Immersive experiences,Technology,Gaming,Education,Art,Culture,Artificial Intelligence"
ArK: Augmented Reality with Knowledge Interactive Emergent Ability,"Despite the growing adoption of mixed reality and interactive AI agents, it
remains challenging for these systems to generate high quality 2D/3D scenes in
unseen environments. The common practice requires deploying an AI agent to
collect large amounts of data for model training for every new task. This
process is costly, or even impossible, for many domains. In this study, we
develop an infinite agent that learns to transfer knowledge memory from general
foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene
understanding and generation in the physical or virtual world. The heart of our
approach is an emerging mechanism, dubbed Augmented Reality with Knowledge
Inference Interaction (ArK), which leverages knowledge-memory to generate
scenes in unseen physical world and virtual reality environments. The knowledge
interactive emergent ability (Figure 1) is demonstrated as the observation
learns i) micro-action of cross-modality: in multi-modality models to collect a
large amount of relevant knowledge memory data for each interaction task (e.g.,
unseen scene understanding) from the physical reality; and ii) macro-behavior
of reality-agnostic: in mix-reality environments to improve interactions that
tailor to different characterized roles, target variables, collaborative
information, and so on. We validate the effectiveness of ArK on the scene
generation and editing tasks. We show that our ArK approach, combined with
large foundation models, significantly improves the quality of generated 2D/3D
scenes, compared to baselines, demonstrating the potential benefit of
incorporating ArK in generative AI for applications such as metaverse and
gaming simulation.","['Qiuyuan Huang', 'Jae Sung Park', 'Abhinav Gupta', 'Paul Bennett', 'Ran Gong', 'Subhojit Som', 'Baolin Peng', 'Owais Khan Mohammed', 'Chris Pal', 'Yejin Choi', 'Jianfeng Gao']",2023-05-01T17:57:01Z,http://arxiv.org/abs/2305.00970v1,['cs.CV'],"augmented reality,knowledge,emergent ability,AI agents,scene generation,transfer learning,knowledge memory,physical reality,virtual reality,generative AI"
Pushing the Limits of 3D Shape Generation at Scale,"We present a significant breakthrough in 3D shape generation by scaling it to
unprecedented dimensions. Through the adaptation of the Auto-Regressive model
and the utilization of large language models, we have developed a remarkable
model with an astounding 3.6 billion trainable parameters, establishing it as
the largest 3D shape generation model to date, named Argus-3D. Our approach
addresses the limitations of existing methods by enhancing the quality and
diversity of generated 3D shapes. To tackle the challenges of high-resolution
3D shape generation, our model incorporates tri-plane features as latent
representations, effectively reducing computational complexity. Additionally,
we introduce a discrete codebook for efficient quantization of these
representations. Leveraging the power of transformers, we enable multi-modal
conditional generation, facilitating the production of diverse and visually
impressive 3D shapes. To train our expansive model, we leverage an ensemble of
publicly-available 3D datasets, consisting of a comprehensive collection of
approximately 900,000 objects from renowned repositories such as ModelNet40,
ShapeNet, Pix3D, 3D-Future, and Objaverse. This diverse dataset empowers our
model to learn from a wide range of object variations, bolstering its ability
to generate high-quality and diverse 3D shapes. Extensive experimentation
demonstrate the remarkable efficacy of our approach in significantly improving
the visual quality of generated 3D shapes. By pushing the boundaries of 3D
generation, introducing novel methods for latent representation learning, and
harnessing the power of transformers for multi-modal conditional generation,
our contributions pave the way for substantial advancements in the field. Our
work unlocks new possibilities for applications in gaming, virtual reality,
product design, and other domains that demand high-quality and diverse 3D
objects.","['Yu Wang', 'Xuelin Qian', 'Jingyang Huo', 'Tiejun Huang', 'Bo Zhao', 'Yanwei Fu']",2023-06-20T13:01:19Z,http://arxiv.org/abs/2306.11510v2,['cs.CV'],"3D shape generation,Auto-Regressive model,language models,trainable parameters,Argus-3D,tri-plane features,computational complexity,discrete codebook,transformers,conditional generation"
Moving Avatars and Agents in Social Extended Reality Environments,"Natural interaction between multiple users within a shared virtual
environment (VE) relies on each other's awareness of the current position of
the interaction partners. This, however, cannot be warranted when users employ
noncontinuous locomotion techniques, such as teleportation, which may cause
confusion among bystanders. In this paper, we pursue two approaches to create a
pleasant experience for both the moving user and the bystanders observing that
movement. First, we will introduce a Smart Avatar system that delivers
continuous full-body human representations for noncontinuous locomotion in
shared virtual reality (VR) spaces. Smart Avatars imitate their assigned user's
real-world movements when close-by and autonomously navigate to their user when
the distance between them exceeds a certain threshold, i.e., after the user
teleports. As part of the Smart Avatar system, we implemented four avatar
transition techniques and compared them to conventional avatar locomotion in a
user study, revealing significant positive effects on the observer's spatial
awareness, as well as pragmatic and hedonic quality scores. Second, we
introduce the concept of Stuttered Locomotion, which can be applied to any
continuous locomotion method. By converting a continuous movement into
short-interval teleport steps, we provide the merits of non-continuous
locomotion for the moving user while observers can easily keep track of their
path. Thus, while the experience for observers is similarly positive as with
continuous motion, a user study confirmed that Stuttered Locomotion can
significantly reduce the occurrence of cybersickness symptoms for the moving
user, making it an attractive choice for shared VEs. We will discuss the
potential of Smart Avatars and Stuttered Locomotion for shared VR experiences,
both when applied individually and in combination.","['Jann Philipp Freiwald', 'Susanne Schmidt', 'Bernhard E. Riecke', 'Frank Steinicke']",2023-06-26T07:51:17Z,http://arxiv.org/abs/2306.14484v1,"['cs.HC', 'cs.AI']","avatars,agents,social extended reality,virtual environment,locomotion techniques,teleportation,Smart Avatar system,Stuttered Locomotion,cybersickness symptoms,shared VR experiences"
"In Time and Space: Towards Usable Adaptive Control for Assistive Robotic
  Arms","Robotic solutions, in particular robotic arms, are becoming more frequently
deployed for close collaboration with humans, for example in manufacturing or
domestic care environments. These robotic arms require the user to control
several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving
grasping and manipulating objects. Standard input devices predominantly have
two DoFs, requiring time-consuming and cognitively demanding mode switches to
select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have
shown to decrease the necessary number of mode switches but were up to now not
able to significantly reduce the perceived workload. Users still bear the
mental workload of incorporating abstract mode switching into their workflow.
We address this by providing feed-forward multimodal feedback using updated
recommendations of ADMC, allowing users to visually compare the current and the
suggested mapping in real-time. We contrast the effectiveness of two new
approaches that a) continuously recommend updated DoF combinations or b) use
discrete thresholds between current robot movements and new recommendations.
Both are compared in a Virtual Reality (VR) in-person study against a classic
control method. Significant results for lowered task completion time, fewer
mode switches, and reduced perceived workload conclusively establish that in
combination with feedforward, ADMC methods can indeed outperform classic mode
switching. A lack of apparent quantitative differences between Continuous and
Threshold reveals the importance of user-centered customization options.
Including these implications in the development process will improve usability,
which is essential for successfully implementing robotic technologies with high
user acceptance.","['Max Pascher', 'Kirill Kronhardt', 'Felix Ferdinand Goldau', 'Udo Frese', 'Jens Gerken']",2023-07-06T11:51:43Z,http://arxiv.org/abs/2307.02933v2,"['cs.HC', 'cs.AI', 'cs.RO']","Assistive robotic arms,Degrees-of-Freedom (DoFs),Adaptive DoF Mapping Controls (ADMCs),Feed-forward feedback,Virtual Reality (VR),Mode switches"
"Apple Vision Pro for Healthcare: ""The Ultimate Display""? -- Entering the
  Wonderland of Precision Medicine","At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user's eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers' eyes to ""outsiders"" or a button on
the top, called ""Digital Crown"", that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the ""Ultimate
Display"", which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.","['Jan Egger', 'Christina Gsaxner', 'Xiaojun Chen', 'Jiang Bian', 'Jens Kleesiek', 'Behrus Puladi']",2023-08-08T15:01:51Z,http://arxiv.org/abs/2308.04313v4,"['cs.AI', 'cs.GR', 'cs.HC']","Apple Vision Pro,Healthcare,Mixed Reality,Virtual Reality,Video See-Through,Augmented Reality,Varjo XR-3,Digital Crown,Ultimate Display,Precision Medicine"
Realistic pedestrian behaviour in the CARLA simulator using VR and mocap,"Simulations are gaining increasingly significance in the field of autonomous
driving due to the demand for rapid prototyping and extensive testing.
Employing physics-based simulation brings several benefits at an affordable
cost, while mitigating potential risks to prototypes, drivers, and vulnerable
road users. However, there exit two primary limitations. Firstly, the reality
gap which refers to the disparity between reality and simulation and prevents
the simulated autonomous driving systems from having the same performance in
the real world. Secondly, the lack of empirical understanding regarding the
behavior of real agents, such as backup drivers or passengers, as well as other
road users such as vehicles, pedestrians, or cyclists. Agent simulation is
commonly implemented through deterministic or randomized probabilistic
pre-programmed models, or generated from real-world data; but it fails to
accurately represent the behaviors adopted by real agents while interacting
within a specific simulated scenario. This paper extends the description of our
proposed framework to enable real-time interaction between real agents and
simulated environments, by means immersive virtual reality and human motion
capture systems within the CARLA simulator for autonomous driving. We have
designed a set of usability examples that allow the analysis of the
interactions between real pedestrians and simulated autonomous vehicles and we
provide a first measure of the user's sensation of presence in the virtual
environment.","['Sergio Martín Serrano', 'David Fernández Llorca', 'Iván García Daza', 'Miguel Ángel Sotelo Vázquez']",2023-09-08T16:30:27Z,http://arxiv.org/abs/2309.04418v1,['cs.RO'],"pedestrian behavior,CARLA simulator,VR,mocap,autonomous driving,simulation,real agents,road users,immersive virtual reality,human motion capture systems"
"WALLABY Pilot Survey: the Potential Polar Ring Galaxies NGC~4632 and
  NGC~6156","We report on the discovery of two potential polar ring galaxies (PRGs) in the
WALLABY Pilot Data Release 1 (PDR1). These untargetted detections,
cross-matched to NGC 4632 and NGC 6156, are some of the first galaxies where
the Hi observations show two distinct components. We used the iDaVIE virtual
reality software to separate the anomalous gas from the galactic gas and find
that the anomalous gas comprises ~ 50% of the total H i content of both
systems. We have generated plausible 3D kinematic models for each galaxy
assuming that the rings are circular and inclined at 90 degrees to the galaxy
bodies. These models show that the data are consistent with PRGs, but do not
definitively prove that the galaxies are PRGs. By projecting these models at
different combinations of main disk inclinations, ring orientations, and
angular resolutions in mock datacubes, we have further investigated the
detectability of similar PRGs in WALLABY. Assuming that these galaxies are
indeed PRGs, the detectability fraction, combined with the size distribution of
WALLABY PDR1 galaxies, implies an incidence rate of ~ 1% - 3%. If this rate
holds true, the WALLABY survey will detect hundreds of new polar ring galaxies.","['N. Deg', 'R. Palleske', 'K. Spekkens', 'J. Wang', 'T. Jarrett', 'J. English', 'X. Lin', 'J. Yeung', 'J. R. Mould', 'B. Catinella', 'H. Dénes', 'A. Elagali', 'B. ~-Q. For', 'P. Kamphuis', 'B. S. Koribalski', 'K. Lee-Waddell', 'C. Murugeshan', 'S. Oh', 'J. Rhee', 'P. Serra', 'T. Westmeier', 'O. I. Wong', 'K. Bekki', 'A. Bosma', 'C. Carignan', 'B. W. Holwerda', 'N. Yu']",2023-09-11T21:54:46Z,http://arxiv.org/abs/2309.05841v2,['astro-ph.GA'],"Polar ring galaxies,WALLABY,Hi observations,3D kinematic models,angular resolutions,detectability fraction"
Staged Contact-Aware Global Human Motion Forecasting,"Scene-aware global human motion forecasting is critical for manifold
applications, including virtual reality, robotics, and sports. The task
combines human trajectory and pose forecasting within the provided scene
context, which represents a significant challenge.
  So far, only Mao et al. NeurIPS'22 have addressed scene-aware global motion,
cascading the prediction of future scene contact points and the global motion
estimation. They perform the latter as the end-to-end forecasting of future
trajectories and poses. However, end-to-end contrasts with the coarse-to-fine
nature of the task and it results in lower performance, as we demonstrate here
empirically.
  We propose a STAGed contact-aware global human motion forecasting STAG, a
novel three-stage pipeline for predicting global human motion in a 3D
environment. We first consider the scene and the respective human interaction
as contact points. Secondly, we model the human trajectory forecasting within
the scene, predicting the coarse motion of the human body as a whole. The third
and last stage matches a plausible fine human joint motion to complement the
trajectory considering the estimated contacts.
  Compared to the state-of-the-art (SoA), STAG achieves a 1.8% and 16.2%
overall improvement in pose and trajectory prediction, respectively, on the
scene-aware GTA-IM dataset. A comprehensive ablation study confirms the
advantages of staged modeling over end-to-end approaches. Furthermore, we
establish the significance of a newly proposed temporal counter called the
""time-to-go"", which tells how long it is before reaching scene contact and
endpoints. Notably, STAG showcases its ability to generalize to datasets
lacking a scene and achieves a new state-of-the-art performance on CMU-Mocap,
without leveraging any social cues. Our code is released at:
https://github.com/L-Scofano/STAG","['Luca Scofano', 'Alessio Sampieri', 'Elisabeth Schiele', 'Edoardo De Matteis', 'Laura Leal-Taixé', 'Fabio Galasso']",2023-09-16T10:47:48Z,http://arxiv.org/abs/2309.08947v1,['cs.CV'],"global human motion forecasting,scene-aware,contact-aware,trajectory forecasting,pose forecasting,end-to-end,STAG,3D environment,ablation study,time-to-go"
"Eve Said Yes: AirBone Authentication for Head-Wearable Smart Voice
  Assistant","Recent advances in machine learning and natural language processing have
fostered the enormous prosperity of smart voice assistants and their services,
e.g., Alexa, Google Home, Siri, etc. However, voice spoofing attacks are deemed
to be one of the major challenges of voice control security, and never stop
evolving such as deep-learning-based voice conversion and speech synthesis
techniques. To solve this problem outside the acoustic domain, we focus on
head-wearable devices, such as earbuds and virtual reality (VR) headsets, which
are feasible to continuously monitor the bone-conducted voice in the vibration
domain. Specifically, we identify that air and bone conduction (AC/BC) from the
same vocalization are coupled (or concurrent) and user-level unique, which
makes them suitable behavior and biometric factors for multi-factor
authentication (MFA). The legitimate user can defeat acoustic domain and even
cross-domain spoofing samples with the proposed two-stage AirBone
authentication. The first stage answers \textit{whether air and bone conduction
utterances are time domain consistent (TC)} and the second stage runs
\textit{bone conduction speaker recognition (BC-SR)}. The security level is
hence increased for two reasons: (1) current acoustic attacks on smart voice
assistants cannot affect bone conduction, which is in the vibration domain; (2)
even for advanced cross-domain attacks, the unique bone conduction features can
detect adversary's impersonation and machine-induced vibration. Finally,
AirBone authentication has good usability (the same level as voice
authentication) compared with traditional MFA and those specially designed to
enhance smart voice security. Our experimental results show that the proposed
AirBone authentication is usable and secure, and can be easily equipped by
commercial off-the-shelf head wearables with good user experience.","['Chenpei Huang', 'Hui Zhong', 'Jie Lian', 'Pavana Prakash', 'Dian Shi', 'Yuan Xu', 'Miao Pan']",2023-09-26T19:03:45Z,http://arxiv.org/abs/2309.15203v1,"['cs.CR', 'cs.HC', 'eess.SP']","machine learning,natural language processing,voice spoofing attacks,deep-learning-based voice conversion,speech synthesis techniques,bone-conducted voice,air and bone conduction,multi-factor authentication,acoustic attacks,smart voice assistants"
"AdaptiX -- A Transitional XR Framework for Development and Evaluation of
  Shared Control Applications in Assistive Robotics","With the ongoing efforts to empower people with mobility impairments and the
increase in technological acceptance by the general public, assistive
technologies, such as collaborative robotic arms, are gaining popularity. Yet,
their widespread success is limited by usability issues, specifically the
disparity between user input and software control along the autonomy continuum.
To address this, shared control concepts provide opportunities to combine the
targeted increase of user autonomy with a certain level of computer assistance.
This paper presents the free and open-source AdaptiX XR framework for
developing and evaluating shared control applications in a high-resolution
simulation environment. The initial framework consists of a simulated robotic
arm with an example scenario in Virtual Reality (VR), multiple standard control
interfaces, and a specialized recording/replay system. AdaptiX can easily be
extended for specific research needs, allowing Human-Robot Interaction (HRI)
researchers to rapidly design and test novel interaction methods, intervention
strategies, and multi-modal feedback techniques, without requiring an actual
physical robotic arm during the early phases of ideation, prototyping, and
evaluation. Also, a Robot Operating System (ROS) integration enables the
controlling of a real robotic arm in a PhysicalTwin approach without any
simulation-reality gap. Here, we review the capabilities and limitations of
AdaptiX in detail and present three bodies of research based on the framework.
AdaptiX can be accessed at https://adaptix.robot-research.de.","['Max Pascher', 'Felix Ferdinand Goldau', 'Kirill Kronhardt', 'Udo Frese', 'Jens Gerken']",2023-10-24T14:44:41Z,http://arxiv.org/abs/2310.15887v3,"['cs.HC', 'cs.AI', 'cs.RO', 'cs.SE', 'H.5; I.2.9; D.2.11; B.4.2']","Transitional XR Framework,Shared Control Applications,Assistive Robotics,Simulation Environment,Human-Robot Interaction,Virtual Reality (VR),Robot Operating System (ROS),PhysicalTwin Approach,Interaction Methods,Multi-modal Feedback Techniques"
"Reconstructing Human Pose from Inertial Measurements: A Generative
  Model-based Compressive Sensing Approach","The ability to sense, localize, and estimate the 3D position and orientation
of the human body is critical in virtual reality (VR) and extended reality (XR)
applications. This becomes more important and challenging with the deployment
of VR/XR applications over the next generation of wireless systems such as 5G
and beyond. In this paper, we propose a novel framework that can reconstruct
the 3D human body pose of the user given sparse measurements from Inertial
Measurement Unit (IMU) sensors over a noisy wireless environment. Specifically,
our framework enables reliable transmission of compressed IMU signals through
noisy wireless channels and effective recovery of such signals at the receiver,
e.g., an edge server. This task is very challenging due to the constraints of
transmit power, recovery accuracy, and recovery latency. To address these
challenges, we first develop a deep generative model at the receiver to recover
the data from linear measurements of IMU signals. The linear measurements of
the IMU signals are obtained by a linear projection with a measurement matrix
based on the compressive sensing theory. The key to the success of our
framework lies in the novel design of the measurement matrix at the
transmitter, which can not only satisfy power constraints for the IMU devices
but also obtain a highly accurate recovery for the IMU signals at the receiver.
This can be achieved by extending the set-restricted eigenvalue condition of
the measurement matrix and combining it with an upper bound for the power
transmission constraint. Our framework can achieve robust performance for
recovering 3D human poses from noisy compressed IMU signals. Additionally, our
pre-trained deep generative model achieves signal reconstruction accuracy
comparable to an optimization-based approach, i.e., Lasso, but is an order of
magnitude faster.","['Nguyen Quang Hieu', 'Dinh Thai Hoang', 'Diep N. Nguyen', 'Mohammad Abu Alsheikh']",2023-10-31T07:13:11Z,http://arxiv.org/abs/2310.20228v3,['cs.HC'],"3D position,orientation,human body,Inertial Measurement Unit (IMU),wireless systems,compressive sensing,generative model,linear measurements,measurement matrix"
SparsePoser: Real-time Full-body Motion Reconstruction from Sparse Data,"Accurate and reliable human motion reconstruction is crucial for creating
natural interactions of full-body avatars in Virtual Reality (VR) and
entertainment applications. As the Metaverse and social applications gain
popularity, users are seeking cost-effective solutions to create full-body
animations that are comparable in quality to those produced by commercial
motion capture systems. In order to provide affordable solutions, though, it is
important to minimize the number of sensors attached to the subject's body.
Unfortunately, reconstructing the full-body pose from sparse data is a heavily
under-determined problem. Some studies that use IMU sensors face challenges in
reconstructing the pose due to positional drift and ambiguity of the poses. In
recent years, some mainstream VR systems have released 6-degree-of-freedom
(6-DoF) tracking devices providing positional and rotational information.
Nevertheless, most solutions for reconstructing full-body poses rely on
traditional inverse kinematics (IK) solutions, which often produce
non-continuous and unnatural poses. In this article, we introduce SparsePoser,
a novel deep learning-based solution for reconstructing a full-body pose from a
reduced set of six tracking devices. Our system incorporates a
convolutional-based autoencoder that synthesizes high-quality continuous human
poses by learning the human motion manifold from motion capture data. Then, we
employ a learned IK component, made of multiple lightweight feed-forward neural
networks, to adjust the hands and feet toward the corresponding trackers. We
extensively evaluate our method on publicly available motion capture datasets
and with real-time live demos. We show that our method outperforms
state-of-the-art techniques using IMU sensors or 6-DoF tracking devices, and
can be used for users with different body dimensions and proportions.","['Jose Luis Ponton', 'Haoran Yun', 'Andreas Aristidou', 'Carlos Andujar', 'Nuria Pelechano']",2023-11-03T18:48:01Z,http://arxiv.org/abs/2311.02191v1,"['cs.GR', 'cs.AI']","SparsePoser,full-body motion reconstruction,sparse data,Virtual Reality,entertainment applications,Metaverse,IMU sensors,6-degree-of-freedom,6-DoF tracking devices,inverse kinematics"
"EU COST Action on future generation optical wireless communication
  technologies, 2nd White paper","NEWFOCUS is an EU COST Action targeted at exploring radical solutions that
could influence the design of future wireless networks. The project aims to
address some of the challenges associated with optical wireless communication
(OWC) and to establish it as a complementary technology to the radio frequency
(RF)-based wireless systems in order to meet the demanding requirements of the
fifth generation (5G) and the future sixth generation (6G) backhaul and access
networks. Only 6G will be able to widely serve the exponential growth in
connected devices (i.e., more than 500 billion) in 2030, real-time holographic
communication, future virtual reality, etc. Space is emerging as the new
frontier in 5 and 6G and beyond communication networks, where it offers
high-speed wireless coverage to remote areas both in lands and sees. This
activity is supported by the recent development of low-altitude Earth orbit
satellite mega-constellations. The focus of this 2nd White Paper is on the use
of OWC as an enabling technology for medium- and long-range links for
deployment in (i) smart-cities and intelligent transportation systems; (ii)
first- and last-mile access and backhaul/fronthaul wireless networks; (iii)
hybrid free-space optics/RF adaptive wireless connections; (iv)
space-to-ground, inter-satellite, ground-to-air, and air-to-air communications;
and (v) underwater communications.","['Z. Ghassemlooy', 'M. A. Khalighi', 'S. Zvanovec', 'A. Shrestha', 'B. Ortega', 'M. Petkovic', 'X. Pang', 'C. Sirtori', 'D. Orsucci', 'A. Shrestha', 'F. Moll', 'G. Cossu', 'V. Spirito', 'M. P. Ninos', 'E. Ciaramella', 'J. Bas', 'M. Amay', 'S. Huang', 'M. Safari', 'T. Gutema', 'W. Popoola', 'Vicente Matus', 'Jose Rabadan', 'Rafael Perez-Jimenez', 'E. Panayirci', 'P. D. Diamantoulakis', 'H. Haas', 'I. C. Ijeh']",2023-06-14T06:53:10Z,http://arxiv.org/abs/2311.02511v1,"['cs.NI', 'eess.SP', 'physics.optics', 'quant-ph']","future generation,optical wireless communication,5G,6G,radio frequency,OWC,backhaul,access networks,smart-cities,free-space optics"
"Well-being in isolation: Exploring artistic immersive virtual
  environments in a simulated lunar habitat to alleviate asthenia symptoms","Revived interest in lunar and planetary exploration is heralding a new era
for human spaceflight, characterized by frequent strain on astronaut's mental
well-being, which stems from increased exposure to isolated, confined, and
extreme (ICE) conditions. Whilst Immersive Virtual Reality (IVR) has been
employed to facilitate self-help interventions to mitigate challenges caused by
isolated environments in several domains, its applicability in support of
future space expeditions remains largely unexplored. To address this
limitation, we administered the use of distinct IVR environments to crew
members (n=5) partaking in a simulated lunar habitat study. Utilizing a
Bayesian approach to scrutinize small group data, we discovered a significant
relationship between IVR usage and a reduction in perceived stress-related
symptoms, particularly those associated with asthenia (syndrome often linked to
chronic fatigue and weakness; a condition characterized by feelings of energy
depletion or exhaustion that can be amplified in ICE conditions). The
reductions were most prominent with the use of interactive virtual
environments. The 'Aesthetic Realities' - virtual environments conceived as art
exhibits - received exceptional praise from our participants. These
environments mark a fascinating convergence of art and science, holding promise
to mitigate effects related to isolation in spaceflight training and beyond.","['Grzegorz Pochwatko', 'Wieslaw Kopec', 'Justyna Swidrak', 'Anna Jaskulska', 'Kinga H. Skorupska', 'Barbara Karpowicz', 'RafaŁEMasłyk', 'Maciej Grzeszczuk', 'Steven Barnes', 'Paulina Borkiewicz', 'PaweŁEKobyliński', 'MichaŁEPabiŁEOrzeszyna', 'Robert Balas', 'Jagoda Lazarek', 'Florian Dufresne', 'Leonie Bensch', 'Tommy Nilsson']",2023-11-15T20:04:00Z,http://arxiv.org/abs/2311.09343v1,"['cs.HC', 'cs.CY', '93B51, 97M50', 'H.1.2; I.3.8; J.4; J.m; K.8.2; J.6']","well-being,isolation,artistic immersive virtual environments,asthenia symptoms,lunar habitat,astronaut,Immersive Virtual Reality,self-help interventions,isolated environments,stress-related symptoms"
Implicit Learning of Scene Geometry from Poses for Global Localization,"Global visual localization estimates the absolute pose of a camera using a
single image, in a previously mapped area. Obtaining the pose from a single
image enables many robotics and augmented/virtual reality applications.
Inspired by latest advances in deep learning, many existing approaches directly
learn and regress 6 DoF pose from an input image. However, these methods do not
fully utilize the underlying scene geometry for pose regression. The challenge
in monocular relocalization is the minimal availability of supervised training
data, which is just the corresponding 6 DoF poses of the images. In this paper,
we propose to utilize these minimal available labels (.i.e, poses) to learn the
underlying 3D geometry of the scene and use the geometry to estimate the 6 DoF
camera pose. We present a learning method that uses these pose labels and rigid
alignment to learn two 3D geometric representations (\textit{X, Y, Z
coordinates}) of the scene, one in camera coordinate frame and the other in
global coordinate frame. Given a single image, it estimates these two 3D scene
representations, which are then aligned to estimate a pose that matches the
pose label. This formulation allows for the active inclusion of additional
learning constraints to minimize 3D alignment errors between the two 3D scene
representations, and 2D re-projection errors between the 3D global scene
representation and 2D image pixels, resulting in improved localization
accuracy. During inference, our model estimates the 3D scene geometry in camera
and global frames and aligns them rigidly to obtain pose in real-time. We
evaluate our work on three common visual localization datasets, conduct
ablation studies, and show that our method exceeds state-of-the-art regression
methods' pose accuracy on all datasets.","['Mohammad Altillawi', 'Shile Li', 'Sai Manoj Prakhya', 'Ziyuan Liu', 'Joan Serrat']",2023-12-04T16:51:23Z,http://arxiv.org/abs/2312.02029v1,"['cs.CV', 'cs.RO']","absolute pose,camera pose,deep learning,6 DoF,monocular relocalization,supervised training data,3D geometry,pose labels,rigid alignment,global coordinate frame"
"Random resistive memory-based deep extreme point learning machine for
  unified visual processing","Visual sensors, including 3D LiDAR, neuromorphic DVS sensors, and
conventional frame cameras, are increasingly integrated into edge-side
intelligent machines. Realizing intensive multi-sensory data analysis directly
on edge intelligent machines is crucial for numerous emerging edge
applications, such as augmented and virtual reality and unmanned aerial
vehicles, which necessitates unified data representation, unprecedented
hardware energy efficiency and rapid model training. However, multi-sensory
data are intrinsically heterogeneous, causing significant complexity in the
system development for edge-side intelligent machines. In addition, the
performance of conventional digital hardware is limited by the physically
separated processing and memory units, known as the von Neumann bottleneck, and
the physical limit of transistor scaling, which contributes to the slowdown of
Moore's law. These limitations are further intensified by the tedious training
of models with ever-increasing sizes. We propose a novel hardware-software
co-design, random resistive memory-based deep extreme point learning machine
(DEPLM), that offers efficient unified point set analysis. We show the system's
versatility across various data modalities and two different learning tasks.
Compared to a conventional digital hardware-based system, our co-design system
achieves huge energy efficiency improvements and training cost reduction when
compared to conventional systems. Our random resistive memory-based deep
extreme point learning machine may pave the way for energy-efficient and
training-friendly edge AI across various data modalities and tasks.","['Shaocong Wang', 'Yizhao Gao', 'Yi Li', 'Woyu Zhang', 'Yifei Yu', 'Bo Wang', 'Ning Lin', 'Hegan Chen', 'Yue Zhang', 'Yang Jiang', 'Dingchen Wang', 'Jia Chen', 'Peng Dai', 'Hao Jiang', 'Peng Lin', 'Xumeng Zhang', 'Xiaojuan Qi', 'Xiaoxin Xu', 'Hayden So', 'Zhongrui Wang', 'Dashan Shang', 'Qi Liu', 'Kwang-Ting Cheng', 'Ming Liu']",2023-12-14T09:46:16Z,http://arxiv.org/abs/2312.09262v1,"['cs.LG', 'cs.AR']","Random resistive memory,deep extreme point learning machine,visual sensors,multi-sensory data analysis,edge intelligent machines,hardware-software co-design,von Neumann bottleneck,transistor scaling,Moore's law,energy efficiency"
"Room Acoustic Rendering Networks with Control of Scattering and Early
  Reflections","Room acoustic synthesis can be used in Virtual Reality (VR), Augmented
Reality (AR) and gaming applications to enhance listeners' sense of immersion,
realism and externalisation. A common approach is to use Geometrical Acoustics
(GA) models to compute impulse responses at interactive speed, and fast
convolution methods to apply said responses in real time. Alternatively,
delay-network-based models are capable of modeling certain aspects of room
acoustics, but with a significantly lower computational cost. In order to
bridge the gap between these classes of models, recent work introduced delay
network designs that approximate Acoustic Radiance Transfer (ART), a GA model
that simulates the transfer of acoustic energy between discrete surface patches
in an environment. This paper presents two key extensions of such designs. The
first extension involves a new physically-based and stability-preserving design
of the feedback matrices, enabling more accurate control of scattering and,
more in general, of late reverberation properties. The second extension allows
an arbitrary number of early reflections to be modeled with high accuracy,
meaning the network can be scaled at will between computational cost and early
reverb precision. The proposed extensions are compared to the baseline
ART-approximating delay network as well as two reference GA models. The
evaluation is based on objective measures of perceptually-relevant features,
including frequency-dependent reverberation times, echo density build-up, and
early decay time. Results show how the proposed extensions result in a
significant improvement over the baseline model, especially for the case of
non-convex geometries or the case of unevenly distributed wall absorption, both
scenarios of broad practical interest.","['Matteo Scerbo', 'Lauri Savioja', 'Enzo De Sena']",2023-12-22T12:47:23Z,http://arxiv.org/abs/2312.14658v1,"['cs.SD', 'eess.AS', '76Q05 (Primary) 93C43, 94A12 (Secondary)']","Room acoustic synthesis,Virtual Reality,Augmented Reality,gaming applications,Geometrical Acoustics,impulse responses,fast convolution methods,delay-network-based models,Acoustic Radiance Transfer,feedback matrices,scattering control,late reverberation properties,early reflections,computational cost,reverberation times,echo density,early decay time,non-convex geometries,wall absorption"
Agent AI: Surveying the Horizons of Multimodal Interaction,"Multi-modal AI systems will likely become a ubiquitous presence in our
everyday lives. A promising approach to making these systems more interactive
is to embody them as agents within physical and virtual environments. At
present, systems leverage existing foundation models as the basic building
blocks for the creation of embodied agents. Embedding agents within such
environments facilitates the ability of models to process and interpret visual
and contextual data, which is critical for the creation of more sophisticated
and context-aware AI systems. For example, a system that can perceive user
actions, human behavior, environmental objects, audio expressions, and the
collective sentiment of a scene can be used to inform and direct agent
responses within the given environment. To accelerate research on agent-based
multimodal intelligence, we define ""Agent AI"" as a class of interactive systems
that can perceive visual stimuli, language inputs, and other
environmentally-grounded data, and can produce meaningful embodied actions. In
particular, we explore systems that aim to improve agents based on
next-embodied action prediction by incorporating external knowledge,
multi-sensory inputs, and human feedback. We argue that by developing agentic
AI systems in grounded environments, one can also mitigate the hallucinations
of large foundation models and their tendency to generate environmentally
incorrect outputs. The emerging field of Agent AI subsumes the broader embodied
and agentic aspects of multimodal interactions. Beyond agents acting and
interacting in the physical world, we envision a future where people can easily
create any virtual reality or simulated scene and interact with agents embodied
within the virtual environment.","['Zane Durante', 'Qiuyuan Huang', 'Naoki Wake', 'Ran Gong', 'Jae Sung Park', 'Bidipta Sarkar', 'Rohan Taori', 'Yusuke Noda', 'Demetri Terzopoulos', 'Yejin Choi', 'Katsushi Ikeuchi', 'Hoi Vo', 'Li Fei-Fei', 'Jianfeng Gao']",2024-01-07T19:11:18Z,http://arxiv.org/abs/2401.03568v2,"['cs.AI', 'cs.HC', 'cs.LG']","Agent AI,multimodal interaction,embodied agents,foundation models,visual stimuli,language inputs,multi-sensory inputs,human feedback,interactive systems,virtual environment"
"Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via
  Transformer-Based 360 Image Outpainting","360 images, with a field-of-view (FoV) of 180x360, provide immersive and
realistic environments for emerging virtual reality (VR) applications, such as
virtual tourism, where users desire to create diverse panoramic scenes from a
narrow FoV photo they take from a viewpoint via portable devices. It thus
brings us to a technical challenge: `How to allow the users to freely create
diverse and immersive virtual scenes from a narrow FoV image with a specified
viewport?' To this end, we propose a transformer-based 360 image outpainting
framework called Dream360, which can generate diverse, high-fidelity, and
high-resolution panoramas from user-selected viewports, considering the
spherical properties of 360 images. Compared with existing methods, e.g., [3],
which primarily focus on inputs with rectangular masks and central locations
while overlooking the spherical property of 360 images, our Dream360 offers
higher outpainting flexibility and fidelity based on the spherical
representation. Dream360 comprises two key learning stages: (I) codebook-based
panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware
refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN
learns a sphere-specific codebook from spherical harmonic (SH) values,
providing a better representation of spherical data distribution for scene
modeling. The frequency-aware refinement matches the resolution and further
improves the semantic consistency and visual fidelity of the generated results.
Our Dream360 achieves significantly lower Frechet Inception Distance (FID)
scores and better visual fidelity than existing methods. We also conducted a
user study involving 15 participants to interactively evaluate the quality of
the generated results in VR, demonstrating the flexibility and superiority of
our Dream360 framework.","['Hao Ai', 'Zidong Cao', 'Haonan Lu', 'Chen Chen', 'Jian Ma', 'Pengyuan Zhou', 'Tae-Kyun Kim', 'Pan Hui', 'Lin Wang']",2024-01-19T09:01:20Z,http://arxiv.org/abs/2401.10564v1,"['cs.CV', 'cs.HC']","Transformer-based,360 images,Virtual reality,Panoramic scenes,Spherical properties,Outpainting,High-fidelity,High-resolution,Codebook-based,Frequency-aware"
"Experiencing an elongated limb in virtual reality modifies the tactile
  distance perception of the corresponding real limb","In measurement, a reference frame is needed to compare the measured object to
something already known. This raises the neuroscientific question of which
reference frame is used by humans when exploring the environment. Previous
studies suggested that, in touch, the body employed as measuring tool also
serves as reference frame. Indeed, an artificial modification of the perceived
dimensions of the body changes the tactile perception of external object
dimensions. However, it is unknown if such a change in tactile perception would
occur when the body schema is modified through the illusion of owning an limb
altered in size. Therefore, employing a virtual hand illusion paradigm with an
elongated forearm of different lengths, we systematically tested the subjective
perception of distance between two points (tactile distance perception task,
TDP task) on the corresponding real forearm following the illusion. Thus, TDP
task is used as a proxy to gauge changes in the body schema. Embodiment of the
virtual arm was found significantly greater after the synchronous visuo-tactile
stimulation condition compared to the asynchronous one, and the forearm
elongation significantly increased the TDP. However, we did not find any link
between the visuo-tactile induced ownership over the elongated arm and TDP
variation, suggesting that vision plays the main role in the modification of
the body schema. Additionally, significant effect of elongation found on TDP
but not on proprioception suggests that these are affected differently by body
schema modifications. These findings confirm the body schema malleability and
its role as reference frame in touch.","['François Le Jeune', ""Marco D'Alonzo"", 'Valeria Piombino', 'Alessia Noccaro', 'Domenico Formica', 'Giovanni Di Pino']",2024-01-23T09:59:00Z,http://arxiv.org/abs/2401.12601v1,['q-bio.NC'],"virtual reality,tactile perception,body schema,reference frame,embodiment,visuo-tactile stimulation,proprioception,illusion,malleability"
SAWEC: Sensing-Assisted Wireless Edge Computing,"Emerging mobile virtual reality (VR) systems will require to continuously
perform complex computer vision tasks on ultra-high-resolution video frames
through the execution of deep neural networks (DNNs)-based algorithms. Since
state-of-the-art DNNs require computational power that is excessive for mobile
devices, techniques based on wireless edge computing (WEC) have been recently
proposed. However, existing WEC methods require the transmission and processing
of a high amount of video data which may ultimately saturate the wireless link.
In this paper, we propose a novel Sensing-Assisted Wireless Edge Computing
(SAWEC) paradigm to address this issue. SAWEC leverages knowledge about the
physical environment to reduce the end-to-end latency and overall computational
burden by transmitting to the edge server only the relevant data for the
delivery of the service. Our intuition is that the transmission of the portion
of the video frames where there are no changes with respect to previous frames
can be avoided. Specifically, we leverage wireless sensing techniques to
estimate the location of objects in the environment and obtain insights about
the environment dynamics. Hence, only the part of the frames where any
environmental change is detected is transmitted and processed. We evaluated
SAWEC by using a 10K 360$^{\circ}$ with a Wi-Fi 6 sensing system operating at
160 MHz and performing localization and tracking. We considered instance
segmentation and object detection as benchmarking tasks for performance
evaluation. We carried out experiments in an anechoic chamber and an entrance
hall with two human subjects in six different setups. Experimental results show
that SAWEC reduces both the channel occupation and end-to-end latency by more
than 90% while improving the instance segmentation and object detection
performance with respect to state-of-the-art WEC approaches.","['Khandaker Foysal Haque', 'Francesca Meneghello', 'Md. Ebtidaul Karim', 'Francesco Restuccia']",2024-02-15T15:39:46Z,http://arxiv.org/abs/2402.10021v2,"['cs.CV', 'cs.NI']","Sensing,Wireless Edge Computing,Deep Neural Networks,Video Frames,Mobile Devices,Edge Server,End-to-end Latency,Computational Burden,Wireless Sensing Techniques,Object Detection."
"Combinatorial Client-Master Multiagent Deep Reinforcement Learning for
  Task Offloading in Mobile Edge Computing","Recently, there has been an explosion of mobile applications that perform
computationally intensive tasks such as video streaming, data mining, virtual
reality, augmented reality, image processing, video processing, face
recognition, and online gaming. However, user devices (UDs), such as tablets
and smartphones, have a limited ability to perform the computation needs of the
tasks. Mobile edge computing (MEC) has emerged as a promising technology to
meet the increasing computing demands of UDs. Task offloading in MEC is a
strategy that meets the demands of UDs by distributing tasks between UDs and
MEC servers. Deep reinforcement learning (DRL) is gaining attention in
task-offloading problems because it can adapt to dynamic changes and minimize
online computational complexity. However, the various types of continuous and
discrete resource constraints on UDs and MEC servers pose challenges to the
design of an efficient DRL-based task-offloading strategy. Existing DRL-based
task-offloading algorithms focus on the constraints of the UDs, assuming the
availability of enough storage resources on the server. Moreover, existing
multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents
and consider homogeneous constraints as a penalty in their reward function. We
proposed a novel combinatorial client-master MADRL (CCM\_MADRL) algorithm for
task offloading in MEC (CCM\_MADRL\_MEC) that enables UDs to decide their
resource requirements and the server to make a combinatorial decision based on
the requirements of the UDs. CCM\_MADRL\_MEC is the first MADRL in task
offloading to consider server storage capacity in addition to the constraints
in the UDs. By taking advantage of the combinatorial action selection,
CCM\_MADRL\_MEC has shown superior convergence over existing MADDPG and
heuristic algorithms.","['Tesfay Zemuy Gebrekidan', 'Sebastian Stein', 'Timothy J. Norman']",2024-02-18T17:17:15Z,http://arxiv.org/abs/2402.11653v1,"['cs.AI', 'cs.DC', 'cs.NI', 'I.2.11']","Combinatorial Client-Master Multiagent Deep Reinforcement Learning,Task Offloading,Mobile Edge Computing,User Devices,MEC Servers,Deep Reinforcement Learning (DRL),Resource Constraints,Multiagent DRL (MADRL),Server Storage Capacity,MADDPG"
"Am I the Odd One? Exploring (In)Congruencies in the Realism of Avatars
  and Virtual Others in Virtual Reality","Virtual humans play a pivotal role in social virtual environments, shaping
users' VR experiences. The diversity in available options and users'
preferences can result in a heterogeneous mix of appearances among a group of
virtual humans. The resulting variety in higher-order anthropomorphic and
realistic cues introduces multiple (in)congruencies, eventually impacting the
plausibility of the experience. In this work, we consider the impact of
(in)congruencies in the realism of a group of virtual humans, including
co-located others and one's self-avatar. In a 2 x 3 mixed design, participants
embodied either (1) a personalized realistic or (2) a customized stylized
self-avatar across three consecutive VR exposures in which they were
accompanied by a group of virtual others being either (1) all realistic, (2)
all stylized, or (3) mixed. Our results indicate groups of virtual others of
higher realism, i.e., potentially more congruent with participants' real-world
experiences and expectations, were considered more human-like, increasing the
feeling of co-presence and the impression of interaction possibilities.
(In)congruencies concerning the homogeneity of the group did not cause
considerable effects. Furthermore, our results indicate that a self-avatar's
congruence with the participant's real-world experiences concerning their own
physical body yielded notable benefits for virtual body ownership and
self-identification for realistic personalized avatars. Notably, the
incongruence between a stylized self-avatar and a group of realistic virtual
others resulted in diminished ratings of self-location and self-identification.
We conclude on the implications of our findings and discuss our results within
current theories of VR experiences, considering (in)congruent visual cues and
their impact on the perception of virtual others, self-representation, and
spatial presence.","['David Mal', 'Nina Döllinger', 'Erik Wolf', 'Stephan Wenninger', 'Mario Botsch', 'Carolin Wienrich', 'Marc Erich Latoschik']",2024-03-11T19:31:36Z,http://arxiv.org/abs/2403.07122v1,['cs.HC'],"avatars,virtual reality,virtual humans,realism,congruencies,virtual others,self-avatar,anthropomorphic cues,plausibility,co-presence"
"Forging the Industrial Metaverse -- Where Industry 5.0, Augmented and
  Mixed Reality, IIoT, Opportunistic Edge Computing and Digital Twins Meet","The Metaverse is a concept that proposes to immerse users into real-time
rendered 3D content virtual worlds delivered through Extended Reality (XR)
devices like Augmented and Mixed Reality (AR/MR) smart glasses and Virtual
Reality (VR) headsets. When the Metaverse concept is applied to industrial
environments, it is called Industrial Metaverse, a hybrid world where
industrial operators work by using some of the latest technologies. Currently,
such technologies are related to the ones fostered by Industry 4.0, which is
evolving towards Industry 5.0, a paradigm that enhances Industry 4.0 by
creating a sustainable and resilient world of industrial human-centric
applications. The Industrial Metaverse can benefit from Industry 5.0, since it
implies making use of dynamic and up-to-date content, as well as fast
human-to-machine interactions. To enable such enhancements, this article
proposes the concept of Meta-Operator: an Industry 5.0 worker that interacts
with Industrial Metaverse applications and with his/her surroundings through
advanced XR devices. This article provides a description of the technologies
that support Meta-Operators: the main components of the Industrial Metaverse,
the latest XR technologies and the use of Opportunistic Edge Computing
communications (to interact with surrounding IoT/IioT devices). Moreover, this
paper analyzes how to create the next generation of Industrial Metaverse
applications based on Industry 5.0, including the integration of AR/MR devices
with IoT/IIoT solutions, the development of advanced communications or the
creation of shared experiences. Finally, this article provides a list of
potential Industry 5.0 applications for the Industrial Metaverse and analyzes
the main challenges and research lines. Thus, this article provides useful
guidelines for the researchers that will create the next generation of
applications for the Industrial Metaverse.","['Tiago M. Fernández-Caramés', 'Paula Fraga-Lamas']",2024-03-17T19:14:28Z,http://arxiv.org/abs/2403.11312v1,"['cs.ET', 'cs.HC']","Industrial Metaverse,Industry 5.0,Augmented Reality,Mixed Reality,IIoT,Edge Computing,Digital Twins,XR technologies,IoT,Meta-Operator"
"Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect
  of Modality on Worker Reaction Times","Given the aging highway infrastructure requiring extensive rebuilding and
enhancements, and the consequent rise in the number of work zones, there is an
urgent need to develop advanced safety systems to protect workers. While
Augmented Reality (AR) holds significant potential for delivering warnings to
workers, its integration into roadway work zones remains relatively unexplored.
The primary objective of this study is to improve safety measures within
roadway work zones by conducting an extensive analysis of how different
combinations of multimodal AR warnings influence the reaction times of workers.
This paper addresses this gap through a series of experiments that aim to
replicate the distinctive conditions of roadway work zones, both in real-world
and virtual reality environments. Our approach comprises three key components:
an advanced AR system prototype, a VR simulation of AR functionality within the
work zone environment, and the Wizard of Oz technique to synchronize user
experiences across experiments. To assess reaction times, we leverage both the
simple reaction time (SRT) technique and an innovative vision-based metric that
utilizes real-time pose estimation. By conducting five experiments in
controlled outdoor work zones and indoor VR settings, our study provides
valuable information on how various multimodal AR warnings impact workers
reaction times. Furthermore, our findings reveal the disparities in reaction
times between VR simulations and real-world scenarios, thereby gauging VR's
capability to mirror the dynamics of roadway work zones. Furthermore, our
results substantiate the potential and reliability of vision-based reaction
time measurements. These insights resonate well with those derived using the
SRT technique, underscoring the viability of this approach for tangible
real-world uses.","['Sepehr Sabeti', 'Fatemeh Banani Ardecani', 'Omidreza Shoghli']",2024-03-22T18:52:10Z,http://arxiv.org/abs/2403.15571v2,"['cs.HC', 'cs.CV', 'eess.IV']","Augmented Reality,Warnings,Roadway Work Zones,Modality,Worker Reaction Times,Multimodal AR,VR Simulation,Wizard of Oz technique,Simple Reaction Time,Pose Estimation."
Tremor Reduction for Accessible Ray Based Interaction in VR Applications,"Comparative to conventional 2D interaction methods, virtual reality (VR)
demonstrates an opportunity for unique interface and interaction design
decisions. Currently, this poses a challenge when developing an accessible VR
experience as existing interaction techniques may not be usable by all users.
It was discovered that many traditional 2D interface interaction methods have
been directly converted to work in a VR space with little alteration to the
input mechanism, such as the use of a laser pointer designed to that of a
traditional cursor. It is recognized that distanceindependent millimetres can
support designers in developing interfaces that scale in virtual worlds.
Relevantly, Fitts law states that as distance increases, user movements are
increasingly slower and performed less accurately. In this paper we propose the
use of a low pass filter, to normalize user input noise, alleviating fine motor
requirements during ray-based interaction. A development study was conducted to
understand the feasibility of implementing such a filter and explore its
effects on end users experience. It demonstrates how an algorithm can provide
an opportunity for a more accurate and consequently less frustrating experience
by filtering and reducing involuntary hand tremors. Further discussion on
existing VR design philosophies is also conducted, analysing evidence that
supports multisensory feedback and psychological models. The completed study
can be downloaded from GitHub.","['Dr Corrie Green', 'Dr Yang Jiang', 'Dr John Isaacs', 'Dr Michael Heron']",2024-05-12T17:07:16Z,http://arxiv.org/abs/2405.07335v1,"['cs.HC', 'cs.CY']","Tremor Reduction,Accessible,Ray Based Interaction,VR Applications,Low Pass Filter,User Input Noise,Fine Motor,Hand Tremors,Multisensory Feedback,Psychological Models"
Carbon Connect: An Ecosystem for Sustainable Computing,"Computing is at a moment of profound opportunity. Emerging applications --
such as capable artificial intelligence, immersive virtual realities, and
pervasive sensor systems -- drive unprecedented demand for computer. Despite
recent advances toward net zero carbon emissions, the computing industry's
gross energy usage continues to rise at an alarming rate, outpacing the growth
of new energy installations and renewable energy deployments. A shift towards
sustainability is needed to spark a transformation in how computer systems are
manufactured, allocated, and consumed.
  Carbon Connect envisions coordinated research thrusts that produce design and
management strategies for sustainable, next-generation computer systems. These
strategies must flatten and then reverse growth trajectories for computing
power and carbon for society's most rapidly growing applications such as
artificial intelligence and virtual spaces. We will require accurate models for
carbon accounting in computing technology. For embodied carbon, we must
re-think conventional design strategies -- over-provisioned monolithic servers,
frequent hardware refresh cycles, custom silicon -- and adopt life-cycle design
strategies that more effectively reduce, reuse and recycle hardware at scale.
For operational carbon, we must not only embrace renewable energy but also
design systems to use that energy more efficiently. Finally, new hardware
design and management strategies must be cognizant of economic policy and
regulatory landscape, aligning private initiatives with societal goals. Many of
these broader goals will require computer scientists to develop deep, enduring
collaborations with researchers in economics, law, and industrial ecology to
spark change in broader practice.","['Benjamin C. Lee', 'David Brooks', 'Arthur van Benthem', 'Udit Gupta', 'Gage Hills', 'Vincent Liu', 'Benjamin Pierce', 'Christopher Stewart', 'Emma Strubell', 'Gu-Yeon Wei', 'Adam Wierman', 'Yuan Yao', 'Minlan Yu']",2024-05-22T17:33:51Z,http://arxiv.org/abs/2405.13858v1,"['cs.DC', 'cs.AR', 'cs.ET', 'cs.LG']","sustainable computing,carbon accounting,renewable energy,computer systems,artificial intelligence,virtual realities,embodied carbon,hardware design,renewable energy deployments"
"Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot
  Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language
  Models","In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation
for parts searching and localization for objects, which is a new paradigm to 3D
segmentation that transcends limitations for previous category-specific 3D
semantic segmentation, 3D instance segmentation, and open-vocabulary 3D
segmentation. We design a simple baseline method, Reasoning3D, with the
capability to understand and execute complex commands for (fine-grained)
segmenting specific parts for 3D meshes with contextual awareness and reasoned
answers for interactive segmentation. Specifically, Reasoning3D leverages an
off-the-shelf pre-trained 2D segmentation network, powered by Large Language
Models (LLMs), to interpret user input queries in a zero-shot manner. Previous
research have shown that extensive pre-training endows foundation models with
prior world knowledge, enabling them to comprehend complex commands, a
capability we can harness to ""segment anything"" in 3D with limited 3D datasets
(source efficient). Experimentation reveals that our approach is generalizable
and can effectively localize and highlight parts of 3D objects (in 3D mesh)
based on implicit textual queries, including these articulated 3d objects and
real-world scanned data. Our method can also generate natural language
explanations corresponding to these 3D models and the decomposition. Moreover,
our training-free approach allows rapid deployment and serves as a viable
universal baseline for future research of part-level 3d (semantic) object
understanding in various fields including robotics, object manipulation, part
assembly, autonomous driving applications, augment reality and virtual reality
(AR/VR), and medical applications. The code, the model weight, the deployment
guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/","['Tianrun Chen', 'Chunan Yu', 'Jing Li', 'Jianqi Zhang', 'Lanyun Zhu', 'Deyi Ji', 'Yong Zhang', 'Ying Zang', 'Zejian Li', 'Lingyun Sun']",2024-05-29T17:56:07Z,http://arxiv.org/abs/2405.19326v1,"['cs.CV', 'cs.GR', 'cs.HC']","Zero-Shot 3D Reasoning Segmentation,Vision-Language Models,Reasoning3D,Large Language Models (LLMs),3D semantic segmentation,3D instance segmentation,Fine-Grained Zero-Shot Reasoning,3D meshes,Textual queries"
SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval,"The retrieval of 3D objects has gained significant importance in recent years
due to its broad range of applications in computer vision, computer graphics,
virtual reality, and augmented reality. However, the retrieval of 3D objects
presents significant challenges due to the intricate nature of 3D models, which
can vary in shape, size, and texture, and have numerous polygons and vertices.
To this end, we introduce a novel SHREC challenge track that focuses on
retrieving relevant 3D animal models from a dataset using sketch queries and
expedites accessing 3D models through available sketches. Furthermore, a new
dataset named ANIMAR was constructed in this study, comprising a collection of
711 unique 3D animal models and 140 corresponding sketch queries. Our contest
requires participants to retrieve 3D models based on complex and detailed
sketches. We receive satisfactory results from eight teams and 204 runs.
Although further improvement is necessary, the proposed task has the potential
to incentivize additional research in the domain of 3D object retrieval,
potentially yielding benefits for a wide range of applications. We also provide
insights into potential areas of future research, such as improving techniques
for feature extraction and matching and creating more diverse datasets to
evaluate retrieval performance. https://aichallenge.hcmus.edu.vn/sketchanimar","['Trung-Nghia Le', 'Tam V. Nguyen', 'Minh-Quan Le', 'Trong-Thuan Nguyen', 'Viet-Tham Huynh', 'Trong-Le Do', 'Khanh-Duy Le', 'Mai-Khiem Tran', 'Nhat Hoang-Xuan', 'Thang-Long Nguyen-Ho', 'Vinh-Tiep Nguyen', 'Nhat-Quynh Le-Pham', 'Huu-Phuc Pham', 'Trong-Vu Hoang', 'Quang-Binh Nguyen', 'Trong-Hieu Nguyen-Mau', 'Tuan-Luc Huynh', 'Thanh-Danh Le', 'Ngoc-Linh Nguyen-Ha', 'Tuong-Vy Truong-Thuy', 'Truong Hoai Phong', 'Tuong-Nghiem Diep', 'Khanh-Duy Ho', 'Xuan-Hieu Nguyen', 'Thien-Phuc Tran', 'Tuan-Anh Yang', 'Kim-Phat Tran', 'Nhu-Vinh Hoang', 'Minh-Quang Nguyen', 'Hoai-Danh Vo', 'Minh-Hoa Doan', 'Hai-Dang Nguyen', 'Akihiro Sugimoto', 'Minh-Triet Tran']",2023-04-12T09:40:38Z,http://arxiv.org/abs/2304.05731v2,['cs.CV'],"3D objects,retrieval,computer vision,computer graphics,virtual reality,augmented reality,SHREC challenge track,dataset,sketch queries"
"Benchmarking the CoW with the TopCoW Challenge: Topology-Aware
  Anatomical Segmentation of the Circle of Willis for CTA and MRA","The Circle of Willis (CoW) is an important network of arteries connecting
major circulations of the brain. Its vascular architecture is believed to
affect the risk, severity, and clinical outcome of serious neuro-vascular
diseases. However, characterizing the highly variable CoW anatomy is still a
manual and time-consuming expert task. The CoW is usually imaged by two
angiographic imaging modalities, magnetic resonance angiography (MRA) and
computed tomography angiography (CTA), but there exist limited public datasets
with annotations on CoW anatomy, especially for CTA. Therefore we organized the
TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The
TopCoW dataset was the first public dataset with voxel-level annotations for
thirteen possible CoW vessel components, enabled by virtual-reality (VR)
technology. It was also the first large dataset with paired MRA and CTA from
the same patients. TopCoW challenge formalized the CoW characterization problem
as a multiclass anatomical segmentation task with an emphasis on topological
metrics. We invited submissions worldwide for the CoW segmentation task, which
attracted over 140 registered participants from four continents. The top
performing teams managed to segment many CoW components to Dice scores around
90%, but with lower scores for communicating arteries and rare variants. There
were also topological mistakes for predictions with high Dice scores.
Additional topological analysis revealed further areas for improvement in
detecting certain CoW components and matching CoW variant topology accurately.
TopCoW represented a first attempt at benchmarking the CoW anatomical
segmentation task for MRA and CTA, both morphologically and topologically.","['Kaiyuan Yang', 'Fabio Musio', 'Yihui Ma', 'Norman Juchler', 'Johannes C. Paetzold', 'Rami Al-Maskari', 'Luciano Höher', 'Hongwei Bran Li', 'Ibrahim Ethem Hamamci', 'Anjany Sekuboyina', 'Suprosanna Shit', 'Houjing Huang', 'Chinmay Prabhakar', 'Ezequiel de la Rosa', 'Diana Waldmannstetter', 'Florian Kofler', 'Fernando Navarro', 'Martin Menten', 'Ivan Ezhov', 'Daniel Rueckert', 'Iris Vos', 'Ynte Ruigrok', 'Birgitta Velthuis', 'Hugo Kuijf', 'Julien Hämmerli', 'Catherine Wurster', 'Philippe Bijlenga', 'Laura Westphal', 'Jeroen Bisschop', 'Elisa Colombo', 'Hakim Baazaoui', 'Andrew Makmur', 'James Hallinan', 'Bene Wiestler', 'Jan S. Kirschke', 'Roland Wiest', 'Emmanuel Montagnon', 'Laurent Letourneau-Guillon', 'Adrian Galdran', 'Francesco Galati', 'Daniele Falcetta', 'Maria A. Zuluaga', 'Chaolong Lin', 'Haoran Zhao', 'Zehan Zhang', 'Sinyoung Ra', 'Jongyun Hwang', 'Hyunjin Park', 'Junqiang Chen', 'Marek Wodzinski', 'Henning Müller', 'Pengcheng Shi', 'Wei Liu', 'Ting Ma', 'Cansu Yalçin', 'Rachika E. Hamadache', 'Joaquim Salvi', 'Xavier Llado', 'Uma Maria Lal-Trehan Estrada', 'Valeriia Abramova', 'Luca Giancardo', 'Arnau Oliver', 'Jialu Liu', 'Haibin Huang', 'Yue Cui', 'Zehang Lin', 'Yusheng Liu', 'Shunzhi Zhu', 'Tatsat R. Patel', 'Vincent M. Tutino', 'Maysam Orouskhani', 'Huayu Wang', 'Mahmud Mossa-Basha', 'Chengcheng Zhu', 'Maximilian R. Rokuss', 'Yannick Kirchhoff', 'Nico Disch', 'Julius Holzschuh', 'Fabian Isensee', 'Klaus Maier-Hein', 'Yuki Sato', 'Sven Hirsch', 'Susanne Wegener', 'Bjoern Menze']",2023-12-29T16:37:08Z,http://arxiv.org/abs/2312.17670v3,"['cs.CV', 'cs.LG', 'q-bio.QM', 'q-bio.TO']","Circle of Willis,CoW,anatomical segmentation,CTA,MRA,vascular architecture,annotations,TopCoW Challenge,multiclass segmentation,topological metrics"
