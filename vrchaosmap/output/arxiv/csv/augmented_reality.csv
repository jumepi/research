title,summary,authors,published,link,category
Inverse Augmented Reality: A Virtual Agent's Perspective,"We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']",2018-08-10T05:23:37Z,http://arxiv.org/abs/1808.03413v1,['cs.HC']
Mobile Augmented Reality Applications,"Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']",2011-06-28T06:08:38Z,http://arxiv.org/abs/1106.5571v1,"['cs.CV', 'H.5.1']"
Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.",['Nico Feld'],2021-01-07T14:43:51Z,http://arxiv.org/abs/2101.02565v1,['cs.HC']
"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']",2018-07-01T06:51:23Z,http://arxiv.org/abs/1807.00279v1,['cs.CY']
"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.","['Omid Ziaee', 'Mohsen Hamedi']",2021-12-03T20:46:50Z,http://arxiv.org/abs/2112.11190v1,['cs.CY']
Augmented Reality Implementation Methods in Mainstream Applications,"Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.","['David Prochazka', 'Tomas Koubek']",2011-06-28T05:57:37Z,http://arxiv.org/abs/1106.5569v1,"['cs.CV', 'H.5.1']"
"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']",2018-07-23T12:36:54Z,http://arxiv.org/abs/1807.10659v1,"['physics.ed-ph', 'cs.CY', 'cs.HC', 'K.3.1; H.5.1; I.3.7']"
"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']",2019-12-27T13:56:13Z,http://arxiv.org/abs/1912.12101v1,"['cs.CV', 'cs.LG', 'cs.RO']"
"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']",2021-04-17T15:47:48Z,http://arxiv.org/abs/2104.08579v2,['cs.HC']
A Survey of Augmented Reality Navigation,"Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.",['Gaurav Bhorkar'],2017-08-16T09:40:53Z,http://arxiv.org/abs/1708.05006v1,['cs.HC']
InAR:Inverse Augmented Reality,"Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.","['Hao Hu', 'Hainan Cui']",2015-08-11T14:17:28Z,http://arxiv.org/abs/1508.02606v1,['cs.CV']
Preprint ARPPS Augmented Reality Pipeline Prospect System,"This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']",2015-08-18T08:18:55Z,http://arxiv.org/abs/1508.04238v1,['cs.CV']
Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']",2018-06-25T08:01:45Z,http://arxiv.org/abs/1806.09316v1,['cs.CV']
Augmented Reality for Education: A Review,"Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.",['Carlo H. Godoy Jr'],2021-08-07T17:27:13Z,http://arxiv.org/abs/2109.02386v1,"['cs.HC', 'cs.CY', 'cs.GT', '91Axx', 'D.m; D.0']"
The Cloud Technologies and Augmented Reality: the Prospects of Use,"The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.","['Maiia V. Popel', 'Mariya P. Shyshkina']",2018-07-05T12:42:24Z,http://arxiv.org/abs/1807.01966v2,"['cs.CY', '68T99', 'K.3.1; I.2.6; K.2']"
"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']",2018-08-08T05:46:18Z,http://arxiv.org/abs/1808.06465v3,['cs.CY']
"Visions of augmented reality in popular culture: Power and (un)readable
  identities when the world becomes a screen","Augmented reality, where digital objects are overlaid and combined with the
ordinary visual surface, is a technology under rapid development, which has
long been a part of visions of the digital future. In this article, I examine
how gaze and power are coded into three pop-cultural visions of augmented
reality. By analyzing representations of augmented reality in science fiction
through the lens of feminist theory on performativity and intelligibility,
visibility and race, gendered gaze, and algorithmic normativity, this paper
provides a critical understanding of augmented reality as a visual technology,
and how it might change or reinforce possible norms and power relations. In
these futures where the screen no longer has any boundaries, both cooperative
and reluctant bodies are inscribed with gendered and racialized digital
markers. Reading visions of augmented reality through feminist theory, I argue
that augmented reality technologies enter into assemblages of people,
discourses, and technologies, where none of the actors necessarily has an
overview. In these assemblages, augmented reality takes on a performative and
norm-bearing role, by forming a grid of intelligibility that codifies
identities, structures hierarchical relationships, and scripts social
interactions.",['Marianne Gunderson'],2023-06-07T13:49:49Z,http://arxiv.org/abs/2306.04434v2,"['cs.CY', 'cs.SI', 'K.4.0']"
eCAR: edge-assisted Collaborative Augmented Reality Framework,"We propose a novel edge-assisted multi-user collaborative augmented reality
framework in a large indoor environment. In Collaborative Augmented Reality,
data communication that synchronizes virtual objects has large network traffic
and high network latency. Due to drift, CAR applications without continuous
data communication for coordinate system alignment have virtual object
inconsistency. In addition, synchronization messages for online virtual object
updates have high latency as the number of collaborative devices increases. To
solve this problem, we implement the CAR framework, called eCAR, which utilizes
edge computing to continuously match the device's coordinate system with less
network traffic. Furthermore, we extend the co-visibility graph of the edge
server to maintain virtual object spatial-temporal consistency in neighboring
devices by synchronizing a local graph. We evaluate the system quantitatively
and qualitatively in the public dataset and a physical indoor environment. eCAR
communicates data for coordinate system alignment between the edge server and
devices with less network traffic and latency. In addition, collaborative
augmented reality synchronization algorithms quickly and accurately host and
resolve virtual objects. The proposed system continuously aligns coordinate
systems to multiple devices in a large indoor environment and shares augmented
reality content. Through our system, users interact with virtual objects and
share augmented reality experiences with neighboring users.","['Jinwoo Jeon', 'Woontack Woo']",2024-05-11T02:07:33Z,http://arxiv.org/abs/2405.06872v1,"['cs.CV', 'cs.RO']"
Augmented Reality in Astrophysics,"Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.","['Frédéric P. A. Vogt', 'Luke J. Shingles']",2013-05-23T20:00:00Z,http://arxiv.org/abs/1305.5534v1,"['astro-ph.IM', 'cs.HC']"
"Personalization of learning using adaptive technologies and augmented
  reality","The research is aimed at developing the recommendations for educators on
using adaptive technologies and augmented reality in personalized learning
implementation. The latest educational technologies related to learning
personalization and the adaptation of its content to the individual needs of
students and group work are considered. The current state of research is
described, the trends of development are determined. Due to a detailed analysis
of scientific works, a retrospective of the development of adaptive and, in
particular, cloud-oriented systems is shown. The preconditions of their
appearance and development, the main scientific ideas that contributed to this
are analyzed. The analysis showed that the scientists point to four possible
types of semantic interaction of augmented reality and adaptive technologies.
The adaptive cloud-based educational systems design is considered as the
promising trend of research. It was determined that adaptability can be
manifested in one or a combination of several aspects: content, evaluation and
consistency. The cloud technology is taken as a platform for integrating
adaptive learning with augmented reality as the effective modern tools to
personalize learning. The prospects of the adaptive cloud-based systems design
in the context of teachers training are evaluated. The essence and place of
assistive technologies in adaptive learning systems design are defined. It is
shown that augmented reality can be successfully applied in inclusive
education. The ways of combining adaptive systems and augmented reality tools
to support the process of teachers training are considered. The recommendations
on the use of adaptive cloud-based systems in teacher education are given.","['Maiia Marienko', 'Yulia Nosenko', 'Mariya Shyshkina']",2020-11-08T21:34:05Z,http://arxiv.org/abs/2011.05802v1,"['cs.CY', 'physics.ed-ph']"
"Affordance Analysis of Virtual and Augmented Reality Mediated
  Communication","Virtual and augmented reality communication platforms are seen as promising
modalities for next-generation remote face-to-face interactions. Our study
attempts to explore non-verbal communication features in relation to their
conversation context for virtual and augmented reality mediated communication
settings. We perform a series of user experiments, triggering nine conversation
tasks in 4 settings, each containing corresponding non-verbal communication
features. Our results indicate that conversation types which involve less
emotional engagement are more likely to be acceptable in virtual reality and
augmented reality settings with low-fidelity avatar representation, compared to
scenarios that involve high emotional engagement or intellectually difficult
discussions. We further systematically analyze and rank the impact of
low-fidelity representation of micro-expressions, body scale, head pose, and
hand gesture in affecting the user experience in one-on-one conversations, and
validate that preserving micro-expression cues plays the most effective role in
improving bi-directional conversations in future virtual and augmented reality
settings.","['Mohammad Keshavarzi', 'Michael Wu', 'Michael N. Chin', 'Robert N. Chin', 'Allen Y. Yang']",2019-04-09T15:07:51Z,http://arxiv.org/abs/1904.04723v1,"['cs.HC', 'cs.MM']"
"Integrative Object and Pose to Task Detection for an
  Augmented-Reality-based Human Assistance System using Neural Networks","As a result of an increasingly automatized and digitized industry, processes
are becoming more complex. Augmented Reality has shown considerable potential
in assisting workers with complex tasks by enhancing user understanding and
experience with spatial information. However, the acceptance and integration of
AR into industrial processes is still limited due to the lack of established
methods and tedious integration efforts. Meanwhile, deep neural networks have
achieved remarkable results in computer vision tasks and bear great prospects
to enrich Augmented Reality applications . In this paper, we propose an
Augmented-Reality-based human assistance system to assist workers in complex
manual tasks where we incorporate deep neural networks for computer vision
tasks. More specifically, we combine Augmented Reality with object and action
detectors to make workflows more intuitive and flexible. To evaluate our system
in terms of user acceptance and efficiency, we conducted several user studies.
We found a significant reduction in time to task completion in untrained
workers and a decrease in error rate. Furthermore, we investigated the users
learning curve with our assistance system.","['Linh Kästner', 'Leon Eversberg', 'Marina Mursa', 'Jens Lambrecht']",2020-08-31T08:24:06Z,http://arxiv.org/abs/2008.13419v1,"['cs.HC', 'cs.CV']"
Tollan-Xicocotitlan: A reconstructed City by augmented reality,"This project presents the analysis, design, implementation and results of
Reconstruction Xicocotitlan Tollan-through augmented reality, which will
release information about the Toltec culture supplemented by presenting an
overview of the main premises of the Xicocotitlan Tollan city supported
dimensional models based on the augmented reality technique showing the user a
virtual representation of buildings in Tollan.","['Martha Rosa Cordero Lopez', 'Marco Antonio Dorantes Gonzalez']",2014-06-19T18:57:03Z,http://arxiv.org/abs/1406.5151v1,"['cs.CE', 'cs.CY']"
The History of Mobile Augmented Reality,"This document summarizes the major milestones in mobile Augmented Reality
between 1968 and 2014. Major parts of the list were compiled by the member of
the Christian Doppler Laboratory for Handheld Augmented Reality in 2010 (author
list in alphabetical order) for the ISMAR society. Later in 2013 it was
updated, and more recent work was added during preparation of this report.
Permission is granted to copy and modify.","['Clemens Arth', 'Raphael Grasset', 'Lukas Gruber', 'Tobias Langlotz', 'Alessandro Mulloni', 'Daniel Wagner']",2015-05-06T10:56:12Z,http://arxiv.org/abs/1505.01319v3,['cs.HC']
Towards an augmented reality fourth generation social networks,"A concept of fourth generation social network is described as one that, built
on the features of augmented reality (AR), is able to implement an enriched
layer of digital information that displays in People Augmented Reality (PAR)
devices data shared by users in social networks. This PAR layer is accessed by
the users in their devices through camera effects when targeting with a mobile
phone to a user holding a mobile device with AGPS and with a profile in social
media. The social network of fourth generation will be a combination between
Facebook and Pokemon Go.","['Andres Montero', 'Borja Belaza']",2017-05-13T08:21:32Z,http://arxiv.org/abs/1705.04798v1,['cs.SI']
"Development of Internet of Things, Augmented Reality and 5G technologies
  (review)","Just as the emergence of personal computers and smartphones has changed the
life of modern society, the Internet of Things, augmented reality and
ultra-fast and reliable telecommunications networks of the new generation, by
combining the physical objects of the real world with the ever-increasing
computing power and intelligence of cyberspace, will make the next big
revolution in all spheres of human activity. Keywords: Internet of Things, 5G,
augmented reality.","['A. S. Smirnov', 'A. V. Tumialis', 'K. S. Golokhvast']",2019-02-21T12:47:51Z,http://arxiv.org/abs/1902.08008v1,['cs.CY']
Revealing Aspects of Hawai'i Tourism Using Situated Augmented Reality,"In this position paper, we present a process artifact that aims to bring
awareness to historical context, contemporary issues, and identity harm
inflicted by tourism in Hawaii. First, we introduce the historical background
and how the work is informed by the positionality of the authors. We discuss
how related augmented reality work can inform strategy for building augmented
reality experiences that address cultural issues. Then, we present a mockup of
the artifact, aimed to bring awareness to 20th century colonialism, recent
Kanaka Maoli art exclusion, and cultural prostitution. We describe how we will
share the app at the workshop and list topics for discussion.","['Karen Abe', 'Jules Park', 'Samir Ghosh']",2024-04-24T03:01:34Z,http://arxiv.org/abs/2404.15610v1,['cs.HC']
Projection Mapping Technologies for AR,"This invited talk will present recent projection mapping technologies for
augmented reality. First, fundamental technologies are briefly explained, which
have been proposed to overcome the technical limitations of ordinary
projectors. Second, augmented reality (AR) applications using projection
mapping technologies are introduced.",['Daisuke Iwai'],2017-04-10T15:13:40Z,http://arxiv.org/abs/1704.02897v1,"['cs.GR', 'H.5.1']"
"Quality-Aware Real-Time Augmented Reality Visualization under Delay
  Constraints","Augmented reality (AR) is one of emerging applications in modern multimedia
systems research. Due to intensive time-consuming computations for AR
visualization in mobile devices, quality-aware real-time computing under delay
constraints is essentially required. Inspired by Lyapunov optimization
framework, this paper proposes a time-average quality maximization method for
the AR visualization under delay considerations.","['Rhoan Lee', 'Soohyun Park', 'Soyi Jung', 'Joongheon Kim']",2022-05-01T06:31:50Z,http://arxiv.org/abs/2205.00407v1,"['cs.DC', 'cs.GR']"
Preserving History through Augmented Reality,"Extended reality can weave together the fabric of the past, present, and
future. A two-day design hackathon was held to bring the community together
through a love for history and a common goal to use technology for good.
Through interviewing an influential community elder, Emile Pitre, and
referencing his book Revolution to Evolution, my team developed an augmented
reality artifact to tell his story and preserve on revolutionary's legacy that
impacted the University of Washington's history forever.",['Annie Yang'],2024-04-20T01:41:53Z,http://arxiv.org/abs/2404.13229v1,['cs.HC']
"Preprint Touch-less Interactive Augmented Reality Game on Vision Based
  Wearable Device","This is the preprint version of our paper on Personal and Ubiquitous
Computing. There is an increasing interest in creating pervasive games based on
emerging interaction technologies. In order to develop touch-less, interactive
and augmented reality games on vision-based wearable device, a touch-less
motion interaction technology is designed and evaluated in this work. Users
interact with the augmented reality games with dynamic hands/feet gestures in
front of the camera, which triggers the interaction event to interact with the
virtual object in the scene. Three primitive augmented reality games with
eleven dynamic gestures are developed based on the proposed touch-less
interaction technology as proof. At last, a comparing evaluation is proposed to
demonstrate the social acceptability and usability of the touch-less approach,
running on a hybrid wearable framework or with Google Glass, as well as
workload assessment, user's emotions and satisfaction.","['Zhihan Lv', 'Alaa Halawani', 'Shengzhong Feng', 'Shafiq ur Rehman', 'Haibo Li']",2015-04-23T22:55:08Z,http://arxiv.org/abs/1504.06359v5,"['cs.HC', 'H.5.1']"
"Interaktion mit 3D-Objekten in Augmented Reality Anwendungen auf mobilen
  Android Geräten","This bachelor's thesis describes the conception and implementation of an
augmented reality application for the Android platform. The intention is to
demonstrate some possibilities of interaction within an augmented reality
environment on mobile devices. For that purpose, a 3D-model is displayed on the
devices' touchscreen using marker-based tracking. This enables the user to
translate, rotate or scale the model as he wishes. He can additionally select
and highlight preassigned parts of the model to display specific information
for that element. To assist developers in modifying the application for
changing requirements without re-writing large portions of the source code, the
information for each part have been encapsulated into its own data type. After
an introduction to augmented reality, its underlying technology and the Android
platform, some possible usage scenarios and the resulting functionalities are
outlined. Finally, the design as well as the developed implementation are
described.",['Lennart Brüggemann'],2016-12-27T21:08:55Z,http://arxiv.org/abs/1701.01644v1,['cs.HC']
Enabling Self-aware Smart Buildings by Augmented Reality,"Conventional HVAC control systems are usually incognizant of the physical
structures and materials of buildings. These systems merely follow pre-set HVAC
control logic based on abstract building thermal response models, which are
rough approximations to true physical models, ignoring dynamic spatial
variations in built environments. To enable more accurate and responsive HVAC
control, this paper introduces the notion of ""self-aware"" smart buildings, such
that buildings are able to explicitly construct physical models of themselves
(e.g., incorporating building structures and materials, and thermal flow
dynamics). The question is how to enable self-aware buildings that
automatically acquire dynamic knowledge of themselves. This paper presents a
novel approach using ""augmented reality"". The extensive user-environment
interactions in augmented reality not only can provide intuitive user
interfaces for building systems, but also can capture the physical structures
and possibly materials of buildings accurately to enable real-time building
simulation and control. This paper presents a building system prototype
incorporating augmented reality, and discusses its applications.","['Muhammad Aftab', 'Sid Chi-Kin Chau', 'Majid Khonji']",2017-08-17T08:56:01Z,http://arxiv.org/abs/1708.05174v2,['cs.HC']
ARbis Pictus: A Study of Language Learning with Augmented Reality,"This paper describes ""ARbis Pictus"" --a novel system for immersive language
learning through dynamic labeling of real-world objects in augmented reality.
We describe a within-subjects lab-based study (N=52) that explores the effect
of our system on participants learning nouns in an unfamiliar foreign language,
compared to a traditional flashcard-based approach. Our results show that the
immersive experience of learning with virtual labels on real-world objects is
both more effective and more enjoyable for the majority of participants,
compared to flashcards. Specifically, when participants learned through
augmented reality, they scored significantly better by 7% (p=0.011) on
productive recall tests performed same-day, and significantly better by 21%
(p=0.001) on 4-day delayed productive recall post tests than when they learned
using the flashcard method. We believe this result is an indication of the
strong potential for language learning in augmented reality, particularly
because of the improvement shown in sustained recall compared to the
traditional approach.","['Adam Ibrahim', 'Brandon Huynh', 'Jonathan Downey', 'Tobias Höllerer', 'Dorothy Chun', ""John O'Donovan""]",2017-11-30T05:46:01Z,http://arxiv.org/abs/1711.11243v2,['cs.HC']
"PlutoAR: An Inexpensive, Interactive And Portable Augmented Reality
  Based Interpreter For K-10 Curriculum","The regular K-10 curriculums often do not get the necessary of affordable
technology involving interactive ways of teaching the prescribed curriculum
with effective analytical skill building. In this paper, we present ""PlutoAR"",
a paper-based augmented reality interpreter which is scalable, affordable,
portable and can be used as a platform for skill building for the kids. PlutoAR
manages to overcome the conventional albeit non-interactive ways of teaching by
incorporating augmented reality (AR) through an interactive toolkit to provide
students with the best of both worlds. Students cut out paper ""tiles"" and place
these tiles one by one on a larger paper surface called ""Launchpad"" and use the
PlutoAR mobile application which runs on any Android device with a camera and
uses augmented reality to output each step of the program like an interpreter.
PlutoAR has inbuilt AR experiences like stories, maze solving using conditional
loops, simple elementary mathematics and the intuition of gravity.","['Shourya Pratap Singh', 'Ankit Kumar Panda', 'Susobhit Panigrahi', 'Ajaya Kumar Dash', 'Debi Prosad Dogra']",2018-09-02T19:05:26Z,http://arxiv.org/abs/1809.00375v2,"['cs.HC', 'cs.CY', 'H.5.2']"
"Immercity: a curation content application in Virtual and Augmented
  reality","When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']",2018-10-24T06:23:46Z,http://arxiv.org/abs/1810.10206v1,"['cs.GR', 'cs.HC']"
"Real or Virtual? Using Brain Activity Patterns to differentiate Attended
  Targets during Augmented Reality Scenarios","Augmented Reality is the fusion of virtual components and our real
surroundings. The simultaneous visibility of generated and natural objects
often requires users to direct their selective attention to a specific target
that is either real or virtual. In this study, we investigated whether this
target is real or virtual by using machine learning techniques to classify
electroencephalographic (EEG) data collected in Augmented Reality scenarios. A
shallow convolutional neural net classified 3 second data windows from 20
participants in a person-dependent manner with an average accuracy above 70\%
if the testing data and training data came from different trials.
Person-independent classification was possible above chance level for 6 out of
20 participants. Thus, the reliability of such a Brain-Computer Interface is
high enough for it to be treated as a useful input mechanism for Augmented
Reality applications.","['Lisa-Marie Vortmann', 'Leonid Schwenke', 'Felix Putze']",2021-01-12T19:08:39Z,http://arxiv.org/abs/2101.05272v1,"['cs.HC', 'cs.LG']"
Visualizing Robot Intent for Object Handovers with Augmented Reality,"Humans are highly skilled in communicating their intent for when and where a
handover would occur. However, even the state-of-the-art robotic
implementations for handovers typically lack of such communication skills. This
study investigates visualization of the robot's internal state and intent for
Human-to-Robot Handovers using Augmented Reality. Specifically, we explore the
use of visualized 3D models of the object and the robotic gripper to
communicate the robot's estimation of where the object is and the pose in which
the robot intends to grasp the object. We tested this design via a user study
with 16 participants, in which each participant handed over a cube-shaped
object to the robot 12 times. Results show communicating robot intent via
augmented reality substantially improves the perceived experience of the users
for handovers. Results also indicate that the effectiveness of augmented
reality is even more pronounced for the perceived safety and fluency of the
interaction when the robot makes errors in localizing the object.","['Rhys Newbury', 'Akansel Cosgun', 'Tysha Crowley-Davis', 'Wesley P. Chan', 'Tom Drummond', 'Elizabeth Croft']",2021-03-06T07:57:01Z,http://arxiv.org/abs/2103.04055v3,['cs.RO']
"Developing an Augmented Reality-Based Game as a Supplementary tool for
  SHS- STEM Precalculus to Avoid Math Anxiety","Math Anxiety is experienced by students. This is caused mainly by poor
academic performance specifically in Calculus and Precalculus in Mathematics.
From 2014 to 2016, an average rating of 70.33 percent for secondary education
and 71.1 percent for elementary was generated during several exams, such as the
National Achievement Test. The result produced is said to be below the national
passing percentage, which shows that Filipino student performance in
mathematics is very poor indeed. Taking into consideration the proof of the
existence of these problems, it is necessary to try and test a need for a
technology which has never before been used as a remedy. Augmented reality is
deemed to be one of the advances in technology that is found to have a good
effect not only on the health of the student, but also on the learning of
Mathematics. Combining the two positive effects of Augmented Reality and the
very impressive outcome of a game-based learning technique in mathematics, it
is wise to say that an Augmented Reality Game-based learning method can be used
as a supplement in Precalculus and Calculus teaching.",['Carlo Hernandez Godoy Jr'],2021-09-20T07:37:46Z,http://arxiv.org/abs/2109.09336v1,"['math.HO', '19XX', 'K.3']"
"Eliciting Multimodal Gesture+Speech Interactions in a Multi-Object
  Augmented Reality Environment","As augmented reality technology and hardware become more mature and
affordable, researchers have been exploring more intuitive and discoverable
interaction techniques for immersive environments. In this paper, we
investigate multimodal interaction for 3D object manipulation in a multi-object
virtual environment. To identify the user-defined gestures, we conducted an
elicitation study involving 24 participants for 22 referents with an augmented
reality headset. It yielded 528 proposals and generated a winning gesture set
with 25 gestures after binning and ranking all gesture proposals. We found that
for the same task, the same gesture was preferred for both one and two object
manipulation, although both hands were used in the two object scenario. We
presented the gestures and speech results, and the differences compared to
similar studies in a single object virtual environment. The study also explored
the association between speech expressions and gesture stroke during object
manipulation, which could improve the recognizer efficiency in augmented
reality headsets.","['Xiaoyan Zhou', 'Adam S. Williams', 'Francisco R. Ortega']",2022-07-25T22:51:40Z,http://arxiv.org/abs/2207.12566v1,['cs.HC']
"ARTiST: Automated Text Simplification for Task Guidance in Augmented
  Reality","Text presented in augmented reality provides in-situ, real-time information
for users. However, this content can be challenging to apprehend quickly when
engaging in cognitively demanding AR tasks, especially when it is presented on
a head-mounted display. We propose ARTiST, an automatic text simplification
system that uses a few-shot prompt and GPT-3 models to specifically optimize
the text length and semantic content for augmented reality. Developed out of a
formative study that included seven users and three experts, our system
combines a customized error calibration model with a few-shot prompt to
integrate the syntactic, lexical, elaborative, and content simplification
techniques, and generate simplified AR text for head-worn displays. Results
from a 16-user empirical study showed that ARTiST lightens the cognitive load
and improves performance significantly over both unmodified text and text
modified via traditional methods. Our work constitutes a step towards
automating the optimization of batch text data for readability and performance
in augmented reality.","['Guande Wu', 'Jing Qian', 'Sonia Castelo', 'Shaoyu Chen', 'Joao Rulff', 'Claudio Silva']",2024-02-29T01:58:49Z,http://arxiv.org/abs/2402.18797v1,"['cs.HC', 'cs.CL', 'H.1.2; I.2.7']"
"A Review on Industrial Augmented Reality Systems for the Industry 4.0
  Shipyard","Shipbuilding companies are upgrading their inner workings in order to create
Shipyards 4.0, where the principles of Industry 4.0 are paving the way to
further digitalized and optimized processes in an integrated network. Among the
different Industry 4.0 technologies, this article focuses on Augmented Reality,
whose application in the industrial field has led to the concept of Industrial
Augmented Reality (IAR). This article first describes the basics of IAR and
then carries out a thorough analysis of the latest IAR systems for industrial
and shipbuilding applications. Then, in order to build a practical IAR system
for shipyard workers, the main hardware and software solutions are compared.
Finally, as a conclusion after reviewing all the aspects related to IAR for
shipbuilding, it is proposed an IAR system architecture that combines Cloudlets
and Fog Computing, which reduce latency response and accelerate rendering tasks
while offloading compute intensive tasks from the Cloud.","['Paula Fraga-Lamas', 'Tiago M Fernandez-Carames', 'Oscar Blanco-Novoa', 'Miguel Vilar-Montesinos']",2024-02-01T17:09:26Z,http://arxiv.org/abs/2405.00010v1,"['cs.DC', 'cs.HC']"
"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data","Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.","['Linh Kästner', 'Daniel Dimitrov', 'Jens Lambrecht']",2020-01-16T09:13:31Z,http://arxiv.org/abs/2001.05703v1,"['cs.CV', 'cs.LG', 'cs.RO']"
Taxonomy of Virtual and Augmented Reality Applications in Education,"This paper presents and analyses existing taxonomies of virtual and augmented
reality and demonstrates knowledge gaps and mixed terminology which may cause
confusion among educators, researchers, and developers. Several such occasions
of confusion are presented. A methodology is then presented to construct a
taxonomy of virtual reality and augmented reality applications based on a
combination of: a faceted analysis approach for the overall design of the
taxonomy; an existing taxonomy of educational objectives to derive the
educational purpose; an information systems analysis to establish important
facets of the taxonomy; and two systematic mapping studies to identify
categories within each facet. Based onUsing thisthe methodology a new taxonomy
is proposed and the implications of its facets (and their combinations of
facets)are demonstrated. The taxonomy focuses on technology used to provide the
virtual or augmented reality as well as the content presented to the user,
including the type of gamification and how it is operated. It also takes into
accountaccommodates a large number of devices and approaches developed
throughout the years and for multiple industries, and proposes and
developsprovides a way to categorize them in order to clarify communication
between researchers, developers and as well as educators. Use of the taxonomy
and implications of choices made during their development is then demonstrated
ion two case studies:, a virtual reality chemical plant for use in chemical
engineering education and an augmented reality dog for veterinary education.","['Jiri Motejlek', 'Esat Alpay']",2021-12-08T23:15:11Z,http://arxiv.org/abs/2112.04619v1,['cs.HC']
"Towards Safer Robot-Assisted Surgery: A Markerless Augmented Reality
  Framework","Robot-assisted surgery is rapidly developing in the medical field, and the
integration of augmented reality shows the potential of improving the surgeons'
operation performance by providing more visual information. In this paper, we
proposed a markerless augmented reality framework to enhance safety by avoiding
intra-operative bleeding which is a high risk caused by the collision between
the surgical instruments and the blood vessel. Advanced stereo reconstruction
and segmentation networks are compared to find out the best combination to
reconstruct the intra-operative blood vessel in the 3D space for the
registration of the pre-operative model, and the minimum distance detection
between the instruments and the blood vessel is implemented. A robot-assisted
lymphadenectomy is simulated on the da Vinci Research Kit in a dry lab, and ten
human subjects performed this operation to explore the usability of the
proposed framework. The result shows that the augmented reality framework can
help the users to avoid the dangerous collision between the instruments and the
blood vessel while not introducing an extra load. It provides a flexible
framework that integrates augmented reality into the medical robot platform to
enhance safety during the operation.","['Ziyang Chen', 'Laura Cruciani', 'Ke Fan', 'Matteo Fontana', 'Elena Lievore', 'Ottavio De Cobelli', 'Gennaro Musi', 'Giancarlo Ferrigno', 'Elena De Momi']",2023-09-14T13:13:46Z,http://arxiv.org/abs/2309.07693v1,['cs.RO']
"The Impact of Social Environment and Interaction Focus on User
  Experience and Social Acceptability of an Augmented Reality Game","One of the most promising technologies inside the Extended Reality (XR)
spectrum is Augmented Reality. This technology is already in people's pockets
regarding Mobile Augmented Reality with their smartphones. The scientific
community still needs answers about how humans could and should interact in
environments where perceived stimuli are different from fully physical or
digital circumstances. Moreover, it is still being determined if people accept
these new technologies in different social environments and interaction
settings or if some obstacles could exist. This paper explores the impact of
the Social Environment and the Focus of social interaction on users while
playing a location-based augmented reality game, measuring it with user
experience and social acceptance indicators. An empirical study in a
within-subject fashion was performed in different social environments and under
different settings of social interaction focus with N = 28 participants
compiling self-reported questionnaires after playing a Scavenger Hunt in
Augmented Reality. The measures from two different Social Environments (Crowded
vs. Uncrowded) resulted in statistically relevant mean differences with
indicators from the Social Acceptability dimension. Moreover, the analyses show
statistically relevant differences between the variances from different degrees
of Social Interaction Focus with Overall Social Presence, Perceived
Psychological Engagement, Perceived Attentional Engagement, and Perceived
Emotional Contagion. The results suggest that a location-based AR game played
in different social environments and settings can influence the user
experience's social dimension. Therefore, they should be carefully considered
while designing immersive technological experiences in public spaces involving
social interactions between players.","['Lorenzo Cocchia', 'Maurizio Vergari', 'Tanja Kojic', 'Francesco Vona', 'Sebastian Moller', 'Franca Garzotto', 'Jan-Niklas Voigt-Antons']",2024-04-25T10:04:36Z,http://arxiv.org/abs/2404.16479v1,['cs.HC']
Mobile Augmented Reality Applications to Discover New Environments,"Although man has become sedentary over time, his wish to travel the world
remains as strong as ever. The aim of this paper is to show how techniques
based on imagery and Augmented Reality (AR) can prove to be of great help when
discovering a new urban environment and observing the evolution of the natural
environment. The study's support is naturally the Smartphone which in just a
few years has become our most familiar device, which we take with us
practically everywhere we go in our daily lives.","['Nehla Ghouaiel', 'Jean-Marc Cieutat', 'Jean-Pierre Jessel']",2013-11-26T14:00:24Z,http://arxiv.org/abs/1311.6670v1,['cs.CY']
"Motion model transitions in GPS-IMU sensor fusion for user tracking in
  augmented reality","Finding the position of the user is an important processing step for
augmented reality (AR) applications. This paper investigates the use of
different motion models in order to choose the most suitable one, and
eventually reduce the Kalman filter errors in sensor fusion for such
applications where the accuracy of user tracking is crucial. A Deterministic
Finite Automaton (DFA) was employed using the innovation parameters of the
filter. Results show that the approach presented here reduces the filter error
compared to a static model and prevents filter divergence. The approach was
tested on a simple AR game in order to justify the accuracy and performance of
the algorithm.",['Erkan Bostanci'],2015-12-09T05:51:11Z,http://arxiv.org/abs/1512.02758v1,['cs.OH']
Augmented Reality Oculus Rift,"This paper covers the whole process of developing an Augmented Reality
Stereoscopig Render Engine for the Oculus Rift. To capture the real world in
form of a camera stream, two cameras with fish-eye lenses had to be installed
on the Oculus Rift DK1 hardware. The idea was inspired by Steptoe
\cite{steptoe2014presence}. After the introduction, a theoretical part covers
all the most neccessary elements to achieve an AR System for the Oculus Rift,
following the implementation part where the code from the AR Stereo Engine is
explained in more detail. A short conclusion section shows some results,
reflects some experiences and in the final chapter some future works will be
discussed. The project can be accessed via the git repository
https://github.com/MaXvanHeLL/ARift.git.","['Markus Höll', 'Nikolaus Heran', 'Vincent Lepetit']",2016-04-29T14:26:55Z,http://arxiv.org/abs/1604.08848v1,['cs.GR']
"Energy-Efficient Resource Allocation for Mobile Edge Computing-Based
  Augmented Reality Applications","Mobile edge computing is a provisioning solution to enable Augmented Reality
(AR) applications on mobile devices. AR mobile applications have inherent
collaborative properties in terms of data collection in the uplink, computing
at the edge, and data delivery in the downlink. In this letter, these features
are leveraged to propose a novel resource allocation approach over both
communication and computation resources. The approach, implemented via
Successive Convex Approximation (SCA), is seen to yield considerable gains in
mobile energy consumption as compared to conventional independent offloading
across users.","['Ali Al-Shuwaili', 'Osvaldo Simeone']",2016-11-28T17:09:11Z,http://arxiv.org/abs/1611.09243v2,['cs.NI']
City Planning with Augmented Reality,"We present an early study designed to analyze how city planning and the
health of senior citizens can benefit from the use of augmented reality (AR)
using Microsoft's HoloLens. We also explore whether AR and VR can be used to
help city planners receive real-time feedback from citizens, such as the
elderly, on virtual plans, allowing for informed decisions to be made before
any construction begins.","['Catherine Angelini', 'Adam S. Williams', 'Mathew Kress', 'Edgar Ramos Vieira', ""Newton D'Souza"", 'Naphtali D. Rishe', 'Joseph Medina', 'Francisco R. Ortega']",2020-01-18T02:22:45Z,http://arxiv.org/abs/2001.06578v1,['cs.HC']
Feasibility of Corneal Imaging for Handheld Augmented Reality,"Smartphones are a popular device class for mobile Augmented Reality but
suffer from a limited input space. Around-device interaction techniques aim at
extending this input space using various sensing modalities. In this paper we
present our work towards extending the input area of mobile devices using
front-facing device-centered cameras that capture reflections in the cornea. As
current generation mobile devices lack high resolution front-facing cameras, we
study the feasibility of around-device interaction using corneal reflective
imaging based on a high resolution camera. We present a workflow, a technical
prototype and a feasibility evaluation.","['Daniel Schneider', 'Jens Grubert']",2017-09-04T14:00:59Z,http://arxiv.org/abs/1709.00965v1,"['cs.HC', 'cs.CV']"
Feature Extraction in Augmented Reality,"Augmented Reality (AR) is used for various applications associated with the
real world. In this paper, first, describe characteristics and essential
services of AR. Brief history on Virtual Reality (VR) and AR is also mentioned
in the introductory section. Then, AR Technologies along with its workflow is
depicted, which includes the complete AR Process consisting of the stages of
Image Acquisition, Feature Extraction, Feature Matching, Geometric
Verification, and Associated Information Retrieval. Feature extraction is the
essence of AR hence its details are furnished in the paper.","['Jekishan K. Parmar', 'Ankit Desai']",2019-11-09T14:24:56Z,http://arxiv.org/abs/1911.09177v1,"['cs.GR', 'cs.CV', 'eess.IV']"
Efficient Pose Tracking from Natural Features in Standard Web Browsers,"Computer Vision-based natural feature tracking is at the core of modern
Augmented Reality applications. Still, Web-based Augmented Reality typically
relies on location-based sensing (using GPS and orientation sensors) or
marker-based approaches to solve the pose estimation problem.
  We present an implementation and evaluation of an efficient natural feature
tracking pipeline for standard Web browsers using HTML5 and WebAssembly. Our
system can track image targets at real-time frame rates tablet PCs (up to 60
Hz) and smartphones (up to 25 Hz).","['Fabian Göttl', 'Philipp Gagel', 'Jens Grubert']",2018-04-23T13:46:01Z,http://arxiv.org/abs/1804.08424v1,"['cs.CV', 'cs.MM', 'cs.NI']"
"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']",2019-03-07T04:29:50Z,http://arxiv.org/abs/1903.02723v1,['cs.HC']
An Augmented Reality Interaction Interface for Autonomous Drone,"Human drone interaction in autonomous navigation incorporates spatial
interaction tasks, including reconstructed 3D map from the drone and human
desired target position. Augmented Reality (AR) devices can be powerful
interactive tools for handling these spatial interactions. In this work, we
build an AR interface that displays the reconstructed 3D map from the drone on
physical surfaces in front of the operator. Spatial target positions can be
further set on the 3D map by intuitive head gaze and hand gesture. The AR
interface is deployed to interact with an autonomous drone to explore an
unknown environment. A user study is further conducted to evaluate the overall
interaction performance.","['Chuhao Liu', 'Shaojie Shen']",2020-08-05T17:02:22Z,http://arxiv.org/abs/2008.02234v1,"['cs.HC', 'cs.RO']"
"SENSAR: A Visual Tool for Intelligent Robots for Collaborative
  Human-Robot Interaction","Establishing common ground between an intelligent robot and a human requires
communication of the robot's intention, behavior, and knowledge to the human to
build trust and assure safety in a shared environment. This paper introduces
SENSAR (Seeing Everything iN Situ with Augmented Reality), an augmented reality
robotic system that enables robots to communicate their sensory and cognitive
data in context over the real-world with rendered graphics, allowing a user to
understand, correct, and validate the robot's perception of the world. Our
system aims to support human-robot interaction research by establishing common
ground where the perceptions of the human and the robot align.","['Andre Cleaver', 'Faizan Muhammad', 'Amel Hassan', 'Elaine Short', 'Jivko Sinapov']",2020-11-09T15:50:32Z,http://arxiv.org/abs/2011.04515v1,['cs.RO']
"Scene Text Detection for Augmented Reality -- Character Bigram Approach
  to reduce False Positive Rate","Natural scene text detection is an important aspect of scene understanding
and could be a useful tool in building engaging augmented reality applications.
In this work, we address the problem of false positives in text spotting. We
propose improving the performace of sliding window text spotters by looking for
character pairs (bigrams) rather than single characters. An efficient
convolutional neural network is designed and trained to detect bigrams. The
proposed detector reduces false positive rate by 28.16% on the ICDAR 2015
dataset. We demonstrate that detecting bigrams is a computationally inexpensive
way to improve sliding window text spotters.","['Sagar Gubbi', 'Bharadwaj Amrutur']",2020-12-26T08:56:10Z,http://arxiv.org/abs/2101.01054v1,"['cs.CV', 'cs.LG']"
"CaminAR: Supporting Walk-and-talk Experiences for Remote Dyads using
  Augmented Reality on Smart Glasses","In this paper, we propose CaminAR, an augmented reality (AR) system for
remote social walking among dyads. CaminAR enables two people who are
physically away from one another to synchronously see and hear each other while
going on a walk. The system shows a partner's avatar superimposed onto the
physical world using smart glasses while walking and talking. Using a
combination of visual and auditory augments, CaminAR simulates the experience
of co-located walking when dyads are apart.","['Victor Chu', 'Andrés Monroy-Hernández']",2022-07-28T06:51:20Z,http://arxiv.org/abs/2207.13904v1,"['cs.HC', 'cs.GR']"
Evaluating Situated Visualization in AR with Eye Tracking,"Augmented reality (AR) technology provides means for embedding visualization
in a real-world context. Such techniques allow situated analyses of live data
in their spatial domain. However, as existing techniques have to be adapted for
this context and new approaches will be developed, the evaluation thereof poses
new challenges for researchers. Apart from established performance measures,
eye tracking has proven to be a valuable means to assess visualizations
qualitatively and quantitatively. We discuss the challenges and opportunities
of eye tracking for the evaluation of situated visualizations. We envision that
an extension of gaze-based evaluation methodology into this field will provide
new insights on how people perceive and interact with visualizations in
augmented reality.","['Kuno Kurzhals', 'Michael Becher', 'Nelusa Pathmanathan', 'Guido Reina']",2022-09-05T09:10:04Z,http://arxiv.org/abs/2209.01846v1,"['cs.HC', 'cs.GR']"
Supporting Electronics Learning through Augmented Reality,"Understanding electronics is a critical area in the maker scene. Many of the
makers' projects require electronics knowledge to connect microcontrollers with
sensors and actuators. Yet, learning electronics is challenging, as internal
component processes remain invisible, and students often fear personal harm or
component damage. Augmented Reality (AR) applications are developed to support
electronics learning and visualize complex processes. This paper reflects on
related work around AR and electronics that characterize open research
challenges around the four characteristics functionality, fidelity, feedback
type, and interactivity.","['Thomas Kosch', 'Julian Rasch', 'Albrecht Schmidt', 'Sebastian Feger']",2022-10-25T07:53:50Z,http://arxiv.org/abs/2210.13820v1,"['cs.HC', 'H.5.1']"
Towards a QoE Model to Evaluate Holographic Augmented Reality Devices,"Augmented reality (AR) technology is developing fast and provides users with
new ways to interact with the real-world surrounding environment. Although the
performance of holographic AR multimedia devices can be measured with
traditional quality-of-service parameters, a quality-of-experience (QoE) model
can better evaluate the device from the perspective of users. As there are
currently no well-recognized models for measuring the QoE of a holographic AR
multimedia device, we present a QoE framework and model it with a fuzzy
inference system to quantitatively evaluate the device.","['Longyu Zhang', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",2022-12-25T15:15:29Z,http://arxiv.org/abs/2212.13842v1,['cs.MM']
"Digital Twin of a Network and Operating Environment Using Augmented
  Reality","We demonstrate the digital twin of a network, network elements, and operating
environment using machine learning. We achieve network card failure
localization and remote collaboration over 86 km of fiber using augmented
reality.","['Haoshuo Chen', 'Xiaonan Xu', 'Jesse E. Simsarian', 'Mijail Szczerban', 'Rob Harby', 'Roland Ryf', 'Mikael Mazur', 'Lauren Dallachiesa', 'Nicolas K. Fontaine', 'John Cloonan', 'Jim Sandoz', 'David T. Neilson']",2023-03-23T19:37:09Z,http://arxiv.org/abs/2303.15221v1,"['cs.NI', 'eess.SP']"
"Augmenting Learning with Augmented Reality: Exploring the Affordances of
  AR in Supporting Mastery of Complex Psychomotor Tasks","This research seeks to explore how Augmented Reality (AR) can support
learning psychomotor tasks that involve complex manipulation and reasoning
processes. The AR prototype was created using Unity and used on HoloLens 2
headsets. Here, we explore the potential of AR as a training or assistive tool
for spatial tasks and the need for intelligent mechanisms to enable adaptive
and personalized interactions between learners and AR. The paper discusses how
integrating AR with Artificial Intelligence (AI) can adaptably scaffold the
learning of complex tasks to accelerate the development of expertise in
psychomotor domains.","['Dong Woo Yoo', 'Sakib Reza', 'Nicholas Wilson', 'Kemi Jona', 'Mohsen Moghaddam']",2023-05-17T01:15:46Z,http://arxiv.org/abs/2305.09875v1,['cs.HC']
"Communicating Robot's Intentions while Assisting Users via Augmented
  Reality","This paper explores the challenges faced by assistive robots in effectively
cooperating with humans, requiring them to anticipate human behavior, predict
their actions' impact, and generate understandable robot actions. The study
focuses on a use-case involving a user with limited mobility needing assistance
with pouring a beverage, where tasks like unscrewing a cap or reaching for
objects demand coordinated support from the robot. Yet, anticipating the
robot's intentions can be challenging for the user, which can hinder effective
collaboration. To address this issue, we propose an innovative solution that
utilizes Augmented Reality (AR) to communicate the robot's intentions and
expected movements to the user, fostering a seamless and intuitive interaction.","['Chao Wang', 'Theodoros Stouraitis', 'Anna Belardinelli', 'Stephan Hasler', 'Michael Gienger']",2023-08-21T08:07:56Z,http://arxiv.org/abs/2308.10552v1,"['cs.RO', 'cs.HC']"
Augmenting Heritage: An Open-Source Multiplatform AR Application,"AI NeRF algorithms, capable of cloud processing, have significantly reduced
hardware requirements and processing efficiency in photogrammetry pipelines.
This accessibility has unlocked the potential for museums, charities, and
cultural heritage sites worldwide to leverage mobile devices for artifact
scanning and processing. However, the adoption of augmented reality platforms
often necessitates the installation of proprietary applications on users'
mobile devices, which adds complexity to development and limits global
availability. This paper presents a case study that demonstrates a
cost-effective pipeline for visualizing scanned museum artifacts using mobile
augmented reality, leveraging an open-source embedded solution on a website.",['Corrie Green'],2023-09-28T16:36:25Z,http://arxiv.org/abs/2310.13700v1,"['cs.HC', 'cs.MM']"
"OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using
  Semantic Understanding in Mixed Reality","One key challenge in augmented reality is the placement of virtual content in
natural locations. Existing automated techniques are only able to work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce a new
open-vocabulary method for object placement. Our eight-stage pipeline leverages
recent advances in segmentation models, vision-language models, and LLMs to
place any virtual object in any AR camera frame or scene. In a preliminary user
study, we show that our method performs at least as well as human experts 57%
of the time.","['Luke Yoffe', 'Aditya Sharma', 'Tobias Höllerer']",2023-12-20T07:34:20Z,http://arxiv.org/abs/2312.12815v1,"['cs.CV', 'cs.AI', 'cs.CL']"
"Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented
  Reality Teleoperation System","Most existing 6-DoF robot grasping solutions depend on strong supervision on
grasp pose to ensure satisfactory performance, which could be laborious and
impractical when the robot works in some restricted area. To this end, we
propose a self-supervised 6-DoF grasp pose detection framework via an Augmented
Reality (AR) teleoperation system that can efficiently learn human
demonstrations and provide 6-DoF grasp poses without grasp pose annotations.
Specifically, the system collects the human demonstration from the AR
environment and contrastively learns the grasping strategy from the
demonstration. For the real-world experiment, the proposed system leads to
satisfactory grasping abilities and learning to grasp unknown objects within
three demonstrations.","['Xiwen Dengxiong', 'Xueting Wang', 'Shi Bai', 'Yunbo Zhang']",2024-04-03T21:16:19Z,http://arxiv.org/abs/2404.03067v1,"['cs.RO', 'cs.CV']"
"With or Without Permission: Site-Specific Augmented Reality for Social
  Justice CHI 2024 Workshop Proceedings","This volume represents the proceedings of With or Without Permission:
Site-Specific Augmented Reality for Social Justice CHI 2024 workshop.","['Rafael M. L. Silva', 'Ana María Cárdenas Gasca', 'Joshua A. Fisher', 'Erica Principe Cruz', 'Cinthya Jauregui', 'Amy Lueck', 'Fannie Liu', 'Andrés Monroy-Hernández', 'Kai Lukoff']",2024-04-08T22:02:40Z,http://arxiv.org/abs/2404.05889v3,['cs.HC']
Augmented reality usage for prototyping speed up,"The first part of the article describes our approach for solution of this
problem by means of Augmented Reality. The merging of the real world model and
digital objects allows streamline the work with the model and speed up the
whole production phase significantly. The main advantage of augmented reality
is the possibility of direct manipulation with the scene using a portable
digital camera. Also adding digital objects into the scene could be done using
identification markers placed on the surface of the model. Therefore it is not
necessary to work with special input devices and lose the contact with the real
world model. Adjustments are done directly on the model. The key problem of
outlined solution is the ability of identification of an object within the
camera picture and its replacement with the digital object. The second part of
the article is focused especially on the identification of exact position and
orientation of the marker within the picture. The identification marker is
generalized into the triple of points which represents a general plane in
space. There is discussed the space identification of these points and the
description of representation of their position and orientation be means of
transformation matrix. This matrix is used for rendering of the graphical
objects (e. g. in OpenGL and Direct3D).","['Jiri Stastny', 'David Prochazka', 'Tomas Koubek', 'Jaromir Landa']",2011-03-10T16:00:52Z,http://arxiv.org/abs/1103.2063v1,"['cs.HC', 'cs.GR']"
Augmented Reality in ICT for Minimum Knowledge Loss,"Informatics world digitizes the human beings, with the contribution made by
all the industrial people. In the recent survey it is proved that people are
not accustomed or they are not able to access the electronic devices to its
extreme usage. Also people are more dependent to the technologies and their
day-to-day activities are ruled by the same. In this paper we discuss on one of
the advanced technology which will soon rule the world and make the people are
more creative and at the same time hassle-free. This concept is introduced as
6th sense technology by an IIT, Mumbai student who is presently Ph.D., scholar
in MIT, USA. Similar to this research there is one more research going on under
the title Augmented Reality. This research makes a new association with the
real world to digital world and allows us to share and manipulate the
information directly with our mental thoughts. A college which implements state
of the art technology for teaching and learning, Higher College of Technology,
Muscat, (HCT) tries to identify the opportunities and limitations of
implementing this augmented reality for teaching and learning. The research
team of HCT, here, tries to give two scenarios in which augmented reality can
fit in. Since this research is in the conceptual level we are trying to
illustrate the history of this technology and how it can be adopted in the
teaching environment","['RamKumar Lakshminarayanan', 'RD. Balaji', 'Binod kumar', 'Malathi Balaji']",2013-05-11T12:32:04Z,http://arxiv.org/abs/1305.2500v1,['cs.OH']
"Merging real and virtual worlds: An analysis of the state of the art and
  practical evaluation of Microsoft Hololens","Achieving a symbiotic blending between reality and virtuality is a dream that
has been lying in the minds of many people for a long time. Advances in various
domains constantly bring us closer to making that dream come true. Augmented
reality as well as virtual reality are in fact trending terms and are expected
to further progress in the years to come.
  This master's thesis aims to explore these areas and starts by defining
necessary terms such as augmented reality (AR) or virtual reality (VR). Usual
taxonomies to classify and compare the corresponding experiences are then
discussed.
  In order to enable those applications, many technical challenges need to be
tackled, such as accurate motion tracking with 6 degrees of freedom (positional
and rotational), that is necessary for compelling experiences and to prevent
user sickness. Additionally, augmented reality experiences typically rely on
image processing to position the superimposed content. To do so, ""paper""
markers or features extracted from the environment are often employed. Both
sets of techniques are explored and common solutions and algorithms are
presented.
  After investigating those technical aspects, I carry out an objective
comparison of the existing state-of-the-art and state-of-the-practice in those
domains, and I discuss present and potential applications in these areas. As a
practical validation, I present the results of an application that I have
developed using Microsoft HoloLens, one of the more advanced affordable
technologies for augmented reality that is available today. Based on the
experience and lessons learned during this development, I discuss the
limitations of current technologies and present some avenues of future
research.",['Adrien Coppens'],2017-06-25T13:10:39Z,http://arxiv.org/abs/1706.08096v1,"['cs.HC', 'cs.CV']"
"The Effects of Object Shape, Fidelity, Color, and Luminance on Depth
  Perception in Handheld Mobile Augmented Reality","Depth perception of objects can greatly affect a user's experience of an
augmented reality (AR) application. Many AR applications require depth matching
of real and virtual objects and have the possibility to be influenced by depth
cues. Color and luminance are depth cues that have been traditionally studied
in two-dimensional (2D) objects. However, there is little research
investigating how the properties of three-dimensional (3D) virtual objects
interact with color and luminance to affect depth perception, despite the
substantial use of 3D objects in visual applications. In this paper, we present
the results of a paired comparison experiment that investigates the effects of
object shape, fidelity, color, and luminance on depth perception of 3D objects
in handheld mobile AR. The results of our study indicate that bright colors are
perceived as nearer than dark colors for a high-fidelity, simple 3D object,
regardless of hue. Additionally, bright red is perceived as nearer than any
other color. These effects were not observed for a low-fidelity version of the
simple object or for a more-complex 3D object. High-fidelity objects had more
perceptual differences than low-fidelity objects, indicating that fidelity
interacts with color and luminance to affect depth perception. These findings
reveal how the properties of 3D models influence the effects of color and
luminance on depth perception in handheld mobile AR and can help developers
select colors for their applications.","['Tiffany D. Do', 'Joseph J. LaViola Jr.', 'Ryan P. McMahan']",2020-08-12T18:12:05Z,http://arxiv.org/abs/2008.05505v1,['cs.HC']
AR-based Modern Healthcare: A Review,"The recent advances of Augmented Reality (AR) in healthcare have shown that
technology is a significant part of the current healthcare system. In recent
days, augmented reality has proposed numerous smart applications in healthcare
domain including wearable access, telemedicine, remote surgery, diagnosis of
medical reports, emergency medicine, etc. The aim of the developed augmented
healthcare application is to improve patient care, increase efficiency, and
decrease costs. This article puts on an effort to review the advances in
AR-based healthcare technologies and goes to peek into the strategies that are
being taken to further this branch of technology. This article explores the
important services of augmented-based healthcare solutions and throws light on
recently invented ones as well as their respective platforms. It also addresses
concurrent concerns and their relevant future challenges. In addition, this
paper analyzes distinct AR security and privacy including security requirements
and attack terminologies. Furthermore, this paper proposes a security model to
minimize security risks. Augmented reality advantages in healthcare, especially
for operating surgery, emergency diagnosis, and medical training is being
demonstrated here thorough proper analysis. To say the least, the article
illustrates a complete overview of augmented reality technology in the modern
healthcare sector by demonstrating its impacts, advancements, current
vulnerabilities; future challenges, and concludes with recommendations to a new
direction for further research.","['Jinat Ara', 'Hanif Bhuiyan', 'Yeasin Arafat Bhuiyan', 'Salma Begum Bhyan', 'Muhammad Ismail Bhuiyan']",2021-01-16T03:53:01Z,http://arxiv.org/abs/2101.06364v1,"['cs.CY', 'cs.AI', 'A.1; H.4; I.2; J.3']"
"Measuring Presence in Augmented Reality Environments: Design and a First
  Test of a Questionnaire","Augmented Reality (AR) enriches a user's real environment by adding spatially
aligned virtual objects (3D models, 2D textures, textual annotations, etc) by
means of special display technologies. These are either worn on the body or
placed in the working environment. From a technical point of view, AR faces
three major challenges: (1) to generate a high quality rendering, (2) to
precisely register (in position and orientation) the virtual objects (VOs) with
the real environment, and (3) to do so in interactive real-time (Regenbrecht,
Wagner, and Baratoff, 2002). The goal is to create the impression that the VOs
are part of the real environment. Therefore, and similar to definitions of
virtual reality (Steuer, 1992), it makes sense to define AR from a
psychological point of view: Augmented Reality conveys the impression that VOs
are present in the real environment. In order to evaluate how well this goal is
reached, a psychological measurement of this type of presence is necessary. In
the following, we will describe technological features of AR systems that make
a special questionnaire version necessary, describe our approach to the
questionnaire development, and the data collection strategy. Finally we will
present first results of the application of the questionnaire in a recent study
with 385 participants.","['Holger Regenbrecht', 'Thomas Schubert']",2021-03-04T04:46:19Z,http://arxiv.org/abs/2103.02831v1,['cs.HC']
"A Novel Visualization System of Using Augmented Reality in Knee
  Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm","Background and aim: Image registration and alignment are the main limitations
of augmented reality-based knee replacement surgery. This research aims to
decrease the registration error, eliminate outcomes that are trapped in local
minima to improve the alignment problems, handle the occlusion, and maximize
the overlapping parts. Methodology: markerless image registration method was
used for Augmented reality-based knee replacement surgery to guide and
visualize the surgical operation. While weight least square algorithm was used
to enhance stereo camera-based tracking by filling border occlusion in right to
left direction and non-border occlusion from left to right direction. Results:
This study has improved video precision to 0.57 mm~0.61 mm alignment error.
Furthermore, with the use of bidirectional points, for example, forwards and
backwards directional cloud point, the iteration on image registration was
decreased. This has led to improve the processing time as well. The processing
time of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear
that this proposed system has focused on overcoming the misalignment difficulty
caused by movement of patient and enhancing the AR visualization during knee
replacement surgery. The proposed system was reliable and favorable which helps
in eliminating alignment error by ascertaining the optimal rigid transformation
between two cloud points and removing the outliers and non-Gaussian noise. The
proposed augmented reality system helps in accurate visualization and
navigation of anatomy of knee such as femur, tibia, cartilage, blood vessels,
etc.","['Nitish Maharjan', 'Abeer Alsadoon', 'P. W. C. Prasad', 'Salma Abdullah', 'Tarik A. Rashid']",2021-03-13T19:18:16Z,http://arxiv.org/abs/2104.05742v1,"['cs.CV', 'cs.AI', 'cs.HC']"
"Exploring Interactions with Printed Data Visualizations in Augmented
  Reality","This paper presents a design space of interaction techniques to engage with
visualizations that are printed on paper and augmented through Augmented
Reality. Paper sheets are widely used to deploy visualizations and provide a
rich set of tangible affordances for interactions, such as touch, folding,
tilting, or stacking. At the same time, augmented reality can dynamically
update visualization content to provide commands such as pan, zoom, filter, or
detail on demand. This paper is the first to provide a structured approach to
mapping possible actions with the paper to interaction commands. This design
space and the findings of a controlled user study have implications for future
designs of augmented reality systems involving paper sheets and visualizations.
Through workshops (N=20) and ideation, we identified 81 interactions that we
classify in three dimensions: 1) commands that can be supported by an
interaction, 2) the specific parameters provided by an (inter)action with
paper, and 3) the number of paper sheets involved in an interaction. We tested
user preference and viability of 11 of these interactions with a prototype
implementation in a controlled study (N=12, HoloLens 2) and found that most of
the interactions are intuitive and engaging to use. We summarized interactions
(e.g., tilt to pan) that have strong affordance to complement ""point"" for data
exploration, physical limitations and properties of paper as a medium, cases
requiring redundancy and shortcuts, and other implications for design.","['Wai Tong', 'Zhutian Chen', 'Meng Xia', 'Leo Yu-Ho Lo', 'Linping Yuan', 'Benjamin Bach', 'Huamin Qu']",2022-08-22T21:22:33Z,http://arxiv.org/abs/2208.10603v1,['cs.HC']
"Towards an Intelligent Assistive System Based on Augmented Reality and
  Serious Games","Age-related cognitive impairment is generally characterized by gradual memory
loss and decision-making difficulties. The aim of this study is to investigate
multi level support and suggest relevant helping means for the elderly with
mild cognitive impairment as well as their caregivers as the primary end-users.
This work reports preliminary results on an intelligent assistive system,
achieved through the integration of Internet of Things, augmented reality, and
adaptive fuzzy decision-making methods. The proposed system operates in
different modes, including automated and semi-automated modes. The former helps
the user complete their daily life activities by showing augmented reality
messages or making automatic changes; while the latter allows manual changes
after the real-time assessment of the user's cognitive state based on the
augmented reality serious game score. We have also evaluated the accuracy of
the serious game score with 37 elderly participants and compared it with users'
paper-based cognitive test results. We further noted that there is an
acceptable correlation between the paper-based test and users' serious game
scores. Moreover, we observed that the system response in the semi-automated
mode causes less data loss compared with the automated mode, as the number of
active devices decreases.","['Fatemeh Ghorbani', 'Mahsa Farshi Taghavi', 'Mehdi Delrobaei']",2023-01-06T11:10:04Z,http://arxiv.org/abs/2301.02461v1,"['cs.HC', 'cs.SY', 'eess.SY', 'I.3']"
"See or Hear? Exploring the Effect of Visual and Audio Hints and
  Gaze-assisted Task Feedback for Visual Search Tasks in Augmented Reality","Augmented reality (AR) is emerging in visual search tasks for increasingly
immersive interactions with virtual objects. We propose an AR approach
providing visual and audio hints along with gaze-assisted instant post-task
feedback for search tasks based on mobile head-mounted display (HMD). The
target case was a book-searching task, in which we aimed to explore the effect
of the hints together with the task feedback with two hypotheses. H1: Since
visual and audio hints can positively affect AR search tasks, the combination
outperforms the individuals. H2: The gaze-assisted instant post-task feedback
can positively affect AR search tasks. The proof-of-concept was demonstrated by
an AR app in HMD and a comprehensive user study (n=96) consisting of two
sub-studies, Study I (n=48) without task feedback and Study II (n=48) with task
feedback. Following quantitative and qualitative analysis, our results
partially verified H1 and completely verified H2, enabling us to conclude that
the synthesis of visual and audio hints conditionally improves the AR visual
search task efficiency when coupled with task feedback.","['Yuchong Zhang', 'Adam Nowak', 'Yueming Xuan', 'Andrzej Romanowski', 'Morten Fjeld']",2023-02-03T12:37:14Z,http://arxiv.org/abs/2302.01690v2,['cs.HC']
"GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted
  Assembly Guidance","Guidance for assemblable parts is a promising field for augmented reality.
Augmented reality assembly guidance requires 6D object poses of target objects
in real time. Especially in time-critical medical or industrial settings,
continuous and markerless tracking of individual parts is essential to
visualize instructions superimposed on or next to the target object parts. In
this regard, occlusions by the user's hand or other objects and the complexity
of different assembly states complicate robust and real-time markerless
multi-object tracking. To address this problem, we present Graph-based Object
Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The
real-time markerless multi-object tracking is initialized via 6D pose
estimation and updates the graph-based assembly poses. The tracking through
various assembly states is achieved by our novel multi-state assembly graph. We
update the multi-state assembly graph by utilizing the relative poses of the
individual assembly parts. Linking the individual objects in this graph enables
more robust object tracking during the assembly process. For evaluation, we
introduce a synthetic dataset of publicly available and 3D printable assembly
assets as a benchmark for future work. Quantitative experiments in synthetic
data and further qualitative study in real test data show that GBOT can
outperform existing work towards enabling context-aware augmented reality
assembly guidance. Dataset and code will be made publically available.","['Shiyu Li', 'Hannah Schieber', 'Niklas Corell', 'Bernhard Egger', 'Julian Kreimeier', 'Daniel Roth']",2024-02-12T14:38:46Z,http://arxiv.org/abs/2402.07677v1,['cs.CV']
"Immersed in Reality Secured by Design -- A Comprehensive Analysis of
  Security Measures in AR/VR Environments","Virtual reality and related technologies such as mixed and augmented reality
have received extensive coverage in both mainstream and fringe media outlets.
When the subject goes to a new AR headset, another AR device, or AR glasses,
the talk swiftly shifts to the technical and design details. Unfortunately, no
one seemed to care about security. Data theft and other forms of cyberattack
pose serious threats to virtual reality systems. Virtual reality goggles are
just specialist versions of computers or Internet of Things devices, whereas
virtual reality experiences are software packages. As a result, AR systems are
just as vulnerable as any other Internet of Things (IoT) device we use on a
daily basis, such as computers, tablets, and phones. Preventing and responding
to common cybersecurity threats and assaults is crucial. Cybercriminals can
exploit virtual reality headsets just like any other computer system. This
paper analysis the data breach induced by these assaults could result in a
variety of concerns, including but not limited to identity theft, the
unauthorized acquisition of personal information or network credentials, damage
to hardware and software, and so on. Augmented reality (AR) allows for
real-time monitoring and visualization of network activity, system logs, and
security alerts. This allows security professionals to immediately identify
threats, monitor suspicious activities, and fix any issues that develop. This
data can be displayed in an aesthetically pleasing and intuitively structured
format using augmented reality interfaces, enabling for faster analysis and
decision-making.","['Sameer Chauhan', 'Luv Sachdeva']",2024-01-30T21:24:52Z,http://arxiv.org/abs/2404.16839v1,"['cs.CR', 'cs.IT', 'math.IT']"
"Achieving Narrative Change Through AR: Displacing the Single Story to
  Create Spatial Justice","The ability of Augmented Reality to overcome the bias of single stories
through multidimensionality is explored in the artifacts of a youth gun
violence prevention project and its goal of narrative change.",['Janice Tisha Samuels'],2024-05-05T15:31:08Z,http://arxiv.org/abs/2405.02971v1,['cs.HC']
A Standalone Markerless 3D Tracker for Handheld Augmented Reality,"This paper presents an implementation of a markerless tracking technique
targeted to the Windows Mobile Pocket PC platform. The primary aim of this work
is to allow the development of standalone augmented reality applications for
handheld devices based on natural feature tracking. In order to achieve this
goal, a subset of two computer vision libraries was ported to the Pocket PC
platform. They were also adapted to use fixed point math, with the purpose of
improving the overall performance of the routines. The port of these libraries
opens up the possibility of having other computer vision tasks being executed
on mobile platforms. A model based tracking approach that relies on edge
information was adopted. Since it does not require a high processing power, it
is suitable for constrained devices such as handhelds. The OpenGL ES graphics
library was used to perform computer vision tasks, taking advantage of existing
graphics hardware acceleration. An augmented reality application was created
using the implemented technique and evaluations were done regarding tracking
performance and accuracy","['Joao Paulo Lima', 'Veronica Teichrieb', 'Judith Kelner']",2009-02-12T18:25:13Z,http://arxiv.org/abs/0902.2187v1,"['cs.CV', 'cs.GR', 'cs.MM']"
A Survey on Web-based AR Applications,"Due to the increase of interest in Augmented Reality (AR), the potential uses
of AR are increasing also. It can benefit the user in various fields such as
education, business, medicine, and other. Augmented Reality supports the real
environment with synthetic environment to give more details and meaning to the
objects in the real word. AR refers to a situation in which the goal is to
supplement a user's perception of the real-world through the addition of
virtual objects. This paper is an attempt to make a survey of web-based
Augmented Reality applications and make a comparison among them.","['Behrang Parhizkar', 'Ashraf Abbas M. Al-Modwahi', 'Arash Habibi Lashkari', 'Mohammad Mehdi Bartaripou', 'Hossein Reza Babae']",2011-11-13T07:13:23Z,http://arxiv.org/abs/1111.2993v1,['cs.MM']
Mobile augmented reality survey: a bottom-up approach,"Augmented Reality (AR) is becoming mobile. Mobile devices have many
constraints but also rich new features that traditional desktop computers do
not have. There are several survey papers on AR, but none is dedicated to
Mobile Augmented Reality (MAR). Our work serves the purpose of closing this
gap. The contents are organized with a bottom-up approach. We first present the
state-of-the-art in system components including hardware platforms, software
frameworks and display devices, follows with enabling technologies such as
tracking and data management. We then survey the latest technologies and
methods to improve run-time performance and energy efficiency for practical
implementation. On top of these, we further introduce the application fields
and several typical MAR applications. Finally we conclude the survey with
several challenge problems, which are under exploration and require great
research efforts in the future.","['Zhanpeng Huang', 'Pan Hui', 'Christoph Peylo', 'Dimitris Chatzopoulos']",2013-09-17T18:13:01Z,http://arxiv.org/abs/1309.4413v2,"['cs.GR', 'cs.HC', 'H.5.1']"
Low-cost Augmented Reality prototype for controlling network devices,"With the evolution of mobile devices, and smart-phones in particular, comes
the ability to create new experiences that enhance the way we see, interact,
and manipulate objects, within the world that surrounds us. It is now possible
to blend data from our senses and our devices in numerous ways that simply were
not possible before using Augmented Reality technology. In a near future, when
all of the office devices as well as your personal electronic gadgets are on a
common wireless network, operating them using a universal remote controller
would be possible. This paper presents an off-the-shelf, low-cost prototype
that leverages the Augmented Reality technology to deliver a novel and
interactive way of operating office network devices around using a mobile
device. We believe this type of system may provide benefits to controlling
multiple integrated devices and visualizing interconnectivity or utilizing
visual elements to pass information from one device to another, or may be
especially beneficial to control devices when interacting with them physically
may be difficult or pose danger or harm.","['Anh Nguyen', 'Amy Banic']",2014-06-12T04:46:32Z,http://arxiv.org/abs/1406.3117v1,"['cs.HC', 'cs.MM']"
Breaking the Barriers to True Augmented Reality,"In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained
considerable commercial traction, with Facebook acquiring Oculus VR for \$2
billion, Magic Leap attracting more than \$500 million of funding, and
Microsoft announcing their HoloLens head-worn computer. Where is humanity
headed: a brave new dystopia-or a paradise come true?
  In this article, we present discussions, which started at the symposium
""Making Augmented Reality Real"", held at Nara Institute of Science and
Technology in August 2014. Ten scientists were invited to this three-day event,
which started with a full day of public presentations and panel discussions
(video recordings are available at the event web page), followed by two days of
roundtable discussions addressing the future of AR and VR.","['Christian Sandor', 'Martin Fuchs', 'Alvaro Cassinelli', 'Hao Li', 'Richard Newcombe', 'Goshiro Yamamoto', 'Steven Feiner']",2015-12-17T05:57:06Z,http://arxiv.org/abs/1512.05471v1,['cs.HC']
Real-time Geometry-Aware Augmented Reality in Minimally Invasive Surgery,"The potential of Augmented Reality (AR) technology to assist minimally
invasive surgeries (MIS) lies in its computational performance and accuracy in
dealing with challenging MIS scenes. Even with the latest hardware and software
technologies, achieving both real-time and accurate augmented information
overlay in MIS is still a formidable task. In this paper, we present a novel
real-time AR framework for MIS that achieves interactive geometric aware
augmented reality in endoscopic surgery with stereo views. Our framework tracks
the movement of the endoscopic camera and simultaneously reconstructs a dense
geometric mesh of the MIS scene. The movement of the camera is predicted by
minimising the re-projection error to achieve a fast tracking performance,
while the 3D mesh is incrementally built by a dense zero mean normalised cross
correlation stereo matching method to improve the accuracy of the surface
reconstruction. Our proposed system does not require any prior template or
pre-operative scan and can infer the geometric information intra-operatively in
real-time. With the geometric information available, our proposed AR framework
is able to interactively add annotations, localisation of tumours and vessels,
and measurement labelling with greater precision and accuracy compared with the
state of the art approaches.","['Long Chen', 'Wen Tang', 'Nigel W. John']",2017-08-03T17:25:38Z,http://arxiv.org/abs/1708.01234v1,['cs.CV']
"Towards a Cloud-based Architecture for Visualization and Augmented
  Reality to Support Collaboration in Manufacturing Automation","In this report, we present our work in visualization and augmented reality
technologies supporting collaboration in manufacturing automation. Our approach
is based on (i) analysis based on spatial models of automation environments,
(ii) next-generation controllers based on single board computers, (iii) cloud-,
service- and web-based technologies and (iv) an emphasis on experimental
development using real automation equipment. The contribution of this paper is
the documentation of two new demonstrators, one for distributed viewing of 3D
scans of factory environments, and another for real time augmented reality
display of the status of a manufacturing plant, each based on technologies
under development in our lab and in particular applied to a mini-factory.","['Ian D. Peake', 'Jan Olaf Blech', 'Shyam Nath', 'Jacob Jacky Aharon', 'Argyll McGhie']",2017-11-16T09:19:32Z,http://arxiv.org/abs/1711.05997v1,['cs.SE']
"Robust Deep-Learning-Based Road-Prediction for Augmented Reality
  Navigation Systems","This paper proposes an approach that predicts the road course from camera
sensors leveraging deep learning techniques. Road pixels are identified by
training a multi-scale convolutional neural network on a large number of
full-scene-labeled night-time road images including adverse weather conditions.
A framework is presented that applies the proposed approach to longer distance
road course estimation, which is the basis for an augmented reality navigation
application. In this framework long range sensor data (radar) and data from a
map database are fused with short range sensor data (camera) to produce a
precise longitudinal and lateral localization and road course estimation. The
proposed approach reliably detects roads with and without lane markings and
thus increases the robustness and availability of road course estimations and
augmented reality navigation. Evaluations on an extensive set of high precision
ground truth data taken from a differential GPS and an inertial measurement
unit show that the proposed approach reaches state-of-the-art performance
without the limitation of requiring existing lane markings.","['Matthias Limmer', 'Julian Forster', 'Dennis Baudach', 'Florian Schüle', 'Roland Schweiger', 'Hendrik P. A. Lensch']",2016-05-31T09:00:33Z,http://arxiv.org/abs/1605.09533v1,"['cs.CV', 'cs.LG', 'cs.RO']"
A survey on haptic technologies for mobile augmented reality,"Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have
gained much research and industry attention these days. The mobile nature of
MAR applications limits users' interaction capabilities such as inputs, and
haptic feedbacks. This survey reviews current research issues in the area of
human computer interaction for MAR and haptic devices. The survey first
presents human sensing capabilities and their applicability in AR applications.
We classify haptic devices into two groups according to the triggered sense:
cutaneous/tactile: touch, active surfaces, and mid-air, kinesthetic:
manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR
applications, we mainly focus our study on wearable haptic devices for each
category and their AR possibilities. To conclude, we discuss the future paths
that haptic feedbacks should follow for MAR applications and their challenges.","['Carlos Bermejo', 'Pan Hui']",2017-09-03T11:12:15Z,http://arxiv.org/abs/1709.00698v3,['cs.HC']
"Analysis of the co-design activity: influence of a mixed artifact and
  contribution of the gestural function in a spatial augmented reality
  environment","Augmented reality provides new possibilities to propose environments where
the designers can take advantage of the physicality of the artifacts while
keeping the versatility of digital environments. Mixed objects can therefore
provide new media in the interactions between stakeholders. Besides, the
increasing interest in user participation in early design phases is limited by
the poor representations or the expensive mock ups to be provided in design
meetings. Therefore, understanding the role of these mixed artifacts by
analyzing and characterizing the interactions is crucial to the development of
both design methods and environments. By focusing on multimodal interactions,
we aim at providing new results in terms of the design process, in particular
by studying the contribution of the gesture in collaborative product
co-creativity sessions but also by understanding the role of these multiple
interactions in an augmented reality environment.","['Maud Poulin', 'Jean-François Boujut', 'Cédric Masclet']",2019-11-15T16:07:35Z,http://arxiv.org/abs/1911.07985v1,['cs.HC']
"Augmented Reality for Human-Swarm Interaction in a Swarm-Robotic
  Chemistry Simulation","We present a method to register individual members of a robotic swarm in an
augmented reality display while showing relevant information about swarm
dynamics to the user that would be otherwise hidden. Individual swarm members
and clusters of the same group are identified by their color, and by blinking
at a specific time interval that is distinct from the time interval at which
their neighbors blink. We show that this problem is an instance of the graph
coloring problem, which can be solved in a distributed manner in O(log(n))
time. We demonstrate our approach using a swarm chemistry simulation in which
robots simulate individual atoms that form molecules following the rules of
chemistry. Augmented reality is then used to display information about the
internal state of individual swarm members as well as their topological
relationship, corresponding to molecular bonds.","['Sumeet Batra', 'John Klingner', 'Nikolaus Correll']",2019-12-02T17:24:10Z,http://arxiv.org/abs/1912.00951v1,"['cs.RO', 'cs.CV']"
"DAARIA: Driver Assistance by Augmented Reality for Intelligent
  Automobile","Taking into account the drivers' state is a major challenge for designing new
advanced driver assistance systems. In this paper we present a driver
assistance system strongly coupled to the user. DAARIA 1 stands for Driver
Assistance by Augmented Reality for Intelligent Automobile. It is an augmented
reality interface powered by several sensors. The detection has two goals: one
is the position of obstacles and the quantification of the danger represented
by them. The other is the driver's behavior. A suitable visualization metaphor
allows the driver to perceive at any time the location of the relevant hazards
while keeping his eyes on the road. First results show that our method could be
applied to a vehicle but also to aerospace, fluvial or maritime navigation.","['Paul George', 'Indira Thouvenin', 'Vincent Fremont', 'Véronique Cherfaoui']",2012-09-27T07:15:22Z,http://arxiv.org/abs/1209.6140v1,"['cs.HC', 'cs.RO']"
"Using the Makerspace to Create Educational Open-source Software for
  Electrical Circuits: A Learning Experience","Virtual learning environments are a useful modality for engaging students in
the classroom by affording them a sense of presence and immersion. The
motivation of this project was to create an open-source augmented reality
electrical circuit application for use in lower division engineering courses to
teach students about electricity fundamentals. Softwares that are readily
available for use on virtual and augmented reality devices do not typically
apply to all disciplines and do not necessarily have a pedagogical or
accessibility focus. Considering this lack of appropriate educational
applications for the current virtual and augmented reality devices, a team of
interdisciplinary students was assembled to create such software. With
extensive usability studies, the application was designed for quick adoption
and improve accessibility by providing multimodal access such as voice
assistant, gray scaling for depth perception and daltonize the app. The
software is available as part of VITaL Laboratory, Sonoma State University.","['Dana Conard', 'Blake Vollmer', 'Corbin Shatto', 'Hannah Bowman', 'Sara Kassis']",2019-08-06T05:35:21Z,http://arxiv.org/abs/1908.01963v1,"['cs.HC', 'physics.ed-ph']"
3D landmark detection for augmented reality based otologic procedures,"Ear consists of the smallest bones in the human body and does not contain
significant amount of distinct landmark points that may be used to register a
preoperative CT-scan with the surgical video in an augmented reality framework.
Learning based algorithms may be used to help the surgeons to identify landmark
points. This paper presents a convolutional neural network approach to landmark
detection in preoperative ear CT images and then discusses an augmented reality
system that can be used to visualize the cochlear axis on an otologic surgical
video.","['Raabid Hussain', 'Alain Lalande', 'Kibrom Berihu Girum', 'Caroline Guigou', 'Alexis Bozorg Grayeli']",2019-09-04T09:31:22Z,http://arxiv.org/abs/1909.01647v1,"['eess.IV', 'cs.CV']"
"Towards Augmented Reality-driven Human-City Interaction: Current
  Research on Mobile Headsets and Future Challenges","Interaction design for Augmented Reality (AR) is gaining increasing attention
from both academia and industry. This survey discusses 260 articles (68.8% of
articles published between 2015 - 2019) to review the field of human
interaction in connected cities with emphasis on augmented reality-driven
interaction. We provide an overview of Human-City Interaction and related
technological approaches, followed by a review of the latest trends of
information visualization, constrained interfaces, and embodied interaction for
AR headsets. We highlight under-explored issues in interface design and input
techniques that warrant further research, and conjecture that AR with
complementary Conversational User Interfaces (CUIs) is a key enabler for
ubiquitous interaction with immersive systems in smart cities. Our work helps
researchers understand the current potential and future needs of AR in
Human-City Interaction.","['Lik Hang Lee', 'Tristan Braud', 'Simo Hosio', 'Pan Hui']",2020-07-17T19:47:02Z,http://arxiv.org/abs/2007.09207v2,"['cs.HC', '68-02', 'B.4; H.5']"
Tackling problems of marker-based augmented reality under water,"Underwater sites are a harsh environment for augmented reality applications.
Obstacles that must be battled include poor visibility conditions, difficult
navigation, and hard manipulation with devices under water. This chapter
focuses on the problem of localizing a device under water using markers. It
discusses various filters that enhance and improve images recorded under water,
and their impact on marker-based tracking. It presents various combinations of
10 image improving algorithms and 4 marker detecting algorithms, and tests
their performance in real situations. All solutions are designed to run
real-time on mobile devices to provide a solid basis for augmented reality.
Usability of this solution is evaluated on locations in Mediterranean Sea. It
is shown that image improving algorithms with carefully chosen parameters can
reduce the problems with visibility under water and improve the detection of
markers. The best results are obtained with marker detecting algorithms that
are specifically designed for underwater environments.","['Jan Čejka', 'Fotis Liarokapis']",2020-10-11T09:54:13Z,http://arxiv.org/abs/2010.11691v1,"['cs.CV', 'cs.GR']"
"Design and Test of an adaptive augmented reality interface to manage
  systems to assist critical missions","We present a user interface (UI) based on augmented reality (AR) with
head-mounted display (HMD) for improving situational awareness during critical
operation and improve human efficiency on operations. The UI displays
contextual information as well as accepts orders given from the headset to
control unmanned aerial vehicles (UAVs) for assisting the rescue team. We
established experiments where people had been put in a stressful situation and
are asked to resolve a complex mission using a headset and a computer.
Comparing both technologies, our results show that augmented reality has the
potential to be an important tool to help those involved in the emergency
situation.","['Dany Naser Addin', 'Benoit Ozell']",2021-03-25T22:28:37Z,http://arxiv.org/abs/2103.14160v1,"['cs.HC', '68U35 (Primary) 68M10 (Secondary)', 'J.2; I.3.4; F.2.2']"
"A multi-plane augmented reality head-up display system based on volume
  holographic optical elements with large area","The traditional head-up display (HUD) system has the disadvantages of a small
area and a single display plane, here we propose and design an augmented
reality (AR) HUD system with multi-plane, large area, high diffraction
efficiency and a single picture generation unit (PGU) based on holographic
optical elements (HOEs). Since volume HOEs have excellent angle selectivity and
wavelength selectivity, HOEs of different wavelengths can be designed to
display images in different planes. Experimental and simulated results verify
the feasibility of this method. Experimental results show that the diffraction
efficiencies of the red, green and blue HOEs are 75.2%, 73.1% and 67.5%. And
the size of HOEs is 20cm*15cm. Moreover, the three HOEs of red, green and blue
display images at different depths of 150cm, 500cm and 1000cm, respectively. In
addition, the field of view (FOV) and eye-box (EB) of the system are
12{\deg}*10{\deg} and 9.5cm*11.2cm. Furthermore, the light transmittance of the
system has reached 60%. It is believed that this technique can be applied to
the augmented reality navigation display of vehicles and aviation.","['Zhenlv Lv', 'Juan Liu', 'Liangfa Xu']",2021-04-12T12:24:25Z,http://arxiv.org/abs/2104.14315v1,"['cs.HC', 'physics.optics']"
Surgical navigation systems based on augmented reality technologies,"This study considers modern surgical navigation systems based on augmented
reality technologies. Augmented reality glasses are used to construct holograms
of the patient's organs from MRI and CT data, subsequently transmitted to the
glasses. This, in addition to seeing the actual patient, the surgeon gains
visualization inside the patient's body (bones, soft tissues, blood vessels,
etc.). The solutions developed at Peter the Great St. Petersburg Polytechnic
University allow reducing the invasiveness of the procedure and preserving
healthy tissues. This also improves the navigation process, making it easier to
estimate the location and size of the tumor to be removed. We describe the
application of developed systems to different types of surgical operations
(removal of a malignant brain tumor, removal of a cyst of the cervical spine).
We consider the specifics of novel navigation systems designed for anesthesia,
for endoscopic operations. Furthermore, we discuss the construction of novel
visualization systems for ultrasound machines. Our findings indicate that the
technologies proposed show potential for telemedicine.","['Vladimir Ivanov', 'Anton Krivtsov', 'Sergey Strelkov', 'Dmitry Gulyaev', 'Denis Godanyuk', 'Nikolay Kalakutsky', 'Artyom Pavlov', 'Marina Petropavloskaya', 'Alexander Smirnov', 'Andrew Yaremenko']",2021-05-13T11:09:12Z,http://arxiv.org/abs/2106.00727v1,['cs.HC']
"Simplifying Robot Programming using Augmented Reality and End-User
  Development","Robots are widespread across diverse application contexts. Teaching robots to
perform tasks, in their respective contexts, demands a high domain and
programming expertise. However, robot programming faces high entry barriers due
to the complexity of robot programming itself. Even for experts robot
programming is a cumbersome and error-prone task where faulty robot programs
can be created, causing damage when being executed on a real robot. To simplify
the process of robot programming, we combine Augmented Reality (AR) with
principles of end-user development. By combining them, the real environment is
extended with useful virtual artifacts that can enable experts as well as
non-professionals to perform complex robot programming tasks. Therefore, Simple
Programming Environment in Augmented Reality with Enhanced Debugging (SPEARED)
was developed as a prototype for an AR-assisted robot programming environment.
SPEARED makes use of AR to project a robot as well as a programming environment
onto the target working space. To evaluate our approach, expert interviews with
domain experts from the area of industrial automation, robotics, and AR were
performed. The experts agreed that SPEARED has the potential to enrich and ease
current robot programming processes.","['Enes Yigitbas', 'Ivan Jovanovikj', 'Gregor Engels']",2021-06-15T07:57:48Z,http://arxiv.org/abs/2106.07944v1,['cs.RO']
ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones,"Virtual try-on technology enables users to try various fashion items using
augmented reality and provides a convenient online shopping experience.
However, most previous works focus on the virtual try-on for clothes while
neglecting that for shoes, which is also a promising task. To this concern,
this work proposes a real-time augmented reality virtual shoe try-on system for
smartphones, namely ARShoe. Specifically, ARShoe adopts a novel multi-branch
network to realize pose estimation and segmentation simultaneously. A solution
to generate realistic 3D shoe model occlusion during the try-on process is
presented. To achieve a smooth and stable try-on effect, this work further
develop a novel stabilization method. Moreover, for training and evaluation, we
construct the very first large-scale foot benchmark with multiple virtual shoe
try-on task-related labels annotated. Exhaustive experiments on our newly
constructed benchmark demonstrate the satisfying performance of ARShoe.
Practical tests on common smartphones validate the real-time performance and
stabilization of the proposed approach.","['Shan An', 'Guangfu Che', 'Jinghao Guo', 'Haogang Zhu', 'Junjie Ye', 'Fangru Zhou', 'Zhaoqi Zhu', 'Dong Wei', 'Aishan Liu', 'Wei Zhang']",2021-08-24T03:54:45Z,http://arxiv.org/abs/2108.10515v1,['cs.CV']
"SceneAR: Scene-based Micro Narratives for Sharing and Remixing in
  Augmented Reality","Short-form digital storytelling has become a popular medium for millions of
people to express themselves. Traditionally, this medium uses primarily 2D
media such as text (e.g., memes), images (e.g., Instagram), gifs (e.g., Giphy),
and videos (e.g., TikTok, Snapchat). To expand the modalities from 2D to 3D
media, we present SceneAR, a smartphone application for creating sequential
scene-based micro narratives in augmented reality (AR). What sets SceneAR apart
from prior work is the ability to share the scene-based stories as AR content
-- no longer limited to sharing images or videos, these narratives can now be
experienced in people's own physical environments. Additionally, SceneAR
affords users the ability to remix AR, empowering them to build-upon others'
creations collectively. We asked 18 people to use SceneAR in a 3-day study.
Based on user interviews, analysis of screen recordings, and the stories they
created, we extracted three themes. From those themes and the study overall, we
derived six strategies for designers interested in supporting short-form AR
narratives.","['Mengyu Chen', 'Andrés Monroy-Hernández', 'Misha Sra']",2021-08-28T14:44:58Z,http://arxiv.org/abs/2108.12661v1,['cs.HC']
FaceEraser: Removing Facial Parts for Augmented Reality,"Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and
nose), and then impose visual elements onto the ``blank'' face for augmented
reality. Conventional object removal methods rely on image inpainting
techniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised
manner with randomly manipulated image pairs. Specifically, given a set of
natural images, randomly masked images are used as inputs and the raw images
are treated as ground truths. Whereas, this technique does not satisfy the
requirements of facial parts removal, as it is hard to obtain ``ground-truth''
images with real ``blank'' faces. To address this issue, we propose a novel
data generation technique to produce paired training data that well mimic the
``blank'' faces. In the mean time, we propose a novel network architecture for
improved inpainting quality for our task. Finally, we demonstrate various
face-oriented augmented reality applications on top of our facial parts removal
model. The source codes are released at
\href{https://github.com/duxingren14/FaceEraser}{duxingren14/FaceEraser} on
github for research purposes.","['Miao Hua', 'Lijie Liu', 'Ziyang Cheng', 'Qian He', 'Bingchuan Li', 'Zili Yi']",2021-09-22T14:30:12Z,http://arxiv.org/abs/2109.10760v2,['cs.CV']
Augmented reality navigation system for visual prosthesis,"The visual functions of visual prostheses such as field of view, resolution
and dynamic range, seriously restrict the person's ability to navigate in
unknown environments. Implanted patients still require constant assistance for
navigating from one location to another. Hence, there is a need for a system
that is able to assist them safely during their journey. In this work, we
propose an augmented reality navigation system for visual prosthesis that
incorporates a software of reactive navigation and path planning which guides
the subject through convenient, obstacle-free route. It consists on four steps:
locating the subject on a map, planning the subject trajectory, showing it to
the subject and re-planning without obstacles. We have also designed a
simulated prosthetic vision environment which allows us to systematically study
navigation performance. Twelve subjects participated in the experiment.
Subjects were guided by the augmented reality navigation system and their
instruction was to navigate through different environments until they reached
two goals, cross the door and find an object (bin), as fast and accurately as
possible. Results show how our augmented navigation system help navigation
performance by reducing the time and distance to reach the goals, even
significantly reducing the number of obstacles collisions, compared to other
baseline methods.","['Melani Sanchez-Garcia', 'Alejandro Perez-Yus', 'Ruben Martinez-Cantin', 'Jose J. Guerrero']",2021-09-30T09:41:40Z,http://arxiv.org/abs/2109.14957v1,['cs.CV']
An immersive Open Source environment using Godot,"We present a sample implementation of a Virtual and Augmented Reality
immersive environment based on Free and Libre Open Source Hardware and Software
and the HTC Vive system, used to enhance the immersive experience of the user
and to track her/his movements. The sense of immersion has increased and
stimulated using a footplate and a Tibetan bridge, connected to the virtual
world as Augmented Reality applications and implemented through an Arduino
board, thereby adopting a low cost, open source hardware and software approach.
The proposed architecture is relatively affordable from the cost point of view,
easy to implement, configure and adapt to different contexts. It can be of
great help for organizing laboratory classes for young students to afford the
implementation of virtual worlds and Augmented Reality applications.","['Francesca Santucci', 'Federico Frenguelli', 'Alessandro De Angelis', 'Ilaria Cuccaro', 'Damiano Perri', 'Marco Simonetti']",2021-11-03T01:57:54Z,http://arxiv.org/abs/2111.01974v1,['cs.GR']
"Implementing augmented reality technology to measure structural changes
  across time","In recent years, augmented reality (AR) technology has been increasingly
employed in structural health monitoring (SHM). In the case of conditions
following a seismic event, inspections are conducted to evaluate the
progression of the damage pattern quantitatively and efficiently respond if the
displacement pattern is determined to be unsafe. Additionally, quantification
of nearby structural changes over short-term and long-term periods can provide
building inspectors with information to improve safety. This paper proposes the
Time Machine Measure (TMM) application on an Augmented Reality (AR)
Head-Mounted-Device (HMD) platform. The main function of the TMM application is
to restore the saved meshes of a past environment and overlay them onto the
real environment so that inspectors can intuitively measure structural
deformation and other movement across time. The proposed TMM application was
verified by experiments meant to simulate a real-world inspection.","['Jiaqi Xu', 'Elijah Wyckoff', 'John-Wesley Hanson', 'Fernando Moreu', 'Derek Doyle']",2021-11-03T23:08:25Z,http://arxiv.org/abs/2111.02555v1,"['cs.HC', 'J.7; I.3.6; I.3.7; I.5.4']"
"Integrating Artificial Intelligence and Augmented Reality in Robotic
  Surgery: An Initial dVRK Study Using a Surgical Education Scenario","Robot-assisted surgery has become progressively more and more popular due to
its clinical advantages. In the meanwhile, the artificial intelligence and
augmented reality in robotic surgery are developing rapidly and receive lots of
attention. However, current methods have not discussed the coherent integration
of AI and AR in robotic surgery. In this paper, we develop a novel system by
seamlessly merging artificial intelligence module and augmented reality
visualization to automatically generate the surgical guidance for robotic
surgery education. Specifically, we first leverage reinforcement leaning to
learn from expert demonstration and then generate 3D guidance trajectory,
providing prior context information of the surgical procedure. Along with other
information such as text hint, the 3D trajectory is then overlaid in the stereo
view of dVRK, where the user can perceive the 3D guidance and learn the
procedure. The proposed system is evaluated through a preliminary experiment on
surgical education task peg-transfer, which proves its feasibility and
potential as the next generation of robot-assisted surgery education solution.","['Yonghao Long', 'Jianfeng Cao', 'Anton Deguet', 'Russell H. Taylor', 'Qi Dou']",2022-01-02T17:34:10Z,http://arxiv.org/abs/2201.00383v2,"['cs.RO', 'cs.AI']"
"Modern Augmented Reality: Applications, Trends, and Future Directions","Augmented reality (AR) is one of the relatively old, yet trending areas in
the intersection of computer vision and computer graphics with numerous
applications in several areas, from gaming and entertainment, to education and
healthcare. Although it has been around for nearly fifty years, it has seen a
lot of interest by the research community in the recent years, mainly because
of the huge success of deep learning models for various computer vision and AR
applications, which made creating new generations of AR technologies possible.
This work tries to provide an overview of modern augmented reality, from both
application-level and technical perspective. We first give an overview of main
AR applications, grouped into more than ten categories. We then give an
overview of around 100 recent promising machine learning based works developed
for AR systems, such as deep learning works for AR shopping (clothing, makeup),
AR based image filters (such as Snapchat's lenses), AR animations, and more. In
the end we discuss about some of the current challenges in AR domain, and the
future directions in this area.","['Shervin Minaee', 'Xiaodan Liang', 'Shuicheng Yan']",2022-02-18T22:12:37Z,http://arxiv.org/abs/2202.09450v2,['cs.CV']
"Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced
  Human-Robot Interaction and Robotic Interfaces","This paper contributes to a taxonomy of augmented reality and robotics based
on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have
emerged as a new way to enhance human-robot interaction (HRI) and robotic
interfaces (e.g., actuated and shape-changing interfaces). Recently, an
increasing number of studies in HCI, HRI, and robotics have demonstrated how AR
enables better interactions between people and robots. However, often research
remains focused on individual explorations and key design strategies, and
research questions are rarely analyzed systematically. In this paper, we
synthesize and categorize this research field in the following dimensions: 1)
approaches to augmenting reality; 2) characteristics of robots; 3) purposes and
benefits; 4) classification of presented information; 5) design components and
strategies for visual augmentation; 6) interaction techniques and modalities;
7) application domains; and 8) evaluation strategies. We formulate key
challenges and opportunities to guide and inform future research in AR and
robotics.","['Ryo Suzuki', 'Adnan Karim', 'Tian Xia', 'Hooman Hedayati', 'Nicolai Marquardt']",2022-03-07T10:22:59Z,http://arxiv.org/abs/2203.03254v1,"['cs.RO', 'cs.CV', 'cs.HC']"
"Development of Augmented Reality Application for Made-to-Order Furniture
  Industry in Pampanga, Philippines","The focus of the study was to develop a mobile application utilizing
marker-less augmented reality for specific made-to-order products to support
furniture and fixtures businesses. The study implemented mixed-methodology to
properly identify the various stakeholders' considerations in developing the
application. Interviews with key informants were conducted to ensure that the
features were appropriate for the intended user needs, and selected ISO
standards were used as evaluation criteria. The results indicate that the
mobile application with marker-less AR technology was found to be highly
acceptable by three evaluators (i.e., customers, owners, and IT experts). The
study also highlighted the use of AR-related technology in this case, where
marker-less has the potential to improve customer purchasing experience even
further. Future studies may include using newer technologies to further improve
the application. The study suggests that Augmented Reality technology could be
used to connect specific businesses directly to consumers regardless of setting
or context.","['Jaymark A. Yambao', 'John Paul P. Miranda', 'Earl Lawrence B. Pelayo']",2022-08-13T11:40:25Z,http://arxiv.org/abs/2208.06632v1,['cs.HC']
"Towards Situation Awareness and Attention Guidance in a Multiplayer
  Environment using Augmented Reality and Carcassonne","Augmented reality (AR) games are a rich environment for researching and
testing computational systems that provide subtle user guidance and training.
In particular computer systems that aim to augment a user's situation awareness
benefit from the range of sensors and computing power available in AR headsets.
In this work-in-progress paper, we present a new environment for research into
situation awareness and attention guidance (SAAG): an augmented reality version
of the board game Carcassonne. We also present our initial work in producing a
SAAG pipeline, including the creation of game state encodings, the development
and training of a gameplay AI, and the design of situation modelling and gaze
tracking systems.","['David Kadish', 'Arezoo Sarkheyli-Hägele', 'Jose Font', 'Diederick C. Niehorster', 'Thomas Pederson']",2022-08-18T23:45:13Z,http://arxiv.org/abs/2208.09094v1,"['cs.HC', 'cs.AI']"
"A systematic review of structural equation modeling in augmented reality
  applications","The purpose of this study is to present a comprehensive review of the use of
structural equation modeling (SEM) in augmented reality (AR) studies in the
context of the COVID-19 pandemic. IEEE Xplore Scopus, Wiley Online Library,
Emerald Insight, and ScienceDirect are the main five data sources for data
collection from Jan 2020 to May 2021. The results showed that a variety of
external factors were used to construct the SEM models rather than using the
parsimonious ones. The reports showed a fair balance between the direct and
indirect methods to contact participants. Despite the COVID-19 pandemic, few
publications addressed the issue of data collection and evaluation methods,
whereas video demonstrations of the augmented reality (AR) apps were utilized","['Vinh The Nguyen', 'Chuyen Thi Hong Nguyen']",2023-01-25T03:21:15Z,http://arxiv.org/abs/2301.11811v1,['cs.HC']
"Has the Virtualization of the Face Changed Facial Perception? A Study of
  the Impact of Photo Editing and Augmented Reality on Facial Perception","Augmented reality and other photo editing filters are popular methods used to
modify faces online. Considering the important role of facial perception in
communication, how do we perceive this increasing number of modified faces? In
this paper we present the results of six surveys that measure familiarity with
different styles of facial filters, perceived strangeness of faces edited with
different filters, and ability to discern whether images are filtered. Our
results demonstrate that faces modified with more traditional face filters are
perceived similarly to unmodified faces, and faces filtered with augmented
reality filters are perceived differently from unmodified faces. We discuss
possible explanations for these results, including a societal adjustment to
traditional photo editing techniques or the inherent differences in the
different types of filters. We conclude with a discussion of how to build
online spaces more responsibly based on our results.","['Louisa Conwill', 'Sam English Anthony', 'Walter J. Scheirer']",2023-03-01T16:09:11Z,http://arxiv.org/abs/2303.00612v3,"['cs.HC', 'cs.CV']"
BrickPal: Augmented Reality-based Assembly Instructions for Brick Models,"The assembly instruction is a mandatory component of Lego-like brick sets.The
conventional production of assembly instructions requires a considerable amount
of manual fine-tuning, which is intractable for casual users and customized
brick sets.Moreover, the traditional paper-based instructions lack
expressiveness and interactivity.To tackle the two problems above, we present
BrickPal, an augmented reality-based system, which visualizes assembly
instructions in an augmented reality head-mounted display. It utilizes Natural
Language Processing (NLP) techniques to generate plausible assembly sequences,
and provide real-time guidance in the AR headset.Our user study demonstrates
BrickPal's effectiveness at assisting users in brick assembly compared to
traditional assembly methods. Additionally, the NLP algorithm-generated
assembly sequences achieve the same usability with manually adapted sequences.","['Yao Shi', 'Xiaofeng Zhang', 'Ran zhang', 'Zhou Yang', 'Xiao Tang', 'Hongni Ye', 'Yi Wu']",2023-07-06T17:42:56Z,http://arxiv.org/abs/2307.03162v1,"['cs.HC', 'cs.AI']"
AR Visualization System for Ship Detection and Recognition Based on AI,"Augmented reality technology has been widely used in industrial design
interaction, exhibition guide, information retrieval and other fields. The
combination of artificial intelligence and augmented reality technology has
also become a future development trend. This project is an AR visualization
system for ship detection and recognition based on AI, which mainly includes
three parts: artificial intelligence module, Unity development module and
Hololens2AR module. This project is based on R3Det algorithm to complete the
detection and recognition of ships in remote sensing images. The recognition
rate of model detection trained on RTX 2080Ti can reach 96%. Then, the 3D model
of the ship is obtained by ship categories and information and generated in the
virtual scene. At the same time, voice module and UI interaction module are
added. Finally, we completed the deployment of the project on Hololens2 through
MRTK. The system realizes the fusion of computer vision and augmented reality
technology, which maps the results of object detection to the AR field, and
makes a brave step toward the future technological trend and intelligent
application.","['Ziqi Ye', 'Limin Huang', 'Yongji Wu', 'Min Hu']",2023-11-21T08:42:44Z,http://arxiv.org/abs/2311.12430v1,"['cs.CV', '68T07', 'I.2; I.4']"
"Augmented Reality Technology in Teaching about Physics: A systematic
  review of opportunities and challenges","The use of augmented reality (AR) allows for the integration of digital
information onto our perception of the physical world. In this article, we
present a comprehensive review of previously published literature on the
implementation of augmented reality in physics education, at the school and the
university level. Our review includes an analysis of 96 papers from the Scopus
and Eric databases, all of which were published between January 1st, 2012 and
January 1st, 2023. We evaluated how AR has been used for facilitating learning
about physics. Potential AR-based learning activities for different physics
topics have been summarized and opportunities, as well as challenges associated
with AR-based learning of physics have been reported. It has been shown that AR
technologies may facilitate physics learning by: providing complementary
visualizations, optimizing cognitive load, allowing for haptic learning,
reducing task completion time and promoting collaborative inquiry. The
potential disadvantages of using AR in physics teaching are mainly related to
the shortcomings of software and hardware technologies (e.g., camera freeze,
visualization delay) and extraneous cognitive load (e.g., paying more attention
to secondary details than to constructing target knowledge).","['A. Vidak', 'I. Movre Šapić', 'V. Mešić', 'V. Gomzi']",2023-11-30T09:35:36Z,http://arxiv.org/abs/2311.18392v1,['physics.ed-ph']
Augmenting reality to diminish distractions for cognitive enhancement,"Smartphones are integral to modern life, yet research highlights the
cognitive drawbacks associated even with their mere presence. Physically
removing them from sight is a solution, but it is sometimes impractical and may
increase anxiety due to fear of missing out. In response, we introduce a simple
but effective use of augmented reality (AR) head-mounted displays, focusing not
on augmenting reality with virtual objects, but on diminishing reality by
selectively removing or occluding distracting objects, from the user's field of
view. We compared cognitive task performance across four conditions: the
smartphone being physically nearby, physically remote, visually removed and
visually occluded via AR. Our findings reveal that using AR to visually cancel
out smartphones significantly mitigates cognitive distractions caused by their
presence. Specifically, the AR interventions had effects similar to physically
removing the phone. These results suggest potential for novel AR applications
designed to diminish reality, thereby enhancing cognitive performance.","['JangHyeon Lee', 'Lawrence H. Kim']",2024-03-06T17:35:48Z,http://arxiv.org/abs/2403.03875v1,['cs.HC']
"Impact of spatial auditory navigation on user experience during
  augmented outdoor navigation tasks","The auditory sense of humans is important when it comes to navigation. The
importance is especially high in cases when an object of interest is visually
partly or fully covered. Interactions with users of technology are mainly
focused on the visual domain of navigation tasks. This paper presents the
results of a literature review and user study exploring the impact of spatial
auditory navigation on user experience during an augmented outdoor navigation
task. For the user test, participants used an augmented reality app guiding
them to different locations with different digital augmentation. We conclude
that the utilization of the auditory sense is yet still underrepresented in
augmented reality applications. In the future, more usage scenarios for
audio-augmented reality such as navigation will enhance user experience and
interaction quality.","['Jan-Niklas Voigt-Antons', 'Zhirou Sun', 'Maurizio Vergari', 'Navid Ashrafi', 'Francesco Vona', 'Tanja Kojic']",2024-04-25T09:57:07Z,http://arxiv.org/abs/2404.16473v1,['cs.HC']
"Deep Learning-based Point Cloud Registration for Augmented
  Reality-guided Surgery","Point cloud registration aligns 3D point clouds using spatial
transformations. It is an important task in computer vision, with applications
in areas such as augmented reality (AR) and medical imaging. This work explores
the intersection of two research trends: the integration of AR into
image-guided surgery and the use of deep learning for point cloud registration.
The main objective is to evaluate the feasibility of applying deep
learning-based point cloud registration methods for image-to-patient
registration in augmented reality-guided surgery. We created a dataset of point
clouds from medical imaging and corresponding point clouds captured with a
popular AR device, the HoloLens 2. We evaluate three well-established deep
learning models in registering these data pairs. While we find that some deep
learning methods show promise, we show that a conventional registration
pipeline still outperforms them on our challenging dataset.","['Maximilian Weber', 'Daniel Wild', 'Jens Kleesiek', 'Jan Egger', 'Christina Gsaxner']",2024-05-06T09:41:31Z,http://arxiv.org/abs/2405.03314v1,"['cs.CV', 'cs.LG']"
"Towards Predictions of the Image Quality of Experience for Augmented
  Reality Scenarios","Augmented Reality (AR) devices are commonly head-worn to overlay
context-dependent information into the field of view of the device operators.
One particular scenario is the overlay of still images, either in a traditional
fashion, or as spherical, i.e., immersive, content. For both media types, we
evaluate the interplay of user ratings as Quality of Experience (QoE) with (i)
the non-referential BRISQUE objective image quality metric and (ii) human
subject dry electrode EEG signals gathered with a commercial device.
Additionally, we employ basic machine learning approaches to assess the
possibility of QoE predictions based on rudimentary subject data. Corroborating
prior research for the overall scenario, we find strong correlations for both
approaches with user ratings as Mean Opinion Scores, which we consider as QoE
metric. In prediction scenarios based on data subsets, we find good performance
for the objective metric as well as the EEG-based approach. While the objective
metric can yield high QoE prediction accuracies overall, it is limited i its
application for individual subjects. The subject-based EEG approach, on the
other hand, enables good predictability of the QoE for both media types, but
with better performance for regular content. Our results can be employed in
practical scenarios by content and network service providers to optimize the
user experience in augmented reality scenarios.","['Brian Bauman', 'Patrick Seeling']",2017-05-02T18:17:50Z,http://arxiv.org/abs/1705.01123v1,['cs.MM']
"An embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.","['Manish Bhattarai', 'Aura Rose Jensen-Curtis', 'Manel MartíNez-Ramón']",2020-09-22T16:55:44Z,http://arxiv.org/abs/2009.10679v1,"['cs.CV', 'cs.AI']"
"Augmented Reality Applied to LEGO Construction: AR-based Building
  Instructions with High Accuracy & Precision and Realistic Object-Hand
  Occlusions","BRICKxAR is a novel Augmented Reality (AR) instruction method for
construction toys such as LEGO. With BRICKxAR, physical LEGO construction is
guided by virtual bricks. Compared with the state-of-the-art, accuracy of the
virtual - physical model alignment is significantly improved through a new
design of marker-based registration, which can achieve an average error less
than 1mm throughout the model. Realistic object occlusion is accomplished to
reveal the true spatial relationship between physical and virtual bricks. LEGO
players' hand detection and occlusion are realized to visualize the correct
spatial relationship between real hands and virtual bricks, and allow virtual
bricks to be ""grasped"" by real hands. The integration of these features makes
AR instructions possible for small-parts assembly, validated through a working
AR prototype for constructing LEGO Arc de Triomphe, quantitative measures of
the accuracies of registration and occlusions, and heuristic evaluation of AR
instruction features.",['Wei Yan'],2019-07-29T17:44:14Z,http://arxiv.org/abs/1907.12549v4,"['cs.HC', 'cs.CY', 'H.5.1; J.6']"
"Modeling an Augmented Reality Game Environment to Enhance Behavior of
  ADHD Patients","The paper generically models an augmented reality game-based environment to
project the gamification of an online cognitive behavioral therapist that
performs instant measurements for patients with a predefined Attention Deficit
Hyperactivity Disorder (ADHD). ADHD is one of the most common
neurodevelopmental disorders in which patients have difficulties related to
inattention, hyperactivity, and impulsivity. Those patients are in need for a
psychological therapy; the use of cognitive behavioral therapy as a
firmly-established treatment is to help in enhancing the way they think and
behave. A major limitation in traditional cognitive behavioral therapies is
that therapists may face difficulty to optimize patients' neuropsychological
stimulus following a specified treatment plan, i.e., therapists struggle to
draw clear images when stimulating patients' mindset to a point where they
should be. Other limitations recognized here include availability,
accessibility and level-of-experience of the therapists. Therefore, the paper
present a gamification model, we term as ""AR-Therapist,"" in order to take
advantages of augmented reality developments to engage patients in both real
and virtual game-based environments. The model provides an on-time measurements
of patients' progress throughout the treatment sessions which, in result,
overcomes limitations observed in traditional cognitive behavioral therapies.","['Saad Alqithami', 'Musaad Alzahrani', 'Abdulkareem Alzahrani', 'Ahmed Mostafa']",2019-11-04T01:57:13Z,http://arxiv.org/abs/1911.01003v1,"['cs.HC', 'cs.CY', 'cs.MA', 'I.6.5; I.6.3; J.3; F.1.2; F.4.1']"
Augmented Reality for Depth Cues in Monocular Minimally Invasive Surgery,"One of the major challenges in Minimally Invasive Surgery (MIS) such as
laparoscopy is the lack of depth perception. In recent years, laparoscopic
scene tracking and surface reconstruction has been a focus of investigation to
provide rich additional information to aid the surgical process and compensate
for the depth perception issue. However, robust 3D surface reconstruction and
augmented reality with depth perception on the reconstructed scene are yet to
be reported. This paper presents our work in this area. First, we adopt a
state-of-the-art visual simultaneous localization and mapping (SLAM) framework
- ORB-SLAM - and extend the algorithm for use in MIS scenes for reliable
endoscopic camera tracking and salient point mapping. We then develop a robust
global 3D surface reconstruction frame- work based on the sparse point clouds
extracted from the SLAM framework. Our approach is to combine an outlier
removal filter within a Moving Least Squares smoothing algorithm and then
employ Poisson surface reconstruction to obtain smooth surfaces from the
unstructured sparse point cloud. Our proposed method has been quantitatively
evaluated compared with ground-truth camera trajectories and the organ model
surface we used to render the synthetic simulation videos. In vivo laparoscopic
videos used in the tests have demonstrated the robustness and accuracy of our
proposed framework on both camera tracking and surface reconstruction,
illustrating the potential of our algorithm for depth augmentation and
depth-corrected augmented reality in MIS with monocular endoscopes.","['Long Chen', 'Wen Tang', 'Nigel W. John', 'Tao Ruan Wan', 'Jian Jun Zhang']",2017-03-01T18:01:52Z,http://arxiv.org/abs/1703.01243v1,['cs.CV']
"On-the-fly Augmented Reality for Orthopaedic Surgery Using a Multi-Modal
  Fiducial","Fluoroscopic X-ray guidance is a cornerstone for percutaneous orthopaedic
surgical procedures. However, two-dimensional observations of the
three-dimensional anatomy suffer from the effects of projective simplification.
Consequently, many X-ray images from various orientations need to be acquired
for the surgeon to accurately assess the spatial relations between the
patient's anatomy and the surgical tools. In this paper, we present an
on-the-fly surgical support system that provides guidance using augmented
reality and can be used in quasi-unprepared operating rooms. The proposed
system builds upon a multi-modality marker and simultaneous localization and
mapping technique to co-calibrate an optical see-through head mounted display
to a C-arm fluoroscopy system. Then, annotations on the 2D X-ray images can be
rendered as virtual objects in 3D providing surgical guidance. We
quantitatively evaluate the components of the proposed system, and finally,
design a feasibility study on a semi-anthropomorphic phantom. The accuracy of
our system was comparable to the traditional image-guided technique while
substantially reducing the number of acquired X-ray images as well as procedure
time. Our promising results encourage further research on the interaction
between virtual and real objects, that we believe will directly benefit the
proposed method. Further, we would like to explore the capabilities of our
on-the-fly augmented reality support system in a larger study directed towards
common orthopaedic interventions.","['Sebastian Andress', 'Alex Johnson', 'Mathias Unberath', 'Alexander Winkler', 'Kevin Yu', 'Javad Fotouhi', 'Simon Weidert', 'Greg Osgood', 'Nassir Navab']",2018-01-04T22:02:33Z,http://arxiv.org/abs/1801.01560v1,['cs.CV']
"The Impact of Augmented-Reality Head-Mounted Displays on Users' Movement
  Behavior: An Exploratory Study","The augmented-reality head-mounted display (e.g., Microsoft HoloLens) is one
of the most innovative technologies in multimedia and human-computer
interaction in recent years. Despite the emerging research of its applications
on engineering, education, medicines, to name a few, its impact on users'
movement behavior is still underexplored. The movement behavior, especially for
office workers with sedentary lifestyles, is related to many chronic
conditions. Unlike the traditional screens, the augmented-reality head-mounted
display (AR-HMD) could enable mobile virtual screens, which might impact on
users' movement behavior. In this paper, we present our initial study to
explore the impact of AR-HMDs on users' movement behavior. We compared the
differences of macro-movements (e.g., sit-stand transitions) and
micro-movements (e.g., moving the head) between two experimental modes (i.e.,
spatial-mapping and tag-along) with a dedicated trivial quiz task using
HoloLens. The study reveals interesting findings: strong evidence supports that
participants had more head-movements in the tag-along mode where higher
simplicity and freedom of moving the virtual screen were given; body
position/direction changes show the same effect with moderate evidence, while
sit-stand transitions show no difference between the two modes with weak
evidence. Our results imply several design considerations and research
opportunities for future work on the ergonomics of AR-HMDs in the perspective
of health.","['Yunlong Wang', 'Harald Reiterer']",2019-05-24T16:20:08Z,http://arxiv.org/abs/1905.10315v2,['cs.HC']
"Augmented reality as a tool for open science platform by research
  collaboration in virtual teams","The provision of open science is defined as a general policy aimed at
overcoming the barriers that hinder the implementation of the European Research
Area (ERA). An open science foundation seeks to capture all the elements needed
for the functioning of ERA: research data, scientific instruments, ICT services
(connections, calculations, platforms, and specific studies such as portals).
Managing shared resources for the community of scholars maximizes the benefits
to society. In the field of digital infrastructure, this has already
demonstrated great benefits. It is expected that applying this principle to an
open science process will improve management by funding organizations in
collaboration with stakeholders through mechanisms such as public consultation.
This will increase the perception of joint ownership of the infrastructure. It
will also create clear and non-discriminatory access rules, along with a sense
of joint ownership that stimulates a higher level of participation,
collaboration and social reciprocity. The article deals with the concept of
open science. The concept of the European cloud of open science and its
structure are presented. According to the study, it has been shown that the
structure of the cloud of open science includes an augmented reality as an
open-science platform. An example of the practical application of this tool is
the general description of MaxWhere, developed by Hungarian scientists, and is
a platform of aggregates of individual 3D spaces.","['Mariya P. Shyshkina', 'Maiia V. Marienko']",2020-02-28T07:32:07Z,http://arxiv.org/abs/2003.07687v1,['cs.CY']
Object Detection in the Context of Mobile Augmented Reality,"In the past few years, numerous Deep Neural Network (DNN) models and
frameworks have been developed to tackle the problem of real-time object
detection from RGB images. Ordinary object detection approaches process
information from the images only, and they are oblivious to the camera pose
with regard to the environment and the scale of the environment. On the other
hand, mobile Augmented Reality (AR) frameworks can continuously track a
camera's pose within the scene and can estimate the correct scale of the
environment by using Visual-Inertial Odometry (VIO). In this paper, we propose
a novel approach that combines the geometric information from VIO with semantic
information from object detectors to improve the performance of object
detection on mobile devices. Our approach includes three components: (1) an
image orientation correction method, (2) a scale-based filtering approach, and
(3) an online semantic map. Each component takes advantage of the different
characteristics of the VIO-based AR framework. We implemented the AR-enhanced
features using ARCore and the SSD Mobilenet model on Android phones. To
validate our approach, we manually labeled objects in image sequences taken
from 12 room-scale AR sessions. The results show that our approach can improve
on the accuracy of generic object detectors by 12% on our dataset.","['Xiang Li', 'Yuan Tian', 'Fuyao Zhang', 'Shuxue Quan', 'Yi Xu']",2020-08-15T05:15:00Z,http://arxiv.org/abs/2008.06655v1,['cs.CV']
"AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder","The ability to interpret social cues comes naturally for most people, but for
those living with Autism Spectrum Disorder (ASD), some experience a deficiency
in this area. This paper presents the development of a multimodal augmented
reality (AR) system which combines the use of computer vision and deep
convolutional neural networks (CNN) in order to assist individuals with the
detection and interpretation of facial expressions in social settings. The
proposed system, which we call AEGIS (Augmented-reality Expression Guided
Interpretation System), is an assistive technology deployable on a variety of
user devices including tablets, smartphones, video conference systems, or
smartglasses, showcasing its extreme flexibility and wide range of use cases,
to allow integration into daily life with ease. Given a streaming video camera
source, each real-world frame is passed into AEGIS, processed for facial
bounding boxes, and then fed into our novel deep convolutional time windowed
neural network (TimeConvNet). We leverage both spatial and temporal information
in order to provide an accurate expression prediction, which is then converted
into its corresponding visualization and drawn on top of the original video
frame. The system runs in real-time, requires minimal set up and is simple to
use. With the use of AEGIS, we can assist individuals living with ASD to learn
to better identify expressions and thus improve their social experiences.","['James Ren Hou Lee', 'Alexander Wong']",2020-10-22T17:20:38Z,http://arxiv.org/abs/2010.11884v1,"['cs.CV', 'cs.HC']"
"Multicenter Assessment of Augmented Reality Registration Methods for
  Image-guided Interventions","Purpose: To evaluate manual and automatic registration times as well as
accuracy with augmented reality during alignment of a holographic 3-dimensional
(3D) model onto the real-world environment.
  Method: 18 participants in various stages of clinical training across two
academic centers registered a 3D CT phantom model onto a CT grid using the
HoloLens 2 augmented reality headset 3 consecutive times. Registration times
and accuracy were compared among different registration methods (hand gesture,
Xbox controller, and automatic registration), levels of clinical experience,
and consecutive attempts. Registration times were also compared with prior
HoloLens 1 data.
  Results: Mean aggregate manual registration times were 27.7, 24.3, and 72.8
seconds for one-handed gesture, two-handed gesture, and Xbox controller,
respectively; mean automatic registration time was 5.3s (ANOVA p<0.0001). No
significant difference in registration times was found among attendings,
residents and fellows, and medical students (p>0.05). Significant improvements
in registration times were detected across consecutive attempts using hand
gestures (p<0.01). Compared with previously reported HoloLens 1 experience,
hand gesture registration times were 81.7% faster (p<0.05). Registration
accuracies were not significantly different across manual registration methods,
measuring at 5.9, 9.5, and 8.6 mm with one-handed gesture, two-handed gesture,
and Xbox controller, respectively (p>0.05).
  Conclusions: Manual registration times decreased significantly with updated
hand gesture maneuvers on HoloLens 2 versus HoloLens 1, approaching the
registration times of automatic registration and outperforming Xbox controller
mediated registration. These results will encourage wider clinical integration
of HoloLens 2 in procedural medical care.","['Ningcheng Li', 'Jonathan Wakim', 'Yilun Koethe', 'Timothy Huber', 'Terence Gade', 'Stephen Hunt', 'Brian Park']",2020-12-03T23:05:48Z,http://arxiv.org/abs/2012.02319v1,"['physics.med-ph', 'cs.CV', 'cs.HC']"
Seeing Thru Walls: Visualizing Mobile Robots in Augmented Reality,"We present an approach for visualizing mobile robots through an Augmented
Reality headset when there is no line-of-sight visibility between the robot and
the human. Three elements are visualized in Augmented Reality: 1) Robot's 3D
model to indicate its position, 2) An arrow emanating from the robot to
indicate its planned movement direction, and 3) A 2D grid to represent the
ground plane. We conduct a user study with 18 participants, in which each
participant are asked to retrieve objects, one at a time, from stations at the
two sides of a T-junction at the end of a hallway where a mobile robot is
roaming. The results show that visualizations improved the perceived safety and
efficiency of the task and led to participants being more comfortable with the
robot within their personal spaces. Furthermore, visualizing the motion intent
in addition to the robot model was found to be more effective than visualizing
the robot model alone. The proposed system can improve the safety of automated
warehouses by increasing the visibility and predictability of robots.","['Morris Gu', 'Akansel Cosgun', 'Wesley P. Chan', 'Tom Drummond', 'Elizabeth Croft']",2021-04-08T06:54:37Z,http://arxiv.org/abs/2104.03547v2,['cs.RO']
Online and Offline Robot Programming via Augmented Reality Workspaces,"Robot programming methods for industrial robots are time consuming and often
require operators to have knowledge in robotics and programming. To reduce
costs associated with reprogramming, various interfaces using augmented reality
have recently been proposed to provide users with more intuitive means of
controlling robots in real-time and programming them without having to code.
However, most solutions require the operator to be close to the real robot's
workspace which implies either removing it from the production line or shutting
down the whole production line due to safety hazards. We propose a novel
augmented reality interface providing the users with the ability to model a
virtual representation of a workspace which can be saved and reused to program
new tasks or adapt old ones without having to be co-located with the real
robot. Similar to previous interfaces, the operators then have the ability to
program robot tasks or control the robot in real-time by manipulating a virtual
robot. We evaluate the intuitiveness and usability of the proposed interface
with a user study where 18 participants programmed a robot manipulator for a
disassembly task.","['Yong Joon Thoo', 'Jérémy Maceiras', 'Philip Abbet', 'Mattia Racca', 'Hakan Girgin', 'Sylvain Calinon']",2021-07-05T09:15:13Z,http://arxiv.org/abs/2107.01884v2,['cs.RO']
"Augmented Reality and Gamification: A Framework for Developing
  Supplementary Learning Tool","The main purpose of the study is to develop a supplementary learning tool
framework by the use of a dynamic mobile application using Unity AR and Vuforia
for Senior High School (SHS) students and teachers to help the learning process
in SHS Earth Science. The researchers will be using the Software Development
Life Cycle (SDLC) Model of methodology to ensure the quality of the software as
well as the correctness of the development process. The expected result of the
study is that Augmented Reality and Gamification will now be used as a
supplementary learning tool in SHS Earth Science. Augmented Reality and
Gamification can now be used as a supplementary learning tool in SHS Earth
Science using the designed framework. Future studies will focus on the
development of the framework and the mobile application. Since the system has a
lot of potential in the education sector and due to the effects of COVID-19,
the software will serve as a pioneer to show that a supplementary tool will
help students learn logically and entertainingly especially since schools
nowadays are transitioning with either distance learning or blended learning.",['Carlo H. Godoy Jr'],2021-08-07T17:03:00Z,http://arxiv.org/abs/2108.03487v1,"['cs.GT', 'cond-mat.other', 'physics.chem-ph', '91Axx', 'K.6.3; K.3']"
Stay in Touch! Shape and Shadow Influence Surface Contact in XR Displays,"The information provided to a person's visual system by extended reality (XR)
displays is not a veridical match to the information provided by the real
world. Due in part to graphical limitations in XR head-mounted displays (HMDs),
which vary by device, our perception of space may be altered. However, we do
not yet know which properties of virtual objects rendered by HMDs --
particularly augmented reality displays -- influence our ability to understand
space. In the current research, we evaluate how immersive graphics affect
spatial perception across three unique XR displays: virtual reality (VR), video
see-through augmented reality (VST AR), and optical see-through augmented
reality (OST AR). We manipulated the geometry of the presented objects as well
as the shading techniques for objects' cast shadows. Shape and shadow were
selected for evaluation as they play an important role in determining where an
object is in space by providing points of contact between an object and its
environment -- be it real or virtual. Our results suggest that a
non-photorealistic (NPR) shading technique, in this case for cast shadows, may
be used to improve depth perception by enhancing perceived surface contact in
XR. Further, the benefit of NPR graphics is more pronounced in AR than in VR
displays. One's perception of ground contact is influenced by an object's
shape, as well. However, the relationship between shape and surface contact
perception is more complicated.","['Haley Adams', 'Holly Gagnon', 'Sarah Creem-Regehr', 'Jeanine Stefanucci', 'Bobby Bodenheimer']",2022-01-06T02:00:41Z,http://arxiv.org/abs/2201.01889v1,"['cs.HC', 'cs.GR']"
"A Review of Augmented Reality Apps for an AR-Based STEM Education
  Framework","Within the past two decades, Augmented Reality (AR) applications have
received increased attention. Augmented Reality is now widely used in the
education sector at level K to 12. AR is expected to be generally adopted in
two to three years in higher education and four to five years in K to 12.
Applying AR technology in the education sector especially in STEM subjects, can
result in having a smart campus. In adopting a SMART Campus strategy, education
practitioners must address many intrinsic issues in science, technology,
engineering, and mathematics (STEM) research. For example, in physics, there
are expensive or insufficient laboratory systems, system faults, and difficulty
simulating other experimental circumstances. In technology, many schools do not
have enough computers. In engineering, there are only a few instructors who are
knowledgeable in computer aided design (CAD). In mathematics, few teachers
incorporate technology into their lessons often because they believe it is
still better to teach through the traditional methods. Hence, In this paper we
discuss how AR is being used now in different learning areas in STEM to open
new doors to researchers and teachers as they transition their schools into
SMART campuses with the use of AR apps. Aligned with this, a suggested
framework for school administrators and policymakers is proposed based on a
review of the positive benefits of different AR apps.",['Carlo H. Godoy Jr.'],2022-01-19T08:30:26Z,http://arxiv.org/abs/2203.07024v1,"['physics.ed-ph', 'cs.HC', 'C.2']"
"Deep Learning and Handheld Augmented Reality Based System for Optimal
  Data Collection in Fault Diagnostics Domain","Compared to current AI or robotic systems, humans navigate their environment
with ease, making tasks such as data collection trivial. However, humans find
it harder to model complex relationships hidden in the data. AI systems,
especially deep learning (DL) algorithms, impressively capture those complex
relationships. Symbiotically coupling humans and computational machines'
strengths can simultaneously minimize the collected data required and build
complex input-to-output mapping models. This paper enables this coupling by
presenting a novel human-machine interaction framework to perform fault
diagnostics with minimal data. Collecting data for diagnosing faults for
complex systems is difficult and time-consuming. Minimizing the required data
will increase the practicability of data-driven models in diagnosing faults.
The framework provides instructions to a human user to collect data that
mitigates the difference between the data used to train and test the fault
diagnostics model. The framework is composed of three components: (1) a
reinforcement learning algorithm for data collection to develop a training
dataset, (2) a deep learning algorithm for diagnosing faults, and (3) a
handheld augmented reality application for data collection for testing data.
The proposed framework has provided above 100\% precision and recall on a novel
dataset with only one instance of each fault condition. Additionally, a
usability study was conducted to gauge the user experience of the handheld
augmented reality application, and all users were able to follow the provided
steps.","['Ryan Nguyen', 'Rahul Rai']",2022-06-15T19:15:26Z,http://arxiv.org/abs/2206.07772v1,['cs.AI']
"Arigatō: Effects of Adaptive Guidance on Engagement and Performance in
  Augmented Reality Learning Environments","Experiential learning (ExL) is the process of learning through experience or
more specifically ""learning through reflection on doing"". In this paper, we
propose a simulation of these experiences, in Augmented Reality (AR),
addressing the problem of language learning. Such systems provide an excellent
setting to support ""adaptive guidance"", in a digital form, within a real
environment. Adaptive guidance allows the instructions and learning content to
be customised for the individual learner, thus creating a unique learning
experience. We developed an adaptive guidance AR system for language learning,
we call Arigat\=o (Augmented Reality Instructional Guidance & Tailored
Omniverse), which offers immediate assistance, resources specific to the
learner's needs, manipulation of these resources, and relevant feedback.
Considering guidance, we employ this prototype to investigate the effect of the
amount of guidance (fixed vs. adaptive-amount) and the type of guidance (fixed
vs. adaptive-associations) on the engagement and consequently the learning
outcomes of language learning in an AR environment. The results for the amount
of guidance show that compared to the adaptive-amount, the fixed-amount of
guidance group scored better in the immediate and delayed (after 7 days) recall
tests. However, this group also invested a significantly higher mental effort
to complete the task. The results for the type of guidance show that the
adaptive-associations group outperforms the fixed-associations group in the
immediate, delayed (after 7 days) recall tests, and learning efficiency. The
adaptive-associations group also showed significantly lower mental effort and
spent less time to complete the task.","['Maheshya Weerasinghe', 'Aaron Quigley', 'Klen Čopič Pucihar', 'Alice Toniolo', 'Angela Miguel', 'Matjaž Kljun']",2022-07-02T11:20:13Z,http://arxiv.org/abs/2207.00798v1,"['cs.HC', 'I.3.7']"
"Augmented Reality's Potential for Identifying and Mitigating Home
  Privacy Leaks","Users face various privacy risks in smart homes, yet there are limited ways
for them to learn about the details of such risks, such as the data practices
of smart home devices and their data flow. In this paper, we present Privacy
Plumber, a system that enables a user to inspect and explore the privacy
""leaks"" in their home using an augmented reality tool. Privacy Plumber allows
the user to learn and understand the volume of data leaving the home and how
that data may affect a user's privacy -- in the same physical context as the
devices in question, because we visualize the privacy leaks with augmented
reality. Privacy Plumber uses ARP spoofing to gather aggregate network traffic
information and presents it through an overlay on top of the device in an
smartphone app. The increased transparency aims to help the user make privacy
decisions and mend potential privacy leaks, such as instruct Privacy Plumber on
what devices to block, on what schedule (i.e., turn off Alexa when sleeping),
etc. Our initial user study with six participants demonstrates participants'
increased awareness of privacy leaks in smart devices, which further
contributes to their privacy decisions (e.g., which devices to block).","['Stefany Cruz', 'Logan Danek', 'Shinan Liu', 'Christopher Kraemer', 'Zixin Wang', 'Nick Feamster', 'Danny Yuxing Huang', 'Yaxing Yao', 'Josiah Hester']",2023-01-27T21:36:23Z,http://arxiv.org/abs/2301.11998v1,['cs.CR']
"Task-oriented and Semantics-aware Communication Framework for
  Avatar-centric Augmented Reality","Upon the advent of the emerging metaverse and its related applications in
Augmented Reality (AR), the current bit-oriented network struggles to support
real-time changes for the vast amount of associated information, hindering its
development. Thus, a critical revolution in the Sixth Generation (6G) networks
is envisioned through the joint exploitation of information context and its
importance to the task, leading to a communication paradigm shift towards
semantic and effectiveness levels. However, current research has not yet
proposed any explicit and systematic communication framework for AR
applications that incorporate these two levels. To fill this research gap, this
paper presents a task-oriented and semantics-aware communication framework for
augmented reality (TSAR) to enhance communication efficiency and effectiveness
in 6G. Specifically, we first analyse the traditional wireless AR point cloud
communication framework and then summarize our proposed semantic information
along with the end-to-end wireless communication. We then detail the design
blocks of the TSAR framework, covering both semantic and effectiveness levels.
Finally, numerous experiments have been conducted to demonstrate that, compared
to the traditional point cloud communication framework, our proposed TSAR
significantly reduces wireless AR application transmission latency by 95.6%,
while improving communication effectiveness in geometry and color aspects by up
to 82.4% and 20.4%, respectively.","['Zhe Wang', 'Yansha Deng', 'A. Hamid Aghvami']",2023-06-27T13:41:54Z,http://arxiv.org/abs/2306.15470v3,"['cs.IT', 'math.IT']"
"That Doesn't Go There: Attacks on Shared State in Multi-User Augmented
  Reality Applications","Augmented Reality (AR) is expected to become a pervasive component in
enabling shared virtual experiences. In order to facilitate collaboration among
multiple users, it is crucial for multi-user AR applications to establish a
consensus on the ""shared state"" of the virtual world and its augmentations,
through which they interact within augmented reality spaces. Current methods to
create and access shared state collect sensor data from devices (e.g., camera
images), process them, and integrate them into the shared state. However, this
process introduces new vulnerabilities and opportunities for attacks.
Maliciously writing false data to ""poison"" the shared state is a major concern
for the security of the downstream victims that depend on it. Another type of
vulnerability arises when reading the shared state; by providing false inputs,
an attacker can view hologram augmentations at locations they are not allowed
to access. In this work, we demonstrate a series of novel attacks on multiple
AR frameworks with shared states, focusing on three publicly-accessible
frameworks. We show that these frameworks, while using different underlying
implementations, scopes, and mechanisms to read from and write to the shared
state, have shared vulnerability to a unified threat model. Our evaluation of
these state-of-art AR applications demonstrates reliable attacks both on
updating and accessing shared state across the different systems. To defend
against such threats, we discuss a number of potential mitigation strategies
that can help enhance the security of multi-user AR applications.","['Carter Slocum', 'Yicheng Zhang', 'Erfan Shayegani', 'Pedram Zaree', 'Nael Abu-Ghazaleh', 'Jiasi Chen']",2023-08-17T18:33:23Z,http://arxiv.org/abs/2308.09146v2,['cs.CR']
Virtual Augmented Reality for Atari Reinforcement Learning,"Reinforcement Learning (RL) has achieved significant milestones in the gaming
domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken
Jie. This victory was also made possible through the Atari Learning Environment
(ALE): The ALE has been foundational in RL research, facilitating significant
RL algorithm developments such as AlphaGo and others. In current Atari video
game RL research, RL agents' perceptions of its environment is based on raw
pixel data from the Atari video game screen with minimal image preprocessing.
Contrarily, cutting-edge ML research, external to the Atari video game RL
research domain, is focusing on enhancing image perception. A notable example
is Meta Research's ""Segment Anything Model"" (SAM), a foundation model capable
of segmenting images without prior training (zero-shot). This paper addresses a
novel methodical question: Can state-of-the-art image segmentation models such
as SAM improve the performance of RL agents playing Atari video games? The
results suggest that SAM can serve as a ""virtual augmented reality"" for the RL
agent, boosting its Atari video game playing performance under certain
conditions. Comparing RL agent performance results from raw and augmented pixel
inputs provides insight into these conditions. Although this paper was limited
by computational constraints, the findings show improved RL agent performance
for augmented pixel inputs and can inform broader research agendas in the
domain of ""virtual augmented reality for video game playing RL agents"".",['Christian A. Schiller'],2023-10-12T19:42:42Z,http://arxiv.org/abs/2310.08683v1,"['cs.LG', 'cs.AI']"
"Augmented Voices: An Augmented Reality Experience Highlighting the
  Social Injustices of Gender-Based Violence in the Muslim South-Asian Diaspora","This paper delves into the distressing prevalence of gender-based violence
(GBV) and its deep-seated psychological ramifications, particularly among
Muslim South Asian women living in diasporic communities. Despite the gravity
of GBV, these women often face formidable barriers in voicing their experiences
and accessing support. ""Augmented Voices"" emerges as a technological beacon,
harnessing the potential of augmented reality (AR) to bridge the digital and
physical realms through mobile devices, enhancing the visibility of these
often-silenced voices. With its technological motivation firmly anchored in the
convergence of AR and real-world interactions, ""Augmented Voices"" offers a
digital platform where storytelling acts as a catalyst, bringing to the fore
the experiences shared by these women. By superimposing their narratives onto
physical locations via Geographic Information System (GIS) Mapping, the
application ""augments their voices"" in the diaspora, providing a conduit for
expression and solidarity. This project, currently at its developmental stage,
aspires to elevate the stories of GBV victims to a level where their struggles
are not just heard but felt, forging a powerful connection between the user and
the narrative. It is designed to transcend the limitations of conventional
storytelling, creating an ""augmented"" reality where voices that are often muted
by societal constraints can resonate powerfully. The project underscores the
urgent imperative to confront GBV, catalyzing societal transformation and
fostering robust support networks for those in the margins. It is a pioneering
example of how technology can become a formidable ally in the fight for social
justice and the empowerment of the oppressed. Additionally, this paper delves
into the AR workflow illustrating its relevance and contribution to the broader
theme of site-specific AR for social justice.",['Hamida Khatri'],2024-04-23T17:17:52Z,http://arxiv.org/abs/2404.15239v1,"['cs.HC', 'cs.ET']"
AIR: Anywhere Immersive Reality with User-Perspective Projection,"Projection-based augmented reality (AR) has much potential, but is limited in
that it requires burdensome installations and prone to geometric distortions on
display surface. To overcome these limitations, we propose AIR. It can be
carried and placed anywhere to project AR using pan/tilting motors, while
providing the user with distortion-free projection of a correct 3D view.","['JungHyun Byun', 'SeungHo Chae', 'YoonSik Yang', 'TackDon Han']",2018-12-01T17:54:36Z,http://arxiv.org/abs/1812.00233v1,['cs.GR']
"Augmented Reality, Cyber-Physical Systems, and Feedback Control for
  Additive Manufacturing: A Review","Our objective in this paper is to review the application of feedback ideas in
the area of additive manufacturing. Both the application of feedback control to
the 3D printing process, and the application of feedback theory to enable users
to interact better with machines, are reviewed. Where appropriate,
opportunities for future work are highlighted.","['Hugo Lhachemi', 'Ammar Malik', 'Robert Shorten']",2019-03-05T13:21:44Z,http://arxiv.org/abs/1903.01808v1,"['cs.HC', 'cs.SY']"
Recent Advances in 3D Object and Hand Pose Estimation,"3D object and hand pose estimation have huge potentials for Augmented
Reality, to enable tangible interfaces, natural interfaces, and blurring the
boundaries between the real and virtual worlds. In this chapter, we present the
recent developments for 3D object and hand pose estimation using cameras, and
discuss their abilities and limitations and the possible future development of
the field.",['Vincent Lepetit'],2020-06-10T16:25:28Z,http://arxiv.org/abs/2006.05927v1,['cs.CV']
"Ergonomic-Centric Holography: Optimizing Realism,Immersion, and Comfort
  for Holographic Display","We introduce ergonomic-centric holography, an algorithmic framework that
simultaneously optimizes for realistic incoherent defocus, unrestricted pupil
movements in the eye box, and high-order diffractions for filtering-free
holography. The proposed method outperforms prior algorithms on holographic
display prototypes operating in unfiltered and pupil-mimicking modes, offering
the potential to enhance next-generation virtual and augmented reality
experiences.","['Liang Shi', 'DongHun Ryu', 'Wojciech Matusik']",2023-06-13T21:08:05Z,http://arxiv.org/abs/2306.08138v2,"['cs.GR', 'physics.optics']"
Ubiquitous Talker: Spoken Language Interaction with Real World Objects,"Augmented reality is a research area that tries to embody an electronic
information space within the real world, through computational devices. A
crucial issue within this area, is the recognition of real world objects or
situations.
  In natural language processing, it is much easier to determine
interpretations of utterances, even if they are ill-formed, when the context or
situation is fixed. We therefore introduce robust, natural language processing
into a system of augmented reality with situation awareness. Based on this
idea, we have developed a portable system, called the Ubiquitous Talker. This
consists of an LCD display that reflects the scene at which a user is looking
as if it is a transparent glass, a CCD camera for recognizing real world
objects with color-bar ID codes, a microphone for recognizing a human voice and
a speaker which outputs a synthesized voice. The Ubiquitous Talker provides its
user with some information related to a recognized object, by using the display
and voice. It also accepts requests or questions as voice inputs. The user
feels as if he/she is talking with the object itself through the system.","['Katashi Nagao', 'Jun Rekimoto']",1995-05-23T07:14:19Z,http://arxiv.org/abs/cmp-lg/9505038v1,"['cmp-lg', 'cs.CL']"
"Towards Quality of Experience Determination for Video in Augmented
  Binocular Vision Scenarios","With the continuous growth in the consumer markets of mobile smartphones and
increasingly in augmented reality wearable devices, several avenues of research
investigate the relationships between the quality perceived by mobile users and
the delivery mechanisms at play to support a high quality of experience for
mobile users. In this paper, we present the first study that evaluates the
relationships of mobile movie quality and the viewer-perceived quality thereof
in an augmented reality setting with see-through devices. We find that
participants tend to overestimate the video quality and exhibit a significant
variation of accuracy that leans onto the movie content and its dynamics. Our
findings, thus, can broadly impact future media adaptation and delivery
mechanisms for this new display format of mobile multimedia.",['Patrick Seeling'],2014-06-04T00:14:06Z,http://arxiv.org/abs/1406.0912v3,"['cs.MM', 'cs.HC']"
When Augmented Reality Meets Big Data,"With computing and sensing woven into the fabric of everyday life, we live in
an era where we are awash in a flood of data from which we can gain rich
insights. Augmented reality (AR) is able to collect and help analyze the
growing torrent of data about user engagement metrics within our personal
mobile and wearable devices. This enables us to blend information from our
senses and the digitalized world in a myriad of ways that was not possible
before. AR and big data have a logical maturity that inevitably converge them.
The tread of harnessing AR and big data to breed new interesting applications
is starting to have a tangible presence. In this paper, we explore the
potential to capture value from the marriage between AR and big data
technologies, following with several challenges that must be addressed to fully
realize this potential.","['Zhanpeng Huang', 'Pan Hui', 'Christoph Peylo']",2014-07-27T13:21:10Z,http://arxiv.org/abs/1407.7223v1,"['cs.DB', 'cs.MM', 'cs.SI', 'H.5.1']"
"WalkieLokie: Relative Positioning for Augmented Reality Using a Dummy
  Acoustic Speaker","We propose and implement a novel relative positioning system, WalkieLokie, to
enable more kinds of Augmented Reality applications, e.g., virtual shopping
guide, virtual business card sharing. WalkieLokie calculates the distance and
direction between an inquiring user and the corresponding target. It only
requires a dummy speaker binding to the target and broadcasting inaudible
acoustic signals. Then the user walking around can obtain the position using a
smart device. The key insight is that when a user walks, the distance between
the smart device and the speaker changes; and the pattern of displacement
(variance of distance) corresponds to the relative position. We use a
second-order phase locked loop to track the displacement and further estimate
the position. To enhance the accuracy and robustness of our strategy, we
propose a synchronization mechanism to synthesize all estimation results from
different timeslots. We show that the mean error of ranging and direction
estimation is 0.63m and 2.46 degrees respectively, which is accurate even in
case of virtual business card sharing. Furthermore, in the shopping mall where
the environment is quite severe, we still achieve high accuracy of positioning
one dummy speaker, and the mean position error is 1.28m.","['Wenchao Huang', 'Yan Xiong', 'Xiang-Yang Li', 'Yiqing Hu', 'Xufei Mao', 'Panlong Yang']",2015-08-22T08:15:04Z,http://arxiv.org/abs/1508.05477v1,['cs.CY']
Immersive Augmented Reality Training for Complex Manufacturing Scenarios,"In the complex manufacturing sector a considerable amount of resources are
focused on developing new skills and training workers. In that context,
increasing the effectiveness of those processes and reducing the investment
required is an outstanding issue. In this paper we present an experiment that
shows how modern Human Computer Interaction (HCI) metaphors such as
collaborative mixed-reality can be used to transmit procedural knowledge and
could eventually replace other forms of face-to-face training. We implement a
real-time Immersive Augmented Reality (IAR) setup with see-through cameras that
allows for collaborative interactions that can simulate conventional forms of
training. The obtained results indicate that people who took the IAR training
achieved the same performance than people in the conventional face-to-face
training condition. These results, their implications for future training and
the use of HCI paradigms in this context are discussed in this paper.","['Mar Gonzalez-Franco', 'Julio Cermeron', 'Katie Li', 'Rodrigo Pizarro', 'Jacob Thorn', 'Windo Hutabarat', 'Ashutosh Tiwari', 'Pablo Bermell-Garcia']",2016-02-05T07:50:25Z,http://arxiv.org/abs/1602.01944v2,['cs.HC']
"3D Character Customization for Non-Professional Users in Handheld
  Augmented Reality","In gaming, customizing individual characters, can create personal bonds
between players and their characters. Hence, character customization is a
standard component in many games. While mobile Augmented Reality (AR) games
become popular, to date, no 3D character editor for AR games exists. We
investigate the feasibility of 3D character customization for smartphone-based
AR in an iterative design process.
  Specifically, we present findings from creating AR prototypes in a handheld
AR setting. In a first user study, we found that a tangible AR prototype
resulted in higher hedonistic measures than a camera-based approach. In a
follow up study, we compared the tangible AR prototype with a non-AR
touchscreen version for selection, scaling, translation and rotation tasks in a
3D character customization setting. The tangible AR version resulted in
significantly better results for stimulation and novelty measures than the
non-AR version. At the same time, it maintained a proficient level in pragmatic
measures such as accuracy and efficiency.","['Iris Seidinger', 'Jens Grubert']",2016-07-22T08:00:55Z,http://arxiv.org/abs/1607.06587v1,"['cs.HC', 'H.5.1']"
"Towards an Understanding of the Effects of Augmented Reality Games on
  Disaster Management","Location-based augmented reality games have entered the mainstream with the
nearly overnight success of Niantic's Pok\'emon Go. Unlike traditional video
games, the fact that players of such games carry out actions in the external,
physical world to accomplish in-game objectives means that the large-scale
adoption of such games motivate people, en masse, to do things and go places
they would not have otherwise done in unprecedented ways. The social
implications of such mass-mobilisation of individual players are, in general,
difficult to anticipate or characterise, even for the short-term. In this work,
we focus on disaster relief, and the short- and long-term implications that a
proliferation of AR games like Pok\'emon Go, may have in disaster-prone regions
of the world. We take a distributed cognition approach and focus on one natural
disaster-prone region of New Zealand, the city of Wellington.",['Markus Luczak-Roesch'],2017-02-21T22:52:43Z,http://arxiv.org/abs/1702.06610v1,"['cs.CY', 'cs.HC']"
"Semantic Augmented Reality Environment with Material-Aware Physical
  Interactions","In Augmented Reality (AR) environment, realistic interactions between the
virtual and real objects play a crucial role in user experience. Much of recent
advances in AR has been largely focused on developing geometry-aware
environment, but little has been done in dealing with interactions at the
semantic level. High-level scene understanding and semantic descriptions in AR
would allow effective design of complex applications and enhanced user
experience. In this paper, we present a novel approach and a prototype system
that enables the deeper understanding of semantic properties of the real world
environment, so that realistic physical interactions between the real and the
virtual objects can be generated. A material-aware AR environment has been
created based on the deep material learning using a fully convolutional network
(FCN). The state-of-the-art dense Simultaneous Localisation and Mapping (SLAM)
has been used for the semantic mapping. Together with efficient accelerated 3D
ray casting, natural and realistic physical interactions are generated for
interactive AR games. Our approach has significant impact on the future
development of advanced AR systems and applications.","['Long Chen', 'Karl Francis', 'Wen Tang']",2017-08-03T16:52:14Z,http://arxiv.org/abs/1708.01208v3,"['cs.CV', 'cs.GT']"
"VisAR: Bringing Interactivity to Static Data Visualizations through
  Augmented Reality","Static visualizations have analytic and expressive value. However, many
interactive tasks cannot be completed using static visualizations. As datasets
grow in size and complexity, static visualizations start losing their analytic
and expressive power for interactive data exploration. Despite this limitation
of static visualizations, there are still many cases where visualizations are
limited to being static (e.g., visualizations on presentation slides or
posters). We believe in many of these cases, static visualizations will benefit
from allowing users to perform interactive tasks on them. Inspired by the
introduction of numerous commercial personal augmented reality (AR) devices, we
propose an AR solution that allows interactive data exploration of datasets on
static visualizations. In particular, we present a prototype system named VisAR
that uses the Microsoft Hololens to enable users to complete interactive tasks
on static visualizations.","['Taeheon Kim', 'Bahador Saket', 'Alex Endert', 'Blair MacIntyre']",2017-08-04T04:54:24Z,http://arxiv.org/abs/1708.01377v1,['cs.HC']
"Distributed Augmented Reality with 3D Lung Dynamics -- A Planning Tool
  Concept","Augmented Reality (AR) systems add visual information to the world by using
advanced display techniques. The advances in miniaturization and reduced costs
make some of these systems feasible for applications in a wide set of fields.
We present a potential component of the cyber infrastructure for the operating
room of the future; a distributed AR based software-hardware system that allows
real-time visualization of 3D lung dynamics superimposed directly on the
patient's body. Several emergency events (e.g. closed and tension pneumothorax)
and surgical procedures related to the lung (e.g. lung transplantation, lung
volume reduction surgery, surgical treatment of lung infections, lung cancer
surgery) could benefit from the proposed prototype.","['Felix G. Hamza-Lup', 'Anand P. Santhanam', 'Celina Imielinska', 'Sanford Meeks', 'Jannick P. Rolland']",2018-11-29T04:10:23Z,http://arxiv.org/abs/1811.11953v1,"['cs.DC', 'cs.HC']"
Recognizing and tracking outdoor objects by using ARToolKit markers,"We created an augmented reality platform for spatial exploration that
recognizes buildings facades and displays various multimedia for different time
points. In order to provide the user with the best user experience fast
recognition and stable tracking are the key elements of any augmented reality
app. In an outdoor environment, lighting, reflective surfaces and occlusion can
drastically affect the user experience. In a setup where these conditions are
similar, marker creation methodology and the app parameters are key. In this
paper we focus on resizing the photo prior marker creating and the importance
of camera calibration and resolution and their effect on the recognition speed
and quality of tracking outdoor objects.","['Blagoj Nenovski', 'Igor Nedelkovski']",2020-01-04T12:56:59Z,http://arxiv.org/abs/2001.01073v1,['cs.HC']
"Unleashing the Potentials of Immersive Augmented Reality for Software
  Engineering","In immersive augmented reality (IAR), users can wear a head-mounted display
to see computer-generated images superimposed to their view of the world. IAR
was shown to be beneficial across several domains, e.g., automotive, medicine,
gaming and engineering, with positive impacts on, e.g., collaboration and
communication. We think that IAR bears a great potential for software
engineering but, as of yet, this research area has been neglected. In this
vision paper, we elicit potentials and obstacles for the use of IAR in software
engineering. We identify possible areas that can be supported with IAR
technology by relating commonly discussed IAR improvements to typical software
engineering tasks. We further demonstrate how innovative use of IAR technology
may fundamentally improve typical activities of a software engineer through a
comprehensive series of usage scenarios outlining practical application.
Finally, we reflect on current limitations of IAR technology based on our
scenarios and sketch research activities necessary to make our vision a
reality. We consider this paper to be relevant to academia and industry alike
in guiding the steps to innovative research and applications for IAR in
software engineering.","['Leonel Merino', 'Mircea Lungu', 'Christoph Seidl']",2020-01-05T12:22:28Z,http://arxiv.org/abs/2001.01223v1,"['cs.SE', 'cs.HC']"
"Developing an Augmented Reality Tourism App through User-Centred Design
  (Extended Version)","Augmented Reality (AR) bridges the gap between the physical and virtual
world. Through overlaying graphics on natural environments, users can immerse
themselves in a tailored environment. This offers great benefits to mobile
tourism, where points of interest (POIs) can be annotated on a smartphone
screen. While a variety of apps currently exist, usability issues can
discourage users from embracing AR. Interfaces can become cluttered with icons,
with POI occlusion posing further challenges. In this paper, we use
user-centred design (UCD) to develop an AR tourism app. We solicit requirements
through a synthesis of domain analysis, tourist observation and semi-structured
interviews. Whereas previous user-centred work has designed mock-ups, we
iteratively develop a full Android app. This includes overhead maps and route
navigation, in addition to a detailed AR browser. The final product is
evaluated by 20 users, who participate in a tourism task in a UK city. Users
regard the system as usable and intuitive, and suggest the addition of further
customisation. We finish by critically analysing the challenges of a
user-centred methodology.","['Meredydd Williams', 'Kelvin K. K. Yao', 'Jason R. C. Nurse']",2020-01-29T23:35:32Z,http://arxiv.org/abs/2001.11131v1,"['cs.HC', 'cs.CY', 'cs.GR', 'cs.SE']"
Multi-layer Visualization for Medical Mixed Reality,"Medical Mixed Reality helps surgeons to contextualize intraoperative data
with video of the surgical scene. Nonetheless, the surgical scene and
anatomical target are often occluded by surgical instruments and surgeon hands.
In this paper and to our knowledge, we propose a multi-layer visualization in
Medical Mixed Reality solution which subtly improves a surgeon's visualization
by making transparent the occluding objects. As an example scenario, we use an
augmented reality C-arm fluoroscope device. A video image is created using a
volumetric-based image synthesization technique and stereo-RGBD cameras mounted
on the C-arm. From this synthesized view, the background which is occluded by
the surgical instruments and surgeon hands is recovered by modifying the
volumetric-based image synthesization technique. The occluding objects can,
therefore, become transparent over the surgical scene. Experimentation with
different augmented reality scenarios yield results demonstrating that the
background of the surgical scenes can be recovered with accuracy between
45%-99%. In conclusion, we presented a solution that a Mixed Reality solution
for medicine, providing transparency to objects occluding the surgical scene.
This work is also the first application of volumetric field for Diminished
Reality/ Mixed Reality.","['Séverine Habert', 'Ma Meng', 'Pascal Fallavollita', 'Nassir Navab']",2017-09-26T12:13:01Z,http://arxiv.org/abs/1709.08962v1,['cs.CV']
"ToARist: An Augmented Reality Tourism App created through User-Centred
  Design","Through Augmented Reality (AR), virtual graphics can transform the physical
world. This offers benefits to mobile tourism, where points of interest (POIs)
can be annotated on a smartphone screen. Although several of these applications
exist, usability issues can discourage adoption. User-centred design (UCD)
solicits frequent feedback, often contributing to usable products. While AR
mock-ups have been constructed through UCD, we develop a novel and functional
tourism app. We solicit requirements through a synthesis of domain analysis,
tourist observation and semi-structured interviews. Through four rounds of
iterative development, users test and refine the app. The final product, dubbed
ToARist, is evaluated by 20 participants, who engage in a tourism task around a
UK city. Users regard the system as usable, but find technical issues can
disrupt AR. We finish by reflecting on our design and critiquing the challenges
of a strict user-centred methodology.","['Meredydd Williams', 'Kelvin K. K. Yao', 'Jason R. C. Nurse']",2018-07-16T09:44:54Z,http://arxiv.org/abs/1807.05759v1,['cs.HC']
"The Problems of Personnel Training for STEM Education in the Modern
  Innovative Learning and Research Environment","The aim of the article is to describe the problems of personnel training that
arise in view of extension of the STEM approach to education, development of
innovative technologies, in particular, virtualization, augmented reality, the
use of ICT outsourcing in educational systems design. The object of research is
the process of formation and development of the educational and scientific
envi- ronment of educational institution. The subject of the study is the
formation and development of the cloud-based learning and research environment
for STEM education. The methods of research are: the analysis of publications
on the prob- lem, generalization of domestic and foreign experience,
theoretical analysis, sys- tem analysis, systematization and generalization of
research facts and laws for the development and design of the model of the
cloud-based learning environ- ment, substantiation of the main conclusions. The
results of the research are the next: the concepts and the model of the
cloud-based environment of STEM edu- cation is substantiated, the problems of
personnel training at the present stage are outlined.",['Mariya Shyshkina'],2018-07-23T12:36:17Z,http://arxiv.org/abs/1807.08562v1,['cs.CY']
"Dynamic Environment Mapping for Augmented Reality Applications on Mobile
  Devices","Augmented Reality is a topic of foremost interest nowadays. Its main goal is
to seamlessly blend virtual content in real-world scenes. Due to the lack of
computational power in mobile devices, rendering a virtual object with
high-quality, coherent appearance and in real-time, remains an area of active
research. In this work, we present a novel pipeline that allows for coupled
environment acquisition and virtual object rendering on a mobile device
equipped with a depth sensor. While keeping human interaction to a minimum, our
system can scan a real scene and project it onto a two-dimensional environment
map containing RGB+Depth data. Furthermore, we define a set of criteria that
allows for an adaptive update of the environment map to account for dynamic
changes in the scene. Then, under the assumption of diffuse surfaces and
distant illumination, our method exploits an analytic expression for the
irradiance in terms of spherical harmonic coefficients, which leads to a very
efficient rendering algorithm. We show that all the processes in our pipeline
can be executed while maintaining an average frame rate of 31Hz on a mobile
device.","['Rafael Monroy', 'Matis Hudon', 'Aljosa Smolic']",2018-09-21T14:10:55Z,http://arxiv.org/abs/1809.08134v1,['cs.CV']
An Edge-Computing Based Architecture for Mobile Augmented Reality,"In order to mitigate the long processing delay and high energy consumption of
mobile augmented reality (AR) applications, mobile edge computing (MEC) has
been recently proposed and is envisioned as a promising means to deliver better
quality of experience (QoE) for AR consumers. In this article, we first present
a comprehensive AR overview, including the indispensable components of general
AR applications, fashionable AR devices, and several existing techniques for
overcoming the thorny latency and energy consumption problems. Then, we propose
a novel hierarchical computation architecture by inserting an edge layer
between the conventional user layer and cloud layer. Based on the proposed
architecture, we further develop an innovated operation mechanism to improve
the performance of mobile AR applications. Three key technologies are also
discussed to further assist the proposed AR architecture. Simulation results
are finally provided to verify that our proposals can significantly improve the
latency and energy performance as compared against existing baseline schemes.","['Jinke Ren', 'Yinghui He', 'Guan Huang', 'Guanding Yu', 'Yunlong Cai', 'Zhaoyang Zhang']",2018-10-05T03:53:10Z,http://arxiv.org/abs/1810.02509v2,"['cs.IT', 'math.IT']"
"Exploring Stereovision-Based 3-D Scene Reconstruction for Augmented
  Reality","Three-dimensional (3-D) scene reconstruction is one of the key techniques in
Augmented Reality (AR), which is related to the integration of image processing
and display systems of complex information. Stereo matching is a computer
vision based approach for 3-D scene reconstruction. In this paper, we explore
an improved stereo matching network, SLED-Net, in which a Single Long
Encoder-Decoder is proposed to replace the stacked hourglass network in PSM-Net
for better contextual information learning. We compare SLED-Net to
state-of-the-art methods recently published, and demonstrate its superior
performance on Scene Flow and KITTI2015 test sets.","['Guang-Yu Nie', 'Yun Liu', 'Cong Wang', 'Yue Liu', 'Yongtian Wang']",2019-02-17T13:09:16Z,http://arxiv.org/abs/1902.06255v1,['cs.CV']
"Development of Head-Mounted Projection Displays for Distributed,
  Collaborative, Augmented Reality Applications","Distributed systems technologies supporting 3D visualization and social
collaboration will be increasing in frequency and type over time. An emerging
type of head-mounted display referred to as the head-mounted projection display
(HMPD) was recently developed that only requires ultralight optics (i.e., less
than 8 g per eye) that enables immersive multiuser, mobile augmented reality 3D
visualization, as well as remote 3D collaborations. In this paper a review of
the development of lightweight HMPD technology is provided, together with
insight into what makes this technology timely and so unique. Two novel
emerging HMPD-based technologies are then described: a teleportal HMPD(T-HMPD)
enabling face-to-face communication and visualization of shared 3D virtual
objects, and a mobile HMPD (M-HMPD) designed for outdoor wearable visualization
and communication. Finally, the use of HMPD in medical visualization and
training, as well as in infospaces, two applications developed in the ODA and
MIND labs respectively, are discussed.","['Jannick P. Rolland', 'Frank Biocca', 'Felix G. Hamza-Lup', 'Yanggang Ha', 'Ricardo Martins']",2019-02-20T20:32:51Z,http://arxiv.org/abs/1902.07769v1,['cs.HC']
Instant Motion Tracking and Its Applications to Augmented Reality,"Augmented Reality (AR) brings immersive experiences to users. With recent
advances in computer vision and mobile computing, AR has scaled across
platforms, and has increased adoption in major products. One of the key
challenges in enabling AR features is proper anchoring of the virtual content
to the real world, a process referred to as tracking. In this paper, we present
a system for motion tracking, which is capable of robustly tracking planar
targets and performing relative-scale 6DoF tracking without calibration. Our
system runs in real-time on mobile phones and has been deployed in multiple
major products on hundreds of millions of devices.","['Jianing Wei', 'Genzhi Ye', 'Tyler Mullen', 'Matthias Grundmann', 'Adel Ahmadyan', 'Tingbo Hou']",2019-07-16T00:13:09Z,http://arxiv.org/abs/1907.06796v1,['cs.CV']
"EyeSec: A Retrofittable Augmented Reality Tool for Troubleshooting
  Wireless Sensor Networks in the Field","Wireless Sensor Networks (WSNs) often lack interfaces for remote debugging.
Thus, fault diagnosis and troubleshooting are conducted at the deployment site.
Currently, WSN operators lack dedicated tools that aid them in this process.
Therefore, we introduce EyeSec, a tool for WSN monitoring and maintenance in
the field. An Augmented Reality Device (AR Device) identifies sensor nodes
using optical markers. Portable Sniffer Units capture network traffic and
extract information. With those data, the AR Device network topology and data
flows between sensor nodes are visualized. Unlike previous tools, EyeSec is
fully portable, independent of any given infrastructure and does not require
dedicated and expensive AR hardware. Using passive inspection only, it can be
retrofitted to already deployed WSNs. We implemented a proof of concept on
low-cost embedded hardware and commodity smart phones and demonstrate the usage
of EyeSec within a WSN test bed using the 6LoWPAN transmission protocol.","['Martin Striegel', 'Carsten Rolfes', 'Johann Heyszl', 'Fabian Helfert', 'Maximilian Hornung', 'Georg Sigl']",2019-07-08T14:11:49Z,http://arxiv.org/abs/1907.12364v1,"['cs.NI', 'cs.CR', 'cs.HC', 'H.4.m; H.5.2; C.2.3; C.2.4']"
Semantic-Aware Label Placement for Augmented Reality in Street View,"In an augmented reality (AR) application, placing labels in a manner that is
clear and readable without occluding the critical information from the
real-world can be a challenging problem. This paper introduces a label
placement technique for AR used in street view scenarios. We propose a
semantic-aware task-specific label placement method by identifying potentially
important image regions through a novel feature map, which we refer to as
guidance map. Given an input image, its saliency information, semantic
information and the task-specific importance prior are integrated into the
guidance map for our labeling task. To learn the task prior, we created a label
placement dataset with the users' labeling preferences, as well as use it for
evaluation. Our solution encodes the constraints for placing labels in an
optimization problem to obtain the final label layout, and the labels will be
placed in appropriate positions to reduce the chances of overlaying important
real-world objects in street view AR scenarios. The experimental validation
shows clearly the benefits of our method over previous solutions in the AR
street view navigation and similar applications.","['Jianqing Jia', 'Semir Elezovikj', 'Heng Fan', 'Shuojin Yang', 'Jing Liu', 'Wei Guo', 'Chiu C. Tan', 'Haibin Ling']",2019-12-15T20:29:37Z,http://arxiv.org/abs/1912.07105v1,['cs.CV']
"Augmented-Reality-Based Visualization of Navigation Data of Mobile
  Robots on the Microsoft Hololens -- Possibilities and Limitations","The demand for mobile robots has rapidly increased in recent years due to the
flexibility and high variety of application fields comparing to static robots.
To deal with complex tasks such as navigation, they work with high amounts of
different sensor data making it difficult to operate with for non-experts. To
enhance user understanding and human robot interaction, we propose an approach
to visualize the navigation stack within a cutting edge 3D Augmented Reality
device -- the Microsoft Hololens. Therefore, relevant navigation stack data
including laser scan, environment map and path planing data are visualized in
3D within the head mounted device. Based on that prototype, we evaluate the
Hololens in terms of computational capabilities and limitations for dealing
with huge amount of real-time data. Results show that the Hololens is capable
of a proper visualization of huge amounts of sensor data. We demonstrate a
proper visualization of navigation stack data in 3D within the Hololens.
However, there are limitations when transferring and displaying different kinds
of data simultaneously.","['Linh Kästner', 'Jens Lambrecht']",2019-12-27T14:13:15Z,http://arxiv.org/abs/1912.12109v1,"['cs.RO', 'cs.SY', 'eess.SY']"
"Development of Interactive Instructional Model Using Augmented Reality
  based on Edutainment to Enhance Emotional Quotient","The research aims to develop an interactive instructional model using
augmented reality based on edutainment to enhance emotional quotient and
evaluate the model. Two phases of the research will be carried out: a
development and an evaluation of the model. Samples are experts in the field of
IT, child psychology, and 7th grade curriculum management. Ten experts are
selected by purposive sampling method. The obtained data are analyzed using
mean and standard deviation. The research result demonstrates the following
findings: 1) The results of this research show that Model consists of 3
elements: IIAR, EduLA, and EQ. EQ is a means to assess EQ based on Time Series
Experimental Design using 2 kinds of tools; i.e. EQ Assessment by programs in
tablet computers, and EQ Assessment by behavioral observation. 2) The ten
experts have evaluated the model and commented that the developed model showed
high suitability.","['Nuttakan Pakprod', 'Panita Wannapiroon']",2014-02-17T09:46:16Z,http://arxiv.org/abs/1402.3942v1,['cs.CY']
"An Improved Tracking using IMU and Vision Fusion for Mobile Augmented
  Reality Applications","Mobile Augmented Reality (MAR) is becoming an important cyber-physical system
application given the ubiquitous availability of mobile phones. With the need
to operate in unprepared environments, accurate and robust registration and
tracking has become an important research problem to solve. In fact, when MAR
is used for tele-interactive applications involving large distances, say from
an accident site to insurance office, tracking at both the ends is desirable
and further it is essential to appropriately fuse inertial and vision sensors
data. In this paper, we present results and discuss some insights gained in
marker-less tracking during the development of a prototype pertaining to an
example use case related to breakdown or damage assessment of a vehicle. The
novelty of this paper is in bringing together different components and modules
with appropriate enhancements towards a complete working system.","['Kriti Kumar', 'Ashley Varghese', 'Pavan K Reddy', 'N Narendra', 'Prashanth Swamy', 'M Girish Chandra', 'P Balamuralidhar']",2014-11-10T06:15:42Z,http://arxiv.org/abs/1411.2335v1,['cs.CV']
Adaptive User Perspective Rendering for Handheld Augmented Reality,"Handheld Augmented Reality commonly implements some variant of magic lens
rendering, which turns only a fraction of the user's real environment into AR
while the rest of the environment remains unaffected. Since handheld AR devices
are commonly equipped with video see-through capabilities, AR magic lens
applications often suffer from spatial distortions, because the AR environment
is presented from the perspective of the camera of the mobile device. Recent
approaches counteract this distortion based on estimations of the user's head
position, rendering the scene from the user's perspective. To this end,
approaches usually apply face-tracking algorithms on the front camera of the
mobile device. However, this demands high computational resources and therefore
commonly affects the performance of the application beyond the already high
computational load of AR applications. In this paper, we present a method to
reduce the computational demands for user perspective rendering by applying
lightweight optical flow tracking and an estimation of the user's motion before
head tracking is started. We demonstrate the suitability of our approach for
computationally limited mobile devices and we compare it to device perspective
rendering, to head tracked user perspective rendering, as well as to fixed
point of view user perspective rendering.","['Peter Mohr', 'Markus Tatzgern', 'Jens Grubert', 'Dieter Schmalstieg', 'Denis Kalkofen']",2017-03-22T21:56:58Z,http://arxiv.org/abs/1703.07869v1,"['cs.HC', 'cs.GR']"
"Google Cardboard Dates Augmented Reality : Issues, Challenges and Future
  Opportunities","The Google's frugal Cardboard solution for immersive Virtual Reality
experiences has come a long way in the VR market. The Google Cardboard VR
applications will support us in the fields such as education, virtual tourism,
entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has
introduced support for developing mixed reality applications for Google
Cardboard which can combine Virtual and Augmented Reality to develop exciting
and immersive experiences. In this work, we present a comprehensive review of
Google Cardboard for AR and also highlight its technical and subjective
limitations by conducting a feasibility study through the inspection of a
Desktop computer use-case. Additionally, we recommend the future avenues for
the Google Cardboard in AR. This work also serves as a guide for Android/iOS
developers as there are no published scholarly articles or well documented
studies exclusively on Google Cardboard with both user and developer's
experience captured at one place.","['Ramakrishna Perla', 'Ramya Hebbalaguppe']",2017-06-05T06:26:25Z,http://arxiv.org/abs/1706.03851v1,['cs.HC']
Interaction Methods for Smart Glasses,"Since the launch of Google Glass in 2014, smart glasses have mainly been
designed to support micro-interactions. The ultimate goal for them to become an
augmented reality interface has not yet been attained due to an encumbrance of
controls. Augmented reality involves superimposing interactive computer
graphics images onto physical objects in the real world. This survey reviews
current research issues in the area of human computer interaction for smart
glasses. The survey first studies the smart glasses available in the market and
afterwards investigates the interaction methods proposed in the wide body of
literature. The interaction methods can be classified into hand-held, touch,
and touchless input. This paper mainly focuses on the touch and touchless
input. Touch input can be further divided into on-device and on-body, while
touchless input can be classified into hands-free and freehand. Next, we
summarize the existing research efforts and trends, in which touch and
touchless input are evaluated by a total of eight interaction goals. Finally,
we discuss several key design challenges and the possibility of multi-modal
input for smart glasses.","['Lik-Hang Lee', 'Pan Hui']",2017-07-31T05:49:03Z,http://arxiv.org/abs/1707.09728v1,['cs.HC']
"A Comparison of Visualisation Methods for Disambiguating Verbal Requests
  in Human-Robot Interaction","Picking up objects requested by a human user is a common task in human-robot
interaction. When multiple objects match the user's verbal description, the
robot needs to clarify which object the user is referring to before executing
the action. Previous research has focused on perceiving user's multimodal
behaviour to complement verbal commands or minimising the number of follow up
questions to reduce task time. In this paper, we propose a system for reference
disambiguation based on visualisation and compare three methods to disambiguate
natural language instructions. In a controlled experiment with a YuMi robot, we
investigated real-time augmentations of the workspace in three conditions --
mixed reality, augmented reality, and a monitor as the baseline -- using
objective measures such as time and accuracy, and subjective measures like
engagement, immersion, and display interference. Significant differences were
found in accuracy and engagement between the conditions, but no differences
were found in task time. Despite the higher error rates in the mixed reality
condition, participants found that modality more engaging than the other two,
but overall showed preference for the augmented reality condition over the
monitor and mixed reality conditions.","['Elena Sibirtseva', 'Dimosthenis Kontogiorgos', 'Olov Nykvist', 'Hakan Karaoguz', 'Iolanda Leite', 'Joakim Gustafson', 'Danica Kragic']",2018-01-26T11:24:47Z,http://arxiv.org/abs/1801.08760v1,"['cs.RO', 'cs.HC']"
"The Helping Hand: An Assistive Manipulation Framework Using Augmented
  Reality and a Tongue-Drive Interfaces","A human-in-the-loop system is proposed to enable collaborative manipulation
tasks for person with physical disabilities. Studies show that the cognitive
burden of subject reduces with increased autonomy of assistive system. Our
framework obtains high-level intent from the user to specify manipulation
tasks. The system processes sensor input to interpret the user's environment.
Augmented reality glasses provide ego-centric visual feedback of the
interpretation and summarize robot affordances on a menu. A tongue drive system
serves as the input modality for triggering a robotic arm to execute the tasks.
Assistance experiments compare the system to Cartesian control and to
state-of-the-art approaches. Our system achieves competitive results with
faster completion time by simplifying manipulation tasks.","['Fu-Jen Chu', 'Ruinian Xu', 'Zhenxuan Zhang', 'Patricio A. Vela', 'Maysam Ghovanloo']",2018-02-01T19:24:22Z,http://arxiv.org/abs/1802.00463v2,['cs.RO']
A Review of Augmented Reality Applications for Building Evacuation,"Evacuation is one of the main disaster management solutions to reduce the
impact of man-made and natural threats on building occupants. To date, several
modern technologies and gamification concepts, e.g. immersive virtual reality
and serious games, have been used to enhance building evacuation preparedness
and effectiveness. Those tools have been used both to investigate human
behavior during building emergencies and to train building occupants on how to
cope with building evacuations.
  Augmented Reality (AR) is novel technology that can enhance this process
providing building occupants with virtual contents to improve their evacuation
performance. This work aims at reviewing existing AR applications developed for
building evacuation. This review identifies the disasters and types of building
those tools have been applied for. Moreover, the application goals, hardware
and evacuation stages affected by AR are also investigated in the review.
Finally, this review aims at identifying the challenges to face for further
development of AR evacuation tools.",['Lovreglio Ruggiero'],2018-04-10T07:50:14Z,http://arxiv.org/abs/1804.04186v1,"['cs.OH', 'cs.CY']"
"MaskFusion: Real-Time Recognition, Tracking and Reconstruction of
  Multiple Moving Objects","We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D
SLAM system that goes beyond traditional systems which output a purely
geometric map of a static scene. MaskFusion recognizes, segments and assigns
semantic class labels to different objects in the scene, while tracking and
reconstructing them even when they move independently from the camera.
  As an RGB-D camera scans a cluttered scene, image-based instance-level
semantic segmentation creates semantic object masks that enable real-time
object recognition and the creation of an object-level representation for the
world map. Unlike previous recognition-based SLAM systems, MaskFusion does not
require known models of the objects it can recognize, and can deal with
multiple independent motions. MaskFusion takes full advantage of using
instance-level semantic segmentation to enable semantic labels to be fused into
an object-aware map, unlike recent semantics enabled SLAM systems that perform
voxel-level semantic segmentation. We show augmented-reality applications that
demonstrate the unique features of the map output by MaskFusion:
instance-aware, semantic and dynamic.","['Martin Rünz', 'Maud Buffier', 'Lourdes Agapito']",2018-04-24T18:15:15Z,http://arxiv.org/abs/1804.09194v2,"['cs.CV', 'cs.RO']"
"Occluded object reconstruction for first responders with augmented
  reality glasses using conditional generative adversarial networks","Firefighters suffer a variety of life-threatening risks, including
line-of-duty deaths, injuries, and exposures to hazardous substances. Support
for reducing these risks is important. We built a partially occluded object
reconstruction method on augmented reality glasses for first responders. We
used a deep learning based on conditional generative adversarial networks to
train associations between the various images of flammable and hazardous
objects and their partially occluded counterparts. Our system then
reconstructed an image of a new flammable object. Finally, the reconstructed
image was superimposed on the input image to provide ""transparency"". The system
imitates human learning about the laws of physics through experience by
learning the shape of flammable objects and the flame characteristics.","['Kyongsik Yun', 'Thomas Lu', 'Edward Chow']",2018-04-20T23:56:10Z,http://arxiv.org/abs/1805.00322v1,"['cs.CV', 'cs.LG']"
CloudAR: A Cloud-based Framework for Mobile Augmented Reality,"Computation capabilities of recent mobile devices enable natural feature
processing for Augmented Reality (AR). However, mobile AR applications are
still faced with scalability and performance challenges. In this paper, we
propose CloudAR, a mobile AR framework utilizing the advantages of cloud and
edge computing through recognition task offloading. We explore the design space
of cloud-based AR exhaustively and optimize the offloading pipeline to minimize
the time and energy consumption. We design an innovative tracking system for
mobile devices which provides lightweight tracking in 6 degree of freedom
(6DoF) and hides the offloading latency from users' perception. We also design
a multi-object image retrieval pipeline that executes fast and accurate image
recognition tasks on servers. In our evaluations, the mobile AR application
built with the CloudAR framework runs at 30 frames per second (FPS) on average
with precise tracking of only 1~2 pixel errors and image recognition of at
least 97% accuracy. Our results also show that CloudAR outperforms one of the
leading commercial AR framework in several performance metrics.","['Wenxiao Zhang', 'Sikun Lin', 'Farshid Hassani Bijarbooneh', 'Hao Fei Cheng', 'And Pan Hui']",2018-05-08T14:38:58Z,http://arxiv.org/abs/1805.03060v1,['cs.MM']
Sensorless Hand Guidance using Microsoft Hololens,"Hand guidance of robots has proven to be a useful tool both for programming
trajectories and in kinesthetic teaching. However hand guidance is usually
relegated to robots possessing joint-torque sensors (JTS). Here we propose to
extend hand guidance to robots lacking those sensors through the use of an
Augmented Reality (AR) device, namely Microsoft's Hololens. Augmented reality
devices have been envisioned as a helpful addition to ease both robot
programming and increase situational awareness of humans working in close
proximity to robots. We reference the robot by using a registration algorithm
to match a robot model to the spatial mesh. The in-built hand tracking
capabilities are then used to calculate the position of the hands relative to
the robot. By decomposing the hand movements into orthogonal rotations we
achieve a completely sensorless hand guidance without any need to build a
dynamic model of the robot itself. We did the first tests our approach on a
commonly used industrial manipulator, the KUKA KR-5.","['David Puljiz', 'Erik Stöhr', 'Katharina S. Riesterer', 'Björn Hein', 'Torsten Kröger']",2019-01-15T16:55:43Z,http://arxiv.org/abs/1901.04933v1,['cs.RO']
Metasurfaces for near-eye augmented reality,"Augmented reality (AR) has the potential to revolutionize the way in which
information is presented by overlaying virtual information onto a person's
direct view of their real-time surroundings. By placing the display on the
surface of the eye, a contact lens display (CLD) provides a versatile solution
for compact AR. However, an unaided human eye cannot visualize patterns on the
CLD simply because of the limited accommodation of the eye. Here, we introduce
a holographic display technology that casts virtual information directly to the
retina so that the eye sees it while maintaining the visualization of the
real-world intact. The key to our design is to introduce metasurfaces to create
a phase distribution that projects virtual information in a pixel-by-pixel
manner. Unlike conventional holographic techniques, our metasurface-based
technique is able to display arbitrary patterns using a single passive
hologram. With a small form-factor, the designed metasurface empowers near-eye
AR excluding the need of extra optical elements, such as a spatial light
modulator, for dynamic image control.","['Shoufeng Lan', 'Xueyue Zhang', 'Mohammad Taghinejad', 'Sean Rodrigues', 'Kyu-Tae Lee', 'Zhaocheng Liu', 'Wenshan Cai']",2019-01-18T20:02:35Z,http://arxiv.org/abs/1901.06408v1,"['cs.GR', 'physics.optics']"
"MVC-3D: Adaptive Design Pattern for Virtual and Augmented Reality
  Systems","In this paper, we present MVC-3D design pattern to develop virtual and
augmented (or mixed) reality interfaces that use new types of sensors,
modalities and implement specific algorithms and simulation models. The
proposed pattern represents the extension of classic MVC pattern by enriching
the View component (interactive View) and adding a specific component
(Library). The results obtained on the development of augmented reality
interfaces showed that the complexity of M, iV and C components is reduced. The
complexity increases only on the Library component (L). This helps the
programmers to well structure their models even if the interface complexity
increases. The proposed design pattern is also used in a design process called
MVC-3D in the loop that enables a seamless evolution from initial prototype to
the final system.","['Samir Benbelkacem', 'Djamel Aouam', 'Nadia Zenati-Henda', 'Abdelkader Bellarbi', 'Ahmed Bouhena', 'Samir Otmane']",2019-03-01T07:38:23Z,http://arxiv.org/abs/1903.00185v1,"['cs.HC', 'cs.SE']"
"Would Gaze-Contingent Rendering Improve Depth Perception in Virtual and
  Augmented Reality?","Near distances are overestimated in virtual reality, and far distances are
underestimated, but an explanation for these distortions remains elusive. One
potential concern is that whilst the eye rotates to look at the virtual scene,
the virtual cameras remain static. Could using eye-tracking to change the
perspective of the virtual cameras as the eye rotates improve depth perception
in virtual reality? This paper identifies 14 distinct perspective distortions
that could in theory occur from keeping the virtual cameras fixed whilst the
eye rotates in the context of near-eye displays. However, the impact of eye
movements on the displayed image depends on the optical, rather than physical,
distance of the display. Since the optical distance of most head-mounted
displays is over 1m, most of these distortions will have only a negligible
effect. The exception are 'gaze-contingent disparities', which will leave near
virtual objects looking displaced from physical objects that are meant to be at
the same distance in augmented reality.",['Paul Linton'],2019-05-24T11:47:08Z,http://arxiv.org/abs/1905.10366v1,['cs.HC']
A Projection-based Augmented Reality for Elderly People with Dementia,"As aging societies grow, researchers are actively studying care systems
concerning the life and diseases of the elderly. Among these diseases, dementia
makes it difficult to maintain daily life due to the degradation of cognitive
functioning, memory, and reasoning, as well as the ability to perform actions.
Moreover, dementia does not have a perfect cure, though therapy and care can
slow its onset and provide patients with physical and mental support. In this
paper, we developed a projection-based augmented reality system robot that can
cover 360 degrees of space. We also propose an application that supports
continuous monitoring of dementia patients to address the difficulties they
face in daily life. The system is also designed to provide therapy
applications, such as entertainment and spatial art, to provide mental care
aids for the patients.","['Hyocheol Ro', 'Yoon Jung Park', 'Tack-Don Han']",2019-08-16T16:27:51Z,http://arxiv.org/abs/1908.06046v1,['cs.HC']
"Enabling Intuitive Human-Robot Teaming Using Augmented Reality and
  Gesture Control","Human-robot teaming offers great potential because of the opportunities to
combine strengths of heterogeneous agents. However, one of the critical
challenges in realizing an effective human-robot team is efficient information
exchange - both from the human to the robot as well as from the robot to the
human. In this work, we present and analyze an augmented reality-enabled,
gesture-based system that supports intuitive human-robot teaming through
improved information exchange. Our proposed system requires no external
instrumentation aside from human-wearable devices and shows promise of
real-world applicability for service-oriented missions. Additionally, we
present preliminary results from a pilot study with human participants, and
highlight lessons learned and open research questions that may help direct
future development, fielding, and experimentation of autonomous HRI systems.","['Jason M. Gregory', 'Christopher Reardon', 'Kevin Lee', 'Geoffrey White', 'Ki Ng', 'Caitlyn Sims']",2019-09-13T19:18:52Z,http://arxiv.org/abs/1909.06415v1,"['cs.RO', 'cs.AI', 'cs.HC']"
"Multi-user Augmented Reality Application for Video Communication in
  Virtual Space","Communication is the most useful tool to impart knowledge, understand ideas,
clarify thoughts and expressions, organize plan and manage every single
day-to-day activity. Although there are different modes of communication,
physical barrier always affects the clarity of the message due to the absence
of body language and facial expressions. These barriers are overcome by video
calling, which is technically the most advance mode of communication at
present. The proposed work concentrates around the concept of video calling in
a more natural and seamless way using Augmented Reality (AR). AR can be helpful
in giving the users an experience of physical presence in each other's
environment. Our work provides an entirely new platform for video calling,
wherein the users can enjoy the privilege of their own virtual space to
interact with the individual's environment. Moreover, there is no limitation of
sharing the same screen space. Any number of participants can be accommodated
over a single conference without having to compromise the screen size.","['Kumar Mridul', 'M. Ramanathan', 'Kunal Ahirwar', 'Mansi Sharma']",2019-09-20T14:32:54Z,http://arxiv.org/abs/1909.09529v1,"['cs.CV', 'cs.MM']"
Negotiation-based Human-Robot Collaboration via Augmented Reality,"Effective human-robot collaboration (HRC) requires extensive communication
among the human and robot teammates, because their actions can potentially
produce conflicts, synergies, or both. We develop a novel augmented reality
(AR) interface to bridge the communication gap between human and robot
teammates. Building on our AR interface, we develop an AR-mediated,
negotiation-based (ARN) framework for HRC. We have conducted experiments both
in simulation and on real robots in an office environment, where multiple
mobile robots work on delivery tasks. The robots could not complete the tasks
on their own, but sometimes need help from their human teammate, rendering
human-robot collaboration necessary. Results suggest that ARN significantly
reduced the human-robot team's task completion time compared to a non-AR
baseline approach.","['Kishan Chandan', 'Vidisha Kudalkar', 'Xiang Li', 'Shiqi Zhang']",2019-09-24T23:34:36Z,http://arxiv.org/abs/1909.11227v3,"['cs.RO', 'cs.MA']"
"Concepts for End-to-end Augmented Reality based Human-Robot Interaction
  Systems","The field of Augmented Reality (AR) based Human Robot Interaction (HRI) has
progressed significantly since its inception more than two decades ago. With
more advanced devices, particularly head-mounted displays (HMD), freely
available programming environments and better connectivity, the possible
application space expanded significantly. Here we present concepts and systems
currently being developed at our lab to enable a truly end-to-end application
of AR in HRI, from setting up the working environment of the robot, through
programming and finally interaction with the programmed robot. Relevant papers
by other authors will also be overviewed. We demonstrate the use of such
technologies with systems not inherently designed to be collaborative, namely
industrial manipulators. By trying to make such industrial systems
easily-installable, collaborative and interactive, the vision of universal
robot co-workers can be pushed one step closer to reality. The main goal of the
paper is to provide a short overview of the capabilities of HMD-based HRI to
researchers unfamiliar with the concepts. For researchers already using such
techniques, the hope is to perhaps introduce some new ideas and to broaden the
field of research.","['David Puljiz', 'Björn Hein']",2019-10-10T11:47:10Z,http://arxiv.org/abs/1910.04494v1,['cs.RO']
"Immersive Insights: A Hybrid Analytics System for Collaborative
  Exploratory Data Analysis","In the past few years, augmented reality (AR) and virtual reality (VR)
technologies have experienced terrific improvements in both accessibility and
hardware capabilities, encouraging the application of these devices across
various domains. While researchers have demonstrated the possible advantages of
AR and VR for certain data science tasks, it is still unclear how these
technologies would perform in the context of exploratory data analysis (EDA) at
large. In particular, we believe it is important to better understand which
level of immersion EDA would concretely benefit from, and to quantify the
contribution of AR and VR with respect to standard analysis workflows.
  In this work, we leverage a Dataspace reconfigurable hybrid reality
environment to study how data scientists might perform EDA in a co-located,
collaborative context. Specifically, we propose the design and implementation
of Immersive Insights, a hybrid analytics system combining high-resolution
displays, table projections, and augmented reality (AR) visualizations of the
data.
  We conducted a two-part user study with twelve data scientists, in which we
evaluated how different levels of data immersion affect the EDA process and
compared the performance of Immersive Insights with a state-of-the-art,
non-immersive data analysis system.","['Marco Cavallo', 'Mishal Dholakia', 'Matous Havlena', 'Kenneth Ocheltree', 'Mark Podlaseck']",2019-10-27T06:44:30Z,http://arxiv.org/abs/1910.12193v1,['cs.HC']
3D Augmented Reality Tangible User Interface using Commodity Hardware,"During the last years, the emerging field of Augmented and Virtual Reality
(AR-VR) has seen tremendous growth. An interface that has also become very
popular for the AR systems is the tangible interface or passive-haptic
interface. Specifically, an interface where users can manipulate digital
information with input devices that are physical objects. This work presents a
low cost Augmented Reality system with a tangible interface that offers
interaction between the real and the virtual world. The system estimates in
real-time the 3D position of a small colored ball (input device), it maps it to
the 3D virtual world and then uses it to control the AR application that runs
in a mobile device. Using the 3D position of our ""input"" device, it allows us
to implement more complicated interactivity compared to a 2D input device.
Finally, we present a simple, fast and robust algorithm that can estimate the
corners of a convex quadrangle. The proposed algorithm is suitable for the fast
registration of markers and significantly improves performance compared to the
state of the art.","['Dimitrios Chamzas', 'Konstantinos Moustakas']",2020-03-02T18:29:58Z,http://arxiv.org/abs/2003.01092v1,['cs.HC']
"Augmented Reality on the Large Scene Based on a Markerless Registration
  Framework","In this paper, a mobile camera positioning method based on forward and
inverse kinematics of robot is proposed, which can realize far point
positioning of imaging position and attitude tracking in large scene
enhancement. Orbit precision motion through the framework overhead cameras and
combining with the ground system of sensor array object such as mobile robot
platform of various sensors, realize the good 3 d image registration, solve any
artifacts that is mobile robot in the large space position initialization
problem, effectively implement the large space no marks augmented reality,
human-computer interaction, and information summary. Finally, the feasibility
and effectiveness of the method are verified by experiments.","['Zhen Ma', 'He Xu', 'Yonghui Zhang', 'Junlong Chen', 'Dongbo Zhao', 'Siqing Chen']",2020-03-03T00:01:05Z,http://arxiv.org/abs/2003.01256v2,['cs.RO']
PointAR: Efficient Lighting Estimation for Mobile Augmented Reality,"We propose an efficient lighting estimation pipeline that is suitable to run
on modern mobile devices, with comparable resource complexities to
state-of-the-art mobile deep learning models. Our pipeline, PointAR, takes a
single RGB-D image captured from the mobile camera and a 2D location in that
image, and estimates 2nd order spherical harmonics coefficients. This estimated
spherical harmonics coefficients can be directly utilized by rendering engines
for supporting spatially variant indoor lighting, in the context of augmented
reality. Our key insight is to formulate the lighting estimation as a point
cloud-based learning problem directly from point clouds, which is in part
inspired by the Monte Carlo integration leveraged by real-time spherical
harmonics lighting. While existing approaches estimate lighting information
with complex deep learning pipelines, our method focuses on reducing the
computational complexity. Through both quantitative and qualitative
experiments, we demonstrate that PointAR achieves lower lighting estimation
errors compared to state-of-the-art methods. Further, our method requires an
order of magnitude lower resource, comparable to that of mobile-specific DNNs.","['Yiqin Zhao', 'Tian Guo']",2020-03-30T19:13:26Z,http://arxiv.org/abs/2004.00006v4,"['cs.CV', 'eess.IV']"
Congestion-aware Evacuation Routing using Augmented Reality Devices,"We present a congestion-aware routing solution for indoor evacuation, which
produces real-time individual-customized evacuation routes among multiple
destinations while keeping tracks of all evacuees' locations. A population
density map, obtained on-the-fly by aggregating locations of evacuees from
user-end Augmented Reality (AR) devices, is used to model the congestion
distribution inside a building. To efficiently search the evacuation route
among all destinations, a variant of A* algorithm is devised to obtain the
optimal solution in a single pass. In a series of simulated studies, we show
that the proposed algorithm is more computationally optimized compared to
classic path planning algorithms; it generates a more time-efficient evacuation
route for each individual that minimizes the overall congestion. A complete
system using AR devices is implemented for a pilot study in real-world
environments, demonstrating the efficacy of the proposed approach.","['Zeyu Zhang', 'Hangxin Liu', 'Ziyuan Jiao', 'Yixin Zhu', 'Song-Chun Zhu']",2020-04-25T22:54:35Z,http://arxiv.org/abs/2004.12246v1,"['cs.HC', 'cs.CV']"
"A Survey Study to Understand Industry Vision for Virtual and Augmented
  Reality Applications in Design and Construction","With advances in Building Information Modeling (BIM), Virtual Reality (VR)
and Augmented Reality (AR) technologies have many potential applications in the
Architecture, Engineering, and Construction (AEC) industry. However, the AEC
industry, relative to other industries, has been slow in adopting AR/VR
technologies, partly due to lack of feasibility studies examining the actual
cost of implementation versus an increase in profit. The main objectives of
this paper are to understand the industry trends in adopting AR/VR technologies
and identifying gaps between AEC research and industry practices. The
identified gaps can lead to opportunities for developing new tools and finding
new use cases. To achieve these goals, two rounds of a survey at two different
time periods (a year apart) were conducted. Responses from 158 industry experts
and researchers were analyzed to assess the current state, growth, and saving
opportunities for AR/VR technologies for the AEC industry. The authors used
t-test for hypothesis testing. The findings show a significant increase in
AR/VR utilization in the AEC industry over the past year from 2017 to 2018. The
industry experts also anticipate strong growth in the use of AR/VR technologies
over the next 5 to 10 years.","['Mojtaba Noghabaei', 'Arsalan Heydarian', 'Vahid Balali', 'Kevin Han']",2020-05-06T13:16:05Z,http://arxiv.org/abs/2005.02795v1,['cs.CY']
"Deep Residual Network based food recognition for enhanced Augmented
  Reality application","Deep neural network based learning approaches is widely utilized for image
classification or object detection based problems with remarkable outcomes.
Realtime Object state estimation of objects can be used to track and estimate
the features that the object of the current frame possesses without causing any
significant delay and misclassification. A system that can detect the features
of such objects in the present state from camera images can be used to enhance
the application of Augmented Reality for improving user experience and
delivering information in a much perceptual way. The focus behind this paper is
to determine the most suitable model to create a low-latency assistance AR to
aid users by providing them nutritional information about the food that they
consume in order to promote healthier life choices. Hence the dataset has been
collected and acquired in such a manner, and we conduct various tests in order
to identify the most suitable DNN in terms of performance and complexity and
establish a system that renders such information realtime to the user.","['Siddarth S', 'Sainath G', 'Vignesh S']",2020-05-08T21:08:58Z,http://arxiv.org/abs/2005.04292v2,"['cs.HC', 'cs.CV']"
"A Novel Approach of using AR and Smart Surgical Glasses Supported Trauma
  Care","BACKGROUND: Augmented reality (AR) is gaining popularity in varying field
such as computer gaming and medical education fields. However, still few of
applications in real surgeries. Orthopedic surgical applications are currently
limited and underdeveloped. - METHODS: The clinic validation was prepared with
the currently available AR equipment and software. A total of 1 Vertebroplasty,
2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal Femoral Fracture, 1 CRIF
for distal radius fracture and 2 ORIF for Tibia Fracture cases were performed
with fluoroscopy combined with AR smart surgical glasses system. - RESULTS: A
total of 1 Vertebroplasty, 2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal
Femoral Fracture, 1 CRIF for distal radius fracture and 2 ORIF for Tibia
Fracture cases are performed to evaluate the benefits of AR surgery. Among the
AR surgeries, surgeons wear the smart surgical are lot reduce of eyes of turns
to focus on the monitors. This paper shows the potential ability of augmented
reality technology for trauma surgery.","['Anurag Lal', 'Ming-Hsien Hu', 'Pei-Yuan Lee', 'Min Liang Wang']",2020-05-25T06:03:30Z,http://arxiv.org/abs/2005.11935v1,"['q-bio.QM', 'cs.HC']"
"What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of
  Robot Cells via Head Mounted Displays and Augmented Reality","Classical methods of modelling and mapping robot work cells are time
consuming, expensive and involve expert knowledge. We present a novel approach
to mapping and cell setup using modern Head Mounted Displays (HMDs) that
possess self-localisation and mapping capabilities. We leveraged these
capabilities to create a point cloud of the environment and build an OctoMap -
a voxel occupancy grid representation of the robot's workspace for path
planning. Through the use of Augmented Reality (AR) interactions, the user can
edit the created Octomap and add security zones. We perform comprehensive tests
of the HoloLens' depth sensing capabilities and the quality of the resultant
point cloud. A high-end laser scanner is used to provide the ground truth for
the evaluation of the point cloud quality. The amount of false-positive and
false-negative voxels in the OctoMap are also tested.","['David Puljiz', 'Franziska Krebs', 'Fabian Bösing', 'Björn Hein']",2020-05-26T12:13:03Z,http://arxiv.org/abs/2005.12651v1,['cs.RO']
Instant 3D Object Tracking with Applications in Augmented Reality,"Tracking object poses in 3D is a crucial building block for Augmented Reality
applications. We propose an instant motion tracking system that tracks an
object's pose in space (represented by its 3D bounding box) in real-time on
mobile devices. Our system does not require any prior sensory calibration or
initialization to function. We employ a deep neural network to detect objects
and estimate their initial 3D pose. Then the estimated pose is tracked using a
robust planar tracker. Our tracker is capable of performing relative-scale
9-DoF tracking in real-time on mobile devices. By combining use of CPU and GPU
efficiently, we achieve 26-FPS+ performance on mobile devices.","['Adel Ahmadyan', 'Tingbo Hou', 'Jianing Wei', 'Liangkai Zhang', 'Artsiom Ablavatski', 'Matthias Grundmann']",2020-06-23T17:48:29Z,http://arxiv.org/abs/2006.13194v1,['cs.CV']
Human-Robot Interaction in a Shared Augmented Reality Workspace,"We design and develop a new shared Augmented Reality (AR) workspace for
Human-Robot Interaction (HRI), which establishes a bi-directional communication
between human agents and robots. In a prototype system, the shared AR workspace
enables a shared perception, so that a physical robot not only perceives the
virtual elements in its own view but also infers the utility of the human
agent--the cost needed to perceive and interact in AR--by sensing the human
agent's gaze and pose. Such a new HRI design also affords a shared
manipulation, wherein the physical robot can control and alter virtual objects
in AR as an active agent; crucially, a robot can proactively interact with
human agents, instead of purely passively executing received commands. In
experiments, we design a resource collection game that qualitatively
demonstrates how a robot perceives, processes, and manipulates in AR and
quantitatively evaluates the efficacy of HRI using the shared AR workspace. We
further discuss how the system can potentially benefit future HRI studies that
are otherwise challenging.","['Shuwen Qiu', 'Hangxin Liu', 'Zeyu Zhang', 'Yixin Zhu', 'Song-Chun Zhu']",2020-07-24T17:18:30Z,http://arxiv.org/abs/2007.12656v2,"['cs.RO', 'cs.HC']"
"Adapting Nielsen's Usability Heuristics to the Context of Mobile
  Augmented Reality","Augmented reality (AR) is an emerging technology in mobile app design during
recent years. However, usability challenges in these apps are prominent. There
are currently no established guidelines for designing and evaluating
interactions in AR as there are in traditional user interfaces. In this work,
we aimed to examine the usability of current mobile AR applications and
interpreting classic usability heuristics in the context of mobile AR.
Particularly, we focused on AR home design apps because of their popularity and
ability to incorporate important mobile AR interaction schemas. Our findings
indicated that it is important for the designers to consider the unfamiliarity
of AR technology to the vast users and to take technological limitations into
consideration when designing mobile AR apps. Our work serves as a first step
for establishing more general heuristics and guidelines for mobile AR.","['Audrey Labrie', 'Jinghui Cheng']",2020-08-07T13:32:18Z,http://arxiv.org/abs/2008.03174v1,['cs.HC']
"Identifying Usability Issues of Software Analytics Applications in
  Immersive Augmented Reality","Software analytics in augmented reality (AR) is said to have great potential.
One reason why this potential is not yet fully exploited may be usability
problems of the AR user interfaces. We present an iterative and qualitative
usability evaluation with 15 subjects of a state-of-the-art application for
software analytics in AR. We could identify and resolve numerous usability
issues. Most of them were caused by applying conventional user interface
elements, such as dialog windows, buttons, and scrollbars. The used city
visualization, however, did not cause any usability issues. Therefore, we argue
that future work should focus on making conventional user interface elements in
AR obsolete by integrating their functionality into the immersive
visualization.","['David Baum', 'Stefan Bechert', 'Ulrich Eisenecker', 'Isabelle Meichsner', 'Richard Müller']",2020-08-13T20:15:28Z,http://arxiv.org/abs/2008.06099v1,['cs.HC']
iviz: A ROS Visualization App for Mobile Devices,"In this work, we introduce iviz, a mobile application for visualizing ROS
data. In the last few years, the popularity of ROS has grown enormously, making
it the standard platform for open source robotic programming. A key reason for
this success is the availability of polished, general-purpose modules for many
tasks, such as localization, mapping, path planning, and quite importantly,
data visualization. However, the availability of the latter is generally
restricted to PCs with the Linux operating system. Thus, users that want to see
what is happening in the system with a smartphone or a tablet are stuck with
solutions such as screen mirroring or using web browser versions of rviz, which
are difficult to interact with from a mobile interface. More importantly, this
makes newer visualization modalities such as Augmented Reality impossible. Our
application iviz, based on the Unity engine, addresses these issues by
providing a visualization platform designed from scratch to be usable in mobile
platforms, such as iOS, Android, and UWP, and including native support for
Augmented Reality for all three platforms. If desired, it can also be used in a
PC with Linux, Windows, or macOS without any changes.","['Antonio Zea', 'Uwe D. Hanebeck']",2020-08-28T16:24:04Z,http://arxiv.org/abs/2008.12725v1,['cs.RO']
"Augmented Reality-Based Advanced Driver-Assistance System for Connected
  Vehicles","With the development of advanced communication technology, connected vehicles
become increasingly popular in our transportation systems, which can conduct
cooperative maneuvers with each other as well as road entities through
vehicle-to-everything communication. A lot of research interests have been
drawn to other building blocks of a connected vehicle system, such as
communication, planning, and control. However, less research studies were
focused on the human-machine cooperation and interface, namely how to visualize
the guidance information to the driver as an advanced driver-assistance system
(ADAS). In this study, we propose an augmented reality (AR)-based ADAS, which
visualizes the guidance information calculated cooperatively by multiple
connected vehicles. An unsignalized intersection scenario is adopted as the use
case of this system, where the driver can drive the connected vehicle crossing
the intersection under the AR guidance, without any full stop at the
intersection. A simulation environment is built in Unity game engine based on
the road network of San Francisco, and human-in-the-loop (HITL) simulation is
conducted to validate the effectiveness of our proposed system regarding travel
time and energy consumption.","['Ziran Wang', 'Kyungtae Han', 'Prashant Tiwari']",2020-08-31T06:14:28Z,http://arxiv.org/abs/2008.13381v1,"['cs.HC', 'cs.MM', 'cs.SY', 'eess.IV', 'eess.SY']"
"Design of achromatic augmented reality visors based on composite
  metasurfaces","A compact near-eye visor (NEV) system that can guide light from a display to
the eye could transform augmented reality (AR) technology. Unfortunately,
existing implementations of such an NEV either suffer from small field of view
or chromatic aberrations. See-through quality and bulkiness further make the
overall performance of the visors unsuitable for a seamless user experience.
Metasurfaces are an emerging class of nanophotonic elements that can
dramatically reduce the size of optical elements while enhancing functionality.
In this paper, we present a design of composite metasurfaces for an
ultra-compact NEV. We simulate the performance of a proof-of-principle visor
corrected for chromatic aberrations while providing a large display field of
view (>77{\deg} both horizontally and vertically), good see-through quality
(>70% transmission and less than a wavelength root mean-square (RMS) wavefront
error over the whole visible wavelength range), as needed for an immersive AR
experience.","['Elyas Bayati', 'Andrew Wolfram', 'Shane Colburn', 'Luocheng Huang', 'Arka Majumdar']",2020-10-01T20:04:34Z,http://arxiv.org/abs/2010.00668v1,"['physics.optics', 'physics.app-ph']"
"Ego-Motion Alignment from Face Detections for Collaborative Augmented
  Reality","Sharing virtual content among multiple smart glasses wearers is an essential
feature of a seamless Collaborative Augmented Reality experience. To enable the
sharing, local coordinate systems of the underlying 6D ego-pose trackers,
running independently on each set of glasses, have to be spatially and
temporally aligned with respect to each other. In this paper, we propose a
novel lightweight solution for this problem, which is referred as ego-motion
alignment. We show that detecting each other's face or glasses together with
tracker ego-poses sufficiently conditions the problem to spatially relate local
coordinate systems. Importantly, the detected glasses can serve as reliable
anchors to bring sufficient accuracy for the targeted practical use. The
proposed idea allows us to abandon the traditional visual localization step
with fiducial markers or scene points as anchors. A novel closed form minimal
solver which solves a Quadratic Eigenvalue Problem is derived and its
refinement with Gaussian Belief Propagation is introduced. Experiments validate
the presented approach and show its high practical potential.","['Branislav Micusik', 'Georgios Evangelidis']",2020-10-05T16:57:48Z,http://arxiv.org/abs/2010.02153v1,['cs.CV']
Making Mobile Augmented Reality Applications Accessible,"Augmented Reality (AR) technology creates new immersive experiences in
entertainment, games, education, retail, and social media. AR content is often
primarily visual and it is challenging to enable access to it non-visually due
to the mix of virtual and real-world content. In this paper, we identify common
constituent tasks in AR by analyzing existing mobile AR applications for iOS,
and characterize the design space of tasks that require accessible
alternatives. For each of the major task categories, we create prototype
accessible alternatives that we evaluate in a study with 10 blind participants
to explore their perceptions of accessible AR. Our study demonstrates that
these prototypes make AR possible to use for blind users and reveals a number
of insights to move forward. We believe our work sets forth not only exemplars
for developers to create accessible AR applications, but also a roadmap for
future research to make AR comprehensively accessible.","['Jaylin Herskovitz', 'Jason Wu', 'Samuel White', 'Amy Pavel', 'Gabriel Reyes', 'Anhong Guo', 'Jeffrey P. Bigham']",2020-10-12T21:23:27Z,http://arxiv.org/abs/2010.06035v1,['cs.HC']
"Real-Time Detection of Simulator Sickness in Virtual Reality Games Based
  on Players' Psychophysiological Data during Gameplay","Virtual Reality (VR) technology has been proliferating in the last decade,
especially in the last few years. However, Simulator Sickness (SS) still
represents a significant problem for its wider adoption. Currently, the most
common way to detect SS is using the Simulator Sickness Questionnaire (SSQ).
SSQ is a subjective measurement and is inadequate for real-time applications
such as VR games. This research aims to investigate how to use machine learning
techniques to detect SS based on in-game characters' and users' physiological
data during gameplay in VR games. To achieve this, we designed an experiment to
collect such data with three types of games. We trained a Long Short-Term
Memory neural network with the dataset eye-tracking and character movement data
to detect SS in real-time. Our results indicate that, in VR games, our model is
an accurate and efficient way to detect SS in real-time.","['Jialin Wang', 'Hai-Ning Liang', 'Diego Monteiro', 'Wenge Xu', 'Hao Chen', 'Qiwen Chen']",2020-10-13T03:53:07Z,http://arxiv.org/abs/2010.06152v1,['cs.HC']
"Developing Augmented Reality based Gaming Model to Teach Ethical
  Education in Primary Schools","Education sector is adopting new technologies for both teaching and learning
pedagogy. Augmented Reality (AR) is a new technology that can be used in the
educational pedagogy to enhance the engagement with students. Students interact
with AR-based educational material for more visualization and explanation.
Therefore, the use of AR in education is becoming more popular. However, most
researches narrate the use of AR technologies in the field of English, Maths,
Science, Culture, Arts, and History education but the absence of ethical
education is visible. In our paper, we design the system and develop an
AR-based mobile game model in the field of Ethical education for pre-primary
students. Students from pre-primary require more interactive lessons than
theoretical concepts. So, we use AR technology to develop a game which offers
interactive procedures where students can learn with fun and engage with the
context. Finally, we develop a prototype that works with our research
objective. We conclude our paper with future works.",['Mohammad Ali'],2020-10-29T04:01:32Z,http://arxiv.org/abs/2010.15346v1,"['cs.CY', 'cs.HC']"
"HAVEN: A Unity-based Virtual Robot Environment to Showcase HRI-based
  Augmented Reality","Due to the COVID-19 pandemic, conducting Human-Robot Interaction (HRI)
studies in person is not permissible due to social distancing practices to
limit the spread of the virus. Therefore, a virtual reality (VR) simulation
with a virtual robot may offer an alternative to real-life HRI studies. Like a
real intelligent robot, a virtual robot can utilize the same advanced
algorithms to behave autonomously. This paper introduces HAVEN (HRI-based
Augmentation in a Virtual robot Environment using uNity), a VR simulation that
enables users to interact with a virtual robot. The goal of this system design
is to enable researchers to conduct HRI Augmented Reality studies using a
virtual robot without being in a real environment. This framework also
introduces two common HRI experiment designs: a hallway passing scenario and
human-robot team object retrieval scenario. Both reflect HAVEN's potential as a
tool for future AR-based HRI studies.","['Andre Cleaver', 'Darren Tang', 'Victoria Chen', 'Jivko Sinapov']",2020-11-06T16:34:17Z,http://arxiv.org/abs/2011.03464v1,['cs.RO']
"Collaborative Augmented Reality on Smartphones via Life-long City-scale
  Maps","In this paper we present the first published end-to-end production
computer-vision system for powering city-scale shared augmented reality
experiences on mobile devices. In doing so we propose a new formulation for an
experience-based mapping framework as an effective solution to the key issues
of city-scale SLAM scalability, robustness, map updates and all-time
all-weather performance required by a production system. Furthermore, we
propose an effective way of synchronising SLAM systems to deliver seamless
real-time localisation of multiple edge devices at the same time. All this in
the presence of network latency and bandwidth limitations. The resulting system
is deployed and tested at scale in San Francisco where it delivers AR
experiences in a mapped area of several hundred kilometers. To foster further
development of this area we offer the data set to the public, constituting the
largest of this kind to date.","['Lukas Platinsky', 'Michal Szabados', 'Filip Hlasek', 'Ross Hemsley', 'Luca Del Pero', 'Andrej Pancik', 'Bryan Baum', 'Hugo Grimmett', 'Peter Ondruska']",2020-11-10T19:45:06Z,http://arxiv.org/abs/2011.05370v1,"['cs.CV', 'cs.RO']"
"Implementing the Cognition Level for Industry 4.0 by integrating
  Augmented Reality and Manufacturing Execution Systems","In the current industrial practices, the exponential growth in terms of
availability and affordability of sensors, data acquisition systems, and
computer networks forces factories to move toward implementing high integrating
Cyber-Physical Systems (CPS) with production, logistics, and services. This
transforms today's factories into Industry 4.0 factories with significant
economic potential. Industry 4.0, also known as the fourth Industrial
Revolution, levers on the integration of cyber technologies, the Internet of
Things, and Services. This paper proposes an Augmented Reality (AR)-based
system that creates a Cognition Level that integrates existent Manufacturing
Execution Systems (MES) to CPS. The idea is to highlight the opportunities
offered by AR technologies to CPS by describing an application scenario. The
system, analyzed in a real factory, shows its capacity to integrate physical
and digital worlds strongly. Furthermore, the conducted survey (based on the
Situation Awareness Global Assessment Technique method) reveals significant
advantages in terms of production monitoring, progress, and workers' Situation
Awareness in general.","['Alfonso Di Pace', 'Giuseppe Fenza', 'Mariacristina Gallo', 'Vincenzo Loia', 'Aldo Meglio', 'Francesco Orciuoli']",2020-11-18T21:53:13Z,http://arxiv.org/abs/2011.10482v1,"['cs.CY', 'cs.AI']"
"Beyond LunAR: An augmented reality UI for deep-space exploration
  missions","As space exploration efforts shift to deep space missions, new challenges
emerge regarding astronaut communication and task completion. While the round
trip propagation delay for lunar communications is 2.6 seconds, the time delay
increases to nearly 22 minutes for Mars missions. This creates a need for
astronaut independence from earth-based assistance, and places greater
significance upon the limited communications that are able to be delivered. To
address this issue, we prototyped an augmented reality user interface for the
new xEMU spacesuit, intended for use on planetary surface missions. This user
interface assists with functions that would usually be completed by flight
controllers in Mission Control, or are currently completed in manners that are
unnecessarily difficult. We accomplish this through features such as AR
model-based task instruction, sampling task assistance, note taking, and
telemetry monitoring and display.","['Sarah Radway', 'Anthony Luo', 'Carmine Elvezio', 'Jenny Cha', 'Sophia Kolak', 'Elijah Zulu', 'Sad Adib']",2020-11-30T04:16:19Z,http://arxiv.org/abs/2011.14535v1,"['cs.HC', 'cs.GR', 'H.5; I.3']"
"Multi-view data capture for dynamic object reconstruction using handheld
  augmented reality mobiles","We propose a system to capture nearly-synchronous frame streams from multiple
and moving handheld mobiles that is suitable for dynamic object 3D
reconstruction. Each mobile executes Simultaneous Localisation and Mapping
on-board to estimate its pose, and uses a wireless communication channel to
send or receive synchronisation triggers. Our system can harvest frames and
mobile poses in real time using a decentralised triggering strategy and a
data-relay architecture that can be deployed either at the Edge or in the
Cloud. We show the effectiveness of our system by employing it for 3D skeleton
and volumetric reconstructions. Our triggering strategy achieves equal
performance to that of an NTP-based synchronisation approach, but offers higher
flexibility, as it can be adjusted online based on application needs. We
created a challenging new dataset, namely 4DM, that involves six handheld
augmented reality mobiles recording an actor performing sports actions
outdoors. We validate our system on 4DM, analyse its strengths and limitations,
and compare its modules with alternative ones.","['M. Bortolon', 'L. Bazzanella', 'F. Poiesi']",2021-03-14T10:26:50Z,http://arxiv.org/abs/2103.07883v2,"['cs.MM', 'cs.CV']"
"Virtual Barriers in Augmented Reality for Safe and Effective Human-Robot
  Cooperation in Manufacturing","Safety is a fundamental requirement in any human-robot collaboration
scenario. To ensure the safety of users for such scenarios, we propose a novel
Virtual Barrier system facilitated by an augmented reality interface. Our
system provides two kinds of Virtual Barriers to ensure safety: 1) a Virtual
Person Barrier which encapsulates and follows the user to protect them from
colliding with the robot, and 2) Virtual Obstacle Barriers which users can
spawn to protect objects or regions that the robot should not enter. To enable
effective human-robot collaboration, our system includes an intuitive robot
programming interface utilizing speech commands and hand gestures, and features
the capability of automatic path re-planning when potential collisions are
detected as a result of a barrier intersecting the robot's planned path. We
compared our novel system with a standard 2D display interface through a user
study, where participants performed a task mimicking an industrial
manufacturing procedure. Results show that our system increases the user's
sense of safety and task efficiency, and makes the interaction more intuitive.","['Khoa Cong Hoang', 'Wesley P. Chan', 'Steven Lay', 'Akansel Cosgun', 'Elizabeth Croft']",2021-04-12T05:36:49Z,http://arxiv.org/abs/2104.05211v1,['cs.RO']
Enhanced LSTM-based Service Decomposition for Mobile Augmented Reality,"Undoubtedly, Mobile Augmented Reality (MAR) applications for 5G and Beyond
wireless networks are witnessing a notable attention recently. However, they
require significant computational and storage resources at the end device
and/or the network via Edge Cloud (EC) support. In this work, a MAR service is
considered under the lenses of microservices where MAR service components can
be decomposed and anchored at different locations ranging from the end device
to different ECs in order to optimize the overall service and network
efficiency. To this end, we propose a mobility aware MAR service decomposition
using a Long Short Term Memory (LSTM) deep neural network to provide efficient
pro-active decision making in real-time. More specifically, the LSTM deep
neural network is trained with optimal solutions derived from a mathematical
programming formulation in an offline manner. Then, decision making at the
inference stage is used to optimize service decomposition of MAR services. A
wide set of numerical investigations reveal that the mobility aware LSTM deep
neural network manage to outperform recently proposed schemes in terms of both
decision making quality as well as computational time.","['Zhaohui Huang', 'Vasilis Friderikos']",2021-04-15T10:19:45Z,http://arxiv.org/abs/2104.07351v1,"['cs.NI', 'eess.SP']"
"Procedural animations in interactive art experiences -- A state of the
  art review","The state of the art review broadly oversees the use of novel research
utilized in the creation of virtual environments applied in interactive art
experiences, with a specific focus on the application of procedural animation
in spatially augmented reality (SAR) exhibitions. These art exhibitions
frequently combine sensory displays that appeal, replace, and augment the
visual, auditory and touch or haptic senses. We analyze and break down
art-technology related innovations in the last three years, and thoroughly
identify the most recent and vibrant applications of interactive art
experiences in the review of numerous installation applications, studies, and
events. Display mediums such as virtual reality, augmented reality, mixed
reality, and robotics are overviewed in the context of art experiences such as
visual art museums, park or historic site tours, live concerts, and theatre. We
explore research and extrapolate how recent innovations can lead to different
applications that will be seen in the future.",['C. Tollola'],2021-05-16T05:14:56Z,http://arxiv.org/abs/2105.09153v1,"['cs.HC', 'cs.MM']"
"A Novel Edge Detection Operator for Identifying Buildings in Augmented
  Reality Applications","Augmented Reality is an environment-enhancing technology, widely applied in
many domains, such as tourism and culture. One of the major challenges in this
field is precise detection and extraction of building information through
Computer Vision techniques. Edge detection is one of the building blocks
operations for many feature extraction solutions in Computer Vision. AR systems
use edge detection for building extraction or for extraction of facade details
from buildings. In this paper, we propose a novel filter operator for edge
detection that aims to extract building contours or facade features better. The
proposed filter gives more weight for finding vertical and horizontal edges
that is an important feature for our aim.","['Ciprian Orhei', 'Silviu Vert', 'Radu Vasiu']",2021-06-02T10:06:50Z,http://arxiv.org/abs/2106.01055v1,['cs.CV']
"Small Object Detection for Near Real-Time Egocentric Perception in a
  Manual Assembly Scenario","Detecting small objects in video streams of head-worn augmented reality
devices in near real-time is a huge challenge: training data is typically
scarce, the input video stream can be of limited quality, and small objects are
notoriously hard to detect. In industrial scenarios, however, it is often
possible to leverage contextual knowledge for the detection of small objects.
Furthermore, CAD data of objects are typically available and can be used to
generate synthetic training data. We describe a near real-time small object
detection pipeline for egocentric perception in a manual assembly scenario: We
generate a training data set based on CAD data and realistic backgrounds in
Unity. We then train a YOLOv4 model for a two-stage detection process: First,
the context is recognized, then the small object of interest is detected. We
evaluate our pipeline on the augmented reality device Microsoft Hololens 2.","['Hooman Tavakoli', 'Snehal Walunj', 'Parsha Pahlevannejad', 'Christiane Plociennik', 'Martin Ruskowski']",2021-06-11T13:59:44Z,http://arxiv.org/abs/2106.06403v1,"['cs.CV', 'cs.AI', 'cs.HC']"
"Effects of Head-locked Augmented Reality on User's performance and
  perceived workload","An augmented reality (AR) environment includes a set of digital elements with
which the users interact while performing certain tasks. Recent AR head-mounted
displays allow users to select how these elements are presented. However, few
studies have been conducted to examine the effect of the way of presenting
augmented content on user performance and workload. This study aims to evaluate
two methods of presenting augmented content - world-locked and head-locked
modes in a data entry task. A total of eighteen participants performed the data
entry task in this study. The effectiveness of each mode is evaluated in terms
of task performance, muscle activity, perceived workload, and usability. The
results show that the task completion time is shorter and the typing speed is
significantly faster in the head-locked mode while the world-locked mode
achieved higher scores in terms of preference. The findings of this study can
be applied to AR user interfaces to improve content presentation and enhance
the user experience.","['Yalda Ghasemi', 'Ankit Singh', 'Myunghee Kim', 'Andrew Johnson', 'Heejin Jeong']",2021-06-26T17:35:39Z,http://arxiv.org/abs/2106.14068v1,['cs.HC']
"Exploring the Effect of Visual Cues on Eye Gaze During AR-Guided Picking
  and Assembly Tasks","In this paper, we present an analysis of eye gaze patterns pertaining to
visual cues in augmented reality (AR) for head-mounted displays (HMDs). We
conducted an experimental study involving a picking and assembly task, which
was guided by different visual cues. We compare these visual cues along
multiple dimensions (in-view vs. out-of-view, static vs. dynamic, sequential
vs. simultaneous) and analyze quantitative metrics such as gaze distribution,
gaze duration, and gaze path distance. Our results indicate that visual cues in
AR significantly affect eye gaze patterns. Specifically, we show that the
effect varies depending on the type of visual cue. We discuss these empirical
results with respect to visual attention theory.","['Arne Seeliger', 'Gerrit Merz', 'Christian Holz', 'Stefan Feuerriegel']",2021-08-10T13:19:39Z,http://arxiv.org/abs/2108.04669v1,['cs.HC']
"An examination of skill requirements for Augmented Reality and Virtual
  Reality job advertisements","The field of Augmented Reality (AR) and Virtual Reality (VR) has seen massive
growth in recent years. Numerous degree programs have started to redesign their
curricula to meet the high market demand of such job positions. In this paper,
we performed a content analysis of online job postings hosted on Indeed.com and
provided a skill classification framework for AR/VR job positions. Furthermore,
we present a ranking of the relevant skills for the job position. Overall, we
noticed that technical skills like UI/UX design, software design, asset design
and graphics rendering are highly desirable for AR/VR positions. Our findings
regarding prominent skill categories could be beneficial for the human resource
departments as well as enhancing existing course curricula to tailor to the
high market demand.","['Amit Verma', 'Pratibha Purohit', 'Timothy Thornton', 'Kamal Lamsal']",2021-08-10T22:06:10Z,http://arxiv.org/abs/2108.04946v1,['cs.CY']
"Soccer line mark segmentation and classification with stochastic
  watershed transform","Augmented reality applications are beginning to change the way sports are
broadcast, providing richer experiences and valuable insights to fans. The
first step of augmented reality systems is camera calibration, possibly based
on detecting the line markings of the playing field. Most existing proposals
for line detection rely on edge detection and Hough transform, but radial
distortion and extraneous edges cause inaccurate or spurious detections of line
markings. We propose a novel strategy to automatically and accurately segment
and classify line markings. First, line points are segmented thanks to a
stochastic watershed transform that is robust to radial distortions, since it
makes no assumptions about line straightness, and is unaffected by the presence
of players or the ball. The line points are then linked to primitive structures
(straight lines and ellipses) thanks to a very efficient procedure that makes
no assumptions about the number of primitives that appear in each image. The
strategy has been tested on a new and public database composed by 60 annotated
images from matches in five stadiums. The results obtained have proven that the
proposed strategy is more robust and accurate than existing approaches,
achieving successful line mark detection even in challenging conditions.","['Daniel Berjón', 'Carlos Cuevas', 'Narciso García']",2021-08-14T00:51:12Z,http://arxiv.org/abs/2108.06432v2,"['cs.CV', 'I.4.6']"
"SketchMeHow: Interactive Projection Guided Task Instruction with User
  Sketches","In this work, we propose an interactive general instruction framework
SketchMeHow to guidance the common users to complete the daily tasks in
real-time. In contrast to the conventional augmented reality-based instruction
systems, the proposed framework utilizes the user sketches as system inputs to
acquire the users' production intentions from the drawing interfaces. Given the
user sketches, the designated task instruction can be analyzed based on the
sub-task division and spatial localization for each task. The projector-camera
system is adopted in the projection guidance to the end-users with the spatial
augmented reality technology. To verify the proposed framework, we conducted
two case studies of domino arrangement and bento production. From our user
studies, the proposed systems can help novice users complete the tasks
efficiently with user satisfaction. We believe the proposed SketchMeHow can
broaden the research topics in sketch-based real-world applications in
human-computer interaction.","['Haoran Xie', 'Yichen Peng', 'Hange Wang', 'Kazunori Miyata']",2021-09-07T12:00:54Z,http://arxiv.org/abs/2109.03013v1,['cs.HC']
ARROCH: Augmented Reality for Robots Collaborating with a Human,"Human-robot collaboration frequently requires extensive communication, e.g.,
using natural language and gestures. Augmented reality (AR) has provided an
alternative way of bridging the communication gap between robots and people.
However, most current AR-based human-robot communication methods are
unidirectional, focusing on how the human adapts to robot behaviors, and are
limited to single-robot domains. In this paper, we develop AR for Robots
Collaborating with a Human (ARROCH), a novel algorithm and system that supports
bidirectional, multi-turn, human-multi-robot communication in indoor multi-room
environments. The human can see through obstacles to observe the robots'
current states and intentions, and provide feedback, while the robots'
behaviors are then adjusted toward human-multi-robot teamwork. Experiments have
been conducted with real robots and human participants using collaborative
delivery tasks. Results show that ARROCH outperformed a standard non-AR
approach in both user experience and teamwork efficiency. In addition, we have
developed a novel simulation environment using Unity (for AR and human
simulation) and Gazebo (for robot simulation). Results in simulation
demonstrate ARROCH's superiority over AR-based baselines in human-robot
collaboration.","['Kishan Chandan', 'Vidisha Kudalkar', 'Xiang Li', 'Shiqi Zhang']",2021-09-21T18:46:19Z,http://arxiv.org/abs/2109.10400v2,['cs.RO']
"Learning Geometric Transformations for Parametric Design: An Augmented
  Reality (AR)-Powered Approach","Despite the remarkable development of parametric modeling methods for
architectural design, a significant problem still exists, which is the lack of
knowledge and skill regarding the professional implementation of parametric
design in architectural modeling. Considering the numerous advantages of
digital/parametric modeling in rapid prototyping and simulation most
instructors encourage students to use digital modeling even from the early
stages of design; however, an appropriate context to learn the basics of
digital design thinking is rarely provided in architectural pedagogy. This
paper presents an educational tool, specifically an Augmented Reality (AR)
intervention, to help students understand the fundamental concepts of
para-metric modeling before diving into complex parametric modeling platforms.
The goal of the AR intervention is to illustrate geometric transformation and
the associated math functions so that students learn the mathematical logic
behind the algorithmic thinking of parametric modeling. We have developed
BRICKxAR_T, an educational AR prototype, that intends to help students learn
geometric transformations in an immersive spatial AR environment. A LEGO set is
used within the AR intervention as a physical manipulative to support physical
interaction and im-prove spatial skill through body gesture.","['Zohreh Shaghaghian', 'Heather Burte', 'Dezhen Song', 'Wei Yan']",2021-09-02T03:49:01Z,http://arxiv.org/abs/2109.10899v1,['cs.HC']
"Here To Stay: Measuring Hologram Stability in Markerless Smartphone
  Augmented Reality","Markerless augmented reality (AR) has the potential to provide engaging
experiences and improve outcomes across a wide variety of industries; the
overlaying of virtual content, or holograms, onto a view of the real world
without the need for predefined markers provides great convenience and
flexibility. However, unwanted hologram movement frequently occurs in
markerless smartphone AR due to challenging visual conditions or device
movement, and resulting error in device pose tracking. We develop a method for
measuring hologram positional errors on commercial smartphone markerless AR
platforms, implement it as an open-source AR app, HoloMeasure, and use the app
to conduct systematic quantitative characterizations of hologram stability
across 6 different user actions, 3 different smartphone models, and over 200
different environments. Our study demonstrates significant levels of spatial
instability in holograms in all but the simplest settings, and underscores the
need for further enhancements to pose tracking algorithms for smartphone-based
markerless AR.","['Tim Scargill', 'Jiasi Chen', 'Maria Gorlatova']",2021-09-29T23:15:43Z,http://arxiv.org/abs/2109.14757v1,['cs.HC']
"RescueAR: Augmented Reality Supported Collaboration for UAV Driven
  Emergency Response Systems","Emergency response events are fast-paced, noisy, and they require teamwork to
accomplish the mission. Furthermore, the increasing deployment of Unmanned
Aerial Vehicles (UAVs) alongside emergency responders, demands a new form of
partnership between humans and UAVs. Traditional radio-based information
exchange between humans during an emergency response suffers from a lack of
visualization and often results in miscommunication. This paper presents a
novel collaboration platform: RescueAR, which utilizes the paradigm of
Location-based Augmented Reality to geotag, share, and visualize information.
RescueAR aims to support the two-way communication between humans and UAVs,
facilitate collaboration across diverse responders, and visualize scene
information relevant to the rescue team's role. According to our feasibility
study, a user study, followed by a focus group session with police officers,
RescueAR can support rescue teams in developing the spatial cognition of the
scene, facilitate the exchange of geolocation information, and complement
existing communication tools during the UAV-supported emergency response.","['Ankit Agrawal', 'Jane Cleland-Huang']",2021-10-01T02:52:18Z,http://arxiv.org/abs/2110.00180v1,['cs.HC']
"Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality
  Collaboration","Visual cues are essential in computer-mediated communication. It is
especially important when communication happens in a collaboration scenario
that requires focusing several users' attention on aspecific object among other
similar ones. This paper explores the effect of visual cues on pointing tasks
in co-located Augmented Reality (AR) collaboration. A user study (N = 32, 16
pairs) was conducted to compare two types of visual cues: Pointing Line (PL)and
Moving Track (MT). Both are head-based visual techniques.Through a series of
collaborative pointing tasks on objects with different states (static and
dynamic) and density levels (low, mediumand high), the results showed that PL
was better on task performance and usability, but MT was rated higher on social
presenceand user preference. Based on our results, some design implicationsare
provided for pointing tasks in co-located AR collaboration.","['Lei Chen', 'Yilin Liu', 'Yue Li', 'Lingyun Yu', 'BoYu Gao', 'Maurizio Caon', 'Yong Yue', 'Hai-Ning Liang']",2021-10-08T11:44:15Z,http://arxiv.org/abs/2110.04045v1,['cs.HC']
"An Augmented Reality Platform for Introducing Reinforcement Learning to
  K-12 Students with Robots","Interactive reinforcement learning, where humans actively assist during an
agent's learning process, has the promise to alleviate the sample complexity
challenges of practical algorithms. However, the inner workings and state of
the robot are typically hidden from the teacher when humans provide feedback.
To create a common ground between the human and the learning robot, in this
paper, we propose an Augmented Reality (AR) system that reveals the hidden
state of the learning to the human users. This paper describes our system's
design and implementation and concludes with a discussion on two directions for
future work which we are pursuing: 1) use of our system in AI education
activities at the K-12 level; and 2) development of a framework for an AR-based
human-in-the-loop reinforcement learning, where the human teacher can see
sensory and cognitive representations of the robot overlaid in the real world.","['Ziyi Zhang', 'Samuel Micah Akai-Nettey', 'Adonai Addo', 'Chris Rogers', 'Jivko Sinapov']",2021-10-10T03:51:39Z,http://arxiv.org/abs/2110.04697v1,"['cs.RO', 'cs.AI']"
Developing a Lecture Video Recording System Using Augmented Reality,"Assistive technology is a prerequisite for making a high-quality lecture
video. It is therefore imperative to edit the lecture video after recording. In
this study, we aim to reduce the cumbersome task of lecture video editing by
developing a system that enables the addition of visual effects in the video
while recording. In particular, we use augmented reality (AR) technology to
digitize and display in real-time lecture materials, assistant agents, and
other recording contents used by the lecturer. Our system realizes such a
mechanism as a lecture recording environment. In addition, our system based on
AR technology can support the work of the lecturer, which is difficult to do by
oneself while conducting the lecture, using the information of the lecturer's
position and the progress of the lecture. We evaluated the system functionality
and performance, and verified the system's correct behavior. If the burden of
making lecture videos can be reduced, the lecturer will be able to devote more
time to improving the quality of lecture contents, which is expected to
contribute to the improvement of lectures.","['Yuma Ito', 'Masato Kikuchi', 'Tadachika Ozono', 'Toramatsu Shintani']",2021-10-11T06:32:27Z,http://arxiv.org/abs/2110.05955v1,['cs.HC']
"State of the Art of Augmented Reality (AR) Capabilities for Civil
  Infrastructure Applications","Augmented Reality (AR) is a technology superimposing interactional virtual
objects onto a real environment. Since the beginning of the millennium, AR
technologies have shown rapid growth, with significant research publications in
engineering and science. However, the civil infrastructure community has
minimally implemented AR technologies to date. One of the challenges that civil
engineers face when understanding and using AR is the lack of a classification
of AR in the context of capabilities for civil infrastructure applications.
Practitioners in civil infrastructure, like most engineering fields, prioritize
understanding the level of maturity of a new technology before considering its
adoption and field implementation. This paper compares the capabilities of
sixteen AR Head-Mounted Devices (HMDs) available in the market since 2017,
ranking them in terms of performance for civil infrastructure implementations.
Finally, the authors recommend a development framework for practical AR
interfaces with civil infrastructure and operations.","['Jiaqi Xu', 'Derek Doyle', 'Fernando Moreu']",2021-10-17T01:34:59Z,http://arxiv.org/abs/2110.08698v1,"['cs.HC', '01-02', 'H.4.3; I.3.6; J.2; J.7; K.1; K.2; K.4.3']"
"Exploring Augmented Reality Games in Accessible Learning: A Systematic
  Review","Augmented Reality (AR) learning games, on average, have been shown to have a
positive impact on student learning. However, the exploration of AR learning
games in special education settings, where accessibility is a concern, has not
been well explored. Thus, the purpose of this study is to explore the use of AR
games in accessible learning applications and to provide a comprehensive
understanding of its advantages over traditional learning approaches. In this
paper, we present our systematic review of previous studies included in major
databases in the past decade. We explored the characteristics of user
evaluation, learning effects on students, and features of implemented systems
mentioned in the literature. The results showed that AR game applications can
promote students learning activities from three perspectives: cognitive,
affective, and retention. We also found there were still several drawbacks to
current AR learning game designs for special needs despite the positive effects
associated with AR game use. Based on our findings, we propose potential design
strategies for future AR learning games for accessible education.","['Minghao Cai', 'Gokce Akcayir', 'Carrie Demmans Epp']",2021-11-16T04:03:51Z,http://arxiv.org/abs/2111.08214v1,['cs.HC']
FaceAtlasAR: Atlas of Facial Acupuncture Points in Augmented Reality,"Acupuncture is a technique in which practitioners stimulate specific points
on the body. Those points, called acupuncture points (or acupoints),
anatomically define areas on the skin relative to specific landmarks on the
body. However, mapping the acupoints to individuals could be challenging for
inexperienced acupuncturists. In this project, we proposed a system to localize
and visualize facial acupoints for individuals in an augmented reality (AR)
context. This system combines a face alignment model and a hair segmentation
model to provide dense reference points for acupoints localization in real-time
(60FPS). The localization process takes the proportional bone (B-cun or
skeletal) measurement method, which is commonly operated by specialists;
however, in the real practice, operators sometimes find it inaccurate due to
skill-related error. With this system, users, even without any skills, can
locate the facial acupoints as a part of the self-training or self-treatment
process.","['Menghe Zhang', 'Jurgen Schulze', 'Dong Zhang']",2021-11-29T18:00:25Z,http://arxiv.org/abs/2111.14755v2,"['cs.GR', 'cs.CV', 'cs.HC']"
"Semantic Interaction in Augmented Reality Environments for Microsoft
  HoloLens","Augmented Reality is a promising technique for human-machine interaction.
Especially in robotics, which always considers systems in their environment, it
is highly beneficial to display visualizations and receive user input directly
in exactly that environment. We explore this idea using the Microsoft HoloLens,
with which we capture indoor environments and display interaction cues with
known object classes. The 3D mesh recorded by the HoloLens is annotated
on-line, as the user moves, with semantic classes using a projective approach,
which allows us to use a state-of-the-art 2D semantic segmentation method. The
results are fused onto the mesh; prominent object segments are identified and
displayed in 3D to the user. Finally, the user can trigger actions by gesturing
at the object. We both present qualitative results and analyze the accuracy and
performance of our method in detail on an indoor dataset.","['Peer Schüett', 'Max Schwarz', 'Sven Behnke']",2021-11-18T14:58:04Z,http://arxiv.org/abs/2112.05846v1,"['cs.CV', 'cs.RO']"
Stay Positive: Non-Negative Image Synthesis for Augmented Reality,"In applications such as optical see-through and projector augmented reality,
producing images amounts to solving non-negative image generation, where one
can only add light to an existing image. Most image generation methods,
however, are ill-suited to this problem setting, as they make the assumption
that one can assign arbitrary color to each pixel. In fact, naive application
of existing methods fails even in simple domains such as MNIST digits, since
one cannot create darker pixels by adding light. We know, however, that the
human visual system can be fooled by optical illusions involving certain
spatial configurations of brightness and contrast. Our key insight is that one
can leverage this behavior to produce high quality images with negligible
artifacts. For example, we can create the illusion of darker patches by
brightening surrounding pixels. We propose a novel optimization procedure to
produce images that satisfy both semantic and non-negativity constraints. Our
approach can incorporate existing state-of-the-art methods, and exhibits strong
performance in a variety of tasks including image-to-image translation and
style transfer.","['Katie Luo', 'Guandao Yang', 'Wenqi Xian', 'Harald Haraldsson', 'Bharath Hariharan', 'Serge Belongie']",2022-02-01T18:55:11Z,http://arxiv.org/abs/2202.00659v1,"['cs.CV', 'cs.GR']"
Context-Based MEC Platform for Augmented-Reality Services in 5G Networks,"Augmented reality (AR) has drawn great attention in recent years. However,
current AR devices have drawbacks, e.g., weak computation ability and large
power consumption. To solve the problem, mobile edge computing (MEC) can be
introduced as a key technology to offload data and computation from AR devices
to MEC servers via 5th Generation Mobile Communication Technology (5G)
networks. To this end, a context-based MEC platform for AR services in 5G
networks is proposed in this paper. On the platform, MEC is employed as a data
processing center while AR devices are simplified as universal input/output
devices, which overcomes their limitations and achieves better user experience.
Moreover, the proof-of-concept (PoC) hardware prototype of the platform, and
two typical use cases providing AR services of navigation and face recognition
respectively are implemented to demonstrate the feasibility and effectiveness
of the platform. Finally, the performance of the platform is also numerically
evaluated, and the results validate the system design and agree well with the
design expectations.","['Yue Wang', 'Tao Yu', 'Kei Sakaguchi']",2022-02-03T14:07:20Z,http://arxiv.org/abs/2202.01600v1,['cs.NI']
Experimental Augmented Reality User Experience,"Augmented Reality (AR) is an emerging field ripe for experimentation,
especially when it comes to developing the kinds of applications and
experiences that will drive mass adoption of the technology. While we aren't
aware of any current consumer product that realize a wearable, wide Field of
View (FoV), AR Head Mounted Display (HMD), such devices will certainly come. In
order for these sophisticated, likely high-cost hardware products to succeed,
it is important they provide a high quality user experience. To that end, we
prototyped 4 experimental applications for wide FoV displays that will likely
exist in the future. Given current AR HMD limitations, we used a AR simulator
built on web technology and VR headsets to demonstrate these applications,
allowing users and designers to peer into the future.","['Josef Spjut', 'Fengyuan Zhu', 'Xiaolei Huang', 'Yichen Shou', 'Ben Boudaoud', 'Omer Shapira', 'Morgan McGuire']",2022-02-10T19:05:46Z,http://arxiv.org/abs/2202.06726v1,"['cs.HC', 'cs.GR']"
ARcall: Real-Time AR Communication using Smartphones and Smartglasses,"Augmented Reality (AR) smartglasses are increasingly regarded as the next
generation personal computing platform. However, there is a lack of
understanding about how to design communication systems using them. We present
ARcall, a novel Augmented Reality-based real-time communication system that
enables an immersive, delightful, and privacy-preserving experience between a
smartphone user and a smartglasses wearer. ARcall allows a remote friend
(Friend) to send and project AR content to a smartglasses wearer (Wearer). The
ARcall system was designed with the practical limits of existing AR glasses in
mind, including shorter battery life and a reduced field of view. We conduct a
qualitative evaluation of the three main components of ARcall: Drop-In,
ARaction, and Micro-Chat. Our results provide novel insights for building
future AR-based communication methods, including, the importance of context
priming, user control over AR content placement, and the feeling of co-presence
while conversing.","['Hemant Bhaskar Surale', 'Yu Jiang Tham', 'Brian A. Smith', 'Rajan Vaish']",2022-03-08T19:32:16Z,http://arxiv.org/abs/2203.04358v1,"['cs.HC', 'cs.SI', 'H.5.2; I.3.7']"
"Medicinal Boxes Recognition on a Deep Transfer Learning Augmented
  Reality Mobile Application","Taking medicines is a fundamental aspect to cure illnesses. However, studies
have shown that it can be hard for patients to remember the correct posology.
More aggravating, a wrong dosage generally causes the disease to worsen.
Although, all relevant instructions for a medicine are summarized in the
corresponding patient information leaflet, the latter is generally difficult to
navigate and understand. To address this problem and help patients with their
medication, in this paper we introduce an augmented reality mobile application
that can present to the user important details on the framed medicine. In
particular, the app implements an inference engine based on a deep neural
network, i.e., a densenet, fine-tuned to recognize a medicinal from its
package. Subsequently, relevant information, such as posology or a simplified
leaflet, is overlaid on the camera feed to help a patient when taking a
medicine. Extensive experiments to select the best hyperparameters were
performed on a dataset specifically collected to address this task; ultimately
obtaining up to 91.30\% accuracy as well as real-time capabilities.","['Danilo Avola', 'Luigi Cinque', 'Alessio Fagioli', 'Gian Luca Foresti', 'Marco Raoul Marini', 'Alessio Mecca', 'Daniele Pannone']",2022-03-26T09:21:56Z,http://arxiv.org/abs/2203.14031v1,['cs.CV']
"BIMxAR: BIM-Empowered Augmented Reality for Learning Architectural
  Representations","Literature review shows limited research investigating the utilization of
Augmented Reality (AR) to improve learning and understanding architectural
representations, specifically section views. In this study, we present an AR
system prototype (BIMxAR), its new and accurate building-scale registration
method, and its novel visualization features that facilitate the comprehension
of building construction systems, materials configuration, and 3D section views
of complex structures through the integration of AR, Building Information
Modeling (BIM), and physical buildings. A pilot user study found improvements
after students studied building section views in a physical building with AR,
though not statistically significant, in terms of scores of the Santa Barbara
Solids Test (SBST) and the Architectural Representations Test (ART). When
incorporating time as a performance factor, the ART timed scores show a
significant improvement in the posttest session. BIMxAR has the potential to
enhance the students spatial abilities, particularly in understanding buildings
and complex section views.","['Ziad Ashour', 'Zohreh Shaghaghian', 'Wei Yan']",2022-04-07T04:32:43Z,http://arxiv.org/abs/2204.03207v2,['cs.HC']
"Understanding AR Activism: An Interview Study with Creators of Augmented
  Reality Experiences for Social Change","The rise of consumer augmented reality (AR) technology has opened up new
possibilities for interventions intended to disrupt and subvert cultural
conventions. From defacing corporate logos to erecting geofenced digital
monuments, more and more people are creating AR experiences for social causes.
We sought to understand this new form of activism, including why people use AR
for these purposes, opportunities and challenges in using it, and how well it
can support activist goals. We conducted semi-structured interviews with twenty
people involved in projects that used AR for a social cause across six
different countries. We found that AR can overcome physical world limitations
of activism to convey immersive, multilayered narratives that aim to reveal
invisible histories and perspectives. At the same time, people experienced
challenges in creating, maintaining, and distributing their AR experiences to
audiences. We discuss open questions and opportunities for creating AR tools
and experiences for social change.","['Rafael M. L. Silva', 'Erica Principe Cruz', 'Daniela K. Rosner', 'Dayton Kelly', 'Andrés Monroy-Hernández', 'Fannie Liu']",2022-04-21T00:12:55Z,http://arxiv.org/abs/2204.09821v1,"['cs.HC', 'cs.CY']"
"Microvision: Static analysis-based approach to visualizing microservices
  in augmented reality","Microservices are supporting digital transformation; however, fundamental
tools and system perspectives are missing to better observe, understand, and
manage these systems, their properties, and their dependencies. Microservices
architecture leans toward decentralization, which yields many advantages to
system operation; it, however, brings challenges to their development.
Microservices lack a system-centric perspective to better cope with system
evolution and quality assessment. In this work, we explore
microservice-specific architecture reconstruction based on static analysis.
Such reconstruction typically results in system models to visualize selected
system-centric perspectives. Conventional models are limited in utility when
the service cardinality is high. We consider an alternative data visualization
using 3D space using augmented reality. To begin testing the feasibility of
deriving such perspectives from microservice systems, we developed and
implemented prototype tools for software architecture reconstruction and
visualization of compared perspectives.","['Tomas Cerny', 'Amr S. Abdelfattah', 'Vincent Bushong', 'Abdullah Al Maruf', 'Davide Taibi']",2022-07-06T21:19:19Z,http://arxiv.org/abs/2207.02974v2,['cs.SE']
"A Brief Note on Building Augmented Reality Models for Scientific
  Visualization","Augmented reality (AR) has revolutionized the video game industry by
providing interactive, three-dimensional visualization. Interestingly, AR
technology has only been sparsely used in scientific visualization. This is, at
least in part, due to the significant technical challenges previously
associated with creating and accessing such models. To ease access to AR for
the scientific community, we introduce a novel visualization pipeline with
which they can create and render AR models. We demonstrate our pipeline by
means of finite element results, but note that our pipeline is generally
applicable to data that may be represented through meshed surfaces.
Specifically, we use two open-source software packages, ParaView and Blender.
The models are then rendered through the <model-viewer> platform, which we
access through Android and iOS smartphones. To demonstrate our pipeline, we
build AR models from static and time-series results of finite element
simulations discretized with continuum, shell, and beam elements. Moreover, we
openly provide python scripts to automate this process. Thus, others may use
our framework to create and render AR models for their own research and
teaching activities.","['Mrudang Mathur', 'Josef M. Brozovich', 'Manuel K. Rausch']",2022-07-30T19:51:29Z,http://arxiv.org/abs/2208.02022v1,"['cs.GR', 'cs.HC']"
Evaluating Cardiovascular Surgical Planning in Mobile Augmented Reality,"Advanced surgical procedures for congenital heart diseases (CHDs) require
precise planning before the surgeries. The conventional approach utilizes
3D-printing and cutting physical heart models, which is a time and resource
intensive process. While rapid advances in augmented reality (AR) technologies
have the potential to streamline surgical planning, there is limited research
that evaluates such AR approaches with medical experts. This paper presents an
evaluation with 6 experts, 4 cardiothoracic surgeons, and 2 cardiologists, from
Children's Healthcare of Atlanta (CHOA) Heart Center to validate the usability
and technical innovations of CardiacAR, a prototype mobile AR surgical planning
application. Potential future improvements based on user feedback are also
proposed to further improve the design of CardiacAR and broaden its access.","['Haoyang Yang', 'Pratham Darrpan Mehta', 'Jonathan Leo', 'Zhiyan Zhou', 'Megan Dass', 'Anish Upadhayay', 'Timothy C. Slesnick', 'Fawwaz Shaw', 'Amanda Randles', 'Duen Horng Chau']",2022-08-22T22:27:59Z,http://arxiv.org/abs/2208.10639v1,['cs.HC']
"Design and Implementation of a Human-Robot Joint Action Framework using
  Augmented Reality and Eye Gaze","When humans work together to complete a joint task, each person builds an
internal model of the situation and how it will evolve. Efficient collaboration
is dependent on how these individual models overlap to form a shared mental
model among team members, which is important for collaborative processes in
human-robot teams. The development and maintenance of an accurate shared mental
model requires bidirectional communication of individual intent and the ability
to interpret the intent of other team members. To enable effective human-robot
collaboration, this paper presents a design and implementation of a novel joint
action framework in human-robot team collaboration, utilizing augmented reality
(AR) technology and user eye gaze to enable bidirectional communication of
intent. We tested our new framework through a user study with 37 participants,
and found that our system improves task efficiency, trust, as well as task
fluency. Therefore, using AR and eye gaze to enable bidirectional communication
is a promising mean to improve core components that influence collaboration
between humans and robots.","['Wesley P. Chan', 'Morgan Crouch', 'Khoa Hoang', 'Charlie Chen', 'Nicole Robinson', 'Elizabeth Croft']",2022-08-25T03:48:12Z,http://arxiv.org/abs/2208.11856v1,['cs.RO']
"Deceiving Audio Design in Augmented Environments : A Systematic Review
  of Audio Effects in Augmented Reality","Recently, a lot of works show promising directions for audio design in
augmented reality (AR). These works are mainly focused on how to improve user
experience and make AR more realistic. But even though these improvements seem
promising, these new possibilities could also be used as an input for
manipulative design. This survey aims to analyze all recent discoveries in
audio development regarding AR and argue what kind of ""manipulative"" effect
this could have on the user. It can be concluded that even though there are
many works explaining the effects of audio design in AR, very few works point
out the risk of harm or manipulation toward the user. Future works could
contain more awareness of this problem or maybe even","['Esmée Henrieke Anne de Haas', 'Lik-Hang Lee']",2022-09-03T08:33:19Z,http://arxiv.org/abs/2209.01367v1,['cs.HC']
"Resource Allocation and Resolution Control in the Metaverse with Mobile
  Augmented Reality","With the development of blockchain and communication techniques, the
Metaverse is considered as a promising next-generation Internet paradigm, which
enables the connection between reality and the virtual world. The key to
rendering a virtual world is to provide users with immersive experiences and
virtual avatars, which is based on virtual reality (VR) technology and high
data transmission rate. However, current VR devices require intensive
computation and communication, and users suffer from high delay while using
wireless VR devices. To build the connection between reality and the virtual
world with current technologies, mobile augmented reality (MAR) is a feasible
alternative solution due to its cheaper communication and computation cost.
This paper proposes an MAR-based connection model for the Metaverse, and
proposes a communication resources allocation algorithm based on outer
approximation (OA) to achieve the best utility. Simulation results show that
our proposed algorithm is able to provide users with basic MAR services for the
Metaverse, and outperforms the benchmark greedy algorithm.","['Peiyuan Si', 'Jun Zhao', 'Huimei Han', 'Kwok-Yan Lam', 'Yang Liu']",2022-09-28T07:09:52Z,http://arxiv.org/abs/2209.13871v1,"['eess.SP', 'cs.SI']"
"Towards Semi-automatic Detection and Localization of Indoor
  Accessibility Issues using Mobile Depth Scanning and Computer Vision","To help improve the safety and accessibility of indoor spaces, researchers
and health professionals have created assessment instruments that enable
homeowners and trained experts to audit and improve homes. With advances in
computer vision, augmented reality (AR), and mobile sensors, new approaches are
now possible. We introduce RASSAR (Room Accessibility and Safety Scanning in
Augmented Reality), a new proof-of-concept prototype for semi-automatically
identifying, categorizing, and localizing indoor accessibility and safety
issues using LiDAR + camera data, machine learning, and AR. We present an
overview of the current RASSAR prototype and a preliminary evaluation in a
single home.","['Xia Su', 'Kaiming Cheng', 'Han Zhang', 'Jaewook Lee', 'Jon E. Froehlich']",2022-10-05T20:07:05Z,http://arxiv.org/abs/2210.02533v1,"['cs.HC', 'cs.CV', 'cs.CY']"
"Augmented Reality and Mixed Reality Measurement Under Different
  Environments: A Survey on Head-Mounted Devices","Augmented Reality (AR) and Mixed Reality (MR) have been two of the most
explosive research topics in the last few years. Head-Mounted Devices (HMDs)
are essential intermediums for using AR and MR technology, playing an important
role in the research progress in these two areas. Behavioral research with
users is one way of evaluating the technical progress and effectiveness of
HMDs. In addition, AR and MR technology is dependent upon virtual interactions
with the real environment. Thus, conditions in real environments can be a
significant factor for AR and MR measurements with users. In this paper, we
survey 87 environmental-related HMD papers with measurements from users,
spanning over 32 years. We provide a thorough review of AR- and MR-related user
experiments with HMDs under different environmental factors. Then, we summarize
trends in this literature over time using a new classification method with four
environmental factors, the presence or absence of user feedback in behavioral
experiments, and ten main categories to subdivide these papers (e.g., domain
and method of user assessment). We also categorize characteristics of the
behavioral experiments, showing similarities and differences among papers.","['Hung-Jui Guo', 'Jonathan Z. Bakdash', 'Laura R. Marusich', 'Balakrishnan Prabhakaran']",2022-10-29T02:03:56Z,http://arxiv.org/abs/2210.16463v1,['cs.HC']
"Learning Visualization Policies of Augmented Reality for Human-Robot
  Collaboration","In human-robot collaboration domains, augmented reality (AR) technologies
have enabled people to visualize the state of robots. Current AR-based
visualization policies are designed manually, which requires a lot of human
efforts and domain knowledge. When too little information is visualized, human
users find the AR interface not useful; when too much information is
visualized, they find it difficult to process the visualized information. In
this paper, we develop a framework, called VARIL, that enables AR agents to
learn visualization policies (what to visualize, when, and how) from
demonstrations. We created a Unity-based platform for simulating warehouse
environments where human-robot teammates collaborate on delivery tasks. We have
collected a dataset that includes demonstrations of visualizing robots' current
and planned behaviors. Results from experiments with real human participants
show that, compared with competitive baselines from the literature, our learned
visualization strategies significantly increase the efficiency of human-robot
teams, while reducing the distraction level of human users. VARIL has been
demonstrated in a built-in-lab mock warehouse.","['Kishan Chandan', 'Jack Albertson', 'Shiqi Zhang']",2022-11-13T22:03:20Z,http://arxiv.org/abs/2211.07028v1,"['cs.RO', 'cs.HC', 'cs.LG']"
"An Augmented Reality Application and User Study for Understanding and
  Learning Spatial Transformation Matrices","Understanding spatial transformations and their mathematical representations
are essential in computer-aided design, robotics, etc. This research has
developed and tested an Augmented Reality (AR) application (BRICKxAR/T) to
enhance students' learning of spatial transformation matrices. BRICKxAR/T
leverages AR features, including information augmentation, physical-virtual
object interplay, and embodied learning, to create a novel and effective
visualization experience for learning. BRICKxAR T has been evaluated as a
learning intervention using LEGO models as example physical and virtual
manipulatives in a user study to assess students' learning gains. The study
compared AR (N=29) vs. non-AR (N=30) learning workshops with pre- and
post-tests on Purdue Visualization of Rotations Test and math questions.
Students' math scores significantly improved after participating in both
workshops with the AR workshop tending to show greater improvements. The
post-workshop survey showed students were inclined to think BRICKxAR/T an
interesting and useful application, and they spent more time learning in AR
than non-AR.","['Zohreh Shaghaghian', 'Heather Burte', 'Dezhen Song', 'Wei Yan']",2022-11-16T22:39:33Z,http://arxiv.org/abs/2212.00110v1,['cs.HC']
Mobile Augmented Reality with Federated Learning in the Metaverse,"The Metaverse is deemed the next evolution of the Internet and has received
much attention recently. Metaverse applications via mobile augmented reality
(MAR) require rapid and accurate object detection to mix digital data with the
real world. As mobile devices evolve, their computational capabilities are
increasing, and thus their computational resources can be leveraged to train
machine learning models. In light of the increasing concerns of user privacy
and data security, federated learning (FL) has become a promising distributed
learning framework for privacy-preserving analytics. In this article, FL and
MAR are brought together in the Metaverse. We discuss the necessity and
rationality of the combination of FL and MAR. The prospective technologies that
support FL and MAR in the Metaverse are also discussed. In addition, existing
challenges that prevent the fulfillment of FL and MAR in the Metaverse and
several application scenarios are presented. Finally, three case studies of
Metaverse FL-MAR systems are demonstrated.","['Xinyu Zhou', 'Jun Zhao']",2022-12-16T07:53:05Z,http://arxiv.org/abs/2212.08324v2,"['cs.LG', 'cs.CR']"
Exploring Text Selection in Augmented Reality Systems,"Text selection is a common and essential activity during text interaction in
all interactive systems. As Augmented Reality (AR) head-mounted displays (HMDs)
become more widespread, they will need to provide effective interaction
techniques for text selection that ensure users can complete a range of text
manipulation tasks (e.g., to highlight, copy, and paste text, send instant
messages, and browse the web). As a relatively new platform, text selection in
AR is largely unexplored and the suitability of interaction techniques
supported by current AR HMDs for text selection tasks is unclear. This research
aims to fill this gap and reports on an experiment with 12 participants, which
compares the performance and usability (user experience and workload) of four
possible techniques (Hand+Pinch, Hand+Dwell, Head+Pinch, and Head+Dwell). Our
results suggest that Head+Dwell should be the default selection technique, as
it is relatively fast, has the lowest error rate and workload, and has the
highest-rated user experience and social acceptance.","['Xinyi Liu', 'Xuanru Meng', 'Becky Spittle', 'Wenge Xu', 'BoYu Gao', 'Hai-Ning Liang']",2022-12-29T15:09:57Z,http://arxiv.org/abs/2212.14336v1,"['cs.HC', 'H.5.1; I.3.7']"
PokAR: Facilitating Poker Play Through Augmented Reality,"We introduce PokAR, an augmented reality (AR) application to facilitate poker
play. PokAR aims to alleviate three difficulties of traditional poker by
leveraging AR technology: (1) need to have physical poker chips, (2) complex
rules of poker, (3) slow game pace caused by laborious tasks. Despite the
potential benefits of AR in poker, not much research has been done in the
field. In fact, PokAR is the first application to enable AR poker on a mobile
device without requiring extra costly equipment. This has been done by creating
a Snapchat Lens which can be used on most mobile devices. We evaluated this
application by instructing 4 participant dyads to use PokAR to engage in poker
play and respond to survey questions about their experience. We found that most
PokAR features were positively received, AR did not significantly improve nor
hinder socialization, PokAR slightly increased the game pace, and participants
had an overall enjoyable experience with the Lens. These findings led to three
major conclusions: (1) AR has the potential to augment and simplify traditional
table games, (2) AR should not be used to replace traditional experiences, only
augment them, (3) Future work includes additional features like increased
tactility and statistical annotations.","['Adam Gamba', 'Andrés Monroy-Hernández']",2023-01-02T02:32:26Z,http://arxiv.org/abs/2301.00505v1,['cs.HC']
"Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile
  Augmented Reality","Lighting understanding plays an important role in virtual object composition,
including mobile augmented reality (AR) applications. Prior work often targets
recovering lighting from the physical environment to support photorealistic AR
rendering. Because the common workflow is to use a back-facing camera to
capture the physical world for overlaying virtual objects, we refer to this
usage pattern as back-facing AR. However, existing methods often fall short in
supporting emerging front-facing mobile AR applications, e.g., virtual try-on
where a user leverages a front-facing camera to explore the effect of various
products (e.g., glasses or hats) of different styles. This lack of support can
be attributed to the unique challenges of obtaining 360$^\circ$ HDR environment
maps, an ideal format of lighting representation, from the front-facing camera
and existing techniques. In this paper, we propose to leverage dual-camera
streaming to generate a high-quality environment map by combining multi-view
lighting reconstruction and parametric directional lighting estimation. Our
preliminary results show improved rendering quality using a dual-camera setup
for front-facing AR compared to a commercial solution.","['Yiqin Zhao', 'Sean Fanello', 'Tian Guo']",2023-01-15T16:52:59Z,http://arxiv.org/abs/2301.06143v1,['cs.CV']
"Evaluating Digital Work Instructions with Augmented Reality versus
  Paper-based Documents for Manual, Object-Specific Repair Tasks in a Case
  Study with Experienced Workers","Manual repair tasks in the industry of maintenance, repair, and overhaul
require experience and object-specific information. Today, many of these repair
tasks are still performed and documented with inefficient paper documents.
Cognitive assistance systems have the potential to reduce costs, errors, and
mental workload by providing all required information digitally. In this case
study, we present an assistance system for object-specific repair tasks for
turbine blades. The assistance system provides digital work instructions and
uses augmented reality to display spatial information. In a user study with ten
experienced metalworkers performing a familiar repair task, we compare time to
task completion, subjective workload, and system usability of the new
assistance system to their established paper-based workflow. All participants
stated that they preferred the assistance system over the paper documents. The
results of the study show that the manual repair task can be completed 21 %
faster and with a 26 % lower perceived workload using the assistance system.","['Leon Eversberg', 'Jens Lambrecht']",2023-01-18T14:46:07Z,http://arxiv.org/abs/2301.07570v2,['cs.HC']
"Evaluating the Possibility of Integrating Augmented Reality and Internet
  of Things Technologies to Help Patients with Alzheimer's Disease","People suffering from Alzheimer's disease (AD) and their caregivers seek
different approaches to cope with memory loss. Although AD patients want to
live independently, they often need help from caregivers. In this situation,
caregivers may attach notes on every single object or take out the contents of
a drawer to make them visible before leaving the patient alone at home. This
study reports preliminary results on an Ambient Assisted Living (AAL) real-time
system, achieved through the Internet of Things (IoT) and Augmented Reality
(AR) concepts, aimed at helping people suffering from AD. The system has two
main sections: the smartphone or windows application allows caregivers to
monitor patients' status at home and be notified if patients are at risk. The
second part allows patients to use smart glasses to recognize QR codes in the
environment and receive information related to tags in the form of audio, text,
or three-dimensional image. This work presents preliminary results and
investigates the possibility of implementing such a system.","['Fatemeh Ghorbani', 'Mohammad Kia', 'Mehdi Delrobaei', 'Quazi Rahman']",2023-01-20T20:39:32Z,http://arxiv.org/abs/2301.08795v1,"['cs.HC', 'cs.AI', 'cs.SY', 'eess.SY', 'J.6']"
"Resource Allocation of Federated Learning Assisted Mobile Augmented
  Reality System in the Metaverse","Metaverse has become a buzzword recently. Mobile augmented reality (MAR) is a
promising approach to providing users with an immersive experience in the
Metaverse. However, due to limitations of bandwidth, latency and computational
resources, MAR cannot be applied on a large scale in the Metaverse yet.
Moreover, federated learning, with its privacy-preserving characteristics, has
emerged as a prospective distributed learning framework in the future Metaverse
world. In this paper, we propose a federated learning assisted MAR system via
non-orthogonal multiple access for the Metaverse. Additionally, to optimize a
weighted sum of energy, latency and model accuracy, a resource allocation
algorithm is devised by setting appropriate transmission power, CPU frequency
and video frame resolution for each user. Experimental results demonstrate that
our proposed algorithm achieves an overall good performance compared to a
random algorithm and greedy algorithm.","['Xinyu Zhou', 'Yang Li', 'Jun Zhao']",2023-01-28T04:14:43Z,http://arxiv.org/abs/2301.12085v2,['cs.SI']
Explainable Human-Robot Training and Cooperation with Augmented Reality,"The current spread of social and assistive robotics applications is
increasingly highlighting the need for robots that can be easily taught and
interacted with, even by users with no technical background. Still, it is often
difficult to grasp what such robots know or to assess if a correct
representation of the task is being formed. Augmented Reality (AR) has the
potential to bridge this gap. We demonstrate three use cases where AR design
elements enhance the explainability and efficiency of human-robot interaction:
1) a human teaching a robot some simple kitchen tasks by demonstration, 2) the
robot showing its plan for solving novel tasks in AR to a human for validation,
and 3) a robot communicating its intentions via AR while assisting people with
limited mobility during daily activities.","['Chao Wang', 'Anna Belardinelli', 'Stephan Hasler', 'Theodoros Stouraitis', 'Daniel Tanneberg', 'Michael Gienger']",2023-02-02T12:07:34Z,http://arxiv.org/abs/2302.01039v1,['cs.HC']
"Teachable Reality: Prototyping Tangible Augmented Reality with Everyday
  Objects by Leveraging Interactive Machine Teaching","This paper introduces Teachable Reality, an augmented reality (AR)
prototyping tool for creating interactive tangible AR applications with
arbitrary everyday objects. Teachable Reality leverages vision-based
interactive machine teaching (e.g., Teachable Machine), which captures
real-world interactions for AR prototyping. It identifies the user-defined
tangible and gestural interactions using an on-demand computer vision model.
Based on this, the user can easily create functional AR prototypes without
programming, enabled by a trigger-action authoring interface. Therefore, our
approach allows the flexibility, customizability, and generalizability of
tangible AR applications that can address the limitation of current
marker-based approaches. We explore the design space and demonstrate various AR
prototypes, which include tangible and deformable interfaces, context-aware
assistants, and body-driven AR applications. The results of our user study and
expert interviews confirm that our approach can lower the barrier to creating
functional AR prototypes while also allowing flexible and general-purpose
prototyping experiences.","['Kyzyl Monteiro', 'Ritik Vatsal', 'Neil Chulpongsatorn', 'Aman Parnami', 'Ryo Suzuki']",2023-02-21T23:03:49Z,http://arxiv.org/abs/2302.11046v1,"['cs.HC', 'cs.CV', 'cs.LG']"
"Comparing 2D and Augmented Reality Visualizations for Microservice
  System Understandability: A Controlled Experiment","Microservice-based systems are often complex to understand, especially when
their sizes grow. Abstracted views help practitioners with the system
understanding from a certain perspective. Recent advancement in interactive
data visualization begs the question of whether established software
engineering models to visualize system design remain the most suited approach
for the service-oriented design of microservices. Our recent work proposed
presenting a 3D visualization for microservices in augmented reality. This
paper analyzes whether such an approach brings any benefits to practitioners
when dealing with selected architectural questions related to system design
quality. For this purpose, we conducted a controlled experiment involving 20
participants investigating their performance in identifying service dependency,
service cardinality, and bottlenecks. Results show that the 3D enables novices
to perform as well as experts in the detection of service dependencies,
especially in large systems, while no differences are reported for the
identification of service cardinality and bottlenecks. We recommend industry
and researchers to further investigate AR for microservice architectural
analysis, especially to ease the onboarding of new developers in
microservice~projects.","['Amr S. Abdelfattah', 'Tomas Cerny', 'Davide Taibi', 'Sira Vegas']",2023-03-03T23:50:14Z,http://arxiv.org/abs/2303.02268v1,"['cs.SE', 'cs.HC']"
The Dark Side of Augmented Reality: Exploring Manipulative Designs in AR,"Augmented Reality (AR) applications are becoming more mainstream, with
successful examples in the mobile environment like Pokemon GO. Current
malicious techniques can exploit these environments' immersive and mixed nature
(physical-virtual) to trick users into providing more personal information,
i.e., dark patterns. Dark patterns are deceiving techniques (e.g., interface
tricks) designed to influence individuals' behavioural decisions. However,
there are few studies regarding dark patterns' potential issues in AR
environments. In this work, using scenario construction to build our
prototypes, we investigate the potential future approaches that dark patterns
can have. We use VR mockups in our user study to analyze the effects of dark
patterns in AR. Our study indicates that dark patterns are effective in
immersive scenarios, and the use of novel techniques such as `haptic grabbing'
to drag participants' attention can influence their movements. Finally, we
discuss the impact of such malicious techniques and what techniques can
mitigate them.","['Xian Wang', 'Lik-Hang Lee', 'Carlos Bermejo Fernandez', 'Pan Hui']",2023-03-06T02:40:43Z,http://arxiv.org/abs/2303.02843v2,['cs.HC']
"Enhancing Human-robot Collaboration by Exploring Intuitive Augmented
  Reality Design Representations","As the use of Augmented Reality (AR) to enhance interactions between human
agents and robotic systems in a work environment continues to grow, robots must
communicate their intents in informative yet straightforward ways. This
improves the human agent's feeling of trust and safety in the work environment
while also reducing task completion time. To this end, we discuss a set of
guidelines for the systematic design of AR interfaces for Human-Robot
Interaction (HRI) systems. Furthermore, we develop design frameworks that would
ride on these guidelines and serve as a base for researchers seeking to explore
this direction further. We develop a series of designs for visually
representing the robot's planned path and reactions, which we evaluate by
conducting a user survey involving 14 participants. Subjects were given
different design representations to review and rate based on their
intuitiveness and informativeness. The collated results showed that our design
representations significantly improved the participants' ease of understanding
the robot's intents over the baselines for the robot's proposed navigation
path, planned arm trajectory, and reactions.","['Chrisantus Eze', 'Christopher Crick']",2023-03-09T19:03:59Z,http://arxiv.org/abs/2303.05539v1,"['cs.RO', 'cs.HC']"
"Adaptive Local Adversarial Attacks on 3D Point Clouds for Augmented
  Reality","As the key technology of augmented reality (AR), 3D recognition and tracking
are always vulnerable to adversarial examples, which will cause serious
security risks to AR systems. Adversarial examples are beneficial to improve
the robustness of the 3D neural network model and enhance the stability of the
AR system. At present, most 3D adversarial attack methods perturb the entire
point cloud to generate adversarial examples, which results in high
perturbation costs and difficulty in reconstructing the corresponding real
objects in the physical world. In this paper, we propose an adaptive local
adversarial attack method (AL-Adv) on 3D point clouds to generate adversarial
point clouds. First, we analyze the vulnerability of the 3D network model and
extract the salient regions of the input point cloud, namely the vulnerable
regions. Second, we propose an adaptive gradient attack algorithm that targets
vulnerable regions. The proposed attack algorithm adaptively assigns different
disturbances in different directions of the three-dimensional coordinates of
the point cloud. Experimental results show that our proposed method AL-Adv
achieves a higher attack success rate than the global attack method.
Specifically, the adversarial examples generated by the AL-Adv demonstrate good
imperceptibility and small generation costs.","['Weiquan Liu', 'Shijun Zheng', 'Cheng Wang']",2023-03-12T11:52:02Z,http://arxiv.org/abs/2303.06641v1,"['cs.CV', 'eess.IV']"
"GazePair: Efficient Pairing of Augmented Reality Devices Using Gaze
  Tracking","As Augmented Reality (AR) devices become more prevalent and commercially
viable, the need for quick, efficient, and secure schemes for pairing these
devices has become more pressing. Current methods to securely exchange
holograms require users to send this information through large data centers,
creating security and privacy concerns. Existing techniques to pair these
devices on a local network and share information fall short in terms of
usability and scalability. These techniques either require hardware not
available on AR devices, intricate physical gestures, removal of the device
from the head, do not scale to multiple pairing partners, or rely on methods
with low entropy to create encryption keys. To that end, we propose a novel
pairing system, called GazePair, that improves on all existing local pairing
techniques by creating an efficient, effective, and intuitive pairing protocol.
GazePair uses eye gaze tracking and a spoken key sequence cue (KSC) to generate
identical, independently generated symmetric encryption keys with 64 bits of
entropy. GazePair also achieves improvements in pairing success rates and times
over current methods. Additionally, we show that GazePair can extend to
multiple users. Finally, we assert that GazePair can be used on any Mixed
Reality (MR) device equipped with eye gaze tracking.","['Matthew Corbett', 'Jiacheng Shang', 'Bo Ji']",2023-03-13T18:32:32Z,http://arxiv.org/abs/2303.07404v1,['cs.CR']
"Investigating the Characteristics and Performance of Augmented Reality
  Applications on Head-Mounted Displays: A Study of the Hololens Application
  Store","Augmented Reality (AR) based on Head-Mounted Displays (HMDs) has gained
significant traction over the recent years. Nevertheless, it remains unclear
what AR HMD-based applications have been developed over the years and what
their system performance is when they are run on HMDs. In this paper, we aim to
shed light into this direction. Our study focuses on the applications available
on the Microsoft Hololens application store given the wide use of the Hololens
headset. Our study has two major parts: (i) we collect metadata about the
applications available on the Microsoft Hololens application store to
understand their characteristics (e.g., categories, pricing, permissions
requested, hardware and software compatibility); and (ii) we interact with
these applications while running on a Hololens 2 headset and collect data about
systems-related metrics (e.g., memory and storage usage, time spent on CPU and
GPU related operations) to investigate the systems performance of applications.
Our study has resulted in several interesting findings, which we share with the
research community.","['Pubudu Wijesooriya', 'Sheikh Muhammad Farjad', 'Nikolaos Stergiou', 'Spyridon Mastorakis']",2023-03-13T23:18:17Z,http://arxiv.org/abs/2303.07523v1,"['cs.MM', 'cs.PF', 'cs.SE']"
"Supporting Piggybacked Co-Located Leisure Activities via Augmented
  Reality","Technology, especially the smartphone, is villainized for taking meaning and
time away from in-person interactions and secluding people into ""digital
bubbles"". We believe this is not an intrinsic property of digital gadgets, but
evidence of a lack of imagination in technology design. Leveraging augmented
reality (AR) toward this end allows us to create experiences for multiple
people, their pets, and their environments. In this work, we explore the design
of AR technology that ""piggybacks"" on everyday leisure to foster co-located
interactions among close ties (with other people and pets. We designed,
developed, and deployed three such AR applications, and evaluated them through
a 41-participant and 19-pet user study. We gained key insights about the
ability of AR to spur and enrich interaction in new channels, the importance of
customization, and the challenges of designing for the physical aspects of AR
devices (e.g., holding smartphones). These insights guide design implications
for the novel research space of co-located AR.","['Samantha Reig', 'Erica Principe Cruz', 'Melissa M. Powers', 'Jennifer He', 'Timothy Chong', 'Yu Jiang Tham', 'Sven Kratz', 'Ava Robinson', 'Brian A. Smith', 'Rajan Vaish', 'Andrés Monroy-Hernández']",2023-03-19T03:09:08Z,http://arxiv.org/abs/2303.10546v1,"['cs.HC', 'cs.CY']"
"Augmented Reality in Service of Human Operations on the Moon: Insights
  from a Virtual Testbed","Future astronauts living and working on the Moon will face extreme
environmental conditions impeding their operational safety and performance.
While it has been suggested that Augmented Reality (AR) Head-Up Displays (HUDs)
could potentially help mitigate some of these adversities, the applicability of
AR in the unique lunar context remains underexplored. To address this
limitation, we have produced an accurate representation of the lunar setting in
virtual reality (VR) which then formed our testbed for the exploration of
prospective operational scenarios with aerospace experts. Herein we present
findings based on qualitative reflections made by the first 6 study
participants. AR was found instrumental in several use cases, including the
support of navigation and risk awareness. Major design challenges were likewise
identified, including the importance of redundancy and contextual
appropriateness. Drawing on these findings, we conclude by outlining directions
for future research aimed at developing AR-based assistive solutions tailored
to the lunar setting.","['Leonie Becker', 'Tommy Nilsson', 'Paul Topf Aguiar de Medeiros', 'Flavie Rometsch']",2023-03-19T15:32:14Z,http://arxiv.org/abs/2303.10686v1,"['cs.HC', 'cs.CY', '93B51, 97M50', 'H.1.2; H.5.2; I.3.8; J.m; K.8.2; J.6']"
"Augmented reality as a Thirdspace: Simultaneous experience of the
  physical and virtual","With the proliferation of devices that display augmented reality (AR), now is
the time for scholars and practitioners to evaluate and engage critically with
emerging applications of the medium. AR mediates the way users see their
bodies, hear their environment and engage with places. Applied in various
forms, including social media, e-commerce, gaming, enterprise and art, the
medium facilitates a hybrid experience of physical and digital spaces. This
article employs a model of real-and-imagined space from geographer Edward Soja
to examine how the user of an AR app navigates the two intertwined spaces of
physical and digital, experiencing what Soja calls a 'Third-space'. The article
illustrates the potential for headset-based AR to engender such a Thirdspace
through the author's practice-led research project, the installation Through
the Wardrobe. This installation demonstrates how AR has the potential to shift
the way that users view and interact with their world with artistic
applications providing an opportunity to question assumptions of social norms,
identity and uses of physical space.",['Rob Eagle'],2023-03-21T22:46:22Z,http://arxiv.org/abs/2303.13550v1,"['cs.HC', 'cs.MM']"
Cross-View Visual Geo-Localization for Outdoor Augmented Reality,"Precise estimation of global orientation and location is critical to ensure a
compelling outdoor Augmented Reality (AR) experience. We address the problem of
geo-pose estimation by cross-view matching of query ground images to a
geo-referenced aerial satellite image database. Recently, neural network-based
methods have shown state-of-the-art performance in cross-view matching.
However, most of the prior works focus only on location estimation, ignoring
orientation, which cannot meet the requirements in outdoor AR applications. We
propose a new transformer neural network-based model and a modified triplet
ranking loss for joint location and orientation estimation. Experiments on
several benchmark cross-view geo-localization datasets show that our model
achieves state-of-the-art performance. Furthermore, we present an approach to
extend the single image query-based geo-localization approach by utilizing
temporal information from a navigation pipeline for robust continuous
geo-localization. Experimentation on several large-scale real-world video
sequences demonstrates that our approach enables high-precision and stable AR
insertion.","['Niluthpol Chowdhury Mithun', 'Kshitij Minhas', 'Han-Pang Chiu', 'Taragay Oskiper', 'Mikhail Sizintsev', 'Supun Samarasekera', 'Rakesh Kumar']",2023-03-28T01:58:03Z,http://arxiv.org/abs/2303.15676v1,"['cs.CV', 'cs.GR']"
"Exploring the Design Space of Employing AI-Generated Content for
  Augmented Reality Display","As the two most common display content of Augmented Reality (AR), the
creation process of image and text often requires a human to execute. However,
due to the rapid advances in Artificial Intelligence (AI), today the media
content can be automatically generated by software. The ever-improving quality
of AI-generated content (AIGC) has opened up new scenarios employing such
content, which is expected to be applied in AR. In this paper, we attempt to
explore the design space for projecting AI-generated image and text into an AR
display. Specifically, we perform an exploratory study and suggest a
``user-function-environment'' design thinking by building a preliminary
prototype and conducting focus groups based on it. With the early insights
presented, we point out the design space and potential applications for
combining AIGC and AR.","['Yongquan Hu', 'Mingyue Yuan', 'Kaiqi Xian', 'Don Samitha Elvitigala', 'Aaron Quigley']",2023-03-29T11:07:27Z,http://arxiv.org/abs/2303.16593v1,['cs.HC']
"Multimodal Grounding for Embodied AI via Augmented Reality Headsets for
  Natural Language Driven Task Planning","Recent advances in generative modeling have spurred a resurgence in the field
of Embodied Artificial Intelligence (EAI). EAI systems typically deploy large
language models to physical systems capable of interacting with their
environment. In our exploration of EAI for industrial domains, we successfully
demonstrate the feasibility of co-located, human-robot teaming. Specifically,
we construct an experiment where an Augmented Reality (AR) headset mediates
information exchange between an EAI agent and human operator for a variety of
inspection tasks. To our knowledge the use of an AR headset for multimodal
grounding and the application of EAI to industrial tasks are novel
contributions within Embodied AI research. In addition, we highlight potential
pitfalls in EAI's construction by providing quantitative and qualitative
analysis on prompt robustness.","['Selma Wanna', 'Fabian Parra', 'Robert Valner', 'Karl Kruusamäe', 'Mitch Pryor']",2023-04-26T16:44:19Z,http://arxiv.org/abs/2304.13676v1,"['cs.RO', 'I.2.9; I.2.7; I.2.4']"
"ARDIE: AR, Dialogue, and Eye Gaze Policies for Human-Robot Collaboration","Human-robot collaboration (HRC) has become increasingly relevant in
industrial, household, and commercial settings. However, the effectiveness of
such collaborations is highly dependent on the human and robots' situational
awareness of the environment. Improving this awareness includes not only
aligning perceptions in a shared workspace, but also bidirectionally
communicating intent and visualizing different states of the environment to
enhance scene understanding. In this paper, we propose ARDIE (Augmented Reality
with Dialogue and Eye Gaze), a novel intelligent agent that leverages
multi-modal feedback cues to enhance HRC. Our system utilizes a decision
theoretic framework to formulate a joint policy that incorporates interactive
augmented reality (AR), natural language, and eye gaze to portray current and
future states of the environment. Through object-specific AR renders, the human
can visualize future object interactions to make adjustments as needed,
ultimately providing an interactive and efficient collaboration between humans
and robots.","['Chelsea Zou', 'Kishan Chandan', 'Yan Ding', 'Shiqi Zhang']",2023-05-08T13:01:27Z,http://arxiv.org/abs/2305.04685v1,['cs.RO']
"Digital Twin-Based 3D Map Management for Edge-Assisted Mobile Augmented
  Reality","In this paper, we design a 3D map management scheme for edge-assisted mobile
augmented reality (MAR) to support the pose estimation of individual MAR
device, which uploads camera frames to an edge server. Our objective is to
minimize the pose estimation uncertainty of the MAR device by periodically
selecting a proper set of camera frames for uploading to update the 3D map. To
address the challenges of the dynamic uplink data rate and the time-varying
pose of the MAR device, we propose a digital twin (DT)-based approach to 3D map
management. First, a DT is created for the MAR device, which emulates 3D map
management based on predicting subsequent camera frames. Second, a model-based
reinforcement learning (MBRL) algorithm is developed, utilizing the data
collected from both the actual and the emulated data to manage the 3D map. With
extensive emulated data provided by the DT, the MBRL algorithm can quickly
provide an adaptive map management policy in a highly dynamic environment.
Simulation results demonstrate that the proposed DT-based 3D map management
outperforms benchmark schemes by achieving lower pose estimation uncertainty
and higher data efficiency in dynamic environments.","['Conghao Zhou', 'Jie Gao', 'Mushu Li', 'Nan Cheng', 'Xuemin Shen', 'Weihua Zhuang']",2023-05-26T01:38:45Z,http://arxiv.org/abs/2305.16571v1,"['cs.NI', 'cs.AI']"
"Look-Ahead Task Offloading for Multi-User Mobile Augmented Reality in
  Edge-Cloud Computing","Mobile augmented reality (MAR) blends a real scenario with overlaid virtual
content, which has been envisioned as one of the ubiquitous interfaces to the
Metaverse. Due to the limited computing power and battery life of MAR devices,
it is common to offload the computation tasks to edge or cloud servers in close
proximity. However, existing offloading solutions developed for MAR tasks
suffer from high migration overhead, poor scalability, and short-sightedness
when applied in provisioning multi-user MAR services. To address these issues,
a MAR service-oriented task offloading scheme is designed and evaluated in
edge-cloud computing networks. Specifically, the task interdependency of MAR
applications is firstly analyzed and modeled by using directed acyclic graphs.
Then, we propose a look-ahead offloading scheme based on a modified Monte Carlo
tree (MMCT) search, which can run several multi-step executions in advance to
get an estimate of the long-term effect of immediate action. Experiment results
show that the proposed offloading scheme can effectively improve the quality of
service (QoS) in provisioning multi-user MAR services, compared to four
benchmark schemes. Furthermore, it is also shown that the proposed solution is
stable and suitable for applications in a highly volatile environment.","['Ruxiao Chen', 'Shuaishuai Guo']",2023-05-31T05:03:40Z,http://arxiv.org/abs/2305.19558v1,"['eess.SP', 'cs.AI']"
"Real-Time Onboard Object Detection for Augmented Reality: Enhancing
  Head-Mounted Display with YOLOv8","This paper introduces a software architecture for real-time object detection
using machine learning (ML) in an augmented reality (AR) environment. Our
approach uses the recent state-of-the-art YOLOv8 network that runs onboard on
the Microsoft HoloLens 2 head-mounted display (HMD). The primary motivation
behind this research is to enable the application of advanced ML models for
enhanced perception and situational awareness with a wearable, hands-free AR
platform. We show the image processing pipeline for the YOLOv8 model and the
techniques used to make it real-time on the resource-limited edge computing
platform of the headset. The experimental results demonstrate that our solution
achieves real-time processing without needing offloading tasks to the cloud or
any other external servers while retaining satisfactory accuracy regarding the
usual mAP metric and measured qualitative performance","['Mikołaj Łysakowski', 'Kamil Żywanowski', 'Adam Banaszczyk', 'Michał R. Nowicki', 'Piotr Skrzypczyński', 'Sławomir K. Tadeja']",2023-06-06T09:35:45Z,http://arxiv.org/abs/2306.03537v1,"['cs.CV', 'cs.HC']"
"ARCOR2: Framework for Collaborative End-User Management of Industrial
  Robotic Workplaces using Augmented Reality","This paper presents a novel framework enabling end-users to perform the
management of complex robotic workplaces using a tablet and augmented reality.
The framework allows users to commission the workplace comprising different
types of robots, machines, or services irrespective of the vendor, set
task-important points in space, specify program steps, generate a code, and
control its execution. More users can collaborate simultaneously, for instance,
within a large-scale workplace. Spatially registered visualization and
programming enable a fast and easy understanding of workplace processes, while
high precision is achieved by combining kinesthetic teaching with specific
graphical tools for relative manipulation of poses. A visually defined program
is for execution translated into Python representation, allowing efficient
involvement of experts. The system was designed and developed in cooperation
with a system integrator based on an offline printed circuit board testing use
case, and its user interface was evaluated multiple times during the
development. The latest evaluation was performed by three experts and indicates
the high potential of the solution.","['Michal Kapinus', 'Zdeněk Materna', 'Daniel Bambušek', 'Vítězslav Beran', 'Pavel Smrž']",2023-06-14T12:20:23Z,http://arxiv.org/abs/2306.08464v1,"['cs.RO', 'cs.HC']"
"A Design Approach and Prototype Implementation for Factory Monitoring
  Based on Virtual and Augmented Reality at the Edge of Industry 4.0","Virtual and augmented reality are currently enjoying a great deal of
attention from the research community and the industry towards their adoption
within industrial spaces and processes. However, the current design and
implementation landscape is still very fluid, while the community as a whole
has not yet consolidated into concrete design directions, other than basic
patterns. Other open issues include the choice over a cloud or edge-based
architecture when designing such systems. Within this work, we present our
approach for a monitoring intervention inside a factory space utilizing both VR
and AR, based primarily on edge computing, while also utilizing the cloud. We
discuss its main design directions, as well as a basic ontology to aid in
simple description of factory assets. In order to highlight the design aspects
of our approach, we present a prototype implementation, based on a use case
scenario in a factory site, within the context of the ENERMAN H2020 project.","['Christos Anagnostopoulos', 'Georgios Mylonas', 'Apostolos P. Fournaris', 'Christos Koulamas']",2023-06-16T08:50:08Z,http://arxiv.org/abs/2306.09692v1,"['cs.HC', 'cs.SY', 'eess.SY']"
"Performance Evaluation of Transport Protocols and Roadmap to a
  High-Performance Transport Design for Immersive Applications","Immersive technologies such as virtual reality (VR), augmented reality (AR),
and holograms will change users' digital experience. These immersive
technologies have a multitude of applications, including telesurgeries,
teleconferencing, Internet shopping, computer games, etc. Holographic-type
communication (HTC) is a type of augmented reality media that provides an
immersive experience to Internet users. However, HTC has different
characteristics and network requirements, and the existing network architecture
and transport protocols may not be able to cope with the stringent network
requirements of HTC. Therefore, in this paper, we provide an in-depth and
critical study of the transport protocols for HTC. We also discuss the
characteristics and the network requirements for HTC. Based on the performance
evaluation of the existing transport protocols, we propose a roadmap to design
new high-performance transport protocols for immersive applications.","['Inayat Ali', 'Seungwoo Hong', 'Pyung-koo Park', 'Tae Yeon Kim']",2023-06-29T05:31:02Z,http://arxiv.org/abs/2306.16692v2,['cs.NI']
"Evaluation of AI-Supported Input Methods in Augmented Reality
  Environment","Augmented Reality (AR) solutions are providing tools that could improve
applications in the medical and industrial fields. Augmentation can provide
additional information in training, visualization, and work scenarios, to
increase efficiency, reliability, and safety, while improving communication
with other devices and systems on the network. Unfortunately, tasks in these
fields often require both hands to execute, reducing the variety of input
methods suitable to control AR applications. People with certain physical
disabilities, where they are not able to use their hands, are also negatively
impacted when using these devices. The goal of this work is to provide novel
hand-free interfacing methods, using AR technology, in association with AI
support approaches to produce an improved Human-Computer interaction solution.","['Akos Nagy', 'Thomas Lagkas', 'Panagiotis Sarigiannidis', 'Vasileios Argyriou']",2023-06-29T17:34:42Z,http://arxiv.org/abs/2306.17132v1,['cs.HC']
"Assessing Augmented Reality Selection Techniques for Passengers in
  Moving Vehicles: A Real-World User Study","Nowadays, cars offer many possibilities to explore the world around you by
providing location-based information displayed on a 2D-Map. However, this
information is often only available to front-seat passengers while being
restricted to in-car displays. To propose a more natural way of interacting
with the environment, we implemented an augmented reality head-mounted display
to overlay points of interest onto the real world. We aim to compare multiple
selection techniques for digital objects located outside a moving car by
investigating head gaze with dwell time, head gaze with hardware button, eye
gaze with hardware button, and hand pointing with gesture confirmation. Our
study was conducted in a moving car under real-world conditions (N=22), with
significant results indicating that hand pointing usage led to slower and less
precise content selection while eye gaze was preferred by participants and
performed on par with the other techniques.","['Robin Connor Schramm', 'Markus Sasalovici', 'Axel Hildebrand', 'Ulrich Schwanecke']",2023-07-12T13:58:11Z,http://arxiv.org/abs/2307.06173v1,['cs.HC']
Design Patterns for Situated Visualization in Augmented Reality,"Situated visualization has become an increasingly popular research area in
the visualization community, fueled by advancements in augmented reality (AR)
technology and immersive analytics. Visualizing data in spatial proximity to
their physical referents affords new design opportunities and considerations
not present in traditional visualization, which researchers are now beginning
to explore. However, the AR research community has an extensive history of
designing graphics that are displayed in highly physical contexts. In this
work, we leverage the richness of AR research and apply it to situated
visualization. We derive design patterns which summarize common approaches of
visualizing data in situ. The design patterns are based on a survey of 293
papers published in the AR and visualization communities, as well as our own
expertise. We discuss design dimensions that help to describe both our patterns
and previous work in the literature. This discussion is accompanied by several
guidelines which explain how to apply the patterns given the constraints
imposed by the real world. We conclude by discussing future research directions
that will help establish a complete understanding of the design of situated
visualization, including the role of interactivity, tasks, and workflows.","['Benjamin Lee', 'Michael Sedlmair', 'Dieter Schmalstieg']",2023-07-18T11:34:28Z,http://arxiv.org/abs/2307.09157v2,['cs.HC']
"Using Abstract Tangible Proxy Objects for Interaction in Optical
  See-through Augmented Reality","Interaction with virtual objects displayed in Optical See-through Augmented
Reality is still mostly done with controllers or hand gestures. A much more
intuitive way of interacting with virtual content is to use physical proxy
objects to interact with the virtual objects. Here, the virtual model is
superimposed on a physical object, which can then be touched and moved to
interact with the virtual object. Since it is not possible to use an exact
replica as a tangible proxy object for every use case, we conducted a study to
determine the extent to which the shape of the physical object can deviate from
the shape of the virtual object without massively impacting performance and
usability, as well as the sense of presence. Our study, in which we
investigated different levels of abstraction for a sofa model, shows that the
physical proxy object can be abstracted to a certain degree. At the same time,
our results indicate that the physical object must have at least a similar
shape as the virtual object in order to serve as a suitable proxy.","['Denise Kahl', 'Antonio Krüger']",2023-08-10T19:34:55Z,http://arxiv.org/abs/2308.05836v1,"['cs.HC', 'H.5.m']"
"Comparative Analysis of Change Blindness in Virtual Reality and
  Augmented Reality Environments","Change blindness is a phenomenon where an individual fails to notice
alterations in a visual scene when a change occurs during a brief interruption
or distraction. Understanding this phenomenon is specifically important for the
technique that uses a visual stimulus, such as Virtual Reality (VR) or
Augmented Reality (AR). Previous research had primarily focused on 2D
environments or conducted limited controlled experiments in 3D immersive
environments. In this paper, we design and conduct two formal user experiments
to investigate the effects of different visual attention-disrupting conditions
(Flickering and Head-Turning) and object alternative conditions (Removal, Color
Alteration, and Size Alteration) on change blindness detection in VR and AR
environments. Our results reveal that participants detected changes more
quickly and had a higher detection rate with Flickering compared to
Head-Turning. Furthermore, they spent less time detecting changes when an
object disappeared compared to changes in color or size. Additionally, we
provide a comparison of the results between VR and AR environments.","['DongHoon Kim', 'Dongyun Han', 'Isaac Cho']",2023-08-24T00:08:39Z,http://arxiv.org/abs/2308.12476v3,['cs.HC']
Augmented Reality in Higher Education: a Case Study in Medical Education,"During lockdown, we piloted a variety of augmented reality (AR) experiences
in collaboration with subject matter experts from different fields aiming at
creating remote teaching and training experiences. In this paper, we present a
case study on how AR can be used as a teaching aid for medical education with
pertinent focus on remote and social distanced learning. We describe the
process of creating an AR experience that can enhance the knowledge and
understanding of anatomy for medical students. The Anatomy Experience is an AR
enhanced learning experience developed in collaboration with the Medical School
of the University of Edinburgh aiming to assist medical students understand the
complex geometry of different parts of the human body. After conducting a focus
group study with medical students, trainees, and trainers, we received very
positive feedback on the Anatomy Experience and its effects on understanding
anatomy, enriching the learning process, and using it as a tool for anatomy
teaching.","['Danai Korre', 'Andrew Sherlock']",2023-08-30T18:11:58Z,http://arxiv.org/abs/2308.16248v1,"['cs.HC', 'cs.ET', 'K.3.1; H.5.1']"
Chinese herb medicine in augmented reality,"Augmented reality becomes popular in education gradually, which provides a
contextual and adaptive learning experience. Here, we develop a Chinese herb
medicine AR platform based the 3dsMax and the Unity that allows users to
visualize and interact with the herb model and learn the related information.
The users use their mobile camera to scan the 2D herb picture to trigger the
presentation of 3D AR model and corresponding text information on the screen in
real-time. The system shows good performance and has high accuracy for the
identification of herbal medicine after interference test and occlusion test.
Users can interact with the herb AR model by rotating, scaling, and viewing
transformation, which effectively enhances learners' interest in Chinese herb
medicine.","['Qianyun Zhu', 'Yifeng Xie', 'Fangyang Ye', 'Zhenyuan Gao', 'Binjie Che', 'Zhenglin Chen', 'Dongmei Yu']",2023-09-25T07:12:58Z,http://arxiv.org/abs/2309.13909v1,['cs.HC']
"Breamy: An augmented reality mHealth prototype for surgical
  decision-making in breast cancer","In 2020, according to WHO, breast cancer affected 2.3 million women
worldwide, resulting in 685,000 fatalities. By the end of the year,
approximately 7.8 million women worldwide had survived their breast cancer
making it the most widespread form of cancer globally. Surgical treatment
decisions, including choosing between oncoplastic options, often require quick
decision-making within an 8-week time frame. However, many women lack the
necessary knowledge and preparation for making such complex informed decisions.
Anxiety and unsatisfactory outcomes can result from inadequate decision-making
processes, leading to complications and the need for revision surgeries. Shared
decision-making and personalized decision aids have shown positive effects on
patient satisfaction and treatment outcomes. This paper introduces Breamy, a
prototype mobile health (mHealth) application that utilizes augmented reality
(AR) technology to assist breast cancer patients in making informed decisions.
The app provides 3D visualizations of different oncoplastic procedures, aiming
to improve confidence in surgical decision-making, reduce decisional regret,
and enhance patient well-being after surgery. To determine the perception of
the usefulness of Breamy, we collected data from 166 participants through an
online survey. The results suggest that Breamy has the potential to reduce
patient's anxiety levels and assist them during the decision-making process.","['Niki Najafi', 'Miranda Addie', 'Sarkis Meterissian', 'Marta Kersten-Oertel']",2023-09-27T17:56:01Z,http://arxiv.org/abs/2309.15893v1,"['cs.HC', 'H.5.2']"
"A Real-time Method for Inserting Virtual Objects into Neural Radiance
  Fields","We present the first real-time method for inserting a rigid virtual object
into a neural radiance field, which produces realistic lighting and shadowing
effects, as well as allows interactive manipulation of the object. By
exploiting the rich information about lighting and geometry in a NeRF, our
method overcomes several challenges of object insertion in augmented reality.
For lighting estimation, we produce accurate, robust and 3D spatially-varying
incident lighting that combines the near-field lighting from NeRF and an
environment lighting to account for sources not covered by the NeRF. For
occlusion, we blend the rendered virtual object with the background scene using
an opacity map integrated from the NeRF. For shadows, with a precomputed field
of spherical signed distance field, we query the visibility term for any point
around the virtual object, and cast soft, detailed shadows onto 3D surfaces.
Compared with state-of-the-art techniques, our approach can insert virtual
object into scenes with superior fidelity, and has a great potential to be
further applied to augmented reality systems.","['Keyang Ye', 'Hongzhi Wu', 'Xin Tong', 'Kun Zhou']",2023-10-09T16:26:34Z,http://arxiv.org/abs/2310.05837v1,"['cs.CV', 'cs.GR']"
MISAR: A Multimodal Instructional System with Augmented Reality,"Augmented reality (AR) requires the seamless integration of visual, auditory,
and linguistic channels for optimized human-computer interaction. While
auditory and visual inputs facilitate real-time and contextual user guidance,
the potential of large language models (LLMs) in this landscape remains largely
untapped. Our study introduces an innovative method harnessing LLMs to
assimilate information from visual, auditory, and contextual modalities.
Focusing on the unique challenge of task performance quantification in AR, we
utilize egocentric video, speech, and context analysis. The integration of LLMs
facilitates enhanced state estimation, marking a step towards more adaptive AR
systems. Code, dataset, and demo will be available at
https://github.com/nguyennm1024/misar.","['Jing Bi', 'Nguyen Manh Nguyen', 'Ali Vosoughi', 'Chenliang Xu']",2023-10-18T04:15:12Z,http://arxiv.org/abs/2310.11699v1,"['cs.CL', 'cs.CV']"
"3D-Mirrorcle: Bridging the Virtual and Real through Depth Alignment in
  AR Mirror Systems","Smart mirrors have emerged as a new form of augmented reality (AR) interface
for home environments. However, due to the parallax in human vision, one major
challenge hindering their development is the depth misalignment between the 3D
mirror reflection and the 2D screen display. This misalignment causes the
display content to appear as if it is floating above the mirror, thereby
disrupting the seamless integration of the two components and impacting the
overall quality and functionality of the mirror. In this study, we introduce
3D-Mirrorcle, an innovative augmented reality (AR) mirror system that
effectively addresses the issue of depth disparity through a hardware-software
co-design on a lenticular grating setup. With our implemented real-time
position adjustment and depth adaptation algorithms, the screen display can be
dynamically aligned to the user's depth perception for a highly realistic and
engaging experience. Our method has been validated through a prototype and
hands-on user experiments that engaged 36 participants, and the results show
significant improvements in terms of accuracy (24.72% $\uparrow$),
immersion(31.4% $\uparrow$), and user satisfaction (44.4% $\uparrow$) compared
to the existing works.","['Yujia Liu', 'Qi Xin', 'Chenzhuo Xiang', 'Yu Zhang', 'Lun Yiu Nie', 'Yingqing Xu']",2023-10-20T16:07:23Z,http://arxiv.org/abs/2310.13617v2,['cs.HC']
"Improving Human Legibility in Collaborative Robot Tasks through
  Augmented Reality and Workspace Preparation","Understanding the intentions of human teammates is critical for safe and
effective human-robot interaction. The canonical approach for human-aware robot
motion planning is to first predict the human's goal or path, and then
construct a robot plan that avoids collision with the human. This method can
generate unsafe interactions if the human model and subsequent predictions are
inaccurate. In this work, we present an algorithmic approach for both arranging
the configuration of objects in a shared human-robot workspace, and projecting
``virtual obstacles'' in augmented reality, optimizing for legibility in a
given task. These changes to the workspace result in more legible human
behavior, improving robot predictions of human goals, thereby improving task
fluency and safety. To evaluate our approach, we propose two user studies
involving a collaborative tabletop task with a manipulator robot, and a
warehouse navigation task with a mobile robot.","['Yi-Shiuan Tung', 'Matthew B. Luebbers', 'Alessandro Roncone', 'Bradley Hayes']",2023-11-09T18:18:28Z,http://arxiv.org/abs/2311.05562v1,['cs.RO']
Use of Augmented Reality in Human Wayfinding: A Systematic Review,"Augmented reality technology has emerged as a promising solution to assist
with wayfinding difficulties, bridging the gap between obtaining navigational
assistance and maintaining an awareness of one's real-world surroundings. This
article presents a systematic review of research literature related to AR
navigation technologies. An in-depth analysis of 65 salient studies was
conducted, addressing four main research topics: 1) current state-of-the-art of
AR navigational assistance technologies, 2) user experiences with these
technologies, 3) the effect of AR on human wayfinding performance, and 4)
impacts of AR on human navigational cognition. Notably, studies demonstrate
that AR can decrease cognitive load and improve cognitive map development, in
contrast to traditional guidance modalities. However, findings regarding
wayfinding performance and user experience were mixed. Some studies suggest
little impact of AR on improving outdoor navigational performance, and certain
information modalities may be distracting and ineffective. This article
discusses these nuances in detail, supporting the conclusion that AR holds
great potential in enhancing wayfinding by providing enriched navigational
cues, interactive experiences, and improved situational awareness.","['Zhiwen Qiu', 'Armin Mostafavi', 'Saleh Kalantari']",2023-11-20T17:00:44Z,http://arxiv.org/abs/2311.11923v1,['cs.HC']
"NavMarkAR: A Landmark-based Augmented Reality (AR) Wayfinding System for
  Enhancing Spatial Learning of Older Adults","Wayfinding in complex indoor environments is often challenging for older
adults due to declines in navigational and spatial-cognition abilities. This
paper introduces NavMarkAR, an augmented reality navigation system designed for
smart-glasses to provide landmark-based guidance, aiming to enhance older
adults' spatial navigation skills. This work addresses a significant gap in
design research, with limited prior studies evaluating cognitive impacts of AR
navigation systems. An initial usability test involved 6 participants, leading
to prototype refinements, followed by a comprehensive study with 32
participants in a university setting. Results indicate improved wayfinding
efficiency and cognitive map accuracy when using NavMarkAR. Future research
will explore long-term cognitive skill retention with such navigational aids.","['Zhiwen Qiu', 'Mojtaba Ashour', 'Xiaohe Zhou', 'Saleh Kalantari']",2023-11-20T22:23:03Z,http://arxiv.org/abs/2311.12220v2,['cs.HC']
"Multi-3D-Models Registration-Based Augmented Reality (AR) Instructions
  for Assembly","This paper introduces a novel, markerless, step-by-step, in-situ 3D Augmented
Reality (AR) instruction method and its application - BRICKxAR (Multi 3D
Models/M3D) - for small parts assembly. BRICKxAR (M3D) realistically visualizes
rendered 3D assembly parts at the assembly location of the physical assembly
model (Figure 1). The user controls the assembly process through a user
interface. BRICKxAR (M3D) utilizes deep learning-trained 3D model-based
registration. Object recognition and tracking become challenging as the
assembly model updates at each step. Additionally, not every part in a 3D
assembly may be visible to the camera during the assembly. BRICKxAR (M3D)
combines multiple assembly phases with a step count to address these
challenges. Thus, using fewer phases simplifies the complex assembly process
while step count facilitates accurate object recognition and precise
visualization of each step. A testing and heuristic evaluation of the BRICKxAR
(M3D) prototype and qualitative analysis were conducted with users and experts
in visualization and human-computer interaction. Providing robust 3D AR
instructions and allowing the handling of the assembly model, BRICKxAR (M3D)
has the potential to be used at different scales ranging from manufacturing
assembly to construction.","['Seda Tuzun Canadinc', 'Wei Yan']",2023-11-27T21:53:17Z,http://arxiv.org/abs/2311.16337v2,"['cs.HC', 'cs.CV']"
"Optimization in Mobile Augmented Reality Systems for the Metaverse over
  Wireless Communications","As the essential technical support for Metaverse, Mobile Augmented Reality
(MAR) has attracted the attention of many researchers. MAR applications rely on
real-time processing of visual and audio data, and thus those heavy workloads
can quickly drain the battery of a mobile device. To address such problem,
edge-based solutions have appeared for handling some tasks that require more
computing power. However, such strategies introduce a new trade-off: reducing
the network latency and overall energy consumption requires limiting the size
of the data sent to the edge server, which, in turn, results in lower accuracy.
In this paper, we design an edge-based MAR system and propose a mathematical
model to describe it and analyze the trade-off between latency, accuracy,
server resources allocation and energy consumption. Furthermore, an algorithm
named LEAO is proposed to solve this problem. We evaluate the performance of
the LEAO and other related algorithms across various simulation scenarios. The
results demonstrate the superiority of the LEAO algorithm. Finally, our work
provides insight into optimization problem in edge-based MAR system for
Metaverse.","['Tianming Lan', 'Jun Zhao']",2023-11-29T13:44:15Z,http://arxiv.org/abs/2311.17630v1,"['cs.NI', 'eess.SP']"
"Evaluating Augmented Reality Communication: How Can We Teach Procedural
  Skill in AR?","Augmented reality (AR) has great potential for use in healthcare
applications, especially remote medical training and supervision. In this
paper, we analyze the usage of an AR communication system to teach a medical
procedure, the placement of a central venous catheter (CVC) under ultrasound
guidance. We examine various AR communication and collaboration components,
including gestural communication, volumetric information, annotations,
augmented objects, and augmented screens. We compare how teaching in AR differs
from teaching through videoconferencing-based communication. Our results
include a detailed medical training steps analysis in which we compare how
verbal and visual communication differs between video and AR training. We
identify procedural steps in which medical experts give visual instructions
utilizing AR components. We examine the change in AR usage and interaction over
time and recognize patterns between users. Moreover, AR design recommendations
are given based on post-training interviews.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Neal Sikka', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Puja Sasankan', 'Christian Gütl']",2023-12-14T17:22:22Z,http://arxiv.org/abs/2312.09152v1,['cs.HC']
"Perception in Pixels: Understanding Avatar Representation in
  Video-Mediated Collaborative Interactions","Despite the abundance of research concerning virtual reality (VR) avatars,
the impact of screen-based or augmented reality (AR) avatars for real-world
applications remain relatively unexplored. Notably, there is a lack of research
examining video-mediated collaborative interaction experiences using AR avatars
for goal-directed group activities. This study bridges this gap with a
mixed-methods, quasi-experimental user study that investigates video-based
small-group interactions when employing AR avatars as opposed to traditional
video for user representation. We found that the use of avatars positively
influenced self-esteem and video-based collaboration satisfaction. In addition,
our group interview findings highlight experiences and perceptions regarding
the dynamic use of avatars in video-mediated collaborative interactions,
including benefits, challenges, and factors that would influence a decision to
use avatars. This study contributes an empirical understanding of avatar
representation in mediating video-based collaborative interactions,
implications and perceptions surrounding the adoption of AR avatars, and a
comprehensive comparison of key characteristics between user representations.","['Pitch Sinlapanuntakul', 'Mark Zachry']",2024-05-06T20:48:37Z,http://arxiv.org/abs/2405.03844v1,"['cs.HC', 'H.5.1; H.5.3; J.4']"
From Virtual Gains to Real Pains: Potential Harms of Immersive Exergames,"Digitalization and virtualization are parts of our everyday lives in almost
all aspects ranging from work, education, and communication to entertainment. A
novel step in this direction is the widespread interest in extended reality
(XR) [2]. The newest consumer-ready head-mounted displays (HMD) such as Meta
Quest 3 or Apple Vision Pro, have reached unprecedented levels of visual
fidelity, interaction capabilities, and computational power. The built-in
pass-through features of these headsets enable both virtual reality (VR) and
augmented reality (AR) with the same devices. However, the immersive nature of
these experiences is not the only groundbreaking difference from established
forms of media.","['Sebastian Cmentowski', 'Sukran Karaosmanoglu', 'Frank Steinicke']",2024-04-23T17:48:59Z,http://arxiv.org/abs/2405.05915v1,['cs.HC']
Unveiling the Era of Spatial Computing,"The evolution of User Interfaces marks a significant transition from
traditional command-line interfaces to more intuitive graphical and touch-based
interfaces, largely driven by the emergence of personal computing devices. The
advent of spatial computing and Extended Reality technologies further pushes
the boundaries, promising a fusion of physical and digital realms through
interactive environments. This paper delves into the progression from All
Realities technologies encompassing Augmented Reality, Virtual Reality, and
Mediated Reality to spatial computing, highlighting their conceptual
differences and applications. We explore enabling technologies such as
Artificial Intelligence, the Internet of Things, 5G, cloud and edge computing,
and blockchain that underpin the development of spatial computing. We further
scrutinize the initial forays into commercial spatial computing devices, with a
focus on Apple's Vision Pro, evaluating its technological advancements
alongside the challenges it faces. Through this examination, we aim to provide
insights into the potential of spatial computing to revolutionize our
interaction with digital information and the physical world.",['Hanzhong Cao'],2024-05-11T03:44:45Z,http://arxiv.org/abs/2405.06895v1,['cs.HC']
"EndoDAC: Efficient Adapting Foundation Model for Self-Supervised Depth
  Estimation from Any Endoscopic Camera","Depth estimation plays a crucial role in various tasks within endoscopic
surgery, including navigation, surface reconstruction, and augmented reality
visualization. Despite the significant achievements of foundation models in
vision tasks, including depth estimation, their direct application to the
medical domain often results in suboptimal performance. This highlights the
need for efficient adaptation methods to adapt these models to endoscopic depth
estimation. We propose Endoscopic Depth Any Camera (EndoDAC) which is an
efficient self-supervised depth estimation framework that adapts foundation
models to endoscopic scenes. Specifically, we develop the Dynamic Vector-Based
Low-Rank Adaptation (DV-LoRA) and employ Convolutional Neck blocks to tailor
the foundational model to the surgical domain, utilizing remarkably few
trainable parameters. Given that camera information is not always accessible,
we also introduce a self-supervised adaptation strategy that estimates camera
intrinsics using the pose encoder. Our framework is capable of being trained
solely on monocular surgical videos from any camera, ensuring minimal training
costs. Experiments demonstrate that our approach obtains superior performance
even with fewer training epochs and unaware of the ground truth camera
intrinsics. Code is available at https://github.com/BeileiCui/EndoDAC.","['Beilei Cui', 'Mobarakol Islam', 'Long Bai', 'An Wang', 'Hongliang Ren']",2024-05-14T14:55:15Z,http://arxiv.org/abs/2405.08672v1,"['eess.IV', 'cs.CV']"
"On-chip integrated metasystem for spin-dependent multi-channel colour
  holography","On-chip integrated metasurface driven by in-plane guided waves is of great
interests in various light field manipulation applications such as colorful
augmented reality and holographic display. However, it remains a challenge to
design colorful multichannel holography by a single on-chip metasurface. Here
we present metasurfaces integrated on top of guided-wave photonic slab that
achieves multi-channel colorful holographic light display. An end-to-end scheme
is used to inverse design the metasurface for projecting off-chip preset
multiple patterns. Particular examples are presented for customized patterns
that were encoded into the metasurface with a single-cell meta-atom, working
simultaneously at RGB color channels and for several different diffractive
distance, with polarization dependence. Holographic images are generated at 18
independent channels with such a single-cell metasurface. The proposed design
scheme is easy to implement and the resulting device is viable to fabrication,
promising a plenty of applications in nanophotonics.","['Zhan-Ying Ma', 'Xian-Jin Liu', 'Yu-Qi Peng', 'Da-Sen Zhang', 'Zhen-Zhen Liu', 'Jun-Jun Xiao']",2024-05-16T14:00:28Z,http://arxiv.org/abs/2405.10104v1,"['physics.optics', 'physics.app-ph']"
"PLASMA -- Platform for Service Management in Digital Remote Maintenance
  Applications","To support maintenance and servicing of industrial machines, service
processes are even today often performed manually and analogously, although
supportive technologies such as augmented reality, virtual reality and digital
platforms already exist. In many cases, neither technicians on-site nor remote
experts have all the essential information and options for suitable actions
available. Existing service products and platforms do not cover all the
required functions in practice in order to map end-to-end processes. PLASMA is
a concept for a Cloud-based remote maintenance platform designed to meet these
demands. But for a real-life implementation of PLASMA, security measures are
essential as we show in this paper.","['Natascha Stumpp', 'Doris Aschenbrenner', 'Manuel Stahl', 'Andreas Aßmuth']",2024-05-20T07:15:41Z,http://arxiv.org/abs/2405.11836v1,['cs.DC']
"NeRF View Synthesis: Subjective Quality Assessment and Objective Metrics
  Evaluation","Neural radiance fields (NeRF) are a groundbreaking computer vision technology
that enables the generation of high-quality, immersive visual content from
multiple viewpoints. This capability holds significant advantages for
applications such as virtual/augmented reality, 3D modelling and content
creation for the film and entertainment industry. However, the evaluation of
NeRF methods poses several challenges, including a lack of comprehensive
datasets, reliable assessment methodologies, and objective quality metrics.
This paper addresses the problem of NeRF quality assessment thoroughly, by
conducting a rigorous subjective quality assessment test that considers several
scene classes and recently proposed NeRF view synthesis methods. Additionally,
the performance of a wide range of state-of-the-art conventional and
learning-based full-reference 2D image and video quality assessment metrics is
evaluated against the subjective scores of the subjective study. The
experimental results are analyzed in depth, providing a comparative evaluation
of several NeRF methods and objective quality metrics, across different classes
of visual scenes, including real and synthetic content for front-face and
360-degree camera trajectories.","['Pedro Martin', 'Antonio Rodrigues', 'Joao Ascenso', 'Maria Paula Queluz']",2024-05-30T14:08:09Z,http://arxiv.org/abs/2405.20078v2,['cs.MM']
"Securing Interactive Sessions Using Mobile Device through Visual Channel
  and Visual Inspection","Communication channel established from a display to a device's camera is
known as visual channel, and it is helpful in securing key exchange protocol.
In this paper, we study how visual channel can be exploited by a network
terminal and mobile device to jointly verify information in an interactive
session, and how such information can be jointly presented in a user-friendly
manner, taking into account that the mobile device can only capture and display
a small region, and the user may only want to authenticate selective
regions-of-interests. Motivated by applications in Kiosk computing and
multi-factor authentication, we consider three security models: (1) the mobile
device is trusted, (2) at most one of the terminal or the mobile device is
dishonest, and (3) both the terminal and device are dishonest but they do not
collude or communicate. We give two protocols and investigate them under the
abovementioned models. We point out a form of replay attack that renders some
other straightforward implementations cumbersome to use. To enhance
user-friendliness, we propose a solution using visual cues embedded into the 2D
barcodes and incorporate the framework of ""augmented reality"" for easy
verifications through visual inspection. We give a proof-of-concept
implementation to show that our scheme is feasible in practice.","['Chengfang Fang', 'Ee-Chien Chang']",2010-03-03T03:31:29Z,http://arxiv.org/abs/1003.0723v2,"['cs.CR', 'cs.CV']"
"ConnectiCity, augmented perception of the city","As we move through cities in our daily lives, we are in a constant state of
transformation of the spaces around us. The form and essence of urban space
directly affects people's behavior, describing in their perception what is
possible or impossible, allowed or prohibited, suggested or advised against. We
are now able to fill and stratify space/time with digital information layers,
completely wrapping cities in a membrane of information and of opportunities
for interaction and communication. Mobile devices, smartphones, wearables,
digital tags, near field communication devices, location based services and
mixed/augmented reality have gone much further in this direction, turning the
world into an essentially read/write, ubiquitous publishing surface. The usage
of mobile devices and ubiquitous technologies alters the understanding of
place. In this process, the definition of (urban) landscape powerfully shifts
from a definition which is purely administrative (e.g.: the borders of the
flower bed in the middle of a roundabout) to one that is multiplied according
to all individuals which experience that location; as a lossless sum of their
perceptions; as a stratification of interpretations and activities which forms
our cognition of space and time. In our research we investigated the
possibilities to use the scenario which sees urban spaces progressively filling
with multiple layers of real-time, ubiquitous, digital information to
conceptualize, design and implement a series of usage scenarios. It is possible
to create multiple layers of narratives which traverse the city and which allow
us to read them in different ways, according to the different strategies and
methodologies enabling us to highlight how cities express points of view on the
environment, culture, economy, transports, energy and politics.","['Salvatore Iaconesi', 'Oriana Persico']",2012-07-18T08:02:33Z,http://arxiv.org/abs/1207.4291v1,"['cs.CY', 'cs.SI', 'physics.soc-ph']"
"Effective Features of Remote Sensing Image Classification Using
  Interactive Adaptive Thresholding Method","Remote sensing image classification can be performed in many different ways
to extract meaningful features. One common approach is to perform edge
detection. A second approach is to try and detect whole shapes, given the fact
that these shapes usually tend to have distinctive properties such as object
foreground or background. To get optimal results, these two approaches can be
combined. This paper adopts a combinatorial optimization method to adaptively
select threshold based features to improve remote sensing image. Feature
selection is an important combinatorial optimization problem in the remote
sensing image classification. The feature selection method has to achieve three
characteristics: first the performance issues by facilitating data collection
and reducing storage space and classification time, second to perform semantics
analysis helping to understand the problem, and third to improve prediction
accuracy by avoiding the curse of dimensionality. The goal of this thresholding
an image is to classify pixels as either dark or light and evaluation of
classification results. Interactive adaptive thresholding is a form of
thresholding that takes into account spatial variations in illumination of
remote sensing image. We present a technique for remote sensing based adaptive
thresholding using the interactive satellite image of the input. However, our
solution is more robust to illumination changes in the remote sensing image.
Additionally, our method is simple and easy to implement but it is effective
algorithm to classify the image pixels. This technique is suitable for
preprocessing the remote sensing image classification, making it a valuable
tool for interactive remote based applications such as augmented reality of the
classification procedure.","['T. Balaji', 'Dr. M. Sumathi']",2014-01-30T05:33:27Z,http://arxiv.org/abs/1401.7743v1,['cs.CV']
"Virtual Windshields: Merging Reality and Digital Content to Improve the
  Driving Experience","In recent years, the use of the automobile as the primary mode of
transportation has been increasing and driving has become an important part of
daily life. Driving is a multi-sensory experience as drivers rely on their
senses to provide them with important information. In a vehicular context human
senses are all too often limited and obstructed. Today, road accidents
constitute the eighth leading cause of death. The escalation of technology has
propelled new ways in which driver's senses may be augmented. The enclosed
aspect of a car, allied with the configuration of the controls and displays
directed towards the driver, offer significant advantages for augmented reality
(AR) systems when considering the amount of immersion it can provide to the
user. In addition, the inherent mobility and virtually unlimited power autonomy
transform cars into perfect mobile computing platforms. However, automobiles
currently present limited network connectivity and thus the created augmented
objects are merely providing information captured by in-vehicle sensors,
cameras and other databases. By combining the new paradigm of Vehicular Ad Hoc
Networking (VANET) with AR human machine interfaces, we show that it is
possible to design novel cooperative Advanced Driver Assistance Systems (ADAS),
that base the creation of AR content on the information collected from
neighbouring vehicles or roadside infrastructures. As such we implement
prototypes of both visual and acoustic AR systems, which can significantly
improve the driving experience. We believe our results contribute to the
formulation of a vision where the vehicle is perceived as an extension of the
body which permeates the human senses to the world outside the vessel, where
the car is used as a better, multi-sensory immersive version of a mobile phone
that integrates touch, vision and sound enhancements, leveraging unique
properties of VANET.",['Michelle Krüger Silvéria'],2014-05-05T14:28:13Z,http://arxiv.org/abs/1405.0910v1,['cs.HC']
Fast keypoint detection in video sequences,"A number of computer vision tasks exploit a succinct representation of the
visual content in the form of sets of local features. Given an input image,
feature extraction algorithms identify a set of keypoints and assign to each of
them a description vector, based on the characteristics of the visual content
surrounding the interest point. Several tasks might require local features to
be extracted from a video sequence, on a frame-by-frame basis. Although
temporal downsampling has been proven to be an effective solution for mobile
augmented reality and visual search, high temporal resolution is a key
requirement for time-critical applications such as object tracking, event
recognition, pedestrian detection, surveillance. In recent years, more and more
computationally efficient visual feature detectors and decriptors have been
proposed. Nonetheless, such approaches are tailored to still images. In this
paper we propose a fast keypoint detection algorithm for video sequences, that
exploits the temporal coherence of the sequence of keypoints. According to the
proposed method, each frame is preprocessed so as to identify the parts of the
input frame for which keypoint detection and description need to be performed.
Our experiments show that it is possible to achieve a reduction in
computational time of up to 40%, without significantly affecting the task
accuracy.","['Luca Baroffio', 'Matteo Cesana', 'Alessandro Redondi', 'Marco Tagliasacchi']",2015-03-24T09:28:28Z,http://arxiv.org/abs/1503.06959v1,"['cs.CV', 'cs.MM']"
"Science Gateway for Distributed Multiscale Course Management in
  e-Science and e-Learning - Use Case for Study and Investigation of
  Functionalized Nanomaterials","The current tendency of human learning and teaching is targeted to
development and integration of digital technologies (like cloud solutions,
mobile technology, learning analytics, big data, augmented reality, natural
interaction technologies, etc.). Our Science Gateway
(http://scigate.imp.kiev.ua) in collaboration with High Performance Computing
Center (http://hpcc.kpi.ua) is aimed on the close cooperation among the main
actors in learning and researching world (teachers, students, scientists,
supporting personnel, volunteers, etc.) with industry and academia to propose
the new frameworks and interoperability requirements for the building blocks of
a digital ecosystem for learning (including informal learning) that develops
and integrates the current and new tools and systems. It is the portal for
management of distributed courses (workflows), tools, resources, and users,
which is constructed on the basis of the Liferay framework and gUSE/WS-PGRADE
technology. It is based on development of multi-level approach (as to
methods/algorithms) for effective study and research through flexible selection
and combination of unified modules (""gaming"" with modules as with LEGO-bricks).
It allows us to provide the flexible and adjustable framework with direct
involvement in real-world and scientific use cases motivated by the educational
aims of students and real scientific aims in labs.","['Yuri Gordienko', 'Serhii Stirenko', 'Olexandr Gatsenko', 'Lev Bekenov']",2015-04-03T10:44:23Z,http://arxiv.org/abs/1504.00802v1,"['cs.CY', 'cond-mat.mtrl-sci']"
"Fractal Fluctuations in Human Walking: Comparison between Auditory and
  Visually Guided Stepping","In human locomotion, sensorimotor synchronization of gait consists of the
coordination of stepping with rhythmic auditory cues (auditory cueing, AC). AC
changes the long-range correlations among consecutive strides (fractal
dynamics) into anti-correlations. Visual cueing (VC) is the alignment of step
lengths with marks on the floor. The effects of VC on the fluctuation structure
of walking have not been investigated. Therefore, the objective was to compare
the effects of AC and VC on the fluctuation pattern of basic spatiotemporal
gait parameters. Thirty-six healthy individuals walked 3 x 500 strides on an
instrumented treadmill with augmented reality capabilities. The conditions were
no cueing (NC), AC, and VC. AC included an isochronous metronome. In VC,
projected stepping stones were synchronized with the treadmill speed. Detrended
fluctuation analysis assessed the correlation structure. The coefficient of
variation (CV) was also assessed. The results showed that AC and VC similarly
induced a strong anti-correlated pattern in the gait parameters. The CVs were
similar between the NC and AC conditions but substantially higher in the VC
condition. AC and VC probably mobilize similar motor control pathways and can
be used alternatively in gait rehabilitation. However, the increased gait
variability induced by VC should be considered.",['Philippe Terrier'],2015-09-07T06:20:48Z,http://arxiv.org/abs/1509.01913v3,['q-bio.NC']
"Free-body Gesture Tracking and Augmented Reality Improvisation for Floor
  and Aerial Dance","This paper describes an updated interactive performance system for floor and
Aerial Dance that controls visual and sonic aspects of the presentation via a
depth sensing camera (MS Kinect). In order to detect, measure and track free
movement in space, 3 degree of freedom (3-DOF) tracking in space (on the ground
and in the air) is performed using IR markers with a method for multi target
tracking capabilities added and described in detail. An improved gesture
tracking and recognition system, called Action Graph (AG), is described in the
paper. Action Graph uses an efficient incremental construction from a single
long sequence of movement features and automatically captures repeated
sub-segments in the movement from start to finish with no manual interaction
needed with other advanced capabilities discussed as well. By using the new
model for the gesture we can unify an entire choreography piece by dynamically
tracking and recognizing gestures and sub-portions of the piece. This gives the
performer the freedom to improvise based on a set of recorded gestures/portions
of the choreography and have the system dynamically respond in relation to the
performer within a set of related rehearsed actions, an ability that has not
been seen in any other system to date.","['Tammuz Dubnov', 'Cheng-i Wang']",2015-09-15T21:54:21Z,http://arxiv.org/abs/1509.04751v1,"['cs.MM', 'cs.CV', 'cs.HC']"
"Sensor Fusion of Camera, GPS and IMU using Fuzzy Adaptive Multiple
  Motion Models","A tracking system that will be used for Augmented Reality (AR) applications
has two main requirements: accuracy and frame rate. The first requirement is
related to the performance of the pose estimation algorithm and how accurately
the tracking system can find the position and orientation of the user in the
environment. Accuracy problems of current tracking devices, considering that
they are low-cost devices, cause static errors during this motion estimation
process. The second requirement is related to dynamic errors (the end-to-end
system delay; occurring because of the delay in estimating the motion of the
user and displaying images based on this estimate. This paper investigates
combining the vision-based estimates with measurements from other sensors, GPS
and IMU, in order to improve the tracking accuracy in outdoor environments. The
idea of using Fuzzy Adaptive Multiple Models (FAMM) was investigated using a
novel fuzzy rule-based approach to decide on the model that results in improved
accuracy and faster convergence for the fusion filter. Results show that the
developed tracking system is more accurate than a conventional GPS-IMU fusion
approach due to additional estimates from a camera and fuzzy motion models. The
paper also presents an application in cultural heritage context.","['Erkan Bostanci', 'Betul Bostanci', 'Nadia Kanwal', 'Adrian F. Clark']",2015-12-09T06:25:09Z,http://arxiv.org/abs/1512.02766v1,"['cs.RO', 'cs.CV']"
Recovering 6D Object Pose and Predicting Next-Best-View in the Crowd,"Object detection and 6D pose estimation in the crowd (scenes with multiple
object instances, severe foreground occlusions and background distractors), has
become an important problem in many rapidly evolving technological areas such
as robotics and augmented reality. Single shot-based 6D pose estimators with
manually designed features are still unable to tackle the above challenges,
motivating the research towards unsupervised feature learning and
next-best-view estimation. In this work, we present a complete framework for
both single shot-based 6D object pose estimation and next-best-view prediction
based on Hough Forests, the state of the art object pose estimator that
performs classification and regression jointly. Rather than using manually
designed features we a) propose an unsupervised feature learnt from
depth-invariant patches using a Sparse Autoencoder and b) offer an extensive
evaluation of various state of the art features. Furthermore, taking advantage
of the clustering performed in the leaf nodes of Hough Forests, we learn to
estimate the reduction of uncertainty in other views, formulating the problem
of selecting the next-best-view. To further improve pose estimation, we propose
an improved joint registration and hypotheses verification module as a final
refinement step to reject false detections. We provide two additional
challenging datasets inspired from realistic scenarios to extensively evaluate
the state of the art and our framework. One is related to domestic environments
and the other depicts a bin-picking scenario mostly found in industrial
settings. We show that our framework significantly outperforms state of the art
both on public and on our datasets.","['Andreas Doumanoglou', 'Rigas Kouskouridas', 'Sotiris Malassiotis', 'Tae-Kyun Kim']",2015-12-23T15:06:05Z,http://arxiv.org/abs/1512.07506v2,['cs.CV']
Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",['Emad Barsoum'],2016-04-21T06:55:42Z,http://arxiv.org/abs/1604.06195v1,['cs.CV']
Inference of Haemoglobin Concentration From Stereo RGB,"Multispectral imaging (MSI) can provide information about tissue oxygenation,
perfusion and potentially function during surgery. In this paper we present a
novel, near real-time technique for intrinsic measurements of total haemoglobin
(THb) and blood oxygenation (SO2) in tissue using only RGB images from a stereo
laparoscope. The high degree of spectral overlap between channels makes
inference of haemoglobin concentration challenging, non-linear and under
constrained. We decompose the problem into two constrained linear sub-problems
and show that with Tikhonov regularisation the estimation significantly
improves, giving robust estimation of the Thb. We demonstrate by using the
co-registered stereo image data from two cameras it is possible to get robust
SO2 estimation as well. Our method is closed from, providing computational
efficiency even with multiple cameras. The method we present requires only
spectral response calibration of each camera, without modification of existing
laparoscopic imaging hardware. We validate our technique on synthetic data from
Monte Carlo simulation % of light transport through soft tissue containing
submerged blood vessels and further, in vivo, on a multispectral porcine data
set.","['Geoffrey Jones', 'Neil T. Clancy', 'Yusuf Helo', 'Simon Arridge', 'Daniel S. Elson', 'Danail Stoyanov']",2016-07-11T13:29:54Z,http://arxiv.org/abs/1607.02936v2,['cs.CV']
"Son of Zorn's Lemma: Targeted Style Transfer Using Instance-aware
  Semantic Segmentation","Style transfer is an important task in which the style of a source image is
mapped onto that of a target image. The method is useful for synthesizing
derivative works of a particular artist or specific painting. This work
considers targeted style transfer, in which the style of a template image is
used to alter only part of a target image. For example, an artist may wish to
alter the style of only one particular object in a target image without
altering the object's general morphology or surroundings. This is useful, for
example, in augmented reality applications (such as the recently released
Pokemon GO), where one wants to alter the appearance of a single real-world
object in an image frame to make it appear as a cartoon. Most notably, the
rendering of real-world objects into cartoon characters has been used in a
number of films and television show, such as the upcoming series Son of Zorn.
We present a method for targeted style transfer that simultaneously segments
and stylizes single objects selected by the user. The method uses a Markov
random field model to smooth and anti-alias outlier pixels near object
boundaries, so that stylized objects naturally blend into their surroundings.","['Carlos Castillo', 'Soham De', 'Xintong Han', 'Bharat Singh', 'Abhay Kumar Yadav', 'Tom Goldstein']",2017-01-09T21:30:03Z,http://arxiv.org/abs/1701.02357v1,"['cs.CV', 'cs.GR']"
Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor,"We present an approach for real-time, robust and accurate hand pose
estimation from moving egocentric RGB-D cameras in cluttered real environments.
Existing methods typically fail for hand-object interactions in cluttered
scenes imaged from egocentric viewpoints, common for virtual or augmented
reality applications. Our approach uses two subsequently applied Convolutional
Neural Networks (CNNs) to localize the hand and regress 3D joint locations.
Hand localization is achieved by using a CNN to estimate the 2D position of the
hand center in the input, even in the presence of clutter and occlusions. The
localized hand position, together with the corresponding input depth value, is
used to generate a normalized cropped image that is fed into a second CNN to
regress relative 3D hand joint locations in real time. For added accuracy,
robustness and temporal stability, we refine the pose estimates using a
kinematic pose tracking energy. To train the CNNs, we introduce a new
photorealistic dataset that uses a merged reality approach to capture and
synthesize large amounts of annotated data of natural hand interaction in
cluttered scenes. Through quantitative and qualitative evaluation, we show that
our method is robust to self-occlusion and occlusions by objects, particularly
in moving egocentric perspectives.","['Franziska Mueller', 'Dushyant Mehta', 'Oleksandr Sotnychenko', 'Srinath Sridhar', 'Dan Casas', 'Christian Theobalt']",2017-04-07T12:23:03Z,http://arxiv.org/abs/1704.02201v2,['cs.CV']
Feedback Techniques in Computer-Based Simulation Training: A Survey,"Computer-based simulation training (CBST) is gaining popularity in a vast
range of applications such as surgery, rehabilitation therapy, military
applications, and driver/pilot training, as it offers a low-cost,
easily-accessible and effective training environment. Typically, CBST systems
comprise of two essential components: 1) a simulation environment that provides
an immersive and interactive learning experience, and 2) a feedback
intervention system that supports knowledge/skill acquisition and decision
making. The simulation environment is created using technologies such as
virtual or augmented reality, and this is an area which has gained much
interest in recent years. The provision of automated feedback in CBST however,
has not been investigated as much, and thus, is the focus of this paper.
Feedback is an essential component in learning, and should be provided to the
trainee during the training process in order to improve skills, to correct
mistakes, and most importantly, to inspire reasoning and critical thinking. In
CBST, feedback should be provided in a useful and timely manner, ideally in a
way that mimics the advice of an experienced tutor. Here, we explore the
provision of feedback in CBST from three perspectives: 1) types of feedback to
be provided, 2) presentation modalities of feedback, and 3) methods for
feedback extraction/learning. This review is aimed at providing insight into
how feedback is extracted, organized, and delivered in current applications, to
be used as a guide to the development of future feedback intervention systems
in CBST applications.","['Sudanthi Wijewickrema', 'Xingjun Ma', 'James Bailey', 'Gregor Kennedy', ""Stephen O'Leary""]",2017-05-12T07:18:58Z,http://arxiv.org/abs/1705.04683v1,['cs.HC']
A Lightweight Approach for On-the-Fly Reflectance Estimation,"Estimating surface reflectance (BRDF) is one key component for complete 3D
scene capture, with wide applications in virtual reality, augmented reality,
and human computer interaction. Prior work is either limited to controlled
environments (\eg gonioreflectometers, light stages, or multi-camera domes), or
requires the joint optimization of shape, illumination, and reflectance, which
is often computationally too expensive (\eg hours of running time) for
real-time applications. Moreover, most prior work requires HDR images as input
which further complicates the capture process. In this paper, we propose a
lightweight approach for surface reflectance estimation directly from $8$-bit
RGB images in real-time, which can be easily plugged into any 3D
scanning-and-fusion system with a commodity RGBD sensor. Our method is
learning-based, with an inference time of less than 90ms per scene and a model
size of less than 340K bytes. We propose two novel network architectures,
HemiCNN and Grouplet, to deal with the unstructured input data from multiple
viewpoints under unknown illumination. We further design a loss function to
resolve the color-constancy and scale ambiguity. In addition, we have created a
large synthetic dataset, SynBRDF, which comprises a total of $500$K RGBD images
rendered with a physically-based ray tracer under a variety of natural
illumination, covering $5000$ materials and $5000$ shapes. SynBRDF is the first
large-scale benchmark dataset for reflectance estimation. Experiments on both
synthetic data and real data show that the proposed method effectively recovers
surface reflectance, and outperforms prior work for reflectance estimation in
uncontrolled environments.","['Kihwan Kim', 'Jinwei Gu', 'Stephen Tyree', 'Pavlo Molchanov', 'Matthias Nießner', 'Jan Kautz']",2017-05-19T19:45:57Z,http://arxiv.org/abs/1705.07162v2,['cs.CV']
"Self-Supervised Siamese Learning on Stereo Image Pairs for Depth
  Estimation in Robotic Surgery","Robotic surgery has become a powerful tool for performing minimally invasive
procedures, providing advantages in dexterity, precision, and 3D vision, over
traditional surgery. One popular robotic system is the da Vinci surgical
platform, which allows preoperative information to be incorporated into live
procedures using Augmented Reality (AR). Scene depth estimation is a
prerequisite for AR, as accurate registration requires 3D correspondences
between preoperative and intraoperative organ models. In the past decade, there
has been much progress on depth estimation for surgical scenes, such as using
monocular or binocular laparoscopes [1,2]. More recently, advances in deep
learning have enabled depth estimation via Convolutional Neural Networks (CNNs)
[3], but training requires a large image dataset with ground truth depths.
Inspired by [4], we propose a deep learning framework for surgical scene depth
estimation using self-supervision for scalable data acquisition. Our framework
consists of an autoencoder for depth prediction, and a differentiable spatial
transformer for training the autoencoder on stereo image pairs without ground
truth depths. Validation was conducted on stereo videos collected in robotic
partial nephrectomy.","['Menglong Ye', 'Edward Johns', 'Ankur Handa', 'Lin Zhang', 'Philip Pratt', 'Guang-Zhong Yang']",2017-05-17T11:10:49Z,http://arxiv.org/abs/1705.08260v1,"['cs.CV', 'cs.RO']"
Learning Spherical Convolution for Fast Features from 360° Imagery,"While 360{\deg} cameras offer tremendous new possibilities in vision,
graphics, and augmented reality, the spherical images they produce make core
feature extraction non-trivial. Convolutional neural networks (CNNs) trained on
images from perspective cameras yield ""flat"" filters, yet 360{\deg} images
cannot be projected to a single plane without significant distortion. A naive
solution that repeatedly projects the viewing sphere to all tangent planes is
accurate, but much too computationally intensive for real problems. We propose
to learn a spherical convolutional network that translates a planar CNN to
process 360{\deg} imagery directly in its equirectangular projection. Our
approach learns to reproduce the flat filter outputs on 360{\deg} data,
sensitive to the varying distortion effects across the viewing sphere. The key
benefits are 1) efficient feature extraction for 360{\deg} images and video,
and 2) the ability to leverage powerful pre-trained networks researchers have
carefully honed (together with massive labeled image training sets) for
perspective images. We validate our approach compared to several alternative
methods in terms of both raw CNN output accuracy as well as applying a
state-of-the-art ""flat"" object detector to 360{\deg} data. Our method yields
the most accurate results while saving orders of magnitude in computation
versus the existing exact reprojection solution.","['Yu-Chuan Su', 'Kristen Grauman']",2017-08-02T20:18:10Z,http://arxiv.org/abs/1708.00919v3,['cs.CV']
"Applying advanced machine learning models to classify
  electro-physiological activity of human brain for use in biometric
  identification","In this article we present the results of our research related to the study
of correlations between specific visual stimulation and the elicited brain's
electro-physiological response collected by EEG sensors from a group of
participants. We will look at how the various characteristics of visual
stimulation affect the measured electro-physiological response of the brain and
describe the optimal parameters found that elicit a steady-state visually
evoked potential (SSVEP) in certain parts of the cerebral cortex where it can
be reliably perceived by the electrode of the EEG device. After that, we
continue with a description of the advanced machine learning pipeline model
that can perform confident classification of the collected EEG data in order to
(a) reliably distinguish signal from noise (about 85% validation score) and (b)
reliably distinguish between EEG records collected from different human
participants (about 80% validation score). Finally, we demonstrate that the
proposed method works reliably even with an inexpensive (less than $100)
consumer-grade EEG sensing device and with participants who do not have
previous experience with EEG technology (EEG illiterate). All this in
combination opens up broad prospects for the development of new types of
consumer devices, [e.g.] based on virtual reality helmets or augmented reality
glasses where EEG sensor can be easily integrated. The proposed method can be
used to improve an online user experience by providing [e.g.] password-less
user identification for VR / AR applications. It can also find a more advanced
application in intensive care units where collected EEG data can be used to
classify the level of conscious awareness of patients during anesthesia or to
automatically detect hardware failures by classifying the input signal as
noise.",['Iaroslav Omelianenko'],2017-08-03T14:50:02Z,http://arxiv.org/abs/1708.01167v1,['cs.LG']
"Adapting a Formal Model Theory to Applications in Augmented Personalized
  Medicine","The goal of this paper is to advance an extensible theory of living systems
using an approach to biomathematics and biocomputation that suitably addresses
self-organized, self-referential and anticipatory systems with multi-temporal
multi-agents. Our first step is to provide foundations for modelling of
emergent and evolving dynamic multi-level organic complexes and their
sustentative processes in artificial and natural life systems. Main
applications are in life sciences, medicine, ecology and astrobiology, as well
as robotics, industrial automation and man-machine interface. Since 2011 over
100 scientists from a number of disciplines have been exploring a substantial
set of theoretical frameworks for a comprehensive theory of life known as
Integral Biomathics. That effort identified the need for a robust core model of
organisms as dynamic wholes, using advanced and adequately computable
mathematics. The work described here for that core combines the advantages of a
situation and context aware multivalent computational logic for active
self-organizing networks, Wandering Logic Intelligence (WLI), and a multi-scale
dynamic category theory, Memory Evolutive Systems (MES), hence WLIMES. This is
presented to the modeller via a formal augmented reality language as a first
step towards practical modelling and simulation of multi-level living systems.
Initial work focuses on the design and implementation of this visual language
and calculus (VLC) and its graphical user interface. The results will be
integrated within the current methodology and practices of theoretical biology
and (personalized) medicine to deepen and to enhance the holistic understanding
of life.","['Plamen L. Simeonov', 'Andrée C. Ehresmann']",2017-10-01T20:10:11Z,http://arxiv.org/abs/1710.03571v4,"['q-bio.OT', 'cs.LO']"
HP-GAN: Probabilistic 3D human motion prediction via GAN,"Predicting and understanding human motion dynamics has many applications,
such as motion synthesis, augmented reality, security, and autonomous vehicles.
Due to the recent success of generative adversarial networks (GAN), there has
been much interest in probabilistic estimation and synthetic data generation
using deep neural network architectures and learning algorithms.
  We propose a novel sequence-to-sequence model for probabilistic human motion
prediction, trained with a modified version of improved Wasserstein generative
adversarial networks (WGAN-GP), in which we use a custom loss function designed
for human motion prediction. Our model, which we call HP-GAN, learns a
probability density function of future human poses conditioned on previous
poses. It predicts multiple sequences of possible future human poses, each from
the same input sequence but a different vector z drawn from a random
distribution. Furthermore, to quantify the quality of the non-deterministic
predictions, we simultaneously train a motion-quality-assessment model that
learns the probability that a given skeleton sequence is a real human motion.
  We test our algorithm on two of the largest skeleton datasets: NTURGB-D and
Human3.6M. We train our model on both single and multiple action types. Its
predictive power for long-term motion estimation is demonstrated by generating
multiple plausible futures of more than 30 frames from just 10 frames of input.
We show that most sequences generated from the same input have more than 50\%
probabilities of being judged as a real human sequence. We will release all the
code used in this paper to Github.","['Emad Barsoum', 'John Kender', 'Zicheng Liu']",2017-11-27T07:07:11Z,http://arxiv.org/abs/1711.09561v1,"['cs.CV', 'cs.AI', 'cs.HC', 'cs.NE']"
"Holographic Visualisation of Radiology Data and Automated Machine
  Learning-based Medical Image Segmentation","Within this thesis we propose a platform for combining Augmented Reality (AR)
hardware with machine learning in a user-oriented pipeline, offering to the
medical staff an intuitive 3D visualization of volumetric Computed Tomography
(CT) and Magnetic Resonance Imaging (MRI) medical image segmentations inside
the AR headset, that does not need human intervention for loading, processing
and segmentation of medical images. The AR visualization, based on Microsoft
HoloLens, employs a modular and thus scalable frontend-backend architecture for
real-time visualizations on multiple AR headsets. As Convolutional Neural
Networks (CNNs) have lastly demonstrated superior performance for the machine
learning task of image semantic segmentation, the pipeline also includes a
fully automated CNN algorithm for the segmentation of the liver from CT scans.
The model is based on the Deep Retinal Image Understanding (DRIU) model which
is a Fully Convolutional Network with side outputs from feature maps with
different resolution, extracted at different stages of the network. The
algorithm is 2.5D which means that the input is a set of consecutive scan
slices. The experiments have been performed on the Liver Tumor Segmentation
Challenge (LiTS) dataset for liver segmentation and demonstrated good results
and flexibility. While multiple approaches exist in the domain, only few of
them have focused on overcoming the practical aspects which still largely hold
this technology away from the operating rooms. In line with this, we also are
next planning an evaluation from medical doctors and radiologists in a
real-world environment.",['Lucian Trestioreanu'],2018-08-15T00:20:35Z,http://arxiv.org/abs/1808.04929v1,"['cs.CV', 'cs.LG']"
SLAMBench2: Multi-Objective Head-to-Head Benchmarking for Visual SLAM,"SLAM is becoming a key component of robotics and augmented reality (AR)
systems. While a large number of SLAM algorithms have been presented, there has
been little effort to unify the interface of such algorithms, or to perform a
holistic comparison of their capabilities. This is a problem since different
SLAM applications can have different functional and non-functional
requirements. For example, a mobile phonebased AR application has a tight
energy budget, while a UAV navigation system usually requires high accuracy.
SLAMBench2 is a benchmarking framework to evaluate existing and future SLAM
systems, both open and close source, over an extensible list of datasets, while
using a comparable and clearly specified list of performance metrics. A wide
variety of existing SLAM algorithms and datasets is supported, e.g.
ElasticFusion, InfiniTAM, ORB-SLAM2, OKVIS, and integrating new ones is
straightforward and clearly specified by the framework. SLAMBench2 is a
publicly-available software framework which represents a starting point for
quantitative, comparable and validatable experimental research to investigate
trade-offs across SLAM systems.","['Bruno Bodin', 'Harry Wagstaff', 'Sajad Saeedi', 'Luigi Nardi', 'Emanuele Vespa', 'John H Mayer', 'Andy Nisbet', 'Mikel Luján', 'Steve Furber', 'Andrew J Davison', 'Paul H. J. Kelly', ""Michael O'Boyle""]",2018-08-21T09:49:51Z,http://arxiv.org/abs/1808.06820v1,['cs.RO']
"DeepHPS: End-to-end Estimation of 3D Hand Pose and Shape by Learning
  from Synthetic Depth","Articulated hand pose and shape estimation is an important problem for
vision-based applications such as augmented reality and animation. In contrast
to the existing methods which optimize only for joint positions, we propose a
fully supervised deep network which learns to jointly estimate a full 3D hand
mesh representation and pose from a single depth image. To this end, a CNN
architecture is employed to estimate parametric representations i.e. hand pose,
bone scales and complex shape parameters. Then, a novel hand pose and shape
layer, embedded inside our deep framework, produces 3D joint positions and hand
mesh. Lack of sufficient training data with varying hand shapes limits the
generalized performance of learning based methods. Also, manually annotating
real data is suboptimal. Therefore, we present SynHand5M: a million-scale
synthetic dataset with accurate joint annotations, segmentation masks and mesh
files of depth maps. Among model based learning (hybrid) methods, we show
improved results on our dataset and two of the public benchmarks i.e. NYU and
ICVL. Also, by employing a joint training strategy with real and synthetic
data, we recover 3D hand mesh and pose from real images in 3.7ms.","['Jameel Malik', 'Ahmed Elhayek', 'Fabrizio Nunnari', 'Kiran Varanasi', 'Kiarash Tamaddon', 'Alexis Heloir', 'Didier Stricker']",2018-08-28T10:29:20Z,http://arxiv.org/abs/1808.09208v1,['cs.CV']
Collaborative Dense SLAM,"In this paper, we present a new system for live collaborative dense surface
reconstruction. Cooperative robotics, multi participant augmented reality and
human-robot interaction are all examples of situations where collaborative
mapping can be leveraged for greater agent autonomy. Our system builds on
ElasticFusion to allow a number of cameras starting with unknown initial
relative positions to maintain local maps utilising the original algorithm.
Carrying out visual place recognition across these local maps the system can
identify when two maps overlap in space, providing an inter-map constraint from
which the system can derive the relative poses of the two maps. Using these
resulting pose constraints, our system performs map merging, allowing multiple
cameras to fuse their measurements into a single shared reconstruction. The
advantage of this approach is that it avoids replication of structures
subsequent to loop closures, where multiple cameras traverse the same regions
of the environment. Furthermore, it allows cameras to directly exploit and
update regions of the environment previously mapped by other cameras within the
system. We provide both quantitative and qualitative analyses using the
synthetic ICL-NUIM dataset and the real-world Freiburg dataset including the
impact of multi-camera mapping on surface reconstruction accuracy, camera pose
estimation accuracy and overall processing time. We also include qualitative
results in the form of sample reconstructions of room sized environments with
up to 3 cameras undergoing intersecting and loopy trajectories.","['Louis Gallagher', 'John B. McDonald']",2018-11-19T11:54:40Z,http://arxiv.org/abs/1811.07632v2,"['cs.CV', 'I.2.10']"
"Generating Classes of 3D Virtual Mandibles for AR-Based Medical
  Simulation","Simulation and modeling represent promising tools for several application
domains from engineering to forensic science and medicine. Advances in 3D
imaging technology convey paradigms such as augmented reality (AR) and mixed
reality inside promising simulation tools for the training industry. Motivated
by the requirement for superimposing anatomically correct 3D models on a Human
Patient Simulator (HPS) and visualizing them in an AR environment, the purpose
of this research effort is to derive method for scaling a source human mandible
to a target human mandible. Results show that, given a distance between two
same landmarks on two different mandibles, a relative scaling factor may be
computed. Using this scaling factor, results show that a 3D virtual mandible
model can be made morphometrically equivalent to a real target-specific
mandible within a 1.30 millimeter average error bound. The virtual mandible may
be further used as a reference target for registering other anatomical models,
such as the lungs, on the HPS. Such registration will be made possible by
physical constraints among the mandible and the spinal column in the horizontal
normal rest position.","['Neha R. Hippalgaonkar', 'Alexa D. Sider', 'Felix G. Hamza-Lup', 'Anand P. Santhanam', 'Bala Jaganathan', 'Celina Imielinska', 'Jannick P. Rolland']",2018-11-20T03:29:56Z,http://arxiv.org/abs/1811.08053v1,['cs.GR']
"Human Intention Estimation based on Hidden Markov Model Motion
  Validation for Safe Flexible Robotized Warehouses","With the substantial growth of logistics businesses the need for larger
warehouses and their automation arises, thus using robots as assistants to
human workers is becoming a priority. In order to operate efficiently and
safely, robot assistants or the supervising system should recognize human
intentions in real-time. Theory of mind (ToM) is an intuitive human conception
of other humans' mental state, i.e., beliefs and desires, and how they cause
behavior. In this paper we propose a ToM based human intention estimation
algorithm for flexible robotized warehouses. We observe human's, i.e., worker's
motion and validate it with respect to the goal locations using generalized
Voronoi diagram based path planning. These observations are then processed by
the proposed hidden Markov model framework which estimates worker intentions in
an online manner, capable of handling changing environments. To test the
proposed intention estimation we ran experiments in a real-world laboratory
warehouse with a worker wearing Microsoft Hololens augmented reality glasses.
Furthermore, in order to demonstrate the scalability of the approach to larger
warehouses, we propose to use virtual reality digital warehouse twins in order
to realistically simulate worker behavior. We conducted intention estimation
experiments in the larger warehouse digital twin with up to 24 running robots.
We demonstrate that the proposed framework estimates warehouse worker
intentions precisely and in the end we discuss the experimental results.","['Tomislav Petković', 'David Puljiz', 'Ivan Marković', 'Björn Hein']",2018-11-20T14:32:31Z,http://arxiv.org/abs/1811.08269v1,['cs.RO']
"Generating Realistic Training Images Based on Tonality-Alignment
  Generative Adversarial Networks for Hand Pose Estimation","Hand pose estimation from a monocular RGB image is an important but
challenging task. The main factor affecting its performance is the lack of a
sufficiently large training dataset with accurate hand-keypoint annotations. In
this work, we circumvent this problem by proposing an effective method for
generating realistic hand poses and show that state-of-the-art algorithms for
hand pose estimation can be greatly improved by utilizing the generated hand
poses as training data. Specifically, we first adopt an augmented reality (AR)
simulator to synthesize hand poses with accurate hand-keypoint labels. Although
the synthetic hand poses come with precise joint labels, eliminating the need
of manual annotations, they look unnatural and are not the ideal training data.
To produce more realistic hand poses, we propose to blend a synthetic hand pose
with a real background, such as arms and sleeves. To this end, we develop
tonality-alignment generative adversarial networks (TAGANs), which align the
tonality and color distributions between synthetic hand poses and real
backgrounds, and can generate high quality hand poses. We evaluate TAGAN on
three benchmarks, including the RHP, STB, and CMU-PS hand pose datasets. With
the aid of the synthesized poses, our method performs favorably against the
state-of-the-arts in both 2D and 3D hand pose estimations.","['Liangjian Chen', 'Shih-Yao Lin', 'Yusheng Xie', 'Hui Tang', 'Yufan Xue', 'Xiaohui Xie', 'Yen-Yu Lin', 'Wei Fan']",2018-11-25T01:18:13Z,http://arxiv.org/abs/1811.09916v4,['cs.CV']
Referencing between a Head-Mounted Device and Robotic Manipulators,"Having a precise and robust transformation between the robot coordinate
system and the AR-device coordinate system is paramount during human-robot
interaction (HRI) based on augmented reality using Head mounted displays (HMD),
both for intuitive information display and for the tracking of human motions.
Most current solutions in this area rely either on the tracking of visual
markers, e.g. QR codes, or on manual referencing, both of which provide
unsatisfying results. Meanwhile a plethora of object detection and referencing
methods exist in the wider robotic and machine vision communities. The
precision of the referencing is likewise almost never measured. Here we would
like to address this issue by firstly presenting an overview of currently used
referencing methods between robots and HMDs. This is followed by a brief
overview of object detection and referencing methods used in the field of
robotics. Based on these methods we suggest three classes of referencing
algorithms we intend to pursue - semi-automatic, on-shot; automatic, one-shot;
and automatic continuous. We describe the general workflows of these three
classes as well as describing our proposed algorithms in each of these classes.
Finally we present the first experimental results of a semi-automatic
referencing algorithm, tested on an industrial KUKA KR-5 manipulator.","['David Puljiz', 'Katharina S. Riesterer', 'Björn Hein', 'Torsten Kröger']",2019-04-04T11:04:27Z,http://arxiv.org/abs/1904.02480v1,['cs.RO']
"Simultaneous achromatic and varifocal imaging with quartic metasurfaces
  in the visible","Two key metrics for imaging systems are their magnification and optical
bandwidth. While high-quality imaging systems today achieve bandwidths spanning
the whole visible spectrum and large changes in magnification via optical zoom,
these often entail lens assemblies with bulky elements unfit for
size-constrained applications. Metalenses present a methodology for
miniaturization but their strong chromatic aberrations and the lack of a
varifocal achromatic element limit their utility. While exemplary broadband
achromatic metalenses are realizable via dispersion engineering, in practice,
these designs are limited to small physical apertures as large area lenses
would require phase compensating scatterers with aspect ratios infeasible for
fabrication. Many applications, however, necessitate larger areas to collect
more photons for better signal-to-noise ratio and furthermore must also operate
with unpolarized light. In this paper, we simultaneously achieve achromatic
operation at visible wavelengths and varifocal control using a
polarization-insensitive, hybrid optical-digital system with area unconstrained
by dispersion-engineered scatterers. We derive phase equations for a pair of
conjugate metasurfaces that generate a focused accelerating beam for chromatic
focal shift control and a wide tunable focal length range of 4.8 mm (a
667-diopter change). Utilizing this conjugate pair, we realize a near
spectrally invariant point spread function across the visible regime. We then
combine the metasurfaces with a post-capture deconvolution algorithm to image
full-color patterns under incoherent white light, demonstrating an achromatic
5x zoom range. Simultaneously achromatic and varifocal metalenses could have
applications in various fields including augmented reality, implantable
microscopes, and machine vision sensors.","['Shane Colburn', 'Arka Majumdar']",2019-04-21T16:45:42Z,http://arxiv.org/abs/1904.09622v2,['physics.optics']
Pivot calibration concept for sensor attached mobile c-arms,"Medical augmented reality has been actively studied for decades and many
methods have been proposed torevolutionize clinical procedures. One example is
the camera augmented mobile C-arm (CAMC), which providesa real-time video
augmentation onto medical images by rigidly mounting and calibrating a camera
to the imagingdevice. Since then, several CAMC variations have been suggested
by calibrating 2D/3D cameras, trackers, andmore recently a Microsoft HoloLens
to the C-arm. Different calibration methods have been applied to establishthe
correspondence between the rigidly attached sensor and the imaging device. A
crucial step for these methodsis the acquisition of X-Ray images or 3D
reconstruction volumes; therefore, requiring the emission of ionizingradiation.
In this work, we analyze the mechanical motion of the device and propose an
alternatative methodto calibrate sensors to the C-arm without emitting any
radiation. Given a sensor is rigidly attached to thedevice, we introduce an
extended pivot calibration concept to compute the fixed translation from the
sensor tothe C-arm rotation center. The fixed relationship between the sensor
and rotation center can be formulated as apivot calibration problem with the
pivot point moving on a locus. Our method exploits the rigid C-arm
motiondescribing a Torus surface to solve this calibration problem. We explain
the geometry of the C-arm motion andits relation to the attached sensor,
propose a calibration algorithm and show its robustness against noise, as
wellas trajectory and observed pose density by computer simulations. We discuss
this geometric-based formulationand its potential extensions to different C-arm
applications.","['Sing Chun Lee', 'Matthias Seibold', 'Philipp Fürnstahl', 'Mazda Farshad', 'Nassir Navab']",2020-01-09T15:57:14Z,http://arxiv.org/abs/2001.03075v1,['cs.RO']
Multi-operator Network Sharing for Massive IoT,"Recent study predicts that by 2020 up to 50 billion IoT devices will be
connected to the Internet, straining the capacity of wireless network that has
already been overloaded with data-hungry mobile applications, such as
high-definition video streaming and virtual reality(VR)/augmented reality(AR).
How to accommodate the demand for both massive scale of IoT devices and
high-speed cellular services in the physically limited spectrum without
significantly increasing the operational and infrastructure costs is one of the
main challenges for operators. In this article, we introduce a new
multi-operator network sharing framework that supports the coexistence of IoT
and high-speed cellular services. Our framework is based on the radio access
network (RAN) sharing architecture recently introduced by 3GPP as a promising
solution for operators to improve their resource utilization and reduce the
system roll-out cost. We evaluate the performance of our proposed framework
using the real base station location data in the city of Dublin collected from
two major operators in Ireland. Numerical results show that our proposed
framework can almost double the total number of IoT devices that can be
supported and coexist with other cellular services compared with the case
without network sharing.","['Yong Xiao', 'Marwan Krunz', 'Tao Shu']",2020-01-25T08:16:28Z,http://arxiv.org/abs/2001.09276v1,"['cs.NI', 'cs.IT', 'eess.SP', 'math.IT']"
"Artificial intelligence in medicine and healthcare: a review and
  classification of current and near-future applications and their ethical and
  social Impact","This paper provides an overview of the current and near-future applications
of Artificial Intelligence (AI) in Medicine and Health Care and presents a
classification according to their ethical and societal aspects, potential
benefits and pitfalls, and issues that can be considered controversial and are
not deeply discussed in the literature.
  This work is based on an analysis of the state of the art of research and
technology, including existing software, personal monitoring devices, genetic
tests and editing tools, personalized digital models, online platforms,
augmented reality devices, and surgical and companion robotics. Motivated by
our review, we present and describe the notion of 'extended personalized
medicine', we then review existing applications of AI in medicine and
healthcare and explore the public perception of medical AI systems, and how
they show, simultaneously, extraordinary opportunities and drawbacks that even
question fundamental medical concepts. Many of these topics coincide with
urgent priorities recently defined by the World Health Organization for the
coming decade. In addition, we study the transformations of the roles of
doctors and patients in an age of ubiquitous information, identify the risk of
a division of Medicine into 'fake-based', 'patient-generated', and
'scientifically tailored', and draw the attention of some aspects that need
further thorough analysis and public debate.","['Emilio Gómez-González', 'Emilia Gomez', 'Javier Márquez-Rivas', 'Manuel Guerrero-Claro', 'Isabel Fernández-Lizaranzu', 'María Isabel Relimpio-López', 'Manuel E. Dorado', 'María José Mayorga-Buiza', 'Guillermo Izquierdo-Ayuso', 'Luis Capitán-Morales']",2020-01-22T15:39:42Z,http://arxiv.org/abs/2001.09778v2,"['cs.CY', 'cs.AI']"
"Design and Simulation of a Hybrid Architecture for Edge Computing in 5G
  and Beyond","Edge Computing in 5G and Beyond is a promising solution for ultra-low latency
applications (e.g. Autonomous Vehicle, Augmented Reality, and Remote Surgery),
which have an extraordinarily low tolerance for the delay and require fast data
processing for a very high volume of data. The requirements of delay-sensitive
applications (e.g. Low latency, proximity, and Location/Context-awareness)
cannot be satisfied by Cloud Computing due to the high latency between User
Equipment and Cloud. Nevertheless, Edge Computing in 5G and beyond can promise
an ultra-high-speed caused by placing computation capabilities closer to
endpoint devices, whereas 5G encourages the speed rate that is 200 times faster
than 4G LTE-Advanced. This paper deeply investigates Edge Computing in 5G and
characterizes it based on the requirements of ultra-low latency applications.
As a contribution, we propose a hybrid architecture that takes advantage of
novel and sustainable technologies (e.g. D2D communication, Massive MIMO, SDN,
and NFV) and has major features such as scalability, reliability and ultra-low
latency support. The proposed architecture is evaluated based on an agent-based
simulation that demonstrates it can satisfy requirements and has the ability to
respond to high volume demands with low latency.","['Hamed Rahimi', 'Yvan Picaud', 'Salvatore Costanzo', 'Giyyarpuram Madhusudan', 'Olivier Boissier', 'kamal Deep Singh']",2020-08-31T18:19:14Z,http://arxiv.org/abs/2009.00041v1,"['cs.DC', 'eess.SP']"
"Approaches, Challenges, and Applications for Deep Visual Odometry:
  Toward to Complicated and Emerging Areas","Visual odometry (VO) is a prevalent way to deal with the relative
localization problem, which is becoming increasingly mature and accurate, but
it tends to be fragile under challenging environments. Comparing with classical
geometry-based methods, deep learning-based methods can automatically learn
effective and robust representations, such as depth, optical flow, feature,
ego-motion, etc., from data without explicit computation. Nevertheless, there
still lacks a thorough review of the recent advances of deep learning-based VO
(Deep VO). Therefore, this paper aims to gain a deep insight on how deep
learning can profit and optimize the VO systems. We first screen out a number
of qualifications including accuracy, efficiency, scalability, dynamicity,
practicability, and extensibility, and employ them as the criteria. Then, using
the offered criteria as the uniform measurements, we detailedly evaluate and
discuss how deep learning improves the performance of VO from the aspects of
depth estimation, feature extraction and matching, pose estimation. We also
summarize the complicated and emerging areas of Deep VO, such as mobile robots,
medical robots, augmented reality and virtual reality, etc. Through the
literature decomposition, analysis, and comparison, we finally put forward a
number of open issues and raise some future research directions in this field.","['Ke Wang', 'Sai Ma', 'Junlan Chen', 'Fan Ren']",2020-09-06T08:25:23Z,http://arxiv.org/abs/2009.02672v1,"['cs.CV', 'cs.AI', 'cs.RO']"
Deep Analog-to-Digital Converter for Wireless Communication,"With the advent of the 5G wireless networks, achieving tens of gigabits per
second throughputs and low, milliseconds, latency has become a reality. This
level of performance will fuel numerous real-time applications, such as
autonomy and augmented reality, where the computationally heavy tasks can be
performed in the cloud. The increase in the bandwidth along with the use of
dense constellations places a significant burden on the speed and accuracy of
analog-to-digital converters (ADC). A popular approach to create wideband ADCs
is utilizing multiple channels each operating at a lower speed in the
time-interleaved fashion. However, an interleaved ADC comes with its own set of
challenges. The parallel architecture is very sensitive to the inter-channel
mismatch, timing jitter, clock skew between different ADC channels as well as
the nonlinearity within individual channels. Consequently, complex
post-calibration is required using digital signal processing (DSP) after the
ADC. The traditional DSP calibration consumes a significant amount of power and
its design requires knowledge of the source and type of errors which are
becoming increasingly difficult to predict in nanometer CMOS processes. In this
paper, instead of individually targeting each source of error, we utilize a
deep learning algorithm to learn the complete and complex ADC behavior and to
compensate for it in realtime. We demonstrate this ""Deep ADC"" technique on an
8G Sample/s 8-channel time-interleaved ADC with the QAM-OFDM modulated data.
Simulation results for different QAM symbol constellations and OFDM subcarriers
show dramatic improvements of approximately 5 bits in the dynamic range with a
concomitant drastic reduction in symbol error rate. We further discuss the
hardware implementation including latency, power consumption, memory
requirements, and chip area.","['Ashkan Samiee', 'Yiming Zhou', 'Tingyi Zhou', 'Bahram Jalali']",2020-09-11T17:36:13Z,http://arxiv.org/abs/2009.05553v1,['eess.SP']
Microscope Based HER2 Scoring System,"The overexpression of human epidermal growth factor receptor 2 (HER2) has
been established as a therapeutic target in multiple types of cancers, such as
breast and gastric cancers. Immunohistochemistry (IHC) is employed as a basic
HER2 test to identify the HER2-positive, borderline, and HER2-negative
patients. However, the reliability and accuracy of HER2 scoring are affected by
many factors, such as pathologists' experience. Recently, artificial
intelligence (AI) has been used in various disease diagnosis to improve
diagnostic accuracy and reliability, but the interpretation of diagnosis
results is still an open problem. In this paper, we propose a real-time HER2
scoring system, which follows the HER2 scoring guidelines to complete the
diagnosis, and thus each step is explainable. Unlike the previous scoring
systems based on whole-slide imaging, our HER2 scoring system is integrated
into an augmented reality (AR) microscope that can feedback AI results to the
pathologists while reading the slide. The pathologists can help select
informative fields of view (FOVs), avoiding the confounding regions, such as
DCIS. Importantly, we illustrate the intermediate results with membrane
staining condition and cell classification results, making it possible to
evaluate the reliability of the diagnostic results. Also, we support the
interactive modification of selecting regions-of-interest, making our system
more flexible in clinical practice. The collaboration of AI and pathologists
can significantly improve the robustness of our system. We evaluate our system
with 285 breast IHC HER2 slides, and the classification accuracy of 95\% shows
the effectiveness of our HER2 scoring system.","['Jun Zhang', 'Kuan Tian', 'Pei Dong', 'Haocheng Shen', 'Kezhou Yan', 'Jianhua Yao', 'Junzhou Huang', 'Xiao Han']",2020-09-15T01:44:39Z,http://arxiv.org/abs/2009.06816v1,"['eess.IV', 'cs.CV', 'cs.SY', 'eess.SY']"
Arbitrary Video Style Transfer via Multi-Channel Correlation,"Video style transfer is getting more attention in AI community for its
numerous applications such as augmented reality and animation productions.
Compared with traditional image style transfer, performing this task on video
presents new challenges: how to effectively generate satisfactory stylized
results for any specified style, and maintain temporal coherence across frames
at the same time. Towards this end, we propose Multi-Channel Correction network
(MCCNet), which can be trained to fuse the exemplar style features and input
content features for efficient style transfer while naturally maintaining the
coherence of input videos. Specifically, MCCNet works directly on the feature
space of style and content domain where it learns to rearrange and fuse style
features based on their similarity with content features. The outputs generated
by MCC are features containing the desired style patterns which can further be
decoded into images with vivid style textures. Moreover, MCCNet is also
designed to explicitly align the features to input which ensures the output
maintains the content structures as well as the temporal continuity. To further
improve the performance of MCCNet under complex light conditions, we also
introduce the illumination loss during training. Qualitative and quantitative
evaluations demonstrate that MCCNet performs well in both arbitrary video and
image style transfer tasks.","['Yingying Deng', 'Fan Tang', 'Weiming Dong', 'Haibin Huang', 'Chongyang Ma', 'Changsheng Xu']",2020-09-17T01:30:46Z,http://arxiv.org/abs/2009.08003v2,"['cs.CV', 'eess.IV']"
"Leveraging Local and Global Descriptors in Parallel to Search
  Correspondences for Visual Localization","Visual localization to compute 6DoF camera pose from a given image has wide
applications such as in robotics, virtual reality, augmented reality, etc. Two
kinds of descriptors are important for the visual localization. One is global
descriptors that extract the whole feature from each image. The other is local
descriptors that extract the local feature from each image patch usually
enclosing a key point. More and more methods of the visual localization have
two stages: at first to perform image retrieval by global descriptors and then
from the retrieval feedback to make 2D-3D point correspondences by local
descriptors. The two stages are in serial for most of the methods. This simple
combination has not achieved superiority of fusing local and global
descriptors. The 3D points obtained from the retrieval feedback are as the
nearest neighbor candidates of the 2D image points only by global descriptors.
Each of the 2D image points is also called a query local feature when
performing the 2D-3D point correspondences. In this paper, we propose a novel
parallel search framework, which leverages advantages of both local and global
descriptors to get nearest neighbor candidates of a query local feature.
Specifically, besides using deep learning based global descriptors, we also
utilize local descriptors to construct random tree structures for obtaining
nearest neighbor candidates of the query local feature. We propose a new
probabilistic model and a new deep learning based local descriptor when
constructing the random trees. A weighted Hamming regularization term to keep
discriminativeness after binarization is given in the loss function for the
proposed local descriptor. The loss function co-trains both real and binary
descriptors of which the results are integrated into the random trees.","['Pengju Zhang', 'Yihong Wu', 'Bingxi Liu']",2020-09-23T01:49:03Z,http://arxiv.org/abs/2009.10891v1,['cs.CV']
"SceneGen: Generative Contextual Scene Augmentation using Scene Graph
  Priors","Spatial computing experiences are constrained by the real-world surroundings
of the user. In such experiences, augmenting virtual objects to existing scenes
require a contextual approach, where geometrical conflicts are avoided, and
functional and plausible relationships to other objects are maintained in the
target environment. Yet, due to the complexity and diversity of user
environments, automatically calculating ideal positions of virtual content that
is adaptive to the context of the scene is considered a challenging task.
Motivated by this problem, in this paper we introduce SceneGen, a generative
contextual augmentation framework that predicts virtual object positions and
orientations within existing scenes. SceneGen takes a semantically segmented
scene as input, and outputs positional and orientational probability maps for
placing virtual content. We formulate a novel spatial Scene Graph
representation, which encapsulates explicit topological properties between
objects, object groups, and the room. We believe providing explicit and
intuitive features plays an important role in informative content creation and
user interaction of spatial computing settings, a quality that is not captured
in implicit models. We use kernel density estimation (KDE) to build a
multivariate conditional knowledge model trained using prior spatial Scene
Graphs extracted from real-world 3D scanned data. To further capture
orientational properties, we develop a fast pose annotation tool to extend
current real-world datasets with orientational labels. Finally, to demonstrate
our system in action, we develop an Augmented Reality application, in which
objects can be contextually augmented in real-time.","['Mohammad Keshavarzi', 'Aakash Parikh', 'Xiyu Zhai', 'Melody Mao', 'Luisa Caldas', 'Allen Y. Yang']",2020-09-25T18:36:27Z,http://arxiv.org/abs/2009.12395v2,"['cs.GR', 'cs.CV']"
"Enhancing a Neurocognitive Shared Visuomotor Model for Object
  Identification, Localization, and Grasping With Learning From Auxiliary Tasks","We present a follow-up study on our unified visuomotor neural model for the
robotic tasks of identifying, localizing, and grasping a target object in a
scene with multiple objects. Our Retinanet-based model enables end-to-end
training of visuomotor abilities in a biologically inspired developmental
approach. In our initial implementation, a neural model was able to grasp
selected objects from a planar surface. We embodied the model on the NICO
humanoid robot. In this follow-up study, we expand the task and the model to
reaching for objects in a three-dimensional space with a novel dataset based on
augmented reality and a simulation environment. We evaluate the influence of
training with auxiliary tasks, i.e., if learning of the primary visuomotor task
is supported by learning to classify and locate different objects. We show that
the proposed visuomotor model can learn to reach for objects in a
three-dimensional space. We analyze the results for biologically-plausible
biases based on object locations or properties. We show that the primary
visuomotor task can be successfully trained simultaneously with one of the two
auxiliary tasks. This is enabled by a complex neurocognitive model with shared
and task-specific components, similar to models found in biological systems.","['Matthias Kerzel', 'Fares Abawi', 'Manfred Eppe', 'Stefan Wermter']",2020-09-26T19:45:15Z,http://arxiv.org/abs/2009.12674v1,"['cs.CV', 'cs.RO']"
"Controlling wheelchairs by body motions: A learning framework for the
  adaptive remapping of space","Learning to operate a vehicle is generally accomplished by forming a new
cognitive map between the body motions and extrapersonal space. Here, we
consider the challenge of remapping movement-to-space representations in
survivors of spinal cord injury, for the control of powered wheelchairs. Our
goal is to facilitate this remapping by developing interfaces between residual
body motions and navigational commands that exploit the degrees of freedom that
disabled individuals are most capable to coordinate. We present a new framework
for allowing spinal cord injured persons to control powered wheelchairs through
signals derived from their residual mobility. The main novelty of this approach
lies in substituting the more common joystick controllers of powered
wheelchairs with a sensor shirt. This allows the whole upper body of the user
to operate as an adaptive joystick. Considerations about learning and risks
have lead us to develop a safe testing environment in 3D Virtual Reality. A
Personal Augmented Reality Immersive System (PARIS) allows us to analyse
learning skills and provide users with an adequate training to control a
simulated wheelchair through the signals generated by body motions in a safe
environment. We provide a description of the basic theory, of the development
phases and of the operation of the complete system. We also present preliminary
results illustrating the processing of the data and supporting of the
feasibility of this approach.","['Tauseef Gulrez', 'Alessandro Tognetti', 'Alon Fishbach', 'Santiago Acosta', 'Christopher Scharver', 'Danilo De Rossi', 'Ferdinando A. Mussa-Ivaldi']",2011-07-27T05:30:40Z,http://arxiv.org/abs/1107.5387v1,"['cs.RO', 'cs.AI', 'cs.NE']"
"Simulation of Color Blindness and a Proposal for Using Google Glass as
  Color-correcting Tool","The human visual color response is driven by specialized cells called cones,
which exist in three types, viz. R, G, and B. Software is developed to simulate
how color images are displayed for different types of color blindness.
Specified the default color deficiency associated with a user, it generates a
preview of the rainbow (in the visible range, from red to violet) and shows up,
side by side with a colorful image provided as input, the display correspondent
colorblind. The idea is to provide an image processing after image acquisition
to enable a better perception ofcolors by the color blind. Examples of
pseudo-correction are shown for the case of Protanopia (red blindness). The
system is adapted into a screen of an i-pad or a cellphone in which the
colorblind observe the camera, the image processed with color detail previously
imperceptible by his naked eye. As prospecting, wearable computer glasses could
be manufactured to provide a corrected image playback. The approach can also
provide augmented reality for human vision by adding the UV or IR responses as
a new feature of Google Glass.","['H. M. de Oliveira', 'J. Ranhel', 'R. B. A. Alves']",2015-02-12T16:36:55Z,http://arxiv.org/abs/1502.03723v1,"['cs.HC', 'cs.CV']"
"Keypoint Encoding for Improved Feature Extraction from Compressed Video
  at Low Bitrates","In many mobile visual analysis applications, compressed video is transmitted
over a communication network and analyzed by a server. Typical processing steps
performed at the server include keypoint detection, descriptor calculation, and
feature matching. Video compression has been shown to have an adverse effect on
feature-matching performance. The negative impact of compression can be reduced
by using the keypoints extracted from the uncompressed video to calculate
descriptors from the compressed video. Based on this observation, we propose to
provide these keypoints to the server as side information and to extract only
the descriptors from the compressed video. First, we introduce four different
frame types for keypoint encoding to address different types of changes in
video content. These frame types represent a new scene, the same scene, a
slowly changing scene, or a rapidly moving scene and are determined by
comparing features between successive video frames. Then, we propose Intra,
Skip and Inter modes of encoding the keypoints for different frame types. For
example, keypoints for new scenes are encoded using the Intra mode, and
keypoints for unchanged scenes are skipped. As a result, the bitrate of the
side information related to keypoint encoding is significantly reduced.
Finally, we present pairwise matching and image retrieval experiments conducted
to evaluate the performance of the proposed approach using the Stanford mobile
augmented reality dataset and 720p format videos. The results show that the
proposed approach offers significantly improved feature matching and image
retrieval performance at a given bitrate.","['Jianshu Chao', 'Eckehard Steinbach']",2015-06-27T17:33:34Z,http://arxiv.org/abs/1506.08316v2,"['cs.MM', 'cs.CV', 'cs.IR']"
"Experiences in Implementing an ICT-Augmented Reality as an Immersive
  Learning System for a Philippine HEI","This paper presents the experiences in building and implementing a 3D
avatar-based virtual world (3D-AVW) as a VLE (3D-AVLE) for the Technological
University of the Philippines-Taguig (TUP-T), a higher education institution
(HEI) in the Philippines. Free and Open Source Software (FOSS) systems were
used, such as the OpenSimulator and various 3D renderers, to create a replica
of the TUP-T campus in a simulated 3D world. The 3D-AVLE runs in a single
server that is connected to the learners' computers via a simply-wired local
area network (LAN). The use of various networking optimization techniques was
experimented on to provide the learners and the instructors alike a seamless
experience and lag-less immersion within the 3D-AVLE. With the current LAN
setup in TUP-T, the optimal number of concurrent users that can be accommodated
without sacrificing connectivity and the quality of virtual experience was
found to be at 30 users, exactly the mean class size in TUP-T. The 3D-AVLE
allows for recording of the learners' experiences which provides the learners a
facility to review the lessons at a later time. Classes in fundamental topics
in engineering sciences were conducted using the usual teaching aid
technologies such as the presentation software, and by dragging-and-dropping
the presentation files to the 3D-AVLE, resulting to increased learning curve by
both the instructors and the learners.","['Nestor R. Valdez', 'Marcelo V. Rivera', 'Jaderick P. Pabico']",2015-06-24T16:26:37Z,http://arxiv.org/abs/1601.06825v1,['cs.CY']
"Mask-off: Synthesizing Face Images in the Presence of Head-mounted
  Displays","A head-mounted display (HMD) could be an important component of augmented
reality system. However, as the upper face region is seriously occluded by the
device, the user experience could be affected in applications such as
telecommunication and multi-player video games. In this paper, we first present
a novel experimental setup that consists of two near-infrared (NIR) cameras to
point to the eye regions and one visible-light RGB camera to capture the
visible face region. The main purpose of this paper is to synthesize realistic
face images without occlusions based on the images captured by these cameras.
To this end, we propose a novel synthesis framework that contains four modules:
3D head reconstruction, face alignment and tracking, face synthesis, and eye
synthesis. In face synthesis, we propose a novel algorithm that can robustly
align and track a personalized 3D head model given a face that is severely
occluded by the HMD. In eye synthesis, in order to generate accurate eye
movements and dynamic wrinkle variations around eye regions, we propose another
novel algorithm to colorize the NIR eye images and further remove the ""red eye""
effects caused by the colorization. Results show that both hardware setup and
system framework are robust to synthesize realistic face images in video
sequences.","['Yajie Zhao', 'Qingguo Xu', 'Xinyu Huang', 'Ruigang Yang']",2016-10-26T19:41:37Z,http://arxiv.org/abs/1610.08481v2,['cs.CV']
Programming Heterogeneous Systems from an Image Processing DSL,"Specialized image processing accelerators are necessary to deliver the
performance and energy efficiency required by important applications in
computer vision, computational photography, and augmented reality. But
creating, ""programming,""and integrating this hardware into a hardware/software
system is difficult. We address this problem by extending the image processing
language, Halide, so users can specify which portions of their applications
should become hardware accelerators, and then we provide a compiler that uses
this code to automatically create the accelerator along with the ""glue"" code
needed for the user's application to access this hardware. Starting with Halide
not only provides a very high-level functional description of the hardware, but
also allows our compiler to generate the complete software program including
the sequential part of the workload, which accesses the hardware for
acceleration. Our system also provides high-level semantics to explore
different mappings of applications to a heterogeneous system, with the added
flexibility of being able to map at various throughput rates.
  We demonstrate our approach by mapping applications to a Xilinx Zynq system.
Using its FPGA with two low-power ARM cores, our design achieves up to 6x
higher performance and 8x lower energy compared to the quad-core ARM CPU on an
NVIDIA Tegra K1, and 3.5x higher performance with 12x lower energy compared to
the K1's 192-core GPU.","['Jing Pu', 'Steven Bell', 'Xuan Yang', 'Jeff Setter', 'Stephen Richardson', 'Jonathan Ragan-Kelley', 'Mark Horowitz']",2016-10-28T21:10:02Z,http://arxiv.org/abs/1610.09405v1,['cs.SE']
"Augmenting the thermal flux experiment: a mixed reality approach with
  the HoloLens","In the field of Virtual Reality (VR) and Augmented Reality (AR) technologies
have made huge progress during the last years and also reached the field of
education. The virtuality continuum, ranging from pure virtuality on one side
to the real world on the other has been successfully covered by the use of
immersive technologies like head-mounted displays, which allow to embed virtual
objects into the real surroundings, leading to a Mixed Reality (MR) experience.
In such an environment digital and real objects do not only co-exist, but
moreover are also able to interact with each other in real-time. These concepts
can be used to merge human perception of reality with digitally visualized
sensor data and thereby making the invisible visible. As a first example, in
this paper we introduce alongside the basic idea of this column an
MR-experiment in thermodynamics for a laboratory course for freshman students
in physics or other science and engineering subjects which uses physical data
from mobile devices for analyzing and displaying physical phenomena to
students.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-09-05T11:52:18Z,http://arxiv.org/abs/1709.01342v1,['physics.ed-ph']
"A Survey of Calibration Methods for Optical See-Through Head-Mounted
  Displays","Optical see-through head-mounted displays (OST HMDs) are a major output
medium for Augmented Reality, which have seen significant growth in popularity
and usage among the general public due to the growing release of
consumer-oriented models, such as the Microsoft Hololens. Unlike Virtual
Reality headsets, OST HMDs inherently support the addition of
computer-generated graphics directly into the light path between a user's eyes
and their view of the physical world. As with most Augmented and Virtual
Reality systems, the physical position of an OST HMD is typically determined by
an external or embedded 6-Degree-of-Freedom tracking system. However, in order
to properly render virtual objects, which are perceived as spatially aligned
with the physical environment, it is also necessary to accurately measure the
position of the user's eyes within the tracking system's coordinate frame. For
over 20 years, researchers have proposed various calibration methods to
determine this needed eye position. However, to date, there has not been a
comprehensive overview of these procedures and their requirements. Hence, this
paper surveys the field of calibration methods for OST HMDs. Specifically, it
provides insights into the fundamentals of calibration techniques, and presents
an overview of both manual and automatic approaches, as well as evaluation
methods and metrics. Finally, it also identifies opportunities for future
research. % relative to the tracking coordinate system, and, hence, its
position in 3D space.","['Jens Grubert', 'Yuta Itoh', 'Kenneth Moser', 'J. Edward Swan II']",2017-09-13T12:55:45Z,http://arxiv.org/abs/1709.04299v1,"['cs.HC', 'cs.CV']"
Multi-level Chaotic Maps for 3D Textured Model Encryption,"With rapid progress of Virtual Reality and Augmented Reality technologies, 3D
contents are the next widespread media in many applications. Thus, the
protection of 3D models is primarily important. Encryption of 3D models is
essential to maintain confidentiality. Previous work on encryption of 3D
surface model often consider the point clouds, the meshes and the textures
individually. In this work, a multi-level chaotic maps models for 3D textured
encryption was presented by observing the different contributions for
recognizing cipher 3D models between vertices (point cloud), polygons and
textures. For vertices which make main contribution for recognizing, we use
high level 3D Lu chaotic map to encrypt them. For polygons and textures which
make relatively smaller contributions for recognizing, we use 2D Arnold's cat
map and 1D Logistic map to encrypt them, respectively. The experimental results
show that our method can get similar performance with the other method use the
same high level chaotic map for point cloud, polygons and textures, while we
use less time. Besides, our method can resist more method of attacks such as
statistic attack, brute-force attack, correlation attack.","['Xin Jin', 'Shuyun Zhu', 'Le Wu', 'Geng Zhao', 'Xiaodong Li', 'Quan Zhou', 'Huimin Lu']",2017-09-25T08:20:17Z,http://arxiv.org/abs/1709.08364v2,"['cs.CV', 'cs.CR']"
SPP-Net: Deep Absolute Pose Regression with Synthetic Views,"Image based localization is one of the important problems in computer vision
due to its wide applicability in robotics, augmented reality, and autonomous
systems. There is a rich set of methods described in the literature how to
geometrically register a 2D image w.r.t.\ a 3D model. Recently, methods based
on deep (and convolutional) feedforward networks (CNNs) became popular for pose
regression. However, these CNN-based methods are still less accurate than
geometry based methods despite being fast and memory efficient. In this work we
design a deep neural network architecture based on sparse feature descriptors
to estimate the absolute pose of an image. Our choice of using sparse feature
descriptors has two major advantages: first, our network is significantly
smaller than the CNNs proposed in the literature for this task---thereby making
our approach more efficient and scalable. Second---and more importantly---,
usage of sparse features allows to augment the training data with synthetic
viewpoints, which leads to substantial improvements in the generalization
performance to unseen poses. Thus, our proposed method aims to combine the best
of the two worlds---feature-based localization and CNN-based pose
regression--to achieve state-of-the-art performance in the absolute pose
estimation. A detailed analysis of the proposed architecture and a rigorous
evaluation on the existing datasets are provided to support our method.","['Pulak Purkait', 'Cheng Zhao', 'Christopher Zach']",2017-12-09T23:45:03Z,http://arxiv.org/abs/1712.03452v1,['cs.CV']
Holoscopic 3D Micro-Gesture Database for Wearable Device Interaction,"With the rapid development of augmented reality (AR) and virtual reality (VR)
technology, human-computer interaction (HCI) has been greatly improved for
gaming interaction of AR and VR control. The finger micro-gesture is one of the
important interactive methods for HCI applications such as in the Google Soli
and Microsoft Kinect projects. However, the progress in this research is slow
due to the lack of high quality public available database. In this paper,
holoscopic 3D camera is used to capture high quality micro-gesture images and a
new unique holoscopic 3D micro-gesture (HoMG) database is produced. The
principle of the holoscopic 3D camera is based on the fly viewing system to see
the objects. HoMG database recorded the image sequence of 3 conventional
gestures from 40 participants under different settings and conditions. For the
purpose of micro-gesture recognition, HoMG has a video subset with 960 videos
and a still image subset with 30635 images. Initial micro-gesture recognition
on both subsets has been conducted using traditional 2D image and video
features and popular classifiers and some encouraging performance has been
achieved. The database will be available for the research communities and speed
up the research in this area.","['Yi Liu', 'Hongying Meng', 'Mohammad Rafiq Swash', 'Yona Falinie A. Gaus', 'Rui Qin']",2017-12-15T07:49:04Z,http://arxiv.org/abs/1712.05570v2,"['cs.HC', '68U99']"
Scene-Specific Pedestrian Detection Based on Parallel Vision,"As a special type of object detection, pedestrian detection in generic scenes
has made a significant progress trained with large amounts of labeled training
data manually. While the models trained with generic dataset work bad when they
are directly used in specific scenes. With special viewpoints, flow light and
backgrounds, datasets from specific scenes are much different from the datasets
from generic scenes. In order to make the generic scene pedestrian detectors
work well in specific scenes, the labeled data from specific scenes are needed
to adapt the models to the specific scenes. While labeling the data manually
spends much time and money, especially for specific scenes, each time with a
new specific scene, large amounts of images must be labeled. What's more, the
labeling information is not so accurate in the pixels manually and different
people make different labeling information. In this paper, we propose an
ACP-based method, with augmented reality's help, we build the virtual world of
specific scenes, and make people walking in the virtual scenes where it is
possible for them to appear to solve this problem of lacking labeled data and
the results show that data from virtual world is helpful to adapt generic
pedestrian detectors to specific scenes.","['Wenwen Zhang', 'Kunfeng Wang', 'Hua Qu', 'Jihong Zhao', 'Fei-Yue Wang']",2017-12-23T09:33:29Z,http://arxiv.org/abs/1712.08745v1,['cs.CV']
Cubic Range Error Model for Stereo Vision with Illuminators,"Use of low-cost depth sensors, such as a stereo camera setup with
illuminators, is of particular interest for numerous applications ranging from
robotics and transportation to mixed and augmented reality. The ability to
quantify noise is crucial for these applications, e.g., when the sensor is used
for map generation or to develop a sensor scheduling policy in a multi-sensor
setup. Range error models provide uncertainty estimates and help weigh the data
correctly in instances where range measurements are taken from different
vantage points or with different sensors. The weighing is important to fuse
range data into a map in a meaningful way, i.e., the high confidence data is
relied on most heavily. Such a model is derived in this work. We show that the
range error for stereo systems with integrated illuminators is cubic and
validate the proposed model experimentally with an off-the-shelf structured
light stereo system. The experiments confirm the validity of the model and
simplify the application of this type of sensor in robotics. The proposed error
model is relevant to any stereo system with low ambient light where the main
light source is located at the camera system. Among others, this is the case
for structured light stereo systems and night stereo systems with headlights.
In this work, we propose that the range error is cubic in range for stereo
systems with integrated illuminators. Experimental validation with an
off-the-shelf structured light stereo system shows that the exponent is between
2.4 and 2.6. The deviation is attributed to our model considering only shot
noise.","['Marius Huber', 'Timo Hinzmann', 'Roland Siegwart', 'Larry H. Matthies']",2018-03-11T10:23:25Z,http://arxiv.org/abs/1803.03932v1,['cs.CV']
Complex-YOLO: Real-time 3D Object Detection on Point Clouds,"Lidar based 3D object detection is inevitable for autonomous driving, because
it directly links to environmental understanding and therefore builds the base
for prediction and motion planning. The capacity of inferencing highly sparse
3D data in real-time is an ill-posed problem for lots of other application
areas besides automated vehicles, e.g. augmented reality, personal robotics or
industrial automation. We introduce Complex-YOLO, a state of the art real-time
3D object detection network on point clouds only. In this work, we describe a
network that expands YOLOv2, a fast 2D standard object detector for RGB images,
by a specific complex regression strategy to estimate multi-class 3D boxes in
Cartesian space. Thus, we propose a specific Euler-Region-Proposal Network
(E-RPN) to estimate the pose of the object by adding an imaginary and a real
fraction to the regression network. This ends up in a closed complex space and
avoids singularities, which occur by single angle estimations. The E-RPN
supports to generalize well during training. Our experiments on the KITTI
benchmark suite show that we outperform current leading methods for 3D object
detection specifically in terms of efficiency. We achieve state of the art
results for cars, pedestrians and cyclists by being more than five times faster
than the fastest competitor. Further, our model is capable of estimating all
eight KITTI-classes, including Vans, Trucks or sitting pedestrians
simultaneously with high accuracy.","['Martin Simon', 'Stefan Milz', 'Karl Amende', 'Horst-Michael Gross']",2018-03-16T12:54:40Z,http://arxiv.org/abs/1803.06199v2,['cs.CV']
Viewport-Driven Rate-Distortion Optimized 360° Video Streaming,"The growing popularity of virtual and augmented reality communications and
360{\deg} video streaming is moving video communication systems into much more
dynamic and resource-limited operating settings. The enormous data volume of
360{\deg} videos requires an efficient use of network bandwidth to maintain the
desired quality of experience for the end user. To this end, we propose a
framework for viewport-driven rate-distortion optimized 360{\deg} video
streaming that integrates the user view navigation pattern and the
spatiotemporal rate-distortion characteristics of the 360{\deg} video content
to maximize the delivered user quality of experience for the given
network/system resources. The framework comprises a methodology for
constructing dynamic heat maps that capture the likelihood of navigating
different spatial segments of a 360{\deg} video over time by the user, an
analysis and characterization of its spatiotemporal rate-distortion
characteristics that leverage preprocessed spatial tilling of the 360{\deg}
view sphere, and an optimization problem formulation that characterizes the
delivered user quality of experience given the user navigation patterns,
360{\deg} video encoding decisions, and the available system/network resources.
Our experimental results demonstrate the advantages of our framework over the
conventional approach of streaming a monolithic uniformly encoded 360{\deg}
video and a state-of-the-art reference method. Considerable video quality gains
of 4 - 5 dB are demonstrated in the case of two popular 4K 360{\deg} videos.","['Jacob Chakareski', 'Ridvan Aksu', 'Xavier Corbillon', 'Gwendal Simon', 'Viswanathan Swaminathan']",2018-03-21T23:58:35Z,http://arxiv.org/abs/1803.08177v1,['cs.MM']
"A Single-shot-per-pose Camera-Projector Calibration System For Imperfect
  Planar Targets","Existing camera-projector calibration methods typically warp feature points
from a camera image to a projector image using estimated homographies, and
often suffer from errors in camera parameters and noise due to imperfect
planarity of the calibration target. In this paper we propose a simple yet
robust solution that explicitly deals with these challenges. Following the
structured light (SL) camera-project calibration framework, a carefully
designed correspondence algorithm is built on top of the De Bruijn patterns.
Such correspondence is then used for initial camera-projector calibration.
Then, to gain more robustness against noises, especially those from an
imperfect planar calibration board, a bundle adjustment algorithm is developed
to jointly optimize the estimated camera and projector models. Aside from the
robustness, our solution requires only one shot of SL pattern for each
calibration board pose, which is much more convenient than multi-shot solutions
in practice. Data validations are conducted on both synthetic and real
datasets, and our method shows clear advantages over existing methods in all
experiments.","['Bingyao Huang', 'Samed Ozdemir', 'Ying Tang', 'Chunyuan Liao', 'Haibin Ling']",2018-03-24T05:25:23Z,http://arxiv.org/abs/1803.09058v2,['cs.CV']
"HDM-Net: Monocular Non-Rigid 3D Reconstruction with Learned Deformation
  Model","Monocular dense 3D reconstruction of deformable objects is a hard ill-posed
problem in computer vision. Current techniques either require dense
correspondences and rely on motion and deformation cues, or assume a highly
accurate reconstruction (referred to as a template) of at least a single frame
given in advance and operate in the manner of non-rigid tracking. Accurate
computation of dense point tracks often requires multiple frames and might be
computationally expensive. Availability of a template is a very strong prior
which restricts system operation to a pre-defined environment and scenarios. In
this work, we propose a new hybrid approach for monocular non-rigid
reconstruction which we call Hybrid Deformation Model Network (HDM-Net). In our
approach, deformation model is learned by a deep neural network, with a
combination of domain-specific loss functions. We train the network with
multiple states of a non-rigidly deforming structure with a known shape at
rest. HDM-Net learns different reconstruction cues including texture-dependent
surface deformations, shading and contours. We show generalisability of HDM-Net
to states not presented in the training dataset, with unseen textures and under
new illumination conditions. Experiments with noisy data and a comparison with
other methods demonstrate robustness and accuracy of the proposed approach and
suggest possible application scenarios of the new technique in interventional
diagnostics and augmented reality.","['Vladislav Golyanik', 'Soshi Shimada', 'Kiran Varanasi', 'Didier Stricker']",2018-03-27T17:31:47Z,http://arxiv.org/abs/1803.10193v2,['cs.CV']
"Automation of the Export Data from Open Journal Systems to the Russian
  Science Citation Index","It is shown that the calculation of scientometric indicators of the scientist
and also the scientific journal continues to be an actual problem nowadays. It
is revealed that the leading scientometric databases have the capabilities of
automated metadata collection from the scientific journal website by the use of
specialized electronic document management systems, in particular Open Journal
Systems. It is established that Open Journal Systems successfully exports
metadata about an article from scientific journals to scientometric databases
Scopus, Web of Science and Google Scholar. However, there is no standard method
of export from Open Journal Systems to such scientometric databases as the
Russian Science Citation Index and Index Copernicus, which determined the need
for research. The aim of the study is to develop the plug-in to the Open
Journal Systems for the export of data from this system to scientometric
database Russian Science Citation Index. As a result of the study, an
infological model for exporting metadata from Open Journal Systems to the
Russian Science Citation Index was proposed. The SirenExpo plug-in was
developed to export data from Open Journal Systems to the Russian Science
Citation Index by the use of the Articulus release preparation system.","['Serhiy O. Semerikov', 'Vladyslav S. Pototskyi', 'Kateryna I. Slovak', 'Svitlana M. Hryshchenko', 'Arnold E. Kiv']",2018-06-30T18:00:24Z,http://arxiv.org/abs/1807.00212v2,"['cs.DL', 'H.3.6; H.3.7']"
"Fast and Accurate Point Cloud Registration using Trees of Gaussian
  Mixtures","Point cloud registration sits at the core of many important and challenging
3D perception problems including autonomous navigation, SLAM, object/scene
recognition, and augmented reality. In this paper, we present a new
registration algorithm that is able to achieve state-of-the-art speed and
accuracy through its use of a hierarchical Gaussian Mixture Model (GMM)
representation. Our method constructs a top-down multi-scale representation of
point cloud data by recursively running many small-scale data likelihood
segmentations in parallel on a GPU. We leverage the resulting representation
using a novel PCA-based optimization criterion that adaptively finds the best
scale to perform data association between spatial subsets of point cloud data.
Compared to previous Iterative Closest Point and GMM-based techniques, our
tree-based point association algorithm performs data association in
logarithmic-time while dynamically adjusting the level of detail to best match
the complexity and spatial distribution characteristics of local scene
geometry. In addition, unlike other GMM methods that restrict covariances to be
isotropic, our new PCA-based optimization criterion well-approximates the true
MLE solution even when fully anisotropic Gaussian covariances are used.
Efficient data association, multi-scale adaptability, and a robust MLE
approximation produce an algorithm that is up to an order of magnitude both
faster and more accurate than current state-of-the-art on a wide variety of 3D
datasets captured from LiDAR to structured light.","['Ben Eckart', 'Kihwan Kim', 'Jan Kautz']",2018-07-06T23:44:51Z,http://arxiv.org/abs/1807.02587v1,['cs.CV']
Hybrid Scene Compression for Visual Localization,"Localizing an image wrt. a 3D scene model represents a core task for many
computer vision applications. An increasing number of real-world applications
of visual localization on mobile devices, e.g., Augmented Reality or autonomous
robots such as drones or self-driving cars, demand localization approaches to
minimize storage and bandwidth requirements. Compressing the 3D models used for
localization thus becomes a practical necessity. In this work, we introduce a
new hybrid compression algorithm that uses a given memory limit in a more
effective way. Rather than treating all 3D points equally, it represents a
small set of points with full appearance information and an additional, larger
set of points with compressed information. This enables our approach to obtain
a more complete scene representation without increasing the memory
requirements, leading to a superior performance compared to previous
compression schemes. As part of our contribution, we show how to handle
ambiguous matches arising from point compression during RANSAC. Besides
outperforming previous compression techniques in terms of pose accuracy under
the same memory constraints, our compression scheme itself is also more
efficient. Furthermore, the localization rates and accuracy obtained with our
approach are comparable to state-of-the-art feature-based methods, while using
a small fraction of the memory.","['Federico Camposeco', 'Andrea Cohen', 'Marc Pollefeys', 'Torsten Sattler']",2018-07-19T16:04:58Z,http://arxiv.org/abs/1807.07512v2,['cs.CV']
Aligning Points to Lines: Provable Approximations,"We suggest a new optimization technique for minimizing the sum $\sum_{i=1}^n
f_i(x)$ of $n$ non-convex real functions that satisfy a property that we call
piecewise log-Lipschitz. This is by forging links between techniques in
computational geometry, combinatorics and convex optimization. As an example
application, we provide the first constant-factor approximation algorithms
whose running-time is polynomial in $n$ for the fundamental problem of
\emph{Points-to-Lines alignment}: Given $n$ points $p_1,\cdots,p_n$ and $n$
lines $\ell_1,\cdots,\ell_n$ on the plane and $z>0$, compute the matching
$\pi:[n]\to[n]$ and alignment (rotation matrix $R$ and a translation vector
$t$) that minimize the sum of Euclidean distances $\sum_{i=1}^n
\mathrm{dist}(Rp_i-t,\ell_{\pi(i)})^z$ between each point to its corresponding
line.
  This problem is non-trivial even if $z=1$ and the matching $\pi$ is given. If
$\pi$ is given, the running time of our algorithms is $O(n^3)$, and even
near-linear in $n$ using core-sets that support: streaming, dynamic, and
distributed parallel computations in poly-logarithmic update time.
Generalizations for handling e.g. outliers or pseudo-distances such as
$M$-estimators for the problem are also provided.
  Experimental results and open source code show that our provable algorithms
improve existing heuristics also in practice. A companion demonstration video
in the context of Augmented Reality shows how such algorithms may be used in
real-time systems.","['Ibrahim Jubran', 'Dan Feldman']",2018-07-23T06:45:15Z,http://arxiv.org/abs/1807.08446v3,"['cs.LG', 'cs.CG', 'stat.ML']"
"Energy-Efficient Mobile-Edge Computation Offloading for Applications
  with Shared Data","Mobile-edge computation offloading (MECO) has been recognized as a promising
solution to alleviate the burden of resource-limited Internet of Thing (IoT)
devices by offloading computation tasks to the edge of cellular networks (also
known as {\em cloudlet}). Specifically, latency-critical applications such as
virtual reality (VR) and augmented reality (AR) have inherent collaborative
properties since part of the input/output data are shared by different users in
proximity. In this paper, we consider a multi-user fog computing system, in
which multiple single-antenna mobile users running applications featuring
shared data can choose between (partially) offloading their individual tasks to
a nearby single-antenna cloudlet for remote execution and performing pure local
computation. The mobile users' energy minimization is formulated as a convex
problem, subject to the total computing latency constraint, the total energy
constraints for individual data downloading, and the computing frequency
constraints for local computing, for which classical Lagrangian duality can be
applied to find the optimal solution. Based upon the semi-closed form solution,
the shared data proves to be transmitted by only one of the mobile users
instead of multiple ones. Besides, compared to those baseline algorithms
without considering the shared data property or the mobile users' local
computing capabilities, the proposed joint computation offloading and
communications resource allocation provides significant energy saving.","['Xiangyu He', 'Hong Xing', 'Yue Chen', 'Arumugam Nallanathan']",2018-09-04T13:52:13Z,http://arxiv.org/abs/1809.00966v1,"['cs.IT', 'math.IT']"
"Navion: A 2mW Fully Integrated Real-Time Visual-Inertial Odometry
  Accelerator for Autonomous Navigation of Nano Drones","This paper presents Navion, an energy-efficient accelerator for
visual-inertial odometry (VIO) that enables autonomous navigation of
miniaturized robots (e.g., nano drones), and virtual/augmented reality on
portable devices. The chip uses inertial measurements and mono/stereo images to
estimate the drone's trajectory and a 3D map of the environment. This estimate
is obtained by running a state-of-the-art VIO algorithm based on non-linear
factor graph optimization, which requires large irregularly structured memories
and heterogeneous computation flow. To reduce the energy consumption and
footprint, the entire VIO system is fully integrated on chip to eliminate
costly off-chip processing and storage. This work uses compression and exploits
both structured and unstructured sparsity to reduce on-chip memory size by
4.1$\times$. Parallelism is used under tight area constraints to increase
throughput by 43%. The chip is fabricated in 65nm CMOS, and can process
752$\times$480 stereo images from EuRoC dataset in real-time at 20 frames per
second (fps) consuming only an average power of 2mW. At its peak performance,
Navion can process stereo images at up to 171 fps and inertial measurements at
up to 52 kHz, while consuming an average of 24mW. The chip is configurable to
maximize accuracy, throughput and energy-efficiency trade-offs and to adapt to
different environments. To the best of our knowledge, this is the first fully
integrated VIO system in an ASIC.","['Amr Suleiman', 'Zhengdong Zhang', 'Luca Carlone', 'Sertac Karaman', 'Vivienne Sze']",2018-09-15T22:54:05Z,http://arxiv.org/abs/1809.05780v1,['cs.RO']
"NestDNN: Resource-Aware Multi-Tenant On-Device Deep Learning for
  Continuous Mobile Vision","Mobile vision systems such as smartphones, drones, and augmented-reality
headsets are revolutionizing our lives. These systems usually run multiple
applications concurrently and their available resources at runtime are dynamic
due to events such as starting new applications, closing existing applications,
and application priority changes. In this paper, we present NestDNN, a
framework that takes the dynamics of runtime resources into account to enable
resource-aware multi-tenant on-device deep learning for mobile vision systems.
NestDNN enables each deep learning model to offer flexible resource-accuracy
trade-offs. At runtime, it dynamically selects the optimal resource-accuracy
trade-off for each deep learning model to fit the model's resource demand to
the system's available runtime resources. In doing so, NestDNN efficiently
utilizes the limited resources in mobile vision systems to jointly maximize the
performance of all the concurrently running applications. Our experiments show
that compared to the resource-agnostic status quo approach, NestDNN achieves as
much as 4.2% increase in inference accuracy, 2.0x increase in video frame
processing rate and 1.7x reduction on energy consumption.","['Biyi Fang', 'Xiao Zeng', 'Mi Zhang']",2018-10-23T21:07:42Z,http://arxiv.org/abs/1810.10090v1,['cs.CV']
scenery: Flexible Virtual Reality Visualization on the Java VM,"Life science today involves computational analysis of a large amount and
variety of data, such as volumetric data acquired by state-of-the-art
microscopes, or mesh data from analysis of such data or simulations.
Visualization is often the first step in making sense of data, and a crucial
part of building and debugging analysis pipelines. It is therefore important
that visualizations can be quickly prototyped, as well as developed or embedded
into full applications. In order to better judge spatiotemporal relationships,
immersive hardware, such as Virtual or Augmented Reality (VR/AR) headsets and
associated controllers are becoming invaluable tools. In this work we introduce
scenery, a flexible VR/AR visualization framework for the Java VM that can
handle mesh and large volumetric data, containing multiple views, timepoints,
and color channels. scenery is free and open-source software, works on all
major platforms, and uses the Vulkan or OpenGL rendering APIs. We introduce
scenery's main features and example applications, such as its use in VR for
microscopy, in the biomedical image analysis software Fiji, or for visualizing
agent-based simulations.","['Ulrik Günther', 'Tobias Pietzsch', 'Aryaman Gupta', 'Kyle I. S. Harrington', 'Pavel Tomancak', 'Stefan Gumhold', 'Ivo F. Sbalzarini']",2019-06-16T17:01:20Z,http://arxiv.org/abs/1906.06726v3,['cs.GR']
"Cross-Domain Conditional Generative Adversarial Networks for
  Stereoscopic Hyperrealism in Surgical Training","Phantoms for surgical training are able to mimic cutting and suturing
properties and patient-individual shape of organs, but lack a realistic visual
appearance that captures the heterogeneity of surgical scenes. In order to
overcome this in endoscopic approaches, hyperrealistic concepts have been
proposed to be used in an augmented reality-setting, which are based on deep
image-to-image transformation methods. Such concepts are able to generate
realistic representations of phantoms learned from real intraoperative
endoscopic sequences. Conditioned on frames from the surgical training process,
the learned models are able to generate impressive results by transforming
unrealistic parts of the image (e.g.\ the uniform phantom texture is replaced
by the more heterogeneous texture of the tissue). Image-to-image synthesis
usually learns a mapping $G:X~\to~Y$ such that the distribution of images from
$G(X)$ is indistinguishable from the distribution $Y$. However, it does not
necessarily force the generated images to be consistent and without artifacts.
In the endoscopic image domain this can affect depth cues and stereo
consistency of a stereo image pair, which ultimately impairs surgical vision.
We propose a cross-domain conditional generative adversarial network approach
(GAN) that aims to generate more consistent stereo pairs. The results show
substantial improvements in depth perception and realism evaluated by 3 domain
experts and 3 medical students on a 3D monitor over the baseline method. In 84
of 90 instances our proposed method was preferred or rated equal to the
baseline.","['Sandy Engelhardt', 'Lalith Sharan', 'Matthias Karck', 'Raffaele De Simone', 'Ivo Wolf']",2019-06-24T15:05:07Z,http://arxiv.org/abs/1906.10011v1,"['eess.IV', 'cs.CV', 'cs.CY']"
"Large-scale, real-time visual-inertial localization revisited","The overarching goals in image-based localization are scale, robustness and
speed. In recent years, approaches based on local features and sparse 3D
point-cloud models have both dominated the benchmarks and seen successful
realworld deployment. They enable applications ranging from robot navigation,
autonomous driving, virtual and augmented reality to device geo-localization.
Recently end-to-end learned localization approaches have been proposed which
show promising results on small scale datasets. However the positioning
accuracy, scalability, latency and compute & storage requirements of these
approaches remain open challenges. We aim to deploy localization at
global-scale where one thus relies on methods using local features and sparse
3D models. Our approach spans from offline model building to real-time
client-side pose fusion. The system compresses appearance and geometry of the
scene for efficient model storage and lookup leading to scalability beyond what
what has been previously demonstrated. It allows for low-latency localization
queries and efficient fusion run in real-time on mobile platforms by combining
server-side localization with real-time visual-inertial-based camera pose
tracking. In order to further improve efficiency we leverage a combination of
priors, nearest neighbor search, geometric match culling and a cascaded pose
candidate refinement step. This combination outperforms previous approaches
when working with large scale models and allows deployment at unprecedented
scale. We demonstrate the effectiveness of our approach on a proof-of-concept
system localizing 2.5 million images against models from four cities in
different regions on the world achieving query latencies in the 200ms range.","['Simon Lynen', 'Bernhard Zeisl', 'Dror Aiger', 'Michael Bosse', 'Joel Hesch', 'Marc Pollefeys', 'Roland Siegwart', 'Torsten Sattler']",2019-06-30T08:45:58Z,http://arxiv.org/abs/1907.00338v1,['cs.CV']
"Semi-Bagging Based Deep Neural Architecture to Extract Text from High
  Entropy Images","Extracting texts of various size and shape from images containing multiple
objects is an important problem in many contexts, especially, in connection to
e-commerce, augmented reality assistance system in natural scene, etc. The
existing works (based on only CNN) often perform sub-optimally when the image
contains regions of high entropy having multiple objects. This paper presents
an end-to-end text detection strategy combining a segmentation algorithm and an
ensemble of multiple text detectors of different types to detect text in every
individual image segments independently. The proposed strategy involves a
super-pixel based image segmenter which splits an image into multiple regions.
A convolutional deep neural architecture is developed which works on each of
the segments and detects texts of multiple shapes, sizes, and structures. It
outperforms the competing methods in terms of coverage in detecting texts in
images especially the ones where the text of various types and sizes are
compacted in a small region along with various other objects. Furthermore, the
proposed text detection method along with a text recognizer outperforms the
existing state-of-the-art approaches in extracting text from high entropy
images. We validate the results on a dataset consisting of product images on an
e-commerce website.","['Pranay Dugar', 'Anirban Chatterjee', 'Rajesh Shreedhar Bhat', 'Saswata Sahoo']",2019-07-02T10:26:14Z,http://arxiv.org/abs/1907.01284v1,"['cs.CV', 'cs.LG', 'eess.IV']"
"Reflective-AR Display: An Interaction Methodology for Virtual-Real
  Alignment in Medical Robotics","Robot-assisted minimally invasive surgery has shown to improve patient
outcomes, as well as reduce complications and recovery time for several
clinical applications. While increasingly configurable robotic arms can
maximize reach and avoid collisions in cluttered environments, positioning them
appropriately during surgery is complicated because safety regulations prevent
automatic driving. We propose a head-mounted display (HMD) based augmented
reality (AR) system designed to guide optimal surgical arm set up. The staff
equipped with HMD aligns the robot with its planned virtual counterpart. In
this user-centric setting, the main challenge is the perspective ambiguities
hindering such collaborative robotic solution. To overcome this challenge, we
introduce a novel registration concept for intuitive alignment of AR content to
its physical counterpart by providing a multi-view AR experience via
reflective-AR displays that simultaneously show the augmentations from multiple
viewpoints. Using this system, users can visualize different perspectives while
actively adjusting the pose to determine the registration transformation that
most closely superimposes the virtual onto the real. The experimental results
demonstrate improvement in the interactive alignment of a virtual and real
robot when using a reflective-AR display. We also present measurements from
configuring a robotic manipulator in a simulated trocar placement surgery using
the AR guidance methodology.","['Javad Fotouhi', 'Tianyu Song', 'Arian Mehrfard', 'Giacomo Taylor', 'Qiaochu Wang', 'Fengfang Xian', 'Alejandro Martin-Gomez', 'Bernhard Fuerst', 'Mehran Armand', 'Mathias Unberath', 'Nassir Navab']",2019-07-23T21:27:44Z,http://arxiv.org/abs/1907.10138v2,"['cs.RO', 'cs.CV']"
Electronic health record in the era of industry 4.0: the French example,"The recent implementation of the Electronic Health Record (EHR) in France is
part of a more general process of digitizing information flows, as the world
enters the fourth industrial revolution in a phenomenon known as Industry 4.0.
Behind this concept lies the concern to allow Man to remain permanently in
control of his destiny, despite an increasingly interconnected world (Internet
of Things, cooperative robots, augmented reality, etc.). Accordingly, the
implementation of EHR must guarantee the respect for the private life of each
citizen. From this perspective, healthcare professionals will therefore have to
constantly ensure the protection of medical confidentiality during Electronic
Data Interchange (EDI). This paper summarises the current state of the use of
EHR in France. Based on a survey conducted by the European Commission to assess
the deployment of digitalisation in the health sector in EU countries, this
article aims to highlight the opportunities and perspectives that Industry 4.0
could bring to the health sector in France. However, this study also identifies
a number of limits related to the application of such a system, the first of
which is cyber threat or transhumanism. To this end, a SWOT matrix identifies
the strengths and weaknesses related to the implementation of the French EHR.","['Sarah Manard', 'Nicolas Vergos', 'Simon Tamayo', 'Frédéric Fontane']",2019-07-24T09:24:24Z,http://arxiv.org/abs/1907.10322v2,['cs.CY']
"Millimeter Wave Base Stations with Cameras: Vision Aided Beam and
  Blockage Prediction","This paper investigates a novel research direction that leverages vision to
help overcome the critical wireless communication challenges. In particular,
this paper considers millimeter wave (mmWave) communication systems, which are
principal components of 5G and beyond. These systems face two important
challenges: (i) the large training overhead associated with selecting the
optimal beam and (ii) the reliability challenge due to the high sensitivity to
link blockages. Interestingly, most of the devices that employ mmWave arrays
will likely also use cameras, such as 5G phones, self-driving vehicles, and
virtual/augmented reality headsets. Therefore, we investigate the potential
gains of employing cameras at the mmWave base stations and leveraging their
visual data to help overcome the beam selection and blockage prediction
challenges. To do that, this paper exploits computer vision and deep learning
tools to predict mmWave beams and blockages directly from the camera RGB images
and the sub-6GHz channels. The experimental results reveal interesting insights
into the effectiveness of such solutions. For example, the deep learning model
is capable of achieving over 90\% beam prediction accuracy, which only requires
snapping a shot of the scene and zero overhead.","['Muhammad Alrabeiah', 'Andrew Hredzak', 'Ahmed Alkhateeb']",2019-11-14T17:28:31Z,http://arxiv.org/abs/1911.06255v2,"['cs.IT', 'eess.SP', 'math.IT']"
ASV: Accelerated Stereo Vision System,"Estimating depth from stereo vision cameras, i.e., ""depth from stereo"", is
critical to emerging intelligent applications deployed in energy- and
performance-constrained devices, such as augmented reality headsets and mobile
autonomous robots. While existing stereo vision systems make trade-offs between
accuracy, performance and energy-efficiency, we describe ASV, an accelerated
stereo vision system that simultaneously improves both performance and
energy-efficiency while achieving high accuracy. The key to ASV is to exploit
unique characteristics inherent to stereo vision, and apply stereo-specific
optimizations, both algorithmically and computationally. We make two
contributions. Firstly, we propose a new stereo algorithm, invariant-based
stereo matching (ISM), that achieves significant speedup while retaining high
accuracy. The algorithm combines classic ""hand-crafted"" stereo algorithms with
recent developments in Deep Neural Networks (DNNs), by leveraging the
correspondence invariant unique to stereo vision systems. Secondly, we observe
that the bottleneck of the ISM algorithm is the DNN inference, and in
particular the deconvolution operations that introduce massive
compute-inefficiencies. We propose a set of software optimizations that
mitigate these inefficiencies. We show that with less than 0.5% hardware area
overhead, these algorithmic and computational optimizations can be effectively
integrated within a conventional DNN accelerator. Overall, ASV achieves 5x
speedup and 85% energy saving with 0.02% accuracy loss compared to today
DNN-based stereo vision systems.","['Yu Feng', 'Paul Whatmough', 'Yuhao Zhu']",2019-11-15T18:44:25Z,http://arxiv.org/abs/1911.07919v1,['cs.CV']
Deep Template-based Object Instance Detection,"Much of the focus in the object detection literature has been on the problem
of identifying the bounding box of a particular class of object in an image.
Yet, in contexts such as robotics and augmented reality, it is often necessary
to find a specific object instance---a unique toy or a custom industrial part
for example---rather than a generic object class. Here, applications can
require a rapid shift from one object instance to another, thus requiring fast
turnaround which affords little-to-no training time. What is more, gathering a
dataset and training a model for every new object instance to be detected can
be an expensive and time-consuming process. In this context, we propose a
generic 2D object instance detection approach that uses example viewpoints of
the target object at test time to retrieve its 2D location in RGB images,
without requiring any additional training (i.e. fine-tuning) step. To this end,
we present an end-to-end architecture that extracts global and local
information of the object from its viewpoints. The global information is used
to tune early filters in the backbone while local viewpoints are correlated
with the input image. Our method offers an improvement of almost 30 mAP over
the previous template matching methods on the challenging Occluded Linemod
dataset (overall mAP of 50.7). Our experiments also show that our single
generic model (not trained on any of the test objects) yields detection results
that are on par with approaches that are trained specifically on the target
objects.","['Jean-Philippe Mercier', 'Mathieu Garon', 'Philippe Giguère', 'Jean-François Lalonde']",2019-11-26T20:38:26Z,http://arxiv.org/abs/1911.11822v3,['cs.CV']
Content Generation for Workforce Training,"Efficient workforce training is needed in today's world in which technology
is continually changing the nature of work. Students need to be prepared to
enter the workforce. Employees need to become lifelong learners to stay
up-to-date in their work and to adapt when job functions are eliminated. The
training needs are across all industries - including manufacturing,
construction, and healthcare. Computing systems, in particular
Virtual/Augmented Reality systems, have been adopted in many training
application and show even more promise in the future. However, there are
fundamental limitations in today's systems that limit the domains where
computing systems can be applied and the extent to which they can be deployed.
These limitations need to be addressed by new computing research. In particular
research is needed at multiple levels:
  - Application Data Collection Level Requiring High Security and Privacy
Protections
  - Training Material Authoring Level
  - Software Systems Level
  - Hardware Level
  To accomplish these research goals, a training community needs to be
established to do research in end-to-end training systems and to create a
community of learning and domain experts available for consulting for in depth
computing research on individual system components.","['Holly Rushmeier', 'Kapil Chalil Madathil', 'Jessica Hodgins', 'Beth Mynatt', 'Tony Derose', 'Blair Macintyre', 'other workshop participants']",2019-12-11T20:25:40Z,http://arxiv.org/abs/1912.05606v1,['cs.CY']
Artificial Intelligence-Enabled Intelligent 6G Networks,"With the rapid development of smart terminals and infrastructures, as well as
diversified applications (e.g., virtual and augmented reality, remote surgery
and holographic projection) with colorful requirements, current networks (e.g.,
4G and upcoming 5G networks) may not be able to completely meet quickly rising
traffic demands. Accordingly, efforts from both industry and academia have
already been put to the research on 6G networks. Recently, artificial
intelligence (AI) has been utilized as a new paradigm for the design and
optimization of 6G networks with a high level of intelligence. Therefore, this
article proposes an AI-enabled intelligent architecture for 6G networks to
realize knowledge discovery, smart resource management, automatic network
adjustment and intelligent service provisioning, where the architecture is
divided into four layers: intelligent sensing layer, data mining and analytics
layer, intelligent control layer and smart application layer. We then review
and discuss the applications of AI techniques for 6G networks and elaborate how
to employ the AI techniques to efficiently and effectively optimize the network
performance, including AI-empowered mobile edge computing, intelligent mobility
and handover management, and smart spectrum management. Moreover, we highlight
important future research directions and potential solutions for AI-enabled
intelligent 6G networks, including computation efficiency, algorithms
robustness, hardware development and energy management.","['Helin Yang', 'Arokiaswami Alphones', 'Zehui Xiong', 'Dusit Niyato', 'Jun Zhao', 'Kaishun Wu']",2019-12-12T03:00:37Z,http://arxiv.org/abs/1912.05744v2,"['cs.NI', 'eess.SP']"
"Point Cloud Rendering after Coding: Impacts on Subjective and Objective
  Quality","Recently, point clouds have shown to be a promising way to represent 3D
visual data for a wide range of immersive applications, from augmented reality
to autonomous cars. Emerging imaging sensors have made easier to perform richer
and denser point cloud acquisition, notably with millions of points, thus
raising the need for efficient point cloud coding solutions. In such a
scenario, it is important to evaluate the impact and performance of several
processing steps in a point cloud communication system, notably the quality
degradations associated to point cloud coding solutions. Moreover, since point
clouds are not directly visualized but rather processed with a rendering
algorithm before shown on any display, the perceived quality of point cloud
data highly depends on the rendering solution. In this context, the main
objective of this paper is to study the impact of several coding and rendering
solutions on the perceived user quality and in the performance of available
objective quality assessment metrics. Another contribution regards the
assessment of recent MPEG point cloud coding solutions for several popular
rendering methods which were never presented before. The conclusions regard the
visibility of three types of coding artifacts for the three considered
rendering approaches as well as the strengths and weakness of objective quality
metrics when point clouds are rendered after coding.","['Alireza Javaheri', 'Catarina Brites', 'Fernando Pereira', 'Joao Ascenso']",2019-12-19T11:40:26Z,http://arxiv.org/abs/1912.09137v2,"['eess.IV', 'cs.MM']"
"A Vision-based Scheme for Kinematic Model Construction of
  Re-configurable Modular Robots","Re-configurable modular robotic (RMR) systems are advantageous for their
reconfigurability and versatility. A new modular robot can be built for a
specific task by using modules as building blocks. However, constructing a
kinematic model for a newly conceived robot requires significant work. Due to
the finite size of module-types, models of all module-types can be built
individually and stored in a database beforehand. With this priori knowledge,
the model construction process can be automated by detecting the modules and
their corresponding interconnections. Previous literature proposed theoretical
frameworks for constructing kinematic models of modular robots, assuming that
such information was known a priori. While well-devised mechanisms and built-in
sensors can be employed to detect these parameters automatically, they
significantly complicate the module design and thus are expensive. In this
paper, we propose a vision-based method to identify kinematic chains and
automatically construct robot models for modular robots. Each module is affixed
with augmented reality (AR) tags that are encoded with unique IDs. An image of
a modular robot is taken and the detected modules are recognized by querying a
database that maintains all module information. The poses of detected modules
are used to compute: (i) the connection between modules and (ii) joint angles
of joint-modules. Finally, the robot serial-link chain is identified and the
kinematic model constructed and visualized. Our experimental results validate
the effectiveness of our approach. While implementation with only our RMR is
shown, our method can be applied to other RMRs where self-identification is not
possible.","['Kewei Lin', 'Juan Rojas', 'Yisheng Guan']",2017-03-11T10:13:25Z,http://arxiv.org/abs/1703.03941v1,['cs.RO']
Benchmarking 6DOF Outdoor Visual Localization in Changing Conditions,"Visual localization enables autonomous vehicles to navigate in their
surroundings and augmented reality applications to link virtual to real worlds.
Practical visual localization approaches need to be robust to a wide variety of
viewing condition, including day-night changes, as well as weather and seasonal
variations, while providing highly accurate 6 degree-of-freedom (6DOF) camera
pose estimates. In this paper, we introduce the first benchmark datasets
specifically designed for analyzing the impact of such factors on visual
localization. Using carefully created ground truth poses for query images taken
under a wide variety of conditions, we evaluate the impact of various factors
on 6DOF camera pose estimation accuracy through extensive experiments with
state-of-the-art localization approaches. Based on our results, we draw
conclusions about the difficulty of different conditions, showing that
long-term localization is far from solved, and propose promising avenues for
future work, including sequence-based localization approaches and the need for
better local features. Our benchmark is available at visuallocalization.net.","['Torsten Sattler', 'Will Maddern', 'Carl Toft', 'Akihiko Torii', 'Lars Hammarstrand', 'Erik Stenborg', 'Daniel Safari', 'Masatoshi Okutomi', 'Marc Pollefeys', 'Josef Sivic', 'Fredrik Kahl', 'Tomas Pajdla']",2017-07-28T02:51:11Z,http://arxiv.org/abs/1707.09092v3,['cs.CV']
Analytical Cost Metrics : Days of Future Past,"As we move towards the exascale era, the new architectures must be capable of
running the massive computational problems efficiently. Scientists and
researchers are continuously investing in tuning the performance of
extreme-scale computational problems. These problems arise in almost all areas
of computing, ranging from big data analytics, artificial intelligence, search,
machine learning, virtual/augmented reality, computer vision, image/signal
processing to computational science and bioinformatics. With Moore's law
driving the evolution of hardware platforms towards exascale, the dominant
performance metric (time efficiency) has now expanded to also incorporate
power/energy efficiency. Therefore, the major challenge that we face in
computing systems research is: ""how to solve massive-scale computational
problems in the most time/power/energy efficient manner?""
  The architectures are constantly evolving making the current performance
optimizing strategies less applicable and new strategies to be invented. The
solution is for the new architectures, new programming models, and applications
to go forward together. Doing this is, however, extremely hard. There are too
many design choices in too many dimensions. We propose the following strategy
to solve the problem: (i) Models - Develop accurate analytical models (e.g.
execution time, energy, silicon area) to predict the cost of executing a given
program, and (ii) Complete System Design - Simultaneously optimize all the cost
models for the programs (computational problems) to obtain the most
time/area/power/energy efficient solution. Such an optimization problem evokes
the notion of codesign.","['Nirmal Prajapati', 'Sanjay Rajopadhye', 'Hristo Djidjev']",2018-02-05T06:51:02Z,http://arxiv.org/abs/1802.01957v1,"['cs.PF', 'cs.PL']"
Security and Privacy Approaches in Mixed Reality: A Literature Survey,"Mixed reality (MR) technology development is now gaining momentum due to
advances in computer vision, sensor fusion, and realistic display technologies.
With most of the research and development focused on delivering the promise of
MR, there is only barely a few working on the privacy and security implications
of this technology. This survey paper aims to put in to light these risks, and
to look into the latest security and privacy work on MR. Specifically, we list
and review the different protection approaches that have been proposed to
ensure user and data security and privacy in MR. We extend the scope to include
work on related technologies such as augmented reality (AR), virtual reality
(VR), and human-computer interaction (HCI) as crucial components, if not the
origins, of MR, as well as numerous related work from the larger area of mobile
devices, wearables, and Internet-of-Things (IoT). We highlight the lack of
investigation, implementation, and evaluation of data protection approaches in
MR. Further challenges and directions on MR security and privacy are also
discussed.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-02-15T23:33:45Z,http://arxiv.org/abs/1802.05797v3,"['cs.CR', 'cs.CY', 'cs.HC']"
Constructing Category-Specific Models for Monocular Object-SLAM,"We present a new paradigm for real-time object-oriented SLAM with a monocular
camera. Contrary to previous approaches, that rely on object-level models, we
construct category-level models from CAD collections which are now widely
available. To alleviate the need for huge amounts of labeled data, we develop a
rendering pipeline that enables synthesis of large datasets from a limited
amount of manually labeled data. Using data thus synthesized, we learn
category-level models for object deformations in 3D, as well as discriminative
object features in 2D. These category models are instance-independent and aid
in the design of object landmark observations that can be incorporated into a
generic monocular SLAM framework. Where typical object-SLAM approaches usually
solve only for object and camera poses, we also estimate object shape
on-the-fly, allowing for a wide range of objects from the category to be
present in the scene. Moreover, since our 2D object features are learned
discriminatively, the proposed object-SLAM system succeeds in several scenarios
where sparse feature-based monocular SLAM fails due to insufficient features or
parallax. Also, the proposed category-models help in object instance retrieval,
useful for Augmented Reality (AR) applications. We evaluate the proposed
framework on multiple challenging real-world scenes and show --- to the best of
our knowledge --- first results of an instance-independent monocular
object-SLAM system and the benefits it enjoys over feature-based SLAM methods.","['Parv Parkhiya', 'Rishabh Khawad', 'J. Krishna Murthy', 'Brojeshwar Bhowmick', 'K. Madhava Krishna']",2018-02-26T13:42:40Z,http://arxiv.org/abs/1802.09292v1,"['cs.RO', 'cs.CV']"
The TUM VI Benchmark for Evaluating Visual-Inertial Odometry,"Visual odometry and SLAM methods have a large variety of applications in
domains such as augmented reality or robotics. Complementing vision sensors
with inertial measurements tremendously improves tracking accuracy and
robustness, and thus has spawned large interest in the development of
visual-inertial (VI) odometry approaches. In this paper, we propose the TUM VI
benchmark, a novel dataset with a diverse set of sequences in different scenes
for evaluating VI odometry. It provides camera images with 1024x1024 resolution
at 20 Hz, high dynamic range and photometric calibration. An IMU measures
accelerations and angular velocities on 3 axes at 200 Hz, while the cameras and
IMU sensors are time-synchronized in hardware. For trajectory evaluation, we
also provide accurate pose ground truth from a motion capture system at high
frequency (120 Hz) at the start and end of the sequences which we accurately
aligned with the camera and IMU measurements. The full dataset with raw and
calibrated data is publicly available. We also evaluate state-of-the-art VI
odometry approaches on our dataset.","['David Schubert', 'Thore Goll', 'Nikolaus Demmel', 'Vladyslav Usenko', 'Jörg Stückler', 'Daniel Cremers']",2018-04-17T09:11:23Z,http://arxiv.org/abs/1804.06120v3,"['cs.CV', 'cs.RO']"
"3D-PhysNet: Learning the Intuitive Physics of Non-Rigid Object
  Deformations","The ability to interact and understand the environment is a fundamental
prerequisite for a wide range of applications from robotics to augmented
reality. In particular, predicting how deformable objects will react to applied
forces in real time is a significant challenge. This is further confounded by
the fact that shape information about encountered objects in the real world is
often impaired by occlusions, noise and missing regions e.g. a robot
manipulating an object will only be able to observe a partial view of the
entire solid. In this work we present a framework, 3D-PhysNet, which is able to
predict how a three-dimensional solid will deform under an applied force using
intuitive physics modelling. In particular, we propose a new method to encode
the physical properties of the material and the applied force, enabling
generalisation over materials. The key is to combine deep variational
autoencoders with adversarial training, conditioned on the applied force and
the material properties. We further propose a cascaded architecture that takes
a single 2.5D depth view of the object and predicts its deformation. Training
data is provided by a physics simulator. The network is fast enough to be used
in real-time applications from partial views. Experimental results show the
viability and the generalisation properties of the proposed architecture.","['Zhihua Wang', 'Stefano Rosa', 'Bo Yang', 'Sen Wang', 'Niki Trigoni', 'Andrew Markham']",2018-04-25T15:53:03Z,http://arxiv.org/abs/1805.00328v2,['cs.CV']
FMHash: Deep Hashing of In-Air-Handwriting for User Identification,"Many mobile systems and wearable devices, such as Virtual Reality (VR) or
Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID
and password for signing into a virtual website. However, they are usually
equipped with gesture capture interfaces to allow the user to interact with the
system directly with hand gestures. Although gesture-based authentication has
been well-studied, less attention is paid to the gesture-based user
identification problem, which is essentially an input method of account ID and
an efficient searching and indexing method of a database of gesture signals. In
this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification
framework that can generate a compact binary hash code from a piece of
in-air-handwriting of an ID string. This hash code enables indexing and fast
search of a large account database using the in-air-handwriting by a hash
table. To demonstrate the effectiveness of the framework, we implemented a
prototype and achieved >99.5% precision and >92.6% recall with exact hash code
match on a dataset of 200 accounts collected by us. The ability of hashing
in-air-handwriting pattern to binary code can be used to achieve convenient
sign-in and sign-up with in-air-handwriting gesture ID on future mobile and
wearable systems connected to the Internet.","['Duo Lu', 'Dijiang Huang', 'Anshul Rai']",2018-06-10T02:15:29Z,http://arxiv.org/abs/1806.03574v2,"['cs.CV', 'D.4.6; I.5.4']"
Potential of Augmented Reality for Intelligent Transportation Systems,"Rapid advances in wireless communication technologies coupled with ongoing
massive development in vehicular networking standards and innovations in
computing, sensing, and analytics have paved the way for intelligent
transportation systems (ITS) to develop rapidly in the near future. ITS
provides a complete solution for the efficient and intelligent management of
real-time traffic, wherein sensory data is collected from within the vehicles
(i.e., via their onboard units) as well as data exchanged between the vehicles,
between the vehicles and their supporting roadside infrastructure/network,
among the vehicles and vulnerable pedestrians, subsequently paving the way for
the realization of the futuristic Internet of Vehicles. The traditional intent
of an ITS system is to detect, monitor, control, and subsequently reduce
traffic congestion based on a real-time analysis of the data pertinent to
certain patterns of the road traffic, including traffic density at a
geographical area of interest, precise velocity of vehicles, current and
predicted travelling trajectories and times, etc. However, merely relying on an
ITS framework is not an optimal solution. In case of dense traffic
environments, where communication broadcasts from hundreds of thousands of
vehicles could potentially choke the entire network (and so could lead to fatal
accidents in the case of autonomous vehicles that depend on reliable
communications for their operational safety), a fall back to the traditional
decentralized vehicular ad hoc network (VANET) approach becomes necessary. It
is therefore of critical importance to enhance the situational awareness of
vehicular drivers so as to enable them to make quick but well-founded manual
decisions in such safety-critical situations.","['Adnan Mahmood', 'Bernard Butler', 'Brendan Jennings']",2018-06-11T03:52:07Z,http://arxiv.org/abs/1806.04724v1,"['cs.HC', 'cs.CY']"
"Edge Cloud Offloading Algorithms: Issues, Methods, and Perspectives","Mobile devices supporting the ""Internet of Things"" (IoT), often have limited
capabilities in computation, battery energy, and storage space, especially to
support resource-intensive applications involving virtual reality (VR),
augmented reality (AR), multimedia delivery and artificial intelligence (AI),
which could require broad bandwidth, low response latency and large
computational power. Edge cloud or edge computing is an emerging topic and
technology that can tackle the deficiency of the currently centralized-only
cloud computing model and move the computation and storage resource closer to
the devices in support of the above-mentioned applications. To make this
happen, efficient coordination mechanisms and ""offloading"" algorithms are
needed to allow the mobile devices and the edge cloud to work together
smoothly. In this survey paper, we investigate the key issues, methods, and
various state-of-the-art efforts related to the offloading problem. We adopt a
new characterizing model to study the whole process of offloading from mobile
devices to the edge cloud. Through comprehensive discussions, we aim to draw an
overall ""big picture"" on the existing efforts and research directions. Our
study also indicates that the offloading algorithms in edge cloud have
demonstrated profound potentials for future technology and application
development.","['Jianyu Wang', 'Jianli Pan', 'Flavio Esposito', 'Prasad Calyam', 'Zhicheng Yang', 'Prasant Mohapatra']",2018-06-16T05:50:46Z,http://arxiv.org/abs/1806.06191v1,['cs.NI']
"Towards Commodity, Web-Based Augmented Reality Applications for Research
  and Education in Chemistry and Structural Biology","This article reports prototype web apps that use commodity, open-source
technologies for augmented and virtual reality to provide immersive,
interactive human-computer interfaces for chemistry, structural biology and
related disciplines. The examples, which run in any standard web browser and
are accessible at
https://lucianoabriata.altervista.org/jsinscience/arjs/armodeling/ together
with demo videos, showcase applications that could go well beyond pedagogy,
i.e. advancing actual utility in research settings: molecular visualization at
atomistic and coarse-grained levels in interactive immersive 3D, coarse-grained
modeling of molecular physics and chemistry, and on-the-fly calculation of
experimental observables and overlay onto experimental data. From this
playground, I depict perspectives on how these emerging technologies might
couple in the future to neural network-based quantum mechanical calculations,
advanced forms of human-computer interaction such as speech-based
communication, and sockets for concurrent collaboration through the internet
-all technologies that are today maturing in web browsers- to deliver the next
generation of tools for truly interactive, immersive molecular modeling that
can streamline human thought and intent with the numerical processing power of
computers.",['Luciano A. Abriata'],2018-06-21T17:21:17Z,http://arxiv.org/abs/1806.08332v5,"['cs.HC', 'cs.ET', 'cs.MM', 'physics.bio-ph', 'q-bio.BM']"
"A Curriculum Domain Adaptation Approach to the Semantic Segmentation of
  Urban Scenes","During the last half decade, convolutional neural networks (CNNs) have
triumphed over semantic segmentation, which is one of the core tasks in many
applications such as autonomous driving and augmented reality. However, to
train CNNs requires a considerable amount of data, which is difficult to
collect and laborious to annotate. Recent advances in computer graphics make it
possible to train CNNs on photo-realistic synthetic imagery with
computer-generated annotations. Despite this, the domain mismatch between the
real images and the synthetic data hinders the models' performance. Hence, we
propose a curriculum-style learning approach to minimizing the domain gap in
urban scene semantic segmentation. The curriculum domain adaptation solves easy
tasks first to infer necessary properties about the target domain; in
particular, the first task is to learn global label distributions over images
and local distributions over landmark superpixels. These are easy to estimate
because images of urban scenes have strong idiosyncrasies (e.g., the size and
spatial relations of buildings, streets, cars, etc.). We then train a
segmentation network, while regularizing its predictions in the target domain
to follow those inferred properties. In experiments, our method outperforms the
baselines on two datasets and two backbone networks. We also report extensive
ablation studies about our approach.","['Yang Zhang', 'Philip David', 'Hassan Foroosh', 'Boqing Gong']",2018-12-24T16:50:49Z,http://arxiv.org/abs/1812.09953v3,['cs.CV']
"Efficient, Dynamic Multi-tenant Edge Computation in EdgeOS","In the future, computing will be immersed in the world around us -- from
augmented reality to autonomous vehicles to the Internet of Things. Many of
these smart devices will offer services that respond in real time to their
physical surroundings, requiring complex processing with strict performance
guarantees. Edge clouds promise a pervasive computational infrastructure a
short network hop away from end devices, but today's operating systems are a
poor fit to meet the goals of scalable isolation, dense multi-tenancy, and
predictable performance required by these emerging applications. In this paper
we present EdgeOS, a micro-kernel based operating system that meets these goals
by blending recent advances in real-time systems and network function
virtualization. EdgeOS introduces a Featherweight Process model that offers
lightweight isolation and supports extreme scalability even under high churn.
Our architecture provides efficient communication mechanisms, and low-overhead
per-client isolation. To achieve high performance networking, EdgeOS employs
kernel bypass paired with the isolation properties of Featherweight Processes.
We have evaluated our EdgeOS prototype for running high scale network
middleboxes using the Click software router and endpoint applications using
memcached. EdgeOS reduces startup latency by 170X compared to Linux processes
and over five orders of magnitude compared to containers, while providing three
orders of magnitude latency improvement when running 300 to 1000 edge-cloud
memcached instances on one server.","['Yuxin Ren', 'Vlad Nitu', 'Guyue Liu', 'Gabriel Parmer', 'Timothy Wood', 'Alain Tchana', 'Riley Kennedy']",2019-01-04T17:31:56Z,http://arxiv.org/abs/1901.01222v1,['cs.OS']
"Image Disentanglement and Uncooperative Re-Entanglement for
  High-Fidelity Image-to-Image Translation","Cross-domain image-to-image translation should satisfy two requirements: (1)
preserve the information that is common to both domains, and (2) generate
convincing images covering variations that appear in the target domain. This is
challenging, especially when there are no example translations available as
supervision. Adversarial cycle consistency was recently proposed as a solution,
with beautiful and creative results, yielding much follow-up work. However,
augmented reality applications cannot readily use such techniques to provide
users with compelling translations of real scenes, because the translations do
not have high-fidelity constraints. In other words, current models are liable
to change details that should be preserved: while re-texturing a face, they may
alter the face's expression in an unpredictable way. In this paper, we
introduce the problem of high-fidelity image-to-image translation, and present
a method for solving it. Our main insight is that low-fidelity translations
typically escape a cycle-consistency penalty, because the back-translator
learns to compensate for the forward-translator's errors. We therefore
introduce an optimization technique that prevents the networks from
cooperating: simply train each network only when its input data is real. Prior
works, in comparison, train each network with a mix of real and generated data.
Experimental results show that our method accurately disentangles the factors
that separate the domains, and converges to semantics-preserving translations
that prior methods miss.","['Adam W. Harley', 'Shih-En Wei', 'Jason Saragih', 'Katerina Fragkiadaki']",2019-01-11T16:08:21Z,http://arxiv.org/abs/1901.03628v2,['cs.CV']
"Phase-only transmissive spatial light modulator based on tunable
  dielectric metasurface","Rapidly developing augmented reality (AR) and 3D holographic display
technologies require spatial light modulators (SLM) with high resolution and
viewing angle to be able to satisfy increasing customer demands. Currently
available SLMs, as well as their performance, are limited by their large pixel
sizes of the order of several micrometres. Further pixel size miniaturization
has been stagnant due to the persistent challenge to reduce the inter-pixel
crosstalk associated with the liquid crystal (LC) cell thickness, which has to
be large enough to accumulate the required 2{\pi} phase difference. Here, we
propose a concept of tunable dielectric metasurfaces modulated by a liquid
crystal environment, which can provide abrupt phase change and uncouple the
phase accumulation from the LC cell thickness, ultimately enabling the pixel
size miniaturization. We present a proof-of-concept metasurface-based SLM
device, configured to generate active beam steering with >35% efficiency and
large beam deflection angle of 11{\deg}, with LC cell thickness of only 1.5
{\mu}m, much smaller than conventional devices. We achieve the pixel size of
1.14 {\mu}m corresponding to the image resolution of 877 lp/mm, which is 30
times larger comparing to the presently available commercial SLM devices. High
resolution and viewing angle of the metasurface-based SLMs opens up a new path
to the next generation of near-eye AR and 3D holographic display technologies.","['Shi-Qiang Li', 'Xuewu Xu', 'Rasna Maruthiyodan Veetil', 'Vytautas Valuckas', 'Ramón Paniagua-Domínguez', 'Arseniy I. Kuznetsov']",2019-01-23T06:26:38Z,http://arxiv.org/abs/1901.07742v1,"['physics.optics', 'physics.app-ph']"
"Joint Service Placement and Request Routing in Multi-cell Mobile Edge
  Computing Networks","The proliferation of innovative mobile services such as augmented reality,
networked gaming, and autonomous driving has spurred a growing need for
low-latency access to computing resources that cannot be met solely by existing
centralized cloud systems. Mobile Edge Computing (MEC) is expected to be an
effective solution to meet the demand for low-latency services by enabling the
execution of computing tasks at the network-periphery, in proximity to
end-users. While a number of recent studies have addressed the problem of
determining the execution of service tasks and the routing of user requests to
corresponding edge servers, the focus has primarily been on the efficient
utilization of computing resources, neglecting the fact that non-trivial
amounts of data need to be stored to enable service execution, and that many
emerging services exhibit asymmetric bandwidth requirements. To fill this gap,
we study the joint optimization of service placement and request routing in
MEC-enabled multi-cell networks with multidimensional
(storage-computation-communication) constraints. We show that this problem
generalizes several problems in literature and propose an algorithm that
achieves close-to-optimal performance using randomized rounding. Evaluation
results demonstrate that our approach can effectively utilize the available
resources to maximize the number of requests served by low-latency edge cloud
servers.","['Konstantinos Poularakis', 'Jaime Llorca', 'Antonia M. Tulino', 'Ian Taylor', 'Leandros Tassiulas']",2019-01-25T16:04:23Z,http://arxiv.org/abs/1901.08946v1,['cs.NI']
"PanopticFusion: Online Volumetric Semantic Mapping at the Level of Stuff
  and Things","We propose PanopticFusion, a novel online volumetric semantic mapping system
at the level of stuff and things. In contrast to previous semantic mapping
systems, PanopticFusion is able to densely predict class labels of a background
region (stuff) and individually segment arbitrary foreground objects (things).
In addition, our system has the capability to reconstruct a large-scale scene
and extract a labeled mesh thanks to its use of a spatially hashed volumetric
map representation. Our system first predicts pixel-wise panoptic labels (class
labels for stuff regions and instance IDs for thing regions) for incoming RGB
frames by fusing 2D semantic and instance segmentation outputs. The predicted
panoptic labels are integrated into the volumetric map together with depth
measurements while keeping the consistency of the instance IDs, which could
vary frame to frame, by referring to the 3D map at that moment. In addition, we
construct a fully connected conditional random field (CRF) model with respect
to panoptic labels for map regularization. For online CRF inference, we propose
a novel unary potential approximation and a map division strategy.
  We evaluated the performance of our system on the ScanNet (v2) dataset.
PanopticFusion outperformed or compared with state-of-the-art offline 3D DNN
methods in both semantic and instance segmentation benchmarks. Also, we
demonstrate a promising augmented reality application using a 3D panoptic map
generated by the proposed system.","['Gaku Narita', 'Takashi Seno', 'Tomoya Ishikawa', 'Yohsuke Kaji']",2019-03-04T11:27:48Z,http://arxiv.org/abs/1903.01177v2,"['cs.CV', 'cs.RO']"
Augmenting Visual SLAM with Wi-Fi Sensing For Indoor Applications,"Recent trends have accelerated the development of spatial applications on
mobile devices and robots. These include navigation, augmented reality,
human-robot interaction, and others. A key enabling technology for such
applications is the understanding of the device's location and the map of the
surrounding environment. This generic problem, referred to as Simultaneous
Localization and Mapping (SLAM), is an extensively researched topic in
robotics. However, visual SLAM algorithms face several challenges including
perceptual aliasing and high computational cost. These challenges affect the
accuracy, efficiency, and viability of visual SLAM algorithms, especially for
long-term SLAM, and their use in resource-constrained mobile devices.
  A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access
in most urban environments. Most robots and mobile devices are equipped with a
Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal
strength to alleviate the challenges faced by visual SLAM algorithms. To
demonstrate the utility of this idea, this work makes the following
contributions: (i) We propose a generic way to integrate Wi-Fi sensing into
visual SLAM algorithms, (ii) We integrate such sensing into three well-known
SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the
performance of such augmentation in comparison to the original visual
algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm.
Overall, we show that our approach can improve the accuracy of visual SLAM
algorithms by 11% on average and reduce computation time on average by 15% to
25%.","['Zakieh S. Hashemifar', 'Charuvahan Adhivarahan', 'Anand Balakrishnan', 'Karthik Dantu']",2019-03-15T17:35:32Z,http://arxiv.org/abs/1903.06687v1,['cs.RO']
"Local Light Field Fusion: Practical View Synthesis with Prescriptive
  Sampling Guidelines","We present a practical and robust deep learning solution for capturing and
rendering novel views of complex real world scenes for virtual exploration.
Previous approaches either require intractably dense view sampling or provide
little to no guidance for how users should sample views of a scene to reliably
render high-quality novel views. Instead, we propose an algorithm for view
synthesis from an irregular grid of sampled views that first expands each
sampled view into a local light field via a multiplane image (MPI) scene
representation, then renders novel views by blending adjacent local light
fields. We extend traditional plenoptic sampling theory to derive a bound that
specifies precisely how densely users should sample views of a given scene when
using our algorithm. In practice, we apply this bound to capture and render
views of real world scenes that achieve the perceptual quality of Nyquist rate
view sampling while using up to 4000x fewer views. We demonstrate our
approach's practicality with an augmented reality smartphone app that guides
users to capture input images of a scene and viewers that enable realtime
virtual exploration on desktop and mobile platforms.","['Ben Mildenhall', 'Pratul P. Srinivasan', 'Rodrigo Ortiz-Cayon', 'Nima Khademi Kalantari', 'Ravi Ramamoorthi', 'Ren Ng', 'Abhishek Kar']",2019-05-02T17:58:52Z,http://arxiv.org/abs/1905.00889v1,"['cs.CV', 'cs.GR']"
"WoodScape: A multi-task, multi-camera fisheye dataset for autonomous
  driving","Fisheye cameras are commonly employed for obtaining a large field of view in
surveillance, augmented reality and in particular automotive applications. In
spite of their prevalence, there are few public datasets for detailed
evaluation of computer vision algorithms on fisheye images. We release the
first extensive fisheye automotive dataset, WoodScape, named after Robert Wood
who invented the fisheye camera in 1906. WoodScape comprises of four surround
view cameras and nine tasks including segmentation, depth estimation, 3D
bounding box detection and soiling detection. Semantic annotation of 40 classes
at the instance level is provided for over 10,000 images and annotation for
other tasks are provided for over 100,000 images. With WoodScape, we would like
to encourage the community to adapt computer vision models for fisheye camera
instead of using naive rectification.","['Senthil Yogamani', 'Ciaran Hughes', 'Jonathan Horgan', 'Ganesh Sistu', 'Padraig Varley', ""Derek O'Dea"", 'Michal Uricar', 'Stefan Milz', 'Martin Simon', 'Karl Amende', 'Christian Witt', 'Hazem Rashed', 'Sumanth Chennupati', 'Sanjaya Nayak', 'Saquib Mansoor', 'Xavier Perroton', 'Patrick Perez']",2019-05-04T13:14:12Z,http://arxiv.org/abs/1905.01489v3,"['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO', 'stat.ML']"
"FlexNGIA: A Flexible Internet Architecture for the Next-Generation
  Tactile Internet","From virtual reality and telepresence, to augmented reality, holoportation,
and remotely controlled robotics, these future network applications promise an
unprecedented development for society, economics and culture by revolutionizing
the way we live, learn, work and play. In order to deploy such futuristic
applications and to cater to their performance requirements, recent trends
stressed the need for the Tactile Internet, an Internet that, according to the
International Telecommunication Union, combines ultra low latency with
extremely high availability, reliability and security. Unfortunately, today's
Internet falls short when it comes to providing such stringent requirements due
to several fundamental limitations in the design of the current network
architecture and communication protocols. This brings the need to rethink the
network architecture and protocols, and efficiently harness recent
technological advances in terms of virtualization and network softwarization to
design the Tactile Internet of the future.
  In this paper, we start by analyzing the characteristics and requirements of
future networking applications. We then highlight the limitations of the
traditional network architecture and protocols and their inability to cater to
these requirements. Afterward, we put forward a novel network architecture
adapted to the Tactile Internet called FlexNGIA, a Flexible Next-Generation
Internet Architecture. We then describe some use-cases where we discuss the
potential mechanisms and control loops that could be offered by FlexNGIA in
order to ensure the required performance and reliability guarantees for future
applications. Finally, we identify the key research challenges to further
develop FlexNGIA towards a full-fledged architecture for the future Tactile
Internet.","['Mohamed Faten Zhani', 'Hesham ElBakoury']",2019-05-17T07:24:51Z,http://arxiv.org/abs/1905.07137v2,"['cs.NI', 'cs.ET']"
"SharpNet: Fast and Accurate Recovery of Occluding Contours in Monocular
  Depth Estimation","We introduce SharpNet, a method that predicts an accurate depth map for an
input color image, with a particular attention to the reconstruction of
occluding contours: Occluding contours are an important cue for object
recognition, and for realistic integration of virtual objects in Augmented
Reality, but they are also notoriously difficult to reconstruct accurately. For
example, they are a challenge for stereo-based reconstruction methods, as
points around an occluding contour are visible in only one image. Inspired by
recent methods that introduce normal estimation to improve depth prediction, we
introduce a novel term that constrains depth and occluding contours
predictions. Since ground truth depth is difficult to obtain with pixel-perfect
accuracy along occluding contours, we use synthetic images for training,
followed by fine-tuning on real data. We demonstrate our approach on the
challenging NYUv2-Depth dataset, and show that our method outperforms the
state-of-the-art along occluding contours, while performing on par with the
best recent methods for the rest of the images. Its accuracy along the
occluding contours is actually better than the `ground truth' acquired by a
depth camera based on structured light. We show this by introducing a new
benchmark based on NYUv2-Depth for evaluating occluding contours in monocular
reconstruction, which is our second contribution.","['Michaël Ramamonjisoa', 'Vincent Lepetit']",2019-05-21T13:08:52Z,http://arxiv.org/abs/1905.08598v3,['cs.CV']
TopoTag: A Robust and Scalable Topological Fiducial Marker System,"Fiducial markers have been playing an important role in augmented reality
(AR), robot navigation, and general applications where the relative pose
between a camera and an object is required. Here we introduce TopoTag, a robust
and scalable topological fiducial marker system, which supports reliable and
accurate pose estimation from a single image. TopoTag uses topological and
geometrical information in marker detection to achieve higher robustness.
Topological information is extensively used for 2D marker detection, and
further corresponding geometrical information for ID decoding. Robust 3D pose
estimation is achieved by taking advantage of all TopoTag vertices. Without
sacrificing bits for higher recall and precision like previous systems, TopoTag
can use full bits for ID encoding. TopoTag supports tens of thousands unique
IDs and easily extends to millions of unique tags resulting in massive
scalability. We collected a large test dataset including in total 169,713
images for evaluation, involving in-plane and out-of-plane rotation, image
blur, different distances and various backgrounds, etc. Experiments on the
dataset and real indoor and outdoor scene tests with a rolling shutter camera
both show that TopoTag significantly outperforms previous fiducial marker
systems in terms of various metrics, including detection accuracy, vertex
jitter, pose jitter and accuracy, etc. In addition, TopoTag supports occlusion
as long as the main tag topological structure is maintained and allows for
flexible shape design where users can customize internal and external marker
shapes. Code for our marker design/generation, marker detection, and dataset
are available at
http://herohuyongtao.github.io/research/publications/topo-tag/.","['Guoxing Yu', 'Yongtao Hu', 'Jingwen Dai']",2019-08-05T02:57:50Z,http://arxiv.org/abs/1908.01450v3,"['cs.CV', 'cs.RO']"
Origin-Destination Flow Maps in Immersive Environments,"Immersive virtual- and augmented-reality headsets can overlay a flat image
against any surface or hang virtual objects in the space around the user. The
technology is rapidly improving and may, in the long term, replace traditional
flat panel displays in many situations. When displays are no longer
intrinsically flat, how should we use the space around the user for abstract
data visualisation? In this paper, we ask this question with respect to
origin-destination flow data in a global geographic context. We report on the
findings of three studies exploring different spatial encodings for flow maps.
The first experiment focuses on different 2D and 3D encodings for flows on flat
maps. We find that participants are significantly more accurate with raised
flow paths whose height is proportional to flow distance but fastest with
traditional straight line 2D flows. In our second and third experiment, we
compared flat maps, 3D globes and a novel interactive design we call MapsLink,
involving a pair of linked flat maps. We find that participants took
significantly more time with MapsLink than other flow maps while the 3D globe
with raised flows was the fastest, most accurate, and most preferred method.
Our work suggests that careful use of the third spatial dimension can resolve
visual clutter in complex flow maps.","['Yalong Yang', 'Tim Dwyer', 'Bernhard Jenny', 'Kim Marriott', 'Maxime Cordeil', 'Haohui Chen']",2019-08-06T11:57:41Z,http://arxiv.org/abs/1908.02089v1,"['cs.HC', 'cs.GR', 'cs.MM']"
Editing Text in the Wild,"In this paper, we are interested in editing text in natural images, which
aims to replace or modify a word in the source image with another one while
maintaining its realistic look. This task is challenging, as the styles of both
background and text need to be preserved so that the edited image is visually
indistinguishable from the source image. Specifically, we propose an end-to-end
trainable style retention network (SRNet) that consists of three modules: text
conversion module, background inpainting module and fusion module. The text
conversion module changes the text content of the source image into the target
text while keeping the original text style. The background inpainting module
erases the original text, and fills the text region with appropriate texture.
The fusion module combines the information from the two former modules, and
generates the edited text images. To our knowledge, this work is the first
attempt to edit text in natural images at the word level. Both visual effects
and quantitative results on synthetic and real-world dataset (ICDAR 2013) fully
confirm the importance and necessity of modular decomposition. We also conduct
extensive experiments to validate the usefulness of our method in various
real-world applications such as text image synthesis, augmented reality (AR)
translation, information hiding, etc.","['Liang Wu', 'Chengquan Zhang', 'Jiaming Liu', 'Junyu Han', 'Jingtuo Liu', 'Errui Ding', 'Xiang Bai']",2019-08-08T12:59:18Z,http://arxiv.org/abs/1908.03047v1,['cs.CV']
"Efficient 3D Reconstruction and Streaming for Group-Scale Multi-Client
  Live Telepresence","Sharing live telepresence experiences for teleconferencing or remote
collaboration receives increasing interest with the recent progress in
capturing and AR/VR technology. Whereas impressive telepresence systems have
been proposed on top of on-the-fly scene capture, data transmission and
visualization, these systems are restricted to the immersion of single or up to
a low number of users into the respective scenarios. In this paper, we direct
our attention on immersing significantly larger groups of people into
live-captured scenes as required in education, entertainment or collaboration
scenarios. For this purpose, rather than abandoning previous approaches, we
present a range of optimizations of the involved reconstruction and streaming
components that allow the immersion of a group of more than 24 users within the
same scene - which is about a factor of 6 higher than in previous work -
without introducing further latency or changing the involved consumer hardware
setup. We demonstrate that our optimized system is capable of generating
high-quality scene reconstructions as well as providing an immersive viewing
experience to a large group of people within these live-captured scenes.","['Patrick Stotko', 'Stefan Krumpen', 'Michael Weinmann', 'Reinhard Klein']",2019-08-08T15:27:10Z,http://arxiv.org/abs/1908.03118v2,"['cs.HC', 'cs.GR']"
HumanMeshNet: Polygonal Mesh Recovery of Humans,"3D Human Body Reconstruction from a monocular image is an important problem
in computer vision with applications in virtual and augmented reality
platforms, animation industry, en-commerce domain, etc. While several of the
existing works formulate it as a volumetric or parametric learning with complex
and indirect reliance on re-projections of the mesh, we would like to focus on
implicitly learning the mesh representation. To that end, we propose a novel
model, HumanMeshNet, that regresses a template mesh's vertices, as well as
receives a regularization by the 3D skeletal locations in a multi-branch,
multi-task setup. The image to mesh vertex regression is further regularized by
the neighborhood constraint imposed by mesh topology ensuring smooth surface
reconstruction. The proposed paradigm can theoretically learn local surface
deformations induced by body shape variations and can therefore learn
high-resolution meshes going ahead. We show comparable performance with SoA (in
terms of surface and joint error) with far lesser computational complexity,
modeling cost and therefore real-time reconstructions on three publicly
available datasets. We also show the generalizability of the proposed paradigm
for a similar task of predicting hand mesh models. Given these initial results,
we would like to exploit the mesh topology in an explicit manner going ahead.","['Abbhinav Venkat', 'Chaitanya Patel', 'Yudhik Agrawal', 'Avinash Sharma']",2019-08-19T00:27:24Z,http://arxiv.org/abs/1908.06544v1,['cs.CV']
Efficient Deep Neural Networks,"The success of deep neural networks (DNNs) is attributable to three factors:
increased compute capacity, more complex models, and more data. These factors,
however, are not always present, especially for edge applications such as
autonomous driving, augmented reality, and internet-of-things. Training DNNs
requires a large amount of data, which is difficult to obtain. Edge devices
such as mobile phones have limited compute capacity, and therefore, require
specialized and efficient DNNs. However, due to the enormous design space and
prohibitive training costs, designing efficient DNNs for different target
devices is challenging. So the question is, with limited data, compute
capacity, and model complexity, can we still successfully apply deep neural
networks?
  This dissertation focuses on the above problems and improving the efficiency
of deep neural networks at four levels. Model efficiency: we designed neural
networks for various computer vision tasks and achieved more than 10x faster
speed and lower energy. Data efficiency: we developed an advanced tool that
enables 6.2x faster annotation of a LiDAR point cloud. We also leveraged domain
adaptation to utilize simulated data, bypassing the need for real data.
Hardware efficiency: we co-designed neural networks and hardware accelerators
and achieved 11.6x faster inference. Design efficiency: the process of finding
the optimal neural networks is time-consuming. Our automated neural
architecture search algorithms discovered, using 421x lower computational cost
than previous search methods, models with state-of-the-art accuracy and
efficiency.",['Bichen Wu'],2019-08-20T23:26:04Z,http://arxiv.org/abs/1908.08926v1,['cs.CV']
"Color Blending in Outdoor Optical See-through AR: The Effect of
  Real-world Backgrounds on User Interface Color","It has been noted anecdotally and through a small number of formal studies
that ambient lighting conditions and dynamic real-world backgrounds affect the
usability of optical see-through augmented reality (AR) displays; especially so
in outdoor environments. Our previous work examined these effects using painted
posters as representative real-world backgrounds. In this paper, we present a
study that employs an experimental testbed that allows AR graphics to be
overlaid onto real-world backgrounds as well as painted posters. Our results
indicate that color blending effects of physical materials as backgrounds are
nearly the same as their corresponding poster backgrounds, even though the
colors of each pair are only a metameric match. More importantly, our results
suggest that given the current capabilities of optical see-through head-mounted
displays (oHMDs), the implications are, at a minimum, a reduced color gamut
available to user interface (UI) designers. In worse cases, there are unknown
or unexpected color interactions that no UI or system designers can plan for;
significantly crippling the usability of the UI or altering the semantic
interpretation of graphical elements. Further, our results support the concept
of an adaptive AR system which can dynamically alter the color of UI elements
based on predicted background color interactions. These interactions can be
studied and predicted through methods such as those presented in this work.","['Joseph L. Gabbard', 'J. Edward Swan II', 'Adam Zarger']",2019-08-25T15:56:28Z,http://arxiv.org/abs/1908.09348v1,['cs.HC']
A True AR Authoring Tool for Interactive Virtual Museums,"In this work, a new and innovative way of spatial computing that appeared
recently in the bibliography called True Augmented Reality (AR), is employed in
cultural heritage preservation. This innovation could be adapted by the Virtual
Museums of the future to enhance the quality of experience. It emphasises, the
fact that a visitor will not be able to tell, at a first glance, if the
artefact that he/she is looking at is real or not and it is expected to draw
the visitors' interest. True AR is not limited to artefacts but extends even to
buildings or life-sized character simulations of statues. It provides the best
visual quality possible so that the users will not be able to tell the real
objects from the augmented ones. Such applications can be beneficial for future
museums, as with True AR, 3D models of various exhibits, monuments, statues,
characters and buildings can be reconstructed and presented to the visitors in
a realistic and innovative way. We also propose our Virtual Reality Sample
application, a True AR playground featuring basic components and tools for
generating interactive Virtual Museum applications, alongside a 3D
reconstructed character (the priest of Asinou church) facilitating the
storyteller of the augmented experience.","['Efstratios Geronikolakis', 'Paul Zikas', 'Steve Kateros', 'Nick Lydatakis', 'Stelios Georgiou', 'Mike Kentros', 'George Papagiannakis']",2019-09-20T11:10:23Z,http://arxiv.org/abs/1909.09429v4,"['cs.GR', 'cs.HC', '68U05', 'I.3.8; I.3.7']"
"Large Scale Joint Semantic Re-Localisation and Scene Understanding via
  Globally Unique Instance Coordinate Regression","In this work we present a novel approach to joint semantic localisation and
scene understanding. Our work is motivated by the need for localisation
algorithms which not only predict 6-DoF camera pose but also simultaneously
recognise surrounding objects and estimate 3D geometry. Such capabilities are
crucial for computer vision guided systems which interact with the environment:
autonomous driving, augmented reality and robotics. In particular, we propose a
two step procedure. During the first step we train a convolutional neural
network to jointly predict per-pixel globally unique instance labels and
corresponding local coordinates for each instance of a static object (e.g. a
building). During the second step we obtain scene coordinates by combining
object center coordinates and local coordinates and use them to perform 6-DoF
camera pose estimation. We evaluate our approach on real world (CamVid-360) and
artificial (SceneCity) autonomous driving datasets. We obtain smaller mean
distance and angular errors than state-of-the-art 6-DoF pose estimation
algorithms based on direct pose regression and pose estimation from scene
coordinates on all datasets. Our contributions include: (i) a novel formulation
of scene coordinate regression as two separate tasks of object instance
recognition and local coordinate regression and a demonstration that our
proposed solution allows to predict accurate 3D geometry of static objects and
estimate 6-DoF pose of camera on (ii) maps larger by several orders of
magnitude than previously attempted by scene coordinate regression methods, as
well as on (iii) lightweight, approximate 3D maps built from 3D primitives such
as building-aligned cuboids.","['Ignas Budvytis', 'Marvin Teichmann', 'Tomas Vojir', 'Roberto Cipolla']",2019-09-23T09:26:27Z,http://arxiv.org/abs/1909.10239v1,['cs.CV']
Real-Time Semantic Stereo Matching,"Scene understanding is paramount in robotics, self-navigation, augmented
reality, and many other fields. To fully accomplish this task, an autonomous
agent has to infer the 3D structure of the sensed scene (to know where it looks
at) and its content (to know what it sees). To tackle the two tasks, deep
neural networks trained to infer semantic segmentation and depth from stereo
images are often the preferred choices. Specifically, Semantic Stereo Matching
can be tackled by either standalone models trained for the two tasks
independently or joint end-to-end architectures. Nonetheless, as proposed so
far, both solutions are inefficient because requiring two forward passes in the
former case or due to the complexity of a single network in the latter,
although jointly tackling both tasks is usually beneficial in terms of
accuracy. In this paper, we propose a single compact and lightweight
architecture for real-time semantic stereo matching. Our framework relies on
coarse-to-fine estimations in a multi-stage fashion, allowing: i) very fast
inference even on embedded devices, with marginal drops in accuracy, compared
to state-of-the-art networks, ii) trade accuracy for speed, according to the
specific application requirements. Experimental results on high-end GPUs as
well as on an embedded Jetson TX2 confirm the superiority of semantic stereo
matching compared to standalone tasks and highlight the versatility of our
framework on any hardware and for any application.","['Pier Luigi Dovesi', 'Matteo Poggi', 'Lorenzo Andraghetti', 'Miquel Martí', 'Hedvig Kjellström', 'Alessandro Pieropan', 'Stefano Mattoccia']",2019-10-01T16:52:29Z,http://arxiv.org/abs/1910.00541v2,"['cs.CV', 'cs.RO']"
"Adaptive Generation of Phantom Limbs Using Visible Hierarchical
  Autoencoders","This paper proposed a hierarchical visible autoencoder in the adaptive
phantom limbs generation according to the kinetic behavior of functional
body-parts, which are measured by heterogeneous kinetic sensors. The proposed
visible hierarchical autoencoder consists of interpretable and multi-correlated
autoencoder pipelines, which is directly derived from the hierarchical network
described in forest data-structure. According to specified kinetic script
(e.g., dancing, running, etc.) and users' physical conditions, hierarchical
network is extracted from human musculoskeletal network, which is fabricated by
multiple body components (e.g., muscle, bone, and joints, etc.) that are
bio-mechanically, functionally, or nervously correlated with each other and
exhibit mostly non-divergent kinetic behaviors. Multi-layer perceptron (MLP)
regressor models, as well as several variations of autoencoder models, are
investigated for the sequential generation of missing or dysfunctional limbs.
The resulting kinematic behavior of phantom limbs will be constructed using
virtual reality and augmented reality (VR/AR), actuators, and potentially
controller for a prosthesis (an artificial device that replaces a missing body
part). The addressed work aims to develop practical innovative exercise methods
that (1) engage individuals at all ages, including those with a chronic health
condition(s) and/or disability, in regular physical activities, (2) accelerate
the rehabilitation of patients, and (3) release users' phantom limb pain. The
physiological and psychological impact of the addressed work will critically be
assessed in future work.","['Dakila Ledesma', 'Yu Liang', 'Dalei Wu']",2019-10-02T19:54:19Z,http://arxiv.org/abs/1910.01191v1,"['cs.HC', 'cs.LG']"
"QoS Provisioning in 60 GHz Communications by Physical and Transport
  Layer Coordination","In the last decades, technological developments in wireless communications
have been coupled with an increasing demand of mobile services. From real-time
applications with focus on entertainment (e.g., high quality video streaming,
virtual and augmented reality), to industrial automation and security scenarios
(e.g., video surveillance), the requirements are constantly pushing the limits
of communication hardware and software. Communications at millimeter wave
frequencies could provide very high throughput and low latency, thanks to the
large chunks of available bandwidth, but operating at such high frequencies
introduces new challenges in terms of channel reliability, which eventually
impact the overall end-to-end performance. In this paper, we introduce a proxy
that coordinates the physical and transport layers to seamlessly adapt to the
variable channel conditions and avoid performance degradation (i.e., latency
spikes or low throughput). We study the performance of the proposed solution
using a simulated IEEE 802.11ad-compliant network, with the integration of
input traces generated from measurements from real devices, and show that the
proposed proxy-based mechanism reduces the latency by up to 50% with respect to
TCP CUBIC on a 60 GHz link.","['Matteo Drago', 'Michele Polese', 'Stepan Kucera', 'Dmitry Kozlov', 'Vitalii Kirillov', 'Michele Zorzi']",2019-10-09T06:48:38Z,http://arxiv.org/abs/1910.03811v1,['cs.NI']
"Light Field Synthesis by Training Deep Network in the Refocused Image
  Domain","Light field imaging, which captures spatio-angular information of incident
light on image sensor, enables many interesting applications like image
refocusing and augmented reality. However, due to the limited sensor
resolution, a trade-off exists between the spatial and angular resolution. To
increase the angular resolution, view synthesis techniques have been adopted to
generate new views from existing views. However, traditional learning-based
view synthesis mainly considers the image quality of each view of the light
field and neglects the quality of the refocused images. In this paper, we
propose a new loss function called refocused image error (RIE) to address the
issue. The main idea is that the image quality of the synthesized light field
should be optimized in the refocused image domain because it is where the light
field is perceived. We analyze the behavior of RIL in the spectral domain and
test the performance of our approach against previous approaches on both real
and software-rendered light field datasets using objective assessment metrics
such as MSE, MAE, PSNR, SSIM, and GMSD. Experimental results show that the
light field generated by our method results in better refocused images than
previous methods.","['Chang-Le Liu', 'Kuang-Tsu Shih', 'Jiun-Woei Huang', 'Homer H. Chen']",2019-10-14T12:13:37Z,http://arxiv.org/abs/1910.06072v5,"['eess.IV', 'cs.CV']"
Depth Camera Based Particle Filter for Robotic Osteotomy Navigation,"Active surgical robots lack acceptance in clinical practice, because they do
not offer the flexibility and usability required for a versatile usage: the
systems require a large installation space or a complicated registration step,
where the preoperative plan is aligned to the patient and transformed to the
base frame of the robot. In this paper, a navigation system for robotic
osteotomies is designed, which uses the raw depth images from a camera mounted
on the flange of a lightweight robot arm. Consequently, the system does not
require any rigid attachment of the robot or fiducials to the bone and the
time-consuming registration step is eliminated. Instead, only a coarse
initialization is required which improves the usability in surgery. The full
six dimensional pose of the iliac crest bone is estimated with a particle
filter at a maximum rate of 90 Hz. The presented method is robust against
changing lighting conditions, blood or tissue on the bone surface and partial
occlusions caused by the surgeons. Proof of the usability in a clinical
environment is successfully provided in a corpse study, where surgeons used an
augmented reality osteotomy template, which was aligned to bone via the
particle filters pose estimates for the resection of transplants from the iliac
crest.","['Tim Übelhör', 'Jonas Gesenhues', 'Nassim Ayoub', 'Ali Modabber', 'Dirk Abel']",2019-10-24T13:53:21Z,http://arxiv.org/abs/1910.11116v1,"['cs.RO', 'cs.CV']"
A Self Validation Network for Object-Level Human Attention Estimation,"Due to the foveated nature of the human vision system, people can focus their
visual attention on a small region of their visual field at a time, which
usually contains only a single object. Estimating this object of attention in
first-person (egocentric) videos is useful for many human-centered real-world
applications such as augmented reality applications and driver assistance
systems. A straightforward solution for this problem is to pick the object
whose bounding box is hit by the gaze, where eye gaze point estimation is
obtained from a traditional eye gaze estimator and object candidates are
generated from an off-the-shelf object detector. However, such an approach can
fail because it addresses the where and the what problems separately, despite
that they are highly related, chicken-and-egg problems. In this paper, we
propose a novel unified model that incorporates both spatial and temporal
evidence in identifying as well as locating the attended object in firstperson
videos. It introduces a novel Self Validation Module that enforces and
leverages consistency of the where and the what concepts. We evaluate on two
public datasets, demonstrating that Self Validation Module significantly
benefits both training and testing and that our model outperforms the
state-of-the-art.","['Zehua Zhang', 'Chen Yu', 'David Crandall']",2019-10-31T04:56:43Z,http://arxiv.org/abs/1910.14260v2,['cs.CV']
"EGO-CH: Dataset and Fundamental Tasks for Visitors
  BehavioralUnderstanding using Egocentric Vision","Equipping visitors of a cultural site with a wearable device allows to easily
collect information about their preferences which can be exploited to improve
the fruition of cultural goods with augmented reality. Moreover, egocentric
video can be processed using computer vision and machine learning to enable an
automated analysis of visitors' behavior. The inferred information can be used
both online to assist the visitor and offline to support the manager of the
site. Despite the positive impact such technologies can have in cultural
heritage, the topic is currently understudied due to the limited number of
public datasets suitable to study the considered problems. To address this
issue, in this paper we propose EGOcentric-Cultural Heritage (EGO-CH), the
first dataset of egocentric videos for visitors' behavior understanding in
cultural sites. The dataset has been collected in two cultural sites and
includes more than $27$ hours of video acquired by $70$ subjects, with labels
for $26$ environments and over $200$ different Points of Interest. A large
subset of the dataset, consisting of $60$ videos, is associated with surveys
filled out by real visitors. To encourage research on the topic, we propose $4$
challenging tasks (room-based localization, point of interest/object
recognition, object retrieval and survey prediction) useful to understand
visitors' behavior and report baseline results on the dataset.","['Francesco Ragusa', 'Antonino Furnari', 'Sebastiano Battiato', 'Giovanni Signorello', 'Giovanni Maria Farinella']",2020-02-03T17:25:23Z,http://arxiv.org/abs/2002.00899v1,['cs.CV']
"L6DNet: Light 6 DoF Network for Robust and Precise Object Pose
  Estimation with Small Datasets","Estimating the 3D pose of an object is a challenging task that can be
considered within augmented reality or robotic applications. In this paper, we
propose a novel approach to perform 6 DoF object pose estimation from a single
RGB-D image. We adopt a hybrid pipeline in two stages: data-driven and
geometric respectively. The data-driven step consists of a classification CNN
to estimate the object 2D location in the image from local patches, followed by
a regression CNN trained to predict the 3D location of a set of keypoints in
the camera coordinate system. To extract the pose information, the geometric
step consists in aligning the 3D points in the camera coordinate system with
the corresponding 3D points in world coordinate system by minimizing a
registration error, thus computing the pose. Our experiments on the standard
dataset LineMod show that our approach is more robust and accurate than
state-of-the-art methods. The approach is also validated to achieve a 6 DoF
positioning task by visual servoing.","['Mathieu Gonzalez', 'Amine Kacete', 'Albert Murienne', 'Eric Marchand']",2020-02-03T17:41:29Z,http://arxiv.org/abs/2002.00911v6,['cs.CV']
"Adaptive Task Partitioning at Local Device or Remote Edge Server for
  Offloading in MEC","Mobile edge computing (MEC) is one of the promising solutions to process
computational-intensive tasks for the emerging time-critical Internet-of-Things
(IoT) use cases, e.g., virtual reality (VR), augmented reality (AR), autonomous
vehicle. The latency can be reduced further, when a task is partitioned and
computed by multiple edge servers' (ESs) collaboration. However, the
state-of-the-art work studies the MEC-enabled offloading based on a static
framework, which partitions tasks at either the local user equipment (UE) or
the primary ES. The dynamic selection between the two offloading schemes has
not been well studied yet. In this paper, we investigate a dynamic offloading
framework in a multi-user scenario. Each UE can decide who partitions a task
according to the network status, e.g., channel quality and allocated
computation resource. Based on the framework, we model the latency to complete
a task, and formulate an optimization problem to minimize the average latency
among UEs. The problem is solved by jointly optimizing task partitioning and
the allocation of the communication and computation resources. The numerical
results show that, compared with the static offloading schemes, the proposed
algorithm achieves the lower latency in all tested scenarios. Moreover, both
mathematical derivation and simulation illustrate that the wireless channel
quality difference between a UE and different ESs can be used as an important
criterion to determine the right scheme.","['Jianhui Liu', 'Qi Zhang']",2020-02-12T09:13:07Z,http://arxiv.org/abs/2002.04858v1,"['cs.NI', 'eess.SP']"
"Workload Scheduling on heterogeneous Mobile Edge Cloud in 5G networks to
  Minimize SLA Violation","Smart devices have become an indispensable part of our lives and gain
increasing applicability in almost every area. Latency-aware applications such
as Augmented Reality (AR), autonomous driving, and online gaming demand more
resources such as network bandwidth and computational capabilities. Since the
traditional mobile networks cannot fulfill the required bandwidth and latency,
Mobile Edge Cloud (MEC) emerged to provide cloud computing capabilities in the
proximity of users on 5G networks. In this paper, we consider a heterogeneous
MEC network with numerous mobile users that send their tasks to MEC servers.
Each task has a maximum acceptable response time. Non-uniform distribution of
users makes some MEC servers hotspots that cannot take more. A solution is to
relocate the tasks among MEC servers, called Workload Migration. We formulate
this problem of task scheduling as a mixed-integer non-linear optimization
problem to minimize the number of Service Level Agreement (SLA) violations.
Since solving this optimization problem has high computational complexity, we
introduce a greedy algorithm called MESA, Migration Enabled Scheduling
Algorithm, which reaches a near-optimal solution quickly. Our experiments show
that in the term of SLA violation, MESA is only 8% and 11% far from the optimal
choice on the average and the worst-case, respectively. Moreover, the migration
enabled solution can reduce SLA violations by about 30% compare to assigning
tasks to MEC servers without migration.","['Mostafa Hadadian Nejad Yousefi', 'Amirmasoud Ghiassi', 'Boshra Sadat Hashemi', 'Maziar Goudarzi']",2020-03-05T18:43:24Z,http://arxiv.org/abs/2003.02820v2,"['cs.DC', 'cs.NI']"
"DeProCams: Simultaneous Relighting, Compensation and Shape
  Reconstruction for Projector-Camera Systems","Image-based relighting, projector compensation and depth/normal
reconstruction are three important tasks of projector-camera systems (ProCams)
and spatial augmented reality (SAR). Although they share a similar pipeline of
finding projector-camera image mappings, in tradition, they are addressed
independently, sometimes with different prerequisites, devices and sampling
images. In practice, this may be cumbersome for SAR applications to address
them one-by-one. In this paper, we propose a novel end-to-end trainable model
named DeProCams to explicitly learn the photometric and geometric mappings of
ProCams, and once trained, DeProCams can be applied simultaneously to the three
tasks. DeProCams explicitly decomposes the projector-camera image mappings into
three subprocesses: shading attributes estimation, rough direct light
estimation and photorealistic neural rendering. A particular challenge
addressed by DeProCams is occlusion, for which we exploit epipolar constraint
and propose a novel differentiable projector direct light mask. Thus, it can be
learned end-to-end along with the other modules. Afterwards, to improve
convergence, we apply photometric and geometric constraints such that the
intermediate results are plausible. In our experiments, DeProCams shows clear
advantages over previous arts with promising quality and meanwhile being fully
differentiable. Moreover, by solving the three tasks in a unified model,
DeProCams waives the need for additional optical devices, radiometric
calibrations and structured light.","['Bingyao Huang', 'Haibin Ling']",2020-03-06T05:49:16Z,http://arxiv.org/abs/2003.03040v2,"['cs.CV', 'cs.GR']"
"Modeling Cross-view Interaction Consistency for Paired Egocentric
  Interaction Recognition","With the development of Augmented Reality (AR), egocentric action recognition
(EAR) plays important role in accurately understanding demands from the user.
However, EAR is designed to help recognize human-machine interaction in single
egocentric view, thus difficult to capture interactions between two
face-to-face AR users. Paired egocentric interaction recognition (PEIR) is the
task to collaboratively recognize the interactions between two persons with the
videos in their corresponding views. Unfortunately, existing PEIR methods
always directly use linear decision function to fuse the features extracted
from two corresponding egocentric videos, which ignore consistency of
interaction in paired egocentric videos. The consistency of interactions in
paired videos, and features extracted from them are correlated to each other.
On top of that, we propose to build the relevance between two views using
biliear pooling, which capture the consistency of two views in feature-level.
Specifically, each neuron in the feature maps from one view connects to the
neurons from another view, which guarantee the compact consistency between two
views. Then all possible paired neurons are used for PEIR for the inside
consistent information of them. To be efficient, we use compact bilinear
pooling with Count Sketch to avoid directly computing outer product in
bilinear. Experimental results on dataset PEV shows the superiority of the
proposed methods on the task PEIR.","['Zhongguo Li', 'Fan Lyu', 'Wei Feng', 'Song Wang']",2020-03-24T05:05:34Z,http://arxiv.org/abs/2003.10663v1,['cs.CV']
Squeezed Deep 6DoF Object Detection Using Knowledge Distillation,"The detection of objects considering a 6DoF pose is a common requirement to
build virtual and augmented reality applications. It is usually a complex task
which requires real-time processing and high precision results for adequate
user experience. Recently, different deep learning techniques have been
proposed to detect objects in 6DoF in RGB images. However, they rely on high
complexity networks, requiring a computational power that prevents them from
working on mobile devices. In this paper, we propose an approach to reduce the
complexity of 6DoF detection networks while maintaining accuracy. We used
Knowledge Distillation to teach portables Convolutional Neural Networks (CNN)
to learn from a real-time 6DoF detection CNN. The proposed method allows
real-time applications using only RGB images while decreasing the hardware
requirements. We used the LINEMOD dataset to evaluate the proposed method, and
the experimental results show that the proposed method reduces the memory
requirement by almost 99\% in comparison to the original architecture with the
cost of reducing half the accuracy in one of the metrics. Code is available at
https://github.com/heitorcfelix/singleshot6Dpose.","['Heitor Felix', 'Walber M. Rodrigues', 'David Macêdo', 'Francisco Simões', 'Adriano L. I. Oliveira', 'Veronica Teichrieb', 'Cleber Zanchettin']",2020-03-30T16:03:03Z,http://arxiv.org/abs/2003.13586v3,"['cs.CV', 'cs.LG', 'eess.IV', '68-04']"
"Event Based, Near Eye Gaze Tracking Beyond 10,000Hz","The cameras in modern gaze-tracking systems suffer from fundamental bandwidth
and power limitations, constraining data acquisition speed to 300 Hz
realistically. This obstructs the use of mobile eye trackers to perform, e.g.,
low latency predictive rendering, or to study quick and subtle eye motions like
microsaccades using head-mounted devices in the wild. Here, we propose a hybrid
frame-event-based near-eye gaze tracking system offering update rates beyond
10,000 Hz with an accuracy that matches that of high-end desktop-mounted
commercial trackers when evaluated in the same conditions. Our system builds on
emerging event cameras that simultaneously acquire regularly sampled frames and
adaptively sampled events. We develop an online 2D pupil fitting method that
updates a parametric model every one or few events. Moreover, we propose a
polynomial regressor for estimating the point of gaze from the parametric pupil
model in real time. Using the first event-based gaze dataset, available at
https://github.com/aangelopoulos/event_based_gaze_tracking , we demonstrate
that our system achieves accuracies of 0.45 degrees--1.75 degrees for fields of
view from 45 degrees to 98 degrees. With this technology, we hope to enable a
new generation of ultra-low-latency gaze-contingent rendering and display
techniques for virtual and augmented reality.","['Anastasios N. Angelopoulos', 'Julien N. P. Martel', 'Amit P. S. Kohli', 'Jorg Conradt', 'Gordon Wetzstein']",2020-04-07T17:57:18Z,http://arxiv.org/abs/2004.03577v3,"['cs.CV', 'cs.HC']"
6G Communication: Envisioning the Key Issues and Challenges,"In 2030, we are going to evidence the 6G mobile communication technology,
which will enable the Internet of Everything. Yet 5G has to be experienced by
people worldwide and B5G has to be developed; the researchers have already
started planning, visioning, and gathering requirements of the 6G. Moreover,
many countries have already initiated the research on 6G. 6G promises
connecting every smart device to the Internet from smartphone to intelligent
vehicles. 6G will provide sophisticated and high QoS such as holographic
communication, augmented reality/virtual reality and many more. Also, it will
focus on Quality of Experience (QoE) to provide rich experiences from 6G
technology. Notably, it is very important to vision the issues and challenges
of 6G technology, otherwise, promises may not be delivered on time. The
requirements of 6G poses new challenges to the research community. To achieve
desired parameters of 6G, researchers are exploring various alternatives.
Hence, there are diverse research challenges to envision, from devices to
softwarization. Therefore, in this article, we discuss the future issues and
challenges to be faced by the 6G technology. We have discussed issues and
challenges from every aspect from hardware to the enabling technologies which
will be utilized by 6G.","['Sabuzima Nayak', 'Ripon Patgiri']",2020-04-07T13:49:20Z,http://arxiv.org/abs/2004.04024v3,"['eess.SP', 'cs.NI', '68-02, 68M10', 'C.2; I.2']"
"TOG: Targeted Adversarial Objectness Gradient Attacks on Real-time
  Object Detection Systems","The rapid growth of real-time huge data capturing has pushed the deep
learning and data analytic computing to the edge systems. Real-time object
recognition on the edge is one of the representative deep neural network (DNN)
powered edge systems for real-world mission-critical applications, such as
autonomous driving and augmented reality. While DNN powered object detection
edge systems celebrate many life-enriching opportunities, they also open doors
for misuse and abuse. This paper presents three Targeted adversarial Objectness
Gradient attacks, coined as TOG, which can cause the state-of-the-art deep
object detection networks to suffer from object-vanishing, object-fabrication,
and object-mislabeling attacks. We also present a universal objectness gradient
attack to use adversarial transferability for black-box attacks, which is
effective on any inputs with negligible attack time cost, low human
perceptibility, and particularly detrimental to object detection edge systems.
We report our experimental measurements using two benchmark datasets (PASCAL
VOC and MS COCO) on two state-of-the-art detection algorithms (YOLO and SSD).
The results demonstrate serious adversarial vulnerabilities and the compelling
need for developing robust object detection systems.","['Ka-Ho Chow', 'Ling Liu', 'Mehmet Emre Gursoy', 'Stacey Truex', 'Wenqi Wei', 'Yanzhao Wu']",2020-04-09T01:36:23Z,http://arxiv.org/abs/2004.04320v1,"['cs.LG', 'cs.CR', 'cs.CV', 'stat.ML']"
Footprints and Free Space from a Single Color Image,"Understanding the shape of a scene from a single color image is a formidable
computer vision task. However, most methods aim to predict the geometry of
surfaces that are visible to the camera, which is of limited use when planning
paths for robots or augmented reality agents. Such agents can only move when
grounded on a traversable surface, which we define as the set of classes which
humans can also walk over, such as grass, footpaths and pavement. Models which
predict beyond the line of sight often parameterize the scene with voxels or
meshes, which can be expensive to use in machine learning frameworks.
  We introduce a model to predict the geometry of both visible and occluded
traversable surfaces, given a single RGB image as input. We learn from stereo
video sequences, using camera poses, per-frame depth and semantic segmentation
to form training data, which is used to supervise an image-to-image network. We
train models from the KITTI driving dataset, the indoor Matterport dataset, and
from our own casually captured stereo footage. We find that a surprisingly low
bar for spatial coverage of training scenes is required. We validate our
algorithm against a range of strong baselines, and include an assessment of our
predictions for a path-planning task.","['Jamie Watson', 'Michael Firman', 'Aron Monszpart', 'Gabriel J. Brostow']",2020-04-14T09:29:17Z,http://arxiv.org/abs/2004.06376v1,['cs.CV']
ALCN: Adaptive Local Contrast Normalization,"To make Robotics and Augmented Reality applications robust to illumination
changes, the current trend is to train a Deep Network with training images
captured under many different lighting conditions. Unfortunately, creating such
a training set is a very unwieldy and complex task. We therefore propose a
novel illumination normalization method that can easily be used for different
problems with challenging illumination conditions. Our preliminary experiments
show that among current normalization methods, the Difference-of Gaussians
method remains a very good baseline, and we introduce a novel illumination
normalization model that generalizes it. Our key insight is then that the
normalization parameters should depend on the input image, and we aim to train
a Convolutional Neural Network to predict these parameters from the input
image. This, however, cannot be done in a supervised manner, as the optimal
parameters are not known a priori. We thus designed a method to train this
network jointly with another network that aims to recognize objects under
different illuminations: The latter network performs well when the former
network predicts good values for the normalization parameters. We show that our
method significantly outperforms standard normalization methods and would also
be appear to be universal since it does not have to be re-trained for each new
application. Our method improves the robustness to light changes of
state-of-the-art 3D object detection and face recognition methods.","['Mahdi Rad', 'Peter M. Roth', 'Vincent Lepetit']",2020-04-15T13:40:03Z,http://arxiv.org/abs/2004.07945v1,['cs.CV']
"Conservative Plane Releasing for Spatial Privacy Protection in Mixed
  Reality","Augmented reality (AR) or mixed reality (MR) platforms require spatial
understanding to detect objects or surfaces, often including their structural
(i.e. spatial geometry) and photometric (e.g. color, and texture) attributes,
to allow applications to place virtual or synthetic objects seemingly
""anchored"" on to real world objects; in some cases, even allowing interactions
between the physical and virtual objects. These functionalities require AR/MR
platforms to capture the 3D spatial information with high resolution and
frequency; however, these pose unprecedented risks to user privacy. Aside from
objects being detected, spatial information also reveals the location of the
user with high specificity, e.g. in which part of the house the user is. In
this work, we propose to leverage spatial generalizations coupled with
conservative releasing to provide spatial privacy while maintaining data
utility. We designed an adversary that builds up on existing place and shape
recognition methods over 3D data as attackers to which the proposed spatial
privacy approach can be evaluated against. Then, we simulate user movement
within spaces which reveals more of their space as they move around utilizing
3D point clouds collected from Microsoft HoloLens. Results show that revealing
no more than 11 generalized planes--accumulated from successively revealed
spaces with large enough radius, i.e. $r\leq1.0m$--can make an adversary fail
in identifying the spatial location of the user for at least half of the time.
Furthermore, if the accumulated spaces are of smaller radius, i.e. each
successively revealed space is $r\leq 0.5m$, we can release up to 29
generalized planes while enjoying both better data utility and privacy.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2020-04-17T01:57:58Z,http://arxiv.org/abs/2004.08029v1,"['cs.CV', 'cs.CR', 'cs.HC']"
"Reference Pose Generation for Long-term Visual Localization via Learned
  Features and View Synthesis","Visual Localization is one of the key enabling technologies for autonomous
driving and augmented reality. High quality datasets with accurate 6
Degree-of-Freedom (DoF) reference poses are the foundation for benchmarking and
improving existing methods. Traditionally, reference poses have been obtained
via Structure-from-Motion (SfM). However, SfM itself relies on local features
which are prone to fail when images were taken under different conditions,
e.g., day/ night changes. At the same time, manually annotating feature
correspondences is not scalable and potentially inaccurate. In this work, we
propose a semi-automated approach to generate reference poses based on feature
matching between renderings of a 3D model and real images via learned features.
Given an initial pose estimate, our approach iteratively refines the pose based
on feature matches against a rendering of the model from the current pose
estimate. We significantly improve the nighttime reference poses of the popular
Aachen Day-Night dataset, showing that state-of-the-art visual localization
methods perform better (up to $47\%$) than predicted by the original reference
poses. We extend the dataset with new nighttime test images, provide
uncertainty estimates for our new reference poses, and introduce a new
evaluation criterion. We will make our reference poses and our framework
publicly available upon publication.","['Zichao Zhang', 'Torsten Sattler', 'Davide Scaramuzza']",2020-05-11T15:13:07Z,http://arxiv.org/abs/2005.05179v4,['cs.CV']
Imposing Regulation on Advanced Algorithms,"This book discusses the necessity and perhaps urgency for the regulation of
algorithms on which new technologies rely; technologies that have the potential
to re-shape human societies. From commerce and farming to medical care and
education, it is difficult to find any aspect of our lives that will not be
affected by these emerging technologies. At the same time, artificial
intelligence, deep learning, machine learning, cognitive computing, blockchain,
virtual reality and augmented reality, belong to the fields most likely to
affect law and, in particular, administrative law. The book examines
universally applicable patterns in administrative decisions and judicial
rulings. First, similarities and divergence in behavior among the different
cases are identified by analyzing parameters ranging from geographical location
and administrative decisions to judicial reasoning and legal basis. As it turns
out, in several of the cases presented, sources of general law, such as
competition or labor law, are invoked as a legal basis, due to the lack of
current specialized legislation. This book also investigates the role and
significance of national and indeed supranational regulatory bodies for
advanced algorithms and considers ENISA, an EU agency that focuses on network
and information security, as an interesting candidate for a European regulator
of advanced algorithms. Lastly, it discusses the involvement of representative
institutions in algorithmic regulation.",['Fotios Fitsilis'],2020-05-16T20:26:54Z,http://arxiv.org/abs/2005.08092v1,"['cs.CY', 'cs.AI', '68T01, 68T99', 'K.5.0; K.5.2; K.5.m']"
"milliEgo: Single-chip mmWave Radar Aided Egomotion Estimation via Deep
  Sensor Fusion","Robust and accurate trajectory estimation of mobile agents such as people and
robots is a key requirement for providing spatial awareness for emerging
capabilities such as augmented reality or autonomous interaction. Although
currently dominated by optical techniques e.g., visual-inertial odometry, these
suffer from challenges with scene illumination or featureless surfaces. As an
alternative, we propose milliEgo, a novel deep-learning approach to robust
egomotion estimation which exploits the capabilities of low-cost mmWave radar.
Although mmWave radar has a fundamental advantage over monocular cameras of
being metric i.e., providing absolute scale or depth, current single chip
solutions have limited and sparse imaging resolution, making existing
point-cloud registration techniques brittle. We propose a new architecture that
is optimized for solving this challenging pose transformation problem.
Secondly, to robustly fuse mmWave pose estimates with additional sensors, e.g.
inertial or visual sensors we introduce a mixed attention approach to deep
fusion. Through extensive experiments, we demonstrate our proposed system is
able to achieve 1.3% 3D error drift and generalizes well to unseen
environments. We also show that the neural architecture can be made highly
efficient and suitable for real-time embedded applications.","['Chris Xiaoxuan Lu', 'Muhamad Risqi U. Saputra', 'Peijun Zhao', 'Yasin Almalioglu', 'Pedro P. B. de Gusmao', 'Changhao Chen', 'Ke Sun', 'Niki Trigoni', 'Andrew Markham']",2020-06-03T13:32:02Z,http://arxiv.org/abs/2006.02266v2,['cs.RO']
A Survey on Deep Learning Techniques for Stereo-based Depth Estimation,"Estimating depth from RGB images is a long-standing ill-posed problem, which
has been explored for decades by the computer vision, graphics, and machine
learning communities. Among the existing techniques, stereo matching remains
one of the most widely used in the literature due to its strong connection to
the human binocular system. Traditionally, stereo-based depth estimation has
been addressed through matching hand-crafted features across multiple images.
Despite the extensive amount of research, these traditional techniques still
suffer in the presence of highly textured areas, large uniform regions, and
occlusions. Motivated by their growing success in solving various 2D and 3D
vision problems, deep learning for stereo-based depth estimation has attracted
growing interest from the community, with more than 150 papers published in
this area between 2014 and 2019. This new generation of methods has
demonstrated a significant leap in performance, enabling applications such as
autonomous driving and augmented reality. In this article, we provide a
comprehensive survey of this new and continuously growing field of research,
summarize the most commonly used pipelines, and discuss their benefits and
limitations. In retrospect of what has been achieved so far, we also conjecture
what the future may hold for deep learning-based stereo for depth estimation
research.","['Hamid Laga', 'Laurent Valentin Jospin', 'Farid Boussaid', 'Mohammed Bennamoun']",2020-06-01T13:09:46Z,http://arxiv.org/abs/2006.02535v1,"['cs.CV', 'cs.GR']"
3D Human Mesh Regression with Dense Correspondence,"Estimating 3D mesh of the human body from a single 2D image is an important
task with many applications such as augmented reality and Human-Robot
interaction. However, prior works reconstructed 3D mesh from global image
feature extracted by using convolutional neural network (CNN), where the dense
correspondences between the mesh surface and the image pixels are missing,
leading to suboptimal solution. This paper proposes a model-free 3D human mesh
estimation framework, named DecoMR, which explicitly establishes the dense
correspondence between the mesh and the local image features in the UV space
(i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts
pixel-to-surface dense correspondence map (i.e., IUV image), with which we
transfer local features from the image space to the UV space. Then the
transferred local image features are processed in the UV space to regress a
location map, which is well aligned with transferred features. Finally we
reconstruct 3D human mesh from the regressed location map with a predefined
mapping function. We also observe that the existing discontinuous UV map are
unfriendly to the learning of network. Therefore, we propose a novel UV map
that maintains most of the neighboring relations on the original mesh surface.
Experiments demonstrate that our proposed local feature alignment and
continuous UV map outperforms existing 3D mesh based methods on multiple public
benchmarks. Code will be made available at
https://github.com/zengwang430521/DecoMR","['Wang Zeng', 'Wanli Ouyang', 'Ping Luo', 'Wentao Liu', 'Xiaogang Wang']",2020-06-10T08:50:53Z,http://arxiv.org/abs/2006.05734v2,['cs.CV']
"An Edge Computing-based Photo Crowdsourcing Framework for Real-time 3D
  Reconstruction","Image-based three-dimensional (3D) reconstruction utilizes a set of photos to
build 3D model and can be widely used in many emerging applications such as
augmented reality (AR) and disaster recovery. Most of existing 3D
reconstruction methods require a mobile user to walk around the target area and
reconstruct objectives with a hand-held camera, which is inefficient and
time-consuming. To meet the requirements of delay intensive and resource hungry
applications in 5G, we propose an edge computing-based photo crowdsourcing
(EC-PCS) framework in this paper. The main objective is to collect a set of
representative photos from ubiquitous mobile and Internet of Things (IoT)
devices at the network edge for real-time 3D model reconstruction, with network
resource and monetary cost considerations. Specifically, we first propose a
photo pricing mechanism by jointly considering their freshness, resolution and
data size. Then, we design a novel photo selection scheme to dynamically select
a set of photos with the required target coverage and the minimum monetary
cost. We prove the NP-hardness of such problem, and develop an efficient
greedy-based approximation algorithm to obtain a near-optimal solution.
Moreover, an optimal network resource allocation scheme is presented, in order
to minimize the maximum uploading delay of the selected photos to the edge
server. Finally, a 3D reconstruction algorithm and a 3D model caching scheme
are performed by the edge server in real time. Extensive experimental results
based on real-world datasets demonstrate the superior performance of our EC-PCS
system over the existing mechanisms.","['Shuai Yu', 'Xu Chen', 'Shuai Wang', 'Lingjun Pu', 'Di Wu']",2020-07-03T09:16:07Z,http://arxiv.org/abs/2007.01562v1,"['cs.NI', 'cs.DC', 'eess.SP']"
Accelerating 3D Deep Learning with PyTorch3D,"Deep learning has significantly improved 2D image recognition. Extending into
3D may advance many new applications including autonomous vehicles, virtual and
augmented reality, authoring 3D content, and even improving 2D recognition.
However despite growing interest, 3D deep learning remains relatively
underexplored. We believe that some of this disparity is due to the engineering
challenges involved in 3D deep learning, such as efficiently processing
heterogeneous data and reframing graphics operations to be differentiable. We
address these challenges by introducing PyTorch3D, a library of modular,
efficient, and differentiable operators for 3D deep learning. It includes a
fast, modular differentiable renderer for meshes and point clouds, enabling
analysis-by-synthesis approaches. Compared with other differentiable renderers,
PyTorch3D is more modular and efficient, allowing users to more easily extend
it while also gracefully scaling to large meshes and images. We compare the
PyTorch3D operators and renderer with other implementations and demonstrate
significant speed and memory improvements. We also use PyTorch3D to improve the
state-of-the-art for unsupervised 3D mesh and point cloud prediction from 2D
images on ShapeNet. PyTorch3D is open-source and we hope it will help
accelerate research in 3D deep learning.","['Nikhila Ravi', 'Jeremy Reizenstein', 'David Novotny', 'Taylor Gordon', 'Wan-Yen Lo', 'Justin Johnson', 'Georgia Gkioxari']",2020-07-16T17:53:02Z,http://arxiv.org/abs/2007.08501v1,"['cs.CV', 'cs.GR', 'cs.LG']"
"OpenRooms: An End-to-End Open Framework for Photorealistic Indoor Scene
  Datasets","We propose a novel framework for creating large-scale photorealistic datasets
of indoor scenes, with ground truth geometry, material, lighting and semantics.
Our goal is to make the dataset creation process widely accessible,
transforming scans into photorealistic datasets with high-quality ground truth
for appearance, layout, semantic labels, high quality spatially-varying BRDF
and complex lighting, including direct, indirect and visibility components.
This enables important applications in inverse rendering, scene understanding
and robotics. We show that deep networks trained on the proposed dataset
achieve competitive performance for shape, material and lighting estimation on
real images, enabling photorealistic augmented reality applications, such as
object insertion and material editing. We also show our semantic labels may be
used for segmentation and multi-task learning. Finally, we demonstrate that our
framework may also be integrated with physics engines, to create virtual
robotics environments with unique ground truth such as friction coefficients
and correspondence to real scenes. The dataset and all the tools to create such
datasets will be made publicly available.","['Zhengqin Li', 'Ting-Wei Yu', 'Shen Sang', 'Sarah Wang', 'Meng Song', 'Yuhan Liu', 'Yu-Ying Yeh', 'Rui Zhu', 'Nitesh Gundavarapu', 'Jia Shi', 'Sai Bi', 'Zexiang Xu', 'Hong-Xing Yu', 'Kalyan Sunkavalli', 'Miloš Hašan', 'Ravi Ramamoorthi', 'Manmohan Chandraker']",2020-07-25T06:48:47Z,http://arxiv.org/abs/2007.12868v3,['cs.CV']
IEEE 802.11be-Wi-Fi 7: New Challenges and Opportunities,"With the emergence of 4k/8k video, the throughput requirement of video
delivery will keep grow to tens of Gbps. Other new high-throughput and
low-latency video applications including augmented reality (AR), virtual
reality (VR), and online gaming, are also proliferating. Due to the related
stringent requirements, supporting these applications over wireless local area
network (WLAN) is far beyond the capabilities of the new WLAN standard -- IEEE
802.11ax. To meet these emerging demands, the IEEE 802.11 will release a new
amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known
as Wireless-Fidelity (Wi-Fi) 7. This article provides the comprehensive survey
on the key medium access control (MAC) layer techniques and physical layer
(PHY) techniques being discussed in the EHT task group, including the
channelization and tone plan, multiple resource units (multi-RU) support, 4096
quadrature amplitude modulation (4096-QAM), preamble designs, multiple link
operations (e.g., multi-link aggregation and channel access), multiple input
multiple output (MIMO) enhancement, multiple access point (multi-AP)
coordination (e.g., multi-AP joint transmission), enhanced link adaptation and
retransmission protocols (e.g., hybrid automatic repeat request (HARQ)). This
survey covers both the critical technologies being discussed in EHT standard
and the related latest progresses from worldwide research. Besides, the
potential developments beyond EHT are discussed to provide some possible future
research directions for WLAN.","['Cailian Deng', 'Xuming Fang', 'Xiao Han', 'Xianbin Wang', 'Li Yan', 'Rong He', 'Yan Long', 'Yuchen Guo']",2020-07-27T09:40:28Z,http://arxiv.org/abs/2007.13401v3,['eess.SP']
Towards Collaborative Drilling with a Cobot Using Admittance Controller,"In the near future, collaborative robots (cobots) are expected to play a
vital role in the manufacturing and automation sectors. It is predicted that
workers will work side by side in collaboration with cobots to surpass fully
automated factories. In this regard, physical human-robot interaction (pHRI)
aims to develop natural communication between the partners to bring speed,
flexibility, and ergonomics to the execution of complex manufacturing tasks.
One challenge in pHRI is to design an optimal interaction controller to balance
the limitations introduced by the contradicting nature of transparency and
stability requirements. In this paper, a general methodology to design an
admittance controller for a pHRI system is developed by considering the
stability and transparency objectives. In our approach, collaborative robot
constrains the movement of human operator to help with a pHRI task while an
augmented reality (AR) interface informs the operator about its phases. To this
end, dynamical characterization of the collaborative robot (LBR IIWA 7 R800,
KUKA Inc.) is presented first. Then, the stability and transparency analyses
for our pHRI task involving collaborative drilling with this robot are
reported. A range of allowable parameters for the admittance controller is
determined by superimposing the stability and transparency graphs. Finally,
three different sets of parameters are selected from the allowable range and
the effect of admittance controllers utilizing these parameter sets on the task
performance is investigated.","['Yusuf Aydin', 'Doganay Sirintuna', 'Cagatay Basdogan']",2020-07-28T22:19:48Z,http://arxiv.org/abs/2007.14503v1,['cs.RO']
Learning Illumination from Diverse Portraits,"We present a learning-based technique for estimating high dynamic range
(HDR), omnidirectional illumination from a single low dynamic range (LDR)
portrait image captured under arbitrary indoor or outdoor lighting conditions.
We train our model using portrait photos paired with their ground truth
environmental illumination. We generate a rich set of such photos by using a
light stage to record the reflectance field and alpha matte of 70 diverse
subjects in various expressions. We then relight the subjects using image-based
relighting with a database of one million HDR lighting environments,
compositing the relit subjects onto paired high-resolution background imagery
recorded during the lighting acquisition. We train the lighting estimation
model using rendering-based loss functions and add a multi-scale adversarial
loss to estimate plausible high frequency lighting detail. We show that our
technique outperforms the state-of-the-art technique for portrait-based
lighting estimation, and we also show that our method reliably handles the
inherent ambiguity between overall lighting strength and surface albedo,
recovering a similar scale of illumination for subjects with diverse skin
tones. We demonstrate that our method allows virtual objects and digital
characters to be added to a portrait photograph with consistent illumination.
Our lighting inference runs in real-time on a smartphone, enabling realistic
rendering and compositing of virtual objects into live video for augmented
reality applications.","['Chloe LeGendre', 'Wan-Chun Ma', 'Rohit Pandey', 'Sean Fanello', 'Christoph Rhemann', 'Jason Dourgarian', 'Jay Busch', 'Paul Debevec']",2020-08-05T23:41:23Z,http://arxiv.org/abs/2008.02396v1,"['cs.CV', 'cs.GR']"
Assisted Perception: Optimizing Observations to Communicate State,"We aim to help users estimate the state of the world in tasks like robotic
teleoperation and navigation with visual impairments, where users may have
systematic biases that lead to suboptimal behavior: they might struggle to
process observations from multiple sensors simultaneously, receive delayed
observations, or overestimate distances to obstacles. While we cannot directly
change the user's internal beliefs or their internal state estimation process,
our insight is that we can still assist them by modifying the user's
observations. Instead of showing the user their true observations, we
synthesize new observations that lead to more accurate internal state estimates
when processed by the user. We refer to this method as assistive state
estimation (ASE): an automated assistant uses the true observations to infer
the state of the world, then generates a modified observation for the user to
consume (e.g., through an augmented reality interface), and optimizes the
modification to induce the user's new beliefs to match the assistant's current
beliefs. We evaluate ASE in a user study with 12 participants who each perform
four tasks: two tasks with known user biases -- bandwidth-limited image
classification and a driving video game with observation delay -- and two with
unknown biases that our method has to learn -- guided 2D navigation and a lunar
lander teleoperation video game. A different assistance strategy emerges in
each domain, such as quickly revealing informative pixels to speed up image
classification, using a dynamics model to undo observation delay in driving,
identifying nearby landmarks for navigation, and exaggerating a visual
indicator of tilt in the lander game. The results show that ASE substantially
improves the task performance of users with bandwidth constraints, observation
delay, and other unknown biases.","['Siddharth Reddy', 'Sergey Levine', 'Anca D. Dragan']",2020-08-06T19:08:05Z,http://arxiv.org/abs/2008.02840v1,"['cs.LG', 'cs.HC', 'cs.RO', 'stat.ML']"
HoliCity: A City-Scale Data Platform for Learning Holistic 3D Structures,"We present HoliCity, a city-scale 3D dataset with rich structural
information. Currently, this dataset has 6,300 real-world panoramas of
resolution $13312 \times 6656$ that are accurately aligned with the CAD model
of downtown London with an area of more than 20 km$^2$, in which the median
reprojection error of the alignment of an average image is less than half a
degree. This dataset aims to be an all-in-one data platform for research of
learning abstracted high-level holistic 3D structures that can be derived from
city CAD models, e.g., corners, lines, wireframes, planes, and cuboids, with
the ultimate goal of supporting real-world applications including city-scale
reconstruction, localization, mapping, and augmented reality. The accurate
alignment of the 3D CAD models and panoramas also benefits low-level 3D vision
tasks such as surface normal estimation, as the surface normal extracted from
previous LiDAR-based datasets is often noisy. We conduct experiments to
demonstrate the applications of HoliCity, such as predicting surface
segmentation, normal maps, depth maps, and vanishing points, as well as test
the generalizability of methods trained on HoliCity and other related datasets.
HoliCity is available at https://holicity.io.","['Yichao Zhou', 'Jingwei Huang', 'Xili Dai', 'Shichen Liu', 'Linjie Luo', 'Zhili Chen', 'Yi Ma']",2020-08-07T17:34:47Z,http://arxiv.org/abs/2008.03286v2,['cs.CV']
Pen-based Interaction with Spreadsheets in Mobile Virtual Reality,"Virtual Reality (VR) can enhance the display and interaction of mobile
knowledge work and in particular, spreadsheet applications. While spreadsheets
are widely used yet are challenging to interact with, especially on mobile
devices, using them in VR has not been explored in depth. A special uniqueness
of the domain is the contrast between the immersive and large display space
afforded by VR, contrasted by the very limited interaction space that may be
afforded for the information worker on the go, such as an airplane seat or a
small work-space. To close this gap, we present a tool-set for enhancing
spreadsheet interaction on tablets using immersive VR headsets and pen-based
input. This combination opens up many possibilities for enhancing the
productivity for spreadsheet interaction. We propose to use the space around
and in front of the tablet for enhanced visualization of spreadsheet data and
meta-data. For example, extending sheet display beyond the bounds of the
physical screen, or easier debugging by uncovering hidden dependencies between
sheet's cells. Combining the precise on-screen input of a pen with spatial
sensing around the tablet, we propose tools for the efficient creation and
editing of spreadsheets functions such as off-the-screen layered menus,
visualization of sheets dependencies, and gaze-and-touch-based switching
between spreadsheet tabs. We study the feasibility of the proposed tool-set
using a video-based online survey and an expert-based assessment of indicative
human performance potential.","['Travis Gesslein', 'Verena Biener', 'Philipp Gagel', 'Daniel Schneider', 'Per Ola Kristensson', 'Eyal Ofek', 'Michel Pahud', 'Jens Grubert']",2020-08-11T06:39:35Z,http://arxiv.org/abs/2008.04543v1,"['cs.HC', 'I.3.7']"
Attention-based 3D Object Reconstruction from a Single Image,"Recently, learning-based approaches for 3D reconstruction from 2D images have
gained popularity due to its modern applications, e.g., 3D printers, autonomous
robots, self-driving cars, virtual reality, and augmented reality. The computer
vision community has applied a great effort in developing functions to
reconstruct the full 3D geometry of objects and scenes. However, to extract
image features, they rely on convolutional neural networks, which are
ineffective in capturing long-range dependencies. In this paper, we propose to
substantially improve Occupancy Networks, a state-of-the-art method for 3D
object reconstruction. For such we apply the concept of self-attention within
the network's encoder in order to leverage complementary input features rather
than those based on local regions, helping the encoder to extract global
information. With our approach, we were capable of improving the original work
in 5.05% of mesh IoU, 0.83% of Normal Consistency, and more than 10X the
Chamfer-L1 distance. We also perform a qualitative study that shows that our
approach was able to generate much more consistent meshes, confirming its
increased generalization power over the current state-of-the-art.","['Andrey Salvi', 'Nathan Gavenski', 'Eduardo Pooch', 'Felipe Tasoniero', 'Rodrigo Barros']",2020-08-11T14:51:18Z,http://arxiv.org/abs/2008.04738v1,['cs.CV']
"Mesorasi: Architecture Support for Point Cloud Analytics via
  Delayed-Aggregation","Point cloud analytics is poised to become a key workload on battery-powered
embedded and mobile platforms in a wide range of emerging application domains,
such as autonomous driving, robotics, and augmented reality, where efficiency
is paramount. This paper proposes Mesorasi, an algorithm-architecture
co-designed system that simultaneously improves the performance and energy
efficiency of point cloud analytics while retaining its accuracy. Our extensive
characterizations of state-of-the-art point cloud algorithms show that, while
structurally reminiscent of convolutional neural networks (CNNs), point cloud
algorithms exhibit inherent compute and memory inefficiencies due to the unique
characteristics of point cloud data. We propose delayed-aggregation, a new
algorithmic primitive for building efficient point cloud algorithms.
Delayed-aggregation hides the performance bottlenecks and reduces the compute
and memory redundancies by exploiting the approximately distributive property
of key operations in point cloud algorithms. Delayed-aggregation let point
cloud algorithms achieve 1.6x speedup and 51.1% energy reduction on a mobile
GPU while retaining the accuracy (-0.9% loss to 1.2% gains). To maximize the
algorithmic benefits, we propose minor extensions to contemporary CNN
accelerators, which can be integrated into a mobile Systems-on-a-Chip (SoC)
without modifying other SoC components. With additional hardware support,
Mesorasi achieves up to 3.6x speedup.","['Yu Feng', 'Boyuan Tian', 'Tiancheng Xu', 'Paul Whatmough', 'Yuhao Zhu']",2020-08-16T18:11:19Z,http://arxiv.org/abs/2008.06967v1,"['cs.CV', 'cs.AR']"
"Retargetable AR: Context-aware Augmented Reality in Indoor Scenes based
  on 3D Scene Graph","In this paper, we present Retargetable AR, a novel AR framework that yields
an AR experience that is aware of scene contexts set in various real
environments, achieving natural interaction between the virtual and real
worlds. To this end, we characterize scene contexts with relationships among
objects in 3D space, not with coordinates transformations. A context assumed by
an AR content and a context formed by a real environment where users experience
AR are represented as abstract graph representations, i.e. scene graphs. From
RGB-D streams, our framework generates a volumetric map in which geometric and
semantic information of a scene are integrated. Moreover, using the semantic
map, we abstract scene objects as oriented bounding boxes and estimate their
orientations. With such a scene representation, our framework constructs, in an
online fashion, a 3D scene graph characterizing the context of a real
environment for AR. The correspondence between the constructed graph and an AR
scene graph denoting the context of AR content provides a semantically
registered content arrangement, which facilitates natural interaction between
the virtual and real worlds. We performed extensive evaluations on our
prototype system through quantitative evaluation of the performance of the
oriented bounding box estimation, subjective evaluation of the AR content
arrangement based on constructed 3D scene graphs, and an online AR
demonstration. The results of these evaluations showed the effectiveness of our
framework, demonstrating that it can provide a context-aware AR experience in a
variety of real scenes.","['Tomu Tahara', 'Takashi Seno', 'Gaku Narita', 'Tomoya Ishikawa']",2020-08-18T09:25:55Z,http://arxiv.org/abs/2008.07817v1,['cs.CV']
"A survey on applications of augmented, mixed and virtual reality for
  nature and environment","Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are
technologies of great potential due to the engaging and enriching experiences
they are capable of providing. Their use is rapidly increasing in diverse
fields such as medicine, manufacturing or entertainment. However, the
possibilities that AR, VR and MR offer in the area of environmental
applications are not yet widely explored. In this paper we present the outcome
of a survey meant to discover and classify existing AR/VR/MR applications that
can benefit the environment or increase awareness on environmental issues. We
performed an exhaustive search over several online publication access platforms
and past proceedings of major conferences in the fields of AR/VR/MR. Identified
relevant papers were filtered based on novelty, technical soundness, impact and
topic relevance, and classified into different categories. Referring to the
selected papers, we discuss how the applications of each category are
contributing to environmental protection, preservation and sensitization
purposes. We further analyse these approaches as well as possible future
directions in the scope of existing and upcoming AR/VR/MR enabling
technologies.","['Jason Rambach', 'Gergana Lilligreen', 'Alexander Schäfer', 'Ramya Bankanal', 'Alexander Wiebel', 'Didier Stricker']",2020-08-27T09:59:27Z,http://arxiv.org/abs/2008.12024v2,"['cs.HC', 'cs.CV', 'cs.CY', 'cs.GT']"
"Pixel-Face: A Large-Scale, High-Resolution Benchmark for 3D Face
  Reconstruction","3D face reconstruction is a fundamental task that can facilitate numerous
applications such as robust facial analysis and augmented reality. It is also a
challenging task due to the lack of high-quality datasets that can fuel current
deep learning-based methods. However, existing datasets are limited in
quantity, realisticity and diversity. To circumvent these hurdles, we introduce
Pixel-Face, a large-scale, high-resolution and diverse 3D face dataset with
massive annotations. Specifically, Pixel-Face contains 855 subjects aging from
18 to 80. Each subject has more than 20 samples with various expressions. Each
sample is composed of high-resolution multi-view RGB images and 3D meshes with
various expressions. Moreover, we collect precise landmarks annotation and 3D
registration result for each data. To demonstrate the advantages of Pixel-Face,
we re-parameterize the 3D Morphable Model (3DMM) into Pixel-3DM using the
collected data. We show that the obtained Pixel-3DM is better in modeling a
wide range of face shapes and expressions. We also carefully benchmark existing
3D face reconstruction methods on our dataset. Moreover, Pixel-Face serves as
an effective training source. We observe that the performance of current face
reconstruction models significantly improves both on existing benchmarks and
Pixel-Face after being fine-tuned using our newly collected data. Extensive
experiments demonstrate the effectiveness of Pixel-3DM and the usefulness of
Pixel-Face.","['Jiangjing Lyu', 'Xiaobo Li', 'Xiangyu Zhu', 'Cheng Cheng']",2020-08-28T02:22:07Z,http://arxiv.org/abs/2008.12444v3,['cs.CV']
"Homography Estimation with Convolutional Neural Networks Under
  Conditions of Variance","Planar homography estimation is foundational to many computer vision
problems, such as Simultaneous Localization and Mapping (SLAM) and Augmented
Reality (AR). However, conditions of high variance confound even the
state-of-the-art algorithms. In this report, we analyze the performance of two
recently published methods using Convolutional Neural Networks (CNNs) that are
meant to replace the more traditional feature-matching based approaches to the
estimation of homography. Our evaluation of the CNN based methods focuses
particularly on measuring the performance under conditions of significant
noise, illumination shift, and occlusion. We also measure the benefits of
training CNNs to varying degrees of noise. Additionally, we compare the effect
of using color images instead of grayscale images for inputs to CNNs. Finally,
we compare the results against baseline feature-matching based homography
estimation methods using SIFT, SURF, and ORB. We find that CNNs can be trained
to be more robust against noise, but at a small cost to accuracy in the
noiseless case. Additionally, CNNs perform significantly better in conditions
of extreme variance than their feature-matching based counterparts. With regard
to color inputs, we conclude that with no change in the CNN architecture to
take advantage of the additional information in the color planes, the
difference in performance using color inputs or grayscale inputs is negligible.
About the CNNs trained with noise-corrupted inputs, we show that training a CNN
to a specific magnitude of noise leads to a ""Goldilocks Zone"" with regard to
the noise levels where that CNN performs best.","['David Niblick', 'Avinash Kak']",2020-10-02T15:11:25Z,http://arxiv.org/abs/2010.01041v2,['cs.CV']
Interface Design for HCI Classroom: From Learners' Perspective,"Having a good Human-Computer Interaction (HCI) design is challenging.
Previous works have contributed significantly to fostering HCI, including
design principle with report study from the instructor view. The questions of
how and to what extent students perceive the design principles are still left
open. To answer this question, this paper conducts a study of HCI adoption in
the classroom. The studio-based learning method was adapted to teach 83
graduate and undergraduate students in 16 weeks long with four activities. A
standalone presentation tool for instant online peer feedback during the
presentation session was developed to help students justify and critique
other's work. Our tool provides a sandbox, which supports multiple application
types, including Web-applications, Object Detection, Web-based Virtual Reality
(VR), and Augmented Reality (AR). After presenting one assignment and two
projects, our results showed that students acquired a better understanding of
the Golden Rules principle over time, which was demonstrated by the development
of visual interface design. The Wordcloud reveals the primary focus was on the
user interface and shed some light on students' interest in user experience.
The inter-rater score indicates the agreement among students that they have the
same level of understanding of the principles. The results show a high level of
guideline compliance with HCI principles, in which we witnessed variations in
visual cognitive styles. Regardless of diversity in visual preference, the
students presented high consistency and a similar perspective on adopting HCI
design principles. The results also elicited suggestions into the development
of the HCI curriculum in the future.","['Huyen N. Nguyen', 'Vinh T. Nguyen', 'Tommy Dang']",2020-10-04T18:49:24Z,http://arxiv.org/abs/2010.01651v1,"['cs.HC', 'H.5.2; H.1.2; K.3.2']"
"Parallax Motion Effect Generation Through Instance Segmentation And
  Depth Estimation","Stereo vision is a growing topic in computer vision due to the innumerable
opportunities and applications this technology offers for the development of
modern solutions, such as virtual and augmented reality applications. To
enhance the user's experience in three-dimensional virtual environments, the
motion parallax estimation is a promising technique to achieve this objective.
In this paper, we propose an algorithm for generating parallax motion effects
from a single image, taking advantage of state-of-the-art instance segmentation
and depth estimation approaches. This work also presents a comparison against
such algorithms to investigate the trade-off between efficiency and quality of
the parallax motion effects, taking into consideration a multi-task learning
network capable of estimating instance segmentation and depth estimation at
once. Experimental results and visual quality assessment indicate that the
PyD-Net network (depth estimation) combined with Mask R-CNN or FBNet networks
(instance segmentation) can produce parallax motion effects with good visual
quality.","['Allan Pinto', 'Manuel A. Córdova', 'Luis G. L. Decker', 'Jose L. Flores-Campana', 'Marcos R. Souza', 'Andreza A. dos Santos', 'Jhonatas S. Conceição', 'Henrique F. Gagliardi', 'Diogo C. Luvizon', 'Ricardo da S. Torres', 'Helio Pedrini']",2020-10-06T12:56:59Z,http://arxiv.org/abs/2010.02680v1,"['cs.CV', 'cs.LG', 'eess.IV']"
SHREC 2020 track: 6D Object Pose Estimation,"6D pose estimation is crucial for augmented reality, virtual reality, robotic
manipulation and visual navigation. However, the problem is challenging due to
the variety of objects in the real world. They have varying 3D shape and their
appearances in captured images are affected by sensor noise, changing lighting
conditions and occlusions between objects. Different pose estimation methods
have different strengths and weaknesses, depending on feature representations
and scene contents. At the same time, existing 3D datasets that are used for
data-driven methods to estimate 6D poses have limited view angles and low
resolution. To address these issues, we organize the Shape Retrieval Challenge
benchmark on 6D pose estimation and create a physically accurate simulator that
is able to generate photo-realistic color-and-depth image pairs with
corresponding ground truth 6D poses. From captured color and depth images, we
use this simulator to generate a 3D dataset which has 400 photo-realistic
synthesized color-and-depth image pairs with various view angles for training,
and another 100 captured and synthetic images for testing. Five research groups
register in this track and two of them submitted their results. Data-driven
methods are the current trend in 6D object pose estimation and our evaluation
results show that approaches which fully exploit the color and geometric
features are more robust for 6D pose estimation of reflective and texture-less
objects and occlusion. This benchmark and comparative evaluation results have
the potential to further enrich and boost the research of 6D object pose
estimation and its applications.","['Honglin Yuan', 'Remco C. Veltkamp', 'Georgios Albanis', 'Nikolaos Zioulis', 'Dimitrios Zarpalas', 'Petros Daras']",2020-10-19T09:45:42Z,http://arxiv.org/abs/2010.09355v1,"['cs.CV', 'cs.LG', 'cs.RO']"
"Improving the generalization of network based relative pose regression:
  dimension reduction as a regularizer","Visual localization occupies an important position in many areas such as
Augmented Reality, robotics and 3D reconstruction. The state-of-the-art visual
localization methods perform pose estimation using geometry based solver within
the RANSAC framework. However, these methods require accurate pixel-level
matching at high image resolution, which is hard to satisfy under significant
changes from appearance, dynamics or perspective of view. End-to-end learning
based regression networks provide a solution to circumvent the requirement for
precise pixel-level correspondences, but demonstrate poor performance towards
cross-scene generalization. In this paper, we explicitly add a learnable
matching layer within the network to isolate the pose regression solver from
the absolute image feature values, and apply dimension regularization on both
the correlation feature channel and the image scale to further improve
performance towards generalization and large viewpoint change. We implement
this dimension regularization strategy within a two-layer pyramid based
framework to regress the localization results from coarse to fine. In addition,
the depth information is fused for absolute translational scale recovery.
Through experiments on real world RGBD datasets we validate the effectiveness
of our design in terms of improving both generalization performance and
robustness towards viewpoint change, and also show the potential of regression
based visual localization networks towards challenging occasions that are
difficult for geometry based visual localization methods.","['Xiaqing Ding', 'Yue Wang', 'Li Tang', 'Yanmei Jiao', 'Rong Xiong']",2020-10-24T06:20:46Z,http://arxiv.org/abs/2010.12796v1,"['cs.CV', 'cs.RO']"
DeepMorph: A System for Hiding Bitstrings in Morphable Vector Drawings,"We introduce DeepMorph, an information embedding technique for vector
drawings. Provided a vector drawing, such as a Scalable Vector Graphics (SVG)
file, our method embeds bitstrings in the image by perturbing the drawing
primitives (lines, circles, etc.). This results in a morphed image that can be
decoded to recover the original bitstring. The use-case is similar to that of
the well-known QR code, but our solution provides creatives with artistic
freedom to transfer digital information via drawings of their own design. The
method comprises two neural networks, which are trained jointly: an encoder
network that transforms a bitstring into a perturbation of the drawing
primitives, and a decoder network that recovers the bitstring from an image of
the morphed drawing. To enable end-to-end training via back propagation, we
introduce a soft rasterizer, which is differentiable with respect to
perturbations of the drawing primitives. In order to add robustness towards
real-world image capture conditions, image corruptions are injected between the
soft rasterizer and the decoder. Further, the addition of an object detection
and camera pose estimation system enables decoding of drawings in complex
scenes as well as use of the drawings as markers for use in augmented reality
applications. We demonstrate that our method reliably recovers bitstrings from
real-world photos of printed drawings, thereby providing a novel solution for
creatives to transfer digital information via artistic imagery.","['Søren Rasmussen', 'Karsten Østergaard Noe', 'Oliver Gyldenberg Hjermitslev', 'Henrik Pedersen']",2020-11-19T11:55:39Z,http://arxiv.org/abs/2011.09783v1,['cs.CV']
"Semantic-Based VPS for Smartphone Localization in Challenging Urban
  Environments","Accurate smartphone-based outdoor localization system in deep urban canyons
are increasingly needed for various IoT applications such as augmented reality,
intelligent transportation, etc. The recently developed feature-based visual
positioning system (VPS) by Google detects edges from smartphone images to
match with pre-surveyed edges in their map database. As smart cities develop,
the building information modeling (BIM) becomes widely available, which
provides an opportunity for a new semantic-based VPS. This article proposes a
novel 3D city model and semantic-based VPS for accurate and robust pose
estimation in urban canyons where global navigation satellite system (GNSS)
tends to fail. In the offline stage, a material segmented city model is used to
generate segmented images. In the online stage, an image is taken with a
smartphone camera that provides textual information about the surrounding
environment. The approach utilizes computer vision algorithms to rectify and
hand segment between the different types of material identified in the
smartphone image. A semantic-based VPS method is then proposed to match the
segmented generated images with the segmented smartphone image. Each generated
image holds a pose that contains the latitude, longitude, altitude, yaw, pitch,
and roll. The candidate with the maximum likelihood is regarded as the precise
pose of the user. The positioning results achieves 2.0m level accuracy in
common high rise along street, 5.5m in foliage dense environment and 15.7m in
alleyway. A 45% positioning improvement to current state-of-the-art method. The
estimation of yaw achieves 2.3{\deg} level accuracy, 8 times the improvement to
smartphone IMU.","['Max Jwo Lem Lee', 'Li-Ta Hsu', 'Hoi-Fung Ng', 'Shang Lee']",2020-11-21T08:18:43Z,http://arxiv.org/abs/2011.10743v1,['cs.RO']
Benchmarking Image Retrieval for Visual Localization,"Visual localization, i.e., camera pose estimation in a known scene, is a core
component of technologies such as autonomous driving and augmented reality.
State-of-the-art localization approaches often rely on image retrieval
techniques for one of two tasks: (1) provide an approximate pose estimate or
(2) determine which parts of the scene are potentially visible in a given query
image. It is common practice to use state-of-the-art image retrieval algorithms
for these tasks. These algorithms are often trained for the goal of retrieving
the same landmark under a large range of viewpoint changes. However, robustness
to viewpoint changes is not necessarily desirable in the context of visual
localization. This paper focuses on understanding the role of image retrieval
for multiple visual localization tasks. We introduce a benchmark setup and
compare state-of-the-art retrieval representations on multiple datasets. We
show that retrieval performance on classical landmark retrieval/recognition
tasks correlates only for some but not all tasks to localization performance.
This indicates a need for retrieval approaches specifically designed for
localization tasks. Our benchmark and evaluation protocols are available at
https://github.com/naver/kapture-localization.","['Noé Pion', 'Martin Humenberger', 'Gabriela Csurka', 'Yohann Cabon', 'Torsten Sattler']",2020-11-24T07:59:52Z,http://arxiv.org/abs/2011.11946v2,"['cs.CV', 'cs.LG']"
"Towards the Development of 3D Engine Assembly Simulation Learning Module
  for Senior High School","The focus of the study is to develop a 3D engine assembly simulation learning
module to address the lack of equipment in one senior high school in the
Philippines. The study used mixed-method to determine the considerations needed
in developing an application for educational use particularly among
laboratory/practical subjects like engine assembly. The study used ISO 25010
quality standards in evaluating the application(n=153 students and 3 ICT
experts).Results showed that the application is moderately acceptable(overall
mean = 3.52) under ISO 25010 quality standards. The study created an engine
simulation learning assembly in which teachers can use to augment their lesson.
The study also highlights the applicability of using 3D-related technologies
for practical and laboratory subjects particularly highly technical-related
subjects. Future studies may develop a similar application in the same context
using mobile and other emerging technology(i.e., Virtual Reality, Augmented
Reality) as well as making the content more customizable. Effectivity of the
system in an actual setting is also worth pursuing. The study highlighted the
potential use of 3D technology in a classroom setting.","['John Paul P. Miranda', 'Jaymark A. Yambao', 'Jhon Asley M. Marcelo', 'Christopher Robert N. Gonzales', 'Vee-jay T. Mungcal']",2020-11-19T10:45:29Z,http://arxiv.org/abs/2011.12767v1,['cs.CY']
"Energy Drain of the Object Detection Processing Pipeline for Mobile
  Devices: Analysis and Implications","Applying deep learning to object detection provides the capability to
accurately detect and classify complex objects in the real world. However,
currently, few mobile applications use deep learning because such technology is
computation-intensive and energy-consuming. This paper, to the best of our
knowledge, presents the first detailed experimental study of a mobile augmented
reality (AR) client's energy consumption and the detection latency of executing
Convolutional Neural Networks (CNN) based object detection, either locally on
the smartphone or remotely on an edge server. In order to accurately measure
the energy consumption on the smartphone and obtain the breakdown of energy
consumed by each phase of the object detection processing pipeline, we propose
a new measurement strategy. Our detailed measurements refine the energy
analysis of mobile AR clients and reveal several interesting perspectives
regarding the energy consumption of executing CNN-based object detection.
Furthermore, several insights and research opportunities are proposed based on
our experimental results. These findings from our experimental study will guide
the design of energy-efficient processing pipeline of CNN-based object
detection.","['Haoxin Wang', 'BaekGyu Kim', 'Jiang Xie', 'Zhu Han']",2020-11-26T00:32:07Z,http://arxiv.org/abs/2011.13075v1,"['cs.PF', 'cs.CV', 'cs.MM', 'cs.NI']"
"IntegriScreen: Visually Supervising Remote User Interactions on
  Compromised Clients","Remote services and applications that users access via their local clients
(laptops or desktops) usually assume that, following a successful user
authentication at the beginning of the session, all subsequent communication
reflects the user's intent. However, this is not true if the adversary gains
control of the client and can therefore manipulate what the user sees and what
is sent to the remote server.
  To protect the user's communication with the remote server despite a
potentially compromised local client, we propose the concept of continuous
visual supervision by a second device equipped with a camera. Motivated by the
rapid increase of the number of incoming devices with front-facing cameras,
such as augmented reality headsets and smart home assistants, we build upon the
core idea that the user's actual intended input is what is shown on the
client's screen, despite what ends up being sent to the remote server. A
statically positioned camera enabled device can, therefore, continuously
analyze the client's screen to enforce that the client behaves honestly despite
potentially being malicious.
  We evaluate the present-day feasibility and deployability of this concept by
developing a fully functional prototype, running a host of experimental tests
on three different mobile devices, and by conducting a user study in which we
analyze participants' use of the system during various simulated attacks.
Experimental evaluation indeed confirms the feasibility of the concept of
visual supervision, given that the system consistently detects over 98% of
evaluated attacks, while study participants with little instruction detect the
remaining attacks with high probability.","['Ivo Sluganovic', 'Enis Ulqinaku', 'Aritra Dhar', 'Daniele Lain', 'Srdjan Capkun', 'Ivan Martinovic']",2020-11-27T20:05:29Z,http://arxiv.org/abs/2011.13979v1,"['cs.CR', 'cs.HC']"
"cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex
  Polytope","During the last years, the emerging field of Augmented & Virtual Reality
(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop
low cost high-quality AR systems where computing poweris in demand. Feature
points are extensively used in these real-time frame-rate and 3D applications,
thereforeefficient high-speed feature detectors are necessary. Corners are such
special features and often are used as thefirst step in the marker alignment in
Augmented Reality (AR). Corners are also used in image registration
andrecognition, tracking, SLAM, robot path finding and 2D or 3D object
detection and retrieval. Therefore thereis a large number of corner detection
algorithms but most of them are too computationally intensive for use
inreal-time applications of any complexity. Many times the border of the image
is a convex polygon. For thisspecial, but quite common case, we have developed
a specific algorithm, cMinMax. The proposed algorithmis faster, approximately
by a factor of 5 compared to the widely used Harris Corner Detection algorithm.
Inaddition is highly parallelizable. The algorithm is suitable for the fast
registration of markers in augmentedreality systems and in applications where a
computationally efficient real time feature detector is necessary.The algorithm
can also be extended to N-dimensional polyhedrons.","['Dimitrios Chamzas', 'Constantinos Chamzas', 'Konstantinos Moustakas']",2020-11-28T00:32:11Z,http://arxiv.org/abs/2011.14035v3,"['cs.CV', 'cs.GR']"
"Irregular Metronomes as Assistive Devices to Promote Healthy Gait
  Patterns","Older adults and people suffering from neurodegenerative disease often
experience difficulty controlling gait during locomotion, ultimately increasing
their risk of falling. To combat these effects, researchers and clinicians have
used metronomes as assistive devices to improve movement timing in hopes of
reducing their risk of falling. Historically, researchers in this area have
relied on metronomes with isochronous interbeat intervals, which may be
problematic because normal healthy gait varies considerably from one step to
the next. More recently, researchers have advocated the use of irregular
metronomes embedded with statistical properties found in healthy populations.
In this paper, we explore the effect of both regular and irregular metronomes
on many statistical properties of interstride intervals. Furthermore, we
investigate how these properties react to mechanical perturbation in the form
of a halted treadmill belt while walking. Our results demonstrate that
metronomes that are either isochronous or random metronome break down the
inherent structure of healthy gait. Metronomes with statistical properties
similar to healthy gait seem to preserve those properties, despite a strong
mechanical perturbation. We discuss the future development of this work in the
context of networked augmented reality metronome devices.","['Aaron D. Likens', 'Spyridon Mastorakis', 'Andreas Skiadopoulos', 'Jenny A. Kent', 'Md Washik Al Azad', 'Nick Stergiou']",2020-11-24T17:31:16Z,http://arxiv.org/abs/2012.00593v1,"['physics.med-ph', 'physics.bio-ph']"
"SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image
  Classifiers","Light-based adversarial attacks use spatial augmented reality (SAR)
techniques to fool image classifiers by altering the physical light condition
with a controllable light source, e.g., a projector. Compared with physical
attacks that place hand-crafted adversarial objects, projector-based ones
obviate modifying the physical entities, and can be performed transiently and
dynamically by altering the projection pattern. However, subtle light
perturbations are insufficient to fool image classifiers, due to the complex
environment and project-and-capture process. Thus, existing approaches focus on
projecting clearly perceptible adversarial patterns, while the more interesting
yet challenging goal, stealthy projector-based attack, remains open. In this
paper, for the first time, we formulate this problem as an end-to-end
differentiable process and propose a Stealthy Projector-based Adversarial
Attack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture
process using a deep neural network named PCNet, then we include PCNet in the
optimization of projector-based attacks such that the generated adversarial
projection is physically plausible. Finally, to generate both robust and
stealthy adversarial projections, we propose an algorithm that uses minimum
perturbation and adversarial confidence thresholds to alternate between the
adversarial loss and stealthiness loss optimization. Our experimental
evaluations show that SPAA clearly outperforms other methods by achieving
higher attack success rates and meanwhile being stealthier, for both targeted
and untargeted attacks.","['Bingyao Huang', 'Haibin Ling']",2020-12-10T18:14:03Z,http://arxiv.org/abs/2012.05858v3,['cs.CV']
GlocalNet: Class-aware Long-term Human Motion Synthesis,"Synthesis of long-term human motion skeleton sequences is essential to aid
human-centric video generation with potential applications in Augmented
Reality, 3D character animations, pedestrian trajectory prediction, etc.
Long-term human motion synthesis is a challenging task due to multiple factors
like, long-term temporal dependencies among poses, cyclic repetition across
poses, bi-directional and multi-scale dependencies among poses, variable speed
of actions, and a large as well as partially overlapping space of temporal pose
variations across multiple class/types of human activities. This paper aims to
address these challenges to synthesize a long-term (> 6000 ms) human motion
trajectory across a large variety of human activity classes (>50). We propose a
two-stage activity generation method to achieve this goal, where the first
stage deals with learning the long-term global pose dependencies in activity
sequences by learning to synthesize a sparse motion trajectory while the second
stage addresses the generation of dense motion trajectories taking the output
of the first stage. We demonstrate the superiority of the proposed method over
SOTA methods using various quantitative evaluation metrics on publicly
available datasets.","['Neeraj Battan', 'Yudhik Agrawal', 'Veeravalli Saisooryarao', 'Aman Goel', 'Avinash Sharma']",2020-12-19T17:50:48Z,http://arxiv.org/abs/2012.10744v1,['cs.CV']
Deep Learning-Based Human Pose Estimation: A Survey,"Human pose estimation aims to locate the human body parts and build human
body representation (e.g., body skeleton) from input data such as images and
videos. It has drawn increasing attention during the past decade and has been
utilized in a wide range of applications including human-computer interaction,
motion analysis, augmented reality, and virtual reality. Although the recently
developed deep learning-based solutions have achieved high performance in human
pose estimation, there still remain challenges due to insufficient training
data, depth ambiguities, and occlusion. The goal of this survey paper is to
provide a comprehensive review of recent deep learning-based solutions for both
2D and 3D pose estimation via a systematic analysis and comparison of these
solutions based on their input data and inference procedures. More than 250
research papers since 2014 are covered in this survey. Furthermore, 2D and 3D
human pose estimation datasets and evaluation metrics are included.
Quantitative performance comparisons of the reviewed methods on popular
datasets are summarized and discussed. Finally, the challenges involved,
applications, and future research directions are concluded. A regularly updated
project page is provided: \url{https://github.com/zczcwh/DL-HPE}","['Ce Zheng', 'Wenhan Wu', 'Chen Chen', 'Taojiannan Yang', 'Sijie Zhu', 'Ju Shen', 'Nasser Kehtarnavaz', 'Mubarak Shah']",2020-12-24T18:49:06Z,http://arxiv.org/abs/2012.13392v5,"['cs.CV', 'cs.GR', 'cs.MM']"
A Hybrid Learner for Simultaneous Localization and Mapping,"Simultaneous localization and mapping (SLAM) is used to predict the dynamic
motion path of a moving platform based on the location coordinates and the
precise mapping of the physical environment. SLAM has great potential in
augmented reality (AR), autonomous vehicles, viz. self-driving cars, drones,
Autonomous navigation robots (ANR). This work introduces a hybrid learning
model that explores beyond feature fusion and conducts a multimodal weight
sewing strategy towards improving the performance of a baseline SLAM algorithm.
It carries out weight enhancement of the front end feature extractor of the
SLAM via mutation of different deep networks' top layers. At the same time, the
trajectory predictions from independently trained models are amalgamated to
refine the location detail. Thus, the integration of the aforesaid early and
late fusion techniques under a hybrid learning framework minimizes the
translation and rotation errors of the SLAM model. This study exploits some
well-known deep learning (DL) architectures, including ResNet18, ResNet34,
ResNet50, ResNet101, VGG16, VGG19, and AlexNet for experimental analysis. An
extensive experimental analysis proves that hybrid learner (HL) achieves
significantly better results than the unimodal approaches and multimodal
approaches with early or late fusion strategies. Hence, it is found that the
Apolloscape dataset taken in this work has never been used in the literature
under SLAM with fusion techniques, which makes this work unique and insightful.","['Thangarajah Akilan', 'Edna Johnson', 'Japneet Sandhu', 'Ritika Chadha', 'Gaurav Taluja']",2021-01-04T18:41:09Z,http://arxiv.org/abs/2101.01158v1,"['cs.RO', 'cs.LG']"
Heatmap-based 2D Landmark Detection with a Varying Number of Landmarks,"Mitral valve repair is a surgery to restore the function of the mitral valve.
To achieve this, a prosthetic ring is sewed onto the mitral annulus. Analyzing
the sutures, which are punctured through the annulus for ring implantation, can
be useful in surgical skill assessment, for quantitative surgery and for
positioning a virtual prosthetic ring model in the scene via augmented reality.
This work presents a neural network approach which detects the sutures in
endoscopic images of mitral valve repair and therefore solves a landmark
detection problem with varying amount of landmarks, as opposed to most other
existing deep learning-based landmark detection approaches. The neural network
is trained separately on two data collections from different domains with the
same architecture and hyperparameter settings. The datasets consist of more
than 1,300 stereo frame pairs each, with a total over 60,000 annotated
landmarks. The proposed heatmap-based neural network achieves a mean positive
predictive value (PPV) of 66.68$\pm$4.67% and a mean true positive rate (TPR)
of 24.45$\pm$5.06% on the intraoperative test dataset and a mean PPV of
81.50\pm5.77\% and a mean TPR of 61.60$\pm$6.11% on a dataset recorded during
surgical simulation. The best detection results are achieved when the camera is
positioned above the mitral valve with good illumination. A detection from a
sideward view is also possible if the mitral valve is well perceptible.","['Antonia Stern', 'Lalith Sharan', 'Gabriele Romano', 'Sven Koehler', 'Matthias Karck', 'Raffaele De Simone', 'Ivo Wolf', 'Sandy Engelhardt']",2021-01-07T19:42:44Z,http://arxiv.org/abs/2101.02737v1,"['cs.CV', 'cs.LG', 'eess.IV']"
Dual chirped micro-comb based parallel ranging at megapixel-line rates,"Laser based ranging (LiDAR) - already ubiquitously used in industrial
monitoring, atmospheric dynamics, or geodesy - is key sensor technology.
Coherent laser ranging, in contrast to time-of-flight approaches, is immune to
ambient light, operates continuous wave allowing higher average powers, and
yields simultaneous velocity and distance information. State-of-the-art
coherent single laser-detector architectures reach hundreds of kilopixel per
second rates. While emerging applications such as autonomous driving, robotics,
and augmented reality mandate megapixel per second point sampling to support
real-time video-rate imaging. Yet, such rates of coherent LiDAR have not been
demonstrated. Here we report a swept dual-soliton microcomb technique enabling
coherent ranging and velocimetry at megapixel per second line scan measurement
rates with up to 64 spectrally dispersed optical channels. It is based on
recent advances in photonic chip-based microcombs that offer a solution to
reduce complexity both on the transmitter and receiver sides.
Multi-heterodyning two synchronously frequency-modulated microcombs yields
distance and velocity information of all individual ranging channels on a
single receiver alleviating the need for individual separation, detection, and
digitization. The reported LiDAR implementation is hardware-efficient,
compatible with photonic integration, and demonstrates the significant
advantages of acquisition speed afforded by the convergence of optical
telecommunication and metrology technologies. We anticipate our research will
motivate further investigation of frequency swept microresonator dual-comb
approach in the neighboring fields of linear and nonlinear spectroscopy,
optical coherence tomography.","['Anton Lukashchuk', 'Johann Riemensberger', 'Maxim Karpov', 'Junqiu Liu', 'Tobias J. Kippenberg']",2021-01-11T15:10:21Z,http://arxiv.org/abs/2101.03952v2,"['physics.optics', 'eess.IV', 'physics.app-ph']"
Boundary-Aware Segmentation Network for Mobile and Web Applications,"Although deep models have greatly improved the accuracy and robustness of
image segmentation, obtaining segmentation results with highly accurate
boundaries and fine structures is still a challenging problem. In this paper,
we propose a simple yet powerful Boundary-Aware Segmentation Network (BASNet),
which comprises a predict-refine architecture and a hybrid loss, for highly
accurate image segmentation. The predict-refine architecture consists of a
densely supervised encoder-decoder network and a residual refinement module,
which are respectively used to predict and refine a segmentation probability
map. The hybrid loss is a combination of the binary cross entropy, structural
similarity and intersection-over-union losses, which guide the network to learn
three-level (ie, pixel-, patch- and map- level) hierarchy representations. We
evaluate our BASNet on two reverse tasks including salient object segmentation,
camouflaged object segmentation, showing that it achieves very competitive
performance with sharp segmentation boundaries. Importantly, BASNet runs at
over 70 fps on a single GPU which benefits many potential real applications.
Based on BASNet, we further developed two (close to) commercial applications:
AR COPY & PASTE, in which BASNet is integrated with augmented reality for
""COPYING"" and ""PASTING"" real-world objects, and OBJECT CUT, which is a
web-based tool for automatic object background removal. Both applications have
already drawn huge amount of attention and have important real-world impacts.
The code and two applications will be publicly available at:
https://github.com/NathanUA/BASNet.","['Xuebin Qin', 'Deng-Ping Fan', 'Chenyang Huang', 'Cyril Diagne', 'Zichen Zhang', ""Adrià Cabeza Sant'Anna"", 'Albert Suàrez', 'Martin Jagersand', 'Ling Shao']",2021-01-12T19:20:26Z,http://arxiv.org/abs/2101.04704v2,['cs.CV']
"TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and
  Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector,
  and Eye Movement Types","We present TEyeD, the world's largest unified public data set of eye images
taken with head-mounted devices. TEyeD was acquired with seven different
head-mounted eye trackers. Among them, two eye trackers were integrated into
virtual reality (VR) or augmented reality (AR) devices. The images in TEyeD
were obtained from various tasks, including car rides, simulator rides, outdoor
sports activities, and daily indoor activities. The data set includes 2D and 3D
landmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and
eye movement types for all images. Landmarks and semantic segmentation are
provided for the pupil, iris and eyelids. Video lengths vary from a few minutes
to several hours. With more than 20 million carefully annotated images, TEyeD
provides a unique, coherent resource and a valuable foundation for advancing
research in the field of computer vision, eye tracking and gaze estimation in
modern VR and AR applications.
  Download: Just connect via FTP as user TEyeDUser and without password to
nephrit.cs.uni-tuebingen.de (ftp://nephrit.cs.uni-tuebingen.de).","['Wolfgang Fuhl', 'Gjergji Kasneci', 'Enkelejda Kasneci']",2021-02-03T15:48:22Z,http://arxiv.org/abs/2102.02115v3,"['eess.IV', 'cs.CV']"
"Millimeter Wave MIMO based Depth Maps for Wireless Virtual and Augmented
  Reality","Augmented and virtual reality systems (AR/VR) are rapidly becoming key
components of the wireless landscape. For immersive AR/VR experience, these
devices should be able to construct accurate depth perception of the
surrounding environment. Current AR/VR devices rely heavily on using RGB-D
depth cameras to achieve this goal. The performance of these depth cameras,
however, has clear limitations in several scenarios, such as the cases with
shiny objects, dark surfaces, and abrupt color transition among other
limitations. In this paper, we propose a novel solution for AR/VR depth map
construction using mmWave MIMO communication transceivers. This is motivated by
the deployment of advanced mmWave communication systems in future AR/VR devices
for meeting the high data rate demands and by the interesting propagation
characteristics of mmWave signals. Accounting for the constraints on these
systems, we develop a comprehensive framework for constructing accurate and
high-resolution depth maps using mmWave systems. In this framework, we
developed new sensing beamforming codebook approaches that are specific for the
depth map construction objective. Using these codebooks, and leveraging tools
from successive interference cancellation, we develop a joint beam processing
approach that can construct high-resolution depth maps using practical mmWave
antenna arrays. Extensive simulation results highlight the potential of the
proposed solution in building accurate depth maps. Further, these simulations
show the promising gains of mmWave based depth perception compared to RGB-based
approaches in several important use cases.","['Abdelrahman Taha', 'Qi Qu', 'Sam Alex', 'Ping Wang', 'William L. Abbott', 'Ahmed Alkhateeb']",2021-02-11T18:57:58Z,http://arxiv.org/abs/2102.06198v2,"['eess.SP', 'cs.IT', 'math.IT']"
Deep Learning Based 3D Segmentation: A Survey,"3D segmentation is a fundamental and challenging problem in computer vision
with applications in autonomous driving, robotics, augmented reality and
medical image analysis. It has received significant attention from the computer
vision, graphics and machine learning communities. Conventional methods for 3D
segmentation, based on hand-crafted features and machine learning classifiers,
lack generalization ability. Driven by their success in 2D computer vision,
deep learning techniques have recently become the tool of choice for 3D
segmentation tasks. This has led to an influx of a large number of methods in
the literature that have been evaluated on different benchmark datasets.
Whereas survey papers on RGB-D and point cloud segmentation exist, there is a
lack of an in-depth and recent survey that covers all 3D data modalities and
application domains. This paper fills the gap and provides a comprehensive
survey of the recent progress made in deep learning based 3D segmentation. It
covers over 180 works, analyzes their strengths and limitations and discusses
their competitive results on benchmark datasets. The survey provides a summary
of the most commonly used pipelines and finally highlights promising research
directions for the future.","['Yong He', 'Hongshan Yu', 'Xiaoyan Liu', 'Zhengeng Yang', 'Wei Sun', 'Ajmal Mian']",2021-03-09T13:58:35Z,http://arxiv.org/abs/2103.05423v3,['cs.CV']
"Simultaneous Multi-View Camera Pose Estimation and Object Tracking with
  Square Planar Markers","Object tracking is a key aspect in many applications such as augmented
reality in medicine (e.g. tracking a surgical instrument) or robotics. Squared
planar markers have become popular tools for tracking since their pose can be
estimated from their four corners. While using a single marker and a single
camera limits the working area considerably, using multiple markers attached to
an object requires estimating their relative position, which is not trivial,
for high accuracy tracking. Likewise, using multiple cameras requires
estimating their extrinsic parameters, also a tedious process that must be
repeated whenever a camera is moved.
  This work proposes a novel method to simultaneously solve the above-mentioned
problems. From a video sequence showing a rigid set of planar markers recorded
from multiple cameras, the proposed method is able to automatically obtain the
three-dimensional configuration of the markers, the extrinsic parameters of the
cameras, and the relative pose between the markers and the cameras at each
frame. Our experiments show that our approach can obtain highly accurate
results for estimating these parameters using low resolution cameras.
  Once the parameters are obtained, tracking of the object can be done in real
time with a low computational cost. The proposed method is a step forward in
the development of cost-effective solutions for object tracking.","['Hamid Sarmadi', 'Rafael Muñoz-Salinas', 'M. A. Berbís', 'R. Medina-Carnicer']",2021-03-16T15:33:58Z,http://arxiv.org/abs/2103.09141v1,['cs.CV']
"Co-Generation and Segmentation for Generalized Surgical Instrument
  Segmentation on Unlabelled Data","Surgical instrument segmentation for robot-assisted surgery is needed for
accurate instrument tracking and augmented reality overlays. Therefore, the
topic has been the subject of a number of recent papers in the CAI community.
Deep learning-based methods have shown state-of-the-art performance for
surgical instrument segmentation, but their results depend on labelled data.
However, labelled surgical data is of limited availability and is a bottleneck
in surgical translation of these methods. In this paper, we demonstrate the
limited generalizability of these methods on different datasets, including
human robot-assisted surgeries. We then propose a novel joint generation and
segmentation strategy to learn a segmentation model with better generalization
capability to domains that have no labelled data. The method leverages the
availability of labelled data in a different domain. The generator does the
domain translation from the labelled domain to the unlabelled domain and
simultaneously, the segmentation model learns using the generated data while
regularizing the generative model. We compared our method with state-of-the-art
methods and showed its generalizability on publicly available datasets and on
our own recorded video frames from robot-assisted prostatectomies. Our method
shows consistently high mean Dice scores on both labelled and unlabelled
domains when data is available only for one of the domains.
  *M. Kalia and T. Aleef contributed equally to the manuscript","['Megha Kalia', 'Tajwar Abrar Aleef', 'Nassir Navab', 'Septimiu E. Salcudean']",2021-03-16T18:41:18Z,http://arxiv.org/abs/2103.09276v1,"['cs.CV', 'cs.LG', 'cs.RO', 'eess.IV']"
"A Novel Deep ML Architecture by Integrating Visual Simultaneous
  Localization and Mapping (vSLAM) into Mask R-CNN for Real-time Surgical Video
  Analysis","Seven million people suffer surgical complications each year, but with
sufficient surgical training and review, 50\% of these complications could be
prevented. To improve surgical performance, existing research uses various deep
learning (DL) technologies including convolutional neural networks (CNN) and
recurrent neural networks (RNN) to automate surgical tool and workflow
detection. However, there is room to improve accuracy; real-time analysis is
also minimal due to the complexity of CNN. In this research, a novel DL
architecture is proposed to integrate visual simultaneous localization and
mapping (vSLAM) into Mask R-CNN. This architecture, vSLAM-CNN (vCNN), for the
first time, integrates the best of both worlds, inclusive of (1) vSLAM for
object detection, by focusing on geometric information for region proposals,
and (2) CNN for object recognition, by focusing on semantic information for
image classification, combining them into one joint end-to-end training
process. This method, using spatio-temporal information in addition to visual
features, is evaluated on M2CAI 2016 challenge datasets, achieving the
state-of-the-art results with 96.8 mAP for tool detection and 97.5 mean Jaccard
score for workflow detection, surpassing all previous works, and reaching a 50
FPS performance, 10x faster than the region-based CNN. A region proposal module
(RPM) replaces the region proposal network (RPN) in Mask R-CNN, accurately
placing bounding boxes and lessening the annotation requirement. Furthermore, a
Microsoft HoloLens 2 application is developed to provide an augmented reality
(AR)-based solution for surgical training and assistance.",['Ella Selina Lan'],2021-03-31T06:59:13Z,http://arxiv.org/abs/2103.16847v3,"['eess.IV', 'cs.CV', 'I.4.0']"
"Demonstrating Cloth Folding to Robots: Design and Evaluation of a 2D and
  a 3D User Interface","An appropriate user interface to collect human demonstration data for
deformable object manipulation has been mostly overlooked in the literature. We
present an interaction design for demonstrating cloth folding to robots. Users
choose pick and place points on the cloth and can preview a visualization of a
simulated cloth before real-robot execution. Two interfaces are proposed: A 2D
display-and-mouse interface where points are placed by clicking on an image of
the cloth, and a 3D Augmented Reality interface where the chosen points are
placed by hand gestures. We conduct a user study with 18 participants, in which
each user completed two sequential folds to achieve a cloth goal shape. Results
show that while both interfaces were acceptable, the 3D interface was found to
be more suitable for understanding the task, and the 2D interface suitable for
repetition. Results also found that fold previews improve three key metrics:
task efficiency, the ability to predict the final shape of the cloth and
overall user satisfaction.","['Benjamin Waymouth', 'Akansel Cosgun', 'Rhys Newbury', 'Tin Tran', 'Wesley P. Chan', 'Tom Drummond', 'Elizabeth Croft']",2021-04-07T07:41:32Z,http://arxiv.org/abs/2104.02968v2,['cs.RO']
"A Perceptual Model for Eccentricity-dependent Spatio-temporal Flicker
  Fusion and its Applications to Foveated Graphics","Virtual and augmented reality (VR/AR) displays strive to provide a
resolution, framerate and field of view that matches the perceptual
capabilities of the human visual system, all while constrained by limited
compute budgets and transmission bandwidths of wearable computing systems.
Foveated graphics techniques have emerged that could achieve these goals by
exploiting the falloff of spatial acuity in the periphery of the visual field.
However, considerably less attention has been given to temporal aspects of
human vision, which also vary across the retina. This is in part due to
limitations of current eccentricity-dependent models of the visual system. We
introduce a new model, experimentally measuring and computationally fitting
eccentricity-dependent critical flicker fusion thresholds jointly for both
space and time. In this way, our model is unique in enabling the prediction of
temporal information that is imperceptible for a certain spatial frequency,
eccentricity, and range of luminance levels. We validate our model with an
image quality user study, and use it to predict potential bandwidth savings 7x
higher than those afforded by current spatial-only foveated models. As such,
this work forms the enabling foundation for new temporally foveated graphics
techniques.","['Brooke Krajancich', 'Petr Kellnhofer', 'Gordon Wetzstein']",2021-04-28T00:51:14Z,http://arxiv.org/abs/2104.13514v5,"['cs.HC', 'cs.GR', 'eess.IV']"
"Digital Twin-Assisted Cooperative Driving at Non-Signalized
  Intersections","Digital Twin, as an emerging technology related to Cyber-Physical Systems
(CPS) and Internet of Things (IoT), has attracted increasing attentions during
the past decade. Conceptually, a Digital Twin is a digital replica of a
physical entity in the real world, and this technology is leveraged in this
study to design a cooperative driving system at non-signalized intersections,
allowing connected vehicles to cooperate with each other to cross intersections
without any full stops. Within the proposed Digital Twin framework, we
developed an enhanced first-in-first-out (FIFO) slot reservation algorithm to
schedule the sequence of crossing vehicles, a consensus motion control
algorithm to calculate vehicles' referenced longitudinal motion, and a
model-based motion estimation algorithm to tackle communication delay and
packet loss. Additionally, an augmented reality (AR) human-machine-interface
(HMI) is designed to provide the guidance to drivers to cooperate with other
connected vehicles. Agent-based modeling and simulation of the proposed system
is conducted in Unity game engine based on a real-world map in San Francisco,
and the human-in-the-loop (HITL) simulation results prove the benefits of the
proposed algorithms with 20% reduction in travel time and 23.7% reduction in
energy consumption, respectively, when compared with traditional signalized
intersections.","['Ziran Wang', 'Kyungtae Han', 'Prashant Tiwari']",2021-05-04T08:26:39Z,http://arxiv.org/abs/2105.01357v1,"['eess.SY', 'cs.HC', 'cs.SY']"
"Evaluating Metrics for Standardized Benchmarking of Remote Presence
  Systems","To reduce the need for business-related air travel and its associated energy
consumption and carbon footprint, the U.S. Department of Energy's ARPA-E is
supporting a research project called SCOTTIE - Systematic Communication
Objectives and Telecommunications Technology Investigations and Evaluations.
SCOTTIE tests virtual and augmented reality platforms in a functional
comparison with face-to-face (FtF) interactions to derive travel replacement
thresholds for common industrial training scenarios. The primary goal of Study
1 is to match the communication effectiveness and learning outcomes obtained
from a FtF control using virtual reality (VR) training scenarios in which a
local expert with physical equipment trains a remote apprentice without
physical equipment immediately present. This application scenario is
commonplace in industrial settings where access to expensive equipment and
materials is limited and a number of apprentices must travel to a central
location in order to undergo training. Supplying an empirically validated
virtual training alternative constitutes a readily adoptable use-case for
businesses looking to reduce time and monetary expenditures associated with
travel. The technology used for three different virtual presence technologies
was strategically selected for feasibility, relatively low cost, business
relevance, and potential for impact through transition. The authors suggest
that the results of this study might generalize to the challenge of virtual
conferences.","['Charles Peasley', 'Rachel Dianiska', 'Emily Oldham', 'Nicholas Wilson', 'Stephen Gilbert', 'Peggy Wu', 'Brett Israelsen', 'James Oliver']",2021-05-04T21:36:53Z,http://arxiv.org/abs/2105.01772v1,"['cs.HC', 'cs.CY']"
"Linking Physical Objects to Their Digital Twins via Fiducial Markers
  Designed for Invisibility to Humans","The ability to label and track physical objects that are assets in digital
representations of the world is foundational to many complex systems. Simple,
yet powerful methods such as bar- and QR-codes have been highly successful,
e.g. in the retail space, but the lack of security, limited information content
and impossibility of seamless integration with the environment have prevented a
large-scale linking of physical objects to their digital twins. This paper
proposes to link digital assets created through BIM with their physical
counterparts using fiducial markers with patterns defined by Cholesteric
Spherical Reflectors (CSRs), selective retroreflectors produced using liquid
crystal self-assembly. The markers leverage the ability of CSRs to encode
information that is easily detected and read with computer vision while
remaining practically invisible to the human eye. We analyze the potential of a
CSR-based infrastructure from the perspective of BIM, critically reviewing the
outstanding challenges in applying this new class of functional materials, and
we discuss extended opportunities arising in assisting autonomous mobile robots
to reliably navigate human-populated environments, as well as in augmented
reality.","['Mathew Schwartz', 'Yong Geng', 'Hakam Agha', 'Rijeesh Kizhakidathazhath', 'Danqing Liu', 'Gabriele Lenzini', 'Jan PF Lagerwall']",2021-05-12T17:09:39Z,http://arxiv.org/abs/2105.05800v1,"['cond-mat.soft', 'cs.SY', 'eess.SY']"
Large-scale Localization Datasets in Crowded Indoor Spaces,"Estimating the precise location of a camera using visual localization enables
interesting applications such as augmented reality or robot navigation. This is
particularly useful in indoor environments where other localization
technologies, such as GNSS, fail. Indoor spaces impose interesting challenges
on visual localization algorithms: occlusions due to people, textureless
surfaces, large viewpoint changes, low light, repetitive textures, etc.
Existing indoor datasets are either comparably small or do only cover a subset
of the mentioned challenges. In this paper, we introduce 5 new indoor datasets
for visual localization in challenging real-world environments. They were
captured in a large shopping mall and a large metro station in Seoul, South
Korea, using a dedicated mapping platform consisting of 10 cameras and 2 laser
scanners. In order to obtain accurate ground truth camera poses, we developed a
robust LiDAR SLAM which provides initial poses that are then refined using a
novel structure-from-motion based optimization. We present a benchmark of
modern visual localization algorithms on these challenging datasets showing
superior performance of structure-based methods using robust image features.
The datasets are available at: https://naverlabs.com/datasets","['Donghwan Lee', 'Soohyun Ryu', 'Suyong Yeon', 'Yonghan Lee', 'Deokhwa Kim', 'Cheolho Han', 'Yohann Cabon', 'Philippe Weinzaepfel', 'Nicolas Guérin', 'Gabriela Csurka', 'Martin Humenberger']",2021-05-19T06:20:49Z,http://arxiv.org/abs/2105.08941v1,['cs.CV']
3D-Aware Ellipse Prediction for Object-Based Camera Pose Estimation,"In this paper, we propose a method for coarse camera pose computation which
is robust to viewing conditions and does not require a detailed model of the
scene. This method meets the growing need of easy deployment of robotics or
augmented reality applications in any environments, especially those for which
no accurate 3D model nor huge amount of ground truth data are available. It
exploits the ability of deep learning techniques to reliably detect objects
regardless of viewing conditions. Previous works have also shown that
abstracting the geometry of a scene of objects by an ellipsoid cloud allows to
compute the camera pose accurately enough for various application needs. Though
promising, these approaches use the ellipses fitted to the detection bounding
boxes as an approximation of the imaged objects. In this paper, we go one step
further and propose a learning-based method which detects improved elliptic
approximations of objects which are coherent with the 3D ellipsoid in terms of
perspective projection. Experiments prove that the accuracy of the computed
pose significantly increases thanks to our method and is more robust to the
variability of the boundaries of the detection boxes. This is achieved with
very little effort in terms of training data acquisition -- a few hundred
calibrated images of which only three need manual object annotation. Code and
models are released at
https://github.com/zinsmatt/3D-Aware-Ellipses-for-Visual-Localization.","['Matthieu Zins', 'Gilles Simon', 'Marie-Odile Berger']",2021-05-24T18:40:18Z,http://arxiv.org/abs/2105.11494v1,"['cs.CV', '65D19', 'I.4']"
"SHD360: A Benchmark Dataset for Salient Human Detection in 360°
  Videos","Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset which contains
various real-life daily scenes. Since so far there is no method proposed for
360{\deg} image/video SHD, we systematically benchmark 11 representative
state-of-the-art salient object detection (SOD) approaches on our SHD360, and
explore key issues derived from extensive experimenting results. We hope our
proposed dataset and benchmark could serve as a good starting point for
advancing human-centric researches towards 360{\deg} panoramic data. The
dataset is available at https://github.com/PanoAsh/SHD360.","['Yi Zhang', 'Lu Zhang', 'Kang Wang', 'Wassim Hamidouche', 'Olivier Deforges']",2021-05-24T23:51:29Z,http://arxiv.org/abs/2105.11578v7,['cs.CV']
Real-time Monocular Depth Estimation with Sparse Supervision on Mobile,"Monocular (relative or metric) depth estimation is a critical task for
various applications, such as autonomous vehicles, augmented reality and image
editing. In recent years, with the increasing availability of mobile devices,
accurate and mobile-friendly depth models have gained importance. Increasingly
accurate models typically require more computational resources, which inhibits
the use of such models on mobile devices. The mobile use case is arguably the
most unrestricted one, which requires highly accurate yet mobile-friendly
architectures. Therefore, we try to answer the following question: How can we
improve a model without adding further complexity (i.e. parameters)? Towards
this end, we systematically explore the design space of a relative depth
estimation model from various dimensions and we show, with key design choices
and ablation studies, even an existing architecture can reach highly
competitive performance to the state of the art, with a fraction of the
complexity. Our study spans an in-depth backbone model selection process,
knowledge distillation, intermediate predictions, model pruning and loss
rebalancing. We show that our model, using only DIW as the supervisory dataset,
achieves 0.1156 WHDR on DIW with 2.6M parameters and reaches 37 FPS on a mobile
GPU, without pruning or hardware-specific optimization. A pruned version of our
model achieves 0.1208 WHDR on DIW with 1M parameters and reaches 44 FPS on a
mobile GPU.","['Mehmet Kerim Yucel', 'Valia Dimaridou', 'Anastasios Drosou', 'Albert Saà-Garriga']",2021-05-25T16:33:28Z,http://arxiv.org/abs/2105.12053v1,['cs.CV']
Multi-Exit Semantic Segmentation Networks,"Semantic segmentation arises as the backbone of many vision systems, spanning
from self-driving cars and robot navigation to augmented reality and
teleconferencing. Frequently operating under stringent latency constraints
within a limited resource envelope, optimising for efficient execution becomes
important. At the same time, the heterogeneous capabilities of the target
platforms and the diverse constraints of different applications require the
design and training of multiple target-specific segmentation models, leading to
excessive maintenance costs. To this end, we propose a framework for converting
state-of-the-art segmentation CNNs to Multi-Exit Semantic Segmentation (MESS)
networks: specially trained models that employ parametrised early exits along
their depth to i) dynamically save computation during inference on easier
samples and ii) save training and maintenance cost by offering a post-training
customisable speed-accuracy trade-off. Designing and training such networks
naively can hurt performance. Thus, we propose a novel two-staged training
scheme for multi-exit networks. Furthermore, the parametrisation of MESS
enables co-optimising the number, placement and architecture of the attached
segmentation heads along with the exit policy, upon deployment via exhaustive
search in <1 GPUh. This allows MESS to rapidly adapt to the device capabilities
and application requirements for each target use-case, offering a
train-once-deploy-everywhere solution. MESS variants achieve latency gains of
up to 2.83x with the same accuracy, or 5.33 pp higher accuracy for the same
computational budget, compared to the original backbone network. Lastly, MESS
delivers orders of magnitude faster architectural customisation, compared to
state-of-the-art techniques.","['Alexandros Kouris', 'Stylianos I. Venieris', 'Stefanos Laskaridis', 'Nicholas D. Lane']",2021-06-07T11:37:03Z,http://arxiv.org/abs/2106.03527v3,"['cs.CV', 'cs.LG']"
"Physics perception in sloshing scenes with guaranteed thermodynamic
  consistency","Physics perception very often faces the problem that only limited data or
partial measurements on the scene are available. In this work, we propose a
strategy to learn the full state of sloshing liquids from measurements of the
free surface. Our approach is based on recurrent neural networks (RNN) that
project the limited information available to a reduced-order manifold so as to
not only reconstruct the unknown information, but also to be capable of
performing fluid reasoning about future scenarios in real time. To obtain
physically consistent predictions, we train deep neural networks on the
reduced-order manifold that, through the employ of inductive biases, ensure the
fulfillment of the principles of thermodynamics. RNNs learn from history the
required hidden information to correlate the limited information with the
latent space where the simulation occurs. Finally, a decoder returns data back
to the high-dimensional manifold, so as to provide the user with insightful
information in the form of augmented reality. This algorithm is connected to a
computer vision system to test the performance of the proposed methodology with
real information, resulting in a system capable of understanding and predicting
future states of the observed fluid in real-time.","['Beatriz Moya', 'Alberto Badias', 'David Gonzalez', 'Francisco Chinesta', 'Elias Cueto']",2021-06-24T20:13:56Z,http://arxiv.org/abs/2106.13301v3,"['cs.CV', 'cs.LG']"
"Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time
  Delays Using Reinforcement Learning","Telerobotic systems must adapt to new environmental conditions and deal with
high uncertainty caused by long-time delays. As one of the best alternatives to
human-level intelligence, Reinforcement Learning (RL) may offer a solution to
cope with these issues. This paper proposes to integrate RL with the Model
Mediated Teleoperation (MMT) concept. The teleoperator interacts with a
simulated virtual environment, which provides instant feedback. Whereas
feedback from the real environment is delayed, feedback from the model is
instantaneous, leading to high transparency. The MMT is realized in combination
with an intelligent system with two layers. The first layer utilizes Dynamic
Movement Primitives (DMP) which accounts for certain changes in the avatar
environment. And, the second layer addresses the problems caused by uncertainty
in the model using RL methods. Augmented reality was also provided to fuse the
avatar device and virtual environment models for the teleoperator. Implemented
on DLR's Exodex Adam hand-arm haptic exoskeleton, the results show RL methods
are able to find different solutions when changes are applied to the object
position after the demonstration. The results also show DMPs to be effective at
adapting to new conditions where there is no uncertainty involved.","['Hadi Beik-Mohammadi', 'Matthias Kerzel', 'Benedikt Pleintinger', 'Thomas Hulin', 'Philipp Reisich', 'Annika Schmidt', 'Aaron Pereira', 'Stefan Wermter', 'Neal Y. Lii']",2021-07-01T10:49:55Z,http://arxiv.org/abs/2107.00359v1,"['cs.RO', 'cs.AI', 'cs.LG']"
"FloorLevel-Net: Recognizing Floor-Level Lines with
  Height-Attention-Guided Multi-task Learning","The ability to recognize the position and order of the floor-level lines that
divide adjacent building floors can benefit many applications, for example,
urban augmented reality (AR). This work tackles the problem of locating
floor-level lines in street-view images, using a supervised deep learning
approach. Unfortunately, very little data is available for training such a
network $-$ current street-view datasets contain either semantic annotations
that lack geometric attributes, or rectified facades without perspective
priors. To address this issue, we first compile a new dataset and develop a new
data augmentation scheme to synthesize training samples by harassing (i) the
rich semantics of existing rectified facades and (ii) perspective priors of
buildings in diverse street views. Next, we design FloorLevel-Net, a multi-task
learning network that associates explicit features of building facades and
implicit floor-level lines, along with a height-attention mechanism to help
enforce a vertical ordering of floor-level lines. The generated segmentations
are then passed to a second-stage geometry post-processing to exploit
self-constrained geometric priors for plausible and consistent reconstruction
of floor-level lines. Quantitative and qualitative evaluations conducted on
assorted facades in existing datasets and street views from Google demonstrate
the effectiveness of our approach. Also, we present context-aware image overlay
results and show the potentials of our approach in enriching AR-related
applications.","['Mengyang Wu', 'Wei Zeng', 'Chi-Wing Fu']",2021-07-06T08:17:59Z,http://arxiv.org/abs/2107.02462v1,['cs.CV']
"SurgeonAssist-Net: Towards Context-Aware Head-Mounted Display-Based
  Augmented Reality for Surgical Guidance","We present SurgeonAssist-Net: a lightweight framework making
action-and-workflow-driven virtual assistance, for a set of predefined surgical
tasks, accessible to commercially available optical see-through head-mounted
displays (OST-HMDs). On a widely used benchmark dataset for laparoscopic
surgical workflow, our implementation competes with state-of-the-art approaches
in prediction accuracy for automated task recognition, and yet requires 7.4x
fewer parameters, 10.2x fewer floating point operations per second (FLOPS), is
7.0x faster for inference on a CPU, and is capable of near real-time
performance on the Microsoft HoloLens 2 OST-HMD. To achieve this, we make use
of an efficient convolutional neural network (CNN) backbone to extract
discriminative features from image data, and a low-parameter recurrent neural
network (RNN) architecture to learn long-term temporal dependencies. To
demonstrate the feasibility of our approach for inference on the HoloLens 2 we
created a sample dataset that included video of several surgical tasks recorded
from a user-centric point-of-view. After training, we deployed our model and
cataloged its performance in an online simulated surgical scenario for the
prediction of the current surgical task. The utility of our approach is
explored in the discussion of several relevant clinical use-cases. Our code is
publicly available at https://github.com/doughtmw/surgeon-assist-net.","['Mitchell Doughty', 'Karan Singh', 'Nilesh R. Ghugre']",2021-07-13T21:12:34Z,http://arxiv.org/abs/2107.06397v1,['cs.CV']
"CodeMapping: Real-Time Dense Mapping for Sparse SLAM using Compact Scene
  Representations","We propose a novel dense mapping framework for sparse visual SLAM systems
which leverages a compact scene representation. State-of-the-art sparse visual
SLAM systems provide accurate and reliable estimates of the camera trajectory
and locations of landmarks. While these sparse maps are useful for
localization, they cannot be used for other tasks such as obstacle avoidance or
scene understanding. In this paper we propose a dense mapping framework to
complement sparse visual SLAM systems which takes as input the camera poses,
keyframes and sparse points produced by the SLAM system and predicts a dense
depth image for every keyframe. We build on CodeSLAM and use a variational
autoencoder (VAE) which is conditioned on intensity, sparse depth and
reprojection error images from sparse SLAM to predict an uncertainty-aware
dense depth map. The use of a VAE then enables us to refine the dense depth
images through multi-view optimization which improves the consistency of
overlapping frames. Our mapper runs in a separate thread in parallel to the
SLAM system in a loosely coupled manner. This flexible design allows for
integration with arbitrary metric sparse SLAM systems without delaying the main
SLAM process. Our dense mapper can be used not only for local mapping but also
globally consistent dense 3D reconstruction through TSDF fusion. We demonstrate
our system running with ORB-SLAM3 and show accurate dense depth estimation
which could enable applications such as robotics and augmented reality.","['Hidenobu Matsuki', 'Raluca Scona', 'Jan Czarnowski', 'Andrew J. Davison']",2021-07-19T16:13:18Z,http://arxiv.org/abs/2107.08994v1,"['cs.CV', 'cs.RO']"
Thermally-reconfigurable metalens,"Thanks to the compact design and multi-functional light-manipulation
capabilities, reconfigurable metalenses, which consist of arrays of
sub-wavelength meta-atoms, offer unique opportunities for advanced optical
systems, from microscopy to augmented reality platforms. Although poorly
explored in the context of reconfigurable metalens, thermo-optical effects in
resonant silicon nanoresonators have recently emerged as a viable strategy to
realize tunable meta-atoms. In this work, we report the proof-of-concept design
of an ultrathin (300 nm thick) and thermo-optically reconfigurable silicon
metalens operating at a fixed, visible wavelength (632 nm). Importantly, we
demonstrate continuous, linear modulation of the focal-length up to 21% (from
165 $\mu$m at 20$\deg$C to 135 $\mu$m at 260$\deg$C). Operating under
right-circularly polarized light, our metalens exhibits an average conversion
efficiency of 26%, close to mechanically modulated devices, and has a
diffraction-limited performance. Overall, we envision that, combined with
machine-learning algorithms for further optimization of the meta-atoms,
thermally-reconfigurable metalenses with improved performance will be possible.
Also, the generality of this approach could offer inspiration for the
realization of active metasurfaces with other emerging material within field of
thermo-nanophotonics.","['Anna Archetti', 'Ren-Jie Lin', 'Nathanaël Restori', 'Fatemeh Kiani', 'Ted V. Tsoulos', 'Giulia Tagliabue']",2021-07-22T06:28:18Z,http://arxiv.org/abs/2107.10475v1,"['physics.optics', 'physics.app-ph']"
Resource Efficient Mountainous Skyline Extraction using Shallow Learning,"Skyline plays a pivotal role in mountainous visual geo-localization and
localization/navigation of planetary rovers/UAVs and virtual/augmented reality
applications. We present a novel mountainous skyline detection approach where
we adapt a shallow learning approach to learn a set of filters to discriminate
between edges belonging to sky-mountain boundary and others coming from
different regions. Unlike earlier approaches, which either rely on extraction
of explicit feature descriptors and their classification, or fine-tuning
general scene parsing deep networks for sky segmentation, our approach learns
linear filters based on local structure analysis. At test time, for every
candidate edge pixel, a single filter is chosen from the set of learned filters
based on pixel's structure tensor, and then applied to the patch around it. We
then employ dynamic programming to solve the shortest path problem for the
resultant multistage graph to get the sky-mountain boundary. The proposed
approach is computationally faster than earlier methods while providing
comparable performance and is more suitable for resource constrained platforms
e.g., mobile devices, planetary rovers and UAVs. We compare our proposed
approach against earlier skyline detection methods using four different data
sets. Our code is available at
\url{https://github.com/TouqeerAhmad/skyline_detection}.","['Touqeer Ahmad', 'Ebrahim Emami', 'Martin Čadík', 'George Bebis']",2021-07-23T02:14:17Z,http://arxiv.org/abs/2107.10997v1,"['cs.CV', 'cs.AI', 'cs.RO', 'I.4.6']"
"80-degree field-of-view transmissive metasurface-based spatial light
  modulator","Compact, lightweight and high-performance spatial light modulators (SLMs) are
crucial for modern optical technologies. The drive for pixel miniaturization,
necessary to improve their performance, has led to a promising alternative,
active optical metasurfaces, which enable tunable subwavelength wavefront
manipulation. Here, we demonstrate an all-solid-state programmable transmissive
SLM device based on Huygens dielectric metasurfaces. The metasurface features
electrical tunability, provided by mature liquid crystals (LCs) technology. In
contrast to conventional LC SLMs, our device enables high resolution with a
pixel size of ~1 um. We demonstrate its performance by realizing programmable
beam steering, which exhibits high side mode suppression ratio of ~6 dB. By
complementing the device with a 3D printed doublet microlens, fabricated using
two-photon polymerization, we enhance the field of view up to ~80 degrees. The
developed prototype paves the way to compact, efficient and multifunctional
devices for next generation augmented reality displays, light detection and
ranging (LiDAR) systems and optical computing.","['Anton V. Baranikov', 'Shi-Qiang Li', 'Damien Eschimese', 'Xuewu Xu', 'Simon Thiele', 'Simon Ristok', 'Rasna Maruthiyodan Veetil', 'Tobias W. W. Mass', 'Parikshit Moitra', 'Harald Giessen', 'Ramon Paniagua-Dominguez', 'Arseniy I. Kuznetsov']",2021-07-23T09:37:32Z,http://arxiv.org/abs/2107.11096v1,"['physics.optics', 'physics.app-ph']"
"ASOD60K: An Audio-Induced Salient Object Detection Dataset for Panoramic
  Videos","Exploring to what humans pay attention in dynamic panoramic scenes is useful
for many fundamental applications, including augmented reality (AR) in retail,
AR-powered recruitment, and visual language navigation. With this goal in mind,
we propose PV-SOD, a new task that aims to segment salient objects from
panoramic videos. In contrast to existing fixation-/object-level saliency
detection tasks, we focus on audio-induced salient object detection (SOD),
where the salient objects are labeled with the guidance of audio-induced eye
movements. To support this task, we collect the first large-scale dataset,
named ASOD60K, which contains 4K-resolution video frames annotated with a
six-level hierarchy, thus distinguishing itself with richness, diversity and
quality. Specifically, each sequence is marked with both its super-/sub-class,
with objects of each sub-class being further annotated with human eye
fixations, bounding boxes, object-/instance-level masks, and associated
attributes (e.g., geometrical distortion). These coarse-to-fine annotations
enable detailed analysis for PV-SOD modelling, e.g., determining the major
challenges for existing SOD models, and predicting scanpaths to study the
long-term eye fixation behaviors of humans. We systematically benchmark 11
representative approaches on ASOD60K and derive several interesting findings.
We hope this study could serve as a good starting point for advancing SOD
research towards panoramic videos. The dataset and benchmark will be made
publicly available at https://github.com/PanoAsh/ASOD60K.",['Yi Zhang'],2021-07-24T15:14:20Z,http://arxiv.org/abs/2107.11629v4,['cs.CV']
VIPose: Real-time Visual-Inertial 6D Object Pose Tracking,"Estimating the 6D pose of objects is beneficial for robotics tasks such as
transportation, autonomous navigation, manipulation as well as in scenarios
beyond robotics like virtual and augmented reality. With respect to single
image pose estimation, pose tracking takes into account the temporal
information across multiple frames to overcome possible detection
inconsistencies and to improve the pose estimation efficiency. In this work, we
introduce a novel Deep Neural Network (DNN) called VIPose, that combines
inertial and camera data to address the object pose tracking problem in
real-time. The key contribution is the design of a novel DNN architecture which
fuses visual and inertial features to predict the objects' relative 6D pose
between consecutive image frames. The overall 6D pose is then estimated by
consecutively combining relative poses. Our approach shows remarkable pose
estimation results for heavily occluded objects that are well known to be very
challenging to handle by existing state-of-the-art solutions. The effectiveness
of the proposed approach is validated on a new dataset called VIYCB with RGB
image, IMU data, and accurate 6D pose annotations created by employing an
automated labeling technique. The approach presents accuracy performances
comparable to state-of-the-art techniques, but with the additional benefit of
being real-time.","['Rundong Ge', 'Giuseppe Loianno']",2021-07-27T06:10:23Z,http://arxiv.org/abs/2107.12617v2,"['cs.RO', 'cs.CV']"
Predicting the Future from First Person (Egocentric) Vision: A Survey,"Egocentric videos can bring a lot of information about how humans perceive
the world and interact with the environment, which can be beneficial for the
analysis of human behaviour. The research in egocentric video analysis is
developing rapidly thanks to the increasing availability of wearable devices
and the opportunities offered by new large-scale egocentric datasets. As
computer vision techniques continue to develop at an increasing pace, the tasks
related to the prediction of future are starting to evolve from the need of
understanding the present. Predicting future human activities, trajectories and
interactions with objects is crucial in applications such as human-robot
interaction, assistive wearable technologies for both industrial and daily
living scenarios, entertainment and virtual or augmented reality. This survey
summarises the evolution of studies in the context of future prediction from
egocentric vision making an overview of applications, devices, existing
problems, commonly used datasets, models and input modalities. Our analysis
highlights that methods for future prediction from egocentric vision can have a
significant impact in a range of applications and that further research efforts
should be devoted to the standardisation of tasks and the proposal of datasets
considering real-world scenarios such as the ones with an industrial vocation.","['Ivan Rodin', 'Antonino Furnari', 'Dimitrios Mavroedis', 'Giovanni Maria Farinella']",2021-07-28T14:58:13Z,http://arxiv.org/abs/2107.13411v1,['cs.CV']
"Deep Feature Tracker: A Novel Application for Deep Convolutional Neural
  Networks","Feature tracking is the building block of many applications such as visual
odometry, augmented reality, and target tracking. Unfortunately, the
state-of-the-art vision-based tracking algorithms fail in surgical images due
to the challenges imposed by the nature of such environments. In this paper, we
proposed a novel and unified deep learning-based approach that can learn how to
track features reliably as well as learn how to detect such reliable features
for tracking purposes. The proposed network dubbed as Deep-PT, consists of a
tracker network which is a convolutional neural network simulating
cross-correlation in terms of deep learning and two fully connected networks
that operate on the output of intermediate layers of the tracker to detect
features and predict trackability of the detected points. The ability to detect
features based on the capabilities of the tracker distinguishes the proposed
method from previous algorithms used in this area and improves the robustness
of the algorithms against dynamics of the scene. The network is trained using
multiple datasets due to the lack of specialized dataset for feature tracking
datasets and extensive comparisons are conducted to compare the accuracy of
Deep-PT against recent pixel tracking algorithms. As the experiments suggest,
the proposed deep architecture deliberately learns what to track and how to
track and outperforms the state-of-the-art methods.","['Mostafa Parchami', 'Saif Iftekar Sayed']",2021-07-30T23:24:29Z,http://arxiv.org/abs/2108.00105v1,"['cs.CV', 'cs.LG', 'cs.RO']"
Deep Image-based Illumination Harmonization,"Integrating a foreground object into a background scene with illumination
harmonization is an important but challenging task in computer vision and
augmented reality community. Existing methods mainly focus on foreground and
background appearance consistency or the foreground object shadow generation,
which rarely consider global appearance and illumination harmonization. In this
paper, we formulate seamless illumination harmonization as an illumination
exchange and aggregation problem. Specifically, we firstly apply a
physically-based rendering method to construct a large-scale, high-quality
dataset (named IH) for our task, which contains various types of foreground
objects and background scenes with different lighting conditions. Then, we
propose a deep image-based illumination harmonization GAN framework named
DIH-GAN, which makes full use of a multi-scale attention mechanism and
illumination exchange strategy to directly infer mapping relationship between
the inserted foreground object and the corresponding background scene.
Meanwhile, we also use adversarial learning strategy to further refine the
illumination harmonization result. Our method can not only achieve harmonious
appearance and illumination for the foreground object but also can generate
compelling shadow cast by the foreground object. Comprehensive experiments on
both our IH dataset and real-world images show that our proposed DIH-GAN
provides a practical and effective solution for image-based object illumination
harmonization editing, and validate the superiority of our method against
state-of-the-art methods. Our IH dataset is available at
https://github.com/zhongyunbao/Dataset.","['Zhongyun Bao', 'Chengjiang Long', 'Gang Fu', 'Daquan Liu', 'Yuanzhen Li', 'Jiaming Wu', 'Chunxia Xiao']",2021-07-31T05:02:52Z,http://arxiv.org/abs/2108.00150v2,['cs.CV']
Investigating Attention Mechanism in 3D Point Cloud Object Detection,"Object detection in three-dimensional (3D) space attracts much interest from
academia and industry since it is an essential task in AI-driven applications
such as robotics, autonomous driving, and augmented reality. As the basic
format of 3D data, the point cloud can provide detailed geometric information
about the objects in the original 3D space. However, due to 3D data's sparsity
and unorderedness, specially designed networks and modules are needed to
process this type of data. Attention mechanism has achieved impressive
performance in diverse computer vision tasks; however, it is unclear how
attention modules would affect the performance of 3D point cloud object
detection and what sort of attention modules could fit with the inherent
properties of 3D data. This work investigates the role of the attention
mechanism in 3D point cloud object detection and provides insights into the
potential of different attention modules. To achieve that, we comprehensively
investigate classical 2D attentions, novel 3D attentions, including the latest
point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the
detailed experiments and analysis, we conclude the effects of different
attention modules. This paper is expected to serve as a reference source for
benefiting attention-embedded 3D point cloud object detection. The code and
trained models are available at:
https://github.com/ShiQiu0419/attentions_in_3D_detection.","['Shi Qiu', 'Yunfan Wu', 'Saeed Anwar', 'Chongyi Li']",2021-08-02T03:54:39Z,http://arxiv.org/abs/2108.00620v2,['cs.CV']
Object Wake-up: 3D Object Rigging from a Single Image,"Given a single image of a general object such as a chair, could we also
restore its articulated 3D shape similar to human modeling, so as to animate
its plausible articulations and diverse motions? This is an interesting new
question that may have numerous downstream augmented reality and virtual
reality applications. Comparing with previous efforts on object manipulation,
our work goes beyond 2D manipulation and rigid deformation, and involves
articulated manipulation. To achieve this goal, we propose an automated
approach to build such 3D generic objects from single images and embed
articulated skeletons in them. Specifically, our framework starts by
reconstructing the 3D object from an input image. Afterwards, to extract
skeletons for generic 3D objects, we develop a novel skeleton prediction method
with a multi-head structure for skeleton probability field estimation by
utilizing the deep implicit functions. A dataset of generic 3D objects with
ground-truth annotated skeletons is collected. Empirically our approach is
demonstrated with satisfactory performance on public datasets as well as our
in-house dataset; our results surpass those of the state-of-the-arts by a
noticeable margin on both 3D reconstruction and skeleton prediction.","['Ji Yang', 'Xinxin Zuo', 'Sen Wang', 'Zhenbo Yu', 'Xingyu Li', 'Bingbing Ni', 'Minglun Gong', 'Li Cheng']",2021-08-05T16:20:12Z,http://arxiv.org/abs/2108.02708v3,['cs.CV']
Automatic Gaze Analysis: A Survey of Deep Learning based Approaches,"Eye gaze analysis is an important research problem in the field of Computer
Vision and Human-Computer Interaction. Even with notable progress in the last
10 years, automatic gaze analysis still remains challenging due to the
uniqueness of eye appearance, eye-head interplay, occlusion, image quality, and
illumination conditions. There are several open questions, including what are
the important cues to interpret gaze direction in an unconstrained environment
without prior knowledge and how to encode them in real-time. We review the
progress across a range of gaze analysis tasks and applications to elucidate
these fundamental questions, identify effective methods in gaze analysis, and
provide possible future directions. We analyze recent gaze estimation and
segmentation methods, especially in the unsupervised and weakly supervised
domain, based on their advantages and reported evaluation metrics. Our analysis
shows that the development of a robust and generic gaze analysis method still
needs to address real-world challenges such as unconstrained setup and learning
with less supervision. We conclude by discussing future research directions for
designing a real-world gaze analysis system that can propagate to other domains
including Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and
Human Computer Interaction (HCI). Project Page:
https://github.com/i-am-shreya/EyeGazeSurvey}{https://github.com/i-am-shreya/EyeGazeSurvey","['Shreya Ghosh', 'Abhinav Dhall', 'Munawar Hayat', 'Jarrod Knibbe', 'Qiang Ji']",2021-08-12T00:30:39Z,http://arxiv.org/abs/2108.05479v3,['cs.CV']
"VR Sickness Prediction from Integrated HMD's Sensors using Multimodal
  Deep Fusion Network","Virtual Reality (VR) sickness commonly known as cybersickness is one of the
major problems for the comfortable use of VR systems. Researchers have proposed
different approaches for predicting cybersickness from bio-physiological data
(e.g., heart rate, breathing rate, electroencephalogram). However, collecting
bio-physiological data often requires external sensors, limiting locomotion and
3D-object manipulation during the virtual reality (VR) experience. Limited
research has been done to predict cybersickness from the data readily available
from the integrated sensors in head-mounted displays (HMDs) (e.g.,
head-tracking, eye-tracking, motion features), allowing free locomotion and
3D-object manipulation. This research proposes a novel deep fusion network to
predict cybersickness severity from heterogeneous data readily available from
the integrated HMD sensors. We extracted 1755 stereoscopic videos,
eye-tracking, and head-tracking data along with the corresponding self-reported
cybersickness severity collected from 30 participants during their VR gameplay.
We applied several deep fusion approaches with the heterogeneous data collected
from the participants. Our results suggest that cybersickness can be predicted
with an accuracy of 87.77\% and a root-mean-square error of 0.51 when using
only eye-tracking and head-tracking data. We concluded that eye-tracking and
head-tracking data are well suited for a standalone cybersickness prediction
framework.","['Rifatul Islam', 'Kevin Desai', 'John Quarles']",2021-08-14T01:28:15Z,http://arxiv.org/abs/2108.06437v1,['cs.HC']
"Category-Level 6D Object Pose Estimation via Cascaded Relation and
  Recurrent Reconstruction Networks","Category-level 6D pose estimation, aiming to predict the location and
orientation of unseen object instances, is fundamental to many scenarios such
as robotic manipulation and augmented reality, yet still remains unsolved.
Precisely recovering instance 3D model in the canonical space and accurately
matching it with the observation is an essential point when estimating 6D pose
for unseen objects. In this paper, we achieve accurate category-level 6D pose
estimation via cascaded relation and recurrent reconstruction networks.
Specifically, a novel cascaded relation network is dedicated for advanced
representation learning to explore the complex and informative relations among
instance RGB image, instance point cloud and category shape prior. Furthermore,
we design a recurrent reconstruction network for iterative residual refinement
to progressively improve the reconstruction and correspondence estimations from
coarse to fine. Finally, the instance 6D pose is obtained leveraging the
estimated dense correspondences between the instance point cloud and the
reconstructed 3D model in the canonical space. We have conducted extensive
experiments on two well-acknowledged benchmarks of category-level 6D pose
estimation, with significant performance improvement over existing approaches.
On the representatively strict evaluation metrics of $3D_{75}$ and $5^{\circ}2
cm$, our method exceeds the latest state-of-the-art SPD by $4.9\%$ and $17.7\%$
on the CAMERA25 dataset, and by $2.7\%$ and $8.5\%$ on the REAL275 dataset.
Codes are available at https://wangjiaze.cn/projects/6DPoseEstimation.html.","['Jiaze Wang', 'Kai Chen', 'Qi Dou']",2021-08-19T15:46:52Z,http://arxiv.org/abs/2108.08755v1,['cs.CV']
"A Multiple-View Geometric Model for Specularity Prediction on General
  Curved Surfaces","Specularity prediction is essential to many computer vision applications,
giving important visual cues usable in Augmented Reality (AR), Simultaneous
Localisation and Mapping (SLAM), 3D reconstruction and material modeling.
However, it is a challenging task requiring numerous information from the scene
including the camera pose, the geometry of the scene, the light sources and the
material properties. Our previous work addressed this task by creating an
explicit model using an ellipsoid whose projection fits the specularity image
contours for a given camera pose. These ellipsoid-based approaches belong to a
family of models called JOint-LIght MAterial Specularity (JOLIMAS), which we
have gradually improved by removing assumptions on the scene geometry. However,
our most recent approach is still limited to uniformly curved surfaces. This
paper generalises JOLIMAS to any surface geometry while improving the quality
of specularity prediction, without sacrificing computation performances. The
proposed method establishes a link between surface curvature and specularity
shape in order to lift the geometric assumptions made in previous work.
Contrary to previous work, our new model is built from a physics-based local
illumination model namely Torrance-Sparrow, providing an improved
reconstruction. Specularity prediction using our new model is tested against
the most recent JOLIMAS version on both synthetic and real sequences with
objects of various general shapes. Our method outperforms previous approaches
in specularity prediction, including the real-time setup, as shown in the
supplementary videos.","['Alexandre Morgand', 'Mohamed Tamaazousti', 'Adrien Bartoli']",2021-08-20T21:21:26Z,http://arxiv.org/abs/2108.09378v2,"['cs.CV', 'cs.GR']"
Direct thermal infrared vision via nanophotonic detector design,"Detection of infrared (IR) photons in a room-temperature IR camera is carried
out by a two-dimensional array of microbolometer pixels which exhibit
temperature-sensitive resistivity. When IR light coming from the far-field is
focused onto this array, microbolometer pixels are heated up in proportion to
the temperatures of the far-field objects. The resulting resistivity change of
each pixel is measured via on-chip electronic readout circuit followed by
analog to digital (A/D) conversion, image processing, and presentation of the
final IR image on a separate information display screen. In this work, we
introduce a new nanophotonic detector as a minimalist alternative to
microbolometer such that the final IR image can be presented without using the
components required for A/D conversion, image processing and display. In our
design, the detector array is illuminated with visible laser light and the
reflected light itself carries the IR image which can be directly viewed. We
realize and numerically demonstrate this functionality using a resonant
waveguide grating structure made of typical materials such as silicon carbide,
silicon nitride, and silica for which lithography techniques are
well-developed. We clarify the requirements to tackle the issues of fabrication
nonuniformities and temperature drifts in the detector array. We envision a
potential near-eye display device for IR vision based on timely use of
diffractive optical waveguides in augmented reality headsets and tunable
visible laser sources. Our work indicates a way to achieve direct thermal IR
vision for suitable use cases with lower cost, smaller form factor, and reduced
power consumption compared to the existing thermal IR cameras.","['Chinmay Khandekar', 'Weiliang Jin', 'Shanhui Fan']",2021-08-26T05:28:05Z,http://arxiv.org/abs/2108.11583v1,"['physics.optics', 'cond-mat.mes-hall', 'physics.app-ph']"
"Neural Étendue Expander for Ultra-Wide-Angle High-Fidelity
  Holographic Display","Holographic displays can generate light fields by dynamically modulating the
wavefront of a coherent beam of light using a spatial light modulator,
promising rich virtual and augmented reality applications. However, the limited
spatial resolution of existing dynamic spatial light modulators imposes a tight
bound on the diffraction angle. As a result, modern holographic displays
possess low \'{e}tendue, which is the product of the display area and the
maximum solid angle of diffracted light. The low \'{e}tendue forces a sacrifice
of either the field-of-view (FOV) or the display size. In this work, we lift
this limitation by presenting neural \'{e}tendue expanders. This new breed of
optical elements, which is learned from a natural image dataset, enables higher
diffraction angles for ultra-wide FOV while maintaining both a compact form
factor and the fidelity of displayed contents to human viewers. With neural
\'{e}tendue expanders, we experimentally achieve 64$\times$ \'{e}tendue
expansion of natural images in full color, expanding the FOV by an order of
magnitude horizontally and vertically, with high-fidelity reconstruction
quality (measured in PSNR) over 29 dB on retinal-resolution images.","['Ethan Tseng', 'Grace Kuo', 'Seung-Hwan Baek', 'Nathan Matsuda', 'Andrew Maimone', 'Florian Schiffers', 'Praneeth Chakravarthula', 'Qiang Fu', 'Wolfgang Heidrich', 'Douglas Lanman', 'Felix Heide']",2021-09-16T17:21:52Z,http://arxiv.org/abs/2109.08123v4,"['eess.IV', 'cs.CV', 'physics.optics']"
"Architecture and Performance Evaluation of Distributed Computation
  Offloading in Edge Computing","Edge computing is an emerging paradigm to enable low-latency applications,
like mobile augmented reality, because it takes the computation on processing
devices that are closer to the users. On the other hand, the need for highly
scalable execution of stateless tasks for cloud systems is driving the
definition of new technologies based on serverless computing. In this paper, we
propose a novel architecture where the two converge to enable low-latency
applications: this is achieved by offloading short-lived stateless tasks from
the user terminals to edge nodes. Furthermore, we design a distributed
algorithm that tackles the research challenge of selecting the best executor,
based on real-time measurements and simple, yet effective, prediction
algorithms. Finally, we describe a new performance evaluation framework
specifically designed for an accurate assessment of algorithms and protocols in
edge computing environments, where the nodes may have very heterogeneous
networking and processing capabilities. The proposed framework relies on the
use of real components on lightweight virtualization mixed with simulated
computation and is well-suited to the analysis of several applications and
network environments. Using our framework, we evaluate our proposed
architecture and algorithms in small- and large-scale edge computing scenarios,
showing that our solution achieves similar or better delay performance than a
centralized solution, with far less network utilization.","['Claudio Cicconetti', 'Marco Conti', 'Andrea Passarella']",2021-09-20T10:31:32Z,http://arxiv.org/abs/2109.09415v1,"['cs.DC', 'cs.NI']"
"The Per-Tau Shell: A Giant Star-Forming Spherical Shell Revealed by 3D
  Dust Observations","A major question in the field of star formation is how molecular clouds form
out of the diffuse Interstellar Medium (ISM). Recent advances in 3D dust
mapping are revolutionizing our view of the structure of the ISM. Using the
highest-resolution 3D dust map to date, we explore the structure of a nearby
star-forming region, which includes the well-known Perseus and Taurus molecular
clouds. We reveal an extended near-spherical shell, 156 pc in diameter,
hereafter the ""Per-Tau Shell"", in which the Perseus and Taurus clouds are
embedded. We also find a large ring structure at the location of Taurus,
hereafter, the ""Tau Ring"". We discuss a formation scenario for the Per-Tau
Shell, in which previous stellar and supernova (SN) feedback events formed a
large expanding shell, where the swept-up ISM has condensed to form both the
shell and the Perseus and Taurus molecular clouds within it. We present
auxiliary observations of HI, H$\alpha$, $^{26}$Al, and X-rays that further
support this scenario, and estimate Per-Tau Shell's age to be $\approx 6-22$
Myrs. The Per-Tau shell offers the first three-dimensional observational view
of a phenomenon long-hypothesized theoretically, molecular cloud formation and
star formation triggered by previous stellar and SN feedback.","['Shmuel Bialy', 'Catherine Zucker', 'Alyssa Goodman', 'Michael M. Foley', 'João Alves', 'Vadim A. Semenov', 'Robert Benjamin', 'Reimar Leike', 'Torsten Enßlin']",2021-09-20T18:00:12Z,http://arxiv.org/abs/2109.09763v1,['astro-ph.GA']
"Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations
  Using Deep Spatio-Temporal Graph CNNs","Several applications such as autonomous driving, augmented reality and
virtual reality require a precise prediction of the 3D human pose. Recently, a
new problem was introduced in the field to predict the 3D human poses from
observed 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN
model that predicts the future 3D skeleton poses in a single pass from the 2D
ones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction
between the skeleton joints by exploiting their spatial configuration. This is
being achieved by formulating the problem as a graph structure while learning a
suitable graph adjacency kernel. By the design, Skeleton-Graph predicts the
future 3D poses without divergence in the long-term, unlike prior works. We
also introduce a new metric that measures the divergence of predictions in the
long term. Our results show an FDE improvement of at least 27% and an ADE of 4%
on both the GTA-IM and PROX datasets respectively in comparison with prior
works. Also, we are 88% and 93% less divergence on the long-term motion
prediction in comparison with prior works on both GTA-IM and PROX datasets.
Code is available at https://github.com/abduallahmohamed/Skeleton-Graph.git","['Abduallah Mohamed', 'Huancheng Chen', 'Zhangyang Wang', 'Christian Claudel']",2021-09-21T15:33:40Z,http://arxiv.org/abs/2109.10257v2,"['cs.CV', 'cs.RO']"
"Accuracy Evaluation of Touch Tasks in Commodity Virtual and Augmented
  Reality Head-Mounted Displays","An increasing number of consumer-oriented head-mounted displays (HMD) for
augmented and virtual reality (AR/VR) are capable of finger and hand tracking.
We report on the accuracy of off-the-shelf VR and AR HMDs when used for
touch-based tasks such as pointing or drawing. Specifically, we report on the
finger tracking accuracy of the VR head-mounted displays Oculus Quest, Vive Pro
and the Leap Motion controller, when attached to a VR HMD, as well as the
finger tracking accuracy of the AR head-mounted displays Microsoft HoloLens 2
and Magic Leap. We present the results of two experiments in which we compare
the accuracy for absolute and relative pointing tasks using both human
participants and a robot. The results suggest that HTC Vive has a lower spatial
accuracy than the Oculus Quest and Leap Motion and that the Microsoft HoloLens
2 provides higher spatial accuracy than Magic Leap One. These findings can
serve as decision support for researchers and practitioners in choosing which
systems to use in the future.","['Daniel Schneider', 'Verena Biener', 'Alexander Otte', 'Travis Gesslein', 'Philipp Gagel', 'Cuauhtli Campos', 'Klen Čopič Pucihar', 'Matjaž Kljun', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson', 'Jens Grubert']",2021-09-22T09:21:29Z,http://arxiv.org/abs/2109.10607v1,"['cs.HC', 'I.3.7']"
Multi-View Video-Based 3D Hand Pose Estimation,"Hand pose estimation (HPE) can be used for a variety of human-computer
interaction applications such as gesture-based control for physical or
virtual/augmented reality devices. Recent works have shown that videos or
multi-view images carry rich information regarding the hand, allowing for the
development of more robust HPE systems. In this paper, we present the
Multi-View Video-Based 3D Hand (MuViHand) dataset, consisting of multi-view
videos of the hand along with ground-truth 3D pose labels. Our dataset includes
more than 402,000 synthetic hand images available in 4,560 videos. The videos
have been simultaneously captured from six different angles with complex
backgrounds and random levels of dynamic lighting. The data has been captured
from 10 distinct animated subjects using 12 cameras in a semi-circle topology
where six tracking cameras only focus on the hand and the other six fixed
cameras capture the entire body. Next, we implement MuViHandNet, a neural
pipeline consisting of image encoders for obtaining visual embeddings of the
hand, recurrent learners to learn both temporal and angular sequential
information, and graph networks with U-Net architectures to estimate the final
3D pose information. We perform extensive experiments and show the challenging
nature of this new dataset as well as the effectiveness of our proposed method.
Ablation studies show the added value of each component in MuViHandNet, as well
as the benefit of having temporal and sequential information in the dataset.","['Leyla Khaleghi', 'Alireza Sepas Moghaddam', 'Joshua Marshall', 'Ali Etemad']",2021-09-24T05:20:41Z,http://arxiv.org/abs/2109.11747v1,"['cs.CV', 'cs.AI']"
"Rendering Spatial Sound for Interoperable Experiences in the Audio
  Metaverse","Interactive audio spatialization technology previously developed for video
game authoring and rendering has evolved into an essential component of
platforms enabling shared immersive virtual experiences for future co-presence,
remote collaboration and entertainment applications. New wearable virtual and
augmented reality displays employ real-time binaural audio computing engines
rendering multiple digital objects and supporting the free navigation of
networked participants or their avatars through a juxtaposition of
environments, real and virtual, often referred to as the Metaverse. These
applications require a parametric audio scene programming interface to
facilitate the creation and deployment of shared, dynamic and realistic virtual
3D worlds on mobile computing platforms and remote servers.
  We propose a practical approach for designing parametric 6-degree-of-freedom
object-based interactive audio engines to deliver the perceptually relevant
binaural cues necessary for audio/visual and virtual/real congruence in
Metaverse experiences. We address the effects of room reverberation, acoustic
reflectors, and obstacles in both the virtual and real environments, and
discuss how such effects may be driven by combinations of pre-computed and
real-time acoustic propagation solvers. We envision an open scene description
model distilled to facilitate the development of interoperable applications
distributed across multiple platforms, where each audio object represents, to
the user, a natural sound source having controllable distance, size,
orientation, and acoustic radiation properties.","['Jean-Marc Jot', 'Rémi Audfray', 'Mark Hertensteiner', 'Brian Schmidt']",2021-09-26T01:24:08Z,http://arxiv.org/abs/2109.12471v1,"['cs.SD', 'eess.AS']"
"DeepEdge: A Deep Reinforcement Learning based Task Orchestrator for Edge
  Computing","The improvements in the edge computing technology pave the road for
diversified applications that demand real-time interaction. However, due to the
mobility of the end-users and the dynamic edge environment, it becomes
challenging to handle the task offloading with high performance. Moreover,
since each application in mobile devices has different characteristics, a task
orchestrator must be adaptive and have the ability to learn the dynamics of the
environment. For this purpose, we develop a deep reinforcement learning based
task orchestrator, DeepEdge, which learns to meet different task requirements
without needing human interaction even under the heavily-loaded stochastic
network conditions in terms of mobile users and applications. Given the dynamic
offloading requests and time-varying communication conditions, we successfully
model the problem as a Markov process and then apply the Double Deep Q-Network
(DDQN) algorithm to implement DeepEdge. To evaluate the robustness of DeepEdge,
we experiment with four different applications including image rendering,
infotainment, pervasive health, and augmented reality in the network under
various loads. Furthermore, we compare the performance of our agent with the
four different task offloading approaches in the literature. Our results show
that DeepEdge outperforms its competitors in terms of the percentage of
satisfactorily completed tasks.","['Baris Yamansavascilar', 'Ahmet Cihat Baktir', 'Cagatay Sonmez', 'Atay Ozgovde', 'Cem Ersoy']",2021-10-05T07:55:19Z,http://arxiv.org/abs/2110.01863v2,"['cs.NI', 'cs.DC', 'cs.LG']"
Pose Refinement with Joint Optimization of Visual Points and Lines,"High-precision camera re-localization technology in a pre-established 3D
environment map is the basis for many tasks, such as Augmented Reality,
Robotics and Autonomous Driving. The point-based visual re-localization
approaches are well-developed in recent decades, but are insufficient in some
feature-less cases. In this paper, we design a complete pipeline for camera
pose refinement with points and lines, which contains the innovatively designed
line extracting CNN named VLSE, the line matching and the pose optimization
approaches. We adopt a novel line representation and customize a hybrid
convolution block based on the Stacked Hourglass network, to detect accurate
and stable line features on images. Then we apply a geometric-based strategy to
obtain precise 2D-3D line correspondences using epipolar constraint and
reprojection filtering. A following point-line joint cost function is
constructed to optimize the camera pose with the initial coarse pose from the
pure point-based localization. Sufficient experiments are conducted on open
datasets, i.e, line extractor on Wireframe and YorkUrban, localization
performance on InLoc duc1 and duc2, to confirm the effectiveness of our
point-line joint pose optimization method.","['Shuang Gao', 'Jixiang Wan', 'Yishan Ping', 'Xudong Zhang', 'Shuzhou Dong', 'Yuchen Yang', 'Haikuan Ning', 'Jijunnan Li', 'Yandong Guo']",2021-10-08T07:22:51Z,http://arxiv.org/abs/2110.03940v2,"['cs.CV', 'cs.RO']"
"Mobility Aware Edge Computing Segmentation Towards Localized
  Orchestration","The current trend in end-user devices' advancements in computing and
communication capabilities makes edge computing an attractive solution to pave
the way for the coveted ultra-low latency services. The success of the edge
computing networking paradigm depends on the proper orchestration of the edge
servers. Several Edge applications and services are intolerant to latency,
especially in 5G and beyond networks, such as intelligent video surveillance,
E-health, Internet of Vehicles, and augmented reality applications. The edge
devices underwent rapid growth in both capabilities and size to cope with the
service demands. Orchestrating it on the cloud was a prominent trend during the
past decade. However, the increasing number of edge devices poses a significant
burden on the orchestration delay. In addition to the growth in edge devices,
the high mobility of users renders traditional orchestration schemes
impractical for contemporary edge networks. Proper segmentation of the edge
space becomes necessary to adapt these schemes to address these challenges. In
this paper, we introduce a segmentation technique employing lax clustering and
segregated mobility-based clustering. We then apply latency mapping to these
clusters. The proposed scheme's main objective is to create subspaces
(segments) that enable light and efficient edge orchestration by reducing the
processing time and the core cloud communication overhead. A bench-marking
simulation is conducted with the results showing decreased mobility-related
failures and reduced orchestration delay.","['Sam Aleyadeh', 'Abdallah Moubayed', 'Abdallah Shami']",2021-10-15T02:17:31Z,http://arxiv.org/abs/2110.07808v1,['cs.NI']
A QoE Model in Point Cloud Video Streaming,"Point cloud video has been widely used by augmented reality (AR) and virtual
reality (VR) applications as it allows users to have an immersive experience of
six degrees of freedom (6DoFs). Yet there is still a lack of research on
quality of experience (QoE) model of point cloud video streaming, which cannot
provide optimization metric for streaming systems. Besides, position and color
information contained in each pixel of point cloud video, and viewport distance
effect caused by 6DoFs viewing procedure make the traditional objective quality
evaluation metric cannot be directly used in point cloud video streaming
system. In this paper we first analyze the subjective and objective factors
related to QoE model. Then an experimental system to simulate point cloud video
streaming is setup and detailed subjective quality evaluation experiments are
carried out. Based on collected mean opinion score (MOS) data, we propose a QoE
model for point cloud video streaming. We also verify the model by actual
subjective scoring, and the results show that the proposed QoE model can
accurately reflect users' visual perception. We also make the experimental
database public to promote the QoE research of point cloud video streaming.","['Jie Li', 'Xiao Wang', 'Zhi Liu', 'Qiyue Li']",2021-11-04T16:29:43Z,http://arxiv.org/abs/2111.02985v4,"['cs.MM', 'eess.IV']"
"EdgeXAR: A 6-DoF Camera Multi-target Interaction Framework for MAR with
  User-friendly Latency Compensation","The computational capabilities of recent mobile devices enable the processing
of natural features for Augmented Reality (AR), but the scalability is still
limited by the devices' computation power and available resources. In this
paper, we propose EdgeXAR, a mobile AR framework that utilizes the advantages
of edge computing through task offloading to support flexible camera-based AR
interaction. We propose a hybrid tracking system for mobile devices that
provides lightweight tracking with 6 Degrees of Freedom and hides the
offloading latency from users' perception. A practical, reliable and unreliable
communication mechanism is used to achieve fast response and consistency of
crucial information. We also propose a multi-object image retrieval pipeline
that executes fast and accurate image recognition tasks on the cloud and edge
servers. Extensive experiments are carried out to evaluate the performance of
EdgeXAR by building mobile AR Apps upon it. Regarding the Quality of Experience
(QoE), the mobile AR Apps powered by EdgeXAR framework run on average at the
speed of 30 frames per second with precise tracking of only 1~2 pixel errors
and accurate image recognition of at least 97% accuracy. As compared to
Vuforia, one of the leading commercial AR frameworks, EdgeXAR transmits 87%
less data while providing a stable 30 FPS performance and reducing the
offloading latency by 50 to 70% depending on the transmission medium. Our work
facilitates the large-scale deployment of AR as the next generation of
ubiquitous interfaces.","['Wenxiao Zhang', 'Sikun Lin', 'Farshid Hassani Bijarbooneh', 'Haofei Cheng', 'Tristan Braud', 'Pengyuan Zhou', 'Lik-hang Lee', 'Pan Hui']",2021-11-09T14:32:40Z,http://arxiv.org/abs/2111.05173v1,['cs.HC']
Leveraging Geometry for Shape Estimation from a Single RGB Image,"Predicting 3D shapes and poses of static objects from a single RGB image is
an important research area in modern computer vision. Its applications range
from augmented reality to robotics and digital content creation. Typically this
task is performed through direct object shape and pose predictions which is
inaccurate. A promising research direction ensures meaningful shape predictions
by retrieving CAD models from large scale databases and aligning them to the
objects observed in the image. However, existing work does not take the object
geometry into account, leading to inaccurate object pose predictions,
especially for unseen objects. In this work we demonstrate how cross-domain
keypoint matches from an RGB image to a rendered CAD model allow for more
precise object pose predictions compared to ones obtained through direct
predictions. We further show that keypoint matches can not only be used to
estimate the pose of an object, but also to modify the shape of the object
itself. This is important as the accuracy that can be achieved with object
retrieval alone is inherently limited to the available CAD models. Allowing
shape adaptation bridges the gap between the retrieved CAD model and the
observed shape. We demonstrate our approach on the challenging Pix3D dataset.
The proposed geometric shape prediction improves the AP mesh over the
state-of-the-art from 33.2 to 37.8 on seen objects and from 8.2 to 17.1 on
unseen objects. Furthermore, we demonstrate more accurate shape predictions
without closely matching CAD models when following the proposed shape
adaptation. Code is publicly available at
https://github.com/florianlanger/leveraging_geometry_for_shape_estimation .","['Florian Langer', 'Ignas Budvytis', 'Roberto Cipolla']",2021-11-10T10:17:56Z,http://arxiv.org/abs/2111.05615v1,['cs.CV']
Temporally Consistent Online Depth Estimation in Dynamic Scenes,"Temporally consistent depth estimation is crucial for online applications
such as augmented reality. While stereo depth estimation has received
substantial attention as a promising way to generate 3D information, there is
relatively little work focused on maintaining temporal stability. Indeed, based
on our analysis, current techniques still suffer from poor temporal
consistency. Stabilizing depth temporally in dynamic scenes is challenging due
to concurrent object and camera motion. In an online setting, this process is
further aggravated because only past frames are available. We present a
framework named Consistent Online Dynamic Depth (CODD) to produce temporally
consistent depth estimates in dynamic scenes in an online setting. CODD
augments per-frame stereo networks with novel motion and fusion networks. The
motion network accounts for dynamics by predicting a per-pixel SE3
transformation and aligning the observations. The fusion network improves
temporal depth consistency by aggregating the current and past estimates. We
conduct extensive experiments and demonstrate quantitatively and qualitatively
that CODD outperforms competing methods in terms of temporal consistency and
performs on par in terms of per-frame accuracy.","['Zhaoshuo Li', 'Wei Ye', 'Dilin Wang', 'Francis X. Creighton', 'Russell H. Taylor', 'Ganesh Venkatesh', 'Mathias Unberath']",2021-11-17T19:00:51Z,http://arxiv.org/abs/2111.09337v3,['cs.CV']
PoseKernelLifter: Metric Lifting of 3D Human Pose using Sound,"Reconstructing the 3D pose of a person in metric scale from a single view
image is a geometrically ill-posed problem. For example, we can not measure the
exact distance of a person to the camera from a single view image without
additional scene assumptions (e.g., known height). Existing learning based
approaches circumvent this issue by reconstructing the 3D pose up to scale.
However, there are many applications such as virtual telepresence, robotics,
and augmented reality that require metric scale reconstruction. In this paper,
we show that audio signals recorded along with an image, provide complementary
information to reconstruct the metric 3D pose of the person.
  The key insight is that as the audio signals traverse across the 3D space,
their interactions with the body provide metric information about the body's
pose. Based on this insight, we introduce a time-invariant transfer function
called pose kernel -- the impulse response of audio signals induced by the body
pose. The main properties of the pose kernel are that (1) its envelope highly
correlates with 3D pose, (2) the time response corresponds to arrival time,
indicating the metric distance to the microphone, and (3) it is invariant to
changes in the scene geometry configurations. Therefore, it is readily
generalizable to unseen scenes. We design a multi-stage 3D CNN that fuses audio
and visual signals and learns to reconstruct 3D pose in a metric scale. We show
that our multi-modal method produces accurate metric reconstruction in real
world scenes, which is not possible with state-of-the-art lifting approaches
including parametric mesh regression and depth regression.","['Zhijian Yang', 'Xiaoran Fan', 'Volkan Isler', 'Hyun Soo Park']",2021-12-01T01:34:56Z,http://arxiv.org/abs/2112.00216v2,"['cs.CV', 'cs.SD', 'eess.AS']"
"My(o) Armband Leaks Passwords: An EMG and IMU Based Keylogging
  Side-Channel Attack","Wearables that constantly collect various sensor data of their users increase
the chances for inferences of unintentional and sensitive information such as
passwords typed on a physical keyboard. We take a thorough look at the
potential of using electromyographic (EMG) data, a sensor modality which is new
to the market but has lately gained attention in the context of wearables for
augmented reality (AR), for a keylogging side-channel attack. Our approach is
based on neural networks for a between-subject attack in a realistic scenario
using the Myo Armband to collect the sensor data. In our approach, the EMG data
has proven to be the most prominent source of information compared to the
accelerometer and gyroscope, increasing the keystroke detection performance.
For our end-to-end approach on raw data, we report a mean balanced accuracy of
about 76 % for the keystroke detection and a mean top-3 key accuracy of about
32 % on 52 classes for the key identification on passwords of varying
strengths. We have created an extensive dataset including more than 310 000
keystrokes recorded from 37 volunteers, which is available as open access along
with the source code used to create the given results.","['Matthias Gazzari', 'Annemarie Mattmann', 'Max Maass', 'Matthias Hollick']",2021-12-04T16:48:56Z,http://arxiv.org/abs/2112.02382v1,"['cs.CR', 'cs.LG', 'eess.SP']"
"Labeling Out-of-View Objects in Immersive Analytics to Support Situated
  Visual Searching","Augmented Reality (AR) embeds digital information into objects of the
physical world. Data can be shown in-situ, thereby enabling real-time visual
comparisons and object search in real-life user tasks, such as comparing
products and looking up scores in a sports game. While there have been studies
on designing AR interfaces for situated information retrieval, there has only
been limited research on AR object labeling for visual search tasks in the
spatial environment. In this paper, we identify and categorize different design
aspects in AR label design and report on a formal user study on labels for
out-of-view objects to support visual search tasks in AR. We design three
visualization techniques for out-of-view object labeling in AR, which
respectively encode the relative physical position (height-encoded), the
rotational direction (angle-encoded), and the label values (value-encoded) of
the objects. We further implement two traditional in-view object labeling
techniques, where labels are placed either next to the respective objects
(situated) or at the edge of the AR FoV (boundary). We evaluate these five
different label conditions in three visual search tasks for static objects. Our
study shows that out-of-view object labels are beneficial when searching for
objects outside the FoV, spatial orientation, and when comparing multiple
spatially sparse objects. Angle-encoded labels with directional cues of the
surrounding objects have the overall best performance with the highest user
satisfaction. We discuss the implications of our findings for future immersive
AR interface design.","['Tica Lin', 'Yalong Yang', 'Johanna Beyer', 'Hanspeter Pfister']",2021-12-06T20:58:04Z,http://arxiv.org/abs/2112.03354v2,"['cs.HC', 'cs.GR']"
"Federated Deep Reinforcement Learning for the Distributed Control of
  NextG Wireless Networks","Next Generation (NextG) networks are expected to support demanding tactile
internet applications such as augmented reality and connected autonomous
vehicles. Whereas recent innovations bring the promise of larger link capacity,
their sensitivity to the environment and erratic performance defy traditional
model-based control rationales. Zero-touch data-driven approaches can improve
the ability of the network to adapt to the current operating conditions. Tools
such as reinforcement learning (RL) algorithms can build optimal control policy
solely based on a history of observations. Specifically, deep RL (DRL), which
uses a deep neural network (DNN) as a predictor, has been shown to achieve good
performance even in complex environments and with high dimensional inputs.
However, the training of DRL models require a large amount of data, which may
limit its adaptability to ever-evolving statistics of the underlying
environment. Moreover, wireless networks are inherently distributed systems,
where centralized DRL approaches would require excessive data exchange, while
fully distributed approaches may result in slower convergence rates and
performance degradation. In this paper, to address these challenges, we propose
a federated learning (FL) approach to DRL, which we refer to federated DRL
(F-DRL), where base stations (BS) collaboratively train the embedded DNN by
only sharing models' weights rather than training data. We evaluate two
distinct versions of F-DRL, value and policy based, and show the superior
performance they achieve compared to distributed and centralized DRL.","['Peyman Tehrani', 'Francesco Restuccia', 'Marco Levorato']",2021-12-07T03:13:20Z,http://arxiv.org/abs/2112.03465v1,['cs.LG']
GAN-Supervised Dense Visual Alignment,"We propose GAN-Supervised Learning, a framework for learning discriminative
models and their GAN-generated training data jointly end-to-end. We apply our
framework to the dense visual alignment problem. Inspired by the classic
Congealing method, our GANgealing algorithm trains a Spatial Transformer to map
random samples from a GAN trained on unaligned data to a common,
jointly-learned target mode. We show results on eight datasets, all of which
demonstrate our method successfully aligns complex data and discovers dense
correspondences. GANgealing significantly outperforms past self-supervised
correspondence algorithms and performs on-par with (and sometimes exceeds)
state-of-the-art supervised correspondence algorithms on several datasets --
without making use of any correspondence supervision or data augmentation and
despite being trained exclusively on GAN-generated data. For precise
correspondence, we improve upon state-of-the-art supervised methods by as much
as $3\times$. We show applications of our method for augmented reality, image
editing and automated pre-processing of image datasets for downstream GAN
training.","['William Peebles', 'Jun-Yan Zhu', 'Richard Zhang', 'Antonio Torralba', 'Alexei A. Efros', 'Eli Shechtman']",2021-12-09T18:59:58Z,http://arxiv.org/abs/2112.05143v2,['cs.CV']
"Extending 3-DoF Metrics to Model User Behaviour Similarity in 6-DoF
  Immersive Applications","Immersive reality technologies, such as Virtual and Augmented Reality, have
ushered a new era of user-centric systems, in which every aspect of the
coding--delivery--rendering chain is tailored to the interaction of the users.
Understanding the actual interactivity and behaviour of the users is still an
open challenge and a key step to enabling such a user-centric system. Our main
goal is to extend the applicability of existing behavioural methodologies for
studying user navigation in the case of 6 Degree-of-Freedom (DoF).
Specifically, we first compare the navigation in 6-DoF with its 3-DoF
counterpart highlighting the main differences and novelties. Then, we define
new metrics aimed at better modelling behavioural similarities between users in
a 6-DoF system. We validate and test our solutions on real navigation paths of
users interacting with dynamic volumetric media in 6-DoF Virtual Reality
conditions. Our results show that metrics that consider both user position and
viewing direction better perform in detecting user similarity while navigating
in a 6-DoF system. Having easy-to-use but robust metrics that underpin multiple
tools and answer the question ``how do we detect if two users look at the same
content?"" open the gate to new solutions for a user-centric system.","['Silvia Rossi', 'Irene Viola', 'Laura Toni', 'Pablo Cesar']",2021-12-17T09:29:06Z,http://arxiv.org/abs/2112.09402v2,"['cs.HC', 'cs.MM']"
Metaverse Shape of Your Life for Future: A bibliometric snapshot,"The metaverse was first introduced in 1992. Many people saw Metaverse as a
new word but the concept of Metaverse is not a new term. However, Zuckerberg's
press release drew all the attention to the Metaverse. This study presents a
bibliometric evaluation of metaverse technology, which has been discussed in
the literature since the nineties. A field study is carried out especially for
the metaverse, which is a new and trendy subject. In this way, descriptive
information is presented on journals, institutions, prominent researchers, and
countries in the field, as well as extra evaluation on the prominent topics in
the field and researchers with heavy citations. In our study, which was carried
out by extracting the data of all documents between the years 1990-2021 from
the Web of Science database, it was seen that there were few studies in the
literature in the historical process for the metaverse, whose popularity has
reached its peak in recent months. In addition, it is seen that the subject is
handled intensively with virtual reality and augmented reality technologies,
and the education sector and digital marketing fields show interest in the
field. Metaverse will probably have entered many areas of our lives in the next
15-20 years, shape our lives by taking advantage of the opportunities of
developing technology.",['Muhammet Damar'],2021-12-08T13:46:16Z,http://arxiv.org/abs/2112.12068v1,"['cs.DL', 'cs.CY']"
Heterogenous Networks: From small cells to 5G NR-U,"With the exponential increase in mobile users, the mobile data demand has
grown tremendously. To meet these demands, cellular operators are constantly
innovating to enhance the capacity of cellular systems. Consequently, operators
have been reusing the licensed spectrum spatially, by deploying 4G/LTE small
cells (e.g., Femto Cells) in the past. However, despite the use of small cells,
licensed spectrum will be unable to meet the consistently rising data traffic
because of data-intensive applications such as augmented reality or virtual
reality (AR/VR) and on-the-go high-definition video streaming. Applications
such AR/VR and online gaming not only place extreme data demands on the
network, but are also latency-critical. To meet the QoS guarantees, cellular
operators have begun leveraging the unlicensed spectrum by coexisting with
Wi-Fi in the 5 GHz band. The standardizing body 3GPP, has prescribed cellular
standards for fair unlicensed coexistence with Wi-Fi, namely LTE Licensed
Assisted Access (LAA), New Radio in unlicensed (NR-U), and NR in Millimeter.
The rapid roll-out of LAA deployments in developed nations like the US, offers
an opportunity to study and analyze the performance of unlicensed coexistence
networks through real-world ground truth. Thus, this paper presents a
high-level overview of past, present, and future of the research in small cell
and unlicensed coexistence communication technologies. It outlines the vision
for future research work in the recently allocated unlicensed spectrum: The 6
GHz band, where the latest Wi-Fi standard, IEEE 802.11ax, will coexist with the
latest cellular technology, 5G New Radio (NR) in unlicensed.","['Vanlin Sathya', 'Srikant Manas Kala', 'Kalpana Naidu']",2021-12-28T18:01:34Z,http://arxiv.org/abs/2112.14240v1,['cs.NI']
"Multi-layer VI-GNSS Global Positioning Framework with Numerical Solution
  aided MAP Initialization","Motivated by the goal of achieving long-term drift-free camera pose
estimation in complex scenarios, we propose a global positioning framework
fusing visual, inertial and Global Navigation Satellite System (GNSS)
measurements in multiple layers. Different from previous loosely- and tightly-
coupled methods, the proposed multi-layer fusion allows us to delicately
correct the drift of visual odometry and keep reliable positioning while GNSS
degrades. In particular, local motion estimation is conducted in the
inner-layer, solving the problem of scale drift and inaccurate bias estimation
in visual odometry by fusing the velocity of GNSS, pre-integration of Inertial
Measurement Unit (IMU) and camera measurement in a tightly-coupled way. The
global localization is achieved in the outer-layer, where the local motion is
further fused with GNSS position and course in a long-term period in a
loosely-coupled way. Furthermore, a dedicated initialization method is proposed
to guarantee fast and accurate estimation for all state variables and
parameters. We give exhaustive tests of the proposed framework on indoor and
outdoor public datasets. The mean localization error is reduced up to 63%, with
a promotion of 69% in initialization accuracy compared with state-of-the-art
works. We have applied the algorithm to Augmented Reality (AR) navigation,
crowd sourcing high-precision map update and other large-scale applications.","['Bing Han', 'Zhongyang Xiao', 'Shuai Huang', 'Tao Zhang']",2022-01-05T12:08:34Z,http://arxiv.org/abs/2201.01561v1,['cs.RO']
Matching-based Service Offloading for Compute-less Driven IoT Networks,"With the advent of the Internet of Things (IoT) and 5G networks, edge
computing is offering new opportunities for business model and use cases
innovations. Service providers can now virtualize the cloud beyond the data
center to meet the latency, data sovereignty, reliability, and interoperability
requirements. Yet, many new applications (e.g., augmented reality, virtual
reality, artificial intelligence) are computation-intensive and
delay-sensitivity. These applications are invoked heavily with similar inputs
that could lead to the same output. Compute-less networks aim to implement a
network with a minimum amount of computation and communication. This can be
realized by offloading prevalent services to the edge and thus minimizing
communication in the core network and eliminating redundant computations using
the computation reuse concept. In this paper, we present matching-based
services offloading schemes for compute-less IoT networks. We adopt the
matching theory to match service offloading to the appropriate edge server(s).
Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme
that aims to offload the most invoked and highly reusable services to the
appropriate edge servers. We further extend WHISTLE to provide horizontal
one-to-many computation reuse sharing among edge servers which leads to
bouncing less computation back to the cloud. We evaluate the efficiency and
effectiveness of WHISTLE with a real-world dataset. The obtained findings show
that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the
computation up to 77%, and decrease the communication up to 71%. Theoretical
analyses also prove the stability of the designed schemes.","['Boubakr Nour', 'Soumaya Cherkaoui']",2022-01-11T20:55:19Z,http://arxiv.org/abs/2201.04195v1,['cs.NI']
"Achieving AI-enabled Robust End-to-End Quality of Experience over Radio
  Access Networks","Emerging applications such as Augmented Reality, the Internet of Vehicles and
Remote Surgery require both computing and networking functions working in
harmony. The End-to-end (E2E) quality of experience (QoE) for these
applications depends on the synchronous allocation of networking and computing
resources. However, the relationship between the resources and the E2E QoE
outcomes is typically stochastic and non-linear. In order to make efficient
resource allocation decisions, it is essential to model these relationships.
This article presents a novel machine-learning based approach to learn these
relationships and concurrently orchestrate both resources for this purpose. The
machine learning models further help make robust allocation decisions regarding
stochastic variations and simplify robust optimization to a conventional
constrained optimization. When resources are insufficient to accommodate all
application requirements, our framework supports executing some of the
applications with minimal degradation (graceful degradation) of E2E QoE. We
also show how we can implement the learning and optimization methods in a
distributed fashion by the Software-Defined Network (SDN) and Kubernetes
technologies. Our results show that deep learning-based modelling achieves E2E
QoE with approximately 99.8\% accuracy, and our robust joint-optimization
technique allocates resources efficiently when compared to existing
differential services alternatives.","['Dibbendu Roy', 'Aravinda S. Rao', 'Tansu Alpcan', 'Goutam Das', 'Marimuthu Palaniswami']",2022-01-13T19:31:35Z,http://arxiv.org/abs/2201.05184v1,"['cs.NI', 'cs.SY', 'eess.SY']"
"A Machine Learning Framework for Distributed Functional Compression over
  Wireless Channels in IoT","IoT devices generating enormous data and state-of-the-art machine learning
techniques together will revolutionize cyber-physical systems. In many diverse
fields, from autonomous driving to augmented reality, distributed IoT devices
compute specific target functions without simple forms like obstacle detection,
object recognition, etc. Traditional cloud-based methods that focus on
transferring data to a central location either for training or inference place
enormous strain on network resources. To address this, we develop, to the best
of our knowledge, the first machine learning framework for distributed
functional compression over both the Gaussian Multiple Access Channel (GMAC)
and orthogonal AWGN channels. Due to the Kolmogorov-Arnold representation
theorem, our machine learning framework can, by design, compute any arbitrary
function for the desired functional compression task in IoT. Importantly the
raw sensory data are never transferred to a central node for training or
inference, thus reducing communication. For these algorithms, we provide
theoretical convergence guarantees and upper bounds on communication. Our
simulations show that the learned encoders and decoders for functional
compression perform significantly better than traditional approaches, are
robust to channel condition changes and sensor outages. Compared to the
cloud-based scenario, our algorithms reduce channel use by two orders of
magnitude.","['Yashas Malur Saidutta', 'Afshin Abdi', 'Faramarz Fekri']",2022-01-24T06:38:39Z,http://arxiv.org/abs/2201.09483v2,"['cs.LG', 'cs.DC', 'cs.IT', 'eess.SP', 'math.IT', 'stat.ML']"
"Enabling Deep Learning on Edge Devices through Filter Pruning and
  Knowledge Transfer","Deep learning models have introduced various intelligent applications to edge
devices, such as image classification, speech recognition, and augmented
reality. There is an increasing need of training such models on the devices in
order to deliver personalized, responsive, and private learning. To address
this need, this paper presents a new solution for deploying and training
state-of-the-art models on the resource-constrained devices. First, the paper
proposes a novel filter-pruning-based model compression method to create
lightweight trainable models from large models trained in the cloud, without
much loss of accuracy. Second, it proposes a novel knowledge transfer method to
enable the on-device model to update incrementally in real time or near real
time using incremental learning on new data and enable the on-device model to
learn the unseen categories with the help of the in-cloud model in an
unsupervised fashion. The results show that 1) our model compression method can
remove up to 99.36% parameters of WRN-28-10, while preserving a Top-1 accuracy
of over 90% on CIFAR-10; 2) our knowledge transfer method enables the
compressed models to achieve more than 90% accuracy on CIFAR-10 and retain good
accuracy on old categories; 3) it allows the compressed models to converge
within real time (three to six minutes) on the edge for incremental learning
tasks; 4) it enables the model to classify unseen categories of data (78.92%
Top-1 accuracy) that it is never trained with.","['Kaiqi Zhao', 'Yitao Chen', 'Ming Zhao']",2022-01-22T00:27:21Z,http://arxiv.org/abs/2201.10947v1,"['cs.LG', 'cs.AI', 'cs.CV']"
"Spherical Convolution empowered FoV Prediction in 360-degree Video
  Multicast with Limited FoV Feedback","Field of view (FoV) prediction is critical in 360-degree video multicast,
which is a key component of the emerging Virtual Reality (VR) and Augmented
Reality (AR) applications. Most of the current prediction methods combining
saliency detection and FoV information neither take into account that the
distortion of projected 360-degree videos can invalidate the weight sharing of
traditional convolutional networks, nor do they adequately consider the
difficulty of obtaining complete multi-user FoV information, which degrades the
prediction performance. This paper proposes a spherical convolution-empowered
FoV prediction method, which is a multi-source prediction framework combining
salient features extracted from 360-degree video with limited FoV feedback
information. A spherical convolution neural network (CNN) is used instead of a
traditional two-dimensional CNN to eliminate the problem of weight sharing
failure caused by video projection distortion. Specifically, salient
spatial-temporal features are extracted through a spherical convolution-based
saliency detection model, after which the limited feedback FoV information is
represented as a time-series model based on a spherical convolution-empowered
gated recurrent unit network. Finally, the extracted salient video features are
combined to predict future user FoVs. The experimental results show that the
performance of the proposed method is better than other prediction methods.","['Jie Li', 'Ling Han', 'Cong Zhang', 'Qiyue Li', 'Zhi Liu']",2022-01-29T08:32:19Z,http://arxiv.org/abs/2201.12525v1,"['cs.CV', 'cs.MM']"
"InfraredTags: Embedding Invisible AR Markers and Barcodes Using
  Low-Cost, Infrared-Based 3D Printing and Imaging Tools","Existing approaches for embedding unobtrusive tags inside 3D objects require
either complex fabrication or high-cost imaging equipment. We present
InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye
that can be 3D printed as part of objects, and detected rapidly by low-cost
near-infrared cameras. We achieve this by printing objects from an
infrared-transmitting filament, which infrared cameras can see through, and by
having air gaps inside for the tag's bits, which appear at a different
intensity in the infrared image.
  We built a user interface that facilitates the integration of common tags (QR
codes, ArUco markers) with the object geometry to make them 3D printable as
InfraredTags. We also developed a low-cost infrared imaging module that
augments existing mobile devices and decodes tags using our image processing
pipeline. Our evaluation shows that the tags can be detected with little
near-infrared illumination (0.2lux) and from distances as far as 250cm. We
demonstrate how our method enables various applications, such as object
tracking and embedding metadata for augmented reality and tangible
interactions.","['Mustafa Doga Dogan', 'Ahmad Taka', 'Michael Lu', 'Yunyi Zhu', 'Akshat Kumar', 'Aakar Gupta', 'Stefanie Mueller']",2022-02-12T23:45:18Z,http://arxiv.org/abs/2202.06165v1,"['cs.HC', 'cs.CV', 'H.5.0; H.5.2']"
Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction,"Lighting prediction from a single image is becoming increasingly important in
many vision and augmented reality (AR) applications in which shading and shadow
consistency between virtual and real objects should be guaranteed. However,
this is a notoriously ill-posed problem, especially for indoor scenarios,
because of the complexity of indoor luminaires and the limited information
involved in 2D images. In this paper, we propose a graph learning-based
framework for indoor lighting estimation. At its core is a new lighting model
(dubbed DSGLight) based on depth-augmented Spherical Gaussians (SG) and a Graph
Convolutional Network (GCN) that infers the new lighting representation from a
single LDR image of limited field-of-view. Our lighting model builds 128 evenly
distributed SGs over the indoor panorama, where each SG encoding the lighting
and the depth around that node. The proposed GCN then learns the mapping from
the input image to DSGLight. Compared with existing lighting models, our
DSGLight encodes both direct lighting and indirect environmental lighting more
faithfully and compactly. It also makes network training and inference more
stable. The estimated depth distribution enables temporally stable shading and
shadows under spatially-varying lighting. Through thorough experiments, we show
that our method obviously outperforms existing methods both qualitatively and
quantitatively.","['Jiayang Bai', 'Jie Guo', 'Chenchen Wan', 'Zhenyu Chen', 'Zhen He', 'Shan Yang', 'Piaopiao Yu', 'Yan Zhang', 'Yanwen Guo']",2022-02-13T12:49:37Z,http://arxiv.org/abs/2202.06300v1,['cs.CV']
"Integrating Immersive Technologies for Algorithmic Design in
  Architecture","Architectural design practice has radically evolved over the course of its
history, due to technological improvements that gave rise to advanced automated
tools for many design tasks. Traditional paper drawings and scale models are
now accompanied by 2D and 3D Computer-Aided Architectural Design (CAAD)
software.
  While such tools improved in many ways, including performance and accuracy
improvements, the modalities of user interaction have mostly remained the same,
with 2D interfaces displayed on 2D screens. The maturation of Augmented Reality
(AR) and Virtual Reality (VR) technology has led to some level of integration
of these immersive technologies into architectural practice, but mostly limited
to visualisation purposes, e.g. to show a finished project to a potential
client.
  We posit that there is potential to employ such technologies earlier in the
architectural design process and therefore explore that possibility with a
focus on Algorithmic Design (AD), a CAAD paradigm that relies on (often visual)
algorithms to generate geometries. The main goal of this dissertation is to
demonstrate that AR and VR can be adopted for AD activities.
  To verify that claim, we follow an iterative prototype-based methodology to
develop research prototype software tools and evaluate them. The three
developed prototypes provide evidence that integrating immersive technologies
into the AD toolset provides opportunities for architects to improve their
workflow and to better present their creations to clients. Based on our
contributions and the feedback we gathered from architectural students and
other researchers that evaluated the developed prototypes, we additionally
provide insights as to future perspectives in the field.",['Adrien Coppens'],2022-02-25T14:18:04Z,http://arxiv.org/abs/2202.12722v1,"['cs.HC', 'cs.GR']"
"CCOMPASSION: A Hybrid Cloudlet Placement Framework over Passive Optical
  Access Networks","Cloud-based computing technology is one of the most significant technical
advents of the last decade and extension of this facility towards access
networks by aggregation of cloudlets is a step further. To fulfill the ravenous
demand for computational resources entangled with the stringent latency
requirements of computationally-heavy applications related to augmented
reality, cognitive assistance and context-aware computation, installation of
cloudlets near the access segment is a very promising solution because of its
support for wide geographical network distribution, low latency, mobility and
heterogeneity. In this paper, we propose a novel framework, Cloudlet Cost
OptiMization over PASSIve Optical Network (CCOMPASSION), and formulate a
nonlinear mixed-integer program to identify optimal cloudlet placement
locations such that installation cost is minimized whilst meeting the capacity
and latency constraints. Considering urban, suburban and rural scenarios as
commonly-used network deployment models, we investigate the feasibility of the
proposed model over them and provide guidance on the overall cloudlet facility
installation over optical access network. We also study the percentage of
incremental energy budget in the presence of cloudlets of the existing network.
The final results from our proposed model can be considered as fundamental
cornerstones for network planning with hybrid cloudlet network architectures.","['Sourav Mondal', 'Goutam Das', 'Elaine Wong']",2022-02-25T15:50:34Z,http://arxiv.org/abs/2202.12778v1,['cs.NI']
"RelMobNet: End-to-end relative camera pose estimation using a robust
  two-stage training","Relative camera pose estimation, i.e. estimating the translation and rotation
vectors using a pair of images taken in different locations, is an important
part of systems in augmented reality and robotics. In this paper, we present an
end-to-end relative camera pose estimation network using a siamese architecture
that is independent of camera parameters. The network is trained using the
Cambridge Landmarks data with four individual scene datasets and a dataset
combining the four scenes. To improve generalization, we propose a novel
two-stage training that alleviates the need of a hyperparameter to balance the
translation and rotation loss scale. The proposed method is compared with
one-stage training CNN-based methods such as RPNet and RCPNet and demonstrate
that the proposed model improves translation vector estimation by 16.11%,
28.88%, and 52.27% on the Kings College, Old Hospital, and St Marys Church
scenes, respectively. For proving texture invariance, we investigate the
generalization of the proposed method augmenting the datasets to different
scene styles, as ablation studies, using generative adversarial networks. Also,
we present a qualitative assessment of epipolar lines of our network
predictions and ground truth poses.","['Praveen Kumar Rajendran', 'Sumit Mishra', 'Luiz Felipe Vecchietti', 'Dongsoo Har']",2022-02-25T17:27:26Z,http://arxiv.org/abs/2202.12838v2,"['cs.CV', 'cs.RO']"
Deep Camera Pose Regression Using Pseudo-LiDAR,"An accurate and robust large-scale localization system is an integral
component for active areas of research such as autonomous vehicles and
augmented reality. To this end, many learning algorithms have been proposed
that predict 6DOF camera pose from RGB or RGB-D images. However, previous
methods that incorporate depth typically treat the data the same way as RGB
images, often adding depth maps as additional channels to RGB images and
passing them through convolutional neural networks (CNNs). In this paper, we
show that converting depth maps into pseudo-LiDAR signals, previously shown to
be useful for 3D object detection, is a better representation for camera
localization tasks by projecting point clouds that can accurately determine
6DOF camera pose. This is demonstrated by first comparing localization
accuracies of a network operating exclusively on pseudo-LiDAR representations,
with networks operating exclusively on depth maps. We then propose FusionLoc, a
novel architecture that uses pseudo-LiDAR to regress a 6DOF camera pose.
FusionLoc is a dual stream neural network, which aims to remedy common issues
with typical 2D CNNs operating on RGB-D images. The results from this
architecture are compared against various other state-of-the-art deep pose
regression implementations using the 7 Scenes dataset. The findings are that
FusionLoc performs better than a number of other camera localization methods,
with a notable improvement being, on average, 0.33m and 4.35{\deg} more
accurate than RGB-D PoseNet. By proving the validity of using pseudo-LiDAR
signals over depth maps for localization, there are new considerations when
implementing large-scale localization systems.","['Ali Raza', 'Lazar Lolic', 'Shahmir Akhter', 'Alfonso Dela Cruz', 'Michael Liut']",2022-02-28T20:30:37Z,http://arxiv.org/abs/2203.00080v1,"['cs.CV', 'cs.LG', 'cs.RO']"
"Standardization of Extended Reality (XR) over 5G and 5G-Advanced 3GPP
  New Radio","Extended Reality (XR) is one of the major innovations to be introduced in
5G/5G-Advanced communication systems. A combination of augmented reality,
virtual reality, and mixed reality, supplemented by cloud gaming, revisits the
way how humans interact with computers, networks, and each other. However,
efficient support of XR services imposes new challenges for existing and future
wireless networks. This article presents a tutorial on integrating support for
the XR into the 3GPP New Radio (NR), summarizing a range of activities handled
within various 3GPP Service and Systems Aspects (SA) and Radio Access Networks
(RAN) groups. The article also delivers a case study evaluating the performance
of different XR services in state-of-the-art NR Release 17. The paper concludes
with a vision of further enhancements to better support XR in future NR
releases and outlines open problems in this area.","['Margarita Gapeyenko', 'Vitaly Petrov', 'Stefano Paris', 'Andrea Marcano', 'Klaus I. Pedersen']",2022-03-04T11:17:34Z,http://arxiv.org/abs/2203.02242v3,['cs.NI']
"ZippyPoint: Fast Interest Point Detection, Description, and Matching
  through Mixed Precision Discretization","Efficient detection and description of geometric regions in images is a
prerequisite in visual systems for localization and mapping. Such systems still
rely on traditional hand-crafted methods for efficient generation of
lightweight descriptors, a common limitation of the more powerful neural
network models that come with high compute and specific hardware requirements.
In this paper, we focus on the adaptations required by detection and
description neural networks to enable their use in computationally limited
platforms such as robots, mobile, and augmented reality devices. To that end,
we investigate and adapt network quantization techniques to accelerate
inference and enable its use on compute limited platforms. In addition, we
revisit common practices in descriptor quantization and propose the use of a
binary descriptor normalization layer, enabling the generation of distinctive
binary descriptors with a constant number of ones. ZippyPoint, our efficient
quantized network with binary descriptors, improves the network runtime speed,
the descriptor matching speed, and the 3D model size, by at least an order of
magnitude when compared to full-precision counterparts. These improvements come
at a minor performance degradation as evaluated on the tasks of homography
estimation, visual localization, and map-free visual relocalization. Code and
models are available at https://github.com/menelaoskanakis/ZippyPoint.","['Menelaos Kanakis', 'Simon Maurer', 'Matteo Spallanzani', 'Ajad Chhatkuli', 'Luc Van Gool']",2022-03-07T18:59:03Z,http://arxiv.org/abs/2203.03610v3,"['cs.CV', 'cs.LG', 'cs.RO']"
"Object-Based Visual Camera Pose Estimation From Ellipsoidal Model and
  3D-Aware Ellipse Prediction","In this paper, we propose a method for initial camera pose estimation from
just a single image which is robust to viewing conditions and does not require
a detailed model of the scene. This method meets the growing need of easy
deployment of robotics or augmented reality applications in any environments,
especially those for which no accurate 3D model nor huge amount of ground truth
data are available. It exploits the ability of deep learning techniques to
reliably detect objects regardless of viewing conditions. Previous works have
also shown that abstracting the geometry of a scene of objects by an ellipsoid
cloud allows to compute the camera pose accurately enough for various
application needs. Though promising, these approaches use the ellipses fitted
to the detection bounding boxes as an approximation of the imaged objects. In
this paper, we go one step further and propose a learning-based method which
detects improved elliptic approximations of objects which are coherent with the
3D ellipsoids in terms of perspective projection. Experiments prove that the
accuracy of the computed pose significantly increases thanks to our method.
This is achieved with very little effort in terms of training data acquisition
- a few hundred calibrated images of which only three need manual object
annotation. Code and models are released at
https://gitlab.inria.fr/tangram/3d-aware-ellipses-for-visual-localization","['Matthieu Zins', 'Gilles Simon', 'Marie-Odile Berger']",2022-03-09T10:00:52Z,http://arxiv.org/abs/2203.04613v1,['cs.CV']
"Distributed On-Sensor Compute System for AR/VR Devices: A
  Semi-Analytical Simulation Framework for Power Estimation","Augmented Reality/Virtual Reality (AR/VR) glasses are widely foreseen as the
next generation computing platform. AR/VR glasses are a complex ""system of
systems"" which must satisfy stringent form factor, computing-, power- and
thermal- requirements. In this paper, we will show that a novel distributed
on-sensor compute architecture, coupled with new semiconductor technologies
(such as dense 3D-IC interconnects and Spin-Transfer Torque Magneto Random
Access Memory, STT-MRAM) and, most importantly, a full hardware-software
co-optimization are the solutions to achieve attractive and socially acceptable
AR/VR glasses. To this end, we developed a semi-analytical simulation framework
to estimate the power consumption of novel AR/VR distributed on-sensor
computing architectures. The model allows the optimization of the main
technological features of the system modules, as well as the computer-vision
algorithm partition strategy across the distributed compute architecture. We
show that, in the case of the compute-intensive machine learning based Hand
Tracking algorithm, the distributed on-sensor compute architecture can reduce
the system power consumption compared to a centralized system, with the
additional benefits in terms of latency and privacy.","['Jorge Gomez', 'Saavan Patel', 'Syed Shakib Sarwar', 'Ziyun Li', 'Raffaele Capoccia', 'Zhao Wang', 'Reid Pinkham', 'Andrew Berkovich', 'Tsung-Hsun Tsai', 'Barbara De Salvo', 'Chiao Liu']",2022-03-14T20:18:24Z,http://arxiv.org/abs/2203.07474v1,"['cs.AR', 'cs.LG']"
FaceMap: Towards Unsupervised Face Clustering via Map Equation,"Face clustering is an essential task in computer vision due to the explosion
of related applications such as augmented reality or photo album management.
The main challenge of this task lies in the imperfectness of similarities among
image feature representations. Given an existing feature extraction model, it
is still an unresolved problem that how can the inherent characteristics of
similarities of unlabelled images be leveraged to improve the clustering
performance. Motivated by answering the question, we develop an effective
unsupervised method, named as FaceMap, by formulating face clustering as a
process of non-overlapping community detection, and minimizing the entropy of
information flows on a network of images. The entropy is denoted by the map
equation and its minimum represents the least description of paths among images
in expectation. Inspired by observations on the ranked transition probabilities
in the affinity graph constructed from facial images, we develop an outlier
detection strategy to adaptively adjust transition probabilities among images.
Experiments with ablation studies demonstrate that FaceMap significantly
outperforms existing methods and achieves new state-of-the-arts on three
popular large-scale datasets for face clustering, e.g., an absolute improvement
of more than $10\%$ and $4\%$ comparing with prior unsupervised and supervised
methods respectively in terms of average of Pairwise F-score. Our code is
publicly available on github.","['Xiaotian Yu', 'Yifan Yang', 'Aibo Wang', 'Ling Xing', 'Hanling Yi', 'Guangming Lu', 'Xiaoyu Wang']",2022-03-21T03:23:09Z,http://arxiv.org/abs/2203.10090v1,"['cs.CV', 'cs.LG']"
"Dual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic
  Representation","Spatial audio methods are gaining a growing interest due to the spread of
immersive audio experiences and applications, such as virtual and augmented
reality. For these purposes, 3D audio signals are often acquired through arrays
of Ambisonics microphones, each comprising four capsules that decompose the
sound field in spherical harmonics. In this paper, we propose a dual quaternion
representation of the spatial sound field acquired through an array of two
First Order Ambisonics (FOA) microphones. The audio signals are encapsulated in
a dual quaternion that leverages quaternion algebra properties to exploit
correlations among them. This augmented representation with 6 degrees of
freedom (6DOF) involves a more accurate coverage of the sound field, resulting
in a more precise sound localization and a more immersive audio experience. We
evaluate our approach on a sound event localization and detection (SELD)
benchmark. We show that our dual quaternion SELD model with temporal
convolution blocks (DualQSELD-TCN) achieves better results with respect to real
and quaternion-valued baselines thanks to our augmented representation of the
sound field. Full code is available at:
https://github.com/ispamm/DualQSELD-TCN.","['Eleonora Grassucci', 'Gioia Mancini', 'Christian Brignone', 'Aurelio Uncini', 'Danilo Comminiello']",2022-04-04T21:11:00Z,http://arxiv.org/abs/2204.01851v2,"['eess.AS', 'cs.LG', 'cs.SD']"
"Robotic Surgery Remote Mentoring via AR with 3D Scene Streaming and Hand
  Interaction","With the growing popularity of robotic surgery, education becomes
increasingly important and urgently needed for the sake of patient safety.
However, experienced surgeons have limited accessibility due to their busy
clinical schedule or working in a distant city, thus can hardly provide
sufficient education resources for novices. Remote mentoring, as an effective
way, can help solve this problem, but traditional methods are limited to plain
text, audio, or 2D video, which are not intuitive nor vivid. Augmented reality
(AR), a thriving technique being widely used for various education scenarios,
is promising to offer new possibilities of visual experience and interactive
teaching. In this paper, we propose a novel AR-based robotic surgery remote
mentoring system with efficient 3D scene visualization and natural 3D hand
interaction. Using a head-mounted display (i.e., HoloLens), the mentor can
remotely monitor the procedure streamed from the trainee's operation side. The
mentor can also provide feedback directly with hand gestures, which is in-turn
transmitted to the trainee and viewed in surgical console as guidance. We
comprehensively validate the system on both real surgery stereo videos and
ex-vivo scenarios of common robotic training tasks (i.e., peg-transfer and
suturing). Promising results are demonstrated regarding the fidelity of
streamed scene visualization, the accuracy of feedback with hand interaction,
and the low-latency of each component in the entire remote mentoring system.
This work showcases the feasibility of leveraging AR technology for reliable,
flexible and low-cost solutions to robotic surgical education, and holds great
potential for clinical applications.","['Yonghao Long', 'Chengkun Li', 'Qi Dou']",2022-04-09T03:17:15Z,http://arxiv.org/abs/2204.04377v2,['cs.CV']
Episodic Memory Question Answering,"Egocentric augmented reality devices such as wearable glasses passively
capture visual data as a human wearer tours a home environment. We envision a
scenario wherein the human communicates with an AI agent powering such a device
by asking questions (e.g., where did you last see my keys?). In order to
succeed at this task, the egocentric AI assistant must (1) construct
semantically rich and efficient scene memories that encode spatio-temporal
information about objects seen during the tour and (2) possess the ability to
understand the question and ground its answer into the semantic memory
representation. Towards that end, we introduce (1) a new task - Episodic Memory
Question Answering (EMQA) wherein an egocentric AI assistant is provided with a
video sequence (the tour) and a question as an input and is asked to localize
its answer to the question within the tour, (2) a dataset of grounded questions
designed to probe the agent's spatio-temporal understanding of the tour, and
(3) a model for the task that encodes the scene as an allocentric, top-down
semantic feature map and grounds the question into the map to localize the
answer. We show that our choice of episodic scene memory outperforms naive,
off-the-shelf solutions for the task as well as a host of very competitive
baselines and is robust to noise in depth, pose as well as camera jitter. The
project page can be found at: https://samyak-268.github.io/emqa .","['Samyak Datta', 'Sameer Dharur', 'Vincent Cartillier', 'Ruta Desai', 'Mukul Khanna', 'Dhruv Batra', 'Devi Parikh']",2022-05-03T17:28:43Z,http://arxiv.org/abs/2205.01652v1,"['cs.CV', 'cs.AI']"
"Ultra-Reliable Distributed Cloud Network Control with End-to-End Latency
  Constraints","We are entering a rapidly unfolding future driven by the delivery of
real-time computation services, such as industrial automation and augmented
reality, collectively referred to as AgI services, over highly distributed
cloud/edge computing networks. The interaction intensive nature of AgI services
is accelerating the need for networking solutions that provide strict latency
guarantees. In contrast to most existing studies that can only characterize
average delay performance, we focus on the critical goal of delivering AgI
services ahead of corresponding deadlines on a per-packet basis, while
minimizing overall cloud network operational cost. To this end, we design a
novel queuing system able to track data packets' lifetime and formalize the
delay-constrained least-cost dynamic network control problem. To address this
challenging problem, we first study the setting with average capacity (or
resource budget) constraints, for which we characterize the delay-constrained
stability region and design a near-optimal control policy leveraging Lyapunov
optimization theory on an equivalent virtual network. Guided by the same
principle, we tackle the peak capacity constrained scenario by developing the
reliable cloud network control (RCNC) algorithm, which employs a two-way
optimization method to make actual and virtual network flow solutions converge
in an iterative manner. Extensive numerical results show the superior
performance of the proposed control policy compared with the state-of-the-art
cloud network control algorithm, and the value of guaranteeing strict
end-to-end deadlines for the delivery of next-generation AgI services.","['Yang Cai', 'Jaime Llorca', 'Antonia M. Tulino', 'Andreas F. Molisch']",2022-05-05T04:04:48Z,http://arxiv.org/abs/2205.02427v1,"['cs.NI', 'cs.SY', 'eess.SY']"
"Trends in Workplace Wearable Technologies and Connected-Worker Solutions
  for Next-Generation Occupational Safety, Health, and Productivity","The workplace influences the safety, health, and productivity of workers at
multiple levels. To protect and promote total worker health, smart hardware,
and software tools have emerged for the identification, elimination,
substitution, and control of occupational hazards. Wearable devices enable
constant monitoring of individual workers and the environment, whereas
connected worker solutions provide contextual information and decision support.
Here, the recent trends in commercial workplace technologies to monitor and
manage occupational risks, injuries, accidents, and diseases are reviewed.
Workplace safety wearables for safe lifting, ergonomics, hazard identification,
sleep monitoring, fatigue management, and heat and cold stress are discussed.
Examples of workplace productivity wearables for asset tracking, augmented
reality, gesture and motion control, brain wave sensing, and work stress
management are given. Workplace health wearables designed for work-related
musculoskeletal disorders, functional movement disorders, respiratory hazards,
cardiovascular health, outdoor sun exposure, and continuous glucose monitoring
are shown. Connected worker platforms are discussed with information about the
architecture, system modules, intelligent operations, and industry
applications. Predictive analytics provide contextual information about
occupational safety risks, resource allocation, equipment failure, and
predictive maintenance. Altogether, these examples highlight the ground-level
benefits of real-time visibility about frontline workers, work environment,
distributed assets, workforce efficiency, and safety compliance","['Vishal Patel', 'Austin Chesmore', 'Christopher M. Legner', 'Santosh Pandey']",2022-05-24T03:15:25Z,http://arxiv.org/abs/2205.11740v1,"['eess.SY', 'cs.SY', 'eess.SP']"
DeepRM: Deep Recurrent Matching for 6D Pose Refinement,"Precise 6D pose estimation of rigid objects from RGB images is a critical but
challenging task in robotics, augmented reality and human-computer interaction.
To address this problem, we propose DeepRM, a novel recurrent network
architecture for 6D pose refinement. DeepRM leverages initial coarse pose
estimates to render synthetic images of target objects. The rendered images are
then matched with the observed images to predict a rigid transform for updating
the previous pose estimate. This process is repeated to incrementally refine
the estimate at each iteration. The DeepRM architecture incorporates LSTM units
to propagate information through each refinement step, significantly improving
overall performance. In contrast to current 2-stage Perspective-n-Point based
solutions, DeepRM is trained end-to-end, and uses a scalable backbone that can
be tuned via a single parameter for accuracy and efficiency. During training, a
multi-scale optical flow head is added to predict the optical flow between the
observed and synthetic images. Optical flow prediction stabilizes the training
process, and enforces the learning of features that are relevant to the task of
pose estimation. Our results demonstrate that DeepRM achieves state-of-the-art
performance on two widely accepted challenging datasets.","['Alexander Avery', 'Andreas Savakis']",2022-05-28T16:18:08Z,http://arxiv.org/abs/2205.14474v5,['cs.CV']
RIDDLE: Lidar Data Compression with Range Image Deep Delta Encoding,"Lidars are depth measuring sensors widely used in autonomous driving and
augmented reality. However, the large volume of data produced by lidars can
lead to high costs in data storage and transmission. While lidar data can be
represented as two interchangeable representations: 3D point clouds and range
images, most previous work focus on compressing the generic 3D point clouds. In
this work, we show that directly compressing the range images can leverage the
lidar scanning pattern, compared to compressing the unprojected point clouds.
We propose a novel data-driven range image compression algorithm, named RIDDLE
(Range Image Deep DeLta Encoding). At its core is a deep model that predicts
the next pixel value in a raster scanning order, based on contextual laser
shots from both the current and past scans (represented as a 4D point cloud of
spherical coordinates and time). The deltas between predictions and original
values can then be compressed by entropy encoding. Evaluated on the Waymo Open
Dataset and KITTI, our method demonstrates significant improvement in the
compression rate (under the same distortion) compared to widely used point
cloud and range image compression algorithms as well as recent deep methods.","['Xuanyu Zhou', 'Charles R. Qi', 'Yin Zhou', 'Dragomir Anguelov']",2022-06-02T21:53:43Z,http://arxiv.org/abs/2206.01738v1,"['eess.IV', 'cs.CV']"
Volumetric Disentanglement for 3D Scene Manipulation,"Recently, advances in differential volumetric rendering enabled significant
breakthroughs in the photo-realistic and fine-detailed reconstruction of
complex 3D scenes, which is key for many virtual reality applications. However,
in the context of augmented reality, one may also wish to effect semantic
manipulations or augmentations of objects within a scene. To this end, we
propose a volumetric framework for (i) disentangling or separating, the
volumetric representation of a given foreground object from the background, and
(ii) semantically manipulating the foreground object, as well as the
background. Our framework takes as input a set of 2D masks specifying the
desired foreground object for training views, together with the associated 2D
views and poses, and produces a foreground-background disentanglement that
respects the surrounding illumination, reflections, and partial occlusions,
which can be applied to both training and novel views. Our method enables the
separate control of pixel color and depth as well as 3D similarity
transformations of both the foreground and background objects. We subsequently
demonstrate the applicability of our framework on a number of downstream
manipulation tasks including object camouflage, non-negative 3D object
inpainting, 3D object translation, 3D object inpainting, and 3D text-based
object manipulation. Full results are given in our project webpage at
https://sagiebenaim.github.io/volumetric-disentanglement/","['Sagie Benaim', 'Frederik Warburg', 'Peter Ebert Christensen', 'Serge Belongie']",2022-06-06T17:57:07Z,http://arxiv.org/abs/2206.02776v1,['cs.CV']
"Ubiquitous knowledge empowers the Smart Factory: The impacts of a
  Service-oriented Digital Twin on enterprises' performance","While the Industry 4.0 is idolizing the potential of an artificial
intelligence embedded into ""things"", it is neglecting the role of the human
component, which is still indispensable in different manufacturing activities,
such as a machine setup or maintenance operations. The present research study
first proposes an Industrial Internet pyramid as emergent human-centric
manufacturing paradigm within Industry 4.0 in which central is the role of a
Ubiquitous Knowledge about the manufacturing system intuitively accessed and
used by the manufacturing employees. Second, the prototype of a
Service-oriented Digital Twin, which leverage on a flexible ontology-oriented
knowledge structure and on augmented reality combined to a vocal interaction
system for an intuitive knowledge retrieval and fruition, has been designed and
developed to deliver this manufacturing knowledge. Two test-beds, complimentary
for the problems in practice (the former on the maintenance-production
interface in a large enterprise, the latter majorly focused in production and
setups in a small and medium enterprise), show the significant benefits in
terms of time, costs and process quality, thus validating the approach
proposed. This research shows that a human-centric and knowledge-driven
approach can drive the performance of Industry 4.0 initiatives and lead a Smart
Factory towards its full potential.","['Francesco Longo', 'Letizia Nicoletti', 'Antonio Padovano']",2022-05-30T16:48:51Z,http://arxiv.org/abs/2206.03268v1,['cs.CY']
"Surround-View Cameras based Holistic Visual Perception for Automated
  Driving","The formation of eyes led to the big bang of evolution. The dynamics changed
from a primitive organism waiting for the food to come into contact for eating
food being sought after by visual sensors. The human eye is one of the most
sophisticated developments of evolution, but it still has defects. Humans have
evolved a biological perception algorithm capable of driving cars, operating
machinery, piloting aircraft, and navigating ships over millions of years.
Automating these capabilities for computers is critical for various
applications, including self-driving cars, augmented reality, and architectural
surveying. Near-field visual perception in the context of self-driving cars can
perceive the environment in a range of $0-10$ meters and 360{\deg} coverage
around the vehicle. It is a critical decision-making component in the
development of safer automated driving. Recent advances in computer vision and
deep learning, in conjunction with high-quality sensors such as cameras and
LiDARs, have fueled mature visual perception solutions. Until now, far-field
perception has been the primary focus. Another significant issue is the limited
processing power available for developing real-time applications. Because of
this bottleneck, there is frequently a trade-off between performance and
run-time efficiency. We concentrate on the following issues in order to address
them: 1) Developing near-field perception algorithms with high performance and
low computational complexity for various visual perception tasks such as
geometric and semantic tasks using convolutional neural networks. 2) Using
Multi-Task Learning to overcome computational bottlenecks by sharing initial
convolutional layers between tasks and developing optimization strategies that
balance tasks.",['Varun Ravi Kumar'],2022-06-11T14:51:30Z,http://arxiv.org/abs/2206.05542v1,['cs.CV']
"Predicting Human Performance in Vertical Hierarchical Menu Selection in
  Immersive AR Using Hand-gesture and Head-gaze","There are currently limited guidelines on designing user interfaces (UI) for
immersive augmented reality (AR) applications. Designers must reflect on their
experience designing UI for desktop and mobile applications and conjecture how
a UI will influence AR users' performance. In this work, we introduce a
predictive model for determining users' performance for a target UI without the
subsequent involvement of participants in user studies. The model is trained on
participants' responses to objective performance measures such as consumed
endurance (CE) and pointing time (PT) using hierarchical drop-down menus. Large
variability in the depth and context of the menus is ensured by randomly and
dynamically creating the hierarchical drop-down menus and associated user tasks
from words contained in the lexical database WordNet. Subjective performance
bias is reduced by incorporating the users' non-verbal standard performance
WAIS-IV during the model training. The semantic information of the menu is
encoded using the Universal Sentence Encoder. We present the results of a user
study that demonstrates that the proposed predictive model achieves high
accuracy in predicting the CE on hierarchical menus of users with various
cognitive abilities. To the best of our knowledge, this is the first work on
predicting CE in designing UI for immersive AR applications.","['Majid Pourmemar', 'Yashas Joshi', 'Charalambos Poullis']",2022-06-19T20:13:08Z,http://arxiv.org/abs/2206.09480v1,"['cs.HC', 'cs.LG']"
"Implicit Channel Learning for Machine Learning Applications in 6G
  Wireless Networks","With the deployment of the fifth generation (5G) wireless systems gathering
momentum across the world, possible technologies for 6G are under active
research discussions. In particular, the role of machine learning (ML) in 6G is
expected to enhance and aid emerging applications such as virtual and augmented
reality, vehicular autonomy, and computer vision. This will result in large
segments of wireless data traffic comprising image, video and speech. The ML
algorithms process these for classification/recognition/estimation through the
learning models located on cloud servers. This requires wireless transmission
of data from edge devices to the cloud server. Channel estimation, handled
separately from recognition step, is critical for accurate learning
performance. Toward combining the learning for both channel and the ML data, we
introduce implicit channel learning to perform the ML tasks without estimating
the wireless channel. Here, the ML models are trained with channel-corrupted
datasets in place of nominal data. Without channel estimation, the proposed
approach exhibits approximately 60% improvement in image and speech
classification tasks for diverse scenarios such as millimeter wave and IEEE
802.11p vehicular channels.","['Ahmet M. Elbir', 'Wei Shi', 'Kumar Vijay Mishra', 'Anastasios K. Papazafeiropoulos', 'Symeon Chatzinotas']",2022-06-24T07:45:10Z,http://arxiv.org/abs/2206.12127v1,"['eess.SP', 'cs.AI', 'cs.IT', 'cs.LG', 'math.IT']"
"Microservice Architecture Reconstruction and Visualization Techniques: A
  Review","Microservice system solutions are driving digital transformation; however,
fundamental tools and system perspectives are missing to better observe,
understand, and manage these systems, their properties, and their dependencies.
Microservices architecture leads towards decentralization, which implies many
advantages to system operation; it, however, brings challenges to their
development. Microservice systems often lack a system-centric perspective that
would help engineers better cope with system evolution and quality assessment.
In this work, we explored microservice-specific architecture reconstruction
based on static analysis. Such reconstruction typically results in system
models to visualize selected system-centric perspectives. Conventional models
involve 2D methods; however, these methods are limited in utility when services
proliferate. We considered various architectural perspectives relevant to
microservices and assessed the relevancy of the traditional method, comparing
it to alternative data visualization using 3D space. As a representative of the
3D method, we considered a 3D graph model presented in augmented reality. To
begin testing the feasibility of deriving such perspectives from microservice
systems, we developed and implemented prototype tools for software architecture
reconstruction and visualization of compared perspectives. Using these
prototypes, we performed a small user study with software practitioners to
highlight the potentials and limitations of these innovative visualizations
used for common practitioner reasoning and tasks.","['Tomas Cerny', 'Amr S. Abdelfattah', 'Vincent Bushong', 'Abdullah Al Maruf', 'Davide Taibi']",2022-07-06T21:45:33Z,http://arxiv.org/abs/2207.02988v2,['cs.SE']
Learning to Separate Voices by Spatial Regions,"We consider the problem of audio voice separation for binaural applications,
such as earphones and hearing aids. While today's neural networks perform
remarkably well (separating $4+$ sources with 2 microphones) they assume a
known or fixed maximum number of sources, K. Moreover, today's models are
trained in a supervised manner, using training data synthesized from generic
sources, environments, and human head shapes.
  This paper intends to relax both these constraints at the expense of a slight
alteration in the problem definition. We observe that, when a received mixture
contains too many sources, it is still helpful to separate them by region,
i.e., isolating signal mixtures from each conical sector around the user's
head. This requires learning the fine-grained spatial properties of each
region, including the signal distortions imposed by a person's head. We propose
a two-stage self-supervised framework in which overheard voices from earphones
are pre-processed to extract relatively clean personalized signals, which are
then used to train a region-wise separation model. Results show promising
performance, underscoring the importance of personalization over a generic
supervised approach. (audio samples available at our project website:
https://uiuc-earable-computing.github.io/binaural/. We believe this result
could help real-world applications in selective hearing, noise cancellation,
and audio augmented reality.","['Zhongweiyang Xu', 'Romit Roy Choudhury']",2022-07-09T06:25:01Z,http://arxiv.org/abs/2207.04203v2,"['cs.SD', 'cs.LG', 'cs.MM', 'eess.AS']"
MeshLoc: Mesh-Based Visual Localization,"Visual localization, i.e., the problem of camera pose estimation, is a
central component of applications such as autonomous robots and augmented
reality systems. A dominant approach in the literature, shown to scale to large
scenes and to handle complex illumination and seasonal changes, is based on
local features extracted from images. The scene representation is a sparse
Structure-from-Motion point cloud that is tied to a specific local feature.
Switching to another feature type requires an expensive feature matching step
between the database images used to construct the point cloud. In this work, we
thus explore a more flexible alternative based on dense 3D meshes that does not
require features matching between database images to build the scene
representation. We show that this approach can achieve state-of-the-art
results. We further show that surprisingly competitive results can be obtained
when extracting features on renderings of these meshes, without any neural
rendering stage, and even when rendering raw scene geometry without color or
texture. Our results show that dense 3D model-based representations are a
promising alternative to existing representations and point to interesting and
challenging directions for future research.","['Vojtech Panek', 'Zuzana Kukelova', 'Torsten Sattler']",2022-07-21T21:21:10Z,http://arxiv.org/abs/2207.10762v2,"['cs.CV', 'I.2.10; I.4.9']"
"OpenFilter: A Framework to Democratize Research Access to Social Media
  AR Filters","Augmented Reality or AR filters on selfies have become very popular on social
media platforms for a variety of applications, including marketing,
entertainment and aesthetics. Given the wide adoption of AR face filters and
the importance of faces in our social structures and relations, there is
increased interest by the scientific community to analyze the impact of such
filters from a psychological, artistic and sociological perspective. However,
there are few quantitative analyses in this area mainly due to a lack of
publicly available datasets of facial images with applied AR filters. The
proprietary, close nature of most social media platforms does not allow users,
scientists and practitioners to access the code and the details of the
available AR face filters. Scraping faces from these platforms to collect data
is ethically unacceptable and should, therefore, be avoided in research. In
this paper, we present OpenFilter, a flexible framework to apply AR filters
available in social media platforms on existing large collections of human
faces. Moreover, we share FairBeauty and B-LFW, two beautified versions of the
publicly available FairFace and LFW datasets and we outline insights derived
from the analysis of these beautified datasets.","['Piera Riccio', 'Bill Psomas', 'Francesco Galati', 'Francisco Escolano', 'Thomas Hofmann', 'Nuria Oliver']",2022-07-19T17:05:25Z,http://arxiv.org/abs/2207.12319v3,"['cs.CV', 'cs.AI', 'I.4.9']"
Student Research Abstract: Microservices-based Systems Visualization,"The evolution of decentralized microservice-based systems is challenging.
These challenges are classified into static and dynamic categories. Regarding
the static perspective, documenting and visualizing the fluid application
topology is something few have been able to accomplish. Building an
architecture map of services design is a complicated task in its interpretation
rather than construction. Therefore, the system-centric and up-to-date view
became essential for such distributed systems. The dynamic perspective
considers the process of investigation and service path detection. Therefore
performing root cause analysis is a burdening task; such that tracing data is
needed to be put in the right context to facilitate the investigation.
Moreover, visualizing these traces over the traditional visualization
techniques couldn't be feasible with the large number of microservices involved
in the system. This paper proposes a visualization concept for
microservices-based systems using the Augmented Reality (AR) technique, which
merges these static and dynamic behaviors into a single centric view. In
addition, we challenge the difficulty related to tracing and debugging an issue
in such distributed systems. This concept is designed to work as a dynamic
documentation and traceability platform for these systems. A proof of concept
and a research study are implemented to show the viability and success of this
proposal.",['Amr S. Abdelfattah'],2022-07-23T21:05:20Z,http://arxiv.org/abs/2207.12998v1,['cs.SE']
"AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
  Sensing","Today's Mixed Reality head-mounted displays track the user's head pose in
world space as well as the user's hands for interaction in both Augmented
Reality and Virtual Reality scenarios. While this is adequate to support user
input, it unfortunately limits users' virtual representations to just their
upper bodies. Current systems thus resort to floating avatars, whose limitation
is particularly evident in collaborative settings. To estimate full-body poses
from the sparse input sources, prior work has incorporated additional trackers
and sensors at the pelvis or lower body, which increases setup complexity and
limits practical application in mobile settings. In this paper, we present
AvatarPoser, the first learning-based method that predicts full-body poses in
world coordinates using only motion input from the user's head and hands. Our
method builds on a Transformer encoder to extract deep features from the input
signals and decouples global motion from the learned local joint orientations
to guide pose estimation. To obtain accurate full-body motions that resemble
motion capture animations, we refine the arm joints' positions using an
optimization routine with inverse kinematics to match the original tracking
input. In our evaluation, AvatarPoser achieved new state-of-the-art results in
evaluations on large motion capture datasets (AMASS). At the same time, our
method's inference speed supports real-time operation, providing a practical
interface to support holistic avatar control and representation for Metaverse
applications.","['Jiaxi Jiang', 'Paul Streli', 'Huajian Qiu', 'Andreas Fender', 'Larissa Laich', 'Patrick Snape', 'Christian Holz']",2022-07-27T20:52:39Z,http://arxiv.org/abs/2207.13784v1,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.HC', '68T07, 68T45, 68U01', 'I.2; I.3; I.4; I.5']"
RAZE: Region Guided Self-Supervised Gaze Representation Learning,"Automatic eye gaze estimation is an important problem in vision based
assistive technology with use cases in different emerging topics such as
augmented reality, virtual reality and human-computer interaction. Over the
past few years, there has been an increasing interest in unsupervised and
self-supervised learning paradigms as it overcomes the requirement of large
scale annotated data. In this paper, we propose RAZE, a Region guided
self-supervised gAZE representation learning framework which leverage from
non-annotated facial image data. RAZE learns gaze representation via auxiliary
supervision i.e. pseudo-gaze zone classification where the objective is to
classify visual field into different gaze zones (i.e. left, right and center)
by leveraging the relative position of pupil-centers. Thus, we automatically
annotate pseudo gaze zone labels of 154K web-crawled images and learn feature
representations via `Ize-Net' framework. `Ize-Net' is a capsule layer based CNN
architecture which can efficiently capture rich eye representation. The
discriminative behaviour of the feature representation is evaluated on four
benchmark datasets: CAVE, TabletGaze, MPII and RT-GENE. Additionally, we
evaluate the generalizability of the proposed network on two other downstream
task (i.e. driver gaze estimation and visual attention estimation) which
demonstrate the effectiveness of the learnt eye gaze representation.","['Neeru Dubey', 'Shreya Ghosh', 'Abhinav Dhall']",2022-08-04T06:23:49Z,http://arxiv.org/abs/2208.02485v2,['cs.CV']
"MonoSIM: Simulating Learning Behaviors of Heterogeneous Point Cloud
  Object Detectors for Monocular 3D Object Detection","Monocular 3D object detection is a fundamental but very important task to
many applications including autonomous driving, robotic grasping and augmented
reality. Existing leading methods tend to estimate the depth of the input image
first, and detect the 3D object based on point cloud. This routine suffers from
the inherent gap between depth estimation and object detection. Besides, the
prediction error accumulation would also affect the performance. In this paper,
a novel method named MonoSIM is proposed. The insight behind introducing
MonoSIM is that we propose to simulate the feature learning behaviors of a
point cloud based detector for monocular detector during the training period.
Hence, during inference period, the learned features and prediction would be
similar to the point cloud based detector as possible. To achieve it, we
propose one scene-level simulation module, one RoI-level simulation module and
one response-level simulation module, which are progressively used for the
detector's full feature learning and prediction pipeline. We apply our method
to the famous M3D-RPN detector and CaDDN detector, conducting extensive
experiments on KITTI and Waymo Open datasets. Results show that our method
consistently improves the performance of different monocular detectors for a
large margin without changing their network architectures. Our codes will be
publicly available at
https://github.com/sunh18/MonoSIM}{https://github.com/sunh18/MonoSIM.","['Han Sun', 'Zhaoxin Fan', 'Zhenbo Song', 'Zhicheng Wang', 'Kejian Wu', 'Jianfeng Lu']",2022-08-19T16:57:11Z,http://arxiv.org/abs/2208.09446v2,['cs.CV']
"An Indoor Localization Dataset and Data Collection Framework with High
  Precision Position Annotation","We introduce a novel technique and an associated high resolution dataset that
aims to precisely evaluate wireless signal based indoor positioning algorithms.
The technique implements an augmented reality (AR) based positioning system
that is used to annotate the wireless signal parameter data samples with high
precision position data. We track the position of a practical and low cost
navigable setup of cameras and a Bluetooth Low Energy (BLE) beacon in an area
decorated with AR markers. We maximize the performance of the AR-based
localization by using a redundant number of markers. Video streams captured by
the cameras are subjected to a series of marker recognition, subset selection
and filtering operations to yield highly precise pose estimations. Our results
show that we can reduce the positional error of the AR localization system to a
rate under 0.05 meters. The position data are then used to annotate the BLE
data that are captured simultaneously by the sensors stationed in the
environment, hence, constructing a wireless signal data set with the ground
truth, which allows a wireless signal based localization system to be evaluated
accurately.","['F. Serhan Daniş', 'A. Teoman Naskali', 'A. Taylan Cemgil', 'Cem Ersoy']",2022-09-06T07:41:11Z,http://arxiv.org/abs/2209.02270v1,"['cs.LG', 'cs.CV', 'cs.NI', 'cs.SY', 'eess.SY']"
"Metaverse for Healthcare: A Survey on Potential Applications, Challenges
  and Future Directions","The rapid progress in digitalization and automation have led to an
accelerated growth in healthcare, generating novel models that are creating new
channels for rendering treatment with reduced cost. The Metaverse is an
emerging technology in the digital space which has huge potential in
healthcare, enabling realistic experiences to the patients as well as the
medical practitioners. The Metaverse is a confluence of multiple enabling
technologies such as artificial intelligence, virtual reality, augmented
reality, internet of medical devices, robotics, quantum computing, etc. through
which new directions for providing quality healthcare treatment and services
can be explored. The amalgamation of these technologies ensures immersive,
intimate and personalized patient care. It also provides adaptive intelligent
solutions that eliminates the barriers between healthcare providers and
receivers. This article provides a comprehensive review of the Metaverse for
healthcare, emphasizing on the state of the art, the enabling technologies for
adopting the Metaverse for healthcare, the potential applications and the
related projects. The issues in the adaptation of the Metaverse for healthcare
applications are also identified and the plausible solutions are highlighted as
part of future research directions.","['Rajeswari Chengoden', 'Nancy Victor', 'Thien Huynh-The', 'Gokul Yenduri', 'Rutvij H. Jhaveri', 'Mamoun Alazab', 'Sweta Bhattacharya', 'Pawan Hegde', 'Praveen Kumar Reddy Maddikunta', 'Thippa Reddy Gadekallu']",2022-09-09T07:40:11Z,http://arxiv.org/abs/2209.04160v1,['cs.AI']
"An Exploration of Hands-free Text Selection for Virtual Reality
  Head-Mounted Displays","Hand-based interaction, such as using a handheld controller or making hand
gestures, has been widely adopted as the primary method for interacting with
both virtual reality (VR) and augmented reality (AR) head-mounted displays
(HMDs). In contrast, hands-free interaction avoids the need for users' hands
and although it can afford additional benefits, there has been limited research
in exploring and evaluating hands-free techniques for these HMDs. As VR HMDs
become ubiquitous, people will need to do text editing, which requires
selecting text segments. Similar to hands-free interaction, text selection is
underexplored. This research focuses on both, text selection via hands-free
interaction. Our exploration involves a user study with 24 participants to
investigate the performance, user experience, and workload of three hands-free
selection mechanisms (Dwell, Blink, Voice) to complement head-based pointing.
Results indicate that Blink outperforms Dwell and Voice in completion time.
Users' subjective feedback also shows that Blink is the preferred technique for
text selection. This work is the first to explore hands-free interaction for
text selection in VR HMDs. Our results provide a solid platform for further
research in this important area.","['Xuanru Meng', 'Wenge Xu', 'Hai-Ning Liang']",2022-09-14T09:25:54Z,http://arxiv.org/abs/2209.06825v2,"['cs.HC', 'H.5.1; I.3.7']"
"MAGES 4.0: Accelerating the world's transition to VR training and
  democratizing the authoring of the medical metaverse","In this work, we propose MAGES 4.0, a novel Software Development Kit (SDK) to
accelerate the creation of collaborative medical training applications in
VR/AR. Our solution is essentially a low-code metaverse authoring platform for
developers to rapidly prototype high-fidelity and high-complexity medical
simulations. MAGES breaks the authoring boundaries across extended reality,
since networked participants can also collaborate using different
virtual/augmented reality as well as mobile and desktop devices, in the same
metaverse world. With MAGES we propose an upgrade to the outdated 150-year-old
master-apprentice medical training model. Our platform incorporates, in a
nutsell, the following novelties: a) 5G edge-cloud remote rendering and physics
dissection layer, b) realistic real-time simulation of organic tissues as
soft-bodies under 10ms, c) a highly realistic cutting and tearing algorithm, d)
neural network assessment for user profiling and, e) a VR recorder to record
and replay or debrief the training simulation from any perspective.","['Paul Zikas', 'Antonis Protopsaltis', 'Nick Lydatakis', 'Mike Kentros', 'Stratos Geronikolakis', 'Steve Kateros', 'Manos Kamarianakis', 'Giannis Evangelou', 'Achilleas Filippidis', 'Eleni Grigoriou', 'Dimitris Angelis', 'Michail Tamiolakis', 'Michael Dodis', 'George Kokiadis', 'John Petropoulos', 'Maria Pateraki', 'George Papagiannakis']",2022-09-19T08:10:35Z,http://arxiv.org/abs/2209.08819v2,"['cs.GR', '97R60, 97U50, 97Q70, 97Q60', 'I.3.0; I.3.7; J.3']"
"A direct time-of-flight image sensor with in-pixel surface detection and
  dynamic vision","3D flash LIDAR is an alternative to the traditional scanning LIDAR systems,
promising precise depth imaging in a compact form factor, and free of moving
parts, for applications such as self-driving cars, robotics and augmented
reality (AR). Typically implemented using single-photon, direct time-of-flight
(dToF) receivers in image sensor format, the operation of the devices can be
hindered by the large number of photon events needing to be processed and
compressed in outdoor scenarios, limiting frame rates and scalability to larger
arrays. We here present a 64x32 pixel (256x128 SPAD) dToF imager that overcomes
these limitations by using pixels with embedded histogramming, which lock onto
and track the return signal. This reduces the size of output data frames
considerably, enabling maximum frame rates in the 10 kFPS range or 100 kFPS for
direct depth readings. The sensor offers selective readout of pixels detecting
surfaces, or those sensing motion, leading to reduced power consumption and
off-chip processing requirements. We demonstrate the application of the sensor
in mid-range LIDAR.","['Istvan Gyongy', 'Ahmet T. Erdogan', 'Neale A. W. Dutton', 'Germán Mora Martín', 'Alistair Gorman', 'Hanning Mai', 'Francesco Mattioli Della Rocca', 'Robert K. Henderson']",2022-09-23T14:38:00Z,http://arxiv.org/abs/2209.11772v1,"['cs.CV', 'eess.IV', 'physics.ins-det']"
Realistic Hair Synthesis with Generative Adversarial Networks,"Recent successes in generative modeling have accelerated studies on this
subject and attracted the attention of researchers. One of the most important
methods used to achieve this success is Generative Adversarial Networks (GANs).
It has many application areas such as; virtual reality (VR), augmented reality
(AR), super resolution, image enhancement. Despite the recent advances in hair
synthesis and style transfer using deep learning and generative modelling, due
to the complex nature of hair still contains unsolved challenges. The methods
proposed in the literature to solve this problem generally focus on making
high-quality hair edits on images. In this thesis, a generative adversarial
network method is proposed to solve the hair synthesis problem. While
developing this method, it is aimed to achieve real-time hair synthesis while
achieving visual outputs that compete with the best methods in the literature.
The proposed method was trained with the FFHQ dataset and then its results in
hair style transfer and hair reconstruction tasks were evaluated. The results
obtained in these tasks and the operating time of the method were compared with
MichiGAN, one of the best methods in the literature. The comparison was made at
a resolution of 128x128. As a result of the comparison, it has been shown that
the proposed method achieves competitive results with MichiGAN in terms of
realistic hair synthesis, and performs better in terms of operating time.","['Muhammed Pektas', 'Aybars Ugur']",2022-09-13T11:48:26Z,http://arxiv.org/abs/2209.12875v1,"['cs.GR', 'cs.CV']"
"Mobile Edge Computing, Metaverse, 6G Wireless Communications, Artificial
  Intelligence, and Blockchain: Survey and Their Convergence","With the advances of the Internet of Things (IoT) and 5G/6G wireless
communications, the paradigms of mobile computing have developed dramatically
in recent years, from centralized mobile cloud computing to distributed fog
computing and mobile edge computing (MEC). MEC pushes compute-intensive
assignments to the edge of the network and brings resources as close to the
endpoints as possible, addressing the shortcomings of mobile devices with
regard to storage space, resource optimisation, computational performance and
efficiency. Compared to cloud computing, as the distributed and closer
infrastructure, the convergence of MEC with other emerging technologies,
including the Metaverse, 6G wireless communications, artificial intelligence
(AI), and blockchain, also solves the problems of network resource allocation,
more network load as well as latency requirements. Accordingly, this paper
investigates the computational paradigms used to meet the stringent
requirements of modern applications. The application scenarios of MEC in mobile
augmented reality (MAR) are provided. Furthermore, this survey presents the
motivation of MEC-based Metaverse and introduces the applications of MEC to the
Metaverse. Particular emphasis is given on a set of technical fusions mentioned
above, e.g., 6G with MEC paradigm, MEC strengthened by blockchain, etc.","['Yitong Wang', 'Jun Zhao']",2022-09-28T14:54:06Z,http://arxiv.org/abs/2209.14147v1,"['cs.DC', 'cs.AI', 'cs.LG', 'cs.SE']"
"Coalitional Game-Theoretical Approach to Coinvestment with Application
  to Edge Computing","We propose in this paper a coinvestment plan between several stakeholders of
different types, namely a physical network owner, operating network nodes, e.g.
a network operator or a tower company, and a set of service providers willing
to use these resources to provide services as video streaming, augmented
reality, autonomous driving assistance, etc. One such scenario is that of
deployment of Edge Computing resources.
  Indeed, although the latter technology is ready, the high Capital Expenditure
(CAPEX) cost of such resources is the barrier to its deployment. For this
reason, a solid economical framework to guide the investment and the returns of
the stakeholders is key to solve this issue. We formalize the coinvestment
framework using coalitional game theory. We provide a solution to calculate how
to divide the profits and costs among the stakeholders, taking into account
their characteristics: traffic load, revenues, utility function. We prove that
it is always possible to form the grand coalition composed of all the
stakeholders, by showing that our game is convex. We derive the payoff of the
stakeholders using the Shapley value concept, and elaborate on some properties
of our game. We show our solution in simulation.","['Rosario Patanè', 'Andrea Araldo', 'Tijani Chahed', 'Diego Kiedanski', 'Daniel Kofman']",2022-09-30T23:58:19Z,http://arxiv.org/abs/2210.00145v1,"['cs.GT', 'cs.NI', '91A80,', 'C.2']"
"SPARC: Sparse Render-and-Compare for CAD model alignment in a single RGB
  image","Estimating 3D shapes and poses of static objects from a single image has
important applications for robotics, augmented reality and digital content
creation. Often this is done through direct mesh predictions which produces
unrealistic, overly tessellated shapes or by formulating shape prediction as a
retrieval task followed by CAD model alignment. Directly predicting CAD model
poses from 2D image features is difficult and inaccurate. Some works, such as
ROCA, regress normalised object coordinates and use those for computing poses.
While this can produce more accurate pose estimates, predicting normalised
object coordinates is susceptible to systematic failure. Leveraging efficient
transformer architectures we demonstrate that a sparse, iterative,
render-and-compare approach is more accurate and robust than relying on
normalised object coordinates. For this we combine 2D image information
including sparse depth and surface normal values which we estimate directly
from the image with 3D CAD model information in early fusion. In particular, we
reproject points sampled from the CAD model in an initial, random pose and
compute their depth and surface normal values. This combined information is the
input to a pose prediction network, SPARC-Net which we train to predict a 9 DoF
CAD model pose update. The CAD model is reprojected again and the next pose
update is predicted. Our alignment procedure converges after just 3 iterations,
improving the state-of-the-art performance on the challenging real-world
dataset ScanNet from 25.0% to 31.8% instance alignment accuracy. Code will be
released at https://github.com/florianlanger/SPARC .","['Florian Langer', 'Gwangbin Bae', 'Ignas Budvytis', 'Roberto Cipolla']",2022-10-03T16:02:10Z,http://arxiv.org/abs/2210.01044v1,['cs.CV']
Force-Aware Interface via Electromyography for Natural VR/AR Interaction,"While tremendous advances in visual and auditory realism have been made for
virtual and augmented reality (VR/AR), introducing a plausible sense of
physicality into the virtual world remains challenging. Closing the gap between
real-world physicality and immersive virtual experience requires a closed
interaction loop: applying user-exerted physical forces to the virtual
environment and generating haptic sensations back to the users. However,
existing VR/AR solutions either completely ignore the force inputs from the
users or rely on obtrusive sensing devices that compromise user experience.
  By identifying users' muscle activation patterns while engaging in VR/AR, we
design a learning-based neural interface for natural and intuitive force
inputs. Specifically, we show that lightweight electromyography sensors,
resting non-invasively on users' forearm skin, inform and establish a robust
understanding of their complex hand activities. Fuelled by a
neural-network-based model, our interface can decode finger-wise forces in
real-time with 3.3% mean error, and generalize to new users with little
calibration. Through an interactive psychophysical study, we show that human
perception of virtual objects' physical properties, such as stiffness, can be
significantly enhanced by our interface. We further demonstrate that our
interface enables ubiquitous control via finger tapping. Ultimately, we
envision our findings to push forward research towards more realistic
physicality in future VR/AR.","['Yunxiang Zhang', 'Benjamin Liang', 'Boyuan Chen', 'Paul Torrens', 'S. Farokh Atashzar', 'Dahua Lin', 'Qi Sun']",2022-10-03T20:51:25Z,http://arxiv.org/abs/2210.01225v1,"['cs.HC', 'cs.GR', 'cs.LG']"
"A Novel Light Field Coding Scheme Based on Deep Belief Network &
  Weighted Binary Images for Additive Layered Displays","Light-field displays create an immersive experience by providing binocular
depth sensation and motion parallax. Stacking light attenuating layers is one
approach to implement a light field display with a broader depth of field, wide
viewing angles and high resolution. Due to the transparent holographic optical
element (HOE) layers, additive layered displays can be integrated into
augmented reality (AR) wearables to overlay virtual objects onto the real
world, creating a seamless mixed reality (XR) experience. This paper proposes a
novel framework for light field representation and coding that utilizes Deep
Belief Network (DBN) and weighted binary images suitable for additive layered
displays. The weighted binary representation of layers makes the framework more
flexible for adaptive bitrate encoding. The framework effectively captures
intrinsic redundancies in the light field data, and thus provides a scalable
solution for light field coding suitable for XR display applications. The
latent code is encoded by H.265 codec generating a rate-scalable bit-stream. We
achieve adaptive bitrate decoding by varying the number of weighted binary
images and the H.265 quantization parameter, while maintaining an optimal
reconstruction quality. The framework is tested on real and synthetic benchmark
datasets, and the results validate the rate-scalable property of the proposed
scheme.","['Sally Khaidem', 'Mansi Sharma']",2022-10-04T08:18:06Z,http://arxiv.org/abs/2210.01447v2,"['cs.CV', 'eess.IV']"
"Integrating Digital Twin and Advanced Intelligent Technologies to
  Realize the Metaverse","The advances in Artificial Intelligence (AI) have led to technological
advancements in a plethora of domains. Healthcare, education, and smart city
services are now enriched with AI capabilities. These technological
advancements would not have been realized without the assistance of fast,
secure, and fault-tolerant communication media. Traditional processing,
communication and storage technologies cannot maintain high levels of
scalability and user experience for immersive services. The metaverse is an
immersive three-dimensional (3D) virtual world that integrates fantasy and
reality into a virtual environment using advanced virtual reality (VR) and
augmented reality (AR) devices. Such an environment is still being developed
and requires extensive research in order for it to be realized to its highest
attainable levels. In this article, we discuss some of the key issues required
in order to attain realization of metaverse services. We propose a framework
that integrates digital twin (DT) with other advanced technologies such as the
sixth generation (6G) communication network, blockchain, and AI, to maintain
continuous end-to-end metaverse services. This article also outlines
requirements for an integrated, DT-enabled metaverse framework and provides a
look ahead into the evolving topic.","['Moayad Aloqaily', 'Ouns Bouachir', 'Fakhri Karray', 'Ismaeel Al Ridhawi', 'Abdulmotaleb El Saddik']",2022-10-03T17:02:58Z,http://arxiv.org/abs/2210.04606v1,"['cs.HC', 'cs.AI']"
CASAPose: Class-Adaptive and Semantic-Aware Multi-Object Pose Estimation,"Applications in the field of augmented reality or robotics often require
joint localisation and 6D pose estimation of multiple objects. However, most
algorithms need one network per object class to be trained in order to provide
the best results. Analysing all visible objects demands multiple inferences,
which is memory and time-consuming. We present a new single-stage architecture
called CASAPose that determines 2D-3D correspondences for pose estimation of
multiple different objects in RGB images in one pass. It is fast and memory
efficient, and achieves high accuracy for multiple objects by exploiting the
output of a semantic segmentation decoder as control input to a keypoint
recognition decoder via local class-adaptive normalisation. Our new
differentiable regression of keypoint locations significantly contributes to a
faster closing of the domain gap between real test and synthetic training data.
We apply segmentation-aware convolutions and upsampling operations to increase
the focus inside the object mask and to reduce mutual interference of occluding
objects. For each inserted object, the network grows by only one output
segmentation map and a negligible number of parameters. We outperform
state-of-the-art approaches in challenging multi-object scenes with
inter-object occlusion and synthetic training.","['Niklas Gard', 'Anna Hilsmann', 'Peter Eisert']",2022-10-11T10:20:01Z,http://arxiv.org/abs/2210.05318v3,['cs.CV']
"A Complementary Framework for Human-Robot Collaboration with a Mixed
  AR-Haptic Interface","There is invariably a trade-off between safety and efficiency for
collaborative robots (cobots) in human-robot collaborations. Robots that
interact minimally with humans can work with high speed and accuracy but cannot
adapt to new tasks or respond to unforeseen changes, whereas robots that work
closely with humans can but only by becoming passive to humans, meaning that
their main tasks suspended and efficiency compromised. Accordingly, this paper
proposes a new complementary framework for human-robot collaboration that
balances the safety of humans and the efficiency of robots. In this framework,
the robot carries out given tasks using a vision-based adaptive controller, and
the human expert collaborates with the robot in the null space. Such a
decoupling drives the robot to deal with existing issues in task space (e.g.,
uncalibrated camera, limited field of view) and in null space (e.g., joint
limits) by itself while allowing the expert to adjust the configuration of the
robot body to respond to unforeseen changes (e.g., sudden invasion, change of
environment) without affecting the robot's main task. Additionally, the robot
can simultaneously learn the expert's demonstration in task space and null
space beforehand with dynamic movement primitives (DMP). Therefore, an expert's
knowledge and a robot's capability are both explored and complementary. Human
demonstration and involvement are enabled via a mixed interaction interface,
i.e., augmented reality (AR) and haptic devices. The stability of the
closed-loop system is rigorously proved with Lyapunov methods. Experimental
results in various scenarios are presented to illustrate the performance of the
proposed method.","['Xiangjie Yan', 'Yongpeng Jiang', 'Chen Chen', 'Leiliang Gong', 'Ming Ge', 'Tao Zhang', 'Xiang Li']",2022-10-12T08:18:24Z,http://arxiv.org/abs/2210.06003v1,['cs.RO']
"""Seeing the Faces Is So Important"" -- Experiences From Online Team
  Meetings on Commercial Virtual Reality Platforms","During the Covid-19 pandemic, online meetings became common for daily
teamwork in the home office. To understand the opportunities and challenges of
meeting in virtual reality (VR) compared to video conferences, we conducted the
weekly team meetings of our human-computer interaction research lab on five
off-the-shelf online meeting platforms over four months. After each of the 12
meetings, we asked the participants (N = 32) to share their experiences,
resulting in 200 completed online questionnaires. We evaluated the ratings of
the overall meeting experience and conducted an exploratory factor analysis of
the quantitative data to compare VR meetings and video calls in terms of
meeting involvement and co-presence. In addition, a thematic analysis of the
qualitative data revealed genuine insights covering five themes: spatial
aspects, meeting atmosphere, expression of emotions, meeting productivity, and
user needs. We reflect on our findings gained under authentic working
conditions, derive lessons learned for running successful team meetings in VR
supporting different kinds of meeting formats, and discuss the team's long-term
platform choice.","['Michael Bonfert', 'Anke V. Reinschluessel', 'Susanne Putze', 'Yenchin Lai', 'Dmitry Alexandrovsky', 'Rainer Malaka', 'Tanja Döring']",2022-10-12T13:19:26Z,http://arxiv.org/abs/2210.06190v2,"['cs.HC', 'H.5.3; H.5.2']"
An Efficient FPGA Accelerator for Point Cloud,"Deep learning-based point cloud processing plays an important role in various
vision tasks, such as autonomous driving, virtual reality (VR), and augmented
reality (AR). The submanifold sparse convolutional network (SSCN) has been
widely used for the point cloud due to its unique advantages in terms of visual
results. However, existing convolutional neural network accelerators suffer
from non-trivial performance degradation when employed to accelerate SSCN
because of the extreme and unstructured sparsity, and the complex computational
dependency between the sparsity of the central activation and the neighborhood
ones. In this paper, we propose a high performance FPGA-based accelerator for
SSCN. Firstly, we develop a zero removing strategy to remove the coarse-grained
redundant regions, thus significantly improving computational efficiency.
Secondly, we propose a concise encoding scheme to obtain the matching
information for efficient point-wise multiplications. Thirdly, we develop a
sparse data matching unit and a computing core based on the proposed encoding
scheme, which can convert the irregular sparse operations into regular
multiply-accumulate operations. Finally, an efficient hardware architecture for
the submanifold sparse convolutional layer is developed and implemented on the
Xilinx ZCU102 field-programmable gate array board, where the 3D submanifold
sparse U-Net is taken as the benchmark. The experimental results demonstrate
that our design drastically improves computational efficiency, and can
dramatically improve the power efficiency by 51 times compared to GPU.","['Zilun Wang', 'Wendong Mao', 'Peixiang Yang', 'Zhongfeng Wang', 'Jun Lin']",2022-10-14T13:34:00Z,http://arxiv.org/abs/2210.07803v1,"['eess.SP', 'cs.AR', 'eess.IV']"
"Artificial Intelligence and Innovation to Reduce the Impact of Extreme
  Weather Events on Sustainable Production","Frequent occurrences of extreme weather events substantially impact the lives
of the less privileged in our societies, particularly in agriculture-inclined
economies. The unpredictability of extreme fires, floods, drought, cyclones,
and others endangers sustainable production and life on land (SDG goal 15),
which translates into food insecurity and poorer populations. Fortunately,
modern technologies such as Artificial Intelligent (AI), the Internet of Things
(IoT), blockchain, 3D printing, and virtual and augmented reality (VR and AR)
are promising to reduce the risk and impact of extreme weather in our
societies. However, research directions on how these technologies could help
reduce the impact of extreme weather are unclear. This makes it challenging to
emploring digital technologies within the spheres of extreme weather. In this
paper, we employed the Delphi Best Worst method and Machine learning approaches
to identify and assess the push factors of technology. The BWM evaluation
revealed that predictive nature was AI's most important criterion and role,
while the mass-market potential was the less important criterion. Based on this
outcome, we tested the predictive ability of machine elarning on a publilcly
available dataset to affrm the predictive rols of AI. We presented the
managerial and methodological implications of the study, which are crucial for
research and practice. The methodology utilized in this study could aid
decision-makers in devising strategies and interventions to safeguard
sustainable production. This will also facilitate allocating scarce resources
and investment in improving AI techniques to reduce the adverse impacts of
extreme events. Correspondingly, we put forward the limitations of this, which
necessitate future research.","['Derrick Effah', 'Chunguang Bai', 'Matthew Quayson']",2022-09-21T06:52:39Z,http://arxiv.org/abs/2210.08962v1,"['cs.CY', 'cs.AI']"
"VRContour: Bringing Contour Delineations of Medical Structures Into
  Virtual Reality","Contouring is an indispensable step in Radiotherapy (RT) treatment planning.
However, today's contouring software is constrained to only work with a 2D
display, which is less intuitive and requires high task loads. Virtual Reality
(VR) has shown great potential in various specialties of healthcare and health
sciences education due to the unique advantages of intuitive and natural
interactions in immersive spaces. VR-based radiation oncology integration has
also been advocated as a target healthcare application, allowing providers to
directly interact with 3D medical structures. We present VRContour and
investigate how to effectively bring contouring for radiation oncology into VR.
Through an autobiographical iterative design, we defined three design spaces
focused on contouring in VR with the support of a tracked tablet and VR stylus,
and investigating dimensionality for information consumption and input (either
2D or 2D + 3D). Through a within-subject study (n = 8), we found that
visualizations of 3D medical structures significantly increase precision, and
reduce mental load, frustration, as well as overall contouring effort.
Participants also agreed with the benefits of using such metaphors for learning
purposes.","['Chen Chen', 'Matin Yarmand', 'Varun Singh', 'Michael V. Sherer', 'James D. Murphy', 'Yang Zhang', 'Nadir Weibel']",2022-10-21T23:22:21Z,http://arxiv.org/abs/2210.12298v2,"['cs.HC', 'cs.CY']"
"Neural Distortion Fields for Spatial Calibration of Wide Field-of-View
  Near-Eye Displays","We propose a spatial calibration method for wide Field-of-View (FoV) Near-Eye
Displays (NEDs) with complex image distortions. Image distortions in NEDs can
destroy the reality of the virtual object and cause sickness. To achieve
distortion-free images in NEDs, it is necessary to establish a pixel-by-pixel
correspondence between the viewpoint and the displayed image. Designing compact
and wide-FoV NEDs requires complex optical designs. In such designs, the
displayed images are subject to gaze-contingent, non-linear geometric
distortions, which explicit geometric models can be difficult to represent or
computationally intensive to optimize.
  To solve these problems, we propose Neural Distortion Field (NDF), a
fully-connected deep neural network that implicitly represents display surfaces
complexly distorted in spaces. NDF takes spatial position and gaze direction as
input and outputs the display pixel coordinate and its intensity as perceived
in the input gaze direction. We synthesize the distortion map from a novel
viewpoint by querying points on the ray from the viewpoint and computing a
weighted sum to project output display coordinates into an image. Experiments
showed that NDF calibrates an augmented reality NED with 90$^{\circ}$ FoV with
about 3.23 pixel (5.8 arcmin) median error using only 8 training viewpoints.
Additionally, we confirmed that NDF calibrates more accurately than the
non-linear polynomial fitting, especially around the center of the FoV.","['Yuichi Hiroi', 'Kiyosato Someya', 'Yuta Itoh']",2022-10-22T08:48:31Z,http://arxiv.org/abs/2210.12389v1,"['cs.CV', 'cs.HC', 'eess.IV']"
"A Flexible-Frame-Rate Vision-Aided Inertial Object Tracking System for
  Mobile Devices","Real-time object pose estimation and tracking is challenging but essential
for emerging augmented reality (AR) applications. In general, state-of-the-art
methods address this problem using deep neural networks which indeed yield
satisfactory results. Nevertheless, the high computational cost of these
methods makes them unsuitable for mobile devices where real-world applications
usually take place. In addition, head-mounted displays such as AR glasses
require at least 90~FPS to avoid motion sickness, which further complicates the
problem. We propose a flexible-frame-rate object pose estimation and tracking
system for mobile devices. It is a monocular visual-inertial-based system with
a client-server architecture. Inertial measurement unit (IMU) pose propagation
is performed on the client side for high speed tracking, and RGB image-based 3D
pose estimation is performed on the server side to obtain accurate poses, after
which the pose is sent to the client side for visual-inertial fusion, where we
propose a bias self-correction mechanism to reduce drift. We also propose a
pose inspection algorithm to detect tracking failures and incorrect pose
estimation. Connected by high-speed networking, our system supports flexible
frame rates up to 120 FPS and guarantees high precision and real-time tracking
on low-end devices. Both simulations and real world experiments show that our
method achieves accurate and robust object tracking.","['Yo-Chung Lau', 'Kuan-Wei Tseng', 'I-Ju Hsieh', 'Hsiao-Ching Tseng', 'Yi-Ping Hung']",2022-10-22T15:26:50Z,http://arxiv.org/abs/2210.12476v1,"['cs.CV', 'cs.RO']"
"Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR)
  for Metaverses","Metaverse is expected to emerge as a new paradigm for the next-generation
Internet, providing fully immersive and personalised experiences to socialize,
work, and play in self-sustaining and hyper-spatio-temporal virtual world(s).
The advancements in different technologies like augmented reality, virtual
reality, extended reality (XR), artificial intelligence (AI), and 5G/6G
communication will be the key enablers behind the realization of AI-XR
metaverse applications. While AI itself has many potential applications in the
aforementioned technologies (e.g., avatar generation, network optimization,
etc.), ensuring the security of AI in critical applications like AI-XR
metaverse applications is profoundly crucial to avoid undesirable actions that
could undermine users' privacy and safety, consequently putting their lives in
danger. To this end, we attempt to analyze the security, privacy, and
trustworthiness aspects associated with the use of various AI techniques in
AI-XR metaverse applications. Specifically, we discuss numerous such challenges
and present a taxonomy of potential solutions that could be leveraged to
develop secure, private, robust, and trustworthy AI-XR applications. To
highlight the real implications of AI-associated adversarial threats, we
designed a metaverse-specific case study and analyzed it through the
adversarial lens. Finally, we elaborate upon various open issues that require
further research interest from the community.","['Adnan Qayyum', 'Muhammad Atif Butt', 'Hassan Ali', 'Muhammad Usman', 'Osama Halabi', 'Ala Al-Fuqaha', 'Qammer H. Abbasi', 'Muhammad Ali Imran', 'Junaid Qadir']",2022-10-24T14:26:59Z,http://arxiv.org/abs/2210.13289v1,"['cs.AI', 'cs.CR', 'cs.CY']"
"THOR-Net: End-to-end Graformer-based Realistic Two Hands and Object
  Reconstruction with Self-supervision","Realistic reconstruction of two hands interacting with objects is a new and
challenging problem that is essential for building personalized Virtual and
Augmented Reality environments. Graph Convolutional networks (GCNs) allow for
the preservation of the topologies of hands poses and shapes by modeling them
as a graph. In this work, we propose the THOR-Net which combines the power of
GCNs, Transformer, and self-supervision to realistically reconstruct two hands
and an object from a single RGB image. Our network comprises two stages; namely
the features extraction stage and the reconstruction stage. In the features
extraction stage, a Keypoint RCNN is used to extract 2D poses, features maps,
heatmaps, and bounding boxes from a monocular RGB image. Thereafter, this 2D
information is modeled as two graphs and passed to the two branches of the
reconstruction stage. The shape reconstruction branch estimates meshes of two
hands and an object using our novel coarse-to-fine GraFormer shape network. The
3D poses of the hands and objects are reconstructed by the other branch using a
GraFormer network. Finally, a self-supervised photometric loss is used to
directly regress the realistic textured of each vertex in the hands' meshes.
Our approach achieves State-of-the-art results in Hand shape estimation on the
HO-3D dataset (10.0mm) exceeding ArtiBoost (10.8mm). It also surpasses other
methods in hand pose estimation on the challenging two hands and object (H2O)
dataset by 5mm on the left-hand pose and 1 mm on the right-hand pose.","['Ahmed Tawfik Aboukhadra', 'Jameel Malik', 'Ahmed Elhayek', 'Nadia Robertini', 'Didier Stricker']",2022-10-25T09:18:50Z,http://arxiv.org/abs/2210.13853v1,['cs.CV']
"A DirectX-Based DICOM Viewer for Multi-User Surgical Planning in
  Augmented Reality","Preoperative medical imaging is an essential part of surgical planning. The
data from medical imaging devices, such as CT and MRI scanners, consist of
stacks of 2D images in DICOM format. Conversely, advances in 3D data
visualization provide further information by assembling cross-sections into 3D
volumetric datasets. As Microsoft unveiled the HoloLens 2 (HL2), which is
considered one of the best Mixed Reality (XR) headsets in the market, it
promised to enhance visualization in 3D by providing an immersive experience to
users. This paper introduces a prototype holographic XR DICOM Viewer for the 3D
visualization of DICOM image sets on HL2 for surgical planning. We first
developed a standalone graphical C++ engine using the native DirectX11 API and
HLSL shaders. Based on that, the prototype further applies the OpenXR API for
potential deployment on a wide range of devices from vendors across the XR
spectrum. With native access to the device, our prototype unravels the
limitation of hardware capabilities on HL2 for 3D volume rendering and
interaction. Moreover, smartphones can act as input devices to provide another
user interaction method by connecting to our server. In this paper, we present
a holographic DICOM viewer for the HoloLens 2 and contribute (i) a prototype
that renders the DICOM image stacks in real-time on HL2, (ii) three types of
user interactions in XR, and (iii) a preliminary qualitative evaluation of our
prototype.","['Menghe Zhang', 'Weichen Liu', 'Nadir Weibel', 'Jurgen Schulze']",2022-10-25T21:22:00Z,http://arxiv.org/abs/2210.14349v1,"['cs.MM', 'cs.HC']"
Automated Reconstruction of 3D Open Surfaces from Sparse Point Clouds,"Real-world 3D data may contain intricate details defined by salient surface
gaps. Automated reconstruction of these open surfaces (e.g., non-watertight
meshes) is a challenging problem for environment synthesis in mixed reality
applications. Current learning-based implicit techniques can achieve high
fidelity on closed-surface reconstruction. However, their dependence on the
distinction between the inside and outside of a surface makes them incapable of
reconstructing open surfaces. Recently, a new class of implicit functions have
shown promise in reconstructing open surfaces by regressing an unsigned
distance field. Yet, these methods rely on a discretized representation of the
raw data, which loses important surface details and can lead to outliers in the
reconstruction. We propose IPVNet, a learning-based implicit model that
predicts the unsigned distance between a surface and a query point in 3D space
by leveraging both raw point cloud data and its discretized voxel counterpart.
Experiments on synthetic and real-world public datasets demonstrates that
IPVNet outperforms the state of the art while producing far fewer outliers in
the reconstruction.","['Mohammad Samiul Arshad', 'William J. Beksi']",2022-10-26T22:02:45Z,http://arxiv.org/abs/2210.15059v2,"['cs.CV', 'cs.GR']"
Learning Variational Motion Prior for Video-based Motion Capture,"Motion capture from a monocular video is fundamental and crucial for us
humans to naturally experience and interact with each other in Virtual Reality
(VR) and Augmented Reality (AR). However, existing methods still struggle with
challenging cases involving self-occlusion and complex poses due to the lack of
effective motion prior modeling. In this paper, we present a novel variational
motion prior (VMP) learning approach for video-based motion capture to resolve
the above issue. Instead of directly building the correspondence between the
video and motion domain, We propose to learn a generic latent space for
capturing the prior distribution of all natural motions, which serve as the
basis for subsequent video-based motion capture tasks. To improve the
generalization capacity of prior space, we propose a transformer-based
variational autoencoder pretrained over marker-based 3D mocap data, with a
novel style-mapping block to boost the generation quality. Afterward, a
separate video encoder is attached to the pretrained motion generator for
end-to-end fine-tuning over task-specific video datasets. Compared to existing
motion prior models, our VMP model serves as a motion rectifier that can
effectively reduce temporal jittering and failure modes in frame-wise pose
estimation, leading to temporally stable and visually realistic motion capture
results. Furthermore, our VMP-based framework models motion at sequence level
and can directly generate motion clips in the forward pass, achieving real-time
motion capture during inference. Extensive experiments over both public
datasets and in-the-wild videos have demonstrated the efficacy and
generalization capability of our framework.","['Xin Chen', 'Zhuo Su', 'Lingbo Yang', 'Pei Cheng', 'Lan Xu', 'Bin Fu', 'Gang Yu']",2022-10-27T02:45:48Z,http://arxiv.org/abs/2210.15134v2,"['cs.CV', 'I.4.8']"
Big Data Meets Metaverse: A Survey,"We are living in the era of big data. The Metaverse is an emerging technology
in the future, and it has a combination of big data, AI (artificial
intelligence), VR (Virtual Reality), AR (Augmented Reality), MR (mixed
reality), and other technologies that will diminish the difference between
online and real-life interaction. It has the goal of becoming a platform where
we can work, go shopping, play around, and socialize. Each user who enters the
Metaverse interacts with the virtual world in a data way. With the development
and application of the Metaverse, the data will continue to grow, thus forming
a big data network, which will bring huge data processing pressure to the
digital world. Therefore, big data processing technology is one of the key
technologies to implement the Metaverse. In this survey, we provide a
comprehensive review of how Metaverse is changing big data. Moreover, we
discuss the key security and privacy of Metaverse big data in detail. Finally,
we summarize the open problems and opportunities of Metaverse, as well as the
future of Metaverse with big data. We hope that this survey will provide
researchers with the research direction and prospects of applying big data in
the Metaverse.","['Jiayi Sun', 'Wensheng Gan', 'Zefeng Chen', 'Junhui Li', 'Philip S. Yu']",2022-10-28T17:22:20Z,http://arxiv.org/abs/2210.16282v1,"['cs.DB', 'cs.CY']"
Analyzing Performance Issues of Virtual Reality Applications,"Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)
and Mixed Reality (MR). XR is an emerging technology that simulates a realistic
environment for users. XR techniques have provided revolutionary user
experiences in various application scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). Due to the
high computational cost of rendering real-time animation in limited-resource
devices and constant interaction with user activity, XR applications often face
performance bottlenecks, and these bottlenecks create a negative impact on the
user experience of XR software. Thus, performance optimization plays an
essential role in many industry-standard XR applications. Even though
identifying performance bottlenecks in traditional software (e.g., desktop
applications) is a widely explored topic, those approaches cannot be directly
applied within XR software due to the different nature of XR applications.
Moreover, XR applications developed in different frameworks such as Unity and
Unreal Engine show different performance bottleneck patterns and thus,
bottleneck patterns of Unity projects can't be applied for Unreal Engine
(UE)-based XR projects. To fill the knowledge gap for XR performance
optimizations of Unreal Engine-based XR projects, we present the first
empirical study on performance optimizations from seven UE XR projects, 78 UE
XR discussion issues and three sources of UE documentation. Our analysis
identified 14 types of performance bugs, including 12 types of bugs related to
UE settings issues and two types of CPP source code-related issues. To further
assist developers in detecting performance bugs based on the identified bug
patterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect
performance bugs in both configuration files and source code.","['Jason Hogan', 'Aaron Salo', 'Dhia Elhaq Rzig', 'Foyzul Hassan', 'Bruce Maxim']",2022-11-03T17:27:36Z,http://arxiv.org/abs/2211.02013v1,['cs.SE']
"EPViSA: Efficient Auction Design for Real-time Physical-Virtual
  Synchronization in the Metaverse","Metaverse can obscure the boundary between the physical and virtual worlds.
Specifically, for the Metaverse in vehicular networks, i.e., the vehicular
Metaverse, vehicles are no longer isolated physical spaces but interfaces that
extend the virtual worlds to the physical world. Accessing the Metaverse via
autonomous vehicles (AVs), drivers and passengers can immerse in and interact
with 3D virtual objects overlaying views of streets on head-up displays (HUD)
via augmented reality (AR). The seamless, immersive, and interactive experience
rather relies on real-time multi-dimensional data synchronization between
physical entities, i.e., AVs, and virtual entities, i.e., Metaverse billboard
providers (MBPs). However, mechanisms to allocate and match synchronizing AV
and MBP pairs to roadside units (RSUs) in a synchronization service market,
which consists of the physical and virtual submarkets, are vulnerable to
adverse selection. In this paper, we propose an enhanced second-score
auction-based mechanism, named EPViSA, to allocate physical and virtual
entities in the synchronization service market of the vehicular Metaverse. The
EPViSA mechanism can determine synchronizing AV and MBP pairs simultaneously
while protecting participants from adverse selection and thus achieving high
total social welfare. We propose a synchronization scoring rule to eliminate
the external effects from the virtual submarkets. Then, a price scaling factor
is introduced to enhance the allocation of synchronizing virtual entities in
the virtual submarkets. Finally, rigorous analysis and extensive experimental
results demonstrate that EPViSA can achieve at least 96\% of the social welfare
compared to the omniscient benchmark while ensuring strategy-proof and adverse
selection free through a simulation testbed.","['Minrui Xu', 'Dusit Niyato', 'Benjamin Wright', 'Hongliang Zhang', 'Jiawen Kang', 'Zehui Xiong', 'Shiwen Mao', 'Zhu Han']",2022-11-13T07:42:03Z,http://arxiv.org/abs/2211.06838v1,['cs.NI']
"Rate-Distortion Modeling for Bit Rate Constrained Point Cloud
  Compression","As being one of the main representation formats of 3D real world and
well-suited for virtual reality and augmented reality applications, point
clouds have gained a lot of popularity. In order to reduce the huge amount of
data, a considerable amount of research on point cloud compression has been
done. However, given a target bit rate, how to properly choose the color and
geometry quantization parameters for compressing point clouds is still an open
issue. In this paper, we propose a rate-distortion model based quantization
parameter selection scheme for bit rate constrained point cloud compression.
Firstly, to overcome the measurement uncertainty in evaluating the distortion
of the point clouds, we propose a unified model to combine the geometry
distortion and color distortion. In this model, we take into account the
correlation between geometry and color variables of point clouds and derive a
dimensionless quantity to represent the overall quality degradation. Then, we
derive the relationships of overall distortion and bit rate with the
quantization parameters. Finally, we formulate the bit rate constrained point
cloud compression as a constrained minimization problem using the derived
polynomial models and deduce the solution via an iterative numerical method.
Experimental results show that the proposed algorithm can achieve optimal
decoded point cloud quality at various target bit rates, and substantially
outperform the video-rate-distortion model based point cloud compression
scheme.","['Pan Gao', 'Shengzhou Luo', 'Manoranjan Paul']",2022-11-19T10:21:06Z,http://arxiv.org/abs/2211.10646v1,"['cs.MM', 'cs.IT', 'eess.IV', 'math.IT']"
"Shape, Pose, and Appearance from a Single Image via Bootstrapped
  Radiance Field Inversion","Neural Radiance Fields (NeRF) coupled with GANs represent a promising
direction in the area of 3D reconstruction from a single view, owing to their
ability to efficiently model arbitrary topologies. Recent work in this area,
however, has mostly focused on synthetic datasets where exact ground-truth
poses are known, and has overlooked pose estimation, which is important for
certain downstream applications such as augmented reality (AR) and robotics. We
introduce a principled end-to-end reconstruction framework for natural images,
where accurate ground-truth poses are not available. Our approach recovers an
SDF-parameterized 3D shape, pose, and appearance from a single image of an
object, without exploiting multiple views during training. More specifically,
we leverage an unconditional 3D-aware generator, to which we apply a hybrid
inversion scheme where a model produces a first guess of the solution which is
then refined via optimization. Our framework can de-render an image in as few
as 10 steps, enabling its use in practical scenarios. We demonstrate
state-of-the-art results on a variety of real and synthetic benchmarks.","['Dario Pavllo', 'David Joseph Tan', 'Marie-Julie Rakotosaona', 'Federico Tombari']",2022-11-21T17:42:42Z,http://arxiv.org/abs/2211.11674v2,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG']"
"NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with
  360° Views","Virtual reality and augmented reality (XR) bring increasing demand for 3D
content. However, creating high-quality 3D content requires tedious work that a
human expert must do. In this work, we study the challenging task of lifting a
single image to a 3D object and, for the first time, demonstrate the ability to
generate a plausible 3D object with 360{\deg} views that correspond well with
the given reference image. By conditioning on the reference image, our model
can fulfill the everlasting curiosity for synthesizing novel views of objects
from images. Our technique sheds light on a promising direction of easing the
workflows for 3D artists and XR designers. We propose a novel framework, dubbed
NeuralLift-360, that utilizes a depth-aware neural radiance representation
(NeRF) and learns to craft the scene guided by denoising diffusion models. By
introducing a ranking loss, our NeuralLift-360 can be guided with rough depth
estimation in the wild. We also adopt a CLIP-guided sampling strategy for the
diffusion prior to provide coherent guidance. Extensive experiments demonstrate
that our NeuralLift-360 significantly outperforms existing state-of-the-art
baselines. Project page: https://vita-group.github.io/NeuralLift-360/","['Dejia Xu', 'Yifan Jiang', 'Peihao Wang', 'Zhiwen Fan', 'Yi Wang', 'Zhangyang Wang']",2022-11-29T17:59:06Z,http://arxiv.org/abs/2211.16431v2,['cs.CV']
HoloBeam: Paper-Thin Near-Eye Displays,"An emerging alternative to conventional Augmented Reality (AR) glasses
designs, Beaming displays promise slim AR glasses free from challenging design
trade-offs, including battery-related limits or computational budget-related
issues. These beaming displays remove active components such as batteries and
electronics from AR glasses and move them to a projector that projects images
to a user from a distance (1-2 meters), where users wear only passive optical
eyepieces. However, earlier implementations of these displays delivered poor
resolutions (7 cycles per degree) without any optical focus cues and were
introduced with a bulky form-factor eyepiece (50 mm thick). This paper
introduces a new milestone for beaming displays, which we call HoloBeam. In
this new design, a custom holographic projector populates a micro-volume
located at some distance (1-2 meters) with multiple planes of images. Users
view magnified copies of these images from this small volume with the help of
an eyepiece that is either a Holographic Optical Element (HOE) or a set of
lenses. Our HoloBeam prototypes demonstrate the thinnest AR glasses to date
with a submillimeter thickness (e.g., HOE film is only 120 um thick). In
addition, HoloBeam prototypes demonstrate near retinal resolutions (24 cycles
per degree) with a 70 degrees-wide field of view.","['Kaan Akşit', 'Yuta Itoh']",2022-12-08T09:53:13Z,http://arxiv.org/abs/2212.05057v2,"['cs.HC', 'cs.AR', 'cs.GR', 'physics.optics', 'I.3.1; I.3.7; I.3.2']"
"Mind The Edge: Refining Depth Edges in Sparsely-Supervised Monocular
  Depth Estimation","Monocular Depth Estimation (MDE) is a fundamental problem in computer vision
with numerous applications. Recently, LIDAR-supervised methods have achieved
remarkable per-pixel depth accuracy in outdoor scenes. However, significant
errors are typically found in the proximity of depth discontinuities, i.e.,
depth edges, which often hinder the performance of depth-dependent applications
that are sensitive to such inaccuracies, e.g., novel view synthesis and
augmented reality. Since direct supervision for the location of depth edges is
typically unavailable in sparse LIDAR-based scenes, encouraging the MDE model
to produce correct depth edges is not straightforward. To the best of our
knowledge this paper is the first attempt to address the depth edges issue for
LIDAR-supervised scenes. In this work we propose to learn to detect the
location of depth edges from densely-supervised synthetic data, and use it to
generate supervision for the depth edges in the MDE training. To quantitatively
evaluate our approach, and due to the lack of depth edges GT in LIDAR-based
scenes, we manually annotated subsets of the KITTI and the DDAD datasets with
depth edges ground truth. We demonstrate significant gains in the accuracy of
the depth edges with comparable per-pixel depth accuracy on several challenging
datasets. Code and datasets are available at
\url{https://github.com/liortalker/MindTheEdge}.","['Lior Talker', 'Aviad Cohen', 'Erez Yosef', 'Alexandra Dana', 'Michael Dinerstein']",2022-12-10T14:49:24Z,http://arxiv.org/abs/2212.05315v3,['cs.CV']
Scene-aware Egocentric 3D Human Pose Estimation,"Egocentric 3D human pose estimation with a single head-mounted fisheye camera
has recently attracted attention due to its numerous applications in virtual
and augmented reality. Existing methods still struggle in challenging poses
where the human body is highly occluded or is closely interacting with the
scene. To address this issue, we propose a scene-aware egocentric pose
estimation method that guides the prediction of the egocentric pose with scene
constraints. To this end, we propose an egocentric depth estimation network to
predict the scene depth map from a wide-view egocentric fisheye camera while
mitigating the occlusion of the human body with a depth-inpainting network.
Next, we propose a scene-aware pose estimation network that projects the 2D
image features and estimated depth map of the scene into a voxel space and
regresses the 3D pose with a V2V network. The voxel-based feature
representation provides the direct geometric connection between 2D image
features and scene geometry, and further facilitates the V2V network to
constrain the predicted pose based on the estimated scene geometry. To enable
the training of the aforementioned networks, we also generated a synthetic
dataset, called EgoGTA, and an in-the-wild dataset based on EgoPW, called
EgoPW-Scene. The experimental results of our new evaluation sequences show that
the predicted 3D egocentric poses are accurate and physically plausible in
terms of human-scene interaction, demonstrating that our method outperforms the
state-of-the-art methods both quantitatively and qualitatively.","['Jian Wang', 'Lingjie Liu', 'Weipeng Xu', 'Kripasindhu Sarkar', 'Diogo Luvizon', 'Christian Theobalt']",2022-12-20T21:35:39Z,http://arxiv.org/abs/2212.11684v3,['cs.CV']
"Pensieve 5G: Implementation of RL-based ABR Algorithm for UHD 4K/8K
  Content Delivery on Commercial 5G SA/NR-DC Network","While the rollout of the fifth-generation mobile network (5G) is underway
across the globe with the intention to deliver 4K/8K UHD videos, Augmented
Reality (AR), and Virtual Reality (VR) content to the mass amounts of users,
the coverage and throughput are still one of the most significant issues,
especially in the rural areas, where only 5G in the low-frequency band are
being deployed. This called for a high-performance adaptive bitrate (ABR)
algorithm that can maximize the user quality of experience given 5G network
characteristics and data rate of UHD contents.
  Recently, many of the newly proposed ABR techniques were machine-learning
based. Among that, Pensieve is one of the state-of-the-art techniques, which
utilized reinforcement-learning to generate an ABR algorithm based on
observation of past decision performance. By incorporating the context of the
5G network and UHD content, Pensieve has been optimized into Pensieve 5G. New
QoE metrics that more accurately represent the QoE of UHD video streaming on
the different types of devices were proposed and used to evaluate Pensieve 5G
against other ABR techniques including the original Pensieve. The results from
the simulation based on the real 5G Standalone (SA) network throughput shows
that Pensieve 5G outperforms both conventional algorithms and Pensieve with the
average QoE improvement of 8.8% and 14.2%, respectively. Additionally, Pensieve
5G also performed well on the commercial 5G NR-NR Dual Connectivity (NR-DC)
Network, despite the training being done solely using the data from the 5G
Standalone (SA) network.","['Kasidis Arunruangsirilert', 'Bo Wei', 'Hang Song', 'Jiro Katto']",2022-12-29T23:03:47Z,http://arxiv.org/abs/2212.14479v1,"['cs.NI', 'cs.LG']"
NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,"Searching long egocentric videos with natural language queries (NLQ) has
compelling applications in augmented reality and robotics, where a fluid index
into everything that a person (agent) has seen before could augment human
memory and surface relevant information on demand. However, the structured
nature of the learning problem (free-form text query inputs, localized video
temporal window outputs) and its needle-in-a-haystack nature makes it both
technically challenging and expensive to supervise. We introduce
Narrations-as-Queries (NaQ), a data augmentation strategy that transforms
standard video-text narrations into training data for a video query
localization model. Validating our idea on the Ego4D benchmark, we find it has
tremendous impact in practice. NaQ improves multiple top models by substantial
margins (even doubling their accuracy), and yields the very best results to
date on the Ego4D NLQ challenge, soundly outperforming all challenge winners in
the CVPR and ECCV 2022 competitions and topping the current public leaderboard.
Beyond achieving the state-of-the-art for NLQ, we also demonstrate unique
properties of our approach such as the ability to perform zero-shot and
few-shot NLQ, and improved performance on queries about long-tail object
categories. Code and models:
{\small\url{http://vision.cs.utexas.edu/projects/naq}}.","['Santhosh Kumar Ramakrishnan', 'Ziad Al-Halah', 'Kristen Grauman']",2023-01-02T16:40:15Z,http://arxiv.org/abs/2301.00746v2,['cs.CV']
Hierarchical Point Attention for Indoor 3D Object Detection,"3D object detection is an essential vision technique for various robotic
systems, such as augmented reality and domestic robots. Transformers as
versatile network architectures have recently seen great success in 3D point
cloud object detection. However, the lack of hierarchy in a plain transformer
restrains its ability to learn features at different scales. Such limitation
makes transformer detectors perform worse on smaller objects and affects their
reliability in indoor environments where small objects are the majority. This
work proposes two novel attention operations as generic hierarchical designs
for point-based transformer detectors. First, we propose Aggregated Multi-Scale
Attention (MS-A) that builds multi-scale tokens from a single-scale input
feature to enable more fine-grained feature learning. Second, we propose
Size-Adaptive Local Attention (Local-A) with adaptive attention regions for
localized feature aggregation within bounding box proposals. Both attention
operations are model-agnostic network modules that can be plugged into existing
point cloud transformers for end-to-end training. We evaluate our method on two
widely used indoor detection benchmarks. By plugging our proposed modules into
the state-of-the-art transformer-based 3D detectors, we improve the previous
best results on both benchmarks, with more significant improvements on smaller
objects.","['Manli Shu', 'Le Xue', 'Ning Yu', 'Roberto Martín-Martín', 'Caiming Xiong', 'Tom Goldstein', 'Juan Carlos Niebles', 'Ran Xu']",2023-01-06T18:52:12Z,http://arxiv.org/abs/2301.02650v2,['cs.CV']
"Generative AI-empowered Effective Physical-Virtual Synchronization in
  the Vehicular Metaverse","Metaverse seamlessly blends the physical world and virtual space via
ubiquitous communication and computing infrastructure. In transportation
systems, the vehicular Metaverse can provide a fully-immersive and hyperreal
traveling experience (e.g., via augmented reality head-up displays, AR-HUDs) to
drivers and users in autonomous vehicles (AVs) via roadside units (RSUs).
However, provisioning real-time and immersive services necessitates effective
physical-virtual synchronization between physical and virtual entities, i.e.,
AVs and Metaverse AR recommenders (MARs). In this paper, we propose a
generative AI-empowered physical-virtual synchronization framework for the
vehicular Metaverse. In physical-to-virtual synchronization, digital twin (DT)
tasks generated by AVs are offloaded for execution in RSU with future route
generation. In virtual-to-physical synchronization, MARs customize diverse and
personal AR recommendations via generative AI models based on user preferences.
Furthermore, we propose a multi-task enhanced auction-based mechanism to match
and price AVs and MARs for RSUs to provision real-time and effective services.
Finally, property analysis and experimental results demonstrate that the
proposed mechanism is strategy-proof and adverse-selection free while
increasing social surplus by 50%.","['Minrui Xu', 'Dusit Niyato', 'Hongliang Zhang', 'Jiawen Kang', 'Zehui Xiong', 'Shiwen Mao', 'Zhu Han']",2023-01-18T16:25:42Z,http://arxiv.org/abs/2301.07636v2,['cs.AI']
DAG-based Task Orchestration for Edge Computing,"As we increase the number of personal computing devices that we carry (mobile
devices, tablets, e-readers, and laptops) and these come equipped with
increasing resources, there is a vast potential computation power that can be
utilized from those devices. Edge computing promises to exploit these
underlying computation resources closer to users to help run latency-sensitive
applications such as augmented reality and video analytics. However, one key
missing piece has been how to incorporate personally owned unmanaged devices
into a usable edge computing system. The primary challenges arise due to the
heterogeneity, lack of interference management, and unpredictable availability
of such devices. In this paper we propose an orchestration framework IBDASH,
which orchestrates application tasks on an edge system that comprises a mix of
commercial and personal edge devices. IBDASH targets reducing both end-to-end
latency of execution and probability of failure for applications that have
dependency among tasks, captured by directed acyclic graphs (DAGs). IBDASH
takes memory constraints of each edge device and network bandwidth into
consideration. To assess the effectiveness of IBDASH, we run real application
tasks on real edge devices with widely varying capabilities.We feed these
measurements into a simulator that runs IBDASH at scale. Compared to three
state-of-the-art edge orchestration schemes, LAVEA, Petrel, and LaTS, and two
intuitive baselines, IBDASH reduces the end-to-end latency and probability of
failure, by 14% and 41% on average respectively. The main takeaway from our
work is that it is feasible to combine personal and commercial devices into a
usable edge computing platform, one that delivers low latency and predictable
and high availability.","['Xiang Li', 'Mustafa Abdallah', 'Shikhar Suryavansh', 'Mung Chiang', 'Saurabh Bagchi']",2023-01-23T05:27:33Z,http://arxiv.org/abs/2301.09278v1,"['cs.DC', 'cs.NI']"
"SONIA: an immersive customizable virtual reality system for the
  education and exploration of brain networks","While mastery of neuroanatomy is important for the investigation of the
brain, there is an increasing interest in exploring the neural pathways to
better understand the roles of neural circuitry in brain functions. To tackle
the limitations of traditional 2D-display-based neuronavigation software in
intuitively visualizing complex 3D anatomies, several virtual reality (VR) and
augmented reality (AR) solutions have been proposed to facilitate
neuroanatomical education. However, with the increasing knowledge on brain
connectivity and the functioning of the sub-systems, there is still a lack of
similar software solutions for the education and exploration of these topics,
which demand more elaborate visualization and interaction strategies. To
address this gap, we designed the immerSive custOmizable Neuro learnIng plAform
(SONIA), a novel user-friendly VR software system with a multi-scale
interaction paradigm that allows flexible customization of learning materials.
With both quantitative and qualitative evaluations through user studies, the
proposed system is shown to have high usability, attractive visual design, and
good educational value. As the first immersive system that integrates
customizable design and detailed narratives of the brain sub-systems for the
education of neuroanatomy and brain connectivity, SONIA showcases new potential
directions and provides valuable insights regarding medical learning and
exploration in VR.","['Owen Hellum', 'Christopher Steele', 'Yiming Xiao']",2023-01-24T01:04:15Z,http://arxiv.org/abs/2301.09772v2,"['cs.HC', 'cs.MM']"
"Passively Addressed Robotic Morphing Surface (PARMS) Based on Machine
  Learning","Reconfigurable morphing surfaces provide new opportunities for advanced
human-machine interfaces and bio-inspired robotics. Morphing into arbitrary
surfaces on demand requires a device with a sufficiently large number of
actuators and an inverse control strategy that can calculate the actuator
stimulation necessary to achieve a target surface. The programmability of a
morphing surface can be improved by increasing the number of independent
actuators, but this increases the complexity of the control system. Thus,
developing compact and efficient control interfaces and control algorithms is a
crucial knowledge gap for the adoption of morphing surfaces in broad
applications. In this work, we describe a passively addressed robotic morphing
surface (PARMS) composed of matrix-arranged ionic actuators. To reduce the
complexity of the physical control interface, we introduce passive matrix
addressing. Matrix addressing allows the control of independent actuators using
only 2N control inputs, which is significantly lower than control inputs
required for traditional direct addressing. Our control algorithm is based on
machine learning using finite element simulations as the training data. This
machine learning approach allows both forward and inverse control with high
precision in real time. Inverse control demonstrations show that the PARMS can
dynamically morph into arbitrary pre-defined surfaces on demand. These
innovations in actuator matrix control may enable future implementation of
PARMS in wearables, haptics, and augmented reality/virtual reality (AR/VR).","['Jue Wang', 'Michael Sotzing', 'Mina Lee', 'Alex Chortos']",2023-01-30T20:51:14Z,http://arxiv.org/abs/2301.13284v2,['cs.RO']
"Playing with Data: An Augmented Reality Approach to Interact with
  Visualizations of Industrial Process Tomography","Industrial process tomography (IPT) is a specialized imaging technique widely
used in industrial scenarios for process supervision and control. Today,
augmented/mixed reality (AR/MR) is increasingly being adopted in many
industrial occasions, even though there is still an obvious gap when it comes
to IPT. To bridge this gap, we propose the first systematic AR approach using
optical see-through (OST) head mounted displays (HMDs) with comparative
evaluation for domain users towards IPT visualization analysis. The
proof-of-concept was demonstrated by a within-subject user study (n=20) with
counterbalancing design. Both qualitative and quantitative measurements were
investigated. The results showed that our AR approach outperformed conventional
settings for IPT data visualization analysis in bringing higher
understandability, reduced task completion time, lower error rates for domain
tasks, increased usability with enhanced user experience, and a better
recommendation level. We summarize the findings and suggest future research
directions for benefiting IPT users with AR/MR.","['Yuchong Zhang', 'Yueming Xuan', 'Rahul Yadav', 'Adel Omrani', 'Morten Fjeld']",2023-02-03T12:19:01Z,http://arxiv.org/abs/2302.01686v5,['cs.HC']
"Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for
  Longer-Range Object Tracking Applications","Digital twin is a problem of augmenting real objects with their digital
counterparts. It can underpin a wide range of applications in augmented reality
(AR), autonomy, and UI/UX. A critical component in a good digital-twin system
is real-time, accurate 3D object tracking. Most existing works solve 3D object
tracking through the lens of robotic grasping, employ older generations of
depth sensors, and measure performance metrics that may not apply to other
digital-twin applications such as in AR. In this work, we create a novel RGB-D
dataset, called Digital Twin Tracking Dataset (DTTD), to enable further
research of the problem and extend potential solutions towards longer ranges
and mm localization accuracy. To reduce point cloud noise from the input
source, we select the latest Microsoft Azure Kinect as the state-of-the-art
time-of-flight (ToF) camera. In total, 103 scenes of 10 common off-the-shelf
objects with rich textures are recorded, with each frame annotated with a
per-pixel semantic segmentation and ground-truth object poses provided by a
commercial motion capturing system. Through extensive experiments with
model-level and dataset-level analysis, we demonstrate that DTTD can help
researchers develop future object tracking methods and analyze new challenges.
The dataset, data generation, annotation, and model evaluation pipeline are
made publicly available as open source code at:
https://github.com/augcog/DTTDv1.","['Weiyu Feng', 'Seth Z. Zhao', 'Chuanyu Pan', 'Adam Chang', 'Yichen Chen', 'Zekun Wang', 'Allen Y. Yang']",2023-02-12T20:06:07Z,http://arxiv.org/abs/2302.05991v2,['cs.CV']
"Multiple Resource Allocation in Multi-Tenant Edge Computing via
  Sub-modular Optimization","Edge Computing (EC) allows users to access computing resources at the network
frontier, which paves the way for deploying delay-sensitive applications such
as Mobile Augmented Reality (MAR). Under the EC paradigm, MAR users connect to
the EC server, open sessions and send continuously frames to be processed. The
EC server sends back virtual information to enhance the human perception of the
world by merging it with the real environment. Resource allocation arises as a
critical challenge when several MAR Service Providers (SPs) compete for limited
resources at the edge of the network. In this paper, we consider EC in a
multi-tenant environment where the resource owner, i.e., the Network Operator
(NO), virtualizes the resources and lets SPs run their services using the
allocated slice of resources. Indeed, for MAR applications, we focus on two
specific resources: CPU and RAM, deployed in some edge node, e.g., a central
office. We study the decision of the NO about how to partition these resources
among several SPs. We model the arrival and service dynamics of users belonging
to different SPs using Erlang queuing model and show that under perfect
information, the interaction between the NO and SPs can be formulated as a
sub-modular maximization problem under multiple Knapsack constraints. To solve
the problem, we use an approximation algorithm, guaranteeing a bounded gap with
respect to the optimal theoretical solution. Our numerical results show that
the proposed algorithm outperforms baseline proportional allocation in terms of
the number of sessions accommodated at the edge for each SP.","['Ayoub Ben-Ameur', 'Andrea Araldo', 'Tijani Chahed']",2023-02-20T10:33:56Z,http://arxiv.org/abs/2302.09888v1,['cs.DC']
"Instance-incremental Scene Graph Generation from Real-world Point Clouds
  via Normalizing Flows","This work introduces a new task of instance-incremental scene graph
generation: Given a scene of the point cloud, representing it as a graph and
automatically increasing novel instances. A graph denoting the object layout of
the scene is finally generated. It is an important task since it helps to guide
the insertion of novel 3D objects into a real-world scene in vision-based
applications like augmented reality. It is also challenging because the
complexity of the real-world point cloud brings difficulties in learning object
layout experiences from the observation data (non-empty rooms with labeled
semantics). We model this task as a conditional generation problem and propose
a 3D autoregressive framework based on normalizing flows (3D-ANF) to address
it. First, we represent the point cloud as a graph by extracting the label
semantics and contextual relationships. Next, a model based on normalizing
flows is introduced to map the conditional generation of graphic elements into
the Gaussian process. The mapping is invertible. Thus, the real-world
experiences represented in the observation data can be modeled in the training
phase, and novel instances can be autoregressively generated based on the
Gaussian process in the testing phase. To evaluate the performance of our
method sufficiently, we implement this new task on the indoor benchmark dataset
3DSSG-O27R16 and our newly proposed graphical dataset of outdoor scenes GPL3D.
Experiments show that our method generates reliable novel graphs from the
real-world point cloud and achieves state-of-the-art performance on the
datasets.","['Chao Qi', 'Jianqin Yin', 'Jinghang Xu', 'Pengxiang Ding']",2023-02-21T03:34:15Z,http://arxiv.org/abs/2302.10425v2,['cs.CV']
Open Challenges for Monocular Single-shot 6D Object Pose Estimation,"Object pose estimation is a non-trivial task that enables robotic
manipulation, bin picking, augmented reality, and scene understanding, to name
a few use cases. Monocular object pose estimation gained considerable momentum
with the rise of high-performing deep learning-based solutions and is
particularly interesting for the community since sensors are inexpensive and
inference is fast. Prior works establish the comprehensive state of the art for
diverse pose estimation problems. Their broad scopes make it difficult to
identify promising future directions. We narrow down the scope to the problem
of single-shot monocular 6D object pose estimation, which is commonly used in
robotics, and thus are able to identify such trends. By reviewing recent
publications in robotics and computer vision, the state of the art is
established at the union of both fields. Following that, we identify promising
research directions in order to help researchers to formulate relevant research
ideas and effectively advance the state of the art. Findings include that
methods are sophisticated enough to overcome the domain shift and that
occlusion handling is a fundamental challenge. We also highlight problems such
as novel object pose estimation and challenging materials handling as central
challenges to advance robotics.","['Stefan Thalhammer', 'Peter Hönig', 'Jean-Baptiste Weibel', 'Markus Vincze']",2023-02-23T07:26:50Z,http://arxiv.org/abs/2302.11827v2,['cs.CV']
AR-Assisted Surgical Care via 5G networks for First Aid Responders,"Surgeons should play a central role in disaster planning and management due
to the overwhelming number of bodily injuries that are typically involved
during most forms of disaster. In fact, various types of surgical procedures
are performed by emergency medical teams after sudden-onset disasters, such as
soft tissue wounds, orthopaedic traumas, abdominal surgeries, etc. HMD-based
Augmented Reality (AR), using state-of-the-art hardware such as the Magic Leap
or the Microsoft HoloLens, have long been foreseen as a key enabler for
clinicians in surgical use cases, especially for procedures performed outside
of the operating room.
  This paper describes the Use Case (UC) ""AR-assisted emergency surgical care"",
identified in the context of the 5G-EPICENTRE EU-funded project. Specifically,
the UC will experiment with holographic AR technology for emergency medical
surgery teams, by overlaying deformable medical models directly on top of the
patient body parts, effectively enabling surgeons to see inside (visualizing
bones, blood vessels, etc.) and perform surgical actions following step-by-step
instructions. The goal is to combine the computational and data-intensive
nature of AR and Computer Vision algorithms with upcoming 5G network
architectures deployed for edge computing so as to satisfy real-time
interaction requirements and provide an efficient and powerful platform for the
pervasive promotion of such applications. By developing the necessary Virtual
Network Functions (VNFs) to manage data-intensive services (e.g., prerendering,
caching, compression) and by exploiting available network resources and
Multi-access Edge Computing (MEC) support, provided by the 5G-EPICENTRE
infrastructure, this UC aims to provide powerful AR-based tools, usable on
site, to first-aid responders.","['Manos Kamarianakis', 'Antonis Protopsaltis', 'George Papagiannakis']",2023-03-01T12:33:31Z,http://arxiv.org/abs/2303.00458v1,['cs.GR']
Depth-based 6DoF Object Pose Estimation using Swin Transformer,"Accurately estimating the 6D pose of objects is crucial for many
applications, such as robotic grasping, autonomous driving, and augmented
reality. However, this task becomes more challenging in poor lighting
conditions or when dealing with textureless objects. To address this issue,
depth images are becoming an increasingly popular choice due to their
invariance to a scene's appearance and the implicit incorporation of essential
geometric characteristics. However, fully leveraging depth information to
improve the performance of pose estimation remains a difficult and
under-investigated problem. To tackle this challenge, we propose a novel
framework called SwinDePose, that uses only geometric information from depth
images to achieve accurate 6D pose estimation. SwinDePose first calculates the
angles between each normal vector defined in a depth image and the three
coordinate axes in the camera coordinate system. The resulting angles are then
formed into an image, which is encoded using Swin Transformer. Additionally, we
apply RandLA-Net to learn the representations from point clouds. The resulting
image and point clouds embeddings are concatenated and fed into a semantic
segmentation module and a 3D keypoints localization module. Finally, we
estimate 6D poses using a least-square fitting approach based on the target
object's predicted semantic mask and 3D keypoints. In experiments on the
LineMod and Occlusion LineMod datasets, SwinDePose outperforms existing
state-of-the-art methods for 6D object pose estimation using depth images. This
demonstrates the effectiveness of our approach and highlights its potential for
improving performance in real-world scenarios. Our code is at
https://github.com/zhujunli1993/SwinDePose.","['Zhujun Li', 'Ioannis Stamos']",2023-03-03T18:25:07Z,http://arxiv.org/abs/2303.02133v2,"['cs.CV', 'cs.RO']"
"PlanarTrack: A Large-scale Challenging Benchmark for Planar Object
  Tracking","Planar object tracking is a critical computer vision problem and has drawn
increasing interest owing to its key roles in robotics, augmented reality, etc.
Despite rapid progress, its further development, especially in the deep
learning era, is largely hindered due to the lack of large-scale challenging
benchmarks. Addressing this, we introduce PlanarTrack, a large-scale
challenging planar tracking benchmark. Specifically, PlanarTrack consists of
1,000 videos with more than 490K images. All these videos are collected in
complex unconstrained scenarios from the wild, which makes PlanarTrack,
compared with existing benchmarks, more challenging but realistic for
real-world applications. To ensure the high-quality annotation, each frame in
PlanarTrack is manually labeled using four corners with multiple-round careful
inspection and refinement. To our best knowledge, PlanarTrack, to date, is the
largest and most challenging dataset dedicated to planar object tracking. In
order to analyze the proposed PlanarTrack, we evaluate 10 planar trackers and
conduct comprehensive comparisons and in-depth analysis. Our results, not
surprisingly, demonstrate that current top-performing planar trackers
degenerate significantly on the challenging PlanarTrack and more efforts are
needed to improve planar tracking in the future. In addition, we further derive
a variant named PlanarTrack$_{\mathbf{BB}}$ for generic object tracking from
PlanarTrack. Our evaluation of 10 excellent generic trackers on
PlanarTrack$_{\mathrm{BB}}$ manifests that, surprisingly,
PlanarTrack$_{\mathrm{BB}}$ is even more challenging than several popular
generic tracking benchmarks and more attention should be paid to handle such
planar objects, though they are rigid. All benchmarks and evaluations will be
released at the project webpage.","['Xinran Liu', 'Xiaoqiong Liu', 'Ziruo Yi', 'Xin Zhou', 'Thanh Le', 'Libo Zhang', 'Yan Huang', 'Qing Yang', 'Heng Fan']",2023-03-14T04:48:18Z,http://arxiv.org/abs/2303.07625v1,['cs.CV']
"Play to Earn in the Metaverse with Mobile Edge Computing over Wireless
  Networks: A Deep Reinforcement Learning Approach","The Metaverse play-to-earn games have been gaining popularity as they enable
players to earn in-game tokens which can be translated to real-world profits.
With the advancements in augmented reality (AR) technologies, users can play AR
games in the Metaverse. However, these high-resolution games are
compute-intensive, and in-game graphical scenes need to be offloaded from
mobile devices to an edge server for computation. In this work, we consider an
optimization problem where the Metaverse Service Provider (MSP)'s objective is
to reduce downlink transmission latency of in-game graphics, the latency of
uplink data transmission, and the worst-case (greatest) battery charge
expenditure of user equipments (UEs), while maximizing the worst-case (lowest)
UE resolution-influenced in-game earning potential through optimizing the
downlink UE-Metaverse Base Station (UE-MBS) assignment and the uplink
transmission power selection. The downlink and uplink transmissions are then
executed asynchronously. We propose a multi-agent, loss-sharing (MALS)
reinforcement learning model to tackle the asynchronous and asymmetric problem.
We then compare the MALS model with other baseline models and show its
superiority over other methods. Finally, we conduct multi-variable optimization
weighting analyses and show the viability of using our proposed MALS algorithm
to tackle joint optimization problems.","['Terence Jie Chua', 'Wenhan Yu', 'Jun Zhao']",2023-03-18T00:06:04Z,http://arxiv.org/abs/2303.10289v2,['cs.NI']
"Transparent matte surfaces enabled by asymmetric diffusion of white
  light","The traditional wisdom for achieving transparency is to minimize disordered
scattering within and on the surface of materials, so as to avoid translucency.
However, the lack of disordered scattering also deprives the possibility of
achieving a matte surface, resulting in the specular reflection and glare on
transparent materials as a severe light pollution issue. In this work, we
propose a solution utilizing optical metasurfaces1-2 to overcome this
long-existing dilemma. Our approach leverages an asymmetric background in
metasurface design to achieve highly asymmetric diffusion of white light,
maximizing diffusion in reflection while minimizing it in transmission across
the entire visible spectrum. Using industrial lithography, we have created
macroscale transparent matte surfaces with both strong matte appearance and
clear transparency, defying the conventional belief that these two optical
features are incompatible. These surfaces provide a remarkable phenomenon of
switching between transparent or matte appearances via the brightness contrast
between the front and rear ambient lights. They also support a unique
application in transparent displays and augmented reality, offering perfectly
preserved clarity, wide viewing angles, full color, and one-sided displays
capabilities. Our findings usher in a new era of optical materials where the
desirable properties of both transparent and matte appearances can be
seamlessly merged.","['Hongchen Chu', 'Xiang Xiong', 'Nicholas X. Fang', 'Feng Wu', 'Runqi Jia', 'Ruwen Peng', 'Mu Wang', 'Yun Lai']",2023-03-22T06:13:31Z,http://arxiv.org/abs/2303.12333v2,['physics.optics']
Ambient Intelligence for Next-Generation AR,"Next-generation augmented reality (AR) promises a high degree of
context-awareness - a detailed knowledge of the environmental, user, social and
system conditions in which an AR experience takes place. This will facilitate
both the closer integration of the real and virtual worlds, and the provision
of context-specific content or adaptations. However, environmental awareness in
particular is challenging to achieve using AR devices alone; not only are these
mobile devices' view of an environment spatially and temporally limited, but
the data obtained by onboard sensors is frequently inaccurate and incomplete.
This, combined with the fact that many aspects of core AR functionality and
user experiences are impacted by properties of the real environment, motivates
the use of ambient IoT devices, wireless sensors and actuators placed in the
surrounding environment, for the measurement and optimization of environment
properties. In this book chapter we categorize and examine the wide variety of
ways in which these IoT sensors and actuators can support or enhance AR
experiences, including quantitative insights and proof-of-concept systems that
will inform the development of future solutions. We outline the challenges and
opportunities associated with several important research directions which must
be addressed to realize the full potential of next-generation AR.","['Tim Scargill', 'Sangjun Eom', 'Ying Chen', 'Maria Gorlatova']",2023-03-23T00:25:08Z,http://arxiv.org/abs/2303.12968v2,['cs.HC']
Augmented Reality Remote Operation of Dual Arm Manipulators in Hot Boxes,"In nuclear isotope and chemistry laboratories, hot cells and gloveboxes
provide scientists with a controlled and safe environment to perform
experiments. Working on experiments in these isolated containment cells
requires scientists to be physically present. For hot cell work today,
scientists manipulate equipment and radioactive material inside through a
bilateral mechanical control mechanism. Motions produced outside the cell with
the master control levers are mechanically transferred to the internal grippers
inside the shielded containment cell. There is a growing need to have the
capability to conduct experiments within these cells remotely. A simple method
to enable remote manipulations within hot cell and glovebox cells is to mount
two robotic arms inside a box to mimic the motions of human hands. An AR
application was built in this work to allow a user wearing a Microsoft HoloLens
2 headset to teleoperate dual arm manipulators by grasping robotic end-effector
digital replicas in AR from a remote location. In addition to the real-time
replica of the physical robotic arms in AR, the application enables users to
view a live video stream attached to the robotic arms and parse a 3D point
cloud of 3D objects in their remote AR environment for better situational
awareness. This work also provides users with virtual fixture to assist in
manipulation and other teleoperation tasks.","['Frank Regal', 'Young Soo Park', 'Jerry Nolan', 'Mitch Pryor']",2023-03-28T15:36:06Z,http://arxiv.org/abs/2303.16055v1,['cs.RO']
Impact of XR on Mental Health: Are we Playing with Fire?,"Extended reality (XR) technology has the incredible potential to
revolutionize mental health treatment and support, bringing a whole new
dimension to the field. Through the use of immersive virtual and augmented
reality experiences, individuals can enter entirely new worlds and realities
that provide a safe and controlled space for therapy and self-exploration.
Whether it's stepping into a calming natural environment, practicing social
interactions or confronting past traumas in a controlled environment, extended
reality offers endless possibilities. Engaging these virtual realities,
individuals can gain a deeper understanding of themselves and their emotions,
learn coping strategies, and practice important life skills in a way that is
both engaging and effective. The wonders of extended reality for mental health
are truly awe-inspiring and offer a powerful tool for improving the well-being
of individuals around the world. However, we should remember, everything has
its disadvantages, and XR is no different. While XR is a revolution, the human
brain is very complex, fragile and unique (like with fingerprints, no two
people have the same brain anatomy), leading to varying conditions, results,
experiences and consequences. This article presents insights and information on
how immersive interactive digital experiences can shape our minds and
behaviors. Research to date suggests that XR experiences can change regions of
the brain responsible for attention and visuospatial skills.",['Benjamin Kenwright'],2023-04-04T09:06:18Z,http://arxiv.org/abs/2304.01648v1,"['cs.HC', 'cs.CY']"
"Performance of 802.11be Wi-Fi 7 with Multi-Link Operation on AR
  Applications","Since its first release in the late 1990s, Wi-Fi has been updated to keep up
with evolving user needs. Recently, Wi-Fi and other radio access technologies
have been pushed to their edge when serving Augmented Reality (AR)
applications. AR applications require high throughput, low latency, and high
reliability to ensure a high-quality user experience. The 802.11be amendment,
which will be marketed as Wi-Fi 7, introduces several features that aim to
enhance its capabilities to support challenging applications like AR. One of
the main features introduced in this amendment is Multi-Link Operation (MLO)
which allows nodes to transmit and receive over multiple links concurrently.
When using MLO, traffic is distributed among links using an
implementation-specific traffic-to-link allocation policy. This paper aims to
evaluate the performance of MLO, using different policies, in serving AR
applications compared to Single-Link (SL). Experimental simulations using an
event-based Wi-Fi simulator have been conducted. Our results show the general
superiority of MLO when serving AR applications. MLO achieves lower latency and
serves a higher number of AR users compared to SL with the same frequency
resources. In addition, increasing the number of links can improve the
performance of MLO. Regarding traffic-to-link allocation policies, we found
that policies can be more susceptible to channel blocking, resulting in
possible performance degradation.","['Molham Alsakati', 'Charlie Pettersson', 'Sebastian Max', 'Vishnu Narayanan Moothedath', 'James Gross']",2023-04-04T10:44:58Z,http://arxiv.org/abs/2304.01693v1,['cs.NI']
"Dynamic dielectric metasurfaces via control of surface lattice
  resonances in non-homogeneous environment","Dynamic control of metamaterials and metasurfaces is crucial for many
photonic technologies, such as flat lenses, displays, augmented reality
devices, and beam steering, to name a few. The dynamic response is typically
achieved by controlling the phase and/or amplitude of individual meta-atom
resonances using electro-optic, phase-change or nonlinear effects. Here, we
propose and demonstrate a new practical strategy for the dynamic control of the
resonant interaction of light with dielectric metasurfaces, exploiting the
dependence of the interaction between meta-atoms in the array on the
inhomogeneity of the surrounding medium. The revealed tuning mechanisms are
based on the concept of the surface lattice resonance (SLR), the development of
which strongly depends on the difference between permittivities of superstrate
and substrate materials. We experimentally demonstrate surface lattice
resonances in dielectric (Si) metasurfaces, and reveal two tuning mechanisms
corresponding to shifting or damping of the SLR in optofluidic environment. The
demonstrated dynamic tuning effect with the observed vivid colour changes may
provide a dynamic metasurface approach with high spectral selectivity and
enhanced sensitivity for sensors, as well as high-resolution for small pixel
size displays.","['Izzatjon Allayarov', 'Andrey B. Evlyukhin', 'Diane J. Roth', 'Boris Chichkov', 'Anatoly V. Zayats', 'Antonio Calà Lesina']",2023-04-04T14:18:46Z,http://arxiv.org/abs/2304.01820v1,"['physics.optics', 'cond-mat.mes-hall', 'cond-mat.mtrl-sci']"
Visual Localization using Imperfect 3D Models from the Internet,"Visual localization is a core component in many applications, including
augmented reality (AR). Localization algorithms compute the camera pose of a
query image w.r.t. a scene representation, which is typically built from
images. This often requires capturing and storing large amounts of data,
followed by running Structure-from-Motion (SfM) algorithms. An interesting, and
underexplored, source of data for building scene representations are 3D models
that are readily available on the Internet, e.g., hand-drawn CAD models, 3D
models generated from building footprints, or from aerial images. These models
allow to perform visual localization right away without the time-consuming
scene capturing and model building steps. Yet, it also comes with challenges as
the available 3D models are often imperfect reflections of reality. E.g., the
models might only have generic or no textures at all, might only provide a
simple approximation of the scene geometry, or might be stretched. This paper
studies how the imperfections of these models affect localization accuracy. We
create a new benchmark for this task and provide a detailed experimental
evaluation based on multiple 3D models per scene. We show that 3D models from
the Internet show promise as an easy-to-obtain scene representation. At the
same time, there is significant room for improvement for visual localization
pipelines. To foster research on this interesting and challenging task, we
release our benchmark at v-pnk.github.io/cadloc.","['Vojtech Panek', 'Zuzana Kukelova', 'Torsten Sattler']",2023-04-12T16:15:05Z,http://arxiv.org/abs/2304.05947v1,"['cs.CV', 'I.2.10; I.4.8; I.4.9']"
"3DoF Localization from a Single Image and an Object Map: the Flatlandia
  Problem and Dataset","Efficient visual localization is crucial to many applications, such as
large-scale deployment of autonomous agents and augmented reality. Traditional
visual localization, while achieving remarkable accuracy, relies on extensive
3D models of the scene or large collections of geolocalized images, which are
often inefficient to store and to scale to novel environments. In contrast,
humans orient themselves using very abstract 2D maps, using the location of
clearly identifiable landmarks. Drawing on this and on the success of recent
works that explored localization on 2D abstract maps, we propose Flatlandia, a
novel visual localization challenge. With Flatlandia, we investigate whether it
is possible to localize a visual query by comparing the layout of its common
objects detected against the known spatial layout of objects in the map. We
formalize the challenge as two tasks at different levels of accuracy to
investigate the problem and its possible limitations; for each, we propose
initial baseline models and compare them against state-of-the-art 6DoF and 3DoF
methods. Code and dataset are publicly available at
github.com/IIT-PAVIS/Flatlandia.","['Matteo Toso', 'Matteo Taiana', 'Stuart James', 'Alessio Del Bue']",2023-04-13T09:53:09Z,http://arxiv.org/abs/2304.06373v4,['cs.CV']
"AdaMTL: Adaptive Input-dependent Inference for Efficient Multi-Task
  Learning","Modern Augmented reality applications require performing multiple tasks on
each input frame simultaneously. Multi-task learning (MTL) represents an
effective approach where multiple tasks share an encoder to extract
representative features from the input frame, followed by task-specific
decoders to generate predictions for each task. Generally, the shared encoder
in MTL models needs to have a large representational capacity in order to
generalize well to various tasks and input data, which has a negative effect on
the inference latency. In this paper, we argue that due to the large variations
in the complexity of the input frames, some computations might be unnecessary
for the output. Therefore, we introduce AdaMTL, an adaptive framework that
learns task-aware inference policies for the MTL models in an input-dependent
manner. Specifically, we attach a task-aware lightweight policy network to the
shared encoder and co-train it alongside the MTL model to recognize unnecessary
computations. During runtime, our task-aware policy network decides which parts
of the model to activate depending on the input frame and the target
computational complexity. Extensive experiments on the PASCAL dataset
demonstrate that AdaMTL reduces the computational complexity by 43% while
improving the accuracy by 1.32% compared to single-task models. Combined with
SOTA MTL methodologies, AdaMTL boosts the accuracy by 7.8% while improving the
efficiency by 3.1X. When deployed on Vuzix M4000 smart glasses, AdaMTL reduces
the inference latency and the energy consumption by up to 21.8% and 37.5%,
respectively, compared to the static MTL model. Our code is publicly available
at https://github.com/scale-lab/AdaMTL.git.","['Marina Neseem', 'Ahmed Agiza', 'Sherief Reda']",2023-04-17T20:17:44Z,http://arxiv.org/abs/2304.08594v1,['cs.CV']
Modular 3D Interface Design for Accessible VR Applications,"Designed with an accessible first design approach, the presented paper
describes how exploiting humans proprioception ability in 3D space can result
in a more natural interaction experience when using a 3D graphical user
interface in a virtual environment. The modularity of the designed interface
empowers the user to decide where they want to place interface elements in 3D
space allowing for a highly customizable experience, both in the context of the
player and the virtual space. Drawing inspiration from todays tangible
interfaces used, such as those in aircraft cockpits, a modular interface is
presented taking advantage of our natural understanding of interacting with 3D
objects and exploiting capabilities that otherwise have not been used in 2D
interaction. Additionally, the designed interface supports multimodal input
mechanisms which also demonstrates the opportunity for the design to cross over
to augmented reality applications. A focus group study was completed to better
understand the usability and constraints of the designed 3D GUI.","['Corrie Green', 'Dr Yang Jiang', 'Dr John Isaacs']",2023-04-08T17:07:46Z,http://arxiv.org/abs/2304.10541v2,['cs.HC']
"Revival of the Silk Road using the applications of AR/VR and its role on
  cultural tourism","This research project seeks to investigate the incorporation of augmented
reality (AR) and virtual reality (VR) technology with human-computer
interaction (HCI) in order to revitalize the Silk Road - specifically in
Kermanshah, Iran - and its effect on cultural tourism. Kermanshah has
underexplored the rich historical significance of the Silk Road, despite the
presence of 24 UNESCO World Heritage sites. From the 2nd century BCE to the
18th century CE, the Silk Road was a vital trade route connecting the West and
the East and had enormous cultural, economic, religious, and political effects.
The purpose of this study is to examine the application of AR/VR technologies
in HCI for the preservation, interpretation, and promotion of the Silk Road's
tangible and intangible cultural heritage in Kermanshah, as well as their
impact on cultural tourism development. The study also investigates how these
innovative technologies can enhance visitors' experiences through immersive and
interactive approaches, promote sustainable tourism practices, and contribute
to the region's broader socioeconomic benefits. The research will analyze the
challenges and opportunities of implementing AR/VR technology in HCI within the
context of cultural heritage and tourism in Kermanshah and the Silk Road region
more broadly. By combining HCI, AR/VR, and cultural tourism, this research
seeks to provide valuable insights into the development of user-centered,
immersive experiences that promote a deeper understanding and appreciation of
the Silk Road's distinctive cultural heritage.",['Sahar Zandi'],2023-04-13T11:31:53Z,http://arxiv.org/abs/2304.10545v1,['cs.HC']
"Privacy Computing Meets Metaverse: Necessity, Taxonomy and Challenges","Metaverse, the core of the next-generation Internet, is a computer-generated
holographic digital environment that simultaneously combines spatio-temporal,
immersive, real-time, sustainable, interoperable, and data-sensitive
characteristics. It cleverly blends the virtual and real worlds, allowing users
to create, communicate, and transact in virtual form. With the rapid
development of emerging technologies including augmented reality, virtual
reality and blockchain, the metaverse system is becoming more and more
sophisticated and widely used in various fields such as social, tourism,
industry and economy. However, the high level of interaction with the real
world also means a huge risk of privacy leakage both for individuals and
enterprises, which has hindered the wide deployment of metaverse. Then, it is
inevitable to apply privacy computing techniques in the framework of metaverse,
which is a current research hotspot. In this paper, we conduct comprehensive
research on the necessity, taxonomy and challenges when privacy computing meets
metaverse. Specifically, we first introduce the underlying technologies and
various applications of metaverse, on which we analyze the challenges of data
usage in metaverse, especially data privacy. Next, we review and summarize
state-of-the-art solutions based on federated learning, differential privacy,
homomorphic encryption, and zero-knowledge proofs for different privacy
problems in metaverse. Finally, we show the current security and privacy
challenges in the development of metaverse and provide open directions for
building a well-established privacy-preserving metaverse system. For easy
access and reference, we integrate the related publications and their codes
into a GitHub repository:
https://github.com/6lyc/Awesome-Privacy-Computing-in-Metaverse.git.","['Chuan Chen', 'Yuecheng Li', 'Zhenpeng Wu', 'Chengyuan Mai', 'Youming Liu', 'Yanming Hu', 'Zibin Zheng', 'Jiawen Kang']",2023-04-23T13:05:58Z,http://arxiv.org/abs/2304.11643v2,"['cs.CR', 'cs.CY']"
"Shape-Net: Room Layout Estimation from Panoramic Images Robust to
  Occlusion using Knowledge Distillation with 3D Shapes as Additional Inputs","Estimating the layout of a room from a single-shot panoramic image is
important in virtual/augmented reality and furniture layout simulation. This
involves identifying three-dimensional (3D) geometry, such as the location of
corners and boundaries, and performing 3D reconstruction. However, occlusion is
a common issue that can negatively impact room layout estimation, and this has
not been thoroughly studied to date. It is possible to obtain 3D shape
information of rooms as drawings of buildings and coordinates of corners from
image datasets, thus we propose providing both 2D panoramic and 3D information
to a model to effectively deal with occlusion. However, simply feeding 3D
information to a model is not sufficient to utilize the shape information for
an occluded area. Therefore, we improve the model by introducing 3D
Intersection over Union (IoU) loss to effectively use 3D information. In some
cases, drawings are not available or the construction deviates from a drawing.
Considering such practical cases, we propose a method for distilling knowledge
from a model trained with both images and 3D information to a model that takes
only images as input. The proposed model, which is called Shape-Net, achieves
state-of-the-art (SOTA) performance on benchmark datasets. We also confirmed
its effectiveness in dealing with occlusion through significantly improved
accuracy on images with occlusion compared with existing models.","['Mizuki Tabata', 'Kana Kurata', 'Junichiro Tamamatsu']",2023-04-25T07:45:43Z,http://arxiv.org/abs/2304.12624v1,['cs.CV']
"CarGameAR: An Integrated AR Car Game Authoring Interface for
  Custom-Built Car Programed on Arduino Board","In this paper, we present CarGameAR: An Integrated AR Car Game Authoring
Interface for Custom-Built Car Programed on Arduino Board. The car consists of
an Arduino board, an H-bridge, and motors. The objective of the project is to
create a system that can move a car in different directions using a computer
application. The system uses Unity software to create a virtual environment
where the user can control the car using keyboard commands. The car's motion is
achieved by sending signals from the computer to the Arduino board, which then
drives the motors through the H-bridge. The project provides a cost-effective
and efficient way to build a car, which can be used for educational purposes,
such as teaching programming. Moreover, this project is not limited to the
control of the car through keyboard commands in a virtual environment. The
system can be adapted to support augmented reality (AR) technology, providing
an even more immersive and engaging user experience. By integrating the car
with AR, the user can control the car's motion using physical gestures and
movements, adding an extra layer of interactivity to the system. This makes the
car an ideal platform for game development in AR, allowing the user to create
driving games that blend the physical and virtual worlds seamlessly.
Additionally, the car's affordability and ease of construction make it an
accessible and valuable tool for teaching programming and principles in a fun
and interactive way. Overall, this project demonstrates the versatility and
potential of the car system, highlighting the various applications and
possibilities it offers for both education and entertainment.","['Dang Bui', 'Wanwan Li', 'Hong Huang']",2023-04-28T20:36:24Z,http://arxiv.org/abs/2305.00084v1,['cs.HC']
"DreamPaint: Few-Shot Inpainting of E-Commerce Items for Virtual Try-On
  without 3D Modeling","We introduce DreamPaint, a framework to intelligently inpaint any e-commerce
product on any user-provided context image. The context image can be, for
example, the user's own image for virtual try-on of clothes from the e-commerce
catalog on themselves, the user's room image for virtual try-on of a piece of
furniture from the e-commerce catalog in their room, etc. As opposed to
previous augmented-reality (AR)-based virtual try-on methods, DreamPaint does
not use, nor does it require, 3D modeling of neither the e-commerce product nor
the user context. Instead, it directly uses 2D images of the product as
available in product catalog database, and a 2D picture of the context, for
example taken from the user's phone camera. The method relies on few-shot fine
tuning a pre-trained diffusion model with the masked latents (e.g., Masked
DreamBooth) of the catalog images per item, whose weights are then loaded on a
pre-trained inpainting module that is capable of preserving the characteristics
of the context image. DreamPaint allows to preserve both the product image and
the context (environment/user) image without requiring text guidance to
describe the missing part (product/context). DreamPaint also allows to
intelligently infer the best 3D angle of the product to place at the desired
location on the user context, even if that angle was previously unseen in the
product's reference 2D images. We compare our results against both text-guided
and image-guided inpainting modules and show that DreamPaint yields superior
performance in both subjective human study and quantitative metrics.","['Mehmet Saygin Seyfioglu', 'Karim Bouyarmane', 'Suren Kumar', 'Amir Tavanaei', 'Ismail B. Tutar']",2023-05-02T08:41:21Z,http://arxiv.org/abs/2305.01257v1,"['cs.CV', 'cs.AI']"
"Listen to Look into the Future: Audio-Visual Egocentric Gaze
  Anticipation","Egocentric gaze anticipation serves as a key building block for the emerging
capability of Augmented Reality. Notably, gaze behavior is driven by both
visual cues and audio signals during daily activities. Motivated by this
observation, we introduce the first model that leverages both the video and
audio modalities for egocentric gaze anticipation. Specifically, we propose a
Contrastive Spatial-Temporal Separable (CSTS) fusion approach that adopts two
modules to separately capture audio-visual correlations in spatial and temporal
dimensions, and applies a contrastive loss on the re-weighted audio-visual
features from fusion modules for representation learning. We conduct extensive
ablation studies and thorough analysis using two egocentric video datasets:
Ego4D and Aria, to validate our model design. We demonstrate the audio improves
the performance by +2.5% and +2.4% on the two datasets. Our model also
outperforms the prior state-of-the-art methods by at least +1.9% and +1.6%.
Moreover, we provide visualizations to show the gaze anticipation results and
provide additional insights into audio-visual representation learning. The code
and data split are available on our website
(https://bolinlai.github.io/CSTS-EgoGazeAnticipation/).","['Bolin Lai', 'Fiona Ryan', 'Wenqi Jia', 'Miao Liu', 'James M. Rehg']",2023-05-06T02:53:13Z,http://arxiv.org/abs/2305.03907v3,['cs.CV']
"An Object SLAM Framework for Association, Mapping, and High-Level Tasks","Object SLAM is considered increasingly significant for robot high-level
perception and decision-making. Existing studies fall short in terms of data
association, object representation, and semantic mapping and frequently rely on
additional assumptions, limiting their performance. In this paper, we present a
comprehensive object SLAM framework that focuses on object-based perception and
object-oriented robot tasks. First, we propose an ensemble data association
approach for associating objects in complicated conditions by incorporating
parametric and nonparametric statistic testing. In addition, we suggest an
outlier-robust centroid and scale estimation algorithm for modeling objects
based on the iForest and line alignment. Then a lightweight and object-oriented
map is represented by estimated general object models. Taking into
consideration the semantic invariance of objects, we convert the object map to
a topological map to provide semantic descriptors to enable multi-map matching.
Finally, we suggest an object-driven active exploration strategy to achieve
autonomous mapping in the grasping scenario. A range of public datasets and
real-world results in mapping, augmented reality, scene matching,
relocalization, and robotic manipulation have been used to evaluate the
proposed object SLAM framework for its efficient performance.","['Yanmin Wu', 'Yunzhou Zhang', 'Delong Zhu', 'Zhiqiang Deng', 'Wenkai Sun', 'Xin Chen', 'Jian Zhang']",2023-05-12T08:10:14Z,http://arxiv.org/abs/2305.07299v1,"['cs.RO', 'cs.CV']"
"Marsellus: A Heterogeneous RISC-V AI-IoT End-Node SoC with 2-to-8b DNN
  Acceleration and 30%-Boost Adaptive Body Biasing","Emerging Artificial Intelligence-enabled Internet-of-Things (AI-IoT)
System-on-a-Chip (SoC) for augmented reality, personalized healthcare, and
nano-robotics need to run many diverse tasks within a power envelope of a few
tens of mW over a wide range of operating conditions: compute-intensive but
strongly quantized Deep Neural Network (DNN) inference, as well as signal
processing and control requiring high-precision floating-point. We present
Marsellus, an all-digital heterogeneous SoC for AI-IoT end-nodes fabricated in
GlobalFoundries 22nm FDX that combines 1) a general-purpose cluster of 16
RISC-V Digital Signal Processing (DSP) cores attuned for the execution of a
diverse range of workloads exploiting 4-bit and 2-bit arithmetic extensions
(XpulpNN), combined with fused MAC&LOAD operations and floating-point support;
2) a 2-8bit Reconfigurable Binary Engine (RBE) to accelerate 3x3 and 1x1
(pointwise) convolutions in DNNs; 3) a set of On-Chip Monitoring (OCM) blocks
connected to an Adaptive Body Biasing (ABB) generator and a hardware control
loop, enabling on-the-fly adaptation of transistor threshold voltages.
Marsellus achieves up to 180 Gop/s or 3.32 Top/s/W on 2-bit precision
arithmetic in software, and up to 637 Gop/s or 12.4 Top/s/W on
hardware-accelerated DNN layers.","['Francesco Conti', 'Gianna Paulin', 'Angelo Garofalo', 'Davide Rossi', 'Alfio Di Mauro', 'Georg Rutishauser', 'Gianmarco Ottavi', 'Manuel Eggimann', 'Hayate Okuhara', 'Luca Benini']",2023-05-15T07:48:50Z,http://arxiv.org/abs/2305.08415v3,"['cs.AR', 'cs.AI', 'cs.LG']"
"Multi-microservice migration modelling, comparison, and potential in
  5G/6G mobile edge computing: A non-average parameter values approach","Cloud, fog, and edge computing integration with future mobile
Internet-of-Things (IoT) devices and related applications in 5G/6G networks
will become more practical in the coming years. Containers became the de facto
virtualization technique that replaced Virtual Memory (VM). Mobile IoT
applications, e.g., intelligent transportation and augmented reality,
incorporating fog-edge, have increased the demand for a millisecond-scale
response and processing time. Edge Computing reduces remote network traffic and
latency. These services must run on edge nodes that are physically close to
devices. However, classical migration techniques may not meet the requirements
of future mission-critical IoT applications. IoT mobile devices have limited
resources for running multiple services, and client-server latency worsens when
fog-edge services must migrate to maintain proximity in light of device
mobility. This study analyzes the performance of the MiGrror migration method
and the pre-copy live migration method when the migration of multiple
VMs/containers is considered. This paper presents mathematical models for the
stated methods and provides migration guidelines and comparisons for services
to be implemented as multiple containers, as in microservice-based
environments. Experiments demonstrate that MiGrror outperforms the pre-copy
technique and, unlike conventional live migrations, can maintain less than 10
milliseconds of downtime and reduce migration time with a minimal bandwidth
overhead. The results show that MiGrror can improve service continuity and
availability for users. Most significant is that the model can use average and
non-average values for different parameters during migration to achieve
improved and more accurate results, while other research typically only uses
average values. This paper shows that using only average parameter values in
migration can lead to inaccurate results.","['Arshin Rezazadeh', 'Hanan Lutfiyya']",2023-05-18T13:46:47Z,http://arxiv.org/abs/2305.10977v1,"['cs.NI', 'cs.DC']"
Towards a Robust Framework for NeRF Evaluation,"Neural Radiance Field (NeRF) research has attracted significant attention
recently, with 3D modelling, virtual/augmented reality, and visual effects
driving its application. While current NeRF implementations can produce high
quality visual results, there is a conspicuous lack of reliable methods for
evaluating them. Conventional image quality assessment methods and analytical
metrics (e.g. PSNR, SSIM, LPIPS etc.) only provide approximate indicators of
performance since they generalise the ability of the entire NeRF pipeline.
Hence, in this paper, we propose a new test framework which isolates the neural
rendering network from the NeRF pipeline and then performs a parametric
evaluation by training and evaluating the NeRF on an explicit radiance field
representation. We also introduce a configurable approach for generating
representations specifically for evaluation purposes. This employs ray-casting
to transform mesh models into explicit NeRF samples, as well as to ""shade""
these representations. Combining these two approaches, we demonstrate how
different ""tasks"" (scenes with different visual effects or learning strategies)
and types of networks (NeRFs and depth-wise implicit neural representations
(INRs)) can be evaluated within this framework. Additionally, we propose a
novel metric to measure task complexity of the framework which accounts for the
visual parameters and the distribution of the spatial data. Our approach offers
the potential to create a comparative objective evaluation framework for NeRF
methods.","['Adrian Azzarelli', 'Nantheera Anantrasirichai', 'David R Bull']",2023-05-29T13:30:26Z,http://arxiv.org/abs/2305.18079v3,"['cs.CV', 'cs.GR']"
Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields,"The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored ""fog"" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF","['Qianqiu Tan', 'Tao Liu', 'Yinling Xie', 'Shuwan Yu', 'Baohua Zhang']",2023-06-08T15:49:30Z,http://arxiv.org/abs/2306.05303v1,['cs.CV']
"HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection","An individualised head-related transfer function (HRTF) is very important for
creating realistic virtual reality (VR) and augmented reality (AR)
environments. However, acoustically measuring high-quality HRTFs requires
expensive equipment and an acoustic lab setting. To overcome these limitations
and to make this measurement more efficient HRTF upsampling has been exploited
in the past where a high-resolution HRTF is created from a low-resolution one.
This paper demonstrates how generative adversarial networks (GANs) can be
applied to HRTF upsampling. We propose a novel approach that transforms the
HRTF data for direct use with a convolutional super-resolution generative
adversarial network (SRGAN). This new approach is benchmarked against three
baselines: barycentric upsampling, spherical harmonic (SH) upsampling and an
HRTF selection approach. Experimental results show that the proposed method
outperforms all three baselines in terms of log-spectral distortion (LSD) and
localisation performance using perceptual models when the input HRTF is sparse
(less than 20 measured positions).","['Aidan O. T. Hogg', 'Mads Jenkins', 'He Liu', 'Isaac Squires', 'Samuel J. Cooper', 'Lorenzo Picinali']",2023-06-09T11:05:09Z,http://arxiv.org/abs/2306.05812v2,"['eess.AS', 'cs.CV', 'cs.HC', 'cs.LG', 'cs.SD', 'eess.SP']"
"Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine
  Perception","We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured
using Aria glasses with extensive object, environment, and human level ground
truth. This ADT release contains 200 sequences of real-world activities
conducted by Aria wearers in two real indoor scenes with 398 object instances
(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two
monochrome camera streams, one RGB camera stream, two IMU streams; b) complete
sensor calibration; c) ground truth data including continuous
6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye
gaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)
photo-realistic synthetic renderings. To the best of our knowledge, there is no
existing egocentric dataset with a level of accuracy, photo-realism and
comprehensiveness comparable to ADT. By contributing ADT to the research
community, our mission is to set a new standard for evaluation in the
egocentric machine perception domain, which includes very challenging research
problems such as 3D object detection and tracking, scene reconstruction and
understanding, sim-to-real learning, human pose prediction - while also
inspiring new machine perception tasks for augmented reality (AR) applications.
To kick start exploration of the ADT research use cases, we evaluated several
existing state-of-the-art methods for object detection, segmentation and image
translation tasks that demonstrate the usefulness of ADT as a benchmarking
dataset.","['Xiaqing Pan', 'Nicholas Charron', 'Yongqian Yang', 'Scott Peters', 'Thomas Whelan', 'Chen Kong', 'Omkar Parkhi', 'Richard Newcombe', 'Carl Yuheng Ren']",2023-06-10T06:46:32Z,http://arxiv.org/abs/2306.06362v2,"['cs.CV', 'cs.AI', 'cs.LG']"
Neural Projection Mapping Using Reflectance Fields,"We introduce a high resolution spatially adaptive light source, or a
projector, into a neural reflectance field that allows to both calibrate the
projector and photo realistic light editing. The projected texture is fully
differentiable with respect to all scene parameters, and can be optimized to
yield a desired appearance suitable for applications in augmented reality and
projection mapping. Our neural field consists of three neural networks,
estimating geometry, material, and transmittance. Using an analytical BRDF
model and carefully selected projection patterns, our acquisition process is
simple and intuitive, featuring a fixed uncalibrated projected and a handheld
camera with a co-located light source. As we demonstrate, the virtual projector
incorporated into the pipeline improves scene understanding and enables various
projection mapping applications, alleviating the need for time consuming
calibration steps performed in a traditional setting per view or projector
location. In addition to enabling novel viewpoint synthesis, we demonstrate
state-of-the-art performance projector compensation for novel viewpoints,
improvement over the baselines in material and scene reconstruction, and three
simply implemented scenarios where projection image optimization is performed,
including the use of a 2D generative model to consistently dictate scene
appearance from multiple viewpoints. We believe that neural projection mapping
opens up the door to novel and exciting downstream tasks, through the joint
optimization of the scene and projection images.","['Yotam Erel', 'Daisuke Iwai', 'Amit H. Bermano']",2023-06-11T05:33:10Z,http://arxiv.org/abs/2306.06595v1,['cs.CV']
"Learning to Estimate 6DoF Pose from Limited Data: A Few-Shot,
  Generalizable Approach using RGB Images","The accurate estimation of six degrees-of-freedom (6DoF) object poses is
essential for many applications in robotics and augmented reality. However,
existing methods for 6DoF pose estimation often depend on CAD templates or
dense support views, restricting their usefulness in realworld situations. In
this study, we present a new cascade framework named Cas6D for few-shot 6DoF
pose estimation that is generalizable and uses only RGB images. To address the
false positives of target object detection in the extreme few-shot setting, our
framework utilizes a selfsupervised pre-trained ViT to learn robust feature
representations. Then, we initialize the nearest top-K pose candidates based on
similarity score and refine the initial poses using feature pyramids to
formulate and update the cascade warped feature volume, which encodes context
at increasingly finer scales. By discretizing the pose search range using
multiple pose bins and progressively narrowing the pose search range in each
stage using predictions from the previous stage, Cas6D can overcome the large
gap between pose candidates and ground truth poses, which is a common failure
mode in sparse-view scenarios. Experimental results on the LINEMOD and GenMOP
datasets demonstrate that Cas6D outperforms state-of-the-art methods by 9.2%
and 3.8% accuracy (Proj-5) under the 32-shot setting compared to OnePose++ and
Gen6D.","['Panwang Pan', 'Zhiwen Fan', 'Brandon Y. Feng', 'Peihao Wang', 'Chenxin Li', 'Zhangyang Wang']",2023-06-13T07:45:42Z,http://arxiv.org/abs/2306.07598v1,['cs.CV']
"Learning to Assist and Communicate with Novice Drone Pilots for Expert
  Level Performance","Multi-task missions for unmanned aerial vehicles (UAVs) involving inspection
and landing tasks are challenging for novice pilots due to the difficulties
associated with depth perception and the control interface. We propose a shared
autonomy system, alongside supplementary information displays, to assist pilots
to successfully complete multi-task missions without any pilot training. Our
approach comprises of three modules: (1) a perception module that encodes
visual information onto a latent representation, (2) a policy module that
augments pilot's actions, and (3) an information augmentation module that
provides additional information to the pilot. The policy module is trained in
simulation with simulated users and transferred to the real world without
modification in a user study (n=29), alongside supplementary information
schemes including learnt red/green light feedback cues and an augmented reality
display. The pilot's intent is unknown to the policy module and is inferred
from the pilot's input and UAV's states. The assistant increased task success
rate for the landing and inspection tasks from [16.67% & 54.29%] respectively
to [95.59% & 96.22%]. With the assistant, inexperienced pilots achieved similar
performance to experienced pilots. Red/green light feedback cues reduced the
required time by 19.53% and trajectory length by 17.86% for the inspection
task, where participants rated it as their preferred condition due to the
intuitive interface and providing reassurance. This work demonstrates that
simple user models can train shared autonomy systems in simulation, and
transfer to physical tasks to estimate user intent and provide effective
assistance and information to the pilot.","['Kal Backman', 'Dana Kulić', 'Hoam Chung']",2023-06-16T02:59:20Z,http://arxiv.org/abs/2306.09600v1,['cs.RO']
"Supporting Construction and Architectural Visualization through BIM and
  AR/VR: A Systematic Literature Review","The Architecture, Engineering, Construction, and Facility Management (AEC/FM)
industry deals with the design, construction, and operation of complex
buildings. Today, Building Information Modeling (BIM) is used to represent
information about a building in a single, non-redundant representation. Here,
Augmented Reality (AR) and Virtual Reality (VR) can improve the visualization
and interaction with the resulting model by augmenting the real world with
information from the BIM model or allowing a user to immerse in a virtual world
generated from the BIM model. This can improve the design, construction, and
operation of buildings. While an increasing number of studies in HCI,
construction, or engineering have shown the potential of using AR and VR
technology together with BIM, often research remains focused on individual
explorations and key design strategies. In addition to that, a systematic
overview and discussion of recent works combining AR/VR with BIM are not yet
fully covered. Therefore, this paper systematically reviews recent approaches
combining AR/VR with BIM and categorizes the literature by the building's
lifecycle phase while systematically describing relevant use cases. In total,
32 out of 447 papers between 2017 and 2022 were categorized. The categorization
shows that most approaches focus on the construction phase and the use case of
review and quality assurance. In the design phase, most approaches use VR,
while in the construction and operation phases, AR is prevalent.","['Enes Yigitbas', 'Alexander Nowosad', 'Gregor Engels']",2023-06-21T13:52:37Z,http://arxiv.org/abs/2306.12274v1,['cs.HC']
"Multi-Agent Deep Reinforcement Learning for Dynamic Avatar Migration in
  AIoT-enabled Vehicular Metaverses with Trajectory Prediction","Avatars, as promising digital assistants in Vehicular Metaverses, can enable
drivers and passengers to immerse in 3D virtual spaces, serving as a practical
emerging example of Artificial Intelligence of Things (AIoT) in intelligent
vehicular environments. The immersive experience is achieved through seamless
human-avatar interaction, e.g., augmented reality navigation, which requires
intensive resources that are inefficient and impractical to process on
intelligent vehicles locally. Fortunately, offloading avatar tasks to RoadSide
Units (RSUs) or cloud servers for remote execution can effectively reduce
resource consumption. However, the high mobility of vehicles, the dynamic
workload of RSUs, and the heterogeneity of RSUs pose novel challenges to making
avatar migration decisions. To address these challenges, in this paper, we
propose a dynamic migration framework for avatar tasks based on real-time
trajectory prediction and Multi-Agent Deep Reinforcement Learning (MADRL).
Specifically, we propose a model to predict the future trajectories of
intelligent vehicles based on their historical data, indicating the future
workloads of RSUs.Based on the expected workloads of RSUs, we formulate the
avatar task migration problem as a long-term mixed integer programming problem.
To tackle this problem efficiently, the problem is transformed into a Partially
Observable Markov Decision Process (POMDP) and solved by multiple DRL agents
with hybrid continuous and discrete actions in decentralized. Numerical results
demonstrate that our proposed algorithm can effectively reduce the latency of
executing avatar tasks by around 25% without prediction and 30% with prediction
and enhance user immersive experiences in the AIoT-enabled Vehicular Metaverse
(AeVeM).","['Junlong Chen', 'Jiawen Kang', 'Minrui Xu', 'Zehui Xiong', 'Dusit Niyato', 'Chuan Chen', 'Abbas Jamalipour', 'Shengli Xie']",2023-06-26T13:27:11Z,http://arxiv.org/abs/2306.14683v1,"['cs.AI', 'cs.LG', 'eess.SP']"
VibHead: An Authentication Scheme for Smart Headsets through Vibration,"Recent years have witnessed the fast penetration of Virtual Reality (VR) and
Augmented Reality (AR) systems into our daily life, the security and privacy
issues of the VR/AR applications have been attracting considerable attention.
Most VR/AR systems adopt head-mounted devices (i.e., smart headsets) to
interact with users and the devices usually store the users' private data.
Hence, authentication schemes are desired for the head-mounted devices.
Traditional knowledge-based authentication schemes for general personal devices
have been proved vulnerable to shoulder-surfing attacks, especially considering
the headsets may block the sight of the users. Although the robustness of the
knowledge-based authentication can be improved by designing complicated secret
codes in virtual space, this approach induces a compromise of usability.
Another choice is to leverage the users' biometrics; however, it either relies
on highly advanced equipments which may not always be available in commercial
headsets or introduce heavy cognitive load to users.
  In this paper, we propose a vibration-based authentication scheme, VibHead,
for smart headsets. Since the propagation of vibration signals through human
heads presents unique patterns for different individuals, VibHead employs a
CNN-based model to classify registered legitimate users based the features
extracted from the vibration signals. We also design a two-step authentication
scheme where the above user classifiers are utilized to distinguish the
legitimate user from illegitimate ones. We implement VibHead on a Microsoft
HoloLens equipped with a linear motor and an IMU sensor which are commonly used
in off-the-shelf personal smart devices. According to the results of our
extensive experiments, with short vibration signals ($\leq 1s$), VibHead has an
outstanding authentication accuracy; both FAR and FRR are around 5%.","['Feng Li', 'Jiayi Zhao', 'Huan Yang', 'Dongxiao Yu', 'Yuanfeng Zhou', 'Yiran Shen']",2023-06-29T15:00:32Z,http://arxiv.org/abs/2306.17002v1,['cs.CR']
"Robust Roadside Perception: an Automated Data Synthesis Pipeline
  Minimizing Human Annotation","Recently, advancements in vehicle-to-infrastructure communication
technologies have elevated the significance of infrastructure-based roadside
perception systems for cooperative driving. This paper delves into one of its
most pivotal challenges: data insufficiency. The lacking of high-quality
labeled roadside sensor data with high diversity leads to low robustness, and
low transfer-ability of current roadside perception systems. In this paper, a
novel solution is proposed to address this problem that creates synthesized
training data using Augmented Reality. A Generative Adversarial Network is then
applied to enhance the reality further, that produces a photo-realistic
synthesized dataset that is capable of training or fine-tuning a roadside
perception detector which is robust to different weather and lighting
conditions. Our approach was rigorously tested at two key intersections in
Michigan, USA: the Mcity intersection and the State St./Ellsworth Rd
roundabout. The Mcity intersection is located within the Mcity test field, a
controlled testing environment. In contrast, the State St./Ellsworth Rd
intersection is a bustling roundabout notorious for its high traffic flow and a
significant number of accidents annually. Experimental results demonstrate that
detectors trained solely on synthesized data exhibit commendable performance
across all conditions. Furthermore, when integrated with labeled data, the
synthesized data can notably bolster the performance of pre-existing detectors,
especially in adverse conditions.","['Rusheng Zhang', 'Depu Meng', 'Lance Bassett', 'Shengyin Shen', 'Zhengxia Zou', 'Henry X. Liu']",2023-06-29T21:00:57Z,http://arxiv.org/abs/2306.17302v2,"['cs.CV', 'cs.RO', 'eess.IV']"
"FedHIL: Heterogeneity Resilient Federated Learning for Robust Indoor
  Localization with Mobile Devices","Indoor localization plays a vital role in applications such as emergency
response, warehouse management, and augmented reality experiences. By deploying
machine learning (ML) based indoor localization frameworks on their mobile
devices, users can localize themselves in a variety of indoor and subterranean
environments. However, achieving accurate indoor localization can be
challenging due to heterogeneity in the hardware and software stacks of mobile
devices, which can result in inconsistent and inaccurate location estimates.
Traditional ML models also heavily rely on initial training data, making them
vulnerable to degradation in performance with dynamic changes across indoor
environments. To address the challenges due to device heterogeneity and lack of
adaptivity, we propose a novel embedded ML framework called FedHIL. Our
framework combines indoor localization and federated learning (FL) to improve
indoor localization accuracy in device-heterogeneous environments while also
preserving user data privacy. FedHIL integrates a domain-specific selective
weight adjustment approach to preserve the ML model's performance for indoor
localization during FL, even in the presence of extremely noisy data.
Experimental evaluations in diverse real-world indoor environments and with
heterogeneous mobile devices show that FedHIL outperforms state-of-the-art FL
and non-FL indoor localization frameworks. FedHIL is able to achieve 1.62x
better localization accuracy on average than the best performing FL-based
indoor localization framework from prior work.","['Danish Gufran', 'Sudeep Pasricha']",2023-07-04T15:34:13Z,http://arxiv.org/abs/2307.01780v1,"['cs.LG', 'cs.MA', 'eess.SP']"
"Articulated 3D Head Avatar Generation using Text-to-Image Diffusion
  Models","The ability to generate diverse 3D articulated head avatars is vital to a
plethora of applications, including augmented reality, cinematography, and
education. Recent work on text-guided 3D object generation has shown great
promise in addressing these needs. These methods directly leverage pre-trained
2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance
fields of generic objects. However, due to the lack of geometry and texture
priors, these methods have limited control over the generated 3D objects,
making it difficult to operate inside a specific domain, e.g., human heads. In
this work, we develop a new approach to text-guided 3D head avatar generation
to address this limitation. Our framework directly operates on the geometry and
texture of an articulable 3D morphable model (3DMM) of a head, and introduces
novel optimization procedures to update the geometry and texture while keeping
the 2D and 3D facial features aligned. The result is a 3D head avatar that is
consistent with the text description and can be readily articulated using the
deformation model of the 3DMM. We show that our diffusion-based articulated
head avatars outperform state-of-the-art approaches for this task. The latter
are typically based on CLIP, which is known to provide limited diversity of
generation and accuracy for 3D object generation.","['Alexander W. Bergman', 'Wang Yifan', 'Gordon Wetzstein']",2023-07-10T19:15:32Z,http://arxiv.org/abs/2307.04859v1,"['cs.CV', 'cs.GR', 'cs.LG']"
"EgoAdapt: A multi-stream evaluation study of adaptation to real-world
  egocentric user video","In egocentric action recognition a single population model is typically
trained and subsequently embodied on a head-mounted device, such as an
augmented reality headset. While this model remains static for new users and
environments, we introduce an adaptive paradigm of two phases, where after
pretraining a population model, the model adapts on-device and online to the
user's experience. This setting is highly challenging due to the change from
population to user domain and the distribution shifts in the user's data
stream. Coping with the latter in-stream distribution shifts is the focus of
continual learning, where progress has been rooted in controlled benchmarks but
challenges faced in real-world applications often remain unaddressed. We
introduce EgoAdapt, a benchmark for real-world egocentric action recognition
that facilitates our two-phased adaptive paradigm, and real-world challenges
naturally occur in the egocentric video streams from Ego4d, such as long-tailed
action distributions and large-scale classification over 2740 actions. We
introduce an evaluation framework that directly exploits the user's data stream
with new metrics to measure the adaptation gain over the population model,
online generalization, and hindsight performance. In contrast to single-stream
evaluation in existing works, our framework proposes a meta-evaluation that
aggregates the results from 50 independent user streams. We provide an
extensive empirical study for finetuning and experience replay.","['Matthias De Lange', 'Hamid Eghbalzadeh', 'Reuben Tan', 'Michael Iuzzolino', 'Franziska Meier', 'Karl Ridgeway']",2023-07-11T20:23:23Z,http://arxiv.org/abs/2307.05784v1,"['cs.CV', 'cs.AI']"
"Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities","In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.","['Kai Li', 'Billy Pik Lik Lau', 'Xin Yuan', 'Wei Ni', 'Mohsen Guizani', 'Chau Yuen']",2023-07-13T11:14:46Z,http://arxiv.org/abs/2307.06687v2,"['cs.HC', 'cs.AI', 'cs.NI']"
Toward Scalable and Controllable AR Experimentation,"To understand how well a proposed augmented reality (AR) solution works,
existing papers often conducted tailored and isolated evaluations for specific
AR tasks, e.g., depth or lighting estimation, and compared them to
easy-to-setup baselines, either using datasets or resorting to time-consuming
data capturing. Conceptually simple, it can be extremely difficult to evaluate
an AR system fairly and in scale to understand its real-world performance. The
difficulties arise for three key reasons: lack of control of the physical
environment, the time-consuming data capturing, and the difficulties to
reproduce baseline results.
  This paper presents our design of an AR experimentation platform, ExpAR,
aiming to provide scalable and controllable AR experimentation. ExpAR is
envisioned to operate as a standalone deployment or a federated platform; in
the latter case, AR researchers can contribute physical resources, including
scene setup and capturing devices, and allow others to time share these
resources. Our design centers around the generic
sensing-understanding-rendering pipeline and is driven by the evaluation
limitations observed in recent AR systems papers. We demonstrate the
feasibility of this vision with a preliminary prototype and our preliminary
evaluations suggest the importance of further investigating different device
capabilities to stream in 30 FPS.
  The ExpAR project site can be found at https://cake.wpi.edu/expar.","['Ashkan Ganj', 'Yiqin Zhao', 'Federico Galbiati', 'Tian Guo']",2023-07-17T15:58:10Z,http://arxiv.org/abs/2307.08587v1,"['cs.DC', 'cs.HC']"
"Education 5.0: Requirements, Enabling Technologies, and Future
  Directions","We are currently in a post-pandemic era in which life has shifted to a
digital world. This has affected many aspects of life, including education and
learning. Education 5.0 refers to the fifth industrial revolution in education
by leveraging digital technologies to eliminate barriers to learning, enhance
learning methods, and promote overall well-being. The concept of Education 5.0
represents a new paradigm in the field of education, one that is focused on
creating a learner-centric environment that leverages the latest technologies
and teaching methods. This paper explores the key requirements of Education 5.0
and the enabling technologies that make it possible, including artificial
intelligence, blockchain, and virtual and augmented reality. We analyze the
potential impact of these technologies on the future of education, including
their ability to improve personalization, increase engagement, and provide
greater access to education. Additionally, we examine the challenges and
ethical considerations associated with Education 5.0 and propose strategies for
addressing these issues. Finally, we offer insights into future directions for
the development of Education 5.0, including the need for ongoing research,
collaboration, and innovation in the field. Overall, this paper provides a
comprehensive overview of Education 5.0, its requirements, enabling
technologies, and future directions, and highlights the potential of this new
paradigm to transform education and improve learning outcomes for students.","['Shabir Ahmad', 'Sabina Umirzakova', 'Ghulam Mujtaba', 'Muhammad Sadiq Amin', 'Taegkeun Whangbo']",2023-07-29T00:31:11Z,http://arxiv.org/abs/2307.15846v1,['cs.CY']
"From Talent Shortage to Workforce Excellence in the CHIPS Act Era:
  Harnessing Industry 4.0 Paradigms for a Sustainable Future in Domestic Chip
  Production","The CHIPS Act is driving the U.S. towards a self-sustainable future in
domestic chip production. Decades of outsourced manufacturing, assembly,
testing, and packaging has diminished the workforce ecosystem, imposing major
limitations on semiconductor companies racing to build new fabrication sites as
part of the CHIPS Act. In response, a systemic alliance between academic
institutions, the industry, government, various consortiums, and organizations
has emerged to establish a pipeline to educate and onboard the next generation
of talent. Establishing a stable and continuous flow of talent requires
significant time investments and comes with no guarantees, particularly
factoring in the low workplace desirability in current fabrication houses for
U.S workforce. This paper will explore the feasibility of two paradigms of
Industry 4.0, automation and Augmented Reality(AR)/Virtual Reality(VR), to
complement ongoing workforce development efforts and optimize workplace
desirability by catalyzing core manufacturing processes and effectively
enhancing the education, onboarding, and professional realms-all with promising
capabilities amid the ongoing talent shortage and trajectory towards advanced
packaging.","['Aida Damanpak Rizi', 'Antika Roy', 'Rouhan Noor', 'Hyo Kang', 'Nitin Varshney', 'Katja Jacob', 'Sindia Rivera-Jimenez', 'Nathan Edwards', 'Volker J. Sorger', 'Hamed Dalir', 'Navid Asadizanjani']",2023-08-01T01:15:51Z,http://arxiv.org/abs/2308.00215v1,['cs.CY']
"Metaverse for Industry 5.0 in NextG Communications: Potential
  Applications and Future Challenges","With the advent of new technologies and endeavors for automation in almost
all day-to-day activities, the recent discussions on the metaverse life have a
greater expectation. Furthermore, we are in the era of the fifth industrial
revolution, where machines and humans collaborate to maximize productivity with
the effective utilization of human intelligence and other resources. Hence,
Industry 5.0 in the metaverse may have tremendous technological integration for
a more immersive experience and enhanced communication.These technological
amalgamations are suitable for the present environment and entirely different
from the previous perception of virtual technologies. This work presents a
comprehensive review of the applications of the metaverse in Industry 5.0
(so-called industrial metaverse). In particular, we first provide a preliminary
to the metaverse and industry 5.0 and discuss key enabling technologies of the
industrial metaverse, including virtual and augmented reality, 3D modeling,
artificial intelligence, edge computing, digital twin, blockchain, and 6G
communication networks. This work then explores diverse metaverse applications
in Industry 5.0 vertical domains like Society 5.0, agriculture, supply chain
management, healthcare, education, and transportation. A number of research
projects are presented to showcase the conceptualization and implementation of
the industrial metaverse. Furthermore, various challenges in realizing the
industrial metaverse, feasible solutions, and future directions for further
research have been presented.","['B. Prabadevi', 'N. Deepa', 'Nancy Victor', 'Thippa Reddy Gadekallu', 'Praveen Kumar Reddy Maddikunta', 'Gokul Yenduri', 'Wei Wang', 'Quoc Viet Pham', 'Thien Huynh-The', 'Madhusanka Liyanage']",2023-07-31T07:21:36Z,http://arxiv.org/abs/2308.02677v1,['cs.CY']
"Service Reservation and Pricing for Green Metaverses: A Stackelberg Game
  Approach","Metaverse enables users to communicate, collaborate and socialize with each
other through their digital avatars. Due to the spatio-temporal
characteristics, co-located users are served well by performing their software
components in a collaborative manner such that a Metaverse service provider
(MSP) eliminates redundant data transmission and processing, ultimately
reducing the total energy consumption. The energyefficient service provision is
crucial for enabling the green and sustainable Metaverse. In this article, we
take an augmented reality (AR) application as an example to achieve this goal.
Moreover, we study an economic issue on how the users reserve offloading
services from the MSP and how the MSP determines an optimal charging price
since each user is rational to decide whether to accept the offloading service
by taking into account the monetary cost. A single-leader multi-follower
Stackelberg game is formulated between the MSP and users while each user
optimizes an offloading probability to minimize the weighted sum of time,
energy consumption and monetary cost. Numerical results show that our scheme
achieves energy savings and satisfies individual rationality simultaneously
compared with the conventional schemes. Finally, we identify and discuss open
directions on how several emerging technologies are combined with the
sustainable green Metaverse.","['Xumin Huang', 'Yuan Wu', 'Jiawen Kang', 'Jiangtian Nie', 'Weifeng Zhong', 'Dong In Kim', 'Shengli Xie']",2023-08-09T12:27:49Z,http://arxiv.org/abs/2308.04914v1,['cs.AI']
"Sound propagation in realistic interactive 3D scenes with parameterized
  sources using deep neural operators","We address the challenge of sound propagation simulations in 3D virtual rooms
with moving sources, which have applications in virtual/augmented reality, game
audio, and spatial computing. Solutions to the wave equation can describe wave
phenomena such as diffraction and interference. However, simulating them using
conventional numerical discretization methods with hundreds of source and
receiver positions is intractable, making stimulating a sound field with moving
sources impractical. To overcome this limitation, we propose using deep
operator networks to approximate linear wave-equation operators. This enables
the rapid prediction of sound propagation in realistic 3D acoustic scenes with
moving sources, achieving millisecond-scale computations. By learning a compact
surrogate model, we avoid the offline calculation and storage of impulse
responses for all relevant source/listener pairs. Our experiments, including
various complex scene geometries, show good agreement with reference solutions,
with root mean squared errors ranging from 0.02 Pa to 0.10 Pa. Notably, our
method signifies a paradigm shift as no prior machine learning approach has
achieved precise predictions of complete wave fields within realistic domains.
We anticipate that our findings will drive further exploration of deep neural
operator methods, advancing research in immersive user experiences within
virtual environments.$","['Nikolas Borrel-Jensen', 'Somdatta Goswami', 'Allan P. Engsig-Karup', 'George Em Karniadakis', 'Cheol-Ho Jeong']",2023-08-09T16:32:51Z,http://arxiv.org/abs/2308.05141v2,"['cs.SD', 'cs.LG', 'eess.AS']"
"Robust Localization with Visual-Inertial Odometry Constraints for
  Markerless Mobile AR","Visual Inertial Odometry (VIO) is an essential component of modern Augmented
Reality (AR) applications. However, VIO only tracks the relative pose of the
device, leading to drift over time. Absolute pose estimation methods infer the
device's absolute pose, but their accuracy depends on the input quality. This
paper introduces VIO-APR, a new framework for markerless mobile AR that
combines an absolute pose regressor (APR) with a local VIO tracking system.
VIO-APR uses VIO to assess the reliability of the APR and the APR to identify
and compensate for VIO drift. This feedback loop results in more accurate
positioning and more stable AR experiences. To evaluate VIO-APR, we created a
dataset that combines camera images with ARKit's VIO system output for six
indoor and outdoor scenes of various scales. Over this dataset, VIO-APR
improves the median accuracy of popular APR by up to 36\% in position and 29\%
in orientation, increases the percentage of frames in the high ($0.25 m,
2^{\circ}$) accuracy level by up to 112\% and reduces the percentage of frames
predicted below the low ($5 m, 10^\circ$) accuracy greatly. We implement
VIO-APR into a mobile AR application using Unity to demonstrate its
capabilities. VIO-APR results in noticeably more accurate localization and a
more stable overall experience.","['Changkun Liu', 'Yukun Zhao', 'Tristan Braud']",2023-08-10T07:21:35Z,http://arxiv.org/abs/2308.05394v2,['cs.CV']
KS-APR: Keyframe Selection for Robust Absolute Pose Regression,"Markerless Mobile Augmented Reality (AR) aims to anchor digital content in
the physical world without using specific 2D or 3D objects. Absolute Pose
Regressors (APR) are end-to-end machine learning solutions that infer the
device's pose from a single monocular image. Thanks to their low computation
cost, they can be directly executed on the constrained hardware of mobile AR
devices. However, APR methods tend to yield significant inaccuracies for input
images that are too distant from the training set. This paper introduces
KS-APR, a pipeline that assesses the reliability of an estimated pose with
minimal overhead by combining the inference results of the APR and the prior
images in the training set. Mobile AR systems tend to rely upon visual-inertial
odometry to track the relative pose of the device during the experience. As
such, KS-APR favours reliability over frequency, discarding unreliable poses.
This pipeline can integrate most existing APR methods to improve accuracy by
filtering unreliable images with their pose estimates. We implement the
pipeline on three types of APR models on indoor and outdoor datasets. The
median error on position and orientation is reduced for all models, and the
proportion of large errors is minimized across datasets. Our method enables
state-of-the-art APRs such as DFNetdm to outperform single-image and sequential
APR methods. These results demonstrate the scalability and effectiveness of
KS-APR for visual localization tasks that do not require one-shot decisions.","['Changkun Liu', 'Yukun Zhao', 'Tristan Braud']",2023-08-10T09:32:20Z,http://arxiv.org/abs/2308.05459v2,['cs.CV']
"Encode-Store-Retrieve: Enhancing Memory Augmentation through
  Language-Encoded Egocentric Perception","We depend on our own memory to encode, store, and retrieve our experiences.
However, memory lapses can occur. One promising avenue for achieving memory
augmentation is through the use of augmented reality head-mounted displays to
capture and preserve egocentric videos, a practice commonly referred to as life
logging. However, a significant challenge arises from the sheer volume of video
data generated through life logging, as the current technology lacks the
capability to encode and store such large amounts of data efficiently. Further,
retrieving specific information from extensive video archives requires
substantial computational power, further complicating the task of quickly
accessing desired content. To address these challenges, we propose a memory
augmentation system that involves leveraging natural language encoding for
video data and storing them in a vector database. This approach harnesses the
power of large vision language models to perform the language encoding process.
Additionally, we propose using large language models to facilitate natural
language querying. Our system underwent extensive evaluation using the QA-Ego4D
dataset and achieved state-of-the-art results with a BLEU score of 8.3,
outperforming conventional machine learning models that scored between 3.4 and
5.8. Additionally, in a user study, our system received a higher mean response
score of 4.13/5 compared to the human participants' score of 2.46/5 on
real-life episodic memory tasks.","['Junxiao Shen', 'John Dudley', 'Per Ola Kristensson']",2023-08-10T18:43:44Z,http://arxiv.org/abs/2308.05822v1,"['cs.CV', 'cs.AI', 'cs.HC']"
"Energy-Efficient Deadline-Aware Edge Computing: Bandit Learning with
  Partial Observations in Multi-Channel Systems","In this paper, we consider a task offloading problem in a multi-access edge
computing (MEC) network, in which edge users can either use their local
processing unit to compute their tasks or offload their tasks to a nearby edge
server through multiple communication channels each with different
characteristics. The main objective is to maximize the energy efficiency of the
edge users while meeting computing tasks deadlines. In the multi-user
multi-channel offloading scenario, users are distributed with partial
observations of the system states. We formulate this problem as a stochastic
optimization problem and leverage \emph{contextual neural multi-armed bandit}
models to develop an energy-efficient deadline-aware solution, dubbed E2DA. The
proposed E2DA framework only relies on partial state information (i.e.,
computation task features) to make offloading decisions. Through extensive
numerical analysis, we demonstrate that the E2DA algorithm can efficiently
learn an offloading policy and achieve close-to-optimal performance in
comparison with several baseline policies that optimize energy consumption
and/or response time. Furthermore, we provide a comprehensive set of results on
the MEC system performance for various applications such as augmented reality
(AR) and virtual reality (VR).","['Babak Badnava', 'Keenan Roach', 'Kenny Cheung', 'Morteza Hashemi', 'Ness B Shroff']",2023-08-12T21:48:04Z,http://arxiv.org/abs/2308.06647v1,"['cs.NI', 'cs.DC', 'cs.IT', 'math.IT']"
"The Impact of Different Virtual Work Environments on Flow, Performance,
  User Emotions, and Preferences","This research explores how different virtual work environments, differing in
the type and amount of elements they include, impact users' flow, performance,
emotional state, and preferences. Pre-study interviews were conducted to inform
the design of three VR work environments: the Dark Room, the Empty Room, and
the Furnished Room. Fifteen participants took part in a user study where they
engaged in a logic-based task simulating deep work while experiencing each
environment. The findings suggest that while objective performance measures did
not differ significantly, subjective experiences and perceptions varied across
the environments. Participants reported feeling less distracted and more
focused in the Dark Room and the Empty Room compared to the Furnished Room. The
Empty Room was associated with the highest levels of relaxation and calmness,
while the Furnished Room was perceived as visually appealing yet more
distracting. These findings highlight the variability of user preferences and
emphasise the importance of considering user comfort and well-being in the
design of virtual work environments. The study contributes to the better
understanding of virtual workspaces and provides insights for designing
environments that promote flow, productivity, and user well-being.","['Alicja Kiluk', 'Viktorija Paneva', 'Sofia Seinfeld', 'Jörg Müller']",2023-08-14T13:29:29Z,http://arxiv.org/abs/2308.07129v1,['cs.HC']
"Feel the Breeze: Promoting Relaxation in Virtual Reality using Mid-Air
  Haptics","Mid-air haptic interfaces employ focused ultrasound waves to generate
touchless haptic sensations on the skin. Prior studies have demonstrated the
potential positive impact of mid-air haptic feedback on virtual experiences,
enhancing aspects such as enjoyment, immersion, and sense of agency. As a
highly immersive environment, Virtual Reality (VR) is being explored as a tool
for stress management and relaxation in current research. However, the impact
of incorporating mid-air haptic stimuli into relaxing experiences in VR has not
been studied thus far. In this paper, for the first time, we design a mid-air
haptic stimulation that is congruent with a relaxing scene in VR, and conduct a
user study investigating the effectiveness of this experience. Our user study
encompasses three different conditions: a control group with no relaxation
intervention, a VR-only relaxation experience, and a VR+Haptics relaxation
experience that includes the mid-air haptic feedback. While we did not find any
significant differences between the conditions, a trend suggesting that the
VR+Haptics condition might be associated with greater pleasure emerged,
requiring further validation with a larger sample size. These initial findings
set the foundation for future investigations into leveraging multimodal
interventions in VR, utilising mid-air haptics to potentially enhance
relaxation experiences.","['Naga Sai Surya Vamsy Malladi', 'Viktorija Paneva', 'Jörg Müller']",2023-08-18T09:45:42Z,http://arxiv.org/abs/2308.09424v1,['cs.HC']
"Metaverse: A Vision, Architectural Elements, and Future Directions for
  Scalable and Realtime Virtual Worlds","With the emergence of Cloud computing, Internet of Things-enabled
Human-Computer Interfaces, Generative Artificial Intelligence, and
high-accurate Machine and Deep-learning recognition and predictive models,
along with the Post Covid-19 proliferation of social networking, and remote
communications, the Metaverse gained a lot of popularity. Metaverse has the
prospective to extend the physical world using virtual and augmented reality so
the users can interact seamlessly with the real and virtual worlds using
avatars and holograms. It has the potential to impact people in the way they
interact on social media, collaborate in their work, perform marketing and
business, teach, learn, and even access personalized healthcare. Several works
in the literature examine Metaverse in terms of hardware wearable devices, and
virtual reality gaming applications. However, the requirements of realizing the
Metaverse in realtime and at a large-scale need yet to be examined for the
technology to be usable. To address this limitation, this paper presents the
temporal evolution of Metaverse definitions and captures its evolving
requirements. Consequently, we provide insights into Metaverse requirements. In
addition to enabling technologies, we lay out architectural elements for
scalable, reliable, and efficient Metaverse systems, and a classification of
existing Metaverse applications along with proposing required future research
directions.","['Leila Ismail', 'Rajkumar Buyya']",2023-08-21T08:23:10Z,http://arxiv.org/abs/2308.10559v2,"['cs.HC', 'cs.AI', 'cs.HC, cs.DC']"
I3DOD: Towards Incremental 3D Object Detection via Prompting,"3D object detection has achieved significant performance in many fields,
e.g., robotics system, autonomous driving, and augmented reality. However, most
existing methods could cause catastrophic forgetting of old classes when
performing on the class-incremental scenarios. Meanwhile, the current
class-incremental 3D object detection methods neglect the relationships between
the object localization information and category semantic information and
assume all the knowledge of old model is reliable. To address the above
challenge, we present a novel Incremental 3D Object Detection framework with
the guidance of prompting, i.e., I3DOD. Specifically, we propose a task-shared
prompts mechanism to learn the matching relationships between the object
localization information and category semantic information. After training on
the current task, these prompts will be stored in our prompt pool, and perform
the relationship of old classes in the next task. Moreover, we design a
reliable distillation strategy to transfer knowledge from two aspects: a
reliable dynamic distillation is developed to filter out the negative knowledge
and transfer the reliable 3D knowledge to new detection model; the relation
feature is proposed to capture the responses relation in feature space and
protect plasticity of the model when learning novel 3D classes. To the end, we
conduct comprehensive experiments on two benchmark datasets and our method
outperforms the state-of-the-art object detection methods by 0.6% - 2.7% in
terms of mAP@0.25.","['Wenqi Liang', 'Gan Sun', 'Chenxi Liu', 'Jiahua Dong', 'Kangru Wang']",2023-08-24T02:54:38Z,http://arxiv.org/abs/2308.12512v1,"['cs.CV', 'cs.AI']"
"RL-LABEL: A Deep Reinforcement Learning Approach Intended for AR Label
  Placement in Dynamic Scenarios","Labels are widely used in augmented reality (AR) to display digital
information. Ensuring the readability of AR labels requires placing them
occlusion-free while keeping visual linkings legible, especially when multiple
labels exist in the scene. Although existing optimization-based methods, such
as force-based methods, are effective in managing AR labels in static
scenarios, they often struggle in dynamic scenarios with constantly moving
objects. This is due to their focus on generating layouts optimal for the
current moment, neglecting future moments and leading to sub-optimal or
unstable layouts over time. In this work, we present RL-LABEL, a deep
reinforcement learning-based method for managing the placement of AR labels in
scenarios involving moving objects. RL-LABEL considers the current and
predicted future states of objects and labels, such as positions and
velocities, as well as the user's viewpoint, to make informed decisions about
label placement. It balances the trade-offs between immediate and long-term
objectives. Our experiments on two real-world datasets show that RL-LABEL
effectively learns the decision-making process for long-term optimization,
outperforming two baselines (i.e., no view management and a force-based method)
by minimizing label occlusions, line intersections, and label movement
distance. Additionally, a user study involving 18 participants indicates that
RL-LABEL excels over the baselines in aiding users to identify, compare, and
summarize data on AR labels within dynamic scenes.","['Chen Zhu-Tian', 'Daniele Chiappalupi', 'Tica Lin', 'Yalong Yang', 'Johanna Beyer', 'Hanspeter Pfister']",2023-08-20T14:23:50Z,http://arxiv.org/abs/2308.13540v3,"['cs.HC', 'cs.GR']"
DM-VTON: Distilled Mobile Real-time Virtual Try-On,"The fashion e-commerce industry has witnessed significant growth in recent
years, prompting exploring image-based virtual try-on techniques to incorporate
Augmented Reality (AR) experiences into online shopping platforms. However,
existing research has primarily overlooked a crucial aspect - the runtime of
the underlying machine-learning model. While existing methods prioritize
enhancing output quality, they often disregard the execution time, which
restricts their applications on a limited range of devices. To address this
gap, we propose Distilled Mobile Real-time Virtual Try-On (DM-VTON), a novel
virtual try-on framework designed to achieve simplicity and efficiency. Our
approach is based on a knowledge distillation scheme that leverages a strong
Teacher network as supervision to guide a Student network without relying on
human parsing. Notably, we introduce an efficient Mobile Generative Module
within the Student network, significantly reducing the runtime while ensuring
high-quality output. Additionally, we propose Virtual Try-on-guided Pose for
Data Synthesis to address the limited pose variation observed in training
images. Experimental results show that the proposed method can achieve 40
frames per second on a single Nvidia Tesla T4 GPU and only take up 37 MB of
memory while producing almost the same output quality as other state-of-the-art
methods. DM-VTON stands poised to facilitate the advancement of real-time AR
applications, in addition to the generation of lifelike attired human figures
tailored for diverse specialized training tasks.
https://sites.google.com/view/ltnghia/research/DMVTON","['Khoi-Nguyen Nguyen-Ngoc', 'Thanh-Tung Phan-Nguyen', 'Khanh-Duy Le', 'Tam V. Nguyen', 'Minh-Triet Tran', 'Trung-Nghia Le']",2023-08-26T07:46:27Z,http://arxiv.org/abs/2308.13798v1,['cs.CV']
"The Effect of an Exergame on the Shadow Play Skill Based on Muscle
  Memory for Young Female Participants: The Case of Forehand Drive in Table
  Tennis","Learning and practicing table tennis with traditional methods is a long,
tedious process and may even lead to the internalization of incorrect
techniques if not supervised by a coach. To overcome these issues, the
presented study proposes an exergame with the aim of enhancing young female
novice players' performance by boosting muscle memory, making practice more
interesting, and decreasing the probability of faulty training. Specifically,
we propose an exergame based on skeleton tracking and a virtual avatar to
support correct shadow practice to learn forehand drive technique without the
presence of a coach. We recruited 44 schoolgirls aged between 8 and 12 years
without a background in playing table tennis and divided them into control and
experimental groups. We examined their stroke skills (via the Mott-Lockhart
test) and the error coefficient of their forehand drives (using a ball machine)
in the pretest, post-test, and follow-up tests (10 days after the post-test).
Our results showed that the experimental group had progress in the short and
long term, while the control group had an improvement only in the short term.
Further, the scale of improvement in the experimental group was significantly
higher than in the control group. Given that the early stages of learning,
particularly in girls children, are important in the internalization of
individual skills in would-be athletes, this method could support promoting
correct training for young females.","['Forouzan Farzinnejad', 'Javad Rasti', 'Navid Khezrian', 'Jens Grubert']",2023-08-28T08:39:26Z,http://arxiv.org/abs/2308.14404v1,['cs.HC']
"A Conceptual Framework for Designing Interactive Human-Centred Building
  Spaces to Enhance User Experience in Specific-Purpose Buildings","Human/User interaction with buildings are mostly restricted to interacting
with building automation systems through user-interfaces that mainly aim to
improve energy efficiency of buildings and ensure comfort of occupants. This
research builds on the existing theories of Human-Building Interaction (HBI)
and proposes a novel conceptual framework for HBI that combines the concepts of
Human-Computer Interaction (HCI) and Ambient Intelligence (AmI). The proposed
framework aims to study the needs of occupants in specific-purpose buildings,
which is currently undermined. Specifically, we explore the application of the
proposed HBI framework to improve the learning experience of students in
academic buildings. Focus groups and semi-structured interviews were conducted
among students who are considered primary occupants of Goodwin Hall, a flagship
smart engineering building at Virginia Tech. Qualitative coding and concept
mapping were used to analyze the qualitative data and determine the impact of
occupant-specific needs on the learning experience of students in academic
buildings. The occupant-specific problem that was found to have the highest
direct impact on learning experience was finding study space and highest
indirect impact was Indoor Environment Quality (IEQ). We discuss new ideas for
designing Intelligent User Interfaces (IUI), e.g. Augmented Reality (AR),
increase the perceivable affordances for building occupants and considering a
context-aware ubiquitous analytics-based strategy to provide services that are
tailored to address the identified needs.","['Nazila Roofigari-Esfahan', 'Elham Morshedzadeh', 'Poorvesh Dongre']",2023-08-28T20:00:03Z,http://arxiv.org/abs/2308.14876v1,['cs.HC']
"PoCL-R: An Open Standard Based Offloading Layer for Heterogeneous
  Multi-Access Edge Computing with Server Side Scalability","We propose a novel computing runtime that exposes remote compute devices via
the cross-vendor open heterogeneous computing standard OpenCL and can execute
compute tasks on the MEC cluster side across multiple servers in a scalable
manner. Intermittent UE connection loss is handled gracefully even if the
device's IP address changes on the way. Network-induced latency is minimized by
transferring data and signaling command completions between remote devices in a
peer-to-peer fashion directly to the target server with a streamlined TCP-based
protocol that yields a command latency of only 60 microseconds on top of
network round-trip latency in synthetic benchmarks. The runtime can utilize
RDMA to speed up inter-server data transfers by an additional 60% compared to
the TCP-based solution. The benefits of the proposed runtime in MEC
applications are demonstrated with a smartphone-based augmented reality
rendering case study. Measurements show up to 19x improvements to frame rate
and 17x improvements to local energy consumption when using the proposed
runtime to offload AR rendering from a smartphone. Scalability to multiple GPU
servers in real-world applications is shown in a computational fluid dynamics
simulation, which scales with the number of servers at roughly 80% efficiency
which is comparable to an MPI port of the same simulation.","['Jan Solanti', 'Michal Babej', 'Julius Ikkala', 'Pekka Jääskeläinen']",2023-09-01T11:57:37Z,http://arxiv.org/abs/2309.00407v1,"['cs.DC', 'cs.NI', 'cs.PF', 'C.1.3; C.2.4; C.3; D.1.3; D.2.0; D.2.6; D.4.7; J.7']"
"Immersive Technologies in Virtual Companions: A Systematic Literature
  Review","The emergence of virtual companions is transforming the evolution of
intelligent systems that effortlessly cater to the unique requirements of
users. These advanced systems not only take into account the user present
capabilities, preferences, and needs but also possess the capability to adapt
dynamically to changes in the environment, as well as fluctuations in the users
emotional state or behavior. A virtual companion is an intelligent software or
application that offers support, assistance, and companionship across various
aspects of users lives. Various enabling technologies are involved in building
virtual companion, among these, Augmented Reality (AR), and Virtual Reality
(VR) are emerging as transformative tools. While their potential for use in
virtual companions or digital assistants is promising, their applications in
these domains remain relatively unexplored. To address this gap, a systematic
review was conducted to investigate the applications of VR, AR, and MR
immersive technologies in the development of virtual companions. A
comprehensive search across PubMed, Scopus, and Google Scholar yielded 28
relevant articles out of a pool of 644. The review revealed that immersive
technologies, particularly VR and AR, play a significant role in creating
digital assistants, offering a wide range of applications that brings various
facilities in the individuals life in areas such as addressing social
isolation, enhancing cognitive abilities and dementia care, facilitating
education, and more. Additionally, AR and MR hold potential for enhancing
Quality of life (QoL) within the context of virtual companion technology. The
findings of this review provide a valuable foundation for further research in
this evolving field.","['Ziaullah Momand', 'Jonathan H. Chan', 'Pornchai Mongkolnam']",2023-09-03T16:39:22Z,http://arxiv.org/abs/2309.01214v1,['cs.HC']
"Orchestration in the Cloud-to-Things Compute Continuum: Taxonomy, Survey
  and Future Directions","IoT systems are becoming an essential part of our environment. Smart cities,
smart manufacturing, augmented reality, and self-driving cars are just some
examples of the wide range of domains, where the applicability of such systems
has been increasing rapidly. These IoT use cases often require simultaneous
access to geographically distributed arrays of sensors, and heterogeneous
remote, local as well as multi-cloud computational resources. This gives birth
to the extended Cloud-to-Things computing paradigm. The emergence of this new
paradigm raised the quintessential need to extend the orchestration
requirements i.e., the automated deployment and run-time management) of
applications from the centralised cloud-only environment to the entire spectrum
of resources in the Cloud-to-Things continuum. In order to cope with this
requirement, in the last few years, there has been a lot of attention to the
development of orchestration systems in both industry and academic
environments. This paper is an attempt to gather the research conducted in the
orchestration for the Cloud-to-Things continuum landscape and to propose a
detailed taxonomy, which is then used to critically review the landscape of
existing research work. We finally discuss the key challenges that require
further attention and also present a conceptual framework based on the
conducted analysis.","['Amjad Ullah', 'Tamas Kiss', 'József Kovács', 'Francesco Tusa', 'James Deslauriers', 'Huseyin Dagdeviren', 'Resmi Arjun', 'Hamed Hamzeh']",2023-09-05T12:21:41Z,http://arxiv.org/abs/2309.02172v1,['cs.DC']
"Context-Aware 3D Object Localization from Single Calibrated Images: A
  Study of Basketballs","Accurately localizing objects in three dimensions (3D) is crucial for various
computer vision applications, such as robotics, autonomous driving, and
augmented reality. This task finds another important application in sports
analytics and, in this work, we present a novel method for 3D basketball
localization from a single calibrated image. Our approach predicts the object's
height in pixels in image space by estimating its projection onto the ground
plane within the image, leveraging the image itself and the object's location
as inputs. The 3D coordinates of the ball are then reconstructed by exploiting
the known projection matrix. Extensive experiments on the public DeepSport
dataset, which provides ground truth annotations for 3D ball location alongside
camera calibration information for each image, demonstrate the effectiveness of
our method, offering substantial accuracy improvements compared to recent work.
Our work opens up new possibilities for enhanced ball tracking and
understanding, advancing computer vision in diverse domains. The source code of
this work is made publicly available at
\url{https://github.com/gabriel-vanzandycke/deepsport}.","['Marcello Davide Caio', 'Gabriel Van Zandycke', 'Christophe De Vleeschouwer']",2023-09-07T11:14:02Z,http://arxiv.org/abs/2309.03640v1,"['cs.CV', 'eess.IV', 'I.4.6; I.4.8']"
"Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended
  Reality","Real-time Stereo Matching is a cornerstone algorithm for many Extended
Reality (XR) applications, such as indoor 3D understanding, video pass-through,
and mixed-reality games. Despite significant advancements in deep stereo
methods, achieving real-time depth inference with high accuracy on a low-power
device remains a major challenge. One of the major difficulties is the lack of
high-quality indoor video stereo training datasets captured by head-mounted
VR/AR glasses. To address this issue, we introduce a novel video stereo
synthetic dataset that comprises photorealistic renderings of various indoor
scenes and realistic camera motion captured by a 6-DoF moving VR/AR
head-mounted display (HMD). This facilitates the evaluation of existing
approaches and promotes further research on indoor augmented reality scenarios.
Our newly proposed dataset enables us to develop a novel framework for
continuous video-rate stereo matching.
  As another contribution, our dataset enables us to proposed a new video-based
stereo matching approach tailored for XR applications, which achieves real-time
inference at an impressive 134fps on a standard desktop computer, or 30fps on a
battery-powered HMD. Our key insight is that disparity and contextual
information are highly correlated and redundant between consecutive stereo
frames. By unrolling an iterative cost aggregation in time (i.e. in the
temporal dimension), we are able to distribute and reuse the aggregated
features over time. This approach leads to a substantial reduction in
computation without sacrificing accuracy. We conducted extensive evaluations
and comparisons and demonstrated that our method achieves superior performance
compared to the current state-of-the-art, making it a strong contender for
real-time stereo matching in VR/AR applications.","['Ziang Cheng', 'Jiayu Yang', 'Hongdong Li']",2023-09-08T07:53:58Z,http://arxiv.org/abs/2309.04183v1,['cs.CV']
"Connecting Everyday Objects with the Metaverse: A Unified Recognition
  Framework","The recent Facebook rebranding to Meta has drawn renewed attention to the
metaverse. Technology giants, amongst others, are increasingly embracing the
vision and opportunities of a hybrid social experience that mixes physical and
virtual interactions. As the metaverse gains in traction, it is expected that
everyday objects may soon connect more closely with virtual elements. However,
discovering this ""hidden"" virtual world will be a crucial first step to
interacting with it in this new augmented world. In this paper, we address the
problem of connecting physical objects with their virtual counterparts,
especially through connections built upon visual markers. We propose a unified
recognition framework that guides approaches to the metaverse access points. We
illustrate the use of the framework through experimental studies under
different conditions, in which an interactive and visually attractive
decoration pattern, an Artcode, is used as the approach to enable the
connection. This paper will be of interest to, amongst others, researchers
working in Interaction Design or Augmented Reality who are seeking techniques
or guidelines for augmenting physical objects in an unobtrusive, complementary
manner.","['Liming Xu', 'Dave Towey', 'Andrew P. French', 'Steve Benford']",2023-09-11T21:20:06Z,http://arxiv.org/abs/2309.06444v1,['cs.HC']
"Hand Gesture Recognition with Two Stage Approach Using Transfer Learning
  and Deep Ensemble Learning","Human-Computer Interaction (HCI) has been the subject of research for many
years, and recent studies have focused on improving its performance through
various techniques. In the past decade, deep learning studies have shown high
performance in various research areas, leading researchers to explore their
application to HCI. Convolutional neural networks can be used to recognize hand
gestures from images using deep architectures. In this study, we evaluated
pre-trained high-performance deep architectures on the HG14 dataset, which
consists of 14 different hand gesture classes. Among 22 different models,
versions of the VGGNet and MobileNet models attained the highest accuracy
rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of
94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models
achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand
gesture recognition on the dataset using an ensemble learning technique, which
combined the four most successful models. By utilizing these models as base
learners and applying the Dirichlet ensemble technique, we achieved an accuracy
rate of 98.88%. These results demonstrate the effectiveness of the deep
ensemble learning technique for HCI and its potential applications in areas
such as augmented reality, virtual reality, and game technologies.","['Serkan Savaş', 'Atilla Ergüzen']",2023-09-20T19:53:05Z,http://arxiv.org/abs/2309.11610v1,"['cs.CV', 'cs.AI', 'cs.HC']"
"Realizing XR Applications Using 5G-Based 3D Holographic Communication
  and Mobile Edge Computing","3D holographic communication has the potential to revolutionize the way
people interact with each other in virtual spaces, offering immersive and
realistic experiences. However, demands for high data rates, extremely low
latency, and high computations to enable this technology pose a significant
challenge. To address this challenge, we propose a novel job scheduling
algorithm that leverages Mobile Edge Computing (MEC) servers in order to
minimize the total latency in 3D holographic communication. One of the
motivations for this work is to prevent the uncanny valley effect, which can
occur when the latency hinders the seamless and real-time rendering of
holographic content, leading to a less convincing and less engaging user
experience. Our proposed algorithm dynamically allocates computation tasks to
MEC servers, considering the network conditions, computational capabilities of
the servers, and the requirements of the 3D holographic communication
application. We conduct extensive experiments to evaluate the performance of
our algorithm in terms of latency reduction, and the results demonstrate that
our approach significantly outperforms other baseline methods. Furthermore, we
present a practical scenario involving Augmented Reality (AR), which not only
illustrates the applicability of our algorithm but also highlights the
importance of minimizing latency in achieving high-quality holographic views.
By efficiently distributing the computation workload among MEC servers and
reducing the overall latency, our proposed algorithm enhances the user
experience in 3D holographic communications and paves the way for the
widespread adoption of this technology in various applications, such as
telemedicine, remote collaboration, and entertainment.","['Dun Yuan', 'Ekram Hossain', 'Di Wu', 'Xue Liu', 'Gregory Dudek']",2023-10-05T21:28:40Z,http://arxiv.org/abs/2310.03908v1,"['cs.NI', 'eess.SP']"
Cross-modal Cognitive Consensus guided Audio-Visual Segmentation,"Audio-Visual Segmentation (AVS) aims to extract the sounding object from a
video frame, which is represented by a pixel-wise segmentation mask for
application scenarios such as multi-modal video editing, augmented reality, and
intelligent robot systems. The pioneering work conducts this task through dense
feature-level audio-visual interaction, which ignores the dimension gap between
different modalities. More specifically, the audio clip could only provide a
Global semantic label in each sequence, but the video frame covers multiple
semantic objects across different Local regions, which leads to mislocalization
of the representationally similar but semantically different object. In this
paper, we propose a Cross-modal Cognitive Consensus guided Network (C3N) to
align the audio-visual semantics from the global dimension and progressively
inject them into the local regions via an attention mechanism. Firstly, a
Cross-modal Cognitive Consensus Inference Module (C3IM) is developed to extract
a unified-modal label by integrating audio/visual classification confidence and
similarities of modality-agnostic label embeddings. Then, we feed the
unified-modal label back to the visual backbone as the explicit semantic-level
guidance via a Cognitive Consensus guided Attention Module (CCAM), which
highlights the local features corresponding to the interested object. Extensive
experiments on the Single Sound Source Segmentation (S4) setting and Multiple
Sound Source Segmentation (MS3) setting of the AVSBench dataset demonstrate the
effectiveness of the proposed method, which achieves state-of-the-art
performance. Code will be available at https://github.com/ZhaofengSHI/AVS-C3N
once accepted.","['Zhaofeng Shi', 'Qingbo Wu', 'Fanman Meng', 'Linfeng Xu', 'Hongliang Li']",2023-10-10T02:04:38Z,http://arxiv.org/abs/2310.06259v4,"['eess.IV', 'cs.SD', 'eess.AS', '68U10', 'I.4.6']"
"Towards immersive generosity: The need for a novel framework to explore
  large audiovisual archives through embodied experiences in immersive
  environments","This article proposes an innovative framework to explore large audiovisual
archives using Immersive Environments to place users inside a dataset and
create an embodied experience. It starts by outlining the need for such a novel
interface to meet the needs of archival scholars and the GLAM sector, and
discusses issues in the current modes of access, mostly restrained to
traditional information retrieval systems based on metadata. The paper presents
the concept of ``generous interfaces"" as a preliminary approach to address
these issues, and argues some of the key reasons why employing Immersive Visual
Storytelling might benefit such frameworks. The theory of embodiment is
leveraged to justify this claim, showing how a more embodied understanding of a
collection can result in a stronger engagement for the public. By placing users
as actors in the experience rather than mere spectators, the emergence of
narrative is driven by their interactions, with benefits in terms of engagement
with the public and understanding of the cultural component. The framework we
propose is applied to two existing installations to analyze them in-depth and
critique them, highlighting the key directions to pursue for further
development.","['Giacomo Alliata', 'Sarah Kenderdine', 'Lily Hibberd', 'Ingrid Mason']",2023-10-10T06:35:19Z,http://arxiv.org/abs/2310.06349v1,['cs.DL']
Sparse Multi-Object Render-and-Compare,"Reconstructing 3D shape and pose of static objects from a single image is an
essential task for various industries, including robotics, augmented reality,
and digital content creation. This can be done by directly predicting 3D shape
in various representations or by retrieving CAD models from a database and
predicting their alignments. Directly predicting 3D shapes often produces
unrealistic, overly smoothed or tessellated shapes. Retrieving CAD models
ensures realistic shapes but requires robust and accurate alignment. Learning
to directly predict CAD model poses from image features is challenging and
inaccurate. Works, such as ROCA, compute poses from predicted normalised object
coordinates which can be more accurate but are susceptible to systematic
failure. SPARC demonstrates that following a ''render-and-compare'' approach
where a network iteratively improves upon its own predictions achieves accurate
alignments. Nevertheless, it performs individual CAD alignment for every object
detected in an image. This approach is slow when applied to many objects as the
time complexity increases linearly with the number of objects and can not learn
inter-object relations. Introducing a new network architecture Multi-SPARC we
learn to perform CAD model alignments for multiple detected objects jointly.
Compared to other single-view methods we achieve state-of-the-art performance
on the challenging real-world dataset ScanNet. By improving the instance
alignment accuracy from 31.8% to 40.3% we perform similar to state-of-the-art
multi-view methods.","['Florian Langer', 'Ignas Budvytis', 'Roberto Cipolla']",2023-10-17T12:01:32Z,http://arxiv.org/abs/2310.11184v1,['cs.CV']
How Can Everyday Users Efficiently Teach Robots by Demonstrations?,"Learning from Demonstration (LfD) is a framework that allows lay users to
easily program robots. However, the efficiency of robot learning and the
robot's ability to generalize to task variations hinges upon the quality and
quantity of the provided demonstrations. Our objective is to guide human
teachers to furnish more effective demonstrations, thus facilitating efficient
robot learning. To achieve this, we propose to use a measure of uncertainty,
namely task-related information entropy, as a criterion for suggesting
informative demonstration examples to human teachers to improve their teaching
skills. In a conducted experiment (N=24), an augmented reality (AR)-based
guidance system was employed to train novice users to produce additional
demonstrations from areas with the highest entropy within the workspace. These
novice users were trained for a few trials to teach the robot a generalizable
task using a limited number of demonstrations. Subsequently, the users'
performance after training was assessed first on the same task (retention) and
then on a novel task (transfer) without guidance. The results indicated a
substantial improvement in robot learning efficiency from the teacher's
demonstrations, with an improvement of up to 198% observed on the novel task.
Furthermore, the proposed approach was compared to a state-of-the-art heuristic
rule and found to improve robot learning efficiency by 210% compared to the
heuristic rule.","['Maram Sakr', 'Zhikai Zhang', 'Benjamin Li', 'Haomiao Zhang', 'H. F. Machiel Van der Loos', 'Dana Kulic', 'Elizabeth Croft']",2023-10-19T18:21:39Z,http://arxiv.org/abs/2310.13083v1,"['cs.RO', 'cs.LG']"
"RD-VIO: Robust Visual-Inertial Odometry for Mobile Augmented Reality in
  Dynamic Environments","It is typically challenging for visual or visual-inertial odometry systems to
handle the problems of dynamic scenes and pure rotation. In this work, we
design a novel visual-inertial odometry (VIO) system called RD-VIO to handle
both of these two problems. Firstly, we propose an IMU-PARSAC algorithm which
can robustly detect and match keypoints in a two-stage process. In the first
state, landmarks are matched with new keypoints using visual and IMU
measurements. We collect statistical information from the matching and then
guide the intra-keypoint matching in the second stage. Secondly, to handle the
problem of pure rotation, we detect the motion type and adapt the
deferred-triangulation technique during the data-association process. We make
the pure-rotational frames into the special subframes. When solving the
visual-inertial bundle adjustment, they provide additional constraints to the
pure-rotational motion. We evaluate the proposed VIO system on public datasets
and online comparison. Experiments show the proposed RD-VIO has obvious
advantages over other methods in dynamic environments. The source code is
available at:
\href{https://github.com/openxrlab/xrslam}{{\fontfamily{pcr}\selectfont
https://github.com/openxrlab/xrslam}}.","['Jinyu Li', 'Xiaokun Pan', 'Gan Huang', 'Ziyang Zhang', 'Nan Wang', 'Hujun Bao', 'Guofeng Zhang']",2023-10-23T16:30:39Z,http://arxiv.org/abs/2310.15072v3,"['cs.RO', 'cs.CV']"
"Enhancing the Spatial Awareness Capability of Multi-Modal Large Language
  Model","The Multi-Modal Large Language Model (MLLM) refers to an extension of the
Large Language Model (LLM) equipped with the capability to receive and infer
multi-modal data. Spatial awareness stands as one of the crucial abilities of
MLLM, encompassing diverse skills related to understanding spatial
relationships among objects and between objects and the scene area. Industries
such as autonomous driving, smart healthcare, robotics, virtual, and augmented
reality heavily demand MLLM's spatial awareness capabilities. However, there
exists a noticeable gap between the current spatial awareness capabilities of
MLLM and the requirements set by human needs. To address this issue, this paper
proposes using more precise spatial position information between objects to
guide MLLM in providing more accurate responses to user-related inquiries.
Specifically, for a particular multi-modal task, we utilize algorithms for
acquiring geometric spatial information and scene graphs to obtain relevant
geometric spatial information and scene details of objects involved in the
query. Subsequently, based on this information, we direct MLLM to address
spatial awareness-related queries posed by the user. Extensive experiments were
conducted in benchmarks such as MME, MM-Vet, and other multi-modal large
language models. The experimental results thoroughly confirm the efficacy of
the proposed method in enhancing the spatial awareness tasks and associated
tasks of MLLM.","['Yongqiang Zhao', 'Zhenyu Li', 'Zhi Jin', 'Feng Zhang', 'Haiyan Zhao', 'Chengfeng Dou', 'Zhengwei Tao', 'Xinhai Xu', 'Donghong Liu']",2023-10-31T10:57:35Z,http://arxiv.org/abs/2310.20357v2,"['cs.AI', 'cs.MM']"
"Digital Twin-based 3D Map Management for Edge-assisted Device Pose
  Tracking in Mobile AR","Edge-device collaboration has the potential to facilitate compute-intensive
device pose tracking for resource-constrained mobile augmented reality (MAR)
devices. In this paper, we devise a 3D map management scheme for edge-assisted
MAR, wherein an edge server constructs and updates a 3D map of the physical
environment by using the camera frames uploaded from an MAR device, to support
local device pose tracking. Our objective is to minimize the uncertainty of
device pose tracking by periodically selecting a proper set of uploaded camera
frames and updating the 3D map. To cope with the dynamics of the uplink data
rate and the user's pose, we formulate a Bayes-adaptive Markov decision process
problem and propose a digital twin (DT)-based approach to solve the problem.
First, a DT is designed as a data model to capture the time-varying uplink data
rate, thereby supporting 3D map management. Second, utilizing extensive
generated data provided by the DT, a model-based reinforcement learning
algorithm is developed to manage the 3D map while adapting to these dynamics.
Numerical results demonstrate that the designed DT outperforms Markov models in
accurately capturing the time-varying uplink data rate, and our devised
DT-based 3D map management scheme surpasses benchmark schemes in reducing
device pose tracking uncertainty.","['Conghao Zhou', 'Jie Gao', 'Mushu Li', 'Nan Cheng', 'Xuemin Shen', 'Weihua Zhuang']",2023-11-08T19:57:45Z,http://arxiv.org/abs/2311.04997v2,['cs.NI']
"Topology of Surface Electromyogram Signals: Hand Gesture Decoding on
  Riemannian Manifolds","Decoding gestures from the upper limb using noninvasive surface
electromyogram (sEMG) signals is of keen interest for the rehabilitation of
amputees, artificial supernumerary limb augmentation, gestural control of
computers, and virtual/augmented realities. We show that sEMG signals recorded
across an array of sensor electrodes in multiple spatial locations around the
forearm evince a rich geometric pattern of global motor unit (MU) activity that
can be leveraged to distinguish different hand gestures. We demonstrate a
simple technique to analyze spatial patterns of muscle MU activity within a
temporal window and show that distinct gestures can be classified in both
supervised and unsupervised manners. Specifically, we construct symmetric
positive definite (SPD) covariance matrices to represent the spatial
distribution of MU activity in a time window of interest, calculated as
pairwise covariance of electrical signals measured across different electrodes.
This allows us to understand and manipulate multivariate sEMG timeseries on a
more natural subspace -the Riemannian manifold. Furthermore, it directly
addresses signal variability across individuals and sessions, which remains a
major challenge in the field. sEMG signals measured at a single electrode lack
contextual information such as how various anatomical and physiological factors
influence the signals and how their combined effect alters the evident
interaction among neighboring muscles. As we show here, analyzing spatial
patterns using covariance matrices on Riemannian manifolds allows us to
robustly model complex interactions across spatially distributed MUs and
provides a flexible and transparent framework to quantify differences in sEMG
signals across individuals. The proposed method is novel in the study of sEMG
signals and its performance exceeds the current benchmarks while maintaining
exceptional computational efficiency.","['Harshavardhana T. Gowda', 'Lee M. Miller']",2023-11-14T21:20:54Z,http://arxiv.org/abs/2311.08548v1,"['eess.SP', 'cs.CV', 'cs.HC', 'q-bio.QM', '53', 'G.3']"
"Mapping Eye Vergence Angle to the Depth of Real and Virtual Objects as
  an Objective Measure of Depth Perception","Recently, extended reality (XR) displays including augmented reality (AR) and
virtual reality (VR) have integrated eye tracking capabilities, which could
enable novel ways of interacting with XR content. The vergence angle of the
eyes constantly changes according to the distance of fixated objects. Here we
measured vergence angle for eye fixations on real and simulated target objects
in three different environments: real objects in the real-world (real), virtual
objects in the real-world (AR), and virtual objects in the virtual world (VR)
using gaze data from an eye-tracking device. In a repeated-measures design with
13 participants, Gaze-measured Vergence Angle (GVA) was measured while
participants fixated on targets at varying distances. As expected, results
showed a significant main effect of target depth such that increasing GVA was
associated with closer targets. However, there were consistent individual
differences in baseline GVA. When these individual differences were controlled
for, there was a small but statistically-significant main effect of environment
(real, AR, VR). Importantly, GVA was stable with respect to the starting depth
of previously fixated targets and invariant to directionality (convergence vs.
divergence). In addition, GVA proved to be a more veridical depth estimate than
subjective depth judgements.","['Mohammed Safayet Arefin', 'J. Edward Swan II', 'Russell Cohen Hoffing', 'Steven Thurman']",2023-11-12T06:50:32Z,http://arxiv.org/abs/2311.09242v2,['cs.HC']
Reconfigurable Image Processing Metasurfaces with Phase-Change Materials,"Optical metasurfaces have been enabling reduced footprint and power
consumption, as well as faster speeds, in the context of analog computing and
image processing. While various image processing and optical computing
functionalities have been recently demonstrated using metasurfaces, most of the
considered devices are static and lack reconfigurability. Yet, the ability to
dynamically reconfigure processing operations is key for metasurfaces to be
able to compete with practical computing systems. Here, we demonstrate a
passive edge-detection metasurface operating in the near-infrared regime whose
image processing response can be drastically modified by temperature variations
smaller than 10{\deg} C around a CMOS-compatible temperature of 65{\deg} C.
Such reconfigurability is achieved by leveraging the insulator-to-metal phase
transition of a thin buried layer of vanadium dioxide which, in turn, strongly
alters the nonlocal response of the metasurface. Importantly, this
reconfigurability is accompanied by performance metrics - such as high
numerical aperture, high efficiency, isotropy, and polarization-independence -
close to optimal, and it is combined with a simple geometry compatible with
large-scale manufacturing. Our work paves the way to a new generation of
ultra-compact, tunable, passive devices for all-optical computation, with
potential applications in augmented reality, remote sensing and bio-medical
imaging.","['Michele Cotrufo', 'Shaban B. Sulejman', 'Lukas Wesemann', 'Md. Ataur Rahman', 'Madhu Bhaskaran', 'Ann Roberts', 'Andrea Alù']",2023-11-22T02:22:57Z,http://arxiv.org/abs/2311.13109v1,"['physics.optics', 'physics.app-ph']"
"Neural-Optic Co-Designed Polarization-Multiplexed Metalens for Compact
  Computational Spectral Imaging","As the realm of spectral imaging applications extends its reach into the
domains of mobile technology and augmented reality, the demands for compact yet
high-fidelity systems become increasingly pronounced. Conventional
methodologies, exemplified by coded aperture snapshot spectral imaging systems,
are significantly limited by their cumbersome physical dimensions and form
factors. To address this inherent challenge, diffractive optical elements
(DOEs) have been repeatedly employed as a means to mitigate issues related to
the bulky nature of these systems. Nonetheless, it's essential to note that the
capabilities of DOEs primarily revolve around the modulation of the phase of
light. Here, we introduce an end-to-end computational spectral imaging
framework based on a polarization-multiplexed metalens. A distinguishing
feature of this approach lies in its capacity to simultaneously modulate
orthogonal polarization channels. When harnessed in conjunction with a neural
network, it facilitates the attainment of high-fidelity spectral
reconstruction. Importantly, the framework is intrinsically fully
differentiable, a feature that permits the joint optimization of both the
metalens structure and the parameters governing the neural network. The
experimental results presented herein validate the exceptional spatial-spectral
reconstruction performance, underscoring the efficacy of this system in
practical, real-world scenarios. This innovative approach transcends the
traditional boundaries separating hardware and software in the realm of
computational imaging and holds the promise of substantially propelling the
miniaturization of spectral imaging systems.","['Qiangbo Zhang', 'Peicheng Lin', 'Chang Wang', 'Yang Zhang', 'Zeqing Yu', 'Xinyu Liu', 'Ting Xu', 'Zhenrong Zheng']",2023-11-26T02:06:30Z,http://arxiv.org/abs/2311.15164v1,"['physics.optics', 'eess.IV']"
"Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information","Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.","['Jie Li', 'Zhixin Li', 'Zhi Liu', 'Pengyuan Zhou', 'Richang Hong', 'Qiyue Li', 'Han Hu']",2023-11-28T03:45:29Z,http://arxiv.org/abs/2311.16462v1,"['cs.CV', 'cs.MM']"
"MD-Splatting: Learning Metric Deformation from 4D Gaussians in Highly
  Deformable Scenes","Accurate 3D tracking in highly deformable scenes with occlusions and shadows
can facilitate new applications in robotics, augmented reality, and generative
AI. However, tracking under these conditions is extremely challenging due to
the ambiguity that arises with large deformations, shadows, and occlusions. We
introduce MD-Splatting, an approach for simultaneous 3D tracking and novel view
synthesis, using video captures of a dynamic scene from various camera poses.
MD-Splatting builds on recent advances in Gaussian splatting, a method that
learns the properties of a large number of Gaussians for state-of-the-art and
fast novel view synthesis. MD-Splatting learns a deformation function to
project a set of Gaussians with non-metric, thus canonical, properties into
metric space. The deformation function uses a neural-voxel encoding and a
multilayer perceptron (MLP) to infer Gaussian position, rotation, and a shadow
scalar. We enforce physics-inspired regularization terms based on local
rigidity, conservation of momentum, and isometry, which leads to trajectories
with smaller trajectory errors. MD-Splatting achieves high-quality 3D tracking
on highly deformable scenes with shadows and occlusions. Compared to
state-of-the-art, we improve 3D tracking by an average of 23.9 %, while
simultaneously achieving high-quality novel view synthesis. With sufficient
texture such as in scene 6, MD-Splatting achieves a median tracking error of
3.39 mm on a cloth of 1 x 1 meters in size. Project website:
https://md-splatting.github.io/.","['Bardienus P. Duisterhof', 'Zhao Mandi', 'Yunchao Yao', 'Jia-Wei Liu', 'Mike Zheng Shou', 'Shuran Song', 'Jeffrey Ichnowski']",2023-11-30T18:53:03Z,http://arxiv.org/abs/2312.00583v1,"['cs.CV', 'cs.RO']"
"Painterly Reality: Enhancing Audience Experience with Paintings through
  Interactive Art","Perceiving paintings entails more than merely engaging the audience's eyes
and brains; their perceptions and experiences of a painting can be intricately
connected with body movement. This paper proposes an interactive art approach
entitled ""Painterly Reality"" that facilitates the perception and interaction
with paintings in a three-dimensional manner. Its objective is to promote
bodily engagement with the painting (i.e., embedded body embodiment and its
movement and interaction) to enhance the audience's experience, while
maintaining its essence. Unlike two-dimensional interactions, this approach
constructs the Painterly Reality by capturing the audience's body embodiment in
real-time and embedding into a three-dimensional painterly world derived from a
given painting input. Through their body embodiment, the audience can navigate
the painterly world and play with the magical realism (i.e., interactive
painterly objects), fostering meaningful experiences via interactions. The
Painterly Reality is subsequently projected through an Augmented Reality Mirror
as a live painting and displayed in front of the audience. Hence, the audience
can gain enhanced experiences through bodily engagement while simultaneously
viewing and appreciating the live painting. The paper implements the proposed
approach as an interactive artwork, entitled ""Everyday Conjunctive,"" with Fong
Tse Ka's painting and installs in a local museum, which successfully enhances
audience experience through bodily engagement.","['Aven Le Zhou', 'Kang Zhang', 'David Yip']",2023-12-02T08:33:34Z,http://arxiv.org/abs/2312.01067v1,"['cs.HC', 'cs.MM']"
"Optimization for the Metaverse over Mobile Edge Computing with Play to
  Earn","The concept of the Metaverse has garnered growing interest from both academic
and industry circles. The decentralization of both the integrity and security
of digital items has spurred the popularity of play-to-earn (P2E) games, where
players are entitled to earn and own digital assets which they may trade for
physical-world currencies. However, these computationally-intensive games are
hardly playable on resource-limited mobile devices and the computational tasks
have to be offloaded to an edge server. Through mobile edge computing (MEC),
users can upload data to the Metaverse Service Provider (MSP) edge servers for
computing. Nevertheless, there is a trade-off between user-perceived in-game
latency and user visual experience. The downlink transmission of
lower-resolution videos lowers user-perceived latency while lowering the visual
fidelity and consequently, earnings of users. In this paper, we design a method
to enhance the Metaverse-based mobile augmented reality (MAR) in-game user
experience. Specifically, we formulate and solve a multi-objective optimization
problem. Given the inherent NP-hardness of the problem, we present a
low-complexity algorithm to address it, mitigating the trade-off between delay
and earnings. The experiment results show that our method can effectively
balance the user-perceived latency and profitability, thus improving the
performance of Metaverse-based MAR systems.","['Chang Liu', 'Terence Jie Chua', 'Jun Zhao']",2023-12-10T12:46:51Z,http://arxiv.org/abs/2312.05871v1,"['cs.DC', 'cs.SY', 'eess.SY', 'math.OC']"
"Lagrangian Properties and Control of Soft Robots Modeled with Discrete
  Cosserat Rods","The characteristic ``in-plane"" bending associated with soft robots'
deformation make them preferred over rigid robots in sophisticated manipulation
and movement tasks. Executing such motion strategies to precision in soft
deformable robots and structures is however fraught with modeling and control
challenges given their infinite degrees-of-freedom. Imposing \textit{piecewise
constant strains} (PCS) across (discretized) Cosserat microsolids on the
continuum material however, their dynamics become amenable to tractable
mathematical analysis. While this PCS model handles the characteristic
difficult-to-model ``in-plane"" bending well, its Lagrangian properties are not
exploited for control in literature neither is there a rigorous study on the
dynamic performance of multisection deformable materials for ``in-plane""
bending that guarantees steady-state convergence. In this sentiment, we first
establish the PCS model's structural Lagrangian properties. Second, we exploit
these for control on various strain goal states. Third, we benchmark our
hypotheses against an Octopus-inspired robot arm under different constant tip
loads. These induce non-constant ``in-plane"" deformation and we regulate strain
states throughout the continuum in these configurations. Our numerical results
establish convergence to desired equilibrium throughout the continuum in all of
our tests. Within the bounds here set, we conjecture that our methods can find
wide adoption in the control of cable- and fluid-driven multisection soft
robotic arms; and may be extensible to the (learning-based) control of
deformable agents employed in simulated, mixed, or augmented reality.","['Lekan Molu', 'Shaoru Chen', 'Audrey Sedal']",2023-12-10T17:02:46Z,http://arxiv.org/abs/2312.05937v1,['cs.RO']
"Mixed Reality Communication for Medical Procedures: Teaching the
  Placement of a Central Venous Catheter","Medical procedures are an essential part of healthcare delivery, and the
acquisition of procedural skills is a critical component of medical education.
Unfortunately, procedural skill is not evenly distributed among medical
providers. Skills may vary within departments or institutions, and across
geographic regions, depending on the provider's training and ongoing
experience. We present a mixed reality real-time communication system to
increase access to procedural skill training and to improve remote emergency
assistance. Our system allows a remote expert to guide a local operator through
a medical procedure. RGBD cameras capture a volumetric view of the local scene
including the patient, the operator, and the medical equipment. The volumetric
capture is augmented onto the remote expert's view to allow the expert to
spatially guide the local operator using visual and verbal instructions. We
evaluated our mixed reality communication system in a study in which experts
teach the ultrasound-guided placement of a central venous catheter (CVC) to
students in a simulation setting. The study compares state-of-the-art video
communication against our system. The results indicate that our system enhances
and offers new possibilities for visual communication compared to video
teleconference-based training.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka', 'David Li', 'Christian Gütl']",2023-12-14T03:11:20Z,http://arxiv.org/abs/2312.08624v1,"['cs.CV', 'cs.HC']"
"Design Space Exploration of Low-Bit Quantized Neural Networks for Visual
  Place Recognition","Visual Place Recognition (VPR) is a critical task for performing global
re-localization in visual perception systems. It requires the ability to
accurately recognize a previously visited location under variations such as
illumination, occlusion, appearance and viewpoint. In the case of robotic
systems and augmented reality, the target devices for deployment are battery
powered edge devices. Therefore whilst the accuracy of VPR methods is important
so too is memory consumption and latency. Recently new works have focused on
the recall@1 metric as a performance measure with limited focus on resource
utilization. This has resulted in methods that use deep learning models too
large to deploy on low powered edge devices. We hypothesize that these large
models are highly over-parameterized and can be optimized to satisfy the
constraints of a low powered embedded system whilst maintaining high recall
performance. Our work studies the impact of compact convolutional network
architecture design in combination with full-precision and mixed-precision
post-training quantization on VPR performance. Importantly we not only measure
performance via the recall@1 score but also measure memory consumption and
latency. We characterize the design implications on memory, latency and recall
scores and provide a number of design recommendations for VPR systems under
these resource limitations.","['Oliver Grainge', 'Michael Milford', 'Indu Bodala', 'Sarvapali D. Ramchurn', 'Shoaib Ehsan']",2023-12-14T15:24:42Z,http://arxiv.org/abs/2312.09028v1,['cs.CV']
"Exploring the current applications and potential of extended reality for
  environmental sustainability in manufacturing","In response to the transformation towards Industry 5.0, there is a growing
call for manufacturing systems that prioritize environmental sustainability,
alongside the emerging application of digital tools. Extended Reality (XR) -
including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) -
is one of the technologies identified as an enabler for Industry 5.0. XR could
potentially also be a driver for more sustainable manufacturing: however, its
potential environmental benefits have received limited attention. This paper
aims to explore the current manufacturing applications and research within the
field of XR technology connected to the environmental sustainability principle.
The objectives of this paper are two-fold: (1) Identify the currently explored
use cases of XR technology in literature and research, addressing environmental
sustainability in manufacturing; (2) Provide guidance and references for
industry and companies to use cases, toolboxes, methodologies, and workflows
for implementing XR in environmental sustainable manufacturing practices. Based
on the categorization of sustainability indicators, developed by the National
Institute of Standards and Technology (NIST), the authors analyzed and mapped
the current literature, with criteria of pragmatic XR use cases for
manufacturing. The exploration resulted in a mapping of the current
applications and use cases of XR technology within manufacturing that has the
potential to drive environmental sustainability. The results are presented as
stated use-cases with reference to the literature, contributing as guidance and
inspiration for future researchers or implementations in industry, using XR as
a driver for environmental sustainability. Furthermore, the authors open up the
discussion for future work and research to increase the attention of XR as a
driver for environmental sustainability.","['Huizhong Cao', 'Henrik Söderlund', 'Mélanie Derspeisse', 'Björn Johansson']",2023-12-29T13:18:01Z,http://arxiv.org/abs/2312.17595v1,"['cs.CY', 'cs.HC']"
"FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D
  Scene Understanding","Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present Foundation Model Embedded Gaussian
Splatting (FMGS), which incorporates vision-language embeddings of foundation
models into 3D Gaussian Splatting (GS). The key contribution of this work is an
efficient method to reconstruct and represent 3D vision-language models. This
is achieved by distilling feature maps generated from image-based foundation
models into those rendered from our 3D model. To ensure high-quality rendering
and fast training, we introduce a novel scene representation by integrating
strengths from both GS and multi-resolution hash encodings (MHE). Our effective
training procedure also introduces a pixel alignment loss that makes the
rendered feature distance of the same semantic entities close, following the
pixel-level semantic boundaries. Our results demonstrate remarkable multi-view
semantic consistency, facilitating diverse downstream tasks, beating
state-of-the-art methods by 10.2 percent on open-vocabulary language-based
object detection, despite that we are 851X faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code on the project page.","['Xingxing Zuo', 'Pouya Samangouei', 'Yunwen Zhou', 'Yan Di', 'Mingyang Li']",2024-01-03T20:39:02Z,http://arxiv.org/abs/2401.01970v2,"['cs.CV', 'cs.AI']"
"Real-and-Present: Investigating the Use of Life-Size 2D Video Avatars in
  HMD-Based AR Teleconferencing","Augmented Reality (AR) teleconferencing allows separately located users to
interact with each other in 3D through agents in their own physical
environments. Existing methods leveraging volumetric capturing and
reconstruction can provide a high-fidelity experience but are often too complex
and expensive for everyday usage. Other solutions target mobile and
effortless-to-setup teleconferencing on AR Head Mounted Displays (HMD). They
directly transplant the conventional video conferencing onto an AR-HMD platform
or use avatars to represent remote participants. However, they can only support
either a high fidelity or a high level of co-presence. Moreover, the limited
Field of View (FoV) of HMDs could further influence users' immersive
experience. To achieve a balance between fidelity and co-presence, we explore
using life-size 2D video-based avatars (video avatars for short) in AR
teleconferencing. Specifically, with the potential effect of FoV on users'
perception of proximity, we first conduct a pilot study to explore the
local-user-centered optimal placement of video avatars in small-group AR
conversations. With the placement results, we then implement a proof-of-concept
prototype of video-avatar-based teleconferencing. We conduct user evaluations
with the prototype to verify its effectiveness in balancing fidelity and
co-presence. Following the indication in the pilot study, we further
quantitatively explore the effect of FoV size on the video avatar's optimal
placement through a user study involving more FoV conditions in a VR-simulated
environment. We regress placement models to serve as references for
computationally determining video avatar placements in such teleconferencing
applications on various existing AR HMDs and future ones with bigger FoVs.","['Xuanyu Wang', 'Weizhan Zhang', 'Christian Sandor', 'Hongbo Fu']",2024-01-04T09:51:33Z,http://arxiv.org/abs/2401.02171v1,['cs.HC']
"Amirkabir campus dataset: Real-world challenges and scenarios of Visual
  Inertial Odometry (VIO) for visually impaired people","Visual Inertial Odometry (VIO) algorithms estimate the accurate camera
trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The
applications of VIO span a diverse range, including augmented reality and
indoor navigation. VIO algorithms hold the potential to facilitate navigation
for visually impaired individuals in both indoor and outdoor settings.
Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges
in dynamic environments, particularly in densely populated corridors. Existing
VIO datasets, e.g., ADVIO, typically fail to effectively exploit these
challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI)
to address the mentioned problem and improve the navigation systems. AUT-VI is
a novel and super-challenging dataset with 126 diverse sequences in 17
different locations. This dataset contains dynamic objects, challenging
loop-closure/map-reuse, different lighting conditions, reflections, and sudden
camera movements to cover all extreme navigation scenarios. Moreover, in
support of ongoing development efforts, we have released the Android
application for data capture to the public. This allows fellow researchers to
easily capture their customized VIO dataset variations. In addition, we
evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry
(VO) methods on our dataset, emphasizing the essential need for this
challenging dataset.","['Ali Samadzadeh', 'Mohammad Hassan Mojab', 'Heydar Soudani', 'Seyed Hesamoddin Mireshghollah', 'Ahmad Nickabadi']",2024-01-07T23:13:51Z,http://arxiv.org/abs/2401.03604v1,['cs.CV']
"Surgical-DINO: Adapter Learning of Foundation Models for Depth
  Estimation in Endoscopic Surgery","Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,
surgical navigation and augmented reality visualization. Although the
foundation model exhibits outstanding performance in many vision tasks,
including depth estimation (e.g., DINOv2), recent works observed its
limitations in medical and surgical domain-specific applications. This work
presents a low-ranked adaptation (LoRA) of the foundation model for surgical
depth estimation. Methods: We design a foundation model-based depth estimation
method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for
depth estimation in endoscopic surgery. We build LoRA layers and integrate them
into DINO to adapt with surgery-specific domain knowledge instead of
conventional fine-tuning. During training, we freeze the DINO image encoder,
which shows excellent visual representation capacity, and only optimize the
LoRA layers and depth decoder to integrate features from the surgical scene.
Results: Our model is extensively validated on a MICCAI challenge dataset of
SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically
show that Surgical-DINO significantly outperforms all the state-of-the-art
models in endoscopic depth estimation tasks. The analysis with ablation studies
has shown evidence of the remarkable effect of our LoRA layers and adaptation.
Conclusion: Surgical-DINO shed some light on the successful adaptation of the
foundation models into the surgical domain for depth estimation. There is clear
evidence in the results that zero-shot prediction on pre-trained weights in
computer vision datasets or naive fine-tuning is not sufficient to use the
foundation model in the surgical domain directly. Code is available at
https://github.com/BeileiCui/SurgicalDINO.","['Beilei Cui', 'Mobarakol Islam', 'Long Bai', 'Hongliang Ren']",2024-01-11T16:22:42Z,http://arxiv.org/abs/2401.06013v2,"['cs.CV', 'cs.AI']"
Enabling Technologies for Web 3.0: A Comprehensive Survey,"Web 3.0 represents the next stage of Internet evolution, aiming to empower
users with increased autonomy, efficiency, quality, security, and privacy. This
evolution can potentially democratize content access by utilizing the latest
developments in enabling technologies. In this paper, we conduct an in-depth
survey of enabling technologies in the context of Web 3.0, such as blockchain,
semantic web, 3D interactive web, Metaverse, Virtual reality/Augmented reality,
Internet of Things technology, and their roles in shaping Web 3.0. We commence
by providing a comprehensive background of Web 3.0, including its concept,
basic architecture, potential applications, and industry adoption.
Subsequently, we examine recent breakthroughs in IoT, 5G, and blockchain
technologies that are pivotal to Web 3.0 development. Following that, other
enabling technologies, including AI, semantic web, and 3D interactive web, are
discussed. Utilizing these technologies can effectively address the critical
challenges in realizing Web 3.0, such as ensuring decentralized identity,
platform interoperability, data transparency, reducing latency, and enhancing
the system's scalability. Finally, we highlight significant challenges
associated with Web 3.0 implementation, emphasizing potential solutions and
providing insights into future research directions in this field.","['Md Arif Hassan', 'Mohammad Behdad Jamshidi', 'Bui Duc Manh', 'Nam H. Chu', 'Chi-Hieu Nguyen', 'Nguyen Quang Hieu', 'Cong T. Nguyen', 'Dinh Thai Hoang', 'Diep N. Nguyen', 'Nguyen Van Huynh', 'Mohammad Abu Alsheikh', 'Eryk Dutkiewicz']",2023-12-29T10:22:18Z,http://arxiv.org/abs/2401.10901v1,['cs.CY']
"MobileARLoc: On-device Robust Absolute Localisation for Pervasive
  Markerless Mobile AR","Recent years have seen significant improvement in absolute camera pose
estimation, paving the way for pervasive markerless Augmented Reality (AR).
However, accurate absolute pose estimation techniques are computation- and
storage-heavy, requiring computation offloading. As such, AR systems rely on
visual-inertial odometry (VIO) to track the device's relative pose between
requests to the server. However, VIO suffers from drift, requiring frequent
absolute repositioning. This paper introduces MobileARLoc, a new framework for
on-device large-scale markerless mobile AR that combines an absolute pose
regressor (APR) with a local VIO tracking system. Absolute pose regressors
(APRs) provide fast on-device pose estimation at the cost of reduced accuracy.
To address APR accuracy and reduce VIO drift, MobileARLoc creates a feedback
loop where VIO pose estimations refine the APR predictions. The VIO system
identifies reliable predictions of APR, which are then used to compensate for
the VIO drift. We comprehensively evaluate MobileARLoc through dataset
simulations. MobileARLoc halves the error compared to the underlying APR and
achieve fast (80\,ms) on-device inference speed.","['Changkun Liu', 'Yukun Zhao', 'Tristan Braud']",2024-01-21T14:48:38Z,http://arxiv.org/abs/2401.11511v3,['cs.CV']
"Full-Body Motion Reconstruction with Sparse Sensing from Graph
  Perspective","Estimating 3D full-body pose from sparse sensor data is a pivotal technique
employed for the reconstruction of realistic human motions in Augmented Reality
and Virtual Reality. However, translating sparse sensor signals into
comprehensive human motion remains a challenge since the sparsely distributed
sensors in common VR systems fail to capture the motion of full human body. In
this paper, we use well-designed Body Pose Graph (BPG) to represent the human
body and translate the challenge into a prediction problem of graph missing
nodes. Then, we propose a novel full-body motion reconstruction framework based
on BPG. To establish BPG, nodes are initially endowed with features extracted
from sparse sensor signals. Features from identifiable joint nodes across
diverse sensors are amalgamated and processed from both temporal and spatial
perspectives. Temporal dynamics are captured using the Temporal Pyramid
Structure, while spatial relations in joint movements inform the spatial
attributes. The resultant features serve as the foundational elements of the
BPG nodes. To further refine the BPG, node features are updated through a graph
neural network that incorporates edge reflecting varying joint relations. Our
method's effectiveness is evidenced by the attained state-of-the-art
performance, particularly in lower body motion, outperforming other baseline
methods. Additionally, an ablation study validates the efficacy of each module
in our proposed framework.","['Feiyu Yao', 'Zongkai Wu', 'Li Yi']",2024-01-22T09:29:42Z,http://arxiv.org/abs/2401.11783v1,['cs.CV']
"Fast Implicit Neural Representation Image Codec in Resource-limited
  Devices","Displaying high-quality images on edge devices, such as augmented reality
devices, is essential for enhancing the user experience. However, these devices
often face power consumption and computing resource limitations, making it
challenging to apply many deep learning-based image compression algorithms in
this field. Implicit Neural Representation (INR) for image compression is an
emerging technology that offers two key benefits compared to cutting-edge
autoencoder models: low computational complexity and parameter-free decoding.
It also outperforms many traditional and early neural compression methods in
terms of quality. In this study, we introduce a new Mixed Autoregressive Model
(MARM) to significantly reduce the decoding time for the current INR codec,
along with a new synthesis network to enhance reconstruction quality. MARM
includes our proposed Autoregressive Upsampler (ARU) blocks, which are highly
computationally efficient, and ARM from previous work to balance decoding time
and reconstruction quality. We also propose enhancing ARU's performance using a
checkerboard two-stage decoding strategy. Moreover, the ratio of different
modules can be adjusted to maintain a balance between quality and speed.
Comprehensive experiments demonstrate that our method significantly improves
computational efficiency while preserving image quality. With different
parameter settings, our method can outperform popular AE-based codecs in
constrained environments in terms of both quality and decoding time, or achieve
state-of-the-art reconstruction quality compared to other INR codecs.","['Xiang Liu', 'Jiahong Chen', 'Bin Chen', 'Zimo Liu', 'Baoyi An', 'Shu-Tao Xia']",2024-01-23T09:37:58Z,http://arxiv.org/abs/2401.12587v1,"['eess.IV', 'cs.CV']"
"Driving Towards Inclusion: Revisiting In-Vehicle Interaction in
  Autonomous Vehicles","This paper presents a comprehensive literature review of the current state of
in-vehicle human-computer interaction (HCI) in the context of self-driving
vehicles, with a specific focus on inclusion and accessibility. This study's
aim is to examine the user-centered design principles for inclusive HCI in
self-driving vehicles, evaluate existing HCI systems, and identify emerging
technologies that have the potential to enhance the passenger experience. The
paper begins by providing an overview of the current state of self-driving
vehicle technology, followed by an examination of the importance of HCI in this
context. Next, the paper reviews the existing literature on inclusive HCI
design principles and evaluates the effectiveness of current HCI systems in
self-driving vehicles. The paper also identifies emerging technologies that
have the potential to enhance the passenger experience, such as voice-activated
interfaces, haptic feedback systems, and augmented reality displays. Finally,
the paper proposes an end-to-end design framework for the development of an
inclusive in-vehicle experience, which takes into consideration the needs of
all passengers, including those with disabilities, or other accessibility
requirements. This literature review highlights the importance of user-centered
design principles in the development of HCI systems for self-driving vehicles
and emphasizes the need for inclusive design to ensure that all passengers can
safely and comfortably use these vehicles. The proposed end-to-end design
framework provides a practical approach to achieving this goal and can serve as
a valuable resource for designers, researchers, and policymakers in this field.","['Ashish Bastola', 'Julian Brinkley', 'Hao Wang', 'Abolfazl Razi']",2024-01-26T00:06:08Z,http://arxiv.org/abs/2401.14571v1,"['cs.HC', 'cs.AI', 'cs.CY']"
On the Emergence of Symmetrical Reality,"Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.","['Zhenliang Zhang', 'Zeyu Zhang', 'Ziyuan Jiao', 'Yao Su', 'Hangxin Liu', 'Wei Wang', 'Song-Chun Zhu']",2024-01-26T16:09:39Z,http://arxiv.org/abs/2401.15132v1,"['cs.HC', 'cs.AI']"
Neural Rendering and Its Hardware Acceleration: A Review,"Neural rendering is a new image and video generation method based on deep
learning. It combines the deep learning model with the physical knowledge of
computer graphics, to obtain a controllable and realistic scene model, and
realize the control of scene attributes such as lighting, camera parameters,
posture and so on. On the one hand, neural rendering can not only make full use
of the advantages of deep learning to accelerate the traditional forward
rendering process, but also provide new solutions for specific tasks such as
inverse rendering and 3D reconstruction. On the other hand, the design of
innovative hardware structures that adapt to the neural rendering pipeline
breaks through the parallel computing and power consumption bottleneck of
existing graphics processors, which is expected to provide important support
for future key areas such as virtual and augmented reality, film and television
creation and digital entertainment, artificial intelligence and the metaverse.
In this paper, we review the technical connotation, main challenges, and
research progress of neural rendering. On this basis, we analyze the common
requirements of neural rendering pipeline for hardware acceleration and the
characteristics of the current hardware acceleration architecture, and then
discuss the design challenges of neural rendering processor architecture.
Finally, the future development trend of neural rendering processor
architecture is prospected.","['Xinkai Yan', 'Jieting Xu', 'Yuchi Huo', 'Hujun Bao']",2024-01-06T07:57:11Z,http://arxiv.org/abs/2402.00028v1,"['cs.GR', 'cs.CV', 'eess.IV']"
"A Review on Blockchain Technologies for an Advanced and Cyber-Resilient
  Automotive Industry","In the last century the automotive industry has arguably transformed society,
being one of the most complex, sophisticated and technologically advanced
industries, with innovations ranging from hybrid, electric and self-driving
smart cars to the development of IoT-connected cars. Due to its complexity, it
requires the involvement of many Industry 4.0 technologies, like robotics,
advanced manufacturing systems, cyber-physical systems or augmented reality.
One of the latest technologies that can benefit the automotive industry is
blockchain, which can enhance its data security, privacy, anonymity,
traceability, accountability, integrity, robustness, transparency,
trustworthiness and authentication, as well as provide long-term sustainability
and a higher operational efficiency to the whole industry. This review analyzes
the great potential of applying blockchain technologies to the automotive
industry emphasizing its cybersecurity features. Thus, the applicability of
blockchain is evaluated after examining the state-of-the-art and devising the
main stakeholders' current challenges. Furthermore, the article describes the
most relevant use cases, since the broad adoption of blockchain unlocks a wide
area of short- and medium-term promising automotive applications that can
create new business models and even disrupt the car-sharing economy as we know
it. Finally, after a Strengths, Weaknesses, Opportunities, and Threats (SWOT)
analysis, some recommendations are enumerated with the aim of guiding
researchers and companies in future cyber-resilient automotive industry
developments.","['Paula Fraga-Lamas', 'Tiago M. Fernandez-Carames']",2024-02-01T19:23:19Z,http://arxiv.org/abs/2402.00954v1,['cs.CR']
"Evaluation of Zadoff-Chu, Kasami and Chirp based encoding schemes for
  Acoustic Local Positioning Systems","The task of determining the physical coordinates of a target in indoor
environments is still a key factor for many applications including people and
robot navigation, user tracking, location-based advertising, augmented reality,
gaming, emergency response or ambient assisted living environments. Among the
different possibilities for indoor positioning, Acoustic Local Positioning
Systems (ALPS) have the potential for centimeter level positioning accuracy
with coverage distances up to tens of meters. In addition, acoustic transducers
are small, low cost and reliable thanks to the room constrained propagation of
these mechanical waves. Waveform design (coding and modulation) is usually
incorporated into these systems to facilitate the detection of the transmitted
signals at the receiver. The aperiodic correlation properties of the emitted
signals have a large impact on how the ALPS cope with common impairment factors
such as multipath propagation, multiple access interference, Doppler shifting,
near-far effect or ambient noise. This work analyzes three of the most
promising families of codes found in the literature for ALPS: Kasami codes,
Zadoff-Chu and Orthogonal Chirp signals. The performance of these codes is
evaluated in terms of time of arrival accuracy and characterized by means of
model simulation under realistic conditions and by means of experimental tests
in controlled environments. The results derived from this study can be of
interest for other applications based on spreading sequences, such as
underwater acoustic systems, ultrasonic imaging or even Code Division Multiple
Access (CDMA) communications systems.","['Santiago Murano', 'Carmen Perez-Rubio', 'David Gualda', 'Fernando J. Alvarez', 'Teodoro Aguilera', 'Carlos de Marziani']",2024-02-04T08:24:13Z,http://arxiv.org/abs/2402.02400v1,['eess.SP']
Perceptual Video Quality Assessment: A Survey,"Perceptual video quality assessment plays a vital role in the field of video
processing due to the existence of quality degradations introduced in various
stages of video signal acquisition, compression, transmission and display. With
the advancement of internet communication and cloud service technology, video
content and traffic are growing exponentially, which further emphasizes the
requirement for accurate and rapid assessment of video quality. Therefore,
numerous subjective and objective video quality assessment studies have been
conducted over the past two decades for both generic videos and specific videos
such as streaming, user-generated content (UGC), 3D, virtual and augmented
reality (VR and AR), high frame rate (HFR), audio-visual, etc. This survey
provides an up-to-date and comprehensive review of these video quality
assessment studies. Specifically, we first review the subjective video quality
assessment methodologies and databases, which are necessary for validating the
performance of video quality metrics. Second, the objective video quality
assessment algorithms for general purposes are surveyed and concluded according
to the methodologies utilized in the quality measures. Third, we overview the
objective video quality assessment measures for specific applications and
emerging topics. Finally, the performances of the state-of-the-art video
quality assessment measures are compared and analyzed. This survey provides a
systematic overview of both classical works and recent progresses in the realm
of video quality assessment, which can help other researchers quickly access
the field and conduct relevant research.","['Xiongkuo Min', 'Huiyu Duan', 'Wei Sun', 'Yucheng Zhu', 'Guangtao Zhai']",2024-02-05T16:13:52Z,http://arxiv.org/abs/2402.03413v1,"['cs.MM', 'cs.CV', 'eess.IV']"
PAS-SLAM: A Visual SLAM System for Planar Ambiguous Scenes,"Visual SLAM (Simultaneous Localization and Mapping) based on planar features
has found widespread applications in fields such as environmental structure
perception and augmented reality. However, current research faces challenges in
accurately localizing and mapping in planar ambiguous scenes, primarily due to
the poor accuracy of the employed planar features and data association methods.
In this paper, we propose a visual SLAM system based on planar features
designed for planar ambiguous scenes, encompassing planar processing, data
association, and multi-constraint factor graph optimization. We introduce a
planar processing strategy that integrates semantic information with planar
features, extracting the edges and vertices of planes to be utilized in tasks
such as plane selection, data association, and pose optimization. Next, we
present an integrated data association strategy that combines plane parameters,
semantic information, projection IoU (Intersection over Union), and
non-parametric tests, achieving accurate and robust plane data association in
planar ambiguous scenes. Finally, we design a set of multi-constraint factor
graphs for camera pose optimization. Qualitative and quantitative experiments
conducted on publicly available datasets demonstrate that our proposed system
competes effectively in both accuracy and robustness in terms of map
construction and camera localization compared to state-of-the-art methods.","['Xinggang Hu', 'Yanmin Wu', 'Mingyuan Zhao', 'Linghao Yang', 'Xiangkui Zhang', 'Xiangyang Ji']",2024-02-09T01:34:26Z,http://arxiv.org/abs/2402.06131v1,"['cs.RO', 'cs.CV']"
"Camera Calibration through Geometric Constraints from Rotation and
  Projection Matrices","The process of camera calibration involves estimating the intrinsic and
extrinsic parameters, which are essential for accurately performing tasks such
as 3D reconstruction, object tracking and augmented reality. In this work, we
propose a novel constraints-based loss for measuring the intrinsic (focal
length: $(f_x, f_y)$ and principal point: $(p_x, p_y)$) and extrinsic
(baseline: ($b$), disparity: ($d$), translation: $(t_x, t_y, t_z)$, and
rotation specifically pitch: $(\theta_p)$) camera parameters. Our novel
constraints are based on geometric properties inherent in the camera model,
including the anatomy of the projection matrix (vanishing points, image of
world origin, axis planes) and the orthonormality of the rotation matrix. Thus
we proposed a novel Unsupervised Geometric Constraint Loss (UGCL) via a
multitask learning framework. Our methodology is a hybrid approach that employs
the learning power of a neural network to estimate the desired parameters along
with the underlying mathematical properties inherent in the camera projection
matrix. This distinctive approach not only enhances the interpretability of the
model but also facilitates a more informed learning process. Additionally, we
introduce a new CVGL Camera Calibration dataset, featuring over 900
configurations of camera parameters, incorporating 63,600 image pairs that
closely mirror real-world conditions. By training and testing on both synthetic
and real-world datasets, our proposed approach demonstrates improvements across
all parameters when compared to the state-of-the-art (SOTA) benchmarks. The
code and the updated dataset can be found here:
https://github.com/CVLABLUMS/CVGL-Camera-Calibration","['Muhammad Waleed', 'Abdul Rauf', 'Murtaza Taj']",2024-02-13T13:07:34Z,http://arxiv.org/abs/2402.08437v2,['cs.CV']
X-maps: Direct Depth Lookup for Event-based Structured Light Systems,"We present a new approach to direct depth estimation for Spatial Augmented
Reality (SAR) applications using event cameras. These dynamic vision sensors
are a great fit to be paired with laser projectors for depth estimation in a
structured light approach. Our key contributions involve a conversion of the
projector time map into a rectified X-map, capturing x-axis correspondences for
incoming events and enabling direct disparity lookup without any additional
search. Compared to previous implementations, this significantly simplifies
depth estimation, making it more efficient, while the accuracy is similar to
the time map-based process. Moreover, we compensate non-linear temporal
behavior of cheap laser projectors by a simple time map calibration, resulting
in improved performance and increased depth estimation accuracy. Since depth
estimation is executed by two lookups only, it can be executed almost instantly
(less than 3 ms per frame with a Python implementation) for incoming events.
This allows for real-time interactivity and responsiveness, which makes our
approach especially suitable for SAR experiences where low latency, high frame
rates and direct feedback are crucial. We present valuable insights gained into
data transformed into X-maps and evaluate our depth from disparity estimation
against the state of the art time map-based results. Additional results and
code are available on our project page: https://fraunhoferhhi.github.io/X-maps/","['Wieland Morgenstern', 'Niklas Gard', 'Simon Baumann', 'Anna Hilsmann', 'Peter Eisert']",2024-02-15T16:29:46Z,http://arxiv.org/abs/2402.10061v1,['cs.CV']
A Disruptive Research Playbook for Studying Disruptive Innovations,"As researchers, we are now witnessing a fundamental change in our
technologically-enabled world due to the advent and diffusion of highly
disruptive technologies such as generative AI, Augmented Reality (AR) and
Virtual Reality (VR). In particular, software engineering has been profoundly
affected by the transformative power of disruptive innovations for decades,
with a significant impact of technical advancements on social dynamics due to
its the socio-technical nature. In this paper, we reflect on the importance of
formulating and addressing research in software engineering through a
socio-technical lens, thus ensuring a holistic understanding of the complex
phenomena in this field. We propose a research playbook with the goal of
providing a guide to formulate compelling and socially relevant research
questions and to identify the appropriate research strategies for empirical
investigations, with an eye on the long-term implications of technologies or
their use. We showcase how to apply the research playbook. Firstly, we show how
it can be used retrospectively to reflect on a prior disruptive technology,
Stack Overflow, and its impact on software development. Secondly, we show it
can be used to question the impact of two current disruptive technologies: AI
and AR/VR. Finally, we introduce a specialized GPT model to support the
researcher in framing future investigations. We conclude by discussing the
broader implications of adopting the playbook for both researchers and
practitioners in software engineering and beyond.","['Margaret-Anne Storey', 'Daniel Russo', 'Nicole Novielli', 'Takashi Kobayashi', 'Dong Wang']",2024-02-20T19:13:36Z,http://arxiv.org/abs/2402.13329v1,['cs.SE']
Deep Homography Estimation for Visual Place Recognition,"Visual place recognition (VPR) is a fundamental task for many applications
such as robot localization and augmented reality. Recently, the hierarchical
VPR methods have received considerable attention due to the trade-off between
accuracy and efficiency. They usually first use global features to retrieve the
candidate images, then verify the spatial consistency of matched local features
for re-ranking. However, the latter typically relies on the RANSAC algorithm
for fitting homography, which is time-consuming and non-differentiable. This
makes existing methods compromise to train the network only in global feature
extraction. Here, we propose a transformer-based deep homography estimation
(DHE) network that takes the dense feature map extracted by a backbone network
as input and fits homography for fast and learnable geometric verification.
Moreover, we design a re-projection error of inliers loss to train the DHE
network without additional homography labels, which can also be jointly trained
with the backbone network to help it extract the features that are more
suitable for local matching. Extensive experiments on benchmark datasets show
that our method can outperform several state-of-the-art methods. And it is more
than one order of magnitude faster than the mainstream hierarchical VPR methods
using RANSAC. The code is released at https://github.com/Lu-Feng/DHE-VPR.","['Feng Lu', 'Shuting Dong', 'Lijun Zhang', 'Bingxi Liu', 'Xiangyuan Lan', 'Dongmei Jiang', 'Chun Yuan']",2024-02-25T13:22:17Z,http://arxiv.org/abs/2402.16086v2,"['cs.CV', 'cs.AI']"
"Lightweight, error-tolerant edge detection using memristor-enabled
  stochastic logics","The demand for efficient edge vision has spurred the interest in developing
stochastic computing approaches for performing image processing tasks.
Memristors with inherent stochasticity readily introduce probability into the
computations and thus enable stochastic image processing computations. Here, we
present a stochastic computing approach for edge detection, a fundamental image
processing technique, facilitated with memristor-enabled stochastic logics.
Specifically, we integrate the memristors with logic circuits and harness the
stochasticity from the memristors to realize compact stochastic logics for
stochastic number encoding and processing. The stochastic numbers, exhibiting
well-regulated probabilities and correlations, can be processed to perform
logic operations with statistical probabilities. This can facilitate
lightweight stochastic edge detection for edge visual scenarios characterized
with high-level noise errors. As a practical demonstration, we implement a
hardware stochastic Roberts cross operator using the stochastic logics, and
prove its exceptional edge detection performance, remarkably, with 95% less
computational cost while withstanding 50% bit-flip errors. The results
underscore the great potential of our stochastic edge detection approach in
developing lightweight, error-tolerant edge vision hardware and systems for
autonomous driving, virtual/augmented reality, medical imaging diagnosis,
industrial automation, and beyond.","['Lekai Song', 'Pengyu Liu', 'Jingfang Pei', 'Yang Liu', 'Songwei Liu', 'Shengbo Wang', 'Leonard W. T. Ng', 'Tawfique Hasan', 'Kong-Pang Pun', 'Shuo Gao', 'Guohua Hu']",2024-02-25T06:23:02Z,http://arxiv.org/abs/2402.16908v2,"['cs.ET', 'cond-mat.mtrl-sci', 'cs.LG', 'eess.IV']"
"Outdoor Environment Reconstruction with Deep Learning on Radio
  Propagation Paths","Conventional methods for outdoor environment reconstruction rely
predominantly on vision-based techniques like photogrammetry and LiDAR, facing
limitations such as constrained coverage, susceptibility to environmental
conditions, and high computational and energy demands. These challenges are
particularly pronounced in applications like augmented reality navigation,
especially when integrated with wearable devices featuring constrained
computational resources and energy budgets. In response, this paper proposes a
novel approach harnessing ambient wireless signals for outdoor environment
reconstruction. By analyzing radio frequency (RF) data, the paper aims to
deduce the environmental characteristics and digitally reconstruct the outdoor
surroundings. Investigating the efficacy of selected deep learning (DL)
techniques on the synthetic RF dataset WAIR-D, the study endeavors to address
the research gap in this domain. Two DL-driven approaches are evaluated
(convolutional U-Net and CLIP+ based on vision transformers), with performance
assessed using metrics like intersection-over-union (IoU), Hausdorff distance,
and Chamfer distance. The results demonstrate promising performance of the
RF-based reconstruction method, paving the way towards lightweight and scalable
reconstruction solutions.","['Hrant Khachatrian', 'Rafayel Mkrtchyan', 'Theofanis P. Raptis']",2024-02-27T09:11:10Z,http://arxiv.org/abs/2402.17336v1,"['cs.NI', 'cs.LG', 'eess.SP']"
"Polarization-Encoded Lenticular Nano-Printing with Single-Layer
  Metasurfaces","Metasurface-based nano-printing has enabled ultrahigh-resolution grayscale or
color image display. However, the maximum number of independent nano-printing
images allowed by one single-layer metasurface is still limited despite many
multiplexing methods that have been proposed to increase the design degree of
freedom. In this work, we substantially push the multiplexing limit of
nano-printing by transforming images at different observation angles into
mapping the corresponding images to different positions in the Fourier space,
and simultaneously controlling the complex electric field across multiple
polarization channels. Our proposed Polarization-Encoded Lenticular
Nano-Printing (Pollen), aided by a modified evolutionary algorithm, allows the
display of several images based on the viewing angle, similar to traditional
lenticular printing but without requiring a lenticular layer. In addition, it
extends the display capability to encompass multiple polarization states.
Empowered by the ability to control the complex amplitude of three polarization
channels, we numerically and experimentally demonstrate the generation of 13
distinguished gray-scale Chinese ink wash painting images, 49 binary patterns,
and three sets of 3D nano-printing images, totaling 25 unique visuals. These
results present the largest number of recorded images with ultra-high
resolution to date. Our innovative Pollen technique is expected to benefit the
development of modern optical applications, including but not limited to
optical encryption, optical data storage, lightweight display, and augmented
reality and virtual reality.","['Lin Deng', 'Ziqiang Cai', 'Yongmin Liu']",2024-03-05T03:21:21Z,http://arxiv.org/abs/2403.02620v1,"['physics.optics', 'physics.app-ph']"
"Vox-Fusion++: Voxel-based Neural Implicit Dense Tracking and Mapping
  with Multi-maps","In this paper, we introduce Vox-Fusion++, a multi-maps-based robust dense
tracking and mapping system that seamlessly fuses neural implicit
representations with traditional volumetric fusion techniques. Building upon
the concept of implicit mapping and positioning systems, our approach extends
its applicability to real-world scenarios. Our system employs a voxel-based
neural implicit surface representation, enabling efficient encoding and
optimization of the scene within each voxel. To handle diverse environments
without prior knowledge, we incorporate an octree-based structure for scene
division and dynamic expansion. To achieve real-time performance, we propose a
high-performance multi-process framework. This ensures the system's suitability
for applications with stringent time constraints. Additionally, we adopt the
idea of multi-maps to handle large-scale scenes, and leverage loop detection
and hierarchical pose optimization strategies to reduce long-term pose drift
and remove duplicate geometry. Through comprehensive evaluations, we
demonstrate that our method outperforms previous methods in terms of
reconstruction quality and accuracy across various scenarios. We also show that
our Vox-Fusion++ can be used in augmented reality and collaborative mapping
applications. Our source code will be publicly available at
\url{https://github.com/zju3dv/Vox-Fusion_Plus_Plus}","['Hongjia Zhai', 'Hai Li', 'Xingrui Yang', 'Gan Huang', 'Yuhang Ming', 'Hujun Bao', 'Guofeng Zhang']",2024-03-19T08:21:54Z,http://arxiv.org/abs/2403.12536v1,['cs.CV']
"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a
  Multi-Objective Genetic Algorithm","Augmented Reality (AR) and Virtual Reality (VR) systems involve
computationally intensive image processing algorithms that can burden
end-devices with limited resources, leading to poor performance in providing
low latency services. Edge-to-cloud computing overcomes the limitations of
end-devices by offloading their computations to nearby edge devices or remote
cloud servers. Although this proves to be sufficient for many applications,
optimal placement of latency sensitive AR/VR services in edge-to-cloud
infrastructures (to provide desirable service response times and reliability)
remain a formidable challenging. To address this challenge, this paper develops
a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of
AR/VR-based services in multi-tier edge-to-cloud environments. The primary
objective of the proposed MOGA is to minimize the response time of all running
services, while maximizing the reliability of the underlying system from both
software and hardware perspectives. To evaluate its performance, we
mathematically modeled all components and developed a tailor-made simulator to
assess its effectiveness on various scales. MOGA was compared with several
heuristics to prove that intuitive solutions, which are usually assumed
sufficient, are not efficient enough for the stated problem. The experimental
results indicated that MOGA can significantly reduce the response time of
deployed services by an average of 67\% on different scales, compared to other
heuristic methods. MOGA also ensures reliability of the 97\% infrastructure
(hardware) and 95\% services (software).","['Mohammadsadeq Garshasbi Herabad', 'Javid Taheri', 'Bestoun S. Ahmed', 'Calin Curescu']",2024-03-19T15:54:56Z,http://arxiv.org/abs/2403.12849v1,['cs.DC']
"Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks","Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.","['Aqeel Anwar', 'Tae Eun Choe', 'Zian Wang', 'Sanja Fidler', 'Minwoo Park']",2024-03-22T17:49:11Z,http://arxiv.org/abs/2403.15370v1,"['cs.CV', 'cs.LG', 'cs.RO']"
"Metasurface-Enabled Multifunctional Single-Frequency Sensors without
  External Power","IoT sensors are crucial for visualizing multidimensional and multimodal
information and enabling future IT applications/services such as cyber-physical
space, digital twins, autonomous driving, smart cities, and virtual/augmented
reality (VR or AR). However, IoT sensors need to be battery-free to
realistically manage and maintain the growing number of available sensing
devices. Here, we provide a novel sensor design approach that employs
metasurfaces to enable multifunctional sensing without requiring an external
power source. Importantly, unlike existing metasurface-based sensors, our
metasurfaces can sense multiple physical parameters even at a fixed frequency
by breaking classic harmonic oscillations in the time domain, making the
proposed sensors viable for usage with limited frequency resources. Moreover,
we provide a method for predicting physical parameters using the machine
learning-based approach of random forest regression. The sensing performance
was confirmed by estimating temperature and light intensity, and excellent
determination coefficients larger than 0.96 were achieved. Our study affords
new opportunities for sensing multiple physical properties without relying on
an external power source or needing multiple frequencies, which markedly
simplifies and facilitates the design of next-generation wireless communication
systems.","['Masaya Tashiro', 'Kosuke Ide', 'Kosei Asano', 'Satoshi Ishii', 'Yuta Sugiura', 'Akira Uchiyama', 'Ashif A. Fathnan', 'Hiroki Wakatsuchi']",2024-03-13T06:31:54Z,http://arxiv.org/abs/2403.15427v2,"['eess.SP', 'physics.app-ph']"
Review Ecosystems to access Educational XR Experiences: a Scoping Review,"Educators, developers, and other stakeholders face challenges when creating,
adapting, and utilizing virtual and augmented reality (XR) experiences for
teaching curriculum topics. User created reviews of these applications provide
important information about their relevance and effectiveness in supporting
achievement of educational outcomes. To make these reviews accessible,
relevant, and useful, they must be readily available and presented in a format
that supports decision-making by educators. This paper identifies best
practices for developing a new review ecosystem by analyzing existing
approaches to providing reviews of interactive experiences. It focuses on the
form and format of these reviews, as well as the mechanisms for sharing
information about experiences and identifying which ones are most effective.
The paper also examines the incentives that drive review creation and
maintenance, ensuring that new experiences receive attention from reviewers and
that relevant information is updated when necessary. The strategies and
opportunities for developing an educational XR (eduXR) review ecosystem include
methods for measuring properties such as quality metrics, engaging a broad
range of stakeholders in the review process, and structuring the system as a
closed loop managed by feedback and incentive structures to ensure stability
and productivity. Computing educators are well-positioned to lead the
development of these review ecosystems, which can relate XR experiences to the
potential opportunities for teaching and learning that they offer.","['Shaun Bangay', 'Adam P. A. Cardilini', 'Sophie McKenzie', 'Maria Nicholas', 'Manjeet Singh']",2024-03-25T22:44:28Z,http://arxiv.org/abs/2403.17243v1,"['cs.CY', 'cs.SI', 'I.7.4; K.3.1']"
"BadPart: Unified Black-box Adversarial Patch Attacks against Pixel-wise
  Regression Tasks","Pixel-wise regression tasks (e.g., monocular depth estimation (MDE) and
optical flow estimation (OFE)) have been widely involved in our daily life in
applications like autonomous driving, augmented reality and video composition.
Although certain applications are security-critical or bear societal
significance, the adversarial robustness of such models are not sufficiently
studied, especially in the black-box scenario. In this work, we introduce the
first unified black-box adversarial patch attack framework against pixel-wise
regression tasks, aiming to identify the vulnerabilities of these models under
query-based black-box attacks. We propose a novel square-based adversarial
patch optimization framework and employ probabilistic square sampling and
score-based gradient estimation techniques to generate the patch effectively
and efficiently, overcoming the scalability problem of previous black-box patch
attacks. Our attack prototype, named BadPart, is evaluated on both MDE and OFE
tasks, utilizing a total of 7 models. BadPart surpasses 3 baseline methods in
terms of both attack performance and efficiency. We also apply BadPart on the
Google online service for portrait depth estimation, causing 43.5% relative
distance error with 50K queries. State-of-the-art (SOTA) countermeasures cannot
defend our attack effectively.","['Zhiyuan Cheng', 'Zhaoyi Liu', 'Tengda Guo', 'Shiwei Feng', 'Dongfang Liu', 'Mingjie Tang', 'Xiangyu Zhang']",2024-04-01T05:01:52Z,http://arxiv.org/abs/2404.00924v3,['cs.CV']
"Improving Facial Landmark Detection Accuracy and Efficiency with
  Knowledge Distillation","The domain of computer vision has experienced significant advancements in
facial-landmark detection, becoming increasingly essential across various
applications such as augmented reality, facial recognition, and emotion
analysis. Unlike object detection or semantic segmentation, which focus on
identifying objects and outlining boundaries, faciallandmark detection aims to
precisely locate and track critical facial features. However, deploying deep
learning-based facial-landmark detection models on embedded systems with
limited computational resources poses challenges due to the complexity of
facial features, especially in dynamic settings. Additionally, ensuring
robustness across diverse ethnicities and expressions presents further
obstacles. Existing datasets often lack comprehensive representation of facial
nuances, particularly within populations like those in Taiwan. This paper
introduces a novel approach to address these challenges through the development
of a knowledge distillation method. By transferring knowledge from larger
models to smaller ones, we aim to create lightweight yet powerful deep learning
models tailored specifically for facial-landmark detection tasks. Our goal is
to design models capable of accurately locating facial landmarks under varying
conditions, including diverse expressions, orientations, and lighting
environments. The ultimate objective is to achieve high accuracy and real-time
performance suitable for deployment on embedded systems. This method was
successfully implemented and achieved a top 6th place finish out of 165
participants in the IEEE ICME 2024 PAIR competition.","['Zong-Wei Hong', 'Yu-Chen Lin']",2024-04-09T05:30:58Z,http://arxiv.org/abs/2404.06029v1,['cs.CV']
"Sparse Points to Dense Clouds: Enhancing 3D Detection with Limited LiDAR
  Data","3D detection is a critical task that enables machines to identify and locate
objects in three-dimensional space. It has a broad range of applications in
several fields, including autonomous driving, robotics and augmented reality.
Monocular 3D detection is attractive as it requires only a single camera,
however, it lacks the accuracy and robustness required for real world
applications. High resolution LiDAR on the other hand, can be expensive and
lead to interference problems in heavy traffic given their active
transmissions. We propose a balanced approach that combines the advantages of
monocular and point cloud-based 3D detection. Our method requires only a small
number of 3D points, that can be obtained from a low-cost, low-resolution
sensor. Specifically, we use only 512 points, which is just 1% of a full LiDAR
frame in the KITTI dataset. Our method reconstructs a complete 3D point cloud
from this limited 3D information combined with a single image. The
reconstructed 3D point cloud and corresponding image can be used by any
multi-modal off-the-shelf detector for 3D object detection. By using the
proposed network architecture with an off-the-shelf multi-modal 3D detector,
the accuracy of 3D detection improves by 20% compared to the state-of-the-art
monocular detection methods and 6% to 9% compare to the baseline multi-modal
methods on KITTI and JackRabbot datasets.","['Aakash Kumar', 'Chen Chen', 'Ajmal Mian', 'Neils Lobo', 'Mubarak Shah']",2024-04-10T03:54:53Z,http://arxiv.org/abs/2404.06715v1,['cs.CV']
"FaceFilterSense: A Filter-Resistant Face Recognition and Facial
  Attribute Analysis Framework","With the advent of social media, fun selfie filters have come into tremendous
mainstream use affecting the functioning of facial biometric systems as well as
image recognition systems. These filters vary from beautification filters and
Augmented Reality (AR)-based filters to filters that modify facial landmarks.
Hence, there is a need to assess the impact of such filters on the performance
of existing face recognition systems. The limitation associated with existing
solutions is that these solutions focus more on the beautification filters.
However, the current AR-based filters and filters which distort facial key
points are in vogue recently and make the faces highly unrecognizable even to
the naked eye. Also, the filters considered are mostly obsolete with limited
variations. To mitigate these limitations, we aim to perform a holistic impact
analysis of the latest filters and propose an user recognition model with the
filtered images. We have utilized a benchmark dataset for baseline images, and
applied the latest filters over them to generate a beautified/filtered dataset.
Next, we have introduced a model FaceFilterNet for beautified user recognition.
In this framework, we also utilize our model to comment on various attributes
of the person including age, gender, and ethnicity. In addition, we have also
presented a filter-wise impact analysis on face recognition, age estimation,
gender, and ethnicity prediction. The proposed method affirms the efficacy of
our dataset with an accuracy of 87.25% and an optimal accuracy for facial
attribute analysis.","['Shubham Tiwari', 'Yash Sethia', 'Ritesh Kumar', 'Ashwani Tanwar', 'Rudresh Dwivedi']",2024-04-12T07:04:56Z,http://arxiv.org/abs/2404.08277v2,['cs.CV']
VRS-NeRF: Visual Relocalization with Sparse Neural Radiance Field,"Visual relocalization is a key technique to autonomous driving, robotics, and
virtual/augmented reality. After decades of explorations, absolute pose
regression (APR), scene coordinate regression (SCR), and hierarchical methods
(HMs) have become the most popular frameworks. However, in spite of high
efficiency, APRs and SCRs have limited accuracy especially in large-scale
outdoor scenes; HMs are accurate but need to store a large number of 2D
descriptors for matching, resulting in poor efficiency. In this paper, we
propose an efficient and accurate framework, called VRS-NeRF, for visual
relocalization with sparse neural radiance field. Precisely, we introduce an
explicit geometric map (EGM) for 3D map representation and an implicit learning
map (ILM) for sparse patches rendering. In this localization process, EGP
provides priors of spare 2D points and ILM utilizes these sparse points to
render patches with sparse NeRFs for matching. This allows us to discard a
large number of 2D descriptors so as to reduce the map size. Moreover,
rendering patches only for useful points rather than all pixels in the whole
image reduces the rendering time significantly. This framework inherits the
accuracy of HMs and discards their low efficiency. Experiments on 7Scenes,
CambridgeLandmarks, and Aachen datasets show that our method gives much better
accuracy than APRs and SCRs, and close performance to HMs but is much more
efficient.","['Fei Xue', 'Ignas Budvytis', 'Daniel Olmeda Reino', 'Roberto Cipolla']",2024-04-14T14:26:33Z,http://arxiv.org/abs/2404.09271v1,"['cs.CV', 'cs.RO']"
"Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic
  Displays","Recently, deep learning-based computer-generated holography (CGH) has
demonstrated tremendous potential in three-dimensional (3D) displays and
yielded impressive display quality. However, most existing deep learning-based
CGH techniques can only generate holograms of 1080p resolution, which is far
from the ultra-high resolution (16K+) required for practical virtual reality
(VR) and augmented reality (AR) applications to support a wide field of view
and large eye box. One of the major obstacles in current CGH frameworks lies in
the limited memory available on consumer-grade GPUs which could not facilitate
the generation of higher-definition holograms. To overcome the aforementioned
challenge, we proposed a divide-conquer-and-merge strategy to address the
memory and computational capacity scarcity in ultra-high-definition CGH
generation. This algorithm empowers existing CGH frameworks to synthesize
higher-definition holograms at a faster speed while maintaining high-fidelity
image display quality. Both simulations and experiments were conducted to
demonstrate the capabilities of the proposed framework. By integrating our
strategy into HoloNet and CCNNs, we achieved significant reductions in GPU
memory usage during the training period by 64.3\% and 12.9\%, respectively.
Furthermore, we observed substantial speed improvements in hologram generation,
with an acceleration of up to 3$\times$ and 2 $\times$, respectively.
Particularly, we successfully trained and inferred 8K definition holograms on
an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore,
we conducted full-color optical experiments to verify the effectiveness of our
method. We believe our strategy can provide a novel approach for memory- and
time-efficient holographic displays.","['Zhenxing Dong', 'Jidong Jia', 'Yan Li', 'Yuye Ling']",2024-02-25T13:58:03Z,http://arxiv.org/abs/2404.10777v1,"['eess.IV', 'cs.GR', 'physics.optics']"
Holographic Parallax Improves 3D Perceptual Realism,"Holographic near-eye displays are a promising technology to solve
long-standing challenges in virtual and augmented reality display systems. Over
the last few years, many different computer-generated holography (CGH)
algorithms have been proposed that are supervised by different types of target
content, such as 2.5D RGB-depth maps, 3D focal stacks, and 4D light fields. It
is unclear, however, what the perceptual implications are of the choice of
algorithm and target content type. In this work, we build a perceptual testbed
of a full-color, high-quality holographic near-eye display. Under natural
viewing conditions, we examine the effects of various CGH supervision formats
and conduct user studies to assess their perceptual impacts on 3D realism. Our
results indicate that CGH algorithms designed for specific viewpoints exhibit
noticeable deficiencies in achieving 3D realism. In contrast, holograms
incorporating parallax cues consistently outperform other formats across
different viewing conditions, including the center of the eyebox. This finding
is particularly interesting and suggests that the inclusion of parallax cues in
CGH rendering plays a crucial role in enhancing the overall quality of the
holographic experience. This work represents an initial stride towards
delivering a perceptually realistic 3D experience with holographic near-eye
displays.","['Dongyeon Kim', 'Seung-Woo Nam', 'Suyeon Choi', 'Jong-Mo Seo', 'Gordon Wetzstein', 'Yoonchan Jeong']",2024-04-18T00:11:51Z,http://arxiv.org/abs/2404.11810v1,['cs.GR']
"Co-designing a Sub-millisecond Latency Event-based Eye Tracking System
  with Submanifold Sparse CNN","Eye-tracking technology is integral to numerous consumer electronics
applications, particularly in the realm of virtual and augmented reality
(VR/AR). These applications demand solutions that excel in three crucial
aspects: low-latency, low-power consumption, and precision. Yet, achieving
optimal performance across all these fronts presents a formidable challenge,
necessitating a balance between sophisticated algorithms and efficient backend
hardware implementations. In this study, we tackle this challenge through a
synergistic software/hardware co-design of the system with an event camera.
Leveraging the inherent sparsity of event-based input data, we integrate a
novel sparse FPGA dataflow accelerator customized for submanifold sparse
convolution neural networks (SCNN). The SCNN implemented on the accelerator can
efficiently extract the embedding feature vector from each representation of
event slices by only processing the non-zero activations. Subsequently, these
vectors undergo further processing by a gated recurrent unit (GRU) and a fully
connected layer on the host CPU to generate the eye centers. Deployment and
evaluation of our system reveal outstanding performance metrics. On the
Event-based Eye-Tracking-AIS2024 dataset, our system achieves 81% p5 accuracy,
99.5% p10 accuracy, and 3.71 Mean Euclidean Distance with 0.7 ms latency while
only consuming 2.29 mJ per inference. Notably, our solution opens up
opportunities for future eye-tracking systems. Code is available at
https://github.com/CASR-HKU/ESDA/tree/eye_tracking.","['Baoheng Zhang', 'Yizhao Gao', 'Jingyuan Li', 'Hayden Kwok-Hay So']",2024-04-22T15:28:42Z,http://arxiv.org/abs/2404.14279v1,"['cs.CV', 'cs.AR']"
Motor Focus: Ego-Motion Prediction with All-Pixel Matching,"Motion analysis plays a critical role in various applications, from virtual
reality and augmented reality to assistive visual navigation. Traditional
self-driving technologies, while advanced, typically do not translate directly
to pedestrian applications due to their reliance on extensive sensor arrays and
non-feasible computational frameworks. This highlights a significant gap in
applying these solutions to human users since human navigation introduces
unique challenges, including the unpredictable nature of human movement,
limited processing capabilities of portable devices, and the need for
directional responsiveness due to the limited perception range of humans. In
this project, we introduce an image-only method that applies motion analysis
using optical flow with ego-motion compensation to predict Motor Focus-where
and how humans or machines focus their movement intentions. Meanwhile, this
paper addresses the camera shaking issue in handheld and body-mounted devices
which can severely degrade performance and accuracy, by applying a Gaussian
aggregation to stabilize the predicted motor focus area and enhance the
prediction accuracy of movement direction. This also provides a robust,
real-time solution that adapts to the user's immediate environment.
Furthermore, in the experiments part, we show the qualitative analysis of motor
focus estimation between the conventional dense optical flow-based method and
the proposed method. In quantitative tests, we show the performance of the
proposed method on a collected small dataset that is specialized for motor
focus estimation tasks.","['Hao Wang', 'Jiayou Qin', 'Xiwen Chen', 'Ashish Bastola', 'John Suchanek', 'Zihao Gong', 'Abolfazl Razi']",2024-04-25T20:45:39Z,http://arxiv.org/abs/2404.17031v1,['cs.CV']
XFeat: Accelerated Features for Lightweight Image Matching,"We introduce a lightweight and accurate architecture for resource-efficient
visual correspondence. Our method, dubbed XFeat (Accelerated Features),
revisits fundamental design choices in convolutional neural networks for
detecting, extracting, and matching local features. Our new model satisfies a
critical need for fast and robust algorithms suitable to resource-limited
devices. In particular, accurate image matching requires sufficiently large
image resolutions - for this reason, we keep the resolution as large as
possible while limiting the number of channels in the network. Besides, our
model is designed to offer the choice of matching at the sparse or semi-dense
levels, each of which may be more suitable for different downstream
applications, such as visual navigation and augmented reality. Our model is the
first to offer semi-dense matching efficiently, leveraging a novel match
refinement module that relies on coarse local descriptors. XFeat is versatile
and hardware-independent, surpassing current deep learning-based local features
in speed (up to 5x faster) with comparable or better accuracy, proven in pose
estimation and visual localization. We showcase it running in real-time on an
inexpensive laptop CPU without specialized hardware optimizations. Code and
weights are available at www.verlab.dcc.ufmg.br/descriptors/xfeat_cvpr24.","['Guilherme Potje', 'Felipe Cadar', 'Andre Araujo', 'Renato Martins', 'Erickson R. Nascimento']",2024-04-30T00:37:55Z,http://arxiv.org/abs/2404.19174v1,['cs.CV']
"Sports Analysis and VR Viewing System Based on Player Tracking and Pose
  Estimation with Multimodal and Multiview Sensors","Sports analysis and viewing play a pivotal role in the current sports domain,
offering significant value not only to coaches and athletes but also to fans
and the media. In recent years, the rapid development of virtual reality (VR)
and augmented reality (AR) technologies have introduced a new platform for
watching games. Visualization of sports competitions in VR/AR represents a
revolutionary technology, providing audiences with a novel immersive viewing
experience. However, there is still a lack of related research in this area. In
this work, we present for the first time a comprehensive system for sports
competition analysis and real-time visualization on VR/AR platforms. First, we
utilize multiview LiDARs and cameras to collect multimodal game data.
Subsequently, we propose a framework for multi-player tracking and pose
estimation based on a limited amount of supervised data, which extracts precise
player positions and movements from point clouds and images. Moreover, we
perform avatar modeling of players to obtain their 3D models. Ultimately, using
these 3D player data, we conduct competition analysis and real-time
visualization on VR/AR. Extensive quantitative experiments demonstrate the
accuracy and robustness of our multi-player tracking and pose estimation
framework. The visualization results showcase the immense potential of our
sports visualization system on the domain of watching games on VR/AR devices.
The multimodal competition dataset we collected and all related code will be
released soon.","['Wenxuan Guo', 'Zhiyu Pan', 'Ziheng Xi', 'Alapati Tuerxun', 'Jianjiang Feng', 'Jie Zhou']",2024-05-02T09:19:43Z,http://arxiv.org/abs/2405.01112v1,['cs.CV']
"OmniActions: Predicting Digital Actions in Response to Real-World
  Multimodal Sensory Inputs with LLMs","The progression to ""Pervasive Augmented Reality"" envisions easy access to
multimodal information continuously. However, in many everyday scenarios, users
are occupied physically, cognitively or socially. This may increase the
friction to act upon the multimodal information that users encounter in the
world. To reduce such friction, future interactive interfaces should
intelligently provide quick access to digital actions based on users' context.
To explore the range of possible digital actions, we conducted a diary study
that required participants to capture and share the media that they intended to
perform actions on (e.g., images or audio), along with their desired actions
and other contextual information. Using this data, we generated a holistic
design space of digital follow-up actions that could be performed in response
to different types of multimodal sensory inputs. We then designed OmniActions,
a pipeline powered by large language models (LLMs) that processes multimodal
sensory inputs and predicts follow-up actions on the target information
grounded in the derived design space. Using the empirical data collected in the
diary study, we performed quantitative evaluations on three variations of LLM
techniques (intent classification, in-context learning and finetuning) and
identified the most effective technique for our task. Additionally, as an
instantiation of the pipeline, we developed an interactive prototype and
reported preliminary user feedback about how people perceive and react to the
action predictions and its errors.","['Jiahao Nick Li', 'Yan Xu', 'Tovi Grossman', 'Stephanie Santosa', 'Michelle Li']",2024-05-06T23:11:00Z,http://arxiv.org/abs/2405.03901v1,"['cs.HC', 'cs.AI']"
3D Hand Mesh Recovery from Monocular RGB in Camera Space,"With the rapid advancement of technologies such as virtual reality, augmented
reality, and gesture control, users expect interactions with computer
interfaces to be more natural and intuitive. Existing visual algorithms often
struggle to accomplish advanced human-computer interaction tasks, necessitating
accurate and reliable absolute spatial prediction methods. Moreover, dealing
with complex scenes and occlusions in monocular images poses entirely new
challenges. This study proposes a network model that performs parallel
processing of root-relative grids and root recovery tasks. The model enables
the recovery of 3D hand meshes in camera space from monocular RGB images. To
facilitate end-to-end training, we utilize an implicit learning approach for 2D
heatmaps, enhancing the compatibility of 2D cues across different subtasks.
Incorporate the Inception concept into spectral graph convolutional network to
explore relative mesh of root, and integrate it with the locally detailed and
globally attentive method designed for root recovery exploration. This approach
improves the model's predictive performance in complex environments and
self-occluded scenes. Through evaluation on the large-scale hand dataset
FreiHAND, we have demonstrated that our proposed model is comparable with
state-of-the-art models. This study contributes to the advancement of
techniques for accurate and reliable absolute spatial prediction in various
human-computer interaction applications.","['Haonan Li', 'Patrick P. K. Chen', 'Yitong Zhou']",2024-05-12T05:36:37Z,http://arxiv.org/abs/2405.07167v1,['cs.CV']
Deep Learning-Based Object Pose Estimation: A Comprehensive Survey,"Object pose estimation is a fundamental computer vision problem with broad
applications in augmented reality and robotics. Over the past decade, deep
learning models, due to their superior accuracy and robustness, have
increasingly supplanted conventional algorithms reliant on engineered point
pair features. Nevertheless, several challenges persist in contemporary
methods, including their dependency on labeled training data, model
compactness, robustness under challenging conditions, and their ability to
generalize to novel unseen objects. A recent survey discussing the progress
made on different aspects of this area, outstanding challenges, and promising
future directions, is missing. To fill this gap, we discuss the recent advances
in deep learning-based object pose estimation, covering all three formulations
of the problem, \emph{i.e.}, instance-level, category-level, and unseen object
pose estimation. Our survey also covers multiple input data modalities,
degrees-of-freedom of output poses, object properties, and downstream tasks,
providing the readers with a holistic understanding of this field.
Additionally, it discusses training paradigms of different domains, inference
modes, application areas, evaluation metrics, and benchmark datasets, as well
as reports the performance of current state-of-the-art methods on these
benchmarks, thereby facilitating the readers in selecting the most suitable
method for their application. Finally, the survey identifies key challenges,
reviews the prevailing trends along with their pros and cons, and identifies
promising directions for future research. We also keep tracing the latest works
at https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation.","['Jian Liu', 'Wei Sun', 'Hui Yang', 'Zhiwen Zeng', 'Chongpei Liu', 'Jin Zheng', 'Xingyu Liu', 'Hossein Rahmani', 'Nicu Sebe', 'Ajmal Mian']",2024-05-13T14:44:22Z,http://arxiv.org/abs/2405.07801v3,['cs.CV']
"Improving the Real-Data Driven Network Evaluation Model for Digital Twin
  Networks","With the emergence and proliferation of new forms of large-scale services
such as smart homes, virtual reality/augmented reality, the increasingly
complex networks are raising concerns about significant operational costs. As a
result, the need for network management automation is emphasized, and Digital
Twin Networks (DTN) technology is expected to become the foundation technology
for autonomous networks. DTN has the advantage of being able to operate and
system networks based on real-time collected data in a closed-loop system, and
currently it is mainly designed for optimization scenarios. To improve network
performance in optimization scenarios, it is necessary to select appropriate
configurations and perform accurate performance evaluation based on real data.
However, most network evaluation models currently use simulation data.
Meanwhile, according to DTN standards documents, artificial intelligence (AI)
models can ensure scalability, real-time performance, and accuracy in
large-scale networks. Various AI research and standardization work is ongoing
to optimize the use of DTN. When designing AI models, it is crucial to consider
the characteristics of the data. This paper presents an autoencoder-based skip
connected message passing neural network (AE-SMPN) as a network evaluation
model using real network data. The model is created by utilizing graph neural
network (GNN) with recurrent neural network (RNN) models to capture the
spatiotemporal features of network data. Additionally, an AutoEncoder (AE) is
employed to extract initial features. The neural network was trained using the
real DTN dataset provided by the Barcelona Neural Networking Center (BNN-UPC),
and the paper presents the analysis of the model structure along with
experimental results.","['Hyeju Shin', 'Ibrahim Aliyu', 'Abubakar Isah', 'Jinsul Kim']",2024-05-14T09:55:03Z,http://arxiv.org/abs/2405.08473v1,['cs.LG']
VICAN: Very Efficient Calibration Algorithm for Large Camera Networks,"The precise estimation of camera poses within large camera networks is a
foundational problem in computer vision and robotics, with broad applications
spanning autonomous navigation, surveillance, and augmented reality. In this
paper, we introduce a novel methodology that extends state-of-the-art Pose
Graph Optimization (PGO) techniques. Departing from the conventional PGO
paradigm, which primarily relies on camera-camera edges, our approach centers
on the introduction of a dynamic element - any rigid object free to move in the
scene - whose pose can be reliably inferred from a single image. Specifically,
we consider the bipartite graph encompassing cameras, object poses evolving
dynamically, and camera-object relative transformations at each time step. This
shift not only offers a solution to the challenges encountered in directly
estimating relative poses between cameras, particularly in adverse
environments, but also leverages the inclusion of numerous object poses to
ameliorate and integrate errors, resulting in accurate camera pose estimates.
Though our framework retains compatibility with traditional PGO solvers, its
efficacy benefits from a custom-tailored optimization scheme. To this end, we
introduce an iterative primal-dual algorithm, capable of handling large graphs.
Empirical benchmarks, conducted on a new dataset of simulated indoor
environments, substantiate the efficacy and efficiency of our approach.","['Gabriel Moreira', 'Manuel Marques', 'João Paulo Costeira', 'Alexander Hauptmann']",2024-03-25T17:47:03Z,http://arxiv.org/abs/2405.10952v1,"['cs.CV', 'cs.RO']"
"AUGlasses: Continuous Action Unit based Facial Reconstruction with
  Low-power IMUs on Smart Glasses","Recent advancements in augmented reality (AR) have enabled the use of various
sensors on smart glasses for applications like facial reconstruction, which is
vital to improve AR experiences for virtual social activities. However, the
size and power constraints of smart glasses demand a miniature and low-power
sensing solution. AUGlasses achieves unobtrusive low-power facial
reconstruction by placing inertial measurement units (IMU) against the temporal
area on the face to capture the skin deformations, which are caused by facial
muscle movements. These IMU signals, along with historical data on facial
action units (AUs), are processed by a transformer-based deep learning model to
estimate AU intensities in real-time, which are then used for facial
reconstruction. Our results show that AUGlasses accurately predicts the
strength (0-5 scale) of 14 key AUs with a cross-user mean absolute error (MAE)
of 0.187 (STD = 0.025) and achieves facial reconstruction with a cross-user MAE
of 1.93 mm (STD = 0.353). We also integrated various preprocessing and training
techniques to ensure robust performance for continuous sensing. Micro-benchmark
tests indicate that our system consistently performs accurate continuous facial
reconstruction with a fine-tuned cross-user model, achieving an AU MAE of 0.35.","['Yanrong Li', 'Tengxiang Zhang', 'Xin Zeng', 'Yuntao Wang', 'Haotian Zhang', 'Yiqiang Chen']",2024-05-22T02:07:56Z,http://arxiv.org/abs/2405.13289v1,"['cs.HC', 'cs.CV']"
"Metabook: An Automatically Generated Augmented Reality Storybook
  Interaction System to Improve Children's Engagement in Storytelling","Storytelling serves as a crucial avenue for children to acquire knowledge,
offering numerous benefits such as enhancing children's sensitivity to various
forms of syntax, diction, and rhetoric; recognizing patterns in language and
human experience; stimulating creativity; and providing practice in
problem-solving, decision-making, and evaluation. However, current storytelling
book facing these problems:1.Traditional 3D storybooks lack flexibility in
dealing with text changing, as adding a new story requires remaking of the 3D
book by artists. 2. Children often have many questions after reading stories,
but traditional 3D books are unable to provide answers or explanations for
children.3.Children can easily feel bored when reading text, and traditional 3D
books still rely on text to tell stories, thus limiting their ability to
increase children's enthusiasm for reading. So, we propose the Metabook: an
automatically generated interactive 3D storybook. Our main contributions are as
follows: First, we propose a story to 3D generation scheme, enabling 3D books
to be automatically generated based on stories. Next, we introduce cartoon
Metahumans for storytelling, utilizing lip-syncing and eye-tracking technology
to enable facial interaction with children, enhancing the fun of reading. Last
but not least, we connect GPT-4 to the brain of the metahuman, which provides
answers and explanations to the questions children have after reading.","['Yibo Wang', 'Yuanyuan Mao', 'Shi-ting Ni']",2024-05-22T14:46:09Z,http://arxiv.org/abs/2405.13701v1,['cs.HC']
"CudaSIFT-SLAM: multiple-map visual SLAM for full procedure mapping in
  real human endoscopy","Monocular visual simultaneous localization and mapping (V-SLAM) is nowadays
an irreplaceable tool in mobile robotics and augmented reality, where it
performs robustly. However, human colonoscopies pose formidable challenges like
occlusions, blur, light changes, lack of texture, deformation, water jets or
tool interaction, which result in very frequent tracking losses. ORB-SLAM3, the
top performing multiple-map V-SLAM, is unable to recover from them by merging
sub-maps or relocalizing the camera, due to the poor performance of its place
recognition algorithm based on ORB features and DBoW2 bag-of-words.
  We present CudaSIFT-SLAM, the first V-SLAM system able to process complete
human colonoscopies in real-time. To overcome the limitations of ORB-SLAM3, we
use SIFT instead of ORB features and replace the DBoW2 direct index with the
more computationally demanding brute-force matching, being able to successfully
match images separated in time for relocation and map merging. Real-time
performance is achieved thanks to CudaSIFT, a GPU implementation for SIFT
extraction and brute-force matching.
  We benchmark our system in the C3VD phantom colon dataset, and in a full real
colonoscopy from the Endomapper dataset, demonstrating the capabilities to
merge sub-maps and relocate in them, obtaining significantly longer sub-maps.
Our system successfully maps in real-time 88 % of the frames in the C3VD
dataset. In a real screening colonoscopy, despite the much higher prevalence of
occluded and blurred frames, the mapping coverage is 53 % in carefully explored
areas and 38 % in the full sequence, a 70 % improvement over ORB-SLAM3.","['Richard Elvira', 'Juan D. Tardós', 'José M. M. Montiel']",2024-05-27T08:26:19Z,http://arxiv.org/abs/2405.16932v1,"['cs.RO', 'cs.CV', 'I.4.9']"
"GOI: Find 3D Gaussians of Interest with an Optimizable Open-vocabulary
  Semantic-space Hyperplane","3D open-vocabulary scene understanding, crucial for advancing augmented
reality and robotic applications, involves interpreting and locating specific
regions within a 3D space as directed by natural language instructions. To this
end, we introduce GOI, a framework that integrates semantic features from 2D
vision-language foundation models into 3D Gaussian Splatting (3DGS) and
identifies 3D Gaussians of Interest using an Optimizable Semantic-space
Hyperplane. Our approach includes an efficient compression method that utilizes
scene priors to condense noisy high-dimensional semantic features into compact
low-dimensional vectors, which are subsequently embedded in 3DGS. During the
open-vocabulary querying process, we adopt a distinct approach compared to
existing methods, which depend on a manually set fixed empirical threshold to
select regions based on their semantic feature distance to the query text
embedding. This traditional approach often lacks universal accuracy, leading to
challenges in precisely identifying specific target areas. Instead, our method
treats the feature selection process as a hyperplane division within the
feature space, retaining only those features that are highly relevant to the
query. We leverage off-the-shelf 2D Referring Expression Segmentation (RES)
models to fine-tune the semantic-space hyperplane, enabling a more precise
distinction between target regions and others. This fine-tuning substantially
improves the accuracy of open-vocabulary queries, ensuring the precise
localization of pertinent 3D Gaussians. Extensive experiments demonstrate GOI's
superiority over previous state-of-the-art methods. Our project page is
available at https://goi-hyperplane.github.io/ .","['Yansong Qu', 'Shaohui Dai', 'Xinyang Li', 'Jianghang Lin', 'Liujuan Cao', 'Shengchuan Zhang', 'Rongrong Ji']",2024-05-27T18:57:18Z,http://arxiv.org/abs/2405.17596v1,['cs.CV']
"Augmented Conversation with Embedded Speech-Driven On-the-Fly
  Referencing in AR","This paper introduces the concept of augmented conversation, which aims to
support co-located in-person conversations via embedded speech-driven
on-the-fly referencing in augmented reality (AR). Today computing technologies
like smartphones allow quick access to a variety of references during the
conversation. However, these tools often create distractions, reducing eye
contact and forcing users to focus their attention on phone screens and
manually enter keywords to access relevant information. In contrast, AR-based
on-the-fly referencing provides relevant visual references in real-time, based
on keywords extracted automatically from the spoken conversation. By embedding
these visual references in AR around the conversation partner, augmented
conversation reduces distraction and friction, allowing users to maintain eye
contact and supporting more natural social interactions. To demonstrate this
concept, we developed \system, a Hololens-based interface that leverages
real-time speech recognition, natural language processing and gaze-based
interactions for on-the-fly embedded visual referencing. In this paper, we
explore the design space of visual referencing for conversations, and describe
our our implementation -- building on seven design guidelines identified
through a user-centered design process. An initial user study confirms that our
system decreases distraction and friction in conversations compared to
smartphone searches, while providing highly useful and relevant information.","['Shivesh Jadon', 'Mehrad Faridan', 'Edward Mah', 'Rajan Vaish', 'Wesley Willett', 'Ryo Suzuki']",2024-05-28T19:10:47Z,http://arxiv.org/abs/2405.18537v1,"['cs.HC', 'cs.LG']"
"HoloDevice: Holographic Cross-Device Interactions for Remote
  Collaboration","This paper introduces holographic cross-device interaction, a new class of
remote cross-device interactions between local physical devices and
holographically rendered remote devices. Cross-device interactions have enabled
a rich set of interactions with device ecologies. Most existing research
focuses on co-located settings (meaning when users and devices are in the same
physical space) to achieve these rich interactions and affordances. In
contrast, holographic cross-device interaction allows remote interactions
between devices at distant locations by providing a rich visual affordance
through real-time holographic rendering of the device's motion, content, and
interactions on mixed reality head-mounted displays. This maintains the
advantages of having a physical device, such as precise input through touch and
pen interaction. Through holographic rendering, not only can remote devices
interact as if they are co-located, but they can also be virtually augmented to
further enrich interactions, going beyond what is possible with existing
cross-device systems. To demonstrate this concept, we developed HoloDevice, a
prototype system for holographic cross-device interaction using the Microsoft
Hololens 2 augmented reality headset. Our contribution is threefold. First, we
introduce the concept of holographic cross-device interaction. Second, we
present a design space containing three unique benefits, which include: (1)
spatial visualization of interaction and motion, (2) rich visual affordances
for intermediate transition, and (3) dynamic and fluid configuration. Last we
discuss a set of implementation demonstrations and use-case scenarios that
further explore the space.","['Neil Chulpongsatorn', 'Thien-Kim Nguyen', 'Nicolai Marquardt', 'Ryo Suzuki']",2024-05-28T22:49:01Z,http://arxiv.org/abs/2405.19377v1,['cs.HC']
Massive MIMO: Ten Myths and One Critical Question,"Wireless communications is one of the most successful technologies in modern
years, given that an exponential growth rate in wireless traffic has been
sustained for over a century (known as Cooper's law). This trend will certainly
continue driven by new innovative applications; for example, augmented reality
and internet-of-things.
  Massive MIMO (multiple-input multiple-output) has been identified as a key
technology to handle orders of magnitude more data traffic. Despite the
attention it is receiving from the communication community, we have personally
witnessed that Massive MIMO is subject to several widespread misunderstandings,
as epitomized by following (fictional) abstract:
  ""The Massive MIMO technology uses a nearly infinite number of high-quality
antennas at the base stations. By having at least an order of magnitude more
antennas than active terminals, one can exploit asymptotic behaviors that some
special kinds of wireless channels have. This technology looks great at first
sight, but unfortunately the signal processing complexity is off the charts and
the antenna arrays would be so huge that it can only be implemented in
millimeter wave bands.""
  The statements above are, in fact, completely false. In this overview
article, we identify ten myths and explain why they are not true. We also ask a
question that is critical for the practical adoption of the technology and
which will require intense future research activities to answer properly. We
provide references to key technical papers that support our claims, while a
further list of related overview and technical papers can be found at the
Massive MIMO Info Point: http://massivemimo.eu","['Emil Björnson', 'Erik G. Larsson', 'Thomas L. Marzetta']",2015-03-23T21:57:46Z,http://arxiv.org/abs/1503.06854v2,"['cs.IT', 'math.IT']"
"Assessing 3D scan quality in Virtual Reality through paired-comparisons
  psychophysics test","Consumer 3D scanners and depth cameras are increasingly being used to
generate content and avatars for Virtual Reality (VR) environments and avoid
the inconveniences of hand modeling; however, it is sometimes difficult to
evaluate quantitatively the mesh quality at which 3D scans should be exported,
and whether the object perception might be affected by its shading. We propose
using a paired-comparisons test based on psychophysics of perception to do that
evaluation. As psychophysics is not subject to opinion, skill level, mental
state, or economic situation it can be considered a quantitative way to measure
how people perceive the mesh quality. In particular, we propose using the
psychophysical measure for the comparison of four different levels of mesh
quality (1K, 5K, 10K and 20K triangles). We present two studies within
subjects: in one we investigate the quality perception variations of seeing an
object in a regular screen monitor against an stereoscopic Head Mounted Display
(HMD); while in the second experiment we aim at detecting the effects of
shading into quality perception. At each iteration of the pair-test comparisons
participants pick the mesh that they think had higher quality; by the end of
the experiment we compile a preference matrix. The matrix evidences the
correlation between real quality and assessed quality. Regarding the shading
mode, we find an interaction with quality and shading when the model has high
definition. Furthermore, we assess the subjective realism of the most/least
preferred scans using an Immersive Augmented Reality (IAR) video-see-through
setup. Results show higher levels of realism were perceived through the HMD
than when using a monitor, although the quality was similarly perceived in both
systems.","['Jacob Thorn', 'Rodrigo Pizarro', 'Bernhard Spanlang', 'Pablo Bermell-Garcia', 'Mar Gonzalez-Franco']",2016-01-31T12:43:34Z,http://arxiv.org/abs/1602.00238v2,"['cs.HC', 'cs.MM']"
"Augmented Reality Meets Computer Vision : Efficient Data Generation for
  Urban Driving Scenes","The success of deep learning in computer vision is based on availability of
large annotated datasets. To lower the need for hand labeled images, virtually
rendered 3D worlds have recently gained popularity. Creating realistic 3D
content is challenging on its own and requires significant human effort. In
this work, we propose an alternative paradigm which combines real and synthetic
data for learning semantic instance segmentation and object detection models.
Exploiting the fact that not all aspects of the scene are equally important for
this task, we propose to augment real-world imagery with virtual objects of the
target category. Capturing real-world images at large scale is easy and cheap,
and directly provides real background appearances without the need for creating
complex 3D models of the environment. We present an efficient procedure to
augment real images with virtual objects. This allows us to create realistic
composite images which exhibit both realistic background appearance and a large
number of complex object arrangements. In contrast to modeling complete 3D
environments, our augmentation approach requires only a few user interactions
in combination with 3D shapes of the target object. Through extensive
experimentation, we conclude the right set of parameters to produce augmented
data which can maximally enhance the performance of instance segmentation
models. Further, we demonstrate the utility of our approach on training
standard deep models for semantic instance segmentation and object detection of
cars in outdoor driving scenes. We test the models trained on our augmented
data on the KITTI 2015 dataset, which we have annotated with pixel-accurate
ground truth, and on Cityscapes dataset. Our experiments demonstrate that
models trained on augmented imagery generalize better than those trained on
synthetic data or models trained on limited amount of annotated real data.","['Hassan Abu Alhaija', 'Siva Karthik Mustikovela', 'Lars Mescheder', 'Andreas Geiger', 'Carsten Rother']",2017-08-04T16:03:52Z,http://arxiv.org/abs/1708.01566v1,['cs.CV']
"LookinGood: Enhancing Performance Capture with Real-time Neural
  Re-Rendering","Motivated by augmented and virtual reality applications such as telepresence,
there has been a recent focus in real-time performance capture of humans under
motion. However, given the real-time constraint, these systems often suffer
from artifacts in geometry and texture such as holes and noise in the final
rendering, poor lighting, and low-resolution textures. We take the novel
approach to augment such real-time performance capture systems with a deep
architecture that takes a rendering from an arbitrary viewpoint, and jointly
performs completion, super resolution, and denoising of the imagery in
real-time. We call this approach neural (re-)rendering, and our live system
""LookinGood"". Our deep architecture is trained to produce high resolution and
high quality images from a coarse rendering in real-time. First, we propose a
self-supervised training method that does not require manual ground-truth
annotation. We contribute a specialized reconstruction error that uses semantic
information to focus on relevant parts of the subject, e.g. the face. We also
introduce a salient reweighing scheme of the loss function that is able to
discard outliers. We specifically design the system for virtual and augmented
reality headsets where the consistency between the left and right eye plays a
crucial role in the final user experience. Finally, we generate temporally
stable results by explicitly minimizing the difference between two consecutive
frames. We tested the proposed system in two different scenarios: one involving
a single RGB-D sensor, and upper body reconstruction of an actor, the second
consisting of full body 360 degree capture. Through extensive experimentation,
we demonstrate how our system generalizes across unseen sequences and subjects.
The supplementary video is available at http://youtu.be/Md3tdAKoLGU.","['Ricardo Martin-Brualla', 'Rohit Pandey', 'Shuoran Yang', 'Pavel Pidlypenskyi', 'Jonathan Taylor', 'Julien Valentin', 'Sameh Khamis', 'Philip Davidson', 'Anastasia Tkach', 'Peter Lincoln', 'Adarsh Kowdle', 'Christoph Rhemann', 'Dan B Goldman', 'Cem Keskin', 'Steve Seitz', 'Shahram Izadi', 'Sean Fanello']",2018-11-12T22:51:19Z,http://arxiv.org/abs/1811.05029v1,['cs.CV']
IGNOR: Image-guided Neural Object Rendering,"We propose a learned image-guided rendering technique that combines the
benefits of image-based rendering and GAN-based image synthesis. The goal of
our method is to generate photo-realistic re-renderings of reconstructed
objects for virtual and augmented reality applications (e.g., virtual
showrooms, virtual tours \& sightseeing, the digital inspection of historical
artifacts). A core component of our work is the handling of view-dependent
effects. Specifically, we directly train an object-specific deep neural network
to synthesize the view-dependent appearance of an object. As input data we are
using an RGB video of the object. This video is used to reconstruct a proxy
geometry of the object via multi-view stereo. Based on this 3D proxy, the
appearance of a captured view can be warped into a new target view as in
classical image-based rendering. This warping assumes diffuse surfaces, in case
of view-dependent effects, such as specular highlights, it leads to artifacts.
To this end, we propose EffectsNet, a deep neural network that predicts
view-dependent effects. Based on these estimations, we are able to convert
observed images to diffuse images. These diffuse images can be projected into
other views. In the target view, our pipeline reinserts the new view-dependent
effects. To composite multiple reprojected images to a final output, we learn a
composition network that outputs photo-realistic results. Using this
image-guided approach, the network does not have to allocate capacity on
``remembering'' object appearance, instead it learns how to combine the
appearance of captured images. We demonstrate the effectiveness of our approach
both qualitatively and quantitatively on synthetic as well as on real data.","['Justus Thies', 'Michael Zollhöfer', 'Christian Theobalt', 'Marc Stamminger', 'Matthias Nießner']",2018-11-26T22:24:25Z,http://arxiv.org/abs/1811.10720v2,['cs.CV']
"XR: Enabling training mode in the human brain XR: Enabling training mode
  in the human brain","The face of simulation-based training has greatly evolved, with the most
recent tools giving the ability to create virtual environments that rival
realism. At first glance, it might appear that what the training sector needs
is the most realistic simulators possible, but traditional simulators are not
necessarily the most efficient or practical training tools. With all that these
new technologies have to offer; the challenge is to go back to the core of
training needs and identify the right vector of sensory cues that will most
effectively enable training mode in the human brain. Bigger and Pricier doesn't
necessarily mean better. Simulation with cross-reality content (XR), which by
definition encompasses virtual reality (VR), mixed reality (MR), and augmented
reality (AR), is the most practical solution for deploying any kind of
simulation-based training. The authors of this paper (a teacher and a
technology expert) share their experiences and expose XR-specific best
practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :
Starting his career in the modeling and simulation community more than 15 years
ago, S{\'e}bastien has focused on learning about the latest simulation
innovations and sharing information on how experts have solved their
challenges. He worked on the COTS integration at CAE and the Presagis focusing
on Simulation and Visualization products. More recently, Sebastien put together
simulation and training teams and strategies for emerging companies like CM
Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games,
focusing on helping companies develop real-time solutions for simulation-based
training. Philippe Lepinard: Former military helicopter pilot and simulation
officer, Philippe L{\'e}pinard is now an associate professor at the University
of Paris-Est Cr{\'e}teil (UPEC). His research is focusing on playful learning
and training through simulation. He is one of the founding members of the
French simulation association.","['Philippe Lépinard', 'Sébastien Lozé']",2019-04-26T07:55:39Z,http://arxiv.org/abs/1904.11704v1,"['cs.GR', 'cs.HC']"
RGB-D Odometry and SLAM,"The emergence of modern RGB-D sensors had a significant impact in many
application fields, including robotics, augmented reality (AR) and 3D scanning.
They are low-cost, low-power and low-size alternatives to traditional range
sensors such as LiDAR. Moreover, unlike RGB cameras, RGB-D sensors provide the
additional depth information that removes the need of frame-by-frame
triangulation for 3D scene reconstruction. These merits have made them very
popular in mobile robotics and AR, where it is of great interest to estimate
ego-motion and 3D scene structure. Such spatial understanding can enable robots
to navigate autonomously without collisions and allow users to insert virtual
entities consistent with the image stream. In this chapter, we review common
formulations of odometry and Simultaneous Localization and Mapping (known by
its acronym SLAM) using RGB-D stream input. The two topics are closely related,
as the former aims to track the incremental camera motion with respect to a
local map of the scene, and the latter to jointly estimate the camera
trajectory and the global map with consistency. In both cases, the standard
approaches minimize a cost function using nonlinear optimization techniques.
This chapter consists of three main parts: In the first part, we introduce the
basic concept of odometry and SLAM and motivate the use of RGB-D sensors. We
also give mathematical preliminaries relevant to most odometry and SLAM
algorithms. In the second part, we detail the three main components of SLAM
systems: camera pose tracking, scene mapping and loop closing. For each
component, we describe different approaches proposed in the literature. In the
final part, we provide a brief discussion on advanced research topics with the
references to the state-of-the-art.","['Javier Civera', 'Seong Hun Lee']",2020-01-19T17:56:11Z,http://arxiv.org/abs/2001.06875v1,"['cs.CV', 'cs.RO']"
"A Review on Object Pose Recovery: from 3D Bounding Box Detectors to Full
  6D Pose Estimators","Object pose recovery has gained increasing attention in the computer vision
field as it has become an important problem in rapidly evolving technological
areas related to autonomous driving, robotics, and augmented reality. Existing
review-related studies have addressed the problem at visual level in 2D, going
through the methods which produce 2D bounding boxes of objects of interest in
RGB images. The 2D search space is enlarged either using the geometry
information available in the 3D space along with RGB (Mono/Stereo) images, or
utilizing depth data from LIDAR sensors and/or RGB-D cameras. 3D bounding box
detectors, producing category-level amodal 3D bounding boxes, are evaluated on
gravity aligned images, while full 6D object pose estimators are mostly tested
at instance-level on the images where the alignment constraint is removed.
Recently, 6D object pose estimation is tackled at the level of categories. In
this paper, we present the first comprehensive and most recent review of the
methods on object pose recovery, from 3D bounding box detectors to full 6D pose
estimators. The methods mathematically model the problem as a classification,
regression, classification & regression, template matching, and point-pair
feature matching task. Based on this, a mathematical-model-based categorization
of the methods is established. Datasets used for evaluating the methods are
investigated with respect to the challenges, and evaluation metrics are
studied. Quantitative results of experiments in the literature are analyzed to
show which category of methods best performs across what types of challenges.
The analyses are further extended comparing two methods, which are our own
implementations, so that the outcomes from the public results are further
solidified. Current position of the field is summarized regarding object pose
recovery, and possible research directions are identified.","['Caner Sahin', 'Guillermo Garcia-Hernando', 'Juil Sock', 'Tae-Kyun Kim']",2020-01-28T22:05:09Z,http://arxiv.org/abs/2001.10609v2,['cs.CV']
"Coding local and global binary visual features extracted from video
  sequences","Binary local features represent an effective alternative to real-valued
descriptors, leading to comparable results for many visual analysis tasks,
while being characterized by significantly lower computational complexity and
memory requirements. When dealing with large collections, a more compact
representation based on global features is often preferred, which can be
obtained from local features by means of, e.g., the Bag-of-Visual-Word (BoVW)
model. Several applications, including for example visual sensor networks and
mobile augmented reality, require visual features to be transmitted over a
bandwidth-limited network, thus calling for coding techniques that aim at
reducing the required bit budget, while attaining a target level of efficiency.
In this paper we investigate a coding scheme tailored to both local and global
binary features, which aims at exploiting both spatial and temporal redundancy
by means of intra- and inter-frame coding. In this respect, the proposed coding
scheme can be conveniently adopted to support the Analyze-Then-Compress (ATC)
paradigm. That is, visual features are extracted from the acquired content,
encoded at remote nodes, and finally transmitted to a central controller that
performs visual analysis. This is in contrast with the traditional approach, in
which visual content is acquired at a node, compressed and then sent to a
central unit for further processing, according to the Compress-Then-Analyze
(CTA) paradigm. In this paper we experimentally compare ATC and CTA by means of
rate-efficiency curves in the context of two different visual analysis tasks:
homography estimation and content-based retrieval. Our results show that the
novel ATC paradigm based on the proposed coding primitives can be competitive
with CTA, especially in bandwidth limited scenarios.","['Luca Baroffio', 'Antonio Canclini', 'Matteo Cesana', 'Alessandro Redondi', 'Marco Tagliasacchi', 'Stefano Tubaro']",2015-02-26T14:23:39Z,http://arxiv.org/abs/1502.07939v1,"['cs.MM', 'cs.CV']"
"The Effect of Pokémon Go on The Pulse of the City: A Natural
  Experiment","Pok\'emon Go, a location-based game that uses augmented reality techniques,
received unprecedented media coverage due to claims that it allowed for greater
access to public spaces, increasing the number of people out on the streets,
and generally improving health, social, and security indices. However, the true
impact of Pok\'emon Go on people's mobility patterns in a city is still largely
unknown. In this paper, we perform a natural experiment using data from mobile
phone networks to evaluate the effect of Pok\'emon Go on the pulse of a big
city: Santiago, capital of Chile. We found significant effects of the game on
the floating population of Santiago compared to movement prior to the game's
release in August 2016: in the following week, up to 13.8\% more people spent
time outside at certain times of the day, even if they do not seem to go out of
their usual way. These effects were found by performing regressions using count
models over the states of the cellphone network during each day under study.
The models used controlled for land use, daily patterns, and points of interest
in the city.
  Our results indicate that, on business days, there are more people on the
street at commuting times, meaning that people did not change their daily
routines but slightly adapted them to play the game. Conversely, on Saturday
and Sunday night, people indeed went out to play, but favored places close to
where they live.
  Even if the statistical effects of the game do not reflect the massive change
in mobility behavior portrayed by the media, at least in terms of expanse, they
do show how ""the street"" may become a new place of leisure. This change should
have an impact on long-term infrastructure investment by city officials, and on
the drafting of public policies aimed at stimulating pedestrian traffic.","['Eduardo Graells-Garrido', 'Leo Ferres', 'Diego Caro', 'Loreto Bravo']",2016-10-25T21:13:55Z,http://arxiv.org/abs/1610.08098v2,['cs.SI']
"In-home and remote use of robotic body surrogates by people with
  profound motor deficits","By controlling robots comparable to the human body, people with profound
motor deficits could potentially perform a variety of physical tasks for
themselves, improving their quality of life. The extent to which this is
achievable has been unclear due to the lack of suitable interfaces by which to
control robotic body surrogates and a dearth of studies involving substantial
numbers of people with profound motor deficits. We developed a novel, web-based
augmented reality interface that enables people with profound motor deficits to
remotely control a PR2 mobile manipulator from Willow Garage, which is a
human-scale, wheeled robot with two arms. We then conducted two studies to
investigate the use of robotic body surrogates. In the first study, 15 novice
users with profound motor deficits from across the United States controlled a
PR2 in Atlanta, GA to perform a modified Action Research Arm Test (ARAT) and a
simulated self-care task. Participants achieved clinically meaningful
improvements on the ARAT and 12 of 15 participants (80%) successfully completed
the simulated self-care task. Participants agreed that the robotic system was
easy to use, was useful, and would provide a meaningful improvement in their
lives. In the second study, one expert user with profound motor deficits had
free use of a PR2 in his home for seven days. He performed a variety of
self-care and household tasks, and also used the robot in novel ways. Taking
both studies together, our results suggest that people with profound motor
deficits can improve their quality of life using robotic body surrogates, and
that they can gain benefit with only low-level robot autonomy and without
invasive interfaces. However, methods to reduce the rate of errors and increase
operational speed merit further investigation.","['Phillip M. Grice', 'Charles C. Kemp']",2018-03-05T03:02:27Z,http://arxiv.org/abs/1803.01477v2,"['cs.RO', 'cs.HC']"
"Computer Simulation of Neural Networks Using Spreadsheets: The Dawn of
  the Age of Camelot","The article substantiates the necessity to develop training methods of
computer simulation of neural networks in the spreadsheet environment. The
systematic review of their application to simulating artificial neural networks
is performed. The authors distinguish basic approaches to solving the problem
of network computer simulation training in the spreadsheet environment, joint
application of spreadsheets and tools of neural network simulation, application
of third-party add-ins to spreadsheets, development of macros using the
embedded languages of spreadsheets; use of standard spreadsheet add-ins for
non-linear optimization, creation of neural networks in the spreadsheet
environment without add-ins and macros. After analyzing a collection of
writings of 1890-1950, the research determines the role of the scientific
journal ""Bulletin of Mathematical Biophysics"", its founder Nicolas Rashevsky
and the scientific community around the journal in creating and developing
models and methods of computational neuroscience. There are identified
psychophysical basics of creating neural networks, mathematical foundations of
neural computing and methods of neuroengineering (image recognition, in
particular). The role of Walter Pitts in combining the descriptive and
quantitative theories of training is discussed. It is shown that to acquire
neural simulation competences in the spreadsheet environment, one should master
the models based on the historical and genetic approach. It is indicated that
there are three groups of models, which are promising in terms of developing
corresponding methods - the continuous two-factor model of Rashevsky, the
discrete model of McCulloch and Pitts, and the discrete-continuous models of
Householder and Landahl.","['Serhiy O. Semerikov', 'Illia O. Teplytskyi', 'Yuliia V. Yechkalo', 'Arnold E. Kiv']",2018-06-29T18:06:15Z,http://arxiv.org/abs/1807.00018v2,"['cs.CY', '68T99', 'K.3.1; I.2.6; K.2']"
"A Survey of Multi-Access Edge Computing in 5G and Beyond: Fundamentals,
  Technology Integration, and State-of-the-Art","Driven by the emergence of new compute-intensive applications and the vision
of the Internet of Things (IoT), it is foreseen that the emerging 5G network
will face an unprecedented increase in traffic volume and computation demands.
However, end users mostly have limited storage capacities and finite processing
capabilities, thus how to run compute-intensive applications on
resource-constrained users has recently become a natural concern. Mobile edge
computing (MEC), a key technology in the emerging fifth generation (5G)
network, can optimize mobile resources by hosting compute-intensive
applications, process large data before sending to the cloud, provide the cloud
computing capabilities within the radio access network (RAN) in close proximity
to mobile users, and offer context-aware services with the help of RAN
information. Therefore, MEC enables a wide variety of applications, where the
real-time response is strictly required, e.g., driverless vehicles, augmented
reality, robotics, and immerse media. Indeed, the paradigm shift from 4G to 5G
could become a reality with the advent of new technological concepts. The
successful realization of MEC in the 5G network is still in its infancy and
demands for constant efforts from both academic and industry communities. In
this survey, we first provide a holistic overview of MEC technology and its
potential use cases and applications. Then, we outline up-to-date researches on
the integration of MEC with the new technologies that will be deployed in 5G
and beyond. We also summarize testbeds and experimental evaluations, and open
source activities, for edge computing. We further summarize lessons learned
from state-of-the-art research works as well as discuss challenges and
potential future directions for MEC research.","['Quoc-Viet Pham', 'Fang Fang', 'Vu Nguyen Ha', 'Md. Jalil Piran', 'Mai Le', 'Long Bao Le', 'Won-Joo Hwang', 'Zhiguo Ding']",2019-06-20T05:44:40Z,http://arxiv.org/abs/1906.08452v2,"['cs.NI', 'cs.DC', 'cs.IT', 'math.IT']"
"A Comparative Study of High-Recall Real-Time Semantic Segmentation Based
  on Swift Factorized Network","Semantic Segmentation (SS) is the task to assign a semantic label to each
pixel of the observed images, which is of crucial significance for autonomous
vehicles, navigation assistance systems for the visually impaired, and
augmented reality devices. However, there is still a long way for SS to be put
into practice as there are two essential challenges that need to be addressed:
efficiency and evaluation criterions for practical application. For specific
application scenarios, different criterions need to be adopted. Recall rate is
an important criterion for many tasks like autonomous vehicles. For autonomous
vehicles, we need to focus on the detection of the traffic objects like cars,
buses, and pedestrians, which should be detected with high recall rates. In
other words, it is preferable to detect it wrongly than miss it, because the
other traffic objects will be dangerous if the algorithm miss them and segment
them as safe roadways. In this paper, our main goal is to explore possible
methods to attain high recall rate. Firstly, we propose a real-time SS network
named Swift Factorized Network (SFN). The proposed network is adapted from
SwiftNet, whose structure is a typical U-shape structure with lateral
connections. Inspired by ERFNet and Global convolution Networks (GCNet), we
propose two different blocks to enlarge valid receptive field. They do not take
up too much calculation resources, but significantly enhance the performance
compared with the baseline network. Secondly, we explore three ways to achieve
higher recall rate, i.e. loss function, classifier and decision rules. We
perform a comprehensive set of experiments on state-of-the-art datasets
including CamVid and Cityscapes. We demonstrate that our SS convolutional
neural networks reach excellent performance. Furthermore, we make a detailed
analysis and comparison of the three proposed methods on the promotion of
recall rate.","['Kaite Xiang', 'Kaiwei Wang', 'Kailun Yang']",2019-07-26T06:36:18Z,http://arxiv.org/abs/1907.11394v1,['cs.CV']
"Toward Better Understanding of Saliency Prediction in Augmented 360
  Degree Videos","Augmented reality (AR) overlays digital content onto the reality. In AR
system, correct and precise estimations of user's visual fixations and head
movements can enhance the quality of experience by allocating more computation
resources on the areas of interest. However, there is inadequate research about
understanding the visual exploration of users when using an AR system or
modeling AR visual attention. To bridge the gap between the saliency prediction
on real-world scene and on scene augmented by virtual information, we construct
the ARVR saliency dataset with 12 diverse videos viewed by 20 people. The
virtual reality (VR) technique is employed to simulate the real-world.
Annotations of object recognition and tracking as augmented contents are
blended into the omnidirectional videos. The saliency annotations of head and
eye movements for both original and augmented videos are collected and together
constitute the ARVR dataset. We also design a model which is capable of solving
the saliency prediction problem in AR. Local block images are extracted to
simulate the viewport and offset the projection distortion. Conspicuous visual
cues in local viewports are extracted to constitute the spatial features. The
optical flow information is estimated as the important temporal feature. We
also consider the interplay between virtual information and reality. The
composition of the augmentation information is distinguished, and the joint
effects of adversarial augmentation and complementary augmentation are
estimated. We generate a graph by taking each block image as one node. Both the
visual saliency mechanism and the characteristics of viewing behaviors are
considered in the computation of edge weights on the graph which are
interpreted as Markov chains. The fraction of the visual attention that is
diverted to each block image is estimated through equilibrium distribution on
of this chain.","['Yucheng Zhu', 'Xiongkuo Min', 'DanDan Zhu', 'Ke Gu', 'Jiantao Zhou', 'Guangtao Zhai', 'Xiaokang Yang', 'Wenjun Zhang']",2019-12-12T14:16:05Z,http://arxiv.org/abs/1912.05971v2,"['eess.IV', 'cs.CV', 'cs.HC']"
"Integrated Millimeter Wave and Sub-6 GHz Wireless Networks: A Roadmap
  for Joint Mobile Broadband and Ultra-Reliable Low-Latency Communications","Next-generation wireless networks must enable emerging technologies such as
augmented reality and connected autonomous vehicles via wide range of wireless
services that span enhanced mobile broadband (eMBB), as well as ultra-reliable
low-latency communication (URLLC). Existing wireless systems that solely rely
on the scarce sub-6 GHz, microwave ($\mu$W) frequency bands will be unable to
meet such stringent and mixed service requirements for future wireless services
due to spectrum scarcity. Meanwhile, operating at high-frequency millimeter
wave (mmWave) bands is seen as an attractive solution, primarily due to the
bandwidth availability and possibility of large-scale multi-antenna
communication. However, mmWave communication is inherently unreliable due to
its susceptibility to blockage, high path loss, and channel uncertainty. Hence,
to provide URLLC and high-speed wireless access, it is desirable to seamlessly
integrate the reliability of $\mu$W networks with the high capacity of mmWave
networks. To this end, in this paper, the first comprehensive tutorial for
\emph{integrated mmWave-$\mu$W} communications is introduced. This envisioned
integrated design will enable wireless networks to achieve URLLC along with
eMBB by leveraging the best of two worlds: reliable, long-range communications
at the $\mu$W bands and directional high-speed communications at the mmWave
frequencies. To achieve this goal, key solution concepts are discussed that
include new architectures for the radio interface, URLLC-aware frame structure
and resource allocation methods along with mobility management, to realize the
potential of integrated mmWave-$\mu$W communications. The opportunities and
challenges of each proposed scheme are discussed and key results are presented
to show the merits of the developed integrated mmWave-$\mu$W framework.","['Omid Semiari', 'Walid Saad', 'Mehdi Bennis', 'Merouane Debbah']",2018-02-11T23:11:03Z,http://arxiv.org/abs/1802.03837v2,"['eess.SP', 'cs.IT', 'cs.NI', 'math.IT']"
"All Reality: Virtual, Augmented, Mixed (X), Mediated (X,Y), and
  Multimediated Reality","The contributions of this paper are: (1) a taxonomy of the ""Realities""
(Virtual, Augmented, Mixed, Mediated, etc.), and (2) some new kinds of
""reality"" that come from nature itself, i.e. that expand our notion beyond
synthetic realities to include also phenomenological realities.
  VR (Virtual Reality) replaces the real world with a simulated experience
(virtual world). AR (Augmented Reality) allows a virtual world to be
experienced while also experiencing the real world at the same time. Mixed
Reality provides blends that interpolate between real and virtual worlds in
various proportions, along a ""Virtuality"" axis, and extrapolate to an ""X-axis"".
Mediated Reality goes a step further by mixing/blending and also modifying
reality. This modifying of reality introduces a second axis. Mediated Reality
is useful as a seeing aid (e.g. modifying reality to make it easier to
understand), and for psychology experiments like Stratton's 1896 upside-down
eyeglasses experiment.
  We propose Multimediated Reality as a multidimensional multisensory mediated
reality that includes not just interactive multimedia-based reality for our
five senses, but also includes additional senses (like sensory sonar, sensory
radar, etc.), as well as our human actions/actuators. These extra senses are
mapped to our human senses using synthetic synesthesia. This allows us to
directly experience real (but otherwise invisible) phenomena, such as wave
propagation and wave interference patterns, so that we can see radio waves and
sound waves and how they interact with objects and each other. Multimediated
reality is multidimensional, multimodal, multisensory, and multiscale. It is
also multidisciplinary, in that we must consider not just the user, but also
how the technology affects others, e.g. how its physical appearance affects
social situations.","['Steve Mann', 'Tom Furness', 'Yu Yuan', 'Jay Iorio', 'Zixin Wang']",2018-04-20T15:40:39Z,http://arxiv.org/abs/1804.08386v1,['cs.HC']
"Deep Learning Coordinated Beamforming for Highly-Mobile Millimeter Wave
  Systems","Supporting high mobility in millimeter wave (mmWave) systems enables a wide
range of important applications such as vehicular communications and wireless
virtual/augmented reality. Realizing this in practice, though, requires
overcoming several challenges. First, the use of narrow beams and the
sensitivity of mmWave signals to blockage greatly impact the coverage and
reliability of highly-mobile links. Second, highly-mobile users in dense mmWave
deployments need to frequently hand-off between base stations (BSs), which is
associated with critical control and latency overhead. Further, identifying the
optimal beamforming vectors in large antenna array mmWave systems requires
considerable training overhead, which significantly affects the efficiency of
these mobile systems. In this paper, a novel integrated machine learning and
coordinated beamforming solution is developed to overcome these challenges and
enable highly-mobile mmWave applications. In the proposed solution, a number of
distributed yet coordinating BSs simultaneously serve a mobile user. This user
ideally needs to transmit only one uplink training pilot sequence that will be
jointly received at the coordinating BSs using omni or quasi-omni beam
patterns. These received signals draw a defining signature not only for the
user location, but also for its interaction with the surrounding environment.
The developed solution then leverages a deep learning model that learns how to
use these signatures to predict the beamforming vectors at the BSs. This
renders a comprehensive solution that supports highly-mobile mmWave
applications with reliable coverage, low latency, and negligible training
overhead. Simulation results show that the proposed deep-learning coordinated
beamforming strategy approaches the achievable rate of the genie-aided solution
that knows the optimal beamforming vectors with no training overhead.","['Ahmed Alkhateeb', 'Sam Alex', 'Paul Varkey', 'Ying Li', 'Qi Qu', 'Djordje Tujkovic']",2018-04-27T04:07:49Z,http://arxiv.org/abs/1804.10334v3,"['cs.IT', 'math.IT']"
Surface Light Field Compression using a Point Cloud Codec,"Light field (LF) representations aim to provide photo-realistic,
free-viewpoint viewing experiences. However, the most popular LF
representations are images from multiple views. Multi-view image-based
representations generally need to restrict the range or degrees of freedom of
the viewing experience to what can be interpolated in the image domain,
essentially because they lack explicit geometry information. We present a new
surface light field (SLF) representation based on explicit geometry, and a
method for SLF compression. First, we map the multi-view images of a scene onto
a 3D geometric point cloud. The color of each point in the point cloud is a
function of viewing direction known as a view map. We represent each view map
efficiently in a B-Spline wavelet basis. This representation is capable of
modeling diverse surface materials and complex lighting conditions in a highly
scalable and adaptive manner. The coefficients of the B-Spline wavelet
representation are then compressed spatially. To increase the spatial
correlation and thus improve compression efficiency, we introduce a smoothing
term to make the coefficients more similar across the 3D space. We compress the
coefficients spatially using existing point cloud compression (PCC) methods. On
the decoder side, the scene is rendered efficiently from any viewing direction
by reconstructing the view map at each point. In contrast to multi-view
image-based LF approaches, our method supports photo-realistic rendering of
real-world scenes from arbitrary viewpoints, i.e., with an unlimited six
degrees of freedom (6DOF). In terms of rate and distortion, experimental
results show that our method achieves superior performance with lighter decoder
complexity compared with a reference image-plus-geometry compression (IGC)
scheme, indicating its potential in practical virtual and augmented reality
applications.","['Xiang Zhang', 'Philip A. Chou', 'Ming-Ting Sun', 'Maolong Tang', 'Shanshe Wang', 'Siwei Ma', 'Wen Gao']",2018-05-29T00:08:30Z,http://arxiv.org/abs/1805.11203v1,['cs.MM']
"Improving Surgical Training Phantoms by Hyperrealism: Deep Unpaired
  Image-to-Image Translation from Real Surgeries","Current `dry lab' surgical phantom simulators are a valuable tool for
surgeons which allows them to improve their dexterity and skill with surgical
instruments. These phantoms mimic the haptic and shape of organs of interest,
but lack a realistic visual appearance. In this work, we present an innovative
application in which representations learned from real intraoperative
endoscopic sequences are transferred to a surgical phantom scenario. The term
hyperrealism is introduced in this field, which we regard as a novel subform of
surgical augmented reality for approaches that involve real-time object
transfigurations. For related tasks in the computer vision community, unpaired
cycle-consistent Generative Adversarial Networks (GANs) have shown excellent
results on still RGB images. Though, application of this approach to continuous
video frames can result in flickering, which turned out to be especially
prominent for this application. Therefore, we propose an extension of
cycle-consistent GANs, named tempCycleGAN, to improve temporal consistency.The
novel method is evaluated on captures of a silicone phantom for training
endoscopic reconstructive mitral valve procedures. Synthesized videos show
highly realistic results with regard to 1) replacement of the silicone
appearance of the phantom valve by intraoperative tissue texture, while 2)
explicitly keeping crucial features in the scene, such as instruments, sutures
and prostheses. Compared to the original CycleGAN approach, tempCycleGAN
efficiently removes flickering between frames. The overall approach is expected
to change the future design of surgical training simulators since the generated
sequences clearly demonstrate the feasibility to enable a considerably more
realistic training experience for minimally-invasive procedures.","['Sandy Engelhardt', 'Raffaele De Simone', 'Peter M. Full', 'Matthias Karck', 'Ivo Wolf']",2018-06-10T10:17:21Z,http://arxiv.org/abs/1806.03627v1,['cs.CY']
Towards real-time unsupervised monocular depth estimation on CPU,"Unsupervised depth estimation from a single image is a very attractive
technique with several implications in robotic, autonomous navigation,
augmented reality and so on. This topic represents a very challenging task and
the advent of deep learning enabled to tackle this problem with excellent
results. However, these architectures are extremely deep and complex. Thus,
real-time performance can be achieved only by leveraging power-hungry GPUs that
do not allow to infer depth maps in application fields characterized by
low-power constraints. To tackle this issue, in this paper we propose a novel
architecture capable to quickly infer an accurate depth map on a CPU, even of
an embedded system, using a pyramid of features extracted from a single input
image. Similarly to state-of-the-art, we train our network in an unsupervised
manner casting depth estimation as an image reconstruction problem. Extensive
experimental results on the KITTI dataset show that compared to the top
performing approach our network has similar accuracy but a much lower
complexity (about 6% of parameters) enabling to infer a depth map for a KITTI
image in about 1.7 s on the Raspberry Pi 3 and at more than 8 Hz on a standard
CPU. Moreover, by trading accuracy for efficiency, our network allows to infer
maps at about 2 Hz and 40 Hz respectively, still being more accurate than most
state-of-the-art slower methods. To the best of our knowledge, it is the first
method enabling such performance on CPUs paving the way for effective
deployment of unsupervised monocular depth estimation even on embedded systems.","['Matteo Poggi', 'Filippo Aleotti', 'Fabio Tosi', 'Stefano Mattoccia']",2018-06-29T14:18:24Z,http://arxiv.org/abs/1806.11430v3,"['cs.CV', 'cs.RO']"
Visual Distortions in 360-degree Videos,"Omnidirectional (or 360-degree) images and videos are emergent signals in
many areas such as robotics and virtual/augmented reality. In particular, for
virtual reality, they allow an immersive experience in which the user is
provided with a 360-degree field of view and can navigate throughout a scene,
e.g., through the use of Head Mounted Displays. Since it represents the full
360-degree field of view from one point of the scene, omnidirectional content
is naturally represented as spherical visual signals. Current approaches for
capturing, processing, delivering, and displaying 360-degree content, however,
present many open technical challenges and introduce several types of
distortions in these visual signals. Some of the distortions are specific to
the nature of 360-degree images, and often different from those encountered in
the classical image communication framework. This paper provides a first
comprehensive review of the most common visual distortions that alter
360-degree signals undergoing state of the art processing in common
applications. While their impact on viewers' visual perception and on the
immersive experience at large is still unknown ---thus, it stays an open
research topic--- this review serves the purpose of identifying the main causes
of visual distortions in the end-to-end 360-degree content distribution
pipeline. It is essential as a basis for benchmarking different processing
techniques, allowing the effective design of new algorithms and applications.
It is also necessary to the deployment of proper psychovisual studies to
characterise the human perception of these new images in interactive and
immersive applications.","['Roberto G. de A. Azevedo', 'Neil Birkbeck', 'Francesca De Simone', 'Ivan Janatra', 'Balu Adsumilli', 'Pascal Frossard']",2019-01-07T14:52:28Z,http://arxiv.org/abs/1901.01848v1,['cs.MM']
"The G332 molecular cloud ring: I. Morphology and physical
  characteristics","We present a morphological and physical analysis of a Giant Molecular Cloud
(GMC) using the carbon monoxide isotopologues ($^{12}$CO, $^{13}$CO, C$^{18}$O
$^{3}P_{2}\rightarrow$ $^{3}P_{1}$) survey of the Galactic Plane (Mopra CO
Southern Galactic Plane Survey), supplemented with neutral carbon maps from the
HEAT telescope in Antarctica. The giant molecular cloud structure (hereinafter
the ring) covers the sky region $332^\circ$ < $\ell$ < $333^\circ$ and
$\mathit{b}$ = $\pm 0.5^\circ$ (hereinafter the G332 region). The mass of the
ring and its distance are determined to be respectively
~2$\times10^{5}\mathrm{M_{\odot}}$ and ~3.7 kpc from Sun. The dark molecular
gas fraction, estimated from the $^{13}$CO and [CI] lines, is $\sim17\%$ for a
CO T$_{\mathrm{ex}}$ between [10,20 K]. Comparing the [CI] integrated intensity
and N(H$_{2}$) traced by $^{13}$CO and $^{12}$CO, we define an
X$\mathrm{_{CI}^{809}}$ factor, analogous to the usual X$_{\mathrm{co}}$,
through the [CI] line. X$\mathrm{_{CI}^{809}}$ ranges between
[1.8,2.0]$\times10^{21}\mathrm{cm}^{-2}\mathrm{K}^{-1}\mathrm{km}^{-1}\mathrm{s}$.
We examined local variation in X$_{\mathrm{co}}$ and T$_{\mathrm{ex}}$ across
the cloud, and find in regions where the star formation activity is not in an
advanced state, an increase in the mean and dispersion of the X$_{\mathrm{co}}$
factor as the excitation temperature decreases. We present a catalogue of
C$^{18}$O clumps within the cloud. The star formation (SF) activity ongoing in
the cloud shows a correlation with T$_{\mathrm{ex}}$, [CI] and CO emissions,
and anti-correlation with X$_{\mathrm{co}}$, suggesting a North-South spatial
gradient in the SF activity. We propose a method to disentangle dust emission
across the Galaxy, using HI and $^{13}$CO data. We describe Virtual Reality
(VR) and Augmented Reality (AR) data visualisation techniques for the analysis
of radio astronomy data.","['Domenico Romano', 'Michael G. Burton', 'Michael C. B. Ashley', 'Sergio Molinari', 'David Rebolledo', 'Catherine Braiding', 'Eugenio Schisano']",2019-01-17T16:03:42Z,http://arxiv.org/abs/1901.05961v1,['astro-ph.GA']
Visual Question Answering for Cultural Heritage,"Technology and the fruition of cultural heritage are becoming increasingly
more entwined, especially with the advent of smart audio guides, virtual and
augmented reality, and interactive installations. Machine learning and computer
vision are important components of this ongoing integration, enabling new
interaction modalities between user and museum. Nonetheless, the most frequent
way of interacting with paintings and statues still remains taking pictures.
Yet images alone can only convey the aesthetics of the artwork, lacking is
information which is often required to fully understand and appreciate it.
Usually this additional knowledge comes both from the artwork itself (and
therefore the image depicting it) and from an external source of knowledge,
such as an information sheet. While the former can be inferred by computer
vision algorithms, the latter needs more structured data to pair visual content
with relevant information. Regardless of its source, this information still
must be be effectively transmitted to the user. A popular emerging trend in
computer vision is Visual Question Answering (VQA), in which users can interact
with a neural network by posing questions in natural language and receiving
answers about the visual content. We believe that this will be the evolution of
smart audio guides for museum visits and simple image browsing on personal
smartphones. This will turn the classic audio guide into a smart personal
instructor with which the visitor can interact by asking for explanations
focused on specific interests. The advantages are twofold: on the one hand the
cognitive burden of the visitor will decrease, limiting the flow of information
to what the user actually wants to hear; and on the other hand it proposes the
most natural way of interacting with a guide, favoring engagement.","['Pietro Bongini', 'Federico Becattini', 'Andrew D. Bagdanov', 'Alberto Del Bimbo']",2020-03-22T10:26:08Z,http://arxiv.org/abs/2003.09853v1,"['cs.CV', 'cs.CL']"
State of the Art on Neural Rendering,"Efficient rendering of photo-realistic virtual worlds is a long standing
effort of computer graphics. Modern graphics techniques have succeeded in
synthesizing photo-realistic images from hand-crafted scene representations.
However, the automatic generation of shape, materials, lighting, and other
aspects of scenes remains a challenging problem that, if solved, would make
photo-realistic computer graphics more widely accessible. Concurrently,
progress in computer vision and machine learning have given rise to a new
approach to image synthesis and editing, namely deep generative models. Neural
rendering is a new and rapidly emerging field that combines generative machine
learning techniques with physical knowledge from computer graphics, e.g., by
the integration of differentiable rendering into network training. With a
plethora of applications in computer graphics and vision, neural rendering is
poised to become a new area in the graphics community, yet no survey of this
emerging field exists. This state-of-the-art report summarizes the recent
trends and applications of neural rendering. We focus on approaches that
combine classic computer graphics techniques with deep generative models to
obtain controllable and photo-realistic outputs. Starting with an overview of
the underlying computer graphics and machine learning concepts, we discuss
critical aspects of neural rendering approaches. This state-of-the-art report
is focused on the many important use cases for the described algorithms such as
novel view synthesis, semantic photo manipulation, facial and body reenactment,
relighting, free-viewpoint video, and the creation of photo-realistic avatars
for virtual and augmented reality telepresence. Finally, we conclude with a
discussion of the social implications of such technology and investigate open
research problems.","['Ayush Tewari', 'Ohad Fried', 'Justus Thies', 'Vincent Sitzmann', 'Stephen Lombardi', 'Kalyan Sunkavalli', 'Ricardo Martin-Brualla', 'Tomas Simon', 'Jason Saragih', 'Matthias Nießner', 'Rohit Pandey', 'Sean Fanello', 'Gordon Wetzstein', 'Jun-Yan Zhu', 'Christian Theobalt', 'Maneesh Agrawala', 'Eli Shechtman', 'Dan B Goldman', 'Michael Zollhöfer']",2020-04-08T04:36:31Z,http://arxiv.org/abs/2004.03805v1,"['cs.CV', 'cs.GR']"
Eye Gaze Controlled Robotic Arm for Persons with SSMI,"Background: People with severe speech and motor impairment (SSMI) often uses
a technique called eye pointing to communicate with outside world. One of their
parents, caretakers or teachers hold a printed board in front of them and by
analyzing their eye gaze manually, their intentions are interpreted. This
technique is often error prone and time consuming and depends on a single
caretaker.
  Objective: We aimed to automate the eye tracking process electronically by
using commercially available tablet, computer or laptop and without requiring
any dedicated hardware for eye gaze tracking. The eye gaze tracker is used to
develop a video see through based AR (augmented reality) display that controls
a robotic device with eye gaze and deployed for a fabric printing task.
  Methodology: We undertook a user centred design process and separately
evaluated the web cam based gaze tracker and the video see through based human
robot interaction involving users with SSMI. We also reported a user study on
manipulating a robotic arm with webcam based eye gaze tracker.
  Results: Using our bespoke eye gaze controlled interface, able bodied users
can select one of nine regions of screen at a median of less than 2 secs and
users with SSMI can do so at a median of 4 secs. Using the eye gaze controlled
human-robot AR display, users with SSMI could undertake representative pick and
drop task at an average duration less than 15 secs and reach a randomly
designated target within 60 secs using a COTS eye tracker and at an average
time of 2 mins using the webcam based eye gaze tracker.","['Vinay Krishna Sharma', 'L. R. D. Murthy', 'KamalPreet Singh Saluja', 'Vimal Mollyn', 'Gourav Sharma', 'Pradipta Biswas']",2020-05-25T09:23:20Z,http://arxiv.org/abs/2005.11994v1,"['cs.HC', 'cs.CV', 'eess.IV', 'I.4; I.2; H.5.2; K.4']"
"A network paradigm for very high capacity mobile and fixed
  telecommunications ecosystem sustainable evolution","For very high capacity networks (VHC), the main objective is to improve the
quality of the end-user experience. This implies compliance with key
performance indicators (KPIs) required by applications. Key performance
indicators at the application level are throughput, download time, round trip
time, and video delay. They depend on the end-to-end connection between the
server and the end-user device. For VHC networks, Telco operators must provide
the required application quality. Moreover, they must meet the objectives of
economic sustainability. Today, Telco operators rarely achieve the above
objectives, mainly due to the push to increase the bit-rate of access networks
without considering the end-to-end KPIs of the applications. The main
contribution of this paper concerns the definition of a deployment framework to
address performance and cost issues for VHC networks. We show three actions on
which it is necessary to focus. First, limiting bit-rate through video
compression. Second, contain the rate of packet loss through artificial
intelligence algorithms for line stabilization. Third, reduce latency (i.e.,
round-trip time) with edge-cloud computing. The concerted and gradual
application of these measures can allow a Telco to get out of the
ultra-broadband ""trap"" of the access network, as defined in the paper. We
propose to work on end-to-end optimization of the bandwidth utilization ratio.
This leads to a better performance experienced by the end-user. It also allows
a Telco operator to create new business models and obtain new revenue streams
at a sustainable cost. To give a clear example, we describe how to realize
mobile virtual and augmented reality, which is one of the most challenging
future services.","['Francesco Vatalaro', 'Gianfranco Ciccarella']",2020-06-02T14:48:34Z,http://arxiv.org/abs/2006.01674v2,['cs.NI']
QUALINET White Paper on Definitions of Immersive Media Experience (IMEx),"With the coming of age of virtual/augmented reality and interactive media,
numerous definitions, frameworks, and models of immersion have emerged across
different fields ranging from computer graphics to literary works. Immersion is
oftentimes used interchangeably with presence as both concepts are closely
related. However, there are noticeable interdisciplinary differences regarding
definitions, scope, and constituents that are required to be addressed so that
a coherent understanding of the concepts can be achieved. Such consensus is
vital for paving the directionality of the future of immersive media
experiences (IMEx) and all related matters. The aim of this white paper is to
provide a survey of definitions of immersion and presence which leads to a
definition of immersive media experience (IMEx). The Quality of Experience
(QoE) for immersive media is described by establishing a relationship between
the concepts of QoE and IMEx followed by application areas of immersive media
experience. Influencing factors on immersive media experience are elaborated as
well as the assessment of immersive media experience. Finally, standardization
activities related to IMEx are highlighted and the white paper is concluded
with an outlook related to future developments.","['Andrew Perkis', 'Christian Timmerer', 'Sabina Baraković', 'Jasmina Baraković Husić', 'Søren Bech', 'Sebastian Bosse', 'Jean Botev', 'Kjell Brunnström', 'Luis Cruz', 'Katrien De Moor', 'Andrea de Polo Saibanti', 'Wouter Durnez', 'Sebastian Egger-Lampl', 'Ulrich Engelke', 'Tiago H. Falk', 'Jesús Gutiérrez', 'Asim Hameed', 'Andrew Hines', 'Tanja Kojic', 'Dragan Kukolj', 'Eirini Liotou', 'Dragorad Milovanovic', 'Sebastian Möller', 'Niall Murray', 'Babak Naderi', 'Manuela Pereira', 'Stuart Perry', 'Antonio Pinheiro', 'Andres Pinilla', 'Alexander Raake', 'Sarvesh Rajesh Agrawal', 'Ulrich Reiter', 'Rafael Rodrigues', 'Raimund Schatz', 'Peter Schelkens', 'Steven Schmidt', 'Saeed Shafiee Sabet', 'Ashutosh Singla', 'Lea Skorin-Kapov', 'Mirko Suznjevic', 'Stefan Uhrig', 'Sara Vlahović', 'Jan-Niklas Voigt-Antons', 'Saman Zadtootaghaj']",2020-06-10T15:59:42Z,http://arxiv.org/abs/2007.07032v2,['cs.MM']
"An In-Depth Exploration of the Effect of 2D/3D Views and Controller
  Types on First Person Shooter Games in Virtual Reality","The amount of interest in Virtual Reality (VR) research has significantly
increased over the past few years, both in academia and industry. The release
of commercial VR Head-Mounted Displays (HMDs) has been a major contributing
factor. However, there is still much to be learned, especially how views and
input techniques, as well as their interaction, affect the VR experience. There
is little work done on First-Person Shooter (FPS) games in VR, and those few
studies have focused on a single aspect of VR FPS. They either focused on the
view, e.g., comparing VR to a typical 2D display or on the controller types. To
the best of our knowledge, there are no studies investigating variations of
2D/3D views in HMDs, controller types, and their interactions. As such, it is
challenging to distinguish findings related to the controller type from those
related to the view. If a study does not control for the input method and finds
that 2D displays lead to higher performance than VR, we cannot generalize the
results because of the confounding variables. To understand their interaction,
we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the
way it is controlled that gives the platforms their respective advantages. To
study the effects of the 2D/3D views, we created a 2D visual technique,
PlaneFrame, that was applied inside the VR headset. Our results show that the
controller type can have a significant positive impact on performance,
immersion, and simulator sickness when associated with a 2D view. They further
our understanding of the interactions that controllers and views have and
demonstrate that comparisons are highly dependent on how both factors go
together. Further, through a series of three experiments, we developed a
technique that can lead to a substantial performance, a good level of
immersion, and can minimize the level of simulator sickness.","['Diego Monteiro', 'Hai-Ning Liang', 'Jialin Wang', 'Hao Chen', 'Nilufar Baghaei']",2020-10-07T08:17:07Z,http://arxiv.org/abs/2010.03256v1,['cs.HC']
Bridging Composite and Real: Towards End-to-end Deep Image Matting,"Extracting accurate foregrounds from natural images benefits many downstream
applications such as film production and augmented reality. However, the furry
characteristics and various appearance of the foregrounds, e.g., animal and
portrait, challenge existing matting methods, which usually require extra user
inputs such as trimap or scribbles. To resolve these problems, we study the
distinct roles of semantics and details for image matting and decompose the
task into two parallel sub-tasks: high-level semantic segmentation and
low-level details matting. Specifically, we propose a novel Glance and Focus
Matting network (GFM), which employs a shared encoder and two separate decoders
to learn both tasks in a collaborative manner for end-to-end natural image
matting. Besides, due to the limitation of available natural images in the
matting task, previous methods typically adopt composite images for training
and evaluation, which result in limited generalization ability on real-world
images. In this paper, we investigate the domain gap issue between composite
images and real-world images systematically by conducting comprehensive
analyses of various discrepancies between the foreground and background images.
We find that a carefully designed composition route RSSN that aims to reduce
the discrepancies can lead to a better model with remarkable generalization
ability. Furthermore, we provide a benchmark containing 2,000 high-resolution
real-world animal images and 10,000 portrait images along with their manually
labeled alpha mattes to serve as a test bed for evaluating matting model's
generalization ability on real-world images. Comprehensive empirical studies
have demonstrated that GFM outperforms state-of-the-art methods and effectively
reduces the generalization error. The code and the datasets will be released at
https://github.com/JizhiziLi/GFM.","['Jizhizi Li', 'Jing Zhang', 'Stephen J. Maybank', 'Dacheng Tao']",2020-10-30T10:57:13Z,http://arxiv.org/abs/2010.16188v3,"['cs.CV', 'cs.LG', 'eess.IV']"
"I-BOT: Interference-Based Orchestration of Tasks for Dynamic Unmanaged
  Edge Computing","In recent years, edge computing has become a popular choice for
latency-sensitive applications like facial recognition and augmented reality
because it is closer to the end users compared to the cloud. Although
infrastructure providers are working toward creating managed edge networks,
personal devices such as laptops and tablets, which are widely available and
are underutilized, can also be used as potential edge devices. We call such
devices Unmanaged Edge Devices (UEDs). Scheduling application tasks on such an
unmanaged edge system is not straightforward because of three fundamental
reasons-heterogeneity in the computational capacity of the UEDs, uncertainty in
the availability of the UEDs (due to devices leaving the system), and
interference among multiple tasks sharing a UED. In this paper, we present
I-BOT, an interference-based orchestration scheme for latency-sensitive tasks
on an Unmanaged Edge Platform (UEP). It minimizes the completion time of
applications and is bandwidth efficient. I-BOT brings forth three innovations.
First, it profiles and predicts the interference patterns of the tasks to make
scheduling decisions. Second, it uses a feedback mechanism to adjust for
changes in the computational capacity of the UEDs and a prediction mechanism to
handle their sporadic exits. Third, it accounts for input dependence of tasks
in its scheduling decision (such as, two tasks requiring the same input data).
To evaluate I-BOT, we run end-to-end simulations with applications representing
autonomous driving, composed of multiple tasks. We compare to two basic
baselines (random and round-robin) and two state-of-the-arts, Lavea [SEC-2017]
and Petrel [MSN-2018]. Compared to these baselines, I-BOT significantly reduces
the average service time of application tasks. This reduction is more
pronounced in dynamic heterogeneous environments, which would be the case in a
UEP.","['Shikhar Suryavansh', 'Chandan Bothra', 'Kwang Taik Kim', 'Mung Chiang', 'Chunyi Peng', 'Saurabh Bagchi']",2020-11-11T17:34:18Z,http://arxiv.org/abs/2011.05925v1,"['cs.DC', 'cs.NI', 'C.2.4; C.3']"
"An XR rapid prototyping framework for interoperability across the
  reality spectrum","Applications of the Extended Reality (XR) spectrum, a superset of Mixed,
Augmented and Virtual Reality, are gaining prominence and can be employed in a
variety of areas, such as virtual museums. Examples can be found in the areas
of education, cultural heritage, health/treatment, entertainment, marketing,
and more. The majority of computer graphics applications nowadays are used to
operate only in one of the above realities. The lack of applications across the
XR spectrum is a real shortcoming. There are many advantages resulting from
this problem's solution. Firstly, releasing an application across the XR
spectrum could contribute in discovering its most suitable reality. Moreover,
an application could be more immersive within a particular reality, depending
on its context. Furthermore, its availability increases to a broader range of
users. For instance, if an application is released both in Virtual and
Augmented Reality, it is accessible to users that may lack the possession of a
VR headset, but not of a mobile AR device. The question that arises at this
point, would be ""Is it possible for a full s/w application stack to be
converted across XR without sacrificing UI/UX in a semi-automatic way?"". It may
be quite difficult, depending on the architecture and application
implementation. Most companies nowadays support only one reality, due to their
lack of UI/UX software architecture or resources to support the complete XR
spectrum. In this work, we present an ""automatic reality transition"" in the
context of virtual museum applications. We propose a development framework,
which will automatically allow this XR transition. This framework transforms
any XR project into different realities such as Augmented or Virtual. It also
reduces the development time while increasing the XR availability of 3D
applications, encouraging developers to release applications across the XR
spectrum.","['Efstratios Geronikolakis', 'George Papagiannakis']",2021-01-05T20:27:47Z,http://arxiv.org/abs/2101.01771v2,"['cs.GR', 'cs.HC', '68U99', 'D.2.2; D.2.12']"
AICP: Augmented Informative Cooperative Perception,"Connected vehicles, whether equipped with advanced driver-assistance systems
or fully autonomous, require human driver supervision and are currently
constrained to visual information in their line-of-sight. A cooperative
perception system among vehicles increases their situational awareness by
extending their perception range. Existing solutions focus on improving
perspective transformation and fast information collection. However, such
solutions fail to filter out large amounts of less relevant data and thus
impose significant network and computation load. Moreover, presenting all this
less relevant data can overwhelm the driver and thus actually hinder them. To
address such issues, we present Augmented Informative Cooperative Perception
(AICP), the first fast-filtering system which optimizes the informativeness of
shared data at vehicles to improve the fused presentation.
  To this end, an informativeness maximization problem is presented for
vehicles to select a subset of data to display to their drivers. Specifically,
we propose (i) a dedicated system design with custom data structure and
lightweight routing protocol for convenient data encapsulation, fast
interpretation and transmission, and (ii) a comprehensive problem formulation
and efficient fitness-based sorting algorithm to select the most valuable data
to display at the application layer.
  We implement a proof-of-concept prototype of AICP with a bandwidth-hungry,
latency-constrained real-life augmented reality application. The prototype adds
only 12.6 milliseconds of latency to a current informativeness-unaware system.
Next, we test the networking performance of AICP at scale and show that ACIP
effectively filters out less relevant packets and decreases the channel busy
time.","['Pengyuan Zhou', 'Pranvera Kortoci', 'Yui-Pan Yau', 'Tristan Braud', 'Xiujun Wang', 'Benjamin Finley', 'Lik-Hang Lee', 'Sasu Tarkoma', 'Jussi Kangasharju', 'Pan Hui']",2021-01-14T09:04:16Z,http://arxiv.org/abs/2101.05508v3,"['cs.MM', 'cs.HC']"
"Worsening Perception: Real-time Degradation of Autonomous Vehicle
  Perception Performance for Simulation of Adverse Weather Conditions","Autonomous vehicles rely heavily upon their perception subsystems to see the
environment in which they operate. Unfortunately, the effect of variable
weather conditions presents a significant challenge to object detection
algorithms, and thus it is imperative to test the vehicle extensively in all
conditions which it may experience. However, development of robust autonomous
vehicle subsystems requires repeatable, controlled testing - while real weather
is unpredictable and cannot be scheduled. Real-world testing in adverse
conditions is an expensive and time-consuming task, often requiring access to
specialist facilities. Simulation is commonly relied upon as a substitute, with
increasingly visually realistic representations of the real-world being
developed. In the context of the complete autonomous vehicle control pipeline,
subsystems downstream of perception need to be tested with accurate recreations
of the perception system output, rather than focusing on subjective visual
realism of the input - whether in simulation or the real world. This study
develops the untapped potential of a lightweight weather augmentation method in
an autonomous racing vehicle - focusing not on visual accuracy, but rather the
effect upon perception subsystem performance in real time. With minimal
adjustment, the prototype developed in this study can replicate the effects of
water droplets on the camera lens, and fading light conditions. This approach
introduces a latency of less than 8 ms using compute hardware well suited to
being carried in the vehicle - rendering it ideal for real-time implementation
that can be run during experiments in simulation, and augmented reality testing
in the real world.","['Ivan Fursa', 'Elias Fandi', 'Valentina Musat', 'Jacob Culley', 'Enric Gil', 'Izzeddin Teeti', 'Louise Bilous', 'Isaac Vander Sluis', 'Alexander Rast', 'Andrew Bradley']",2021-03-03T23:49:02Z,http://arxiv.org/abs/2103.02760v4,"['cs.RO', 'cs.CV']"
Physically Inspired Dense Fusion Networks for Relighting,"Image relighting has emerged as a problem of significant research interest
inspired by augmented reality applications. Physics-based traditional methods,
as well as black box deep learning models, have been developed. The existing
deep networks have exploited training to achieve a new state of the art;
however, they may perform poorly when training is limited or does not represent
problem phenomenology, such as the addition or removal of dense shadows. We
propose a model which enriches neural networks with physical insight. More
precisely, our method generates the relighted image with new illumination
settings via two different strategies and subsequently fuses them using a
weight map (w). In the first strategy, our model predicts the material
reflectance parameters (albedo) and illumination/geometry parameters of the
scene (shading) for the relit image (we refer to this strategy as intrinsic
image decomposition (IID)). The second strategy is solely based on the black
box approach, where the model optimizes its weights based on the ground-truth
images and the loss terms in the training stage and generates the relit output
directly (we refer to this strategy as direct). While our proposed method
applies to both one-to-one and any-to-any relighting problems, for each case we
introduce problem-specific components that enrich the model performance: 1) For
one-to-one relighting we incorporate normal vectors of the surfaces in the
scene to adjust gloss and shadows accordingly in the image. 2) For any-to-any
relighting, we propose an additional multiscale block to the architecture to
enhance feature extraction. Experimental results on the VIDIT 2020 and the
VIDIT 2021 dataset (used in the NTIRE 2021 relighting challenge) reveals that
our proposal can outperform many state-of-the-art methods in terms of
well-known fidelity metrics and perceptual loss.","['Amirsaeed Yazdani', 'Tiantong Guo', 'Vishal Monga']",2021-05-05T17:33:45Z,http://arxiv.org/abs/2105.02209v1,"['cs.CV', 'cs.AI']"
"A systematic review of physical-digital play technology and
  developmentally relevant child behaviour","New interactive physical-digital play technologies are shaping the way
children plan. These technologies refer to digital play technologies that
engage children in analogue forms of behaviour, either alone or with others.
Current interactive physical-digital play technologies include robots, digital
agents, mixed or augmented reality devices, and smart-eye based gaming. Little
is known, however, about the ways in which these technologies could promote or
damage child development. This systematic review was aimed at understanding if
and how these physical-digital play technologies promoted developmentally
relevant behaviour in typically developing 0 to 12 year-olds. Psychology,
Education, and Computer Science databases were searched producing 635 paper. A
total of 31 papers met the inclusion criteria, of which 17 were of high enough
quality to be included for synthesis. Results indicate that these new
interactive play technologies could have a positive effect on children's
developmentally relevant behaviour. The review indicated specific ways in which
different behaviour were promoted. Providing information about own performance
promoted self-monitoring. Slowing interactivity, play interdependency, and
joint object accessibility promoted collaboration. Offering delimited choices
promoted decision making. Problem solving and physical activity were promoted
by requiring children to engage in them to keep playing. Four principles
underpinned the ways in which physical digital play technologies afforded child
behaviour. These included social expectations framing play situations, the
directiveness of action regulations (inviting, guiding or forcing behaviours),
the technical features of play technologies (digital play mechanics and
physical characteristics), and the alignment between play goals, play
technology and the play behaviours promoted.","['Pablo E. Torres', 'Philip I. N. Ulrich', 'Veronica Cucuiat', 'Mutlu Cukurova', 'Maria Fercovic De la Presa', 'Rose Luckin', 'Amanda Carr', 'Thomas Dylan', 'Abigail Durrant', 'John Vines', 'Shaun Lawson']",2021-05-22T13:41:40Z,http://arxiv.org/abs/2105.10731v3,['cs.HC']
"CONECT4: Desarrollo de componentes basados en Realidad Mixta, Realidad
  Virtual Y Conocimiento Experto para generación de entornos de aprendizaje
  Hombre-Máquina","This work presents the results of project CONECT4, which addresses the
research and development of new non-intrusive communication methods for the
generation of a human-machine learning ecosystem oriented to predictive
maintenance in the automotive industry. Through the use of innovative
technologies such as Augmented Reality, Virtual Reality, Digital Twin and
expert knowledge, CONECT4 implements methodologies that allow improving the
efficiency of training techniques and knowledge management in industrial
companies. The research has been supported by the development of content and
systems with a low level of technological maturity that address solutions for
the industrial sector applied in training and assistance to the operator. The
results have been analyzed in companies in the automotive sector, however, they
are exportable to any other type of industrial sector. -- --
  En esta publicaci\'on se presentan los resultados del proyecto CONECT4, que
aborda la investigaci\'on y desarrollo de nuevos m\'etodos de comunicaci\'on no
intrusivos para la generaci\'on de un ecosistema de aprendizaje
hombre-m\'aquina orientado al mantenimiento predictivo en la industria de
automoci\'on. A trav\'es del uso de tecnolog\'ias innovadoras como la Realidad
Aumentada, la Realidad Virtual, el Gemelo Digital y conocimiento experto,
CONECT4 implementa metodolog\'ias que permiten mejorar la eficiencia de las
t\'ecnicas de formaci\'on y gesti\'on de conocimiento en las empresas
industriales. La investigaci\'on se ha apoyado en el desarrollo de contenidos y
sistemas con un nivel de madurez tecnol\'ogico bajo que abordan soluciones para
el sector industrial aplicadas en la formaci\'on y asistencia al operario. Los
resultados han sido analizados en empresas del sector de automoci\'on, no
obstante, son exportables a cualquier otro tipo de sector industrial.","['Santiago González', 'Alvaro García', 'Ana Núñez']",2021-05-24T11:51:46Z,http://arxiv.org/abs/2105.11216v1,['cs.HC']
"FGLP: A Federated Fine-Grained Location Prediction System for Mobile
  Users","Fine-grained location prediction on smart phones can be used to improve
app/system performance. Application scenarios include video quality adaptation
as a function of the 5G network quality at predicted user locations, and
augmented reality apps that speed up content rendering based on predicted user
locations. Such use cases require prediction error in the same range as the GPS
error, and no existing works on location prediction can achieve this level of
accuracy. We present a system for fine-grained location prediction (FGLP) of
mobile users, based on GPS traces collected on the phones. FGLP has two
components: a federated learning framework and a prediction model. The
framework runs on the phones of the users and also on a server that coordinates
learning from all users in the system. FGLP represents the user location data
as relative points in an abstract 2D space, which enables learning across
different physical spaces. The model merges Bidirectional Long Short-Term
Memory (BiLSTM) and Convolutional Neural Networks (CNN), where BiLSTM learns
the speed and direction of the mobile users, and CNN learns information such as
user movement preferences. FGLP uses federated learning to protect user privacy
and reduce bandwidth consumption. Our experimental results, using a dataset
with over 600,000 users, demonstrate that FGLP outperforms baseline models in
terms of prediction accuracy. We also demonstrate that FGLP works well in
conjunction with transfer learning, which enables model reusability. Finally,
benchmark results on several types of Android phones demonstrate FGLP's
feasibility in real life.","['Xiaopeng Jiang', 'Shuai Zhao', 'Guy Jacobson', 'Rittwik Jana', 'Wen-Ling Hsu', 'Manoop Talasila', 'Syed Anwar Aftab', 'Yi Chen', 'Cristian Borcea']",2021-06-13T02:09:29Z,http://arxiv.org/abs/2106.08946v1,"['cs.LG', 'cs.AI', 'cs.DC', 'cs.SY', 'eess.SY']"
"Applying VertexShuffle Toward 360-Degree Video Super-Resolution on
  Focused-Icosahedral-Mesh","With the emerging of 360-degree image/video, augmented reality (AR) and
virtual reality (VR), the demand for analysing and processing spherical signals
get tremendous increase. However, plenty of effort paid on planar signals that
projected from spherical signals, which leading to some problems, e.g. waste of
pixels, distortion. Recent advances in spherical CNN have opened up the
possibility of directly analysing spherical signals. However, they pay
attention to the full mesh which makes it infeasible to deal with situations in
real-world application due to the extremely large bandwidth requirement. To
address the bandwidth waste problem associated with 360-degree video streaming
and save computation, we exploit Focused Icosahedral Mesh to represent a small
area and construct matrices to rotate spherical content to the focused mesh
area. We also proposed a novel VertexShuffle operation that can significantly
improve both the performance and the efficiency compared to the original
MeshConv Transpose operation introduced in UGSCNN. We further apply our
proposed methods on super resolution model, which is the first to propose a
spherical super-resolution model that directly operates on a mesh
representation of spherical pixels of 360-degree data. To evaluate our model,
we also collect a set of high-resolution 360-degree videos to generate a
spherical image dataset. Our experiments indicate that our proposed spherical
super-resolution model achieves significant benefits in terms of both
performance and inference time compared to the baseline spherical
super-resolution model that uses the simple MeshConv Transpose operation. In
summary, our model achieves great super-resolution performance on 360-degree
inputs, achieving 32.79 dB PSNR on average when super-resoluting 16x vertices
on the mesh.","['Na Li', 'Yao Liu']",2021-06-21T16:53:57Z,http://arxiv.org/abs/2106.11253v1,"['eess.IV', 'cs.CV', 'cs.MM']"
"Resilient and Latency-aware Orchestration of Network Slices Using
  Multi-connectivity in MEC-enabled 5G Networks","Network slicing (NS) and multi-access edge computing (MEC) are new paradigms
which play key roles in 5G and beyond networks. NS allows network operators
(NOs) to divide the available network resources into multiple logical NSs for
providing dedicated virtual networks tailored to the specific service/business
requirements. MEC enables NOs to provide diverse ultra-low latency services for
supporting the needs of different industry verticals by moving computing
facilities to the network edge. NS can be constructed by instantiating a set of
virtual network functions (VNFs) on top of MEC cloud servers for provisioning
diverse latency-sensitive communication services (e.g., autonomous driving and
augmented reality) on demand at a lesser cost and time. However, VNFs, MEC
cloud servers, and communication links are subject to failures due to software
bugs, misconfiguration, overloading, hardware faults, cyber attacks, power
outage, and natural/man-made disaster. Failure of a critical network component
disrupts services abruptly and leads to users' dissatisfaction, which may
result in revenue loss for the NOs. In this paper, we present a novel approach
based on multi-connectivity in 5G networks to tackle this problem and our
proposed approach is resilient against i) failure of VNFs, ii) failure of local
servers within MEC, iii) failure of communication links, and iv) failure of an
entire MEC cloud facility in regional level. To this end, we formulate the
problem as a binary integer programming (BIP) model in order to optimally
deploy NSs with the minimum cost, and prove it is NP-hard. To overcome time
complexity, we propose an efficient genetic algorithm based heuristic to obtain
near-optimal solution in polynomial time. By extensive simulations, we show
that our proposed approach not only reduces resource wastage, but also improves
throughput while providing high resiliency against failures.","['Prabhu Kaliyammal Thiruvasagam', 'Abhishek Chakraborty', 'C Siva Ram Murthy']",2021-07-01T17:14:59Z,http://arxiv.org/abs/2107.00618v1,['cs.NI']
"Mutually improved endoscopic image synthesis and landmark detection in
  unpaired image-to-image translation","The CycleGAN framework allows for unsupervised image-to-image translation of
unpaired data. In a scenario of surgical training on a physical surgical
simulator, this method can be used to transform endoscopic images of phantoms
into images which more closely resemble the intra-operative appearance of the
same surgical target structure. This can be viewed as a novel augmented reality
approach, which we coined Hyperrealism in previous work. In this use case, it
is of paramount importance to display objects like needles, sutures or
instruments consistent in both domains while altering the style to a more
tissue-like appearance. Segmentation of these objects would allow for a direct
transfer, however, contouring of these, partly tiny and thin foreground objects
is cumbersome and perhaps inaccurate. Instead, we propose to use landmark
detection on the points when sutures pass into the tissue. This objective is
directly incorporated into a CycleGAN framework by treating the performance of
pre-trained detector models as an additional optimization goal. We show that a
task defined on these sparse landmark labels improves consistency of synthesis
by the generator network in both domains. Comparing a baseline CycleGAN
architecture to our proposed extension (DetCycleGAN), mean precision (PPV)
improved by +61.32, mean sensitivity (TPR) by +37.91, and mean F1 score by
+0.4743. Furthermore, it could be shown that by dataset fusion, generated
intra-operative images can be leveraged as additional training data for the
detection network itself. The data is released within the scope of the AdaptOR
MICCAI Challenge 2021 at https://adaptor2021.github.io/, and code at
https://github.com/Cardio-AI/detcyclegan_pytorch.","['Lalith Sharan', 'Gabriele Romano', 'Sven Koehler', 'Halvar Kelm', 'Matthias Karck', 'Raffaele De Simone', 'Sandy Engelhardt']",2021-07-14T19:09:50Z,http://arxiv.org/abs/2107.06941v2,"['cs.CV', 'cs.AI']"
"High-speed object detection with a single-photon time-of-flight image
  sensor","3D time-of-flight (ToF) imaging is used in a variety of applications such as
augmented reality (AR), computer interfaces, robotics and autonomous systems.
Single-photon avalanche diodes (SPADs) are one of the enabling technologies
providing accurate depth data even over long ranges. By developing SPADs in
array format with integrated processing combined with pulsed, flood-type
illumination, high-speed 3D capture is possible. However, array sizes tend to
be relatively small, limiting the lateral resolution of the resulting depth
maps, and, consequently, the information that can be extracted from the image
for applications such as object detection. In this paper, we demonstrate that
these limitations can be overcome through the use of convolutional neural
networks (CNNs) for high-performance object detection. We present outdoor
results from a portable SPAD camera system that outputs 16-bin photon timing
histograms with 64x32 spatial resolution. The results, obtained with exposure
times down to 2 ms (equivalent to 500 FPS) and in signal-to-background (SBR)
ratios as low as 0.05, point to the advantages of providing the CNN with full
histogram data rather than point clouds alone. Alternatively, a combination of
point cloud and active intensity data may be used as input, for a similar level
of performance. In either case, the GPU-accelerated processing time is less
than 1 ms per frame, leading to an overall latency (image acquisition plus
processing) in the millisecond range, making the results relevant for
safety-critical computer vision applications which would benefit from faster
than human reaction times.","['Germán Mora-Martín', 'Alex Turpin', 'Alice Ruget', 'Abderrahim Halimi', 'Robert Henderson', 'Jonathan Leach', 'Istvan Gyongy']",2021-07-28T14:53:44Z,http://arxiv.org/abs/2107.13407v1,"['eess.IV', 'cs.CV', 'physics.optics']"
Evolution of NOMA Toward Next Generation Multiple Access (NGMA) for 6G,"Due to the explosive growth in the number of wireless devices and diverse
wireless services, such as virtual/augmented reality and
Internet-of-Everything, next generation wireless networks face unprecedented
challenges caused by heterogeneous data traffic, massive connectivity, and
ultra-high bandwidth efficiency and ultra-low latency requirements. To address
these challenges, advanced multiple access schemes are expected to be
developed, namely next generation multiple access (NGMA), which are capable of
supporting massive numbers of users in a more resource- and
complexity-efficient manner than existing multiple access schemes. As the
research on NGMA is in a very early stage, in this paper, we explore the
evolution of NGMA with a particular focus on non-orthogonal multiple access
(NOMA), i.e., the transition from NOMA to NGMA. In particular, we first review
the fundamental capacity limits of NOMA, elaborate on the new requirements for
NGMA, and discuss several possible candidate techniques. Moreover, given the
high compatibility and flexibility of NOMA, we provide an overview of current
research efforts on multi-antenna techniques for NOMA, promising future
application scenarios of NOMA, and the interplay between NOMA and other
emerging physical layer techniques. Furthermore, we discuss advanced
mathematical tools for facilitating the design of NOMA communication systems,
including conventional optimization approaches and new machine learning
techniques. Next, we propose a unified framework for NGMA based on multiple
antennas and NOMA, where both downlink and uplink transmissions are considered,
thus setting the foundation for this emerging research area. Finally, several
practical implementation challenges for NGMA are highlighted as motivation for
future work.","['Yuanwei Liu', 'Shuowen Zhang', 'Xidong Mu', 'Zhiguo Ding', 'Robert Schober', 'Naofal Al-Dhahir', 'Ekram Hossain', 'Xuemin Shen']",2021-08-10T10:15:54Z,http://arxiv.org/abs/2108.04561v2,"['cs.IT', 'eess.SP', 'math.IT']"
"Development of A Fully Data-Driven Artificial Intelligence and Deep
  Learning for URLLC Application in 6G Wireless Systems: A Survey","The full future of the sixth generation will develop a fully data-driven that
provide terabit rate per second, and adopt an average of 1000+ massive number
of connections per person in 10 years 2030 virtually instantaneously.
Data-driven for ultra-reliable and low latency communication is a new service
paradigm provided by a new application of future sixth-generation wireless
communication and network architecture, involving 100+ Gbps data rates with one
millisecond latency. The key constraint is the amount of computing power
available to spread massive data and well-designed artificial neural networks.
Artificial Intelligence provides a new technique to design wireless networks by
apply learning, predicting, and make decisions to manage the stream of big data
training individuals, which provides more the capacity to transform that expert
learning to develop the performance of wireless networks. We study the
developing technologies that will be the driving force are artificial
intelligence, communication systems to guarantee low latency. This paper aims
to discuss the efficiency of the developing network and alleviate the great
challenge for application scenarios and study Holographic radio, enhanced
wireless channel coding, enormous Internet of Things integration, and haptic
communication for virtual and augmented reality provide new services on the 6G
network. Furthermore, improving a multi-level architecture for ultra-reliable
and low latency in deep Learning allows for data-driven AI and 6G networks for
device intelligence, as well as allowing innovations based on effective
learning capabilities. These difficulties must be solved in order to meet the
needs of future smart networks. Furthermore, this research categorizes various
unexplored research gaps between machine learning and the sixth generation.","['Adeeb Salh', 'Lukman Audah', 'Qazwan Abdullah', 'Abdullah Noorsaliza', 'Nor Shahida Mohd Shah', 'Jameel Mukred', 'Shipun Hamzah']",2021-08-04T03:05:18Z,http://arxiv.org/abs/2108.10076v1,"['eess.SP', 'cs.CC']"
Cyber-Security in the Emerging World of Smart Everything,"The fourth industrial revolution (4IR) is a revolution many authors believe
have come to stay. It is a revolution that has been fast blurring the line
between physical, digital and biological technologies. These disruptive
technologies largely rely on high-speed internet connectivity, Cloud
technologies, Augmented Reality, Additive Manufacturing, Data science and
Artificial Intelligence. Most developed economies have embraced the it while
the developing economies are struggling to adopt 4IR because they lack the
requisite skills, knowledge and technology. Thus, this study investigates
Nigeria as one of the developing economies to understand her readiness for 4IR
and the level of preparedness to mitigate the sophisticated cyber-attacks that
comes with it. The investigation adopted quantitative research approach and
developed an online questionnaire that was shared amongst the population of
interest that includes academic, industry experts and relevant stakeholders.
The questionnaire returned 116 valid responses which were analysed with
descriptive statistical tools in SPSS. Results suggest that 60 of the
respondents opined that Nigerian government at are not showing enough evidence
to demonstrate her preparedness to leverage these promised potentials by
developing 4IR relevant laws, strong institutional frameworks and policies.
They lack significant development capacity to mitigate risks associated with
digital ecosystem and cyber ecosystem that are ushered in by the 4IR. In the
universities, 52 of the courses offered at the undergraduate and 42 at the
post-graduate levels are relevant in the development of skills required in the
revolution. The study recommends that the government at all levels make
adequate efforts in developing the countrys intangible assets. In all, this
paper posits that successful implementation of these could equip Nigeria to
embrace the 4IR in all its aspects.","['Elochukwu A. Ukwandu', 'Ephraim N. C. Okafor', 'Charles Ikerionwu', 'Comfort Olebara', 'Celestine Ugwu']",2021-09-13T09:46:50Z,http://arxiv.org/abs/2109.05821v1,['cs.CY']
"3D Hand Pose and Shape Estimation from RGB Images for Keypoint-Based
  Hand Gesture Recognition","Estimating the 3D pose of a hand from a 2D image is a well-studied problem
and a requirement for several real-life applications such as virtual reality,
augmented reality, and hand gesture recognition. Currently, reasonable
estimations can be computed from single RGB images, especially when a
multi-task learning approach is used to force the system to consider the shape
of the hand when its pose is determined. However, depending on the method used
to represent the hand, the performance can drop considerably in real-life
tasks, suggesting that stable descriptions are required to achieve satisfactory
results. In this paper, we present a keypoint-based end-to-end framework for 3D
hand and pose estimation and successfully apply it to the task of hand gesture
recognition as a study case. Specifically, after a pre-processing step in which
the images are normalized, the proposed pipeline uses a multi-task semantic
feature extractor generating 2D heatmaps and hand silhouettes from RGB images,
a viewpoint encoder to predict the hand and camera view parameters, a stable
hand estimator to produce the 3D hand pose and shape, and a loss function to
guide all of the components jointly during the learning phase. Tests were
performed on a 3D pose and shape estimation benchmark dataset to assess the
proposed framework, which obtained state-of-the-art performance. Our system was
also evaluated on two hand-gesture recognition benchmark datasets and
significantly outperformed other keypoint-based approaches, indicating that it
is an effective solution that is able to generate stable 3D estimates for hand
pose and shape.","['Danilo Avola', 'Luigi Cinque', 'Alessio Fagioli', 'Gian Luca Foresti', 'Adriano Fragomeni', 'Daniele Pannone']",2021-09-28T17:07:43Z,http://arxiv.org/abs/2109.13879v2,['cs.CV']
"All One Needs to Know about Metaverse: A Complete Survey on
  Technological Singularity, Virtual Ecosystem, and Research Agenda","Since the popularisation of the Internet in the 1990s, the cyberspace has
kept evolving. We have created various computer-mediated virtual environments
including social networks, video conferencing, virtual 3D worlds (e.g., VR
Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible
Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and
unconnected, have bought us various degrees of digital transformation. The term
`metaverse' has been coined to further facilitate the digital transformation in
every aspect of our physical lives. At the core of the metaverse stands the
vision of an immersive Internet as a gigantic, unified, persistent, and shared
realm. While the metaverse may seem futuristic, catalysed by emerging
technologies such as Extended Reality, 5G, and Artificial Intelligence, the
digital `big bang' of our cyberspace is not far away. This survey paper
presents the first effort to offer a comprehensive framework that examines the
latest metaverse development under the dimensions of state-of-the-art
technologies and metaverse ecosystems, and illustrates the possibility of the
digital `big bang'. First, technologies are the enablers that drive the
transition from the current Internet to the metaverse. We thus examine eight
enabling technologies rigorously - Extended Reality, User Interactivity
(Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer
Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks.
In terms of applications, the metaverse ecosystem allows human users to live
and play within a self-sustaining, persistent, and shared realm. Therefore, we
discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy,
Social Acceptability, Security and Privacy, and Trust and Accountability.
Finally, we propose a concrete research agenda for the development of the
metaverse.","['Lik-Hang Lee', 'Tristan Braud', 'Pengyuan Zhou', 'Lin Wang', 'Dianlei Xu', 'Zijun Lin', 'Abhishek Kumar', 'Carlos Bermejo', 'Pan Hui']",2021-10-06T07:44:52Z,http://arxiv.org/abs/2110.05352v3,"['cs.CY', 'A.1; K.0']"
"On the Effect of Selfie Beautification Filters on Face Detection and
  Recognition","Beautification and augmented reality filters are very popular in applications
that use selfie images captured with smartphones or personal devices. However,
they can distort or modify biometric features, severely affecting the
capability of recognizing individuals' identity or even detecting the face.
Accordingly, we address the effect of such filters on the accuracy of automated
face detection and recognition. The social media image filters studied either
modify the image contrast or illumination or occlude parts of the face with for
example artificial glasses or animal noses. We observe that the effect of some
of these filters is harmful both to face detection and identity recognition,
specially if they obfuscate the eye or (to a lesser extent) the nose. To
counteract such effect, we develop a method to reconstruct the applied
manipulation with a modified version of the U-NET segmentation network. This is
observed to contribute to a better face detection and recognition accuracy.
From a recognition perspective, we employ distance measures and trained machine
learning algorithms applied to features extracted using a ResNet-34 network
trained to recognize faces. We also evaluate if incorporating filtered images
to the training set of machine learning approaches are beneficial for identity
recognition. Our results show good recognition when filters do not occlude
important landmarks, specially the eyes (identification accuracy >99%, EER<2%).
The combined effect of the proposed approaches also allow to mitigate the
effect produced by filters that occlude parts of the face, achieving an
identification accuracy of >92% with the majority of perturbations evaluated,
and an EER <8%. Although there is room for improvement, when neither U-NET
reconstruction nor training with filtered images is applied, the accuracy with
filters that severely occlude the eye is <72% (identification) and >12% (EER)","['Pontus Hedman', 'Vasilios Skepetzis', 'Kevin Hernandez-Diaz', 'Josef Bigun', 'Fernando Alonso-Fernandez']",2021-10-17T22:10:56Z,http://arxiv.org/abs/2110.08934v4,['cs.CV']
"Point detection through multi-instance deep heatmap regression for
  sutures in endoscopy","Purpose: Mitral valve repair is a complex minimally invasive surgery of the
heart valve. In this context, suture detection from endoscopic images is a
highly relevant task that provides quantitative information to analyse suturing
patterns, assess prosthetic configurations and produce augmented reality
visualisations. Facial or anatomical landmark detection tasks typically contain
a fixed number of landmarks, and use regression or fixed heatmap-based
approaches to localize the landmarks. However in endoscopy, there are a varying
number of sutures in every image, and the sutures may occur at any location in
the annulus, as they are not semantically unique. Method: In this work, we
formulate the suture detection task as a multi-instance deep heatmap regression
problem, to identify entry and exit points of sutures. We extend our previous
work, and introduce the novel use of a 2D Gaussian layer followed by a
differentiable 2D spatial Soft-Argmax layer to function as a local non-maximum
suppression. Results: We present extensive experiments with multiple heatmap
distribution functions and two variants of the proposed model. In the
intra-operative domain, Variant 1 showed a mean F1 of +0.0422 over the
baseline. Similarly, in the simulator domain, Variant 1 showed a mean F1 of
+0.0865 over the baseline. Conclusion: The proposed model shows an improvement
over the baseline in the intra-operative and the simulator domains. The data is
made publicly available within the scope of the MICCAI AdaptOR2021 Challenge
https://adaptor2021.github.io/, and the code at
https://github.com/Cardio-AI/suture-detection-pytorch/.
DOI:10.1007/s11548-021-02523-w. The link to the open access article can be
found here: https://link.springer.com/article/10.1007%2Fs11548-021-02523-w","['Lalith Sharan', 'Gabriele Romano', 'Julian Brand', 'Halvar Kelm', 'Matthias Karck', 'Raffaele De Simone', 'Sandy Engelhardt']",2021-11-16T13:45:23Z,http://arxiv.org/abs/2111.08468v1,"['cs.CV', 'cs.AI']"
"Edge computing for cyber-physical systems: A systematic mapping study
  emphasizing trustworthiness","Edge computing is projected to have profound implications in the coming
decades, proposed to provide solutions for applications such as augmented
reality, predictive functionalities, and collaborative Cyber-Physical Systems
(CPS). For such applications, edge computing addresses the new computational
needs, as well as privacy, availability, and real-time constraints, by
providing local high-performance computing capabilities to deal with the
limitations and constraints of cloud and embedded systems. Our interests lie in
the applications of edge computing as part of CPS, where several properties (or
attributes) of trustworthiness, including safety, security, and
predictability/availability are of particular concern, each facing challenges
for the introduction of edge-based CPS. We present the results of a systematic
mapping study, a kind of systematic literature survey, investigating the use of
edge computing for CPS with a special emphasis on trustworthiness. The main
contributions of this study are a detailed description of the current research
efforts in edge-based CPS and the identification and discussion of trends and
research gaps. The results show that the main body of research in edge-based
CPS only to a very limited extent consider key attributes of system
trustworthiness, despite many efforts referring to critical CPS and
applications like intelligent transportation. More research and industrial
efforts will be needed on aspects of trustworthiness of future edge-based CPS
including their experimental evaluation. Such research needs to consider the
multiple interrelated attributes of trustworthiness including safety, security,
and predictability, and new methodologies and architectures to address them. It
is further important to provide bridges and collaboration between edge
computing and CPS disciplines.","['José Manuel Gaspar Sánchez', 'Nils Jörgensen', 'Martin Törngren', 'Rafia Inam', 'Andrii Berezovskyi', 'Lei Feng', 'Elena Fersman', 'Muhammad Rusyadi Ramli', 'Kaige Tan']",2021-11-26T11:48:55Z,http://arxiv.org/abs/2112.00619v1,"['cs.DC', 'cs.NI']"
Pose Estimation of Specific Rigid Objects,"In this thesis, we address the problem of estimating the 6D pose of rigid
objects from a single RGB or RGB-D input image, assuming that 3D models of the
objects are available. This problem is of great importance to many application
fields such as robotic manipulation, augmented reality, and autonomous driving.
First, we propose EPOS, a method for 6D object pose estimation from an RGB
image. The key idea is to represent an object by compact surface fragments and
predict the probability distribution of corresponding fragments at each pixel
of the input image by a neural network. Each pixel is linked with a
data-dependent number of fragments, which allows systematic handling of
symmetries, and the 6D poses are estimated from the links by a RANSAC-based
fitting method. EPOS outperformed all RGB and most RGB-D and D methods on
several standard datasets. Second, we present HashMatch, an RGB-D method that
slides a window over the input image and searches for a match against
templates, which are pre-generated by rendering 3D object models in different
orientations. The method applies a cascade of evaluation stages to each window
location, which avoids exhaustive matching against all templates. Third, we
propose ObjectSynth, an approach to synthesize photorealistic images of 3D
object models for training methods based on neural networks. The images yield
substantial improvements compared to commonly used images of objects rendered
on top of random photographs. Fourth, we introduce T-LESS, the first dataset
for 6D object pose estimation that includes 3D models and RGB-D images of
industry-relevant objects. Fifth, we define BOP, a benchmark that captures the
status quo in the field. BOP comprises eleven datasets in a unified format, an
evaluation methodology, an online evaluation system, and public challenges held
at international workshops organized at the ICCV and ECCV conferences.",['Tomas Hodan'],2021-12-30T14:36:47Z,http://arxiv.org/abs/2112.15075v1,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.LG', 'cs.RO']"
"The Hitchhiker's Guide to Fused Twins: A Review of Access to Digital
  Twins in situ in Smart Cities","Smart Cities already surround us, and yet they are still incomprehensibly far
from directly impacting everyday life. While current Smart Cities are often
inaccessible, the experience of everyday citizens may be enhanced with a
combination of the emerging technologies Digital Twins (DTs) and Situated
Analytics. DTs represent their Physical Twin (PT) in the real world via models,
simulations, (remotely) sensed data, context awareness, and interactions.
However, interaction requires appropriate interfaces to address the complexity
of the city. Ultimately, leveraging the potential of Smart Cities requires
going beyond assembling the DT to be comprehensive and accessible. Situated
Analytics allows for the anchoring of city information in its spatial context.
We advance the concept of embedding the DT into the PT through Situated
Analytics to form Fused Twins (FTs). This fusion allows access to data in the
location that it is generated in an embodied context that can make the data
more understandable. Prototypes of FTs are rapidly emerging from different
domains, but Smart Cities represent the context with the most potential for FTs
in the future. This paper reviews DTs, Situated Analytics, and Smart Cities as
the foundations of FTs. Regarding DTs, we define five components (Physical,
Data, Analytical, Virtual, and Connection environments) that we relate to
several cognates (i.e., similar but different terms) from existing literature.
Regarding Situated Analytics, we review the effects of user embodiment on
cognition and cognitive load. Finally, we classify existing partial examples of
FTs from the literature and address their construction from Augmented Reality,
Geographic Information Systems, Building/City Information Models, and DTs and
provide an overview of future direction","['Jascha Grübel', 'Tyler Thrash', 'Leonel Aguilar', 'Michal Gath-Morad', 'Julia Chatain', 'Robert W. Sumner', 'Christoph Hölscher', 'Victor R. Schinazi']",2022-02-15T00:17:40Z,http://arxiv.org/abs/2202.07104v2,"['cs.CY', 'cs.GL', 'cs.HC']"
"Multi-Agent Task Assignment in Vehicular Edge Computing: A
  Regret-Matching Learning-Based Approach","Vehicular edge computing has recently been proposed to support
computation-intensive applications in Intelligent Transportation Systems (ITS)
such as self-driving cars and augmented reality. Despite progress in this area,
significant challenges remain to efficiently allocate limited computation
resources to a range of time-critical ITS tasks. To this end, the current paper
develops a new task assignment scheme for vehicles in a highway. Because of the
high speed of vehicles and the limited communication range of road side units
(RSUs), the computation tasks of participating vehicles are to be dynamically
migrated across multiple servers. We formulate a binary nonlinear programming
(BNLP) problem of assigning computation tasks from vehicles to RSUs and a
macrocell base station. To deal with the potentially large size of the
formulated optimization problem, we develop a distributed multi-agent
regret-matching learning algorithm. Based on the regret minimization principle,
the proposed algorithm employs a forgetting method that allows the learning
process to quickly adapt to and effectively handle the high mobility feature of
vehicle networks. We theoretically prove that it converges to the correlated
equilibrium solutions of the considered BNLP problem. Simulation results with
practical parameter settings show that the proposed algorithm offers the lowest
total delay and cost of processing tasks, as well as utility fairness among
agents. Importantly, our algorithm converges much faster than existing methods
as the problem size grows, demonstrating its clear advantage in large-scale
vehicular networks.","['Bach Long Nguyen', 'Duong D. Nguyen', 'Hung X. Nguyen', 'Duy T. Ngo', 'Markus Wagner']",2022-03-10T10:49:52Z,http://arxiv.org/abs/2203.05281v3,"['eess.SY', 'cs.SY']"
"CaRTS: Causality-driven Robot Tool Segmentation from Vision and
  Kinematics Data","Vision-based segmentation of the robotic tool during robot-assisted surgery
enables downstream applications, such as augmented reality feedback, while
allowing for inaccuracies in robot kinematics. With the introduction of deep
learning, many methods were presented to solve instrument segmentation directly
and solely from images. While these approaches made remarkable progress on
benchmark datasets, fundamental challenges pertaining to their robustness
remain. We present CaRTS, a causality-driven robot tool segmentation algorithm,
that is designed based on a complementary causal model of the robot tool
segmentation task. Rather than directly inferring segmentation masks from
observed images, CaRTS iteratively aligns tool models with image observations
by updating the initially incorrect robot kinematic parameters through forward
kinematics and differentiable rendering to optimize image feature similarity
end-to-end. We benchmark CaRTS with competing techniques on both synthetic as
well as real data from the dVRK, generated in precisely controlled scenarios to
allow for counterfactual synthesis. On training-domain test data, CaRTS
achieves a Dice score of 93.4 that is preserved well (Dice score of 91.8) when
tested on counterfactually altered test data, exhibiting low brightness, smoke,
blood, and altered background patterns. This compares favorably to Dice scores
of 95.0 and 86.7, respectively, of the SOTA image-based method. Future work
will involve accelerating CaRTS to achieve video framerate and estimating the
impact occlusion has in practice. Despite these limitations, our results are
promising: In addition to achieving high segmentation accuracy, CaRTS provides
estimates of the true robot kinematics, which may benefit applications such as
force estimation. Code is available at: https://github.com/hding2455/CaRTS","['Hao Ding', 'Jintan Zhang', 'Peter Kazanzides', 'Jie Ying Wu', 'Mathias Unberath']",2022-03-15T22:26:19Z,http://arxiv.org/abs/2203.09475v3,"['cs.RO', 'cs.CV']"
"Millimeter-Wave Sensing for Avoidance of High-Risk Ground Conditions for
  Mobile Robots","Mobile robot autonomy has made significant advances in recent years, with
navigation algorithms well developed and used commercially in certain
well-defined environments, such as warehouses. The common link in usage
scenarios is that the environments in which the robots are utilized have a high
degree of certainty. Operating environments are often designed to be robot
friendly, for example augmented reality markers are strategically placed and
the ground is typically smooth, level, and clear of debris. For robots to be
useful in a wider range of environments, especially environments that are not
sanitized for their use, robots must be able to handle uncertainty. This
requires a robot to incorporate new sensors and sources of information, and to
be able to use this information to make decisions regarding navigation and the
overall mission. When using autonomous mobile robots in unstructured and poorly
defined environments, such as a natural disaster site or in a rural
environment, ground condition is of critical importance and is a common cause
of failure. Examples include loss of traction due to high levels of ground
water, hidden cavities, or material boundary failures. To evaluate a
non-contact sensing method to mitigate these risks, Frequency Modulated
Continuous Wave (FMCW) radar is integrated with an Unmanned Ground Vehicle
(UGV), representing a novel application of FMCW to detect new measurands for
Robotic Autonomous Systems (RAS) navigation, informing on terrain integrity and
adding to the state-of-the-art in sensing for optimized autonomous path
planning. In this paper, the FMCW is first evaluated in a desktop setting to
determine its performance in anticipated ground conditions. The FMCW is then
fixed to a UGV and the sensor system is tested and validated in a
representative environment containing regions with significant levels of ground
water saturation.","['Jamie Blanche', 'Shivoh Chirayil Nandakumar', 'Daniel Mitchell', 'Sam Harper', 'Keir Groves', 'Andrew West', 'Barry Lennox', 'Simon Watson', 'David Flynn', 'Ikuo Yamamoto']",2022-03-30T10:02:11Z,http://arxiv.org/abs/2203.16180v1,['cs.RO']
Extended Reality for Mental Health Evaluation -A Scoping Review,"Mental health disorders are the leading cause of health-related problems
globally. It is projected that mental health disorders will be the leading
cause of morbidity among adults as the incidence rates of anxiety and
depression grows globally. Recently, extended reality (XR), a general term
covering virtual reality (VR), augmented reality (AR) and mixed reality (MR),
is paving a new way to deliver mental health care. In this paper, we conduct a
scoping review on the development and application of XR in the area of mental
disorders. We performed a scoping database search to identify the relevant
studies indexed in Google Scholar, PubMed, and the ACM Digital Library. A
search period between August 2016 and December 2023 was defined to select
articles related to the usage of VR, AR, and MR in a mental health context. We
identified a total of 85 studies from 27 countries across the globe. By
performing data analysis, we found that most of the studies focused on
developed countries such as the US (16.47%) and Germany (12.94%). None of the
studies were for African countries. The majority of the articles reported that
XR techniques led to a significant reduction in symptoms of anxiety or
depression. More studies were published in the year 2021, i.e., 31.76% (n =
31). This could indicate that mental disorder intervention received a higher
attention when COVID-19 emerged. Most studies (n = 65) focused on a population
between 18 and 65 years old, only a few studies focused on teenagers (n = 2).
Also, more studies were done experimentally (n = 67, 78.82%) rather than by
analytical and modeling approaches (n = 8, 9.41%). This shows that there is a
rapid development of XR technology for mental health care. Furthermore, these
studies showed that XR technology can effectively be used for evaluating mental
disorders in similar or better way as the conventional approaches.","['Omisore Olatunji', 'Ifeanyi Odenigbo', 'Joseph Orji', 'Amelia Beltran', 'Nilufar Baghaei', 'Meier Sandra', 'Rita Orji']",2022-04-04T09:46:30Z,http://arxiv.org/abs/2204.01348v2,"['cs.HC', 'cs.CV']"
Updating Industrial Robots for Emerging Technologies,"Industrial arms need to evolve beyond their standard shape to embrace new and
emerging technologies. In this paper, we shall first perform an analysis of
four popular but different modern industrial robot arms. By seeing the common
trends we will try to extrapolate and expand these trends for the future. Here,
particular focus will be on interaction based on augmented reality (AR) through
head-mounted displays (HMD), but also through smartphones. Long-term
human-robot interaction and personalization of said interaction will also be
considered. The use of AR in human-robot interaction has proven to enhance
communication and information exchange. A basic addition to industrial arm
design would be the integration of QR markers on the robot, both for accessing
information and adding tracking capabilities to more easily display AR
overlays. In a recent example of information access, Mercedes Benz added QR
markers on their cars to help rescue workers estimate the best places to cut
and evacuate people after car crashes. One has also to deal with safety in an
environment that will be more and more about collaboration. The QR markers can
therefore be combined with RF-based ranging modules, developed in the
EU-project SafeLog, that can be used both for safety as well as for tracking of
human positions while in close proximity interactions with the industrial arms.
The industrial arms of the future should also be intuitive to program and
interact with. This would be achieved through AR and head mounted displays as
well as the already mentioned RF-based person tracking. Finally, a more
personalized interaction between the robots and humans can be achieved through
life-long learning AI and disembodied, personalized agents. We propose a design
that not only exists in the physical world, but also partly in the digital
world of mixed reality.","['David Puljiz', 'Björn Hein']",2022-04-07T16:08:02Z,http://arxiv.org/abs/2204.03538v2,['cs.RO']
"Investigating the Role of Image Retrieval for Visual Localization -- An
  exhaustive benchmark","Visual localization, i.e., camera pose estimation in a known scene, is a core
component of technologies such as autonomous driving and augmented reality.
State-of-the-art localization approaches often rely on image retrieval
techniques for one of two purposes: (1) provide an approximate pose estimate or
(2) determine which parts of the scene are potentially visible in a given query
image. It is common practice to use state-of-the-art image retrieval algorithms
for both of them. These algorithms are often trained for the goal of retrieving
the same landmark under a large range of viewpoint changes which often differs
from the requirements of visual localization. In order to investigate the
consequences for visual localization, this paper focuses on understanding the
role of image retrieval for multiple visual localization paradigms. First, we
introduce a novel benchmark setup and compare state-of-the-art retrieval
representations on multiple datasets using localization performance as metric.
Second, we investigate several definitions of ""ground truth"" for image
retrieval. Using these definitions as upper bounds for the visual localization
paradigms, we show that there is still sgnificant room for improvement. Third,
using these tools and in-depth analysis, we show that retrieval performance on
classical landmark retrieval or place recognition tasks correlates only for
some but not all paradigms to localization performance. Finally, we analyze the
effects of blur and dynamic scenes in the images. We conclude that there is a
need for retrieval approaches specifically designed for localization paradigms.
Our benchmark and evaluation protocols are available at
https://github.com/naver/kapture-localization.","['Martin Humenberger', 'Yohann Cabon', 'Noé Pion', 'Philippe Weinzaepfel', 'Donghwan Lee', 'Nicolas Guérin', 'Torsten Sattler', 'Gabriela Csurka']",2022-05-31T12:59:01Z,http://arxiv.org/abs/2205.15761v1,"['cs.CV', 'cs.AI', 'cs.LG']"
"EyeCoD: Eye Tracking System Acceleration via FlatCam-based Algorithm &
  Accelerator Co-Design","Eye tracking has become an essential human-machine interaction modality for
providing immersive experience in numerous virtual and augmented reality
(VR/AR) applications desiring high throughput (e.g., 240 FPS), small-form, and
enhanced visual privacy. However, existing eye tracking systems are still
limited by their: (1) large form-factor largely due to the adopted bulky
lens-based cameras; and (2) high communication cost required between the camera
and backend processor, thus prohibiting their more extensive applications. To
this end, we propose a lensless FlatCam-based eye tracking algorithm and
accelerator co-design framework dubbed EyeCoD to enable eye tracking systems
with a much reduced form-factor and boosted system efficiency without
sacrificing the tracking accuracy, paving the way for next-generation eye
tracking solutions. On the system level, we advocate the use of lensless
FlatCams to facilitate the small form-factor need in mobile eye tracking
systems. On the algorithm level, EyeCoD integrates a predict-then-focus
pipeline that first predicts the region-of-interest (ROI) via segmentation and
then only focuses on the ROI parts to estimate gaze directions, greatly
reducing redundant computations and data movements. On the hardware level, we
further develop a dedicated accelerator that (1) integrates a novel workload
orchestration between the aforementioned segmentation and gaze estimation
models, (2) leverages intra-channel reuse opportunities for depth-wise layers,
and (3) utilizes input feature-wise partition to save activation memory size.
On-silicon measurement validates that our EyeCoD consistently reduces both the
communication and computation costs, leading to an overall system speedup of
10.95x, 3.21x, and 12.85x over CPUs, GPUs, and a prior-art eye tracking
processor called CIS-GEP, respectively, while maintaining the tracking
accuracy.","['Haoran You', 'Cheng Wan', 'Yang Zhao', 'Zhongzhi Yu', 'Yonggan Fu', 'Jiayi Yuan', 'Shang Wu', 'Shunyao Zhang', 'Yongan Zhang', 'Chaojian Li', 'Vivek Boominathan', 'Ashok Veeraraghavan', 'Ziyun Li', 'Yingyan Lin']",2022-06-02T05:35:43Z,http://arxiv.org/abs/2206.00877v2,"['cs.HC', 'cs.AR']"
"PixSelect: Less but Reliable Pixels for Accurate and Efficient
  Localization","Accurate camera pose estimation is a fundamental requirement for numerous
applications, such as autonomous driving, mobile robotics, and augmented
reality. In this work, we address the problem of estimating the global 6 DoF
camera pose from a single RGB image in a given environment. Previous works
consider every part of the image valuable for localization. However, many image
regions such as the sky, occlusions, and repetitive non-distinguishable
patterns cannot be utilized for localization. In addition to adding unnecessary
computation efforts, extracting and matching features from such regions produce
many wrong matches which in turn degrades the localization accuracy and
efficiency. Our work addresses this particular issue and shows by exploiting an
interesting concept of sparse 3D models that we can exploit discriminatory
environment parts and avoid useless image regions for the sake of a single
image localization. Interestingly, through avoiding selecting keypoints from
non-reliable image regions such as trees, bushes, cars, pedestrians, and
occlusions, our work acts naturally as an outlier filter. This makes our system
highly efficient in that minimal set of correspondences is needed and highly
accurate as the number of outliers is low. Our work exceeds state-ofthe-art
methods on outdoor Cambridge Landmarks dataset. With only relying on single
image at inference, it outweighs in terms of accuracy methods that exploit pose
priors and/or reference 3D models while being much faster. By choosing as
little as 100 correspondences, it surpasses similar methods that localize from
thousands of correspondences, while being more efficient. In particular, it
achieves, compared to these methods, an improvement of localization by 33% on
OldHospital scene. Furthermore, It outstands direct pose regressors even those
that learn from sequence of images",['Mohammad Altillawi'],2022-06-08T09:46:03Z,http://arxiv.org/abs/2206.03775v1,"['cs.CV', 'cs.RO']"
"DeepFormableTag: End-to-end Generation and Recognition of Deformable
  Fiducial Markers","Fiducial markers have been broadly used to identify objects or embed messages
that can be detected by a camera. Primarily, existing detection methods assume
that markers are printed on ideally planar surfaces. Markers often fail to be
recognized due to various imaging artifacts of optical/perspective distortion
and motion blur. To overcome these limitations, we propose a novel deformable
fiducial marker system that consists of three main parts: First, a fiducial
marker generator creates a set of free-form color patterns to encode
significantly large-scale information in unique visual codes. Second, a
differentiable image simulator creates a training dataset of photorealistic
scene images with the deformed markers, being rendered during optimization in a
differentiable manner. The rendered images include realistic shading with
specular reflection, optical distortion, defocus and motion blur, color
alteration, imaging noise, and shape deformation of markers. Lastly, a trained
marker detector seeks the regions of interest and recognizes multiple marker
patterns simultaneously via inverse deformation transformation. The deformable
marker creator and detector networks are jointly optimized via the
differentiable photorealistic renderer in an end-to-end manner, allowing us to
robustly recognize a wide range of deformable markers with high accuracy. Our
deformable marker system is capable of decoding 36-bit messages successfully at
~29 fps with severe shape deformation. Results validate that our system
significantly outperforms the traditional and data-driven marker methods. Our
learning-based marker system opens up new interesting applications of fiducial
markers, including cost-effective motion capture of the human body, active 3D
scanning using our fiducial markers' array as structured light patterns, and
robust augmented reality rendering of virtual objects on dynamic surfaces.","['Mustafa B. Yaldiz', 'Andreas Meuleman', 'Hyeonjoong Jang', 'Hyunho Ha', 'Min H. Kim']",2022-06-16T09:29:26Z,http://arxiv.org/abs/2206.08026v1,"['cs.CV', 'cs.GR']"
"Action-Specific Perception & Performance on a Fitts's Law Task in
  Virtual Reality: The Role of Haptic Feedback","While user's perception & performance are predominantly examined
independently in virtual reality, the Action-Specific Perception (ASP) theory
postulates that the performance of an individual on a task modulates this
individual's spatial & time perception pertinent to the task's components &
procedures. This paper examines the association between performance &
perception & the potential effects that tactile feedback modalities could
generate. This paper reports a user study (N=24), in which participants
performed a Fitts's law target acquisition task by using three feedback
modalities: visual, visuo-electrotactile, & visuo-vibrotactile. The users
completed 3 Target Sizes X 2 Distances X 3 feedback modalities = 18 trials. The
size perception, distance perception, & (movement) time perception were
assessed at the end of each trial. Performance-wise, the results showed that
electrotactile feedback facilitates a significantly better accuracy compared to
vibrotactile & visual feedback, while vibrotactile provided the worst accuracy.
Electrotactile & visual feedback enabled a comparable reaction time, while the
vibrotactile offered a substantially slower reaction time than visual feedback.
Although amongst feedback types the pattern of differences in perceptual
aspects were comparable to performance differences, none of them was
statistically significant. However, performance indeed modulated perception.
Significant action-specific effects on spatial & time perception were detected.
Changes in accuracy modulate both size perception & time perception, while
changes in movement speed modulate distance perception. Also, the index of
difficulty was found to modulate perception. These outcomes highlighted the
importance of haptic feedback on performance, & importantly the significance of
action-specific effects on spatial & time perception in VR, which should be
considered in future VR studies.","['Panagiotis Kourtesis', 'Sebastian Vizcay', 'Maud Marchal', 'Claudio Pacchierotti', 'Ferran Argelaguet']",2022-07-15T11:07:15Z,http://arxiv.org/abs/2207.07400v2,"['cs.HC', 'cs.CY', 'I.3.7; H.5.2; J.3; J.7']"
Leaky-wave metasurfaces for integrated photonics,"Metasurfaces have been rapidly advancing our command over the many degrees of
freedom of light within compact, lightweight devices. However, so far, they
have mostly been limited to manipulating light in free space. Grating couplers
provide the opportunity of bridging far-field optical radiation and in-plane
guided waves, and thus have become fundamental building blocks in photonic
integrated circuits. However, their operation and degree of light control is
much more limited than metasurfaces. Metasurfaces integrated on top of guided
wave photonic systems have been explored to control the scattering of light
off-chip with enhanced functionalities - namely, point-by-point manipulation of
amplitude, phase or polarization. However, these efforts have so far been
limited to controlling one or two optical degrees of freedom at best, and to
device configurations much more complex compared to conventional grating
couplers. Here, we introduce leaky-wave metasurfaces, which are based on
symmetry-broken photonic crystal slabs that support quasi-bound states in the
continuum. This platform has a compact form factor equivalent to the one of
conventional grating couplers, but it provides full command over amplitude,
phase and polarization (four optical degrees of freedom) across large
apertures. We present experimental demonstrations of various functionalities
for operation at wavelengths near 1.55 microns based on leaky-wave
metasurfaces, including devices for phase and amplitude control at a fixed
polarization state, and devices controlling all four optical degrees of
freedom. Our results merge the fields of guided and free-space optics under the
umbrella of metasurfaces, exploiting the hybrid nature of quasi-bound states in
the continuum, for opportunities to advance in disruptive ways imaging,
communications, augmented reality, quantum optics, LIDAR and integrated
photonic systems.","['Heqing Huang', 'Adam C. Overvig', 'Yuan Xu', 'Stephanie C. Malek', 'Cheng-Chia Tsai', 'Andrea Alù', 'Nanfang Yu']",2022-07-18T20:43:06Z,http://arxiv.org/abs/2207.08936v1,"['physics.optics', 'physics.app-ph']"
"Temporal View Synthesis of Dynamic Scenes through 3D Object Motion
  Estimation with Multi-Plane Images","The challenge of graphically rendering high frame-rate videos on low compute
devices can be addressed through periodic prediction of future frames to
enhance the user experience in virtual reality applications. This is studied
through the problem of temporal view synthesis (TVS), where the goal is to
predict the next frames of a video given the previous frames and the head poses
of the previous and the next frames. In this work, we consider the TVS of
dynamic scenes in which both the user and objects are moving. We design a
framework that decouples the motion into user and object motion to effectively
use the available user motion while predicting the next frames. We predict the
motion of objects by isolating and estimating the 3D object motion in the past
frames and then extrapolating it. We employ multi-plane images (MPI) as a 3D
representation of the scenes and model the object motion as the 3D displacement
between the corresponding points in the MPI representation. In order to handle
the sparsity in MPIs while estimating the motion, we incorporate partial
convolutions and masked correlation layers to estimate corresponding points.
The predicted object motion is then integrated with the given user or camera
motion to generate the next frame. Using a disocclusion infilling module, we
synthesize the regions uncovered due to the camera and object motion. We
develop a new synthetic dataset for TVS of dynamic scenes consisting of 800
videos at full HD resolution. We show through experiments on our dataset and
the MPI Sintel dataset that our model outperforms all the competing methods in
the literature.","['Nagabhushan Somraj', 'Pranali Sancheti', 'Rajiv Soundararajan']",2022-08-19T17:40:13Z,http://arxiv.org/abs/2208.09463v1,['cs.CV']
"A semi-supervised Teacher-Student framework for surgical tool detection
  and localization","Surgical tool detection in minimally invasive surgery is an essential part of
computer-assisted interventions. Current approaches are mostly based on
supervised methods which require large fully labeled data to train supervised
models and suffer from pseudo label bias because of class imbalance issues.
However large image datasets with bounding box annotations are often scarcely
available. Semi-supervised learning (SSL) has recently emerged as a means for
training large models using only a modest amount of annotated data; apart from
reducing the annotation cost. SSL has also shown promise to produce models that
are more robust and generalizable. Therefore, in this paper we introduce a
semi-supervised learning (SSL) framework in surgical tool detection paradigm
which aims to mitigate the scarcity of training data and the data imbalance
through a knowledge distillation approach. In the proposed work, we train a
model with labeled data which initialises the Teacher-Student joint learning,
where the Student is trained on Teacher-generated pseudo labels from unlabeled
data. We propose a multi-class distance with a margin based classification loss
function in the region-of-interest head of the detector to effectively
segregate foreground classes from background region. Our results on
m2cai16-tool-locations dataset indicate the superiority of our approach on
different supervised data settings (1%, 2%, 5%, 10% of annotated data) where
our model achieves overall improvements of 8%, 12% and 27% in mAP (on 1%
labeled data) over the state-of-the-art SSL methods and a fully supervised
baseline, respectively. The code is available at
https://github.com/Mansoor-at/Semi-supervised-surgical-tool-det","['Mansoor Ali', 'Gilberto Ochoa-Ruiz', 'Sharib Ali']",2022-08-21T17:21:31Z,http://arxiv.org/abs/2208.09926v1,"['cs.CV', 'cs.LG']"
Leaning-Based Control of an Immersive-Telepresence Robot,"In this paper, we present an implementation of a leaning-based control of a
differential drive telepresence robot and a user study in simulation, with the
goal of bringing the same functionality to a real telepresence robot. The
participants used a balance board to control the robot and viewed the virtual
environment through a head-mounted display. The main motivation for using a
balance board as the control device stems from Virtual Reality (VR) sickness;
even small movements of your own body matching the motions seen on the screen
decrease the sensory conflict between vision and vestibular organs, which lies
at the heart of most theories regarding the onset of VR sickness. To test the
hypothesis that the balance board as a control method would be less sickening
than using joysticks, we designed a user study (N=32, 15 women) in which the
participants drove a simulated differential drive robot in a virtual
environment with either a Nintendo Wii Balance Board or joysticks. However, our
pre-registered main hypotheses were not supported; the joystick did not cause
any more VR sickness on the participants than the balance board, and the board
proved to be statistically significantly more difficult to use, both
subjectively and objectively. Analyzing the open-ended questions revealed these
results to be likely connected, meaning that the difficulty of use seemed to
affect sickness; even unlimited training time before the test did not make the
use as easy as the familiar joystick. Thus, making the board easier to use is a
key to enable its potential; we present a few possibilities towards this goal.","['Joona Halkola', 'Markku Suomalainen', 'Basak Sakcak', 'Katherine J. Mimnaugh', 'Juho Kalliokoski', 'Alexis P. Chambers', 'Timo Ojala', 'Steven M. LaValle']",2022-08-22T21:37:49Z,http://arxiv.org/abs/2208.10613v1,"['cs.RO', 'cs.HC', 'cs.MM']"
"AI and 6G into the Metaverse: Fundamentals, Challenges and Future
  Research Trends","Since Facebook was renamed Meta, a lot of attention, debate, and exploration
have intensified about what the Metaverse is, how it works, and the possible
ways to exploit it. It is anticipated that Metaverse will be a continuum of
rapidly emerging technologies, usecases, capabilities, and experiences that
will make it up for the next evolution of the Internet. Several researchers
have already surveyed the literature on artificial intelligence (AI) and
wireless communications in realizing the Metaverse. However, due to the rapid
emergence and continuous evolution of technologies, there is a need for a
comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both
in realizing the immersive experiences of Metaverse. Therefore, in this survey,
we first introduce the background and ongoing progress in augmented reality
(AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed
by the technical aspects of AI and 6G. Then, we survey the role of AI in the
Metaverse by reviewing the state-of-the-art in deep learning, computer vision,
and Edge AI to extract the requirements of 6G in Metaverse. Next, we
investigate the promising services of B5G/6G towards Metaverse, followed by
identifying the role of AI in 6G networks and 6G networks for AI in support of
Metaverse applications, and the need for sustainability in Metaverse. Finally,
we enlist the existing and potential applications, usecases, and projects to
highlight the importance of progress in the Metaverse. Moreover, in order to
provide potential research directions to researchers, we underline the
challenges, research gaps, and lessons learned identified from the literature
review of the aforementioned technologies.","['Muhammad Zawish', 'Fayaz Ali Dharejo', 'Sunder Ali Khowaja', 'Kapal Dev', 'Steven Davy', 'Nawab Muhammad Faseeh Qureshi', 'Paolo Bellavista']",2022-08-23T12:48:53Z,http://arxiv.org/abs/2208.10921v2,"['cs.AI', 'cs.HC', 'cs.NI', 'cs.SI']"
"Resource Allocation for Mobile Metaverse with the Internet of Vehicles
  over 6G Wireless Communications: A Deep Reinforcement Learning Approach","Improving the interactivity and interconnectivity between people is one of
the highlights of the Metaverse. The Metaverse relies on a core approach,
digital twinning, which is a means to replicate physical world objects, people,
actions and scenes onto the virtual world. Being able to access scenes and
information associated with the physical world, in the Metaverse in real-time
and under mobility, is essential in developing a highly accessible, interactive
and interconnective experience for all users. This development allows users
from other locations to access high-quality real-world and up-to-date
information about events happening in another location, and socialize with
others hyper-interactively. Nevertheless, receiving continual, smooth updates
generated by others from the Metaverse is a challenging task due to the large
data size of the virtual world graphics and the need for low latency
transmission. With the development of Mobile Augmented Reality (MAR), users can
interact via the Metaverse in a highly interactive manner, even under mobility.
Hence in our work, we considered an environment with users in moving Internet
of Vehicles (IoV), downloading real-time virtual world updates from Metaverse
Service Provider Cell Stations (MSPCSs) via wireless communications. We design
an environment with multiple cell stations, where there will be a handover of
users' virtual world graphic download tasks between cell stations. As
transmission latency is the primary concern in receiving virtual world updates
under mobility, our work aims to allocate system resources to minimize the
total time taken for users in vehicles to download their virtual world scenes
from the cell stations. We utilize deep reinforcement learning and evaluate the
performance of the algorithms under different environmental configurations. Our
work provides a use case of the Metaverse over AI-enabled 6G communications.","['Terence Jie Chua', 'Wenhan Yu', 'Jun Zhao']",2022-09-27T14:28:04Z,http://arxiv.org/abs/2209.13425v1,"['cs.NI', 'cs.LG', 'eess.SP']"
"A Multi Camera Unsupervised Domain Adaptation Pipeline for Object
  Detection in Cultural Sites through Adversarial Learning and Self-Training","Object detection algorithms allow to enable many interesting applications
which can be implemented in different devices, such as smartphones and wearable
devices. In the context of a cultural site, implementing these algorithms in a
wearable device, such as a pair of smart glasses, allow to enable the use of
augmented reality (AR) to show extra information about the artworks and enrich
the visitors' experience during their tour. However, object detection
algorithms require to be trained on many well annotated examples to achieve
reasonable results. This brings a major limitation since the annotation process
requires human supervision which makes it expensive in terms of time and costs.
A possible solution to reduce these costs consist in exploiting tools to
automatically generate synthetic labeled images from a 3D model of the site.
However, models trained with synthetic data do not generalize on real images
acquired in the target scenario in which they are supposed to be used.
Furthermore, object detectors should be able to work with different wearable
devices or different mobile devices, which makes generalization even harder. In
this paper, we present a new dataset collected in a cultural site to study the
problem of domain adaptation for object detection in the presence of multiple
unlabeled target domains corresponding to different cameras and a labeled
source domain obtained considering synthetic images for training purposes. We
present a new domain adaptation method which outperforms current
state-of-the-art approaches combining the benefits of aligning the domains at
the feature and pixel level with a self-training process. We release the
dataset at the following link https://iplab.dmi.unict.it/OBJ-MDA/ and the code
of the proposed architecture at https://github.com/fpv-iplab/STMDA-RetinaNet.","['Giovanni Pasqualino', 'Antonino Furnari', 'Giovanni Maria Farinella']",2022-10-03T10:40:58Z,http://arxiv.org/abs/2210.00808v1,"['cs.CV', 'cs.LG']"
"Towards Transcervical Ultrasound Image Guidance for Transoral Robotic
  Surgery","Purpose: Trans-oral robotic surgery (TORS) using the da Vinci surgical robot
is a new minimally-invasive surgery method to treat oropharyngeal tumors, but
it is a challenging operation. Augmented reality (AR) based on intra-operative
ultrasound (US) has the potential to enhance the visualization of the anatomy
and cancerous tumors to provide additional tools for decision-making in
surgery. Methods: We propose and carry out preliminary evaluations of a
US-guided AR system for TORS, with the transducer placed on the neck for a
transcervical view. Firstly, we perform a novel MRI-transcervical 3D US
registration study. Secondly, we develop a US-robot calibration method with an
optical tracker and an AR system to display the anatomy mesh model in the
real-time endoscope images inside the surgeon console. Results: Our AR system
reaches a mean projection error of 26.81 and 27.85 pixels for the projection
from the US to stereo cameras in a water bath experiment. The average target
registration error for MRI to 3D US is 8.90 mm for the 3D US transducer and
5.85 mm for freehand 3D US, and the average distance between the vessel
centerlines is 2.32 mm. Conclusion: We demonstrate the first proof-of-concept
transcervical US-guided AR system for TORS and the feasibility of
trans-cervical 3D US-MRI registration. Our results show that trans-cervical 3D
US is a promising technique for TORS image guidance.","['Wanwen Chen', 'Megha Kalia', 'Qi Zeng', 'Emily H. T. Pang', 'Razeyeh Bagherinasab', 'Thomas D. Milner', 'Farahna Sabiq', 'Eitan Prisman', 'Septimiu E. Salcudean']",2022-11-29T19:12:25Z,http://arxiv.org/abs/2211.16544v2,"['cs.RO', 'cs.HC']"
"FADEC: FPGA-based Acceleration of Video Depth Estimation by HW/SW
  Co-design","3D reconstruction from videos has become increasingly popular for various
applications, including navigation for autonomous driving of robots and drones,
augmented reality (AR), and 3D modeling. This task often combines traditional
image/video processing algorithms and deep neural networks (DNNs). Although
recent developments in deep learning have improved the accuracy of the task,
the large number of calculations involved results in low computation speed and
high power consumption. Although there are various domain-specific hardware
accelerators for DNNs, it is not easy to accelerate the entire process of
applications that alternate between traditional image/video processing
algorithms and DNNs. Thus, FPGA-based end-to-end acceleration is required for
such complicated applications in low-power embedded environments.
  This paper proposes a novel FPGA-based accelerator for DeepVideoMVS, a
DNN-based depth estimation method for 3D reconstruction. We employ HW/SW
co-design to appropriately utilize heterogeneous components in modern SoC
FPGAs, such as programmable logic (PL) and CPU, according to the inherent
characteristics of the method. As some operations are unsuitable for hardware
implementation, we determine the operations to be implemented in software
through analyzing the number of times each operation is performed and its
memory access pattern, and then considering comprehensive aspects: the ease of
hardware implementation and degree of expected acceleration by hardware. The
hardware and software implementations are executed in parallel on the PL and
CPU to hide their execution latencies. The proposed accelerator was developed
on a Xilinx ZCU104 board by using NNgen, an open-source high-level synthesis
(HLS) tool. Experiments showed that the proposed accelerator operates 60.2
times faster than the software-only implementation on the same FPGA board with
minimal accuracy degradation.","['Nobuho Hashimoto', 'Shinya Takamaeda-Yamazaki']",2022-12-01T08:30:09Z,http://arxiv.org/abs/2212.00357v2,['cs.AR']
Beyond the Metaverse: XV (eXtended meta/uni/Verse),"We propose the term and concept XV (eXtended meta/omni/uni/Verse) as an
alternative to, and generalization of, the shared/social virtual reality widely
known as ``metaverse''. XV is shared/social XR. We, and many others, use XR
(eXtended Reality) as a broad umbrella term and concept to encompass all the
other realities, where X is an ``anything'' variable, like in mathematics, to
denote any reality, X $\in$ \{physical, virtual, augmented, \ldots \} reality.
Therefore XV inherits this generality from XR. We begin with a very simple
organized taxonomy of all these realities in terms of two simple building
blocks: (1) physical reality (PR) as made of ``atoms'', and (2) virtual reality
(VR) as made of ``bits''. Next we introduce XV as combining all these realities
with extended society as a three-dimensional space and taxonomy of (1)
``atoms'' (physical reality), (2) ``bits'' (virtuality), and (3) ``genes''
(sociality). Thus those working in the liminal space between Virtual Reality
(VR), Augmented Reality (AR), metaverse, and their various extensions, can
describe their work and research as existing in the new field of XV. XV
includes the metaverse along with extensions of reality itself like shared
seeing in the infrared, ultraviolet, and shared seeing of electromagnetic radio
waves, sound waves, and electric currents in motors. For example, workers in a
mechanical room can look at a pump and see a superimposed time-varying waveform
of the actual rotating magnetic field inside its motor, in real time, while
sharing this vision across multiple sites.
  Presented at IEEE Standards Association, Behind and Beyond the Metaverse: XV
(eXtended meta/uni/Verse), Thurs. Dec. 8, 2022, 2:15-3:30pm, EST.","['Steve Mann', 'Yu Yuan', 'Tom Furness', 'Joseph Paradiso', 'Thomas Coughlin']",2022-12-15T16:49:32Z,http://arxiv.org/abs/2212.07960v1,"['eess.SY', 'cs.HC', 'cs.SY', 'eess.IV']"
"A Unified Architecture for Dynamic Role Allocation and Collaborative
  Task Planning in Mixed Human-Robot Teams","The growing deployment of human-robot collaborative processes in several
industrial applications, such as handling, welding, and assembly, unfolds the
pursuit of systems which are able to manage large heterogeneous teams and, at
the same time, monitor the execution of complex tasks. In this paper, we
present a novel architecture for dynamic role allocation and collaborative task
planning in a mixed human-robot team of arbitrary size. The architecture
capitalizes on a centralized reactive and modular task-agnostic planning method
based on Behavior Trees (BTs), in charge of actions scheduling, while the
allocation problem is formulated through a Mixed-Integer Linear Program (MILP),
that assigns dynamically individual roles or collaborations to the agents of
the team. Different metrics used as MILP cost allow the architecture to favor
various aspects of the collaboration (e.g. makespan, ergonomics, human
preferences). Human preference are identified through a negotiation phase, in
which, an human agent can accept/refuse to execute the assigned task.In
addition, bilateral communication between humans and the system is achieved
through an Augmented Reality (AR) custom user interface that provides intuitive
functionalities to assist and coordinate workers in different action phases.
The computational complexity of the proposed methodology outperforms literature
approaches in industrial sized jobs and teams (problems up to 50 actions and 20
agents in the team with collaborations are solved within 1 s). The different
allocated roles, as the cost functions change, highlights the flexibility of
the architecture to several production requirements. Finally, the subjective
evaluation demonstrating the high usability level and the suitability for the
targeted scenario.","['Edoardo Lamon', 'Fabio Fusaro', 'Elena De Momi', 'Arash Ajoudani']",2023-01-19T12:30:56Z,http://arxiv.org/abs/2301.08038v2,"['cs.RO', 'cs.AI', 'cs.HC', 'cs.MA']"
"Listen2Scene: Interactive material-aware binaural sound propagation for
  reconstructed 3D scenes","We present an end-to-end binaural audio rendering approach (Listen2Scene) for
virtual reality (VR) and augmented reality (AR) applications. We propose a
novel neural-network-based binaural sound propagation method to generate
acoustic effects for indoor 3D models of real environments. Any clean audio or
dry audio can be convolved with the generated acoustic effects to render audio
corresponding to the real environment. We propose a graph neural network that
uses both the material and the topology information of the 3D scenes and
generates a scene latent vector. Moreover, we use a conditional generative
adversarial network (CGAN) to generate acoustic effects from the scene latent
vector. Our network can handle holes or other artifacts in the reconstructed 3D
mesh model. We present an efficient cost function for the generator network to
incorporate spatial audio effects. Given the source and the listener position,
our learning-based binaural sound propagation approach can generate an acoustic
effect in 0.1 milliseconds on an NVIDIA GeForce RTX 2080 Ti GPU. We have
evaluated the accuracy of our approach with binaural acoustic effects generated
using an interactive geometric sound propagation algorithm and captured real
acoustic effects / real-world recordings. We also performed a perceptual
evaluation and observed that the audio rendered by our approach is more
plausible than audio rendered using prior learning-based and geometric-based
sound propagation algorithms. We quantitatively evaluated the accuracy of our
approach using statistical acoustic parameters, and energy decay curves. The
demo videos, code and dataset are available online
(https://anton-jeran.github.io/Listen2Scene/).","['Anton Ratnarajah', 'Dinesh Manocha']",2023-02-02T04:09:23Z,http://arxiv.org/abs/2302.02809v4,"['eess.AS', 'cs.CV', 'cs.LG', 'cs.MM', 'cs.SD']"
WSD: Wild Selfie Dataset for Face Recognition in Selfie Images,"With the rise of handy smart phones in the recent years, the trend of
capturing selfie images is observed. Hence efficient approaches are required to
be developed for recognising faces in selfie images. Due to the short distance
between the camera and face in selfie images, and the different visual effects
offered by the selfie apps, face recognition becomes more challenging with
existing approaches. A dataset is needed to be developed to encourage the study
to recognize faces in selfie images. In order to alleviate this problem and to
facilitate the research on selfie face images, we develop a challenging Wild
Selfie Dataset (WSD) where the images are captured from the selfie cameras of
different smart phones, unlike existing datasets where most of the images are
captured in controlled environment. The WSD dataset contains 45,424 images from
42 individuals (i.e., 24 female and 18 male subjects), which are divided into
40,862 training and 4,562 test images. The average number of images per subject
is 1,082 with minimum and maximum number of images for any subject are 518 and
2,634, respectively. The proposed dataset consists of several challenges,
including but not limited to augmented reality filtering, mirrored images,
occlusion, illumination, scale, expressions, view-point, aspect ratio, blur,
partial faces, rotation, and alignment. We compare the proposed dataset with
existing benchmark datasets in terms of different characteristics. The
complexity of WSD dataset is also observed experimentally, where the
performance of the existing state-of-the-art face recognition methods is poor
on WSD dataset, compared to the existing datasets. Hence, the proposed WSD
dataset opens up new challenges in the area of face recognition and can be
beneficial to the community to study the specific challenges related to selfie
images and develop improved methods for face recognition in selfie images.","['Laxman Kumarapu', 'Shiv Ram Dubey', 'Snehasis Mukherjee', 'Parkhi Mohan', 'Sree Pragna Vinnakoti', 'Subhash Karthikeya']",2023-02-14T18:43:21Z,http://arxiv.org/abs/2302.07245v1,['cs.CV']
"SceneHGN: Hierarchical Graph Networks for 3D Indoor Scene Generation
  with Fine-Grained Geometry","3D indoor scenes are widely used in computer graphics, with applications
ranging from interior design to gaming to virtual and augmented reality. They
also contain rich information, including room layout, as well as furniture
type, geometry, and placement. High-quality 3D indoor scenes are highly
demanded while it requires expertise and is time-consuming to design
high-quality 3D indoor scenes manually. Existing research only addresses
partial problems: some works learn to generate room layout, and other works
focus on generating detailed structure and geometry of individual furniture
objects. However, these partial steps are related and should be addressed
together for optimal synthesis. We propose SCENEHGN, a hierarchical graph
network for 3D indoor scenes that takes into account the full hierarchy from
the room level to the object level, then finally to the object part level.
Therefore for the first time, our method is able to directly generate plausible
3D room content, including furniture objects with fine-grained geometry, and
their layout. To address the challenge, we introduce functional regions as
intermediate proxies between the room and object levels to make learning more
manageable. To ensure plausibility, our graph-based representation incorporates
both vertical edges connecting child nodes with parent nodes from different
levels, and horizontal edges encoding relationships between nodes at the same
level. Extensive experiments demonstrate that our method produces superior
generation results, even when comparing results of partial steps with
alternative methods that can only achieve these. We also demonstrate that our
method is effective for various applications such as part-level room editing,
room interpolation, and room generation by arbitrary room boundaries.","['Lin Gao', 'Jia-Mu Sun', 'Kaichun Mo', 'Yu-Kun Lai', 'Leonidas J. Guibas', 'Jie Yang']",2023-02-16T15:31:59Z,http://arxiv.org/abs/2302.10237v1,"['cs.GR', 'cs.CV']"
"RL meets Multi-Link Operation in IEEE 802.11be: Multi-Headed Recurrent
  Soft-Actor Critic-based Traffic Allocation","IEEE 802.11be -Extremely High Throughput-, commercially known as
Wireless-Fidelity (Wi-Fi) 7 is the newest IEEE 802.11 amendment that comes to
address the increasingly throughput hungry services such as Ultra High
Definition (4K/8K) Video and Virtual/Augmented Reality (VR/AR). To do so, IEEE
802.11be presents a set of novel features that will boost the Wi-Fi technology
to its edge. Among them, Multi-Link Operation (MLO) devices are anticipated to
become a reality, leaving Single-Link Operation (SLO) Wi-Fi in the past. To
achieve superior throughput and very low latency, a careful design approach
must be taken, on how the incoming traffic is distributed in MLO capable
devices. In this paper, we present a Reinforcement Learning (RL) algorithm
named Multi-Headed Recurrent Soft-Actor Critic (MH-RSAC) to distribute incoming
traffic in 802.11be MLO capable networks. Moreover, we compare our results with
two non-RL baselines previously proposed in the literature named: Single Link
Less Congested Interface (SLCI) and Multi-Link Congestion-aware Load balancing
at flow arrivals (MCAA). Simulation results reveal that the MH-RSAC algorithm
is able to obtain gains in terms of Throughput Drop Ratio (TDR) up to 35.2% and
6% when compared with the SLCI and MCAA algorithms, respectively. Finally, we
observed that our scheme is able to respond more efficiently to high throughput
and dynamic traffic such as VR and Web Browsing (WB) when compared with the
baselines. Results showed an improvement of the MH-RSAC scheme in terms of Flow
Satisfaction (FS) of up to 25.6% and 6% over the the SCLI and MCAA algorithms.","['Pedro Enrique Iturria Rivera', 'Marcel Chenier', 'Bernard Herscovici', 'Burak Kantarci', 'Melike Erol-Kantarci']",2023-03-15T22:14:28Z,http://arxiv.org/abs/2303.08959v1,['cs.NI']
"Mobile Edge Adversarial Detection for Digital Twinning to the Metaverse
  with Deep Reinforcement Learning","Real-time Digital Twinning of physical world scenes onto the Metaverse is
necessary for a myriad of applications such as augmented-reality (AR) assisted
driving. In AR assisted driving, physical environment scenes are first captured
by Internet of Vehicles (IoVs) and are uploaded to the Metaverse. A central
Metaverse Map Service Provider (MMSP) will aggregate information from all IoVs
to develop a central Metaverse Map. Information from the Metaverse Map can then
be downloaded into individual IoVs on demand and be delivered as AR scenes to
the driver. However, the growing interest in developing AR assisted driving
applications which relies on digital twinning invites adversaries. These
adversaries may place physical adversarial patches on physical world objects
such as cars, signboards, or on roads, seeking to contort the virtual world
digital twin. Hence, there is a need to detect these physical world adversarial
patches. Nevertheless, as real-time, accurate detection of adversarial patches
is compute-intensive, these physical world scenes have to be offloaded to the
Metaverse Map Base Stations (MMBS) for computation. Hence in our work, we
considered an environment with moving Internet of Vehicles (IoV), uploading
real-time physical world scenes to the MMBSs. We formulated a realistic joint
variable optimization problem where the MMSPs' objective is to maximize
adversarial patch detection mean average precision (mAP), while minimizing the
computed AR scene up-link transmission latency and IoVs' up-link transmission
idle count, through optimizing the IoV-MMBS allocation and IoV up-link scene
resolution selection. We proposed a Heterogeneous Action Proximal Policy
Optimization (HAPPO) (discrete-continuous) algorithm to tackle the proposed
problem. Extensive experiments shows HAPPO outperforms baseline models when
compared against key metrics.","['Terence Jie Chua', 'Wenhan Yu', 'Jun Zhao']",2023-03-18T00:03:50Z,http://arxiv.org/abs/2303.10288v1,"['cs.NI', 'cs.AI']"
"Fusing Structure from Motion and Simulation-Augmented Pose Regression
  from Optical Flow for Challenging Indoor Environments","The localization of objects is a crucial task in various applications such as
robotics, virtual and augmented reality, and the transportation of goods in
warehouses. Recent advances in deep learning have enabled the localization
using monocular visual cameras. While structure from motion (SfM) predicts the
absolute pose from a point cloud, absolute pose regression (APR) methods learn
a semantic understanding of the environment through neural networks. However,
both fields face challenges caused by the environment such as motion blur,
lighting changes, repetitive patterns, and feature-less structures. This study
aims to address these challenges by incorporating additional information and
regularizing the absolute pose using relative pose regression (RPR) methods.
RPR methods suffer under different challenges, i.e., motion blur. The optical
flow between consecutive images is computed using the Lucas-Kanade algorithm,
and the relative pose is predicted using an auxiliary small recurrent
convolutional network. The fusion of absolute and relative poses is a complex
task due to the mismatch between the global and local coordinate systems.
State-of-the-art methods fusing absolute and relative poses use pose graph
optimization (PGO) to regularize the absolute pose predictions using relative
poses. In this work, we propose recurrent fusion networks to optimally align
absolute and relative pose predictions to improve the absolute pose prediction.
We evaluate eight different recurrent units and construct a simulation
environment to pre-train the APR and RPR networks for better generalized
training. Additionally, we record a large database of different scenarios in a
challenging large-scale indoor environment that mimics a warehouse with
transportation robots. We conduct hyperparameter searches and experiments to
show the effectiveness of our recurrent fusion method compared to PGO.","['Felix Ott', 'Lucas Heublein', 'David Rügamer', 'Bernd Bischl', 'Christopher Mutschler']",2023-04-14T16:58:23Z,http://arxiv.org/abs/2304.07250v3,"['cs.CV', 'cs.AI', '68U01', 'I.2.9; I.2.10; I.4.1; I.4.10; I.5.4']"
A flexible algorithm to offload DAG applications for edge computing,"Multi-access Edge Computing (MEC) is an enabling technology to leverage new
network applications, such as virtual/augmented reality, by providing faster
task processing at the network edge. This is done by deploying servers closer
to the end users to run the network applications. These applications are often
intensive in terms of task processing, memory usage, and communication; thus
mobile devices may take a long time or even not be able to run them
efficiently. By transferring (offloading) the execution of these applications
to the servers at the network edge, it is possible to achieve a lower
completion time (makespan) and meet application requirements. However,
offloading multiple entire applications to the edge server can overwhelm its
hardware and communication channel, as well as underutilize the mobile devices'
hardware. In this paper, network applications are modeled as Directed Acyclic
Graphs (DAGs) and partitioned into tasks, and only part of these tasks are
offloaded to the edge server. This is the DAG application partitioning and
offloading problem, which is known to be NP-hard. To approximate its solution,
this paper proposes the FlexDO algorithm. FlexDO combines a greedy phase with a
permutation phase to find a set of offloading decisions, and then chooses the
one that achieves the shortest makespan. FlexDO is compared with a proposal
from the literature and two baseline decisions, considering realistic DAG
applications extracted from the Alibaba Cluster Trace Program. Results show
that FlexDO is consistently only 3.9% to 8.9% above the optimal makespan in all
test scenarios, which include different levels of CPU availability, a
multi-user case, and different communication channel transmission rates. FlexDO
outperforms both baseline solutions by a wide margin, and is three times closer
to the optimal makespan than its competitor.","['Gabriel F. C. de Queiroz', 'José F. de Rezende', 'Valmir C. Barbosa']",2023-06-15T19:29:04Z,http://arxiv.org/abs/2306.09458v1,['cs.NI']
Non-volatile Phase-only Transmissive Spatial Light Modulators,"Free-space modulation of light is crucial for many applications, from light
detection and ranging to virtual or augmented reality. Traditional means of
modulating free-space light involves spatial light modulators based on liquid
crystals and microelectromechanical systems, which are bulky, have large pixel
areas (~10 micron x 10 micron), and require high driving voltage. Recent
progress in meta-optics has shown promise to circumvent some of the
limitations. By integrating active materials with sub-wavelength pixels in a
meta-optic, the power consumption can be dramatically reduced while achieving a
faster speed. However, these reconfiguration methods are volatile and hence
require constant application of control signals, leading to phase jitter and
crosstalk. Additionally, to control a large number of pixels, it is essential
to implement a memory within each pixel to have a tractable number of control
signals. Here, we develop a device with nonvolatile, electrically programmable,
phase-only modulation of free-space infrared radiation in transmission using
the low-loss phase-change material (PCM) Sb2Se3. By coupling an ultra-thin PCM
layer to a high quality (Q)-factor (Q~406) diatomic metasurface, we demonstrate
a phase-only modulation of ~0.25pi (~0.2pi) in simulation (experiment), ten
times larger than a bare PCM layer of the same thickness. The device shows
excellent endurance over 1,000 switching cycles. We then advance the device
geometry, to enable independent control of 17 meta-molecules, achieving ten
deterministic resonance levels with a 2pi phase shift. By independently
controlling the phase delay of pixels, we further show tunable far-field beam
shaping. Our work paves the way to realizing non-volatile transmissive
phase-only spatial light modulators.","['Zhuoran Fang', 'Rui Chen', 'Johannes E. Fröch', 'Quentin A. A. Tanguy', 'Asir Intisar Khan', 'Xiangjin Wu', 'Virat Tara', 'Arnab Manna', 'David Sharp', 'Christopher Munley', 'Forrest Miller', 'Yang Zhao', 'Sarah J. Geiger', 'Karl F. Böhringer', 'Matthew Reynolds', 'Eric Pop', 'Arka Majumdar']",2023-07-22T15:26:45Z,http://arxiv.org/abs/2307.12103v1,['physics.optics']
"Apple Vision Pro for Healthcare: ""The Ultimate Display""? -- Entering the
  Wonderland of Precision Medicine","At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user's eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers' eyes to ""outsiders"" or a button on
the top, called ""Digital Crown"", that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the ""Ultimate
Display"", which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.","['Jan Egger', 'Christina Gsaxner', 'Xiaojun Chen', 'Jiang Bian', 'Jens Kleesiek', 'Behrus Puladi']",2023-08-08T15:01:51Z,http://arxiv.org/abs/2308.04313v4,"['cs.AI', 'cs.GR', 'cs.HC']"
ARGUS: Visualization of AI-Assisted Task Guidance in AR,"The concept of augmented reality (AR) assistants has captured the human
imagination for decades, becoming a staple of modern science fiction. To pursue
this goal, it is necessary to develop artificial intelligence (AI)-based
methods that simultaneously perceive the 3D environment, reason about physical
tasks, and model the performer, all in real-time. Within this framework, a wide
variety of sensors are needed to generate data across different modalities,
such as audio, video, depth, speech, and time-of-flight. The required sensors
are typically part of the AR headset, providing performer sensing and
interaction through visual, audio, and haptic feedback. AI assistants not only
record the performer as they perform activities, but also require machine
learning (ML) models to understand and assist the performer as they interact
with the physical world. Therefore, developing such assistants is a challenging
task. We propose ARGUS, a visual analytics system to support the development of
intelligent AR assistants. Our system was designed as part of a multi year-long
collaboration between visualization researchers and ML and AR experts. This
co-design process has led to advances in the visualization of ML in AR. Our
system allows for online visualization of object, action, and step detection as
well as offline analysis of previously recorded AR sessions. It visualizes not
only the multimodal sensor data streams but also the output of the ML models.
This allows developers to gain insights into the performer activities as well
as the ML models, helping them troubleshoot, improve, and fine tune the
components of the AR assistant.","['Sonia Castelo', 'Joao Rulff', 'Erin McGowan', 'Bea Steers', 'Guande Wu', 'Shaoyu Chen', 'Iran Roman', 'Roque Lopez', 'Ethan Brewer', 'Chen Zhao', 'Jing Qian', 'Kyunghyun Cho', 'He He', 'Qi Sun', 'Huy Vo', 'Juan Bello', 'Michael Krone', 'Claudio Silva']",2023-08-11T17:27:01Z,http://arxiv.org/abs/2308.06246v1,['cs.HC']
Working with XR in Public: Effects on Users and Bystanders,"Recent commercial off-the-shelf virtual and augmented reality devices have
been promoted as tools for knowledge work and research findings show how this
kind of work can benefit from the affordances of extended reality (XR). One
major advantage that XR can provide is the enlarged display space that can be
used to display virtual screens which is a feature already readily available in
many commercial devices. This could be especially helpful in mobile contexts,
in which users might not have access to their optimal physical work setup. Such
situations often occur in a public setting, for example when working on a train
while traveling to a business meeting. At the same time, the use of XR devices
is still uncommon in public, which might impact both users and bystanders.
Hence, there is a need to better understand the implications of using XR
devices for work in public both on the user itself, as well as on bystanders.
We report the results of a study in a university cafeteria in which
participants used three different systems. In one setup they only used a laptop
with a single screen, in a second setup, they combined the laptop with an
optical see-through AR headset, and in the third, they combined the laptop with
an immersive VR headset. In addition, we also collected 231 responses from
bystanders through a questionnaire. The combined results indicate that (1)
users feel safer if they can see their physical surroundings; (2) current use
of XR in public makes users stand out; and (3) prior XR experience can
influence how users feel when using XR in public.","['Verena Biener', 'Snehanjali Kalamkar', 'John J Dudley', 'Jinghui Hu', 'Per Ola Kristensson', 'Jörg Müller', 'Jens Grubert']",2023-10-15T09:43:18Z,http://arxiv.org/abs/2310.09786v1,['cs.HC']
"Well-being in isolation: Exploring artistic immersive virtual
  environments in a simulated lunar habitat to alleviate asthenia symptoms","Revived interest in lunar and planetary exploration is heralding a new era
for human spaceflight, characterized by frequent strain on astronaut's mental
well-being, which stems from increased exposure to isolated, confined, and
extreme (ICE) conditions. Whilst Immersive Virtual Reality (IVR) has been
employed to facilitate self-help interventions to mitigate challenges caused by
isolated environments in several domains, its applicability in support of
future space expeditions remains largely unexplored. To address this
limitation, we administered the use of distinct IVR environments to crew
members (n=5) partaking in a simulated lunar habitat study. Utilizing a
Bayesian approach to scrutinize small group data, we discovered a significant
relationship between IVR usage and a reduction in perceived stress-related
symptoms, particularly those associated with asthenia (syndrome often linked to
chronic fatigue and weakness; a condition characterized by feelings of energy
depletion or exhaustion that can be amplified in ICE conditions). The
reductions were most prominent with the use of interactive virtual
environments. The 'Aesthetic Realities' - virtual environments conceived as art
exhibits - received exceptional praise from our participants. These
environments mark a fascinating convergence of art and science, holding promise
to mitigate effects related to isolation in spaceflight training and beyond.","['Grzegorz Pochwatko', 'Wieslaw Kopec', 'Justyna Swidrak', 'Anna Jaskulska', 'Kinga H. Skorupska', 'Barbara Karpowicz', 'Rafał Masłyk', 'Maciej Grzeszczuk', 'Steven Barnes', 'Paulina Borkiewicz', 'Paweł Kobyliński', 'Michał Pabiś-Orzeszyna', 'Robert Balas', 'Jagoda Lazarek', 'Florian Dufresne', 'Leonie Bensch', 'Tommy Nilsson']",2023-11-15T20:04:00Z,http://arxiv.org/abs/2311.09343v1,"['cs.HC', 'cs.CY', '93B51, 97M50', 'H.1.2; I.3.8; J.4; J.m; K.8.2; J.6']"
"Should I use metaverse or not? An investigation of university students
  behavioral intention to use MetaEducation technology","Metaverse, a burgeoning technological trend that combines virtual and
augmented reality, provides users with a fully digital environment where they
can assume a virtual identity through a digital avatar and interact with others
as they were in the real world. Its applications span diverse domains such as
economy (with its entry into the cryptocurrency field), finance, social life,
working environment, healthcare, real estate, and education. During the
COVID-19 and post-COVID-19 era, universities have rapidly adopted e-learning
technologies to provide students with online access to learning content and
platforms, rendering previous considerations on integrating such technologies
or preparing institutional infrastructures virtually obsolete. In light of this
context, the present study proposes a framework for analyzing university
students' acceptance and intention to use metaverse technologies in education,
drawing upon the Technology Acceptance Model (TAM). The study aims to
investigate the relationship between students' intention to use metaverse
technologies in education, hereafter referred to as MetaEducation, and selected
TAM constructs, including Attitude, Perceived Usefulness, Perceived Ease of
Use, Self-efficacy of metaverse technologies in education, and Subjective Norm.
Notably, Self-efficacy and Subjective Norm have a positive influence on
Attitude and Perceived Usefulness, whereas Perceived Ease of Use does not
exhibit a strong correlation with Attitude or Perceived Usefulness. The authors
postulate that the weak associations between the study's constructs may be
attributed to limited knowledge regarding MetaEducation and its potential
benefits. Further investigation and analysis of the study's proposed model are
warranted to comprehensively understand the complex dynamics involved in the
acceptance and utilization of MetaEducation technologies in the realm of higher
education","['Nikolaos Misirlis', 'Yiannis Nikolaidis', 'Anna Sabidussi']",2023-11-26T09:45:34Z,http://arxiv.org/abs/2311.15251v1,"['cs.CY', 'cs.SI']"
"Odor-Based Molecular Communications: State-of-the-Art, Vision,
  Challenges, and Frontier Directions","Humankind mimics the processes and strategies that nature has perfected and
uses them as a model to address its problems. That has recently found a new
direction, i.e., a novel communication technology called molecular
communication (MC), using molecules to encode, transmit, and receive
information. Despite extensive research, an innate MC method with plenty of
natural instances, i.e., olfactory or odor communication, has not yet been
studied with the tools of information and communication technologies (ICT).
Existing studies focus on digitizing this sense and developing actuators
without inspecting the principles of odor-based information coding and MC,
which significantly limits its application potential. Hence, there is a need to
focus cross-disciplinary research efforts to reveal the fundamentals of this
unconventional communication modality from an ICT perspective. The ways of
natural odor MC in nature need to be anatomized and engineered for end-to-end
communication among humans and human-made things to enable several multi-sense
augmented reality technologies reinforced with olfactory senses for novel
applications and solutions in the Internet of Everything (IoE). This paper
introduces the concept of odor-based molecular communication (OMC) and provides
a comprehensive examination of olfactory systems. It explores odor
communication in nature, including aspects of odor information, channels,
reception, spatial perception, and cognitive functions. Additionally, a
comprehensive comparison of various communication systems sets the foundation
for further investigation. By highlighting the unique characteristics,
advantages, and potential applications of OMC through this comparative
analysis, the paper lays the groundwork for exploring the modeling of an
end-to-end OMC channel, considering the design of OMC transmitters and
receivers, and developing innovative OMC techniques.","['Dilara Aktas', 'Beyza Ezgi Ortlek', 'Meltem Civas', 'Elham Baradari', 'Ayse Sila Okcu', 'Melanie Whitfield', 'Oktay Cetinkaya', 'Ozgur Baris Akan']",2023-11-29T15:33:45Z,http://arxiv.org/abs/2311.17727v1,"['eess.SP', 'cs.SY', 'eess.SY']"
Subspace Hybrid MVDR Beamforming for Augmented Hearing,"Signal-dependent beamformers are advantageous over signal-independent
beamformers when the acoustic scenario - be it real-world or simulated - is
straightforward in terms of the number of sound sources, the ambient sound
field and their dynamics. However, in the context of augmented reality audio
using head-worn microphone arrays, the acoustic scenarios encountered are often
far from straightforward. The design of robust, high-performance, adaptive
beamformers for such scenarios is an on-going challenge. This is due to the
violation of the typically required assumptions on the noise field caused by,
for example, rapid variations resulting from complex acoustic environments,
and/or rotations of the listener's head. This work proposes a multi-channel
speech enhancement algorithm which utilises the adaptability of
signal-dependent beamformers while still benefiting from the computational
efficiency and robust performance of signal-independent super-directive
beamformers. The algorithm has two stages. (i) The first stage is a hybrid
beamformer based on a dictionary of weights corresponding to a set of noise
field models. (ii) The second stage is a wide-band subspace post-filter to
remove any artifacts resulting from (i). The algorithm is evaluated using both
real-world recordings and simulations of a cocktail-party scenario. Noise
suppression, intelligibility and speech quality results show a significant
performance improvement by the proposed algorithm compared to the baseline
super-directive beamformer. A data-driven implementation of the noise field
dictionary is shown to provide more noise suppression, and similar speech
intelligibility and quality, compared to a parametric dictionary.","['Sina Hafezi', 'Alastair H. Moore', 'Pierre H. Guiraud', 'Patrick A. Naylor', 'Jacob Donley', 'Vladimir Tourbabin', 'Thomas Lunner']",2023-11-30T16:34:50Z,http://arxiv.org/abs/2311.18689v1,"['eess.AS', 'cs.SD', 'eess.SP']"
"Room Acoustic Rendering Networks with Control of Scattering and Early
  Reflections","Room acoustic synthesis can be used in Virtual Reality (VR), Augmented
Reality (AR) and gaming applications to enhance listeners' sense of immersion,
realism and externalisation. A common approach is to use Geometrical Acoustics
(GA) models to compute impulse responses at interactive speed, and fast
convolution methods to apply said responses in real time. Alternatively,
delay-network-based models are capable of modeling certain aspects of room
acoustics, but with a significantly lower computational cost. In order to
bridge the gap between these classes of models, recent work introduced delay
network designs that approximate Acoustic Radiance Transfer (ART), a GA model
that simulates the transfer of acoustic energy between discrete surface patches
in an environment. This paper presents two key extensions of such designs. The
first extension involves a new physically-based and stability-preserving design
of the feedback matrices, enabling more accurate control of scattering and,
more in general, of late reverberation properties. The second extension allows
an arbitrary number of early reflections to be modeled with high accuracy,
meaning the network can be scaled at will between computational cost and early
reverb precision. The proposed extensions are compared to the baseline
ART-approximating delay network as well as two reference GA models. The
evaluation is based on objective measures of perceptually-relevant features,
including frequency-dependent reverberation times, echo density build-up, and
early decay time. Results show how the proposed extensions result in a
significant improvement over the baseline model, especially for the case of
non-convex geometries or the case of unevenly distributed wall absorption, both
scenarios of broad practical interest.","['Matteo Scerbo', 'Lauri Savioja', 'Enzo De Sena']",2023-12-22T12:47:23Z,http://arxiv.org/abs/2312.14658v1,"['cs.SD', 'eess.AS', '76Q05 (Primary) 93C43, 94A12 (Secondary)']"
"STRIDE: Single-video based Temporally Continuous Occlusion Robust 3D
  Pose Estimation","The capability to accurately estimate 3D human poses is crucial for diverse
fields such as action recognition, gait recognition, and virtual/augmented
reality. However, a persistent and significant challenge within this field is
the accurate prediction of human poses under conditions of severe occlusion.
Traditional image-based estimators struggle with heavy occlusions due to a lack
of temporal context, resulting in inconsistent predictions. While video-based
models benefit from processing temporal data, they encounter limitations when
faced with prolonged occlusions that extend over multiple frames. This
challenge arises because these models struggle to generalize beyond their
training datasets, and the variety of occlusions is hard to capture in the
training data. Addressing these challenges, we propose STRIDE (Single-video
based TempoRally contInuous occlusion Robust 3D Pose Estimation), a novel
Test-Time Training (TTT) approach to fit a human motion prior for each video.
This approach specifically handles occlusions that were not encountered during
the model's training. By employing STRIDE, we can refine a sequence of noisy
initial pose estimates into accurate, temporally coherent poses during test
time, effectively overcoming the limitations of prior methods. Our framework
demonstrates flexibility by being model-agnostic, allowing us to use any
off-the-shelf 3D pose estimation method for improving robustness and temporal
consistency. We validate STRIDE's efficacy through comprehensive experiments on
challenging datasets like Occluded Human3.6M, Human3.6M, and OCMotion, where it
not only outperforms existing single-image and video-based pose estimation
models but also showcases superior handling of substantial occlusions,
achieving fast, robust, accurate, and temporally consistent 3D pose estimates.","['Rohit Lal', 'Saketh Bachu', 'Yash Garg', 'Arindam Dutta', 'Calvin-Khang Ta', 'Dripta S. Raychaudhuri', 'Hannah Dela Cruz', 'M. Salman Asif', 'Amit K. Roy-Chowdhury']",2023-12-24T11:05:10Z,http://arxiv.org/abs/2312.16221v2,['cs.CV']
EgoGen: An Egocentric Synthetic Data Generator,"Understanding the world in first-person view is fundamental in Augmented
Reality (AR). This immersive perspective brings dramatic visual changes and
unique challenges compared to third-person views. Synthetic data has empowered
third-person-view vision models, but its application to embodied egocentric
perception tasks remains largely unexplored. A critical challenge lies in
simulating natural human movements and behaviors that effectively steer the
embodied cameras to capture a faithful egocentric representation of the 3D
world. To address this challenge, we introduce EgoGen, a new synthetic data
generator that can produce accurate and rich ground-truth training data for
egocentric perception tasks. At the heart of EgoGen is a novel human motion
synthesis model that directly leverages egocentric visual inputs of a virtual
human to sense the 3D environment. Combined with collision-avoiding motion
primitives and a two-stage reinforcement learning approach, our motion
synthesis model offers a closed-loop solution where the embodied perception and
movement of the virtual human are seamlessly coupled. Compared to previous
works, our model eliminates the need for a pre-defined global path, and is
directly applicable to dynamic environments. Combined with our easy-to-use and
scalable data generation pipeline, we demonstrate EgoGen's efficacy in three
tasks: mapping and localization for head-mounted cameras, egocentric camera
tracking, and human mesh recovery from egocentric views. EgoGen will be fully
open-sourced, offering a practical solution for creating realistic egocentric
training data and aiming to serve as a useful tool for egocentric computer
vision research. Refer to our project page: https://ego-gen.github.io/.","['Gen Li', 'Kaifeng Zhao', 'Siwei Zhang', 'Xiaozhong Lyu', 'Mihai Dusmanu', 'Yan Zhang', 'Marc Pollefeys', 'Siyu Tang']",2024-01-16T18:55:22Z,http://arxiv.org/abs/2401.08739v2,"['cs.CV', 'cs.AI']"
"Performance Analysis of 6G Multiuser Massive MIMO-OFDM THz Wireless
  Systems with Hybrid Beamforming under Intercarrier Interference","6G networks are expected to provide more diverse capabilities than their
predecessors and are likely to support applications beyond current mobile
applications, such as virtual and augmented reality (VR/AR), AI, and the
Internet of Things (IoT). In contrast to typical multiple-input multiple-output
(MIMO) systems, THz MIMO precoding cannot be conducted totally at baseband
using digital precoders due to the restricted number of signal mixers and
analog-to-digital converters that can be supported due to their cost and power
consumption. In this thesis, we analyzed the performance of multiuser massive
MIMO-OFDM THz wireless systems with hybrid beamforming. Carrier frequency
offset (CFO) is one of the most well-known disturbances for OFDM. For
practicality, we accounted for CFO, which results in Intercarrier Interference.
Incorporating the combined impact of molecular absorption, high sparsity, and
multi-path fading, we analyzed a three-dimensional wideband THz channel and the
carrier frequency offset in multi-carrier systems. With this model, we first
presented a two-stage wideband hybrid beamforming technique comprising
Riemannian manifolds optimization for analog beamforming and then a
zero-forcing (ZF) approach for digital beamforming. We adjusted the objective
function to reduce complexity, and instead of maximizing the bit rate, we
determined parameters by minimizing interference. Numerical results demonstrate
the significance of considering ICI for practical implementation for the THz
system. We demonstrated how our change in problem formulation minimizes latency
without compromising results. We also evaluated spectral efficiency by varying
the number of RF chains and antennas. The spectral efficiency grows as the
number of RF chains and antennas increases, but the spectral efficiency of
antennas declines when the number of users increases.","['Md Saheed Ullah', 'Zulqarnain Bin Ashraf', 'Sudipta Chandra Sarker']",2024-01-22T20:36:16Z,http://arxiv.org/abs/2401.12351v1,"['cs.IT', 'eess.SP', 'math.IT']"
Acoustic Local Positioning With Encoded Emission Beacons,"Acoustic local positioning systems (ALPSs) are an interesting alternative for
indoor positioning due to certain advantages over other approaches, including
their relatively high accuracy, low cost, and room-level signal propagation.
Centimeter-level or fine-grained indoor positioning can be an asset for robot
navigation, guiding a person to, for instance, a particular piece in a museum
or to a specific product in a shop, targeted advertising, or augmented reality.
In airborne system applications, acoustic positioning can be based on using
opportunistic signals or sounds produced by the person or object to be located
(e.g., noise from appliances or the speech from a speaker) or from encoded
emission beacons (or anchors) specifically designed for this purpose. This work
presents a review of the different challenges that designers of systems based
on encoded emission beacons must address in order to achieve suitable
performance. At low-level processing, the waveform design (coding and
modulation) and the processing of the received signal are key factors to
address such drawbacks as multipath propagation, multiple-access interference,
nearfar effect, or Doppler shifting. With regards to high-level system design,
the issues to be addressed are related to the distribution of beacons, ease of
deployment, and calibration and positioning algorithms, including the possible
fusion of information. Apart from theoretical discussions, this work also
includes the description of an ALPS that was implemented, installed in a large
area and tested for mobile robot navigation. In addition to practical interest
for real applications, airborne ALPSs can also be used as an excellent platform
to test complex algorithms, which can be subsequently adapted for other
positioning systems, such as underwater acoustic systems or ultrawideband
radiofrequency (UWB RF) systems.","['Jesus Urena', 'Alvaro Hernandez', 'Juan Jesus Garcia', 'Jose Manuel Villadangos', 'Maria del Carmen Perez', 'David Gualda', 'Fernando J. Alvarez', 'Teodoro Aguilera']",2024-02-04T07:51:39Z,http://arxiv.org/abs/2402.02384v1,"['eess.SP', 'cs.AR', 'cs.SD', 'eess.AS']"
"BioDrone: A Bionic Drone-based Single Object Tracking Benchmark for
  Robust Vision","Single object tracking (SOT) is a fundamental problem in computer vision,
with a wide range of applications, including autonomous driving, augmented
reality, and robot navigation. The robustness of SOT faces two main challenges:
tiny target and fast motion. These challenges are especially manifested in
videos captured by unmanned aerial vehicles (UAV), where the target is usually
far away from the camera and often with significant motion relative to the
camera. To evaluate the robustness of SOT methods, we propose BioDrone -- the
first bionic drone-based visual benchmark for SOT. Unlike existing UAV
datasets, BioDrone features videos captured from a flapping-wing UAV system
with a major camera shake due to its aerodynamics. BioDrone hence highlights
the tracking of tiny targets with drastic changes between consecutive frames,
providing a new robust vision benchmark for SOT. To date, BioDrone offers the
largest UAV-based SOT benchmark with high-quality fine-grained manual
annotations and automatically generates frame-level labels, designed for robust
vision analyses. Leveraging our proposed BioDrone, we conduct a systematic
evaluation of existing SOT methods, comparing the performance of 20
representative models and studying novel means of optimizing a SOTA method
(KeepTrack KeepTrack) for robust SOT. Our evaluation leads to new baselines and
insights for robust SOT. Moving forward, we hope that BioDrone will not only
serve as a high-quality benchmark for robust SOT, but also invite future
research into robust computer vision. The database, toolkits, evaluation
server, and baseline results are available at http://biodrone.aitestunion.com.","['Xin Zhao', 'Shiyu Hu', 'Yipei Wang', 'Jing Zhang', 'Yimin Hu', 'Rongshuai Liu', 'Haibin Ling', 'Yin Li', 'Renshu Li', 'Kun Liu', 'Jiadong Li']",2024-02-07T01:57:56Z,http://arxiv.org/abs/2402.04519v1,['cs.CV']
"Combinatorial Client-Master Multiagent Deep Reinforcement Learning for
  Task Offloading in Mobile Edge Computing","Recently, there has been an explosion of mobile applications that perform
computationally intensive tasks such as video streaming, data mining, virtual
reality, augmented reality, image processing, video processing, face
recognition, and online gaming. However, user devices (UDs), such as tablets
and smartphones, have a limited ability to perform the computation needs of the
tasks. Mobile edge computing (MEC) has emerged as a promising technology to
meet the increasing computing demands of UDs. Task offloading in MEC is a
strategy that meets the demands of UDs by distributing tasks between UDs and
MEC servers. Deep reinforcement learning (DRL) is gaining attention in
task-offloading problems because it can adapt to dynamic changes and minimize
online computational complexity. However, the various types of continuous and
discrete resource constraints on UDs and MEC servers pose challenges to the
design of an efficient DRL-based task-offloading strategy. Existing DRL-based
task-offloading algorithms focus on the constraints of the UDs, assuming the
availability of enough storage resources on the server. Moreover, existing
multiagent DRL (MADRL)--based task-offloading algorithms are homogeneous agents
and consider homogeneous constraints as a penalty in their reward function. We
proposed a novel combinatorial client-master MADRL (CCM\_MADRL) algorithm for
task offloading in MEC (CCM\_MADRL\_MEC) that enables UDs to decide their
resource requirements and the server to make a combinatorial decision based on
the requirements of the UDs. CCM\_MADRL\_MEC is the first MADRL in task
offloading to consider server storage capacity in addition to the constraints
in the UDs. By taking advantage of the combinatorial action selection,
CCM\_MADRL\_MEC has shown superior convergence over existing MADDPG and
heuristic algorithms.","['Tesfay Zemuy Gebrekidan', 'Sebastian Stein', 'Timothy J. Norman']",2024-02-18T17:17:15Z,http://arxiv.org/abs/2402.11653v1,"['cs.AI', 'cs.DC', 'cs.NI', 'I.2.11']"
Real-Time Multimodal Cognitive Assistant for Emergency Medical Services,"Emergency Medical Services (EMS) responders often operate under
time-sensitive conditions, facing cognitive overload and inherent risks,
requiring essential skills in critical thinking and rapid decision-making. This
paper presents CognitiveEMS, an end-to-end wearable cognitive assistant system
that can act as a collaborative virtual partner engaging in the real-time
acquisition and analysis of multimodal data from an emergency scene and
interacting with EMS responders through Augmented Reality (AR) smart glasses.
CognitiveEMS processes the continuous streams of data in real-time and
leverages edge computing to provide assistance in EMS protocol selection and
intervention recognition. We address key technical challenges in real-time
cognitive assistance by introducing three novel components: (i) a Speech
Recognition model that is fine-tuned for real-world medical emergency
conversations using simulated EMS audio recordings, augmented with synthetic
data generated by large language models (LLMs); (ii) an EMS Protocol Prediction
model that combines state-of-the-art (SOTA) tiny language models with EMS
domain knowledge using graph-based attention mechanisms; (iii) an EMS Action
Recognition module which leverages multimodal audio and video data and protocol
predictions to infer the intervention/treatment actions taken by the responders
at the incident scene. Our results show that for speech recognition we achieve
superior performance compared to SOTA (WER of 0.290 vs. 0.618) on
conversational data. Our protocol prediction component also significantly
outperforms SOTA (top-3 accuracy of 0.800 vs. 0.200) and the action recognition
achieves an accuracy of 0.727, while maintaining an end-to-end latency of 3.78s
for protocol prediction on the edge and 0.31s on the server.","['Keshara Weerasinghe', 'Saahith Janapati', 'Xueren Ge', 'Sion Kim', 'Sneha Iyer', 'John A. Stankovic', 'Homa Alemzadeh']",2024-03-11T13:56:57Z,http://arxiv.org/abs/2403.06734v1,"['cs.AI', 'cs.CL', 'cs.CV']"
Sensory Glove-Based Surgical Robot User Interface,"Robotic surgery has reached a high level of maturity and has become an
integral part of standard surgical care. However, existing surgeon consoles are
bulky and take up valuable space in the operating room, present challenges for
surgical team coordination, and their proprietary nature makes it difficult to
take advantage of recent technological advances, especially in virtual and
augmented reality. One potential area for further improvement is the
integration of modern sensory gloves into robotic platforms, allowing surgeons
to control robotic arms directly with their hand movements intuitively. We
propose one such system that combines an HTC Vive tracker, a Manus Meta Prime 3
XR sensory glove, and God Vision wireless smart glasses. The system controls
one arm of a da Vinci surgical robot. In addition to moving the arm, the
surgeon can use fingers to control the end-effector of the surgical instrument.
Hand gestures are used to implement clutching and similar functions. In
particular, we introduce clutching of the instrument orientation, a
functionality not available in the da Vinci system. The vibrotactile elements
of the glove are used to provide feedback to the user when gesture commands are
invoked. A preliminary evaluation of the system shows that it has excellent
tracking accuracy and allows surgeons to efficiently perform common surgical
training tasks with minimal practice with the new interface; this suggests that
the interface is highly intuitive. The proposed system is inexpensive, allows
rapid prototyping, and opens opportunities for further innovations in the
design of surgical robot interfaces.","['Leonardo Borgioli', 'Ki-Hwan Oh', 'Alberto Mangano', 'Alvaro Ducas', 'Luciano Ambrosini', 'Federico Pinto', 'Paula A Lopez', 'Jessica Cassiani', 'Milos Zefran', 'Liaohai Chen', 'Pier Cristoforo Giulianotti']",2024-03-20T19:26:27Z,http://arxiv.org/abs/2403.13941v1,"['cs.RO', 'cs.SY', 'eess.SY']"
"Leveraging Artificial Intelligence to Promote Awareness in Augmented
  Reality Systems","Recent developments in artificial intelligence (AI) have permeated through an
array of different immersive environments, including virtual, augmented, and
mixed realities. AI brings a wealth of potential that centers on its ability to
critically analyze environments, identify relevant artifacts to a goal or
action, and then autonomously execute decision-making strategies to optimize
the reward-to-risk ratio. However, the inherent benefits of AI are not without
disadvantages as the autonomy and communication methodology can interfere with
the human's awareness of their environment. More specifically in the case of
autonomy, the relevant human-computer interaction literature cites that high
autonomy results in an ""out-of-the-loop"" experience for the human such that
they are not aware of critical artifacts or situational changes that require
their attention. At the same time, low autonomy of an AI system can limit the
human's own autonomy with repeated requests to approve its decisions. In these
circumstances, humans enter into supervisor roles, which tend to increase their
workload and, therefore, decrease their awareness in a multitude of ways. In
this position statement, we call for the development of human-centered AI in
immersive environments to sustain and promote awareness. It is our position
then that we believe with the inherent risk presented in both AI and AR/VR
systems, we need to examine the interaction between them when we integrate the
two to create a new system for any unforeseen risks, and that it is crucial to
do so because of its practical application in many high-risk environments.","['Wangfan Li', 'Rohit Mallick', 'Carlos Toxtli-Hernandez', 'Christopher Flathmann', 'Nathan J. McNeese']",2024-04-23T17:47:51Z,http://arxiv.org/abs/2405.05916v1,['cs.HC']
"Reasoning3D -- Grounding and Reasoning in 3D: Fine-Grained Zero-Shot
  Open-Vocabulary 3D Reasoning Part Segmentation via Large Vision-Language
  Models","In this paper, we introduce a new task: Zero-Shot 3D Reasoning Segmentation
for parts searching and localization for objects, which is a new paradigm to 3D
segmentation that transcends limitations for previous category-specific 3D
semantic segmentation, 3D instance segmentation, and open-vocabulary 3D
segmentation. We design a simple baseline method, Reasoning3D, with the
capability to understand and execute complex commands for (fine-grained)
segmenting specific parts for 3D meshes with contextual awareness and reasoned
answers for interactive segmentation. Specifically, Reasoning3D leverages an
off-the-shelf pre-trained 2D segmentation network, powered by Large Language
Models (LLMs), to interpret user input queries in a zero-shot manner. Previous
research have shown that extensive pre-training endows foundation models with
prior world knowledge, enabling them to comprehend complex commands, a
capability we can harness to ""segment anything"" in 3D with limited 3D datasets
(source efficient). Experimentation reveals that our approach is generalizable
and can effectively localize and highlight parts of 3D objects (in 3D mesh)
based on implicit textual queries, including these articulated 3d objects and
real-world scanned data. Our method can also generate natural language
explanations corresponding to these 3D models and the decomposition. Moreover,
our training-free approach allows rapid deployment and serves as a viable
universal baseline for future research of part-level 3d (semantic) object
understanding in various fields including robotics, object manipulation, part
assembly, autonomous driving applications, augment reality and virtual reality
(AR/VR), and medical applications. The code, the model weight, the deployment
guide, and the evaluation protocol are: http://tianrun-chen.github.io/Reason3D/","['Tianrun Chen', 'Chunan Yu', 'Jing Li', 'Jianqi Zhang', 'Lanyun Zhu', 'Deyi Ji', 'Yong Zhang', 'Ying Zang', 'Zejian Li', 'Lingyun Sun']",2024-05-29T17:56:07Z,http://arxiv.org/abs/2405.19326v1,"['cs.CV', 'cs.GR', 'cs.HC']"
SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval,"The retrieval of 3D objects has gained significant importance in recent years
due to its broad range of applications in computer vision, computer graphics,
virtual reality, and augmented reality. However, the retrieval of 3D objects
presents significant challenges due to the intricate nature of 3D models, which
can vary in shape, size, and texture, and have numerous polygons and vertices.
To this end, we introduce a novel SHREC challenge track that focuses on
retrieving relevant 3D animal models from a dataset using sketch queries and
expedites accessing 3D models through available sketches. Furthermore, a new
dataset named ANIMAR was constructed in this study, comprising a collection of
711 unique 3D animal models and 140 corresponding sketch queries. Our contest
requires participants to retrieve 3D models based on complex and detailed
sketches. We receive satisfactory results from eight teams and 204 runs.
Although further improvement is necessary, the proposed task has the potential
to incentivize additional research in the domain of 3D object retrieval,
potentially yielding benefits for a wide range of applications. We also provide
insights into potential areas of future research, such as improving techniques
for feature extraction and matching and creating more diverse datasets to
evaluate retrieval performance. https://aichallenge.hcmus.edu.vn/sketchanimar","['Trung-Nghia Le', 'Tam V. Nguyen', 'Minh-Quan Le', 'Trong-Thuan Nguyen', 'Viet-Tham Huynh', 'Trong-Le Do', 'Khanh-Duy Le', 'Mai-Khiem Tran', 'Nhat Hoang-Xuan', 'Thang-Long Nguyen-Ho', 'Vinh-Tiep Nguyen', 'Nhat-Quynh Le-Pham', 'Huu-Phuc Pham', 'Trong-Vu Hoang', 'Quang-Binh Nguyen', 'Trong-Hieu Nguyen-Mau', 'Tuan-Luc Huynh', 'Thanh-Danh Le', 'Ngoc-Linh Nguyen-Ha', 'Tuong-Vy Truong-Thuy', 'Truong Hoai Phong', 'Tuong-Nghiem Diep', 'Khanh-Duy Ho', 'Xuan-Hieu Nguyen', 'Thien-Phuc Tran', 'Tuan-Anh Yang', 'Kim-Phat Tran', 'Nhu-Vinh Hoang', 'Minh-Quang Nguyen', 'Hoai-Danh Vo', 'Minh-Hoa Doan', 'Hai-Dang Nguyen', 'Akihiro Sugimoto', 'Minh-Triet Tran']",2023-04-12T09:40:38Z,http://arxiv.org/abs/2304.05731v2,['cs.CV']
Project Aria: A New Tool for Egocentric Multi-Modal AI Research,"Egocentric, multi-modal data as available on future augmented reality (AR)
devices provides unique challenges and opportunities for machine perception.
These future devices will need to be all-day wearable in a socially acceptable
form-factor to support always available, context-aware and personalized AI
applications. Our team at Meta Reality Labs Research built the Aria device, an
egocentric, multi-modal data recording and streaming device with the goal to
foster and accelerate research in this area. In this paper, we describe the
Aria device hardware including its sensor configuration and the corresponding
software tools that enable recording and processing of such data.","['Jakob Engel', 'Kiran Somasundaram', 'Michael Goesele', 'Albert Sun', 'Alexander Gamino', 'Andrew Turner', 'Arjang Talattof', 'Arnie Yuan', 'Bilal Souti', 'Brighid Meredith', 'Cheng Peng', 'Chris Sweeney', 'Cole Wilson', 'Dan Barnes', 'Daniel DeTone', 'David Caruso', 'Derek Valleroy', 'Dinesh Ginjupalli', 'Duncan Frost', 'Edward Miller', 'Elias Mueggler', 'Evgeniy Oleinik', 'Fan Zhang', 'Guruprasad Somasundaram', 'Gustavo Solaira', 'Harry Lanaras', 'Henry Howard-Jenkins', 'Huixuan Tang', 'Hyo Jin Kim', 'Jaime Rivera', 'Ji Luo', 'Jing Dong', 'Julian Straub', 'Kevin Bailey', 'Kevin Eckenhoff', 'Lingni Ma', 'Luis Pesqueira', 'Mark Schwesinger', 'Maurizio Monge', 'Nan Yang', 'Nick Charron', 'Nikhil Raina', 'Omkar Parkhi', 'Peter Borschowa', 'Pierre Moulon', 'Prince Gupta', 'Raul Mur-Artal', 'Robbie Pennington', 'Sachin Kulkarni', 'Sagar Miglani', 'Santosh Gondi', 'Saransh Solanki', 'Sean Diener', 'Shangyi Cheng', 'Simon Green', 'Steve Saarinen', 'Suvam Patra', 'Tassos Mourikis', 'Thomas Whelan', 'Tripti Singh', 'Vasileios Balntas', 'Vijay Baiyya', 'Wilson Dreewes', 'Xiaqing Pan', 'Yang Lou', 'Yipu Zhao', 'Yusuf Mansour', 'Yuyang Zou', 'Zhaoyang Lv', 'Zijian Wang', 'Mingfei Yan', 'Carl Ren', 'Renzo De Nardi', 'Richard Newcombe']",2023-08-24T20:42:21Z,http://arxiv.org/abs/2308.13561v3,"['cs.HC', 'cs.CV']"
