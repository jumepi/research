title,summary,authors,published,link
Inverse Augmented Reality: A Virtual Agent's Perspective,"We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']",2018-08-10T05:23:37Z,http://arxiv.org/abs/1808.03413v1
"Symmetrical Reality: Toward a Unified Framework for Physical and Virtual
  Reality","In this paper, we review the background of physical reality, virtual reality,
and some traditional mixed forms of them. Based on the current knowledge, we
propose a new unified concept called symmetrical reality to describe the
physical and virtual world in a unified perspective. Under the framework of
symmetrical reality, the traditional virtual reality, augmented reality,
inverse virtual reality, and inverse augmented reality can be interpreted using
a unified presentation. We analyze the characteristics of symmetrical reality
from two different observation locations (i.e., from the physical world and
from the virtual world), where all other forms of physical and virtual reality
can be treated as special cases of symmetrical reality.","['Zhenliang Zhang', 'Cong Wang', 'Dongdong Weng', 'Yue Liu', 'Yongtian Wang']",2019-03-07T04:29:50Z,http://arxiv.org/abs/1903.02723v1
"SelectVisAR: Selective Visualisation of Virtual Environments in
  Augmented Reality","When establishing a visual connection between a virtual reality user and an
augmented reality user, it is important to consider whether the augmented
reality user faces a surplus of information. Augmented reality, compared to
virtual reality, involves two, not one, planes of information: the physical and
the virtual. We propose SelectVisAR, a selective visualisation system of
virtual environments in augmented reality. Our system enables an augmented
reality spectator to perceive a co-located virtual reality user in the context
of four distinct visualisation conditions: Interactive, Proximity, Everything,
and Dollhouse. We explore an additional two conditions, Context and Spotlight,
in a follow-up study. Our design uses a human-centric approach to information
filtering, selectively visualising only parts of the virtual environment
related to the interactive possibilities of a virtual reality user. The
research investigates how selective visualisations can be helpful or trivial
for the augmented reality user when observing a virtual reality user.","['Robbe Cools', 'Jihae Han', 'Adalberto L. Simeone']",2021-04-17T15:47:48Z,http://arxiv.org/abs/2104.08579v2
Augmentix -- An Augmented Reality System for asymmetric Teleteaching,"Using augmented reality in education is already a common concept, as it has
the potential to turn learning into a motivational learning experience.
However, current research only covers the students site of learning. Almost no
research focuses on the teachers' site and whether augmented reality could
potentially improve his/her workflow of teaching the students or not. Many
researchers do not differentiate between multiple user roles, like a student
and a teacher. To allow investigation into these lacks of research, a teaching
system ""Augmentix"" is presented, which includes a differentiation between the
two user roles ""teacher"" and ""student"" to potentially enhances the teachers
workflow by using augmented reality. In this system's setting the student can
explore a virtual city in virtual reality and the teacher can guide him with
augmented reality.",['Nico Feld'],2021-01-07T14:43:51Z,http://arxiv.org/abs/2101.02565v1
Mobile Augmented Reality Applications,"Augmented reality have undergone considerable improvement in past years. Many
special techniques and hardware devices were developed, but the crucial
breakthrough came with the spread of intelligent mobile phones. This enabled
mass spread of augmented reality applications. However mobile devices have
limited hardware capabilities, which narrows down the methods usable for scene
analysis. In this article we propose an augmented reality application which is
using cloud computing to enable using of more complex computational methods
such as neural networks. Our goal is to create an affordable augmented reality
application suitable which will help car designers in by 'virtualizing' car
modifications.","['David Prochazka', 'Michael Stencl', 'Ondrej Popelka', 'Jiri Stastny']",2011-06-28T06:08:38Z,http://arxiv.org/abs/1106.5571v1
"Using Blippar Augmented Reality Browser in the Practical Training of
  Mechanical Engineers","The purpose of the study is to justify the expediency of using the Blippar
augmented reality browser for professional and practical training of future
mechanical engineers. Tasks of the research: to analyze the expediency of using
augmented reality tools in the professional training of bachelors of applied
mechanics; to carry out the selection of augmented reality tools, which is
expedient to use in the training of future engineer mechanics; to develop
educational materials using the chosen augmented reality tools. The object of
the study is the professional training of future mechanical engineers. The
subject of the study is the use of the augmented reality tools in the
professional training of bachelors of applied mechanics. The paper analyzes the
relevance and expediency of the use of the augmented reality tools in the
professional training of future mechanical engineers. It is determined that the
augmented reality tools will promote the development of ICT competence and
graphic competence of bachelors of applied mechanics The model of the use of
the augmented reality tools in the training of future mechanical engineers is
proposed. As the main tool, the Blippar browser and Blippbuilder's cloud-based
script development tool are chosen. An example of the creation of markers and
scenes of augmented reality using the selected tools is given. The advantages
and disadvantages of used tools are indicated. The proposed learning tools and
methods can be applied to vocational and practical training of mechanical
engineers.","['Andrii Striuk', 'Maryna Rassovytska', 'Svitlana Shokaliuk']",2018-07-01T06:51:23Z,http://arxiv.org/abs/1807.00279v1
"Augmented reality applications in manufacturing and its future scope in
  Industry 4.0","Augmented reality technology is one of the leading technologies in the
context of Industry 4.0. The promising potential application of augmented
reality in industrial production systems has received much attention, which led
to the concept of industrial augmented reality. On the one hand, this
technology provides a suitable platform that facilitates the registration of
information and access to them to help make decisions and allows concurrent
training for the user while executing the production processes. This leads to
increased work speed and accuracy of the user as a process operator and
consequently offers economic benefits to the companies. Moreover, recent
advances in the internet of things, smart sensors, and advanced algorithms have
increased the possibility of widespread and more effective use of augmented
reality. Currently, many research pieces are being done to expand the
application of augmented reality and increase its effectiveness in industrial
production processes. This research demonstrates the influence of augmented
reality in Industry 4.0 while critically reviewing the industrial augmented
reality history. Afterward, the paper discusses the critical role of industrial
augmented reality by analyzing some use cases and their prospects. With a
systematic analysis, this paper discusses the main future directions for
industrial augmented reality applications in industry 4.0. The article
investigates various areas of application for this technology and its impact on
improving production conditions. Finally, the challenges that this technology
faces and its research opportunities are discussed.","['Omid Ziaee', 'Mohsen Hamedi']",2021-12-03T20:46:50Z,http://arxiv.org/abs/2112.11190v1
Augmented Reality Implementation Methods in Mainstream Applications,"Augmented reality has became an useful tool in many areas from space
exploration to military applications. Although used theoretical principles are
well known for almost a decade, the augmented reality is almost exclusively
used in high budget solutions with a special hardware. However, in last few
years we could see rising popularity of many projects focused on deployment of
the augmented reality on different mobile devices. Our article is aimed on
developers who consider development of an augmented reality application for the
mainstream market. Such developers will be forced to keep the application
price, therefore also the development price, at reasonable level. Usage of
existing image processing software library could bring a significant cut-down
of the development costs. In the theoretical part of the article is presented
an overview of the augmented reality application structure. Further, an
approach for selection appropriate library as well as the review of the
existing software libraries focused in this area is described. The last part of
the article outlines our implementation of key parts of the augmented reality
application using the OpenCV library.","['David Prochazka', 'Tomas Koubek']",2011-06-28T05:57:37Z,http://arxiv.org/abs/1106.5569v1
"Using technology of augmented reality in a mobile-based learning
  environment of the higher educational institution","The definition of the augmented reality concept is based on the analysis of
scientific publications. It is noted that online experiments with augmented
reality provide students with the opportunity to observe and describe the
operation with real systems by changing their parameters, and also partially
replace experimental installations with objects of augmented reality. The
scheme for realizing the augmented reality is considered. The possibilities of
working with augmented reality objects in teaching physics is highlighted. It
is indicated that the use of the augmented reality tools allows to increase the
realness of the research; provides emotional and cognitive experience, helps
attract students to systematic training; provides correct information about the
installation in the process of experimentation; creates new ways of
representing real objects in the learning process.","['Yevhenii O. Modlo', 'Yuliia V. Yechkalo', 'Serhiy O. Semerikov', 'Viktoriia V. Tkachuk']",2018-07-23T12:36:54Z,http://arxiv.org/abs/1807.10659v1
"A 3D-Deep-Learning-based Augmented Reality Calibration Method for
  Robotic Environments using Depth Sensor Data","Augmented Reality and mobile robots are gaining much attention within
industries due to the high potential to make processes cost and time efficient.
To facilitate augmented reality, a calibration between the Augmented Reality
device and the environment is necessary. This is a challenge when dealing with
mobile robots due to the mobility of all entities making the environment
dynamic. On this account, we propose a novel approach to calibrate the
Augmented Reality device using 3D depth sensor data. We use the depth camera of
a cutting edge Augmented Reality Device - the Microsoft Hololens for deep
learning based calibration. Therefore, we modified a neural network based on
the recently published VoteNet architecture which works directly on the point
cloud input observed by the Hololens. We achieve satisfying results and
eliminate external tools like markers, thus enabling a more intuitive and
flexible work flow for Augmented Reality integration. The results are adaptable
to work with all depth cameras and are promising for further research.
Furthermore, we introduce an open source 3D point cloud labeling tool, which is
to our knowledge the first open source tool for labeling raw point cloud data.","['Linh Kästner', 'Vlad Catalin Frasineanu', 'Jens Lambrecht']",2019-12-27T13:56:13Z,http://arxiv.org/abs/1912.12101v1
A Survey of Augmented Reality Navigation,"Navigation has been a popular area of research in both academia and industry.
Combined with maps, and different localization technologies, navigation systems
have become robust and more usable. By combining navigation with augmented
reality, it can be improved further to become realistic and user friendly. This
paper surveys existing researches carried out in this area, describes existing
techniques for building augmented reality navigation systems, and the problems
faced.",['Gaurav Bhorkar'],2017-08-16T09:40:53Z,http://arxiv.org/abs/1708.05006v1
"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']",2022-01-13T16:54:36Z,http://arxiv.org/abs/2201.07003v1
Augmented Reality in Astrophysics,"Augmented Reality consists of merging live images with virtual layers of
information. The rapid growth in the popularity of smartphones and tablets over
recent years has provided a large base of potential users of Augmented Reality
technology, and virtual layers of information can now be attached to a wide
variety of physical objects. In this article, we explore the potential of
Augmented Reality for astrophysical research with two distinct experiments: (1)
Augmented Posters and (2) Augmented Articles. We demonstrate that the emerging
technology of Augmented Reality can already be used and implemented without
expert knowledge using currently available apps. Our experiments highlight the
potential of Augmented Reality to improve the communication of scientific
results in the field of astrophysics. We also present feedback gathered from
the Australian astrophysics community that reveals evidence of some interest in
this technology by astronomers who experimented with Augmented Posters. In
addition, we discuss possible future trends for Augmented Reality applications
in astrophysics, and explore the current limitations associated with the
technology. This Augmented Article, the first of its kind, is designed to allow
the reader to directly experiment with this technology.","['Frédéric P. A. Vogt', 'Luke J. Shingles']",2013-05-23T20:00:00Z,http://arxiv.org/abs/1305.5534v1
InAR:Inverse Augmented Reality,"Augmented reality is the art to seamlessly fuse virtual objects into real
ones. In this short note, we address the opposite problem, the inverse
augmented reality, that is, given a perfectly augmented reality scene where
human is unable to distinguish real objects from virtual ones, how the machine
could help do the job. We show by structure from motion (SFM), a simple 3D
reconstruction technique from images in computer vision, the real and virtual
objects can be easily separated in the reconstructed 3D scene.","['Hao Hu', 'Hainan Cui']",2015-08-11T14:17:28Z,http://arxiv.org/abs/1508.02606v1
Preprint ARPPS Augmented Reality Pipeline Prospect System,"This is the preprint version of our paper on ICONIP. Outdoor augmented
reality geographic information system (ARGIS) is the hot application of
augmented reality over recent years. This paper concludes the key solutions of
ARGIS, designs the mobile augmented reality pipeline prospect system (ARPPS),
and respectively realizes the machine vision based pipeline prospect system
(MVBPPS) and the sensor based pipeline prospect system (SBPPS). With the
MVBPPS's realization, this paper studies the neural network based 3D features
matching method.","['Xiaolei Zhang', 'Yong Han', 'DongSheng Hao', 'Zhihan Lv']",2015-08-18T08:18:55Z,http://arxiv.org/abs/1508.04238v1
Vision-based Pose Estimation for Augmented Reality : A Comparison Study,"Augmented reality aims to enrich our real world by inserting 3D virtual
objects. In order to accomplish this goal, it is important that virtual
elements are rendered and aligned in the real scene in an accurate and visually
acceptable way. The solution of this problem can be related to a pose
estimation and 3D camera localization. This paper presents a survey on
different approaches of 3D pose estimation in augmented reality and gives
classification of key-points-based techniques. The study given in this paper
may help both developers and researchers in the field of augmented reality.","['Hayet Belghit', 'Abdelkader Bellarbi', 'Nadia Zenati', 'Samir Otmane']",2018-06-25T08:01:45Z,http://arxiv.org/abs/1806.09316v1
Augmented Reality for Education: A Review,"Augmented Reality, or simply AR, is the incorporation of information in
digital format that includes live footage of a certain user's real-time
environment. Also now, various universities are using Augmented Reality.
Applying the technology in the education sector can result in having a smart
campus. In line with that, this paper will discuss how Augmented Reality is
being used now in different learning areas.",['Carlo H. Godoy Jr'],2021-08-07T17:27:13Z,http://arxiv.org/abs/2109.02386v1
The Cloud Technologies and Augmented Reality: the Prospects of Use,"The article discusses the prospects of the augmented reality using as a
component of a cloud-based environment. The research goals are the next: to
explore the possibility of the augmented reality using with the involvement of
the cloud-based environment components. The research objectives are the next:
to consider the notion of augmented reality; to analyze the experience the
augmented reality using within the cloud environment / system; to outline the
prospects of the augmented reality using in educational institutions; to
consider the technical conditions of the augmented reality use. The object of
research is: the educational process in educational institutions of Ukraine of
different levels of accreditation. The subject of research is: the educational
process in a cloud-based environment in educational institutions of Ukraine.
The research methods used are the next: analysis of scientific publications,
observations. The results of the research are the next: on the basis of the
analysis of scientific works, it has been established that the experience of
the augmented reality using in the systems based on cloud technologies already
exists. However, the success of such a combination has not yet been proven.
Currently, laboratory tests are known, while the experiment was not carried out
under natural conditions in control and experimental groups. It is revealed
that the attraction of the augmented reality for the educators requires the
development of new methodologies, didactic materials, updating and updating of
the curriculum. The main conclusions and recommendations: the main principles
of augmented reality use in the learning process are: designing of the
environment that is flexible enough, attention should be paid to the teaching
and didactic issues; adjusting the educational content for mastering the
material provided by the curriculum.","['Maiia V. Popel', 'Mariya P. Shyshkina']",2018-07-05T12:42:24Z,http://arxiv.org/abs/1807.01966v2
"Immercity: a curation content application in Virtual and Augmented
  reality","When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']",2018-10-24T06:23:46Z,http://arxiv.org/abs/1810.10206v1
"The Potential of Using Google Expeditions and Google Lens Tools under
  STEM-education in Ukraine","The expediency of using the augmented reality in the case of using of
STEM-education in Ukraine is shown. The features of the augmented reality and
its classification are described. The possibilities of using the Google
Expeditions and Google Lens as platforms of the augmented reality is analyzed.
A comparison, analysis, synthesis, induction and deduction was carried out to
study the potential of using augmented reality platforms in the educational
process. Main haracteristics of Google Expeditions and Google Lens are
described. There determined that augmented reality tools can improve students
motivation to learn and correspond to trends of STEM-education. However, there
problems of using of augmented reality platforms, such as the lack of awareness
of this system by teachers, the lack of guidance, the absence of the
Ukrainian-language interface and responding of educational programs of the
Ministry of Education and Science of Ukraine. There proposed to involve
methodical and pedagogical specialists to development of methodical provision
of the tools of augmented reality.","['Yevhenii B. Shapovalov', 'Zhanna I. Bilyk', 'Artem I. Atamas', 'Viktor B. Shapovalov', 'Aleksandr D. Uchitel']",2018-08-08T05:46:18Z,http://arxiv.org/abs/1808.06465v3
"Visions of augmented reality in popular culture: Power and (un)readable
  identities when the world becomes a screen","Augmented reality, where digital objects are overlaid and combined with the
ordinary visual surface, is a technology under rapid development, which has
long been a part of visions of the digital future. In this article, I examine
how gaze and power are coded into three pop-cultural visions of augmented
reality. By analyzing representations of augmented reality in science fiction
through the lens of feminist theory on performativity and intelligibility,
visibility and race, gendered gaze, and algorithmic normativity, this paper
provides a critical understanding of augmented reality as a visual technology,
and how it might change or reinforce possible norms and power relations. In
these futures where the screen no longer has any boundaries, both cooperative
and reluctant bodies are inscribed with gendered and racialized digital
markers. Reading visions of augmented reality through feminist theory, I argue
that augmented reality technologies enter into assemblages of people,
discourses, and technologies, where none of the actors necessarily has an
overview. In these assemblages, augmented reality takes on a performative and
norm-bearing role, by forming a grid of intelligibility that codifies
identities, structures hierarchical relationships, and scripts social
interactions.",['Marianne Gunderson'],2023-06-07T13:49:49Z,http://arxiv.org/abs/2306.04434v2
eCAR: edge-assisted Collaborative Augmented Reality Framework,"We propose a novel edge-assisted multi-user collaborative augmented reality
framework in a large indoor environment. In Collaborative Augmented Reality,
data communication that synchronizes virtual objects has large network traffic
and high network latency. Due to drift, CAR applications without continuous
data communication for coordinate system alignment have virtual object
inconsistency. In addition, synchronization messages for online virtual object
updates have high latency as the number of collaborative devices increases. To
solve this problem, we implement the CAR framework, called eCAR, which utilizes
edge computing to continuously match the device's coordinate system with less
network traffic. Furthermore, we extend the co-visibility graph of the edge
server to maintain virtual object spatial-temporal consistency in neighboring
devices by synchronizing a local graph. We evaluate the system quantitatively
and qualitatively in the public dataset and a physical indoor environment. eCAR
communicates data for coordinate system alignment between the edge server and
devices with less network traffic and latency. In addition, collaborative
augmented reality synchronization algorithms quickly and accurately host and
resolve virtual objects. The proposed system continuously aligns coordinate
systems to multiple devices in a large indoor environment and shares augmented
reality content. Through our system, users interact with virtual objects and
share augmented reality experiences with neighboring users.","['Jinwoo Jeon', 'Woontack Woo']",2024-05-11T02:07:33Z,http://arxiv.org/abs/2405.06872v1
"Immersed in Reality Secured by Design -- A Comprehensive Analysis of
  Security Measures in AR/VR Environments","Virtual reality and related technologies such as mixed and augmented reality
have received extensive coverage in both mainstream and fringe media outlets.
When the subject goes to a new AR headset, another AR device, or AR glasses,
the talk swiftly shifts to the technical and design details. Unfortunately, no
one seemed to care about security. Data theft and other forms of cyberattack
pose serious threats to virtual reality systems. Virtual reality goggles are
just specialist versions of computers or Internet of Things devices, whereas
virtual reality experiences are software packages. As a result, AR systems are
just as vulnerable as any other Internet of Things (IoT) device we use on a
daily basis, such as computers, tablets, and phones. Preventing and responding
to common cybersecurity threats and assaults is crucial. Cybercriminals can
exploit virtual reality headsets just like any other computer system. This
paper analysis the data breach induced by these assaults could result in a
variety of concerns, including but not limited to identity theft, the
unauthorized acquisition of personal information or network credentials, damage
to hardware and software, and so on. Augmented reality (AR) allows for
real-time monitoring and visualization of network activity, system logs, and
security alerts. This allows security professionals to immediately identify
threats, monitor suspicious activities, and fix any issues that develop. This
data can be displayed in an aesthetically pleasing and intuitively structured
format using augmented reality interfaces, enabling for faster analysis and
decision-making.","['Sameer Chauhan', 'Luv Sachdeva']",2024-01-30T21:24:52Z,http://arxiv.org/abs/2404.16839v1
"All Reality: Virtual, Augmented, Mixed (X), Mediated (X,Y), and
  Multimediated Reality","The contributions of this paper are: (1) a taxonomy of the ""Realities""
(Virtual, Augmented, Mixed, Mediated, etc.), and (2) some new kinds of
""reality"" that come from nature itself, i.e. that expand our notion beyond
synthetic realities to include also phenomenological realities.
  VR (Virtual Reality) replaces the real world with a simulated experience
(virtual world). AR (Augmented Reality) allows a virtual world to be
experienced while also experiencing the real world at the same time. Mixed
Reality provides blends that interpolate between real and virtual worlds in
various proportions, along a ""Virtuality"" axis, and extrapolate to an ""X-axis"".
Mediated Reality goes a step further by mixing/blending and also modifying
reality. This modifying of reality introduces a second axis. Mediated Reality
is useful as a seeing aid (e.g. modifying reality to make it easier to
understand), and for psychology experiments like Stratton's 1896 upside-down
eyeglasses experiment.
  We propose Multimediated Reality as a multidimensional multisensory mediated
reality that includes not just interactive multimedia-based reality for our
five senses, but also includes additional senses (like sensory sonar, sensory
radar, etc.), as well as our human actions/actuators. These extra senses are
mapped to our human senses using synthetic synesthesia. This allows us to
directly experience real (but otherwise invisible) phenomena, such as wave
propagation and wave interference patterns, so that we can see radio waves and
sound waves and how they interact with objects and each other. Multimediated
reality is multidimensional, multimodal, multisensory, and multiscale. It is
also multidisciplinary, in that we must consider not just the user, but also
how the technology affects others, e.g. how its physical appearance affects
social situations.","['Steve Mann', 'Tom Furness', 'Yu Yuan', 'Jay Iorio', 'Zixin Wang']",2018-04-20T15:40:39Z,http://arxiv.org/abs/1804.08386v1
Preserving History through Augmented Reality,"Extended reality can weave together the fabric of the past, present, and
future. A two-day design hackathon was held to bring the community together
through a love for history and a common goal to use technology for good.
Through interviewing an influential community elder, Emile Pitre, and
referencing his book Revolution to Evolution, my team developed an augmented
reality artifact to tell his story and preserve on revolutionary's legacy that
impacted the University of Washington's history forever.",['Annie Yang'],2024-04-20T01:41:53Z,http://arxiv.org/abs/2404.13229v1
"Affordance Analysis of Virtual and Augmented Reality Mediated
  Communication","Virtual and augmented reality communication platforms are seen as promising
modalities for next-generation remote face-to-face interactions. Our study
attempts to explore non-verbal communication features in relation to their
conversation context for virtual and augmented reality mediated communication
settings. We perform a series of user experiments, triggering nine conversation
tasks in 4 settings, each containing corresponding non-verbal communication
features. Our results indicate that conversation types which involve less
emotional engagement are more likely to be acceptable in virtual reality and
augmented reality settings with low-fidelity avatar representation, compared to
scenarios that involve high emotional engagement or intellectually difficult
discussions. We further systematically analyze and rank the impact of
low-fidelity representation of micro-expressions, body scale, head pose, and
hand gesture in affecting the user experience in one-on-one conversations, and
validate that preserving micro-expression cues plays the most effective role in
improving bi-directional conversations in future virtual and augmented reality
settings.","['Mohammad Keshavarzi', 'Michael Wu', 'Michael N. Chin', 'Robert N. Chin', 'Allen Y. Yang']",2019-04-09T15:07:51Z,http://arxiv.org/abs/1904.04723v1
"Personalization of learning using adaptive technologies and augmented
  reality","The research is aimed at developing the recommendations for educators on
using adaptive technologies and augmented reality in personalized learning
implementation. The latest educational technologies related to learning
personalization and the adaptation of its content to the individual needs of
students and group work are considered. The current state of research is
described, the trends of development are determined. Due to a detailed analysis
of scientific works, a retrospective of the development of adaptive and, in
particular, cloud-oriented systems is shown. The preconditions of their
appearance and development, the main scientific ideas that contributed to this
are analyzed. The analysis showed that the scientists point to four possible
types of semantic interaction of augmented reality and adaptive technologies.
The adaptive cloud-based educational systems design is considered as the
promising trend of research. It was determined that adaptability can be
manifested in one or a combination of several aspects: content, evaluation and
consistency. The cloud technology is taken as a platform for integrating
adaptive learning with augmented reality as the effective modern tools to
personalize learning. The prospects of the adaptive cloud-based systems design
in the context of teachers training are evaluated. The essence and place of
assistive technologies in adaptive learning systems design are defined. It is
shown that augmented reality can be successfully applied in inclusive
education. The ways of combining adaptive systems and augmented reality tools
to support the process of teachers training are considered. The recommendations
on the use of adaptive cloud-based systems in teacher education are given.","['Maiia Marienko', 'Yulia Nosenko', 'Mariya Shyshkina']",2020-11-08T21:34:05Z,http://arxiv.org/abs/2011.05802v1
"Virtualization of Classical Reality: Limits and Possibilities in
  Physical Simulation","This study explores the virtualization of classical reality and aims to
establish a clear framework to determine the limits and possibilities of
virtual reality. It addresses two primary questions: whether an observer's
senses can perceive a different reality through appropriate equipment, and
whether it is possible to simulate a reality without the laws of physics. As
virtual and augmented reality are increasingly used in various fields, it is
crucial to provide well-founded responses to these inquiries. Understanding the
limitations and achievability of virtual reality is essential for creating
realistic environments in education, entertainment, and other domains.
Additionally, considering the role of physics and scientific rigor in virtual
contexts is important. The study presents a theoretical framework divided into
three sections: Methods, Results, and Discussion. The Methods section explains
the nature of computers and their ability to create perceived virtual reality.
The Results section introduces the theoretical framework, emphasizing
observable simulation and interactive simulation and highlighting their
distinctions. Finally, the Discussion section builds upon the theoretical
foundation to provide comprehensive insights and answers to the research
questions. This study enhances our understanding of the boundaries and
possibilities of virtual reality, offering concrete answers and valuable
knowledge for the development and application of virtual reality in various
domains.",['Francesco Sisini'],2023-06-12T08:36:13Z,http://arxiv.org/abs/2306.07955v1
Multi-layer Visualization for Medical Mixed Reality,"Medical Mixed Reality helps surgeons to contextualize intraoperative data
with video of the surgical scene. Nonetheless, the surgical scene and
anatomical target are often occluded by surgical instruments and surgeon hands.
In this paper and to our knowledge, we propose a multi-layer visualization in
Medical Mixed Reality solution which subtly improves a surgeon's visualization
by making transparent the occluding objects. As an example scenario, we use an
augmented reality C-arm fluoroscope device. A video image is created using a
volumetric-based image synthesization technique and stereo-RGBD cameras mounted
on the C-arm. From this synthesized view, the background which is occluded by
the surgical instruments and surgeon hands is recovered by modifying the
volumetric-based image synthesization technique. The occluding objects can,
therefore, become transparent over the surgical scene. Experimentation with
different augmented reality scenarios yield results demonstrating that the
background of the surgical scenes can be recovered with accuracy between
45%-99%. In conclusion, we presented a solution that a Mixed Reality solution
for medicine, providing transparency to objects occluding the surgical scene.
This work is also the first application of volumetric field for Diminished
Reality/ Mixed Reality.","['Séverine Habert', 'Ma Meng', 'Pascal Fallavollita', 'Nassir Navab']",2017-09-26T12:13:01Z,http://arxiv.org/abs/1709.08962v1
"Integrative Object and Pose to Task Detection for an
  Augmented-Reality-based Human Assistance System using Neural Networks","As a result of an increasingly automatized and digitized industry, processes
are becoming more complex. Augmented Reality has shown considerable potential
in assisting workers with complex tasks by enhancing user understanding and
experience with spatial information. However, the acceptance and integration of
AR into industrial processes is still limited due to the lack of established
methods and tedious integration efforts. Meanwhile, deep neural networks have
achieved remarkable results in computer vision tasks and bear great prospects
to enrich Augmented Reality applications . In this paper, we propose an
Augmented-Reality-based human assistance system to assist workers in complex
manual tasks where we incorporate deep neural networks for computer vision
tasks. More specifically, we combine Augmented Reality with object and action
detectors to make workflows more intuitive and flexible. To evaluate our system
in terms of user acceptance and efficiency, we conducted several user studies.
We found a significant reduction in time to task completion in untrained
workers and a decrease in error rate. Furthermore, we investigated the users
learning curve with our assistance system.","['Linh Kästner', 'Leon Eversberg', 'Marina Mursa', 'Jens Lambrecht']",2020-08-31T08:24:06Z,http://arxiv.org/abs/2008.13419v1
Mixed Reality Interaction Techniques,"This chapter gives an overview of interaction techniques for mixed reality
including augmented and virtual reality (AR/VR). Various modalities for input
and output are discussed. Specifically, techniques for tangible and
surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and
mouse-based, as well as haptic interaction are discussed. Furthermore, the
combination of multiple modalities in multisensory and multimodal interaction,
as well as interaction using multiple physical or virtual displays, are
presented. Finally, interaction with intelligent virtual agents is considered.",['Jens Grubert'],2021-03-10T10:47:10Z,http://arxiv.org/abs/2103.05984v1
"Beyond the Screen: Reshaping the Workplace with Virtual and Augmented
  Reality","Although extended reality technologies have enjoyed an explosion in
popularity in recent years, few applications are effectively used outside the
entertainment or academic contexts. This work consists of a literature review
regarding the effective integration of such technologies in the workplace. It
aims to provide an updated view of how they are being used in that context.
First, we examine existing research concerning virtual, augmented, and
mixed-reality applications. We also analyze which have made their way to the
workflows of companies and institutions. Furthermore, we circumscribe the
aspects of extended reality technologies that determined this applicability.","['Nuno Verdelho Trindade', 'Alfredo Ferreira', 'João Madeiras Pereira']",2023-12-01T08:05:22Z,http://arxiv.org/abs/2312.00408v1
Taxonomy of Virtual and Augmented Reality Applications in Education,"This paper presents and analyses existing taxonomies of virtual and augmented
reality and demonstrates knowledge gaps and mixed terminology which may cause
confusion among educators, researchers, and developers. Several such occasions
of confusion are presented. A methodology is then presented to construct a
taxonomy of virtual reality and augmented reality applications based on a
combination of: a faceted analysis approach for the overall design of the
taxonomy; an existing taxonomy of educational objectives to derive the
educational purpose; an information systems analysis to establish important
facets of the taxonomy; and two systematic mapping studies to identify
categories within each facet. Based onUsing thisthe methodology a new taxonomy
is proposed and the implications of its facets (and their combinations of
facets)are demonstrated. The taxonomy focuses on technology used to provide the
virtual or augmented reality as well as the content presented to the user,
including the type of gamification and how it is operated. It also takes into
accountaccommodates a large number of devices and approaches developed
throughout the years and for multiple industries, and proposes and
developsprovides a way to categorize them in order to clarify communication
between researchers, developers and as well as educators. Use of the taxonomy
and implications of choices made during their development is then demonstrated
ion two case studies:, a virtual reality chemical plant for use in chemical
engineering education and an augmented reality dog for veterinary education.","['Jiri Motejlek', 'Esat Alpay']",2021-12-08T23:15:11Z,http://arxiv.org/abs/2112.04619v1
Tollan-Xicocotitlan: A reconstructed City by augmented reality,"This project presents the analysis, design, implementation and results of
Reconstruction Xicocotitlan Tollan-through augmented reality, which will
release information about the Toltec culture supplemented by presenting an
overview of the main premises of the Xicocotitlan Tollan city supported
dimensional models based on the augmented reality technique showing the user a
virtual representation of buildings in Tollan.","['Martha Rosa Cordero Lopez', 'Marco Antonio Dorantes Gonzalez']",2014-06-19T18:57:03Z,http://arxiv.org/abs/1406.5151v1
The History of Mobile Augmented Reality,"This document summarizes the major milestones in mobile Augmented Reality
between 1968 and 2014. Major parts of the list were compiled by the member of
the Christian Doppler Laboratory for Handheld Augmented Reality in 2010 (author
list in alphabetical order) for the ISMAR society. Later in 2013 it was
updated, and more recent work was added during preparation of this report.
Permission is granted to copy and modify.","['Clemens Arth', 'Raphael Grasset', 'Lukas Gruber', 'Tobias Langlotz', 'Alessandro Mulloni', 'Daniel Wagner']",2015-05-06T10:56:12Z,http://arxiv.org/abs/1505.01319v3
Towards an augmented reality fourth generation social networks,"A concept of fourth generation social network is described as one that, built
on the features of augmented reality (AR), is able to implement an enriched
layer of digital information that displays in People Augmented Reality (PAR)
devices data shared by users in social networks. This PAR layer is accessed by
the users in their devices through camera effects when targeting with a mobile
phone to a user holding a mobile device with AGPS and with a profile in social
media. The social network of fourth generation will be a combination between
Facebook and Pokemon Go.","['Andres Montero', 'Borja Belaza']",2017-05-13T08:21:32Z,http://arxiv.org/abs/1705.04798v1
"Development of Internet of Things, Augmented Reality and 5G technologies
  (review)","Just as the emergence of personal computers and smartphones has changed the
life of modern society, the Internet of Things, augmented reality and
ultra-fast and reliable telecommunications networks of the new generation, by
combining the physical objects of the real world with the ever-increasing
computing power and intelligence of cyberspace, will make the next big
revolution in all spheres of human activity. Keywords: Internet of Things, 5G,
augmented reality.","['A. S. Smirnov', 'A. V. Tumialis', 'K. S. Golokhvast']",2019-02-21T12:47:51Z,http://arxiv.org/abs/1902.08008v1
"I-nteract: A cyber-physical system for real-time interaction with
  physical and virtual objects using mixed reality technologies for additive
  manufacturing","This paper presents I-nteract, a cyber-physical system that enables real-time
interaction with real and virtual objects in a mixed augmented reality
environment to design 3D models for additive manufacturing. The system has been
developed using mixed reality technologies such as HoloLens, for augmenting
visual feedback, and haptic gloves, for augmenting haptic force feedback. The
efficacy of the system has been demonstrated by generating 3D model using a
novel scanning method to 3D print a customized orthopedic cast for human arm,
by estimating spring rates of compression springs, and by simulating
interaction with a virtual spring using hand.","['Ammar Malik', 'Hugo Lhachemi', 'Robert Shorten']",2020-02-14T22:57:35Z,http://arxiv.org/abs/2002.06280v1
Revealing Aspects of Hawai'i Tourism Using Situated Augmented Reality,"In this position paper, we present a process artifact that aims to bring
awareness to historical context, contemporary issues, and identity harm
inflicted by tourism in Hawaii. First, we introduce the historical background
and how the work is informed by the positionality of the authors. We discuss
how related augmented reality work can inform strategy for building augmented
reality experiences that address cultural issues. Then, we present a mockup of
the artifact, aimed to bring awareness to 20th century colonialism, recent
Kanaka Maoli art exclusion, and cultural prostitution. We describe how we will
share the app at the workshop and list topics for discussion.","['Karen Abe', 'Jules Park', 'Samir Ghosh']",2024-04-24T03:01:34Z,http://arxiv.org/abs/2404.15610v1
Projection Mapping Technologies for AR,"This invited talk will present recent projection mapping technologies for
augmented reality. First, fundamental technologies are briefly explained, which
have been proposed to overcome the technical limitations of ordinary
projectors. Second, augmented reality (AR) applications using projection
mapping technologies are introduced.",['Daisuke Iwai'],2017-04-10T15:13:40Z,http://arxiv.org/abs/1704.02897v1
"Quality-Aware Real-Time Augmented Reality Visualization under Delay
  Constraints","Augmented reality (AR) is one of emerging applications in modern multimedia
systems research. Due to intensive time-consuming computations for AR
visualization in mobile devices, quality-aware real-time computing under delay
constraints is essentially required. Inspired by Lyapunov optimization
framework, this paper proposes a time-average quality maximization method for
the AR visualization under delay considerations.","['Rhoan Lee', 'Soohyun Park', 'Soyi Jung', 'Joongheon Kim']",2022-05-01T06:31:50Z,http://arxiv.org/abs/2205.00407v1
"Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced
  Human-Robot Interaction and Robotic Interfaces","This paper contributes to a taxonomy of augmented reality and robotics based
on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have
emerged as a new way to enhance human-robot interaction (HRI) and robotic
interfaces (e.g., actuated and shape-changing interfaces). Recently, an
increasing number of studies in HCI, HRI, and robotics have demonstrated how AR
enables better interactions between people and robots. However, often research
remains focused on individual explorations and key design strategies, and
research questions are rarely analyzed systematically. In this paper, we
synthesize and categorize this research field in the following dimensions: 1)
approaches to augmenting reality; 2) characteristics of robots; 3) purposes and
benefits; 4) classification of presented information; 5) design components and
strategies for visual augmentation; 6) interaction techniques and modalities;
7) application domains; and 8) evaluation strategies. We formulate key
challenges and opportunities to guide and inform future research in AR and
robotics.","['Ryo Suzuki', 'Adnan Karim', 'Tian Xia', 'Hooman Hedayati', 'Nicolai Marquardt']",2022-03-07T10:22:59Z,http://arxiv.org/abs/2203.03254v1
"Merging real and virtual worlds: An analysis of the state of the art and
  practical evaluation of Microsoft Hololens","Achieving a symbiotic blending between reality and virtuality is a dream that
has been lying in the minds of many people for a long time. Advances in various
domains constantly bring us closer to making that dream come true. Augmented
reality as well as virtual reality are in fact trending terms and are expected
to further progress in the years to come.
  This master's thesis aims to explore these areas and starts by defining
necessary terms such as augmented reality (AR) or virtual reality (VR). Usual
taxonomies to classify and compare the corresponding experiences are then
discussed.
  In order to enable those applications, many technical challenges need to be
tackled, such as accurate motion tracking with 6 degrees of freedom (positional
and rotational), that is necessary for compelling experiences and to prevent
user sickness. Additionally, augmented reality experiences typically rely on
image processing to position the superimposed content. To do so, ""paper""
markers or features extracted from the environment are often employed. Both
sets of techniques are explored and common solutions and algorithms are
presented.
  After investigating those technical aspects, I carry out an objective
comparison of the existing state-of-the-art and state-of-the-practice in those
domains, and I discuss present and potential applications in these areas. As a
practical validation, I present the results of an application that I have
developed using Microsoft HoloLens, one of the more advanced affordable
technologies for augmented reality that is available today. Based on the
experience and lessons learned during this development, I discuss the
limitations of current technologies and present some avenues of future
research.",['Adrien Coppens'],2017-06-25T13:10:39Z,http://arxiv.org/abs/1706.08096v1
"Cross-Reality Re-Rendering: Manipulating between Digital and Physical
  Realities","The advent of personalized reality has arrived. Rapid development in AR/MR/VR
enables users to augment or diminish their perception of the physical world.
Robust tooling for digital interface modification enables users to change how
their software operates. As digital realities become an increasingly-impactful
aspect of human lives, we investigate the design of a system that enables users
to manipulate the perception of both their physical realities and digital
realities. Users can inspect their view history from either reality, and
generate interventions that can be interoperably rendered cross-reality in
real-time. Personalized interventions can be generated with mask, text, and
model hooks. Collaboration between users scales the availability of
interventions. We verify our implementation against our design requirements
with cognitive walkthroughs, personas, and scalability tests.",['Siddhartha Datta'],2022-11-15T09:31:52Z,http://arxiv.org/abs/2211.08005v2
"The Impact of Social Environment and Interaction Focus on User
  Experience and Social Acceptability of an Augmented Reality Game","One of the most promising technologies inside the Extended Reality (XR)
spectrum is Augmented Reality. This technology is already in people's pockets
regarding Mobile Augmented Reality with their smartphones. The scientific
community still needs answers about how humans could and should interact in
environments where perceived stimuli are different from fully physical or
digital circumstances. Moreover, it is still being determined if people accept
these new technologies in different social environments and interaction
settings or if some obstacles could exist. This paper explores the impact of
the Social Environment and the Focus of social interaction on users while
playing a location-based augmented reality game, measuring it with user
experience and social acceptance indicators. An empirical study in a
within-subject fashion was performed in different social environments and under
different settings of social interaction focus with N = 28 participants
compiling self-reported questionnaires after playing a Scavenger Hunt in
Augmented Reality. The measures from two different Social Environments (Crowded
vs. Uncrowded) resulted in statistically relevant mean differences with
indicators from the Social Acceptability dimension. Moreover, the analyses show
statistically relevant differences between the variances from different degrees
of Social Interaction Focus with Overall Social Presence, Perceived
Psychological Engagement, Perceived Attentional Engagement, and Perceived
Emotional Contagion. The results suggest that a location-based AR game played
in different social environments and settings can influence the user
experience's social dimension. Therefore, they should be carefully considered
while designing immersive technological experiences in public spaces involving
social interactions between players.","['Lorenzo Cocchia', 'Maurizio Vergari', 'Tanja Kojic', 'Francesco Vona', 'Sebastian Moller', 'Franca Garzotto', 'Jan-Niklas Voigt-Antons']",2024-04-25T10:04:36Z,http://arxiv.org/abs/2404.16479v1
Beyond the Metaverse: XV (eXtended meta/uni/Verse),"We propose the term and concept XV (eXtended meta/omni/uni/Verse) as an
alternative to, and generalization of, the shared/social virtual reality widely
known as ``metaverse''. XV is shared/social XR. We, and many others, use XR
(eXtended Reality) as a broad umbrella term and concept to encompass all the
other realities, where X is an ``anything'' variable, like in mathematics, to
denote any reality, X $\in$ \{physical, virtual, augmented, \ldots \} reality.
Therefore XV inherits this generality from XR. We begin with a very simple
organized taxonomy of all these realities in terms of two simple building
blocks: (1) physical reality (PR) as made of ``atoms'', and (2) virtual reality
(VR) as made of ``bits''. Next we introduce XV as combining all these realities
with extended society as a three-dimensional space and taxonomy of (1)
``atoms'' (physical reality), (2) ``bits'' (virtuality), and (3) ``genes''
(sociality). Thus those working in the liminal space between Virtual Reality
(VR), Augmented Reality (AR), metaverse, and their various extensions, can
describe their work and research as existing in the new field of XV. XV
includes the metaverse along with extensions of reality itself like shared
seeing in the infrared, ultraviolet, and shared seeing of electromagnetic radio
waves, sound waves, and electric currents in motors. For example, workers in a
mechanical room can look at a pump and see a superimposed time-varying waveform
of the actual rotating magnetic field inside its motor, in real time, while
sharing this vision across multiple sites.
  Presented at IEEE Standards Association, Behind and Beyond the Metaverse: XV
(eXtended meta/uni/Verse), Thurs. Dec. 8, 2022, 2:15-3:30pm, EST.","['Steve Mann', 'Yu Yuan', 'Tom Furness', 'Joseph Paradiso', 'Thomas Coughlin']",2022-12-15T16:49:32Z,http://arxiv.org/abs/2212.07960v1
"A Comparison of Visualisation Methods for Disambiguating Verbal Requests
  in Human-Robot Interaction","Picking up objects requested by a human user is a common task in human-robot
interaction. When multiple objects match the user's verbal description, the
robot needs to clarify which object the user is referring to before executing
the action. Previous research has focused on perceiving user's multimodal
behaviour to complement verbal commands or minimising the number of follow up
questions to reduce task time. In this paper, we propose a system for reference
disambiguation based on visualisation and compare three methods to disambiguate
natural language instructions. In a controlled experiment with a YuMi robot, we
investigated real-time augmentations of the workspace in three conditions --
mixed reality, augmented reality, and a monitor as the baseline -- using
objective measures such as time and accuracy, and subjective measures like
engagement, immersion, and display interference. Significant differences were
found in accuracy and engagement between the conditions, but no differences
were found in task time. Despite the higher error rates in the mixed reality
condition, participants found that modality more engaging than the other two,
but overall showed preference for the augmented reality condition over the
monitor and mixed reality conditions.","['Elena Sibirtseva', 'Dimosthenis Kontogiorgos', 'Olov Nykvist', 'Hakan Karaoguz', 'Iolanda Leite', 'Joakim Gustafson', 'Danica Kragic']",2018-01-26T11:24:47Z,http://arxiv.org/abs/1801.08760v1
Augmenting reality to diminish distractions for cognitive enhancement,"Smartphones are integral to modern life, yet research highlights the
cognitive drawbacks associated even with their mere presence. Physically
removing them from sight is a solution, but it is sometimes impractical and may
increase anxiety due to fear of missing out. In response, we introduce a simple
but effective use of augmented reality (AR) head-mounted displays, focusing not
on augmenting reality with virtual objects, but on diminishing reality by
selectively removing or occluding distracting objects, from the user's field of
view. We compared cognitive task performance across four conditions: the
smartphone being physically nearby, physically remote, visually removed and
visually occluded via AR. Our findings reveal that using AR to visually cancel
out smartphones significantly mitigates cognitive distractions caused by their
presence. Specifically, the AR interventions had effects similar to physically
removing the phone. These results suggest potential for novel AR applications
designed to diminish reality, thereby enhancing cognitive performance.","['JangHyeon Lee', 'Lawrence H. Kim']",2024-03-06T17:35:48Z,http://arxiv.org/abs/2403.03875v1
"Impact of spatial auditory navigation on user experience during
  augmented outdoor navigation tasks","The auditory sense of humans is important when it comes to navigation. The
importance is especially high in cases when an object of interest is visually
partly or fully covered. Interactions with users of technology are mainly
focused on the visual domain of navigation tasks. This paper presents the
results of a literature review and user study exploring the impact of spatial
auditory navigation on user experience during an augmented outdoor navigation
task. For the user test, participants used an augmented reality app guiding
them to different locations with different digital augmentation. We conclude
that the utilization of the auditory sense is yet still underrepresented in
augmented reality applications. In the future, more usage scenarios for
audio-augmented reality such as navigation will enhance user experience and
interaction quality.","['Jan-Niklas Voigt-Antons', 'Zhirou Sun', 'Maurizio Vergari', 'Navid Ashrafi', 'Francesco Vona', 'Tanja Kojic']",2024-04-25T09:57:07Z,http://arxiv.org/abs/2404.16473v1
Feature Extraction in Augmented Reality,"Augmented Reality (AR) is used for various applications associated with the
real world. In this paper, first, describe characteristics and essential
services of AR. Brief history on Virtual Reality (VR) and AR is also mentioned
in the introductory section. Then, AR Technologies along with its workflow is
depicted, which includes the complete AR Process consisting of the stages of
Image Acquisition, Feature Extraction, Feature Matching, Geometric
Verification, and Associated Information Retrieval. Feature extraction is the
essence of AR hence its details are furnished in the paper.","['Jekishan K. Parmar', 'Ankit Desai']",2019-11-09T14:24:56Z,http://arxiv.org/abs/1911.09177v1
The Economics of Augmented and Virtual Reality,"This paper explores the economics of Augmented Reality (AR) and Virtual
Reality (VR) technologies within decision-making contexts. Two metrics are
proposed: Context Entropy, the informational complexity of an environment, and
Context Immersivity, the value from full immersion. The analysis suggests that
AR technologies assist in understanding complex contexts, while VR technologies
provide access to distant, risky, or expensive environments. The paper provides
a framework for assessing the value of AR and VR applications in various
business sectors by evaluating the pre-existing context entropy and context
immersivity. The goal is to identify areas where immersive technologies can
significantly impact and distinguish those that may be overhyped.","['Joshua Gans', 'Abhishek Nagaraj']",2023-05-26T12:26:14Z,http://arxiv.org/abs/2305.16872v1
"OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using
  Semantic Understanding in Mixed Reality","One key challenge in augmented reality is the placement of virtual content in
natural locations. Existing automated techniques are only able to work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce a new
open-vocabulary method for object placement. Our eight-stage pipeline leverages
recent advances in segmentation models, vision-language models, and LLMs to
place any virtual object in any AR camera frame or scene. In a preliminary user
study, we show that our method performs at least as well as human experts 57%
of the time.","['Luke Yoffe', 'Aditya Sharma', 'Tobias Höllerer']",2023-12-20T07:34:20Z,http://arxiv.org/abs/2312.12815v1
"Preprint Touch-less Interactive Augmented Reality Game on Vision Based
  Wearable Device","This is the preprint version of our paper on Personal and Ubiquitous
Computing. There is an increasing interest in creating pervasive games based on
emerging interaction technologies. In order to develop touch-less, interactive
and augmented reality games on vision-based wearable device, a touch-less
motion interaction technology is designed and evaluated in this work. Users
interact with the augmented reality games with dynamic hands/feet gestures in
front of the camera, which triggers the interaction event to interact with the
virtual object in the scene. Three primitive augmented reality games with
eleven dynamic gestures are developed based on the proposed touch-less
interaction technology as proof. At last, a comparing evaluation is proposed to
demonstrate the social acceptability and usability of the touch-less approach,
running on a hybrid wearable framework or with Google Glass, as well as
workload assessment, user's emotions and satisfaction.","['Zhihan Lv', 'Alaa Halawani', 'Shengzhong Feng', 'Shafiq ur Rehman', 'Haibo Li']",2015-04-23T22:55:08Z,http://arxiv.org/abs/1504.06359v5
"Interaktion mit 3D-Objekten in Augmented Reality Anwendungen auf mobilen
  Android Geräten","This bachelor's thesis describes the conception and implementation of an
augmented reality application for the Android platform. The intention is to
demonstrate some possibilities of interaction within an augmented reality
environment on mobile devices. For that purpose, a 3D-model is displayed on the
devices' touchscreen using marker-based tracking. This enables the user to
translate, rotate or scale the model as he wishes. He can additionally select
and highlight preassigned parts of the model to display specific information
for that element. To assist developers in modifying the application for
changing requirements without re-writing large portions of the source code, the
information for each part have been encapsulated into its own data type. After
an introduction to augmented reality, its underlying technology and the Android
platform, some possible usage scenarios and the resulting functionalities are
outlined. Finally, the design as well as the developed implementation are
described.",['Lennart Brüggemann'],2016-12-27T21:08:55Z,http://arxiv.org/abs/1701.01644v1
Enabling Self-aware Smart Buildings by Augmented Reality,"Conventional HVAC control systems are usually incognizant of the physical
structures and materials of buildings. These systems merely follow pre-set HVAC
control logic based on abstract building thermal response models, which are
rough approximations to true physical models, ignoring dynamic spatial
variations in built environments. To enable more accurate and responsive HVAC
control, this paper introduces the notion of ""self-aware"" smart buildings, such
that buildings are able to explicitly construct physical models of themselves
(e.g., incorporating building structures and materials, and thermal flow
dynamics). The question is how to enable self-aware buildings that
automatically acquire dynamic knowledge of themselves. This paper presents a
novel approach using ""augmented reality"". The extensive user-environment
interactions in augmented reality not only can provide intuitive user
interfaces for building systems, but also can capture the physical structures
and possibly materials of buildings accurately to enable real-time building
simulation and control. This paper presents a building system prototype
incorporating augmented reality, and discusses its applications.","['Muhammad Aftab', 'Sid Chi-Kin Chau', 'Majid Khonji']",2017-08-17T08:56:01Z,http://arxiv.org/abs/1708.05174v2
ARbis Pictus: A Study of Language Learning with Augmented Reality,"This paper describes ""ARbis Pictus"" --a novel system for immersive language
learning through dynamic labeling of real-world objects in augmented reality.
We describe a within-subjects lab-based study (N=52) that explores the effect
of our system on participants learning nouns in an unfamiliar foreign language,
compared to a traditional flashcard-based approach. Our results show that the
immersive experience of learning with virtual labels on real-world objects is
both more effective and more enjoyable for the majority of participants,
compared to flashcards. Specifically, when participants learned through
augmented reality, they scored significantly better by 7% (p=0.011) on
productive recall tests performed same-day, and significantly better by 21%
(p=0.001) on 4-day delayed productive recall post tests than when they learned
using the flashcard method. We believe this result is an indication of the
strong potential for language learning in augmented reality, particularly
because of the improvement shown in sustained recall compared to the
traditional approach.","['Adam Ibrahim', 'Brandon Huynh', 'Jonathan Downey', 'Tobias Höllerer', 'Dorothy Chun', ""John O'Donovan""]",2017-11-30T05:46:01Z,http://arxiv.org/abs/1711.11243v2
"PlutoAR: An Inexpensive, Interactive And Portable Augmented Reality
  Based Interpreter For K-10 Curriculum","The regular K-10 curriculums often do not get the necessary of affordable
technology involving interactive ways of teaching the prescribed curriculum
with effective analytical skill building. In this paper, we present ""PlutoAR"",
a paper-based augmented reality interpreter which is scalable, affordable,
portable and can be used as a platform for skill building for the kids. PlutoAR
manages to overcome the conventional albeit non-interactive ways of teaching by
incorporating augmented reality (AR) through an interactive toolkit to provide
students with the best of both worlds. Students cut out paper ""tiles"" and place
these tiles one by one on a larger paper surface called ""Launchpad"" and use the
PlutoAR mobile application which runs on any Android device with a camera and
uses augmented reality to output each step of the program like an interpreter.
PlutoAR has inbuilt AR experiences like stories, maze solving using conditional
loops, simple elementary mathematics and the intuition of gravity.","['Shourya Pratap Singh', 'Ankit Kumar Panda', 'Susobhit Panigrahi', 'Ajaya Kumar Dash', 'Debi Prosad Dogra']",2018-09-02T19:05:26Z,http://arxiv.org/abs/1809.00375v2
Instant Motion Tracking and Its Applications to Augmented Reality,"Augmented Reality (AR) brings immersive experiences to users. With recent
advances in computer vision and mobile computing, AR has scaled across
platforms, and has increased adoption in major products. One of the key
challenges in enabling AR features is proper anchoring of the virtual content
to the real world, a process referred to as tracking. In this paper, we present
a system for motion tracking, which is capable of robustly tracking planar
targets and performing relative-scale 6DoF tracking without calibration. Our
system runs in real-time on mobile phones and has been deployed in multiple
major products on hundreds of millions of devices.","['Jianing Wei', 'Genzhi Ye', 'Tyler Mullen', 'Matthias Grundmann', 'Adel Ahmadyan', 'Tingbo Hou']",2019-07-16T00:13:09Z,http://arxiv.org/abs/1907.06796v1
"Real or Virtual? Using Brain Activity Patterns to differentiate Attended
  Targets during Augmented Reality Scenarios","Augmented Reality is the fusion of virtual components and our real
surroundings. The simultaneous visibility of generated and natural objects
often requires users to direct their selective attention to a specific target
that is either real or virtual. In this study, we investigated whether this
target is real or virtual by using machine learning techniques to classify
electroencephalographic (EEG) data collected in Augmented Reality scenarios. A
shallow convolutional neural net classified 3 second data windows from 20
participants in a person-dependent manner with an average accuracy above 70\%
if the testing data and training data came from different trials.
Person-independent classification was possible above chance level for 6 out of
20 participants. Thus, the reliability of such a Brain-Computer Interface is
high enough for it to be treated as a useful input mechanism for Augmented
Reality applications.","['Lisa-Marie Vortmann', 'Leonid Schwenke', 'Felix Putze']",2021-01-12T19:08:39Z,http://arxiv.org/abs/2101.05272v1
Visualizing Robot Intent for Object Handovers with Augmented Reality,"Humans are highly skilled in communicating their intent for when and where a
handover would occur. However, even the state-of-the-art robotic
implementations for handovers typically lack of such communication skills. This
study investigates visualization of the robot's internal state and intent for
Human-to-Robot Handovers using Augmented Reality. Specifically, we explore the
use of visualized 3D models of the object and the robotic gripper to
communicate the robot's estimation of where the object is and the pose in which
the robot intends to grasp the object. We tested this design via a user study
with 16 participants, in which each participant handed over a cube-shaped
object to the robot 12 times. Results show communicating robot intent via
augmented reality substantially improves the perceived experience of the users
for handovers. Results also indicate that the effectiveness of augmented
reality is even more pronounced for the perceived safety and fluency of the
interaction when the robot makes errors in localizing the object.","['Rhys Newbury', 'Akansel Cosgun', 'Tysha Crowley-Davis', 'Wesley P. Chan', 'Tom Drummond', 'Elizabeth Croft']",2021-03-06T07:57:01Z,http://arxiv.org/abs/2103.04055v3
"Developing an Augmented Reality-Based Game as a Supplementary tool for
  SHS- STEM Precalculus to Avoid Math Anxiety","Math Anxiety is experienced by students. This is caused mainly by poor
academic performance specifically in Calculus and Precalculus in Mathematics.
From 2014 to 2016, an average rating of 70.33 percent for secondary education
and 71.1 percent for elementary was generated during several exams, such as the
National Achievement Test. The result produced is said to be below the national
passing percentage, which shows that Filipino student performance in
mathematics is very poor indeed. Taking into consideration the proof of the
existence of these problems, it is necessary to try and test a need for a
technology which has never before been used as a remedy. Augmented reality is
deemed to be one of the advances in technology that is found to have a good
effect not only on the health of the student, but also on the learning of
Mathematics. Combining the two positive effects of Augmented Reality and the
very impressive outcome of a game-based learning technique in mathematics, it
is wise to say that an Augmented Reality Game-based learning method can be used
as a supplement in Precalculus and Calculus teaching.",['Carlo Hernandez Godoy Jr'],2021-09-20T07:37:46Z,http://arxiv.org/abs/2109.09336v1
"Eliciting Multimodal Gesture+Speech Interactions in a Multi-Object
  Augmented Reality Environment","As augmented reality technology and hardware become more mature and
affordable, researchers have been exploring more intuitive and discoverable
interaction techniques for immersive environments. In this paper, we
investigate multimodal interaction for 3D object manipulation in a multi-object
virtual environment. To identify the user-defined gestures, we conducted an
elicitation study involving 24 participants for 22 referents with an augmented
reality headset. It yielded 528 proposals and generated a winning gesture set
with 25 gestures after binning and ranking all gesture proposals. We found that
for the same task, the same gesture was preferred for both one and two object
manipulation, although both hands were used in the two object scenario. We
presented the gestures and speech results, and the differences compared to
similar studies in a single object virtual environment. The study also explored
the association between speech expressions and gesture stroke during object
manipulation, which could improve the recognizer efficiency in augmented
reality headsets.","['Xiaoyan Zhou', 'Adam S. Williams', 'Francisco R. Ortega']",2022-07-25T22:51:40Z,http://arxiv.org/abs/2207.12566v1
"The AR/VR Technology Stack: A Central Repository of Software Development
  Libraries, Platforms, and Tools","A comprehensive repository of software development libraries, platforms, and
tools specifically to the domains of augmented, virtual, and mixed reality.",['Jasmine Roberts'],2023-05-13T05:50:26Z,http://arxiv.org/abs/2305.07842v1
"ARTiST: Automated Text Simplification for Task Guidance in Augmented
  Reality","Text presented in augmented reality provides in-situ, real-time information
for users. However, this content can be challenging to apprehend quickly when
engaging in cognitively demanding AR tasks, especially when it is presented on
a head-mounted display. We propose ARTiST, an automatic text simplification
system that uses a few-shot prompt and GPT-3 models to specifically optimize
the text length and semantic content for augmented reality. Developed out of a
formative study that included seven users and three experts, our system
combines a customized error calibration model with a few-shot prompt to
integrate the syntactic, lexical, elaborative, and content simplification
techniques, and generate simplified AR text for head-worn displays. Results
from a 16-user empirical study showed that ARTiST lightens the cognitive load
and improves performance significantly over both unmodified text and text
modified via traditional methods. Our work constitutes a step towards
automating the optimization of batch text data for readability and performance
in augmented reality.","['Guande Wu', 'Jing Qian', 'Sonia Castelo', 'Shaoyu Chen', 'Joao Rulff', 'Claudio Silva']",2024-02-29T01:58:49Z,http://arxiv.org/abs/2402.18797v1
"A Review on Industrial Augmented Reality Systems for the Industry 4.0
  Shipyard","Shipbuilding companies are upgrading their inner workings in order to create
Shipyards 4.0, where the principles of Industry 4.0 are paving the way to
further digitalized and optimized processes in an integrated network. Among the
different Industry 4.0 technologies, this article focuses on Augmented Reality,
whose application in the industrial field has led to the concept of Industrial
Augmented Reality (IAR). This article first describes the basics of IAR and
then carries out a thorough analysis of the latest IAR systems for industrial
and shipbuilding applications. Then, in order to build a practical IAR system
for shipyard workers, the main hardware and software solutions are compared.
Finally, as a conclusion after reviewing all the aspects related to IAR for
shipbuilding, it is proposed an IAR system architecture that combines Cloudlets
and Fog Computing, which reduce latency response and accelerate rendering tasks
while offloading compute intensive tasks from the Cloud.","['Paula Fraga-Lamas', 'Tiago M Fernandez-Carames', 'Oscar Blanco-Novoa', 'Miguel Vilar-Montesinos']",2024-02-01T17:09:26Z,http://arxiv.org/abs/2405.00010v1
Extended Reality for Knowledge Work in Everyday Environments,"Virtual and Augmented Reality have the potential to change information work.
The ability to modify the workers senses can transform everyday environments
into a productive office, using portable head-mounted displays combined with
conventional interaction devices, such as keyboards and tablets. While a stream
of better, cheaper and lighter HMDs have been introduced for consumers in
recent years, there are still many challenges to be addressed to allow this
vision to become reality. This chapter summarizes the state of the art in the
field of extended reality for knowledge work in everyday environments and
proposes steps to address the open challenges.","['Verena Biener', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson', 'Jens Grubert']",2021-11-06T19:20:35Z,http://arxiv.org/abs/2111.03942v1
"A Markerless Deep Learning-based 6 Degrees of Freedom PoseEstimation for
  with Mobile Robots using RGB Data","Augmented Reality has been subject to various integration efforts within
industries due to its ability to enhance human machine interaction and
understanding. Neural networks have achieved remarkable results in areas of
computer vision, which bear great potential to assist and facilitate an
enhanced Augmented Reality experience. However, most neural networks are
computationally intensive and demand huge processing power thus, are not
suitable for deployment on Augmented Reality devices. In this work we propose a
method to deploy state of the art neural networks for real time 3D object
localization on augmented reality devices. As a result, we provide a more
automated method of calibrating the AR devices with mobile robotic systems. To
accelerate the calibration process and enhance user experience, we focus on
fast 2D detection approaches which are extracting the 3D pose of the object
fast and accurately by using only 2D input. The results are implemented into an
Augmented Reality application for intuitive robot control and sensor data
visualization. For the 6D annotation of 2D images, we developed an annotation
tool, which is, to our knowledge, the first open source tool to be available.
We achieve feasible results which are generally applicable to any AR device
thus making this work promising for further research in combining high
demanding neural networks with Internet of Things devices.","['Linh Kästner', 'Daniel Dimitrov', 'Jens Lambrecht']",2020-01-16T09:13:31Z,http://arxiv.org/abs/2001.05703v1
"Towards Safer Robot-Assisted Surgery: A Markerless Augmented Reality
  Framework","Robot-assisted surgery is rapidly developing in the medical field, and the
integration of augmented reality shows the potential of improving the surgeons'
operation performance by providing more visual information. In this paper, we
proposed a markerless augmented reality framework to enhance safety by avoiding
intra-operative bleeding which is a high risk caused by the collision between
the surgical instruments and the blood vessel. Advanced stereo reconstruction
and segmentation networks are compared to find out the best combination to
reconstruct the intra-operative blood vessel in the 3D space for the
registration of the pre-operative model, and the minimum distance detection
between the instruments and the blood vessel is implemented. A robot-assisted
lymphadenectomy is simulated on the da Vinci Research Kit in a dry lab, and ten
human subjects performed this operation to explore the usability of the
proposed framework. The result shows that the augmented reality framework can
help the users to avoid the dangerous collision between the instruments and the
blood vessel while not introducing an extra load. It provides a flexible
framework that integrates augmented reality into the medical robot platform to
enhance safety during the operation.","['Ziyang Chen', 'Laura Cruciani', 'Ke Fan', 'Matteo Fontana', 'Elena Lievore', 'Ottavio De Cobelli', 'Gennaro Musi', 'Giancarlo Ferrigno', 'Elena De Momi']",2023-09-14T13:13:46Z,http://arxiv.org/abs/2309.07693v1
"An XR rapid prototyping framework for interoperability across the
  reality spectrum","Applications of the Extended Reality (XR) spectrum, a superset of Mixed,
Augmented and Virtual Reality, are gaining prominence and can be employed in a
variety of areas, such as virtual museums. Examples can be found in the areas
of education, cultural heritage, health/treatment, entertainment, marketing,
and more. The majority of computer graphics applications nowadays are used to
operate only in one of the above realities. The lack of applications across the
XR spectrum is a real shortcoming. There are many advantages resulting from
this problem's solution. Firstly, releasing an application across the XR
spectrum could contribute in discovering its most suitable reality. Moreover,
an application could be more immersive within a particular reality, depending
on its context. Furthermore, its availability increases to a broader range of
users. For instance, if an application is released both in Virtual and
Augmented Reality, it is accessible to users that may lack the possession of a
VR headset, but not of a mobile AR device. The question that arises at this
point, would be ""Is it possible for a full s/w application stack to be
converted across XR without sacrificing UI/UX in a semi-automatic way?"". It may
be quite difficult, depending on the architecture and application
implementation. Most companies nowadays support only one reality, due to their
lack of UI/UX software architecture or resources to support the complete XR
spectrum. In this work, we present an ""automatic reality transition"" in the
context of virtual museum applications. We propose a development framework,
which will automatically allow this XR transition. This framework transforms
any XR project into different realities such as Augmented or Virtual. It also
reduces the development time while increasing the XR availability of 3D
applications, encouraging developers to release applications across the XR
spectrum.","['Efstratios Geronikolakis', 'George Papagiannakis']",2021-01-05T20:27:47Z,http://arxiv.org/abs/2101.01771v2
"CaminAR: Supporting Walk-and-talk Experiences for Remote Dyads using
  Augmented Reality on Smart Glasses","In this paper, we propose CaminAR, an augmented reality (AR) system for
remote social walking among dyads. CaminAR enables two people who are
physically away from one another to synchronously see and hear each other while
going on a walk. The system shows a partner's avatar superimposed onto the
physical world using smart glasses while walking and talking. Using a
combination of visual and auditory augments, CaminAR simulates the experience
of co-located walking when dyads are apart.","['Victor Chu', 'Andrés Monroy-Hernández']",2022-07-28T06:51:20Z,http://arxiv.org/abs/2207.13904v1
"Augmenting Learning with Augmented Reality: Exploring the Affordances of
  AR in Supporting Mastery of Complex Psychomotor Tasks","This research seeks to explore how Augmented Reality (AR) can support
learning psychomotor tasks that involve complex manipulation and reasoning
processes. The AR prototype was created using Unity and used on HoloLens 2
headsets. Here, we explore the potential of AR as a training or assistive tool
for spatial tasks and the need for intelligent mechanisms to enable adaptive
and personalized interactions between learners and AR. The paper discusses how
integrating AR with Artificial Intelligence (AI) can adaptably scaffold the
learning of complex tasks to accelerate the development of expertise in
psychomotor domains.","['Dong Woo Yoo', 'Sakib Reza', 'Nicholas Wilson', 'Kemi Jona', 'Mohsen Moghaddam']",2023-05-17T01:15:46Z,http://arxiv.org/abs/2305.09875v1
Augmenting Heritage: An Open-Source Multiplatform AR Application,"AI NeRF algorithms, capable of cloud processing, have significantly reduced
hardware requirements and processing efficiency in photogrammetry pipelines.
This accessibility has unlocked the potential for museums, charities, and
cultural heritage sites worldwide to leverage mobile devices for artifact
scanning and processing. However, the adoption of augmented reality platforms
often necessitates the installation of proprietary applications on users'
mobile devices, which adds complexity to development and limits global
availability. This paper presents a case study that demonstrates a
cost-effective pipeline for visualizing scanned museum artifacts using mobile
augmented reality, leveraging an open-source embedded solution on a website.",['Corrie Green'],2023-09-28T16:36:25Z,http://arxiv.org/abs/2310.13700v1
AR-based Modern Healthcare: A Review,"The recent advances of Augmented Reality (AR) in healthcare have shown that
technology is a significant part of the current healthcare system. In recent
days, augmented reality has proposed numerous smart applications in healthcare
domain including wearable access, telemedicine, remote surgery, diagnosis of
medical reports, emergency medicine, etc. The aim of the developed augmented
healthcare application is to improve patient care, increase efficiency, and
decrease costs. This article puts on an effort to review the advances in
AR-based healthcare technologies and goes to peek into the strategies that are
being taken to further this branch of technology. This article explores the
important services of augmented-based healthcare solutions and throws light on
recently invented ones as well as their respective platforms. It also addresses
concurrent concerns and their relevant future challenges. In addition, this
paper analyzes distinct AR security and privacy including security requirements
and attack terminologies. Furthermore, this paper proposes a security model to
minimize security risks. Augmented reality advantages in healthcare, especially
for operating surgery, emergency diagnosis, and medical training is being
demonstrated here thorough proper analysis. To say the least, the article
illustrates a complete overview of augmented reality technology in the modern
healthcare sector by demonstrating its impacts, advancements, current
vulnerabilities; future challenges, and concludes with recommendations to a new
direction for further research.","['Jinat Ara', 'Hanif Bhuiyan', 'Yeasin Arafat Bhuiyan', 'Salma Begum Bhyan', 'Muhammad Ismail Bhuiyan']",2021-01-16T03:53:01Z,http://arxiv.org/abs/2101.06364v1
"Switchable Virtual, Augmented, and Mixed Reality through Optical
  Cloaking","A switchable virtual reality (VR), augmented reality (AR), and mixed reality
(MR) system is proposed using digital optical cloaking. Optical cloaking allows
completely opaque VR devices to be ""cloaked,"" switching to AR or MR while
providing correct three-dimensional (3D) parallax and perspective of the real
world, without the need for transparent optics. On the other hand, 3D capture
and display devices with non-zero thicknesses, require optical cloaking to
properly display captured reality. A simplified stereoscopic system with two
cameras and existing VR systems can be an approximation for limited VR, AR, or
MR. To provide true 3D visual effects, multiple input cameras, a 3D display,
and a simple linear calculation amounting to cloaking can be used. Since the
display size requirements for VR, AR, and MR are usually small, with increasing
computing power and pixel densities, the framework presented here can provide a
widely deployable VR, AR, MR design.",['Joseph S. Choi'],2018-02-06T07:18:51Z,http://arxiv.org/abs/1802.01826v1
"Resource Allocation and Resolution Control in the Metaverse with Mobile
  Augmented Reality","With the development of blockchain and communication techniques, the
Metaverse is considered as a promising next-generation Internet paradigm, which
enables the connection between reality and the virtual world. The key to
rendering a virtual world is to provide users with immersive experiences and
virtual avatars, which is based on virtual reality (VR) technology and high
data transmission rate. However, current VR devices require intensive
computation and communication, and users suffer from high delay while using
wireless VR devices. To build the connection between reality and the virtual
world with current technologies, mobile augmented reality (MAR) is a feasible
alternative solution due to its cheaper communication and computation cost.
This paper proposes an MAR-based connection model for the Metaverse, and
proposes a communication resources allocation algorithm based on outer
approximation (OA) to achieve the best utility. Simulation results show that
our proposed algorithm is able to provide users with basic MAR services for the
Metaverse, and outperforms the benchmark greedy algorithm.","['Peiyuan Si', 'Jun Zhao', 'Huimei Han', 'Kwok-Yan Lam', 'Yang Liu']",2022-09-28T07:09:52Z,http://arxiv.org/abs/2209.13871v1
"Teachable Reality: Prototyping Tangible Augmented Reality with Everyday
  Objects by Leveraging Interactive Machine Teaching","This paper introduces Teachable Reality, an augmented reality (AR)
prototyping tool for creating interactive tangible AR applications with
arbitrary everyday objects. Teachable Reality leverages vision-based
interactive machine teaching (e.g., Teachable Machine), which captures
real-world interactions for AR prototyping. It identifies the user-defined
tangible and gestural interactions using an on-demand computer vision model.
Based on this, the user can easily create functional AR prototypes without
programming, enabled by a trigger-action authoring interface. Therefore, our
approach allows the flexibility, customizability, and generalizability of
tangible AR applications that can address the limitation of current
marker-based approaches. We explore the design space and demonstrate various AR
prototypes, which include tangible and deformable interfaces, context-aware
assistants, and body-driven AR applications. The results of our user study and
expert interviews confirm that our approach can lower the barrier to creating
functional AR prototypes while also allowing flexible and general-purpose
prototyping experiences.","['Kyzyl Monteiro', 'Ritik Vatsal', 'Neil Chulpongsatorn', 'Aman Parnami', 'Ryo Suzuki']",2023-02-21T23:03:49Z,http://arxiv.org/abs/2302.11046v1
CPR Emergency Assistance Through Mixed Reality Communication,"We design and evaluate a mixed reality real-time communication system for
remote assistance during CPR emergencies. Our system allows an expert to guide
a first responder, remotely, on how to give first aid. RGBD cameras capture a
volumetric view of the local scene including the patient, the first responder,
and the environment. The volumetric capture is augmented onto the remote
expert's view to spatially guide the first responder using visual and verbal
instructions. We evaluate the mixed reality communication system in a research
study in which participants face a simulated emergency. The first responder
moves the patient to the recovery position and performs chest compressions as
well as mouth-to-mask ventilation. Our study compares mixed reality against
videoconferencing-based assistance using CPR performance measures, cognitive
workload surveys, and semi-structured interviews. We find that more visual
communication including gestures and objects is used by the remote expert when
assisting in mixed reality compared to videoconferencing. Moreover, the
performance and the workload of the first responder during simulation do not
differ significantly between the two technologies.","['Manuel Rebol', 'Alexander Steinmaurer', 'Florian Gamillscheg', 'Krzysztof Pietroszek', 'Christian Gütl', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka']",2023-12-14T17:21:15Z,http://arxiv.org/abs/2312.09150v1
"Extended Reality (XR) Codec Adaptation in 5G using Multi-Agent
  Reinforcement Learning with Attention Action Selection","Extended Reality (XR) services will revolutionize applications over 5th and
6th generation wireless networks by providing seamless virtual and augmented
reality experiences. These applications impose significant challenges on
network infrastructure, which can be addressed by machine learning algorithms
due to their adaptability. This paper presents a Multi- Agent Reinforcement
Learning (MARL) solution for optimizing codec parameters of XR traffic,
comparing it to the Adjust Packet Size (APS) algorithm. Our cooperative
multi-agent system uses an Optimistic Mixture of Q-Values (oQMIX) approach for
handling Cloud Gaming (CG), Augmented Reality (AR), and Virtual Reality (VR)
traffic. Enhancements include an attention mechanism and slate-Markov Decision
Process (MDP) for improved action selection. Simulations show our solution
outperforms APS with average gains of 30.1%, 15.6%, 16.5% 50.3% in XR index,
jitter, delay, and Packet Loss Ratio (PLR), respectively. APS tends to increase
throughput but also packet losses, whereas oQMIX reduces PLR, delay, and jitter
while maintaining goodput.","['Pedro Enrique Iturria-Rivera', 'Raimundas Gaigalas', 'Medhat Elsayed', 'Majid Bavand', 'Yigit Ozcan', 'Melike Erol-Kantarci']",2024-05-24T18:34:00Z,http://arxiv.org/abs/2405.15872v1
Breaking the Barriers to True Augmented Reality,"In recent years, Augmented Reality (AR) and Virtual Reality (VR) have gained
considerable commercial traction, with Facebook acquiring Oculus VR for \$2
billion, Magic Leap attracting more than \$500 million of funding, and
Microsoft announcing their HoloLens head-worn computer. Where is humanity
headed: a brave new dystopia-or a paradise come true?
  In this article, we present discussions, which started at the symposium
""Making Augmented Reality Real"", held at Nara Institute of Science and
Technology in August 2014. Ten scientists were invited to this three-day event,
which started with a full day of public presentations and panel discussions
(video recordings are available at the event web page), followed by two days of
roundtable discussions addressing the future of AR and VR.","['Christian Sandor', 'Martin Fuchs', 'Alvaro Cassinelli', 'Hao Li', 'Richard Newcombe', 'Goshiro Yamamoto', 'Steven Feiner']",2015-12-17T05:57:06Z,http://arxiv.org/abs/1512.05471v1
"Procedural animations in interactive art experiences -- A state of the
  art review","The state of the art review broadly oversees the use of novel research
utilized in the creation of virtual environments applied in interactive art
experiences, with a specific focus on the application of procedural animation
in spatially augmented reality (SAR) exhibitions. These art exhibitions
frequently combine sensory displays that appeal, replace, and augment the
visual, auditory and touch or haptic senses. We analyze and break down
art-technology related innovations in the last three years, and thoroughly
identify the most recent and vibrant applications of interactive art
experiences in the review of numerous installation applications, studies, and
events. Display mediums such as virtual reality, augmented reality, mixed
reality, and robotics are overviewed in the context of art experiences such as
visual art museums, park or historic site tours, live concerts, and theatre. We
explore research and extrapolate how recent innovations can lead to different
applications that will be seen in the future.",['C. Tollola'],2021-05-16T05:14:56Z,http://arxiv.org/abs/2105.09153v1
AR Visualization System for Ship Detection and Recognition Based on AI,"Augmented reality technology has been widely used in industrial design
interaction, exhibition guide, information retrieval and other fields. The
combination of artificial intelligence and augmented reality technology has
also become a future development trend. This project is an AR visualization
system for ship detection and recognition based on AI, which mainly includes
three parts: artificial intelligence module, Unity development module and
Hololens2AR module. This project is based on R3Det algorithm to complete the
detection and recognition of ships in remote sensing images. The recognition
rate of model detection trained on RTX 2080Ti can reach 96%. Then, the 3D model
of the ship is obtained by ship categories and information and generated in the
virtual scene. At the same time, voice module and UI interaction module are
added. Finally, we completed the deployment of the project on Hololens2 through
MRTK. The system realizes the fusion of computer vision and augmented reality
technology, which maps the results of object detection to the AR field, and
makes a brave step toward the future technological trend and intelligent
application.","['Ziqi Ye', 'Limin Huang', 'Yongji Wu', 'Min Hu']",2023-11-21T08:42:44Z,http://arxiv.org/abs/2311.12430v1
"Measuring Presence in Augmented Reality Environments: Design and a First
  Test of a Questionnaire","Augmented Reality (AR) enriches a user's real environment by adding spatially
aligned virtual objects (3D models, 2D textures, textual annotations, etc) by
means of special display technologies. These are either worn on the body or
placed in the working environment. From a technical point of view, AR faces
three major challenges: (1) to generate a high quality rendering, (2) to
precisely register (in position and orientation) the virtual objects (VOs) with
the real environment, and (3) to do so in interactive real-time (Regenbrecht,
Wagner, and Baratoff, 2002). The goal is to create the impression that the VOs
are part of the real environment. Therefore, and similar to definitions of
virtual reality (Steuer, 1992), it makes sense to define AR from a
psychological point of view: Augmented Reality conveys the impression that VOs
are present in the real environment. In order to evaluate how well this goal is
reached, a psychological measurement of this type of presence is necessary. In
the following, we will describe technological features of AR systems that make
a special questionnaire version necessary, describe our approach to the
questionnaire development, and the data collection strategy. Finally we will
present first results of the application of the questionnaire in a recent study
with 385 participants.","['Holger Regenbrecht', 'Thomas Schubert']",2021-03-04T04:46:19Z,http://arxiv.org/abs/2103.02831v1
AIR: Anywhere Immersive Reality with User-Perspective Projection,"Projection-based augmented reality (AR) has much potential, but is limited in
that it requires burdensome installations and prone to geometric distortions on
display surface. To overcome these limitations, we propose AIR. It can be
carried and placed anywhere to project AR using pan/tilting motors, while
providing the user with distortion-free projection of a correct 3D view.","['JungHyun Byun', 'SeungHo Chae', 'YoonSik Yang', 'TackDon Han']",2018-12-01T17:54:36Z,http://arxiv.org/abs/1812.00233v1
The Age of Synthetic Realities: Challenges and Opportunities,"Synthetic realities are digital creations or augmentations that are
contextually generated through the use of Artificial Intelligence (AI) methods,
leveraging extensive amounts of data to construct new narratives or realities,
regardless of the intent to deceive. In this paper, we delve into the concept
of synthetic realities and their implications for Digital Forensics and society
at large within the rapidly advancing field of AI. We highlight the crucial
need for the development of forensic techniques capable of identifying harmful
synthetic creations and distinguishing them from reality. This is especially
important in scenarios involving the creation and dissemination of fake news,
disinformation, and misinformation. Our focus extends to various forms of
media, such as images, videos, audio, and text, as we examine how synthetic
realities are crafted and explore approaches to detecting these malicious
creations. Additionally, we shed light on the key research challenges that lie
ahead in this area. This study is of paramount importance due to the rapid
progress of AI generative techniques and their impact on the fundamental
principles of Forensic Science.","['João Phillipe Cardenuto', 'Jing Yang', 'Rafael Padilha', 'Renjie Wan', 'Daniel Moreira', 'Haoliang Li', 'Shiqi Wang', 'Fernanda Andaló', 'Sébastien Marcel', 'Anderson Rocha']",2023-06-09T15:55:10Z,http://arxiv.org/abs/2306.11503v1
"Augmented Voices: An Augmented Reality Experience Highlighting the
  Social Injustices of Gender-Based Violence in the Muslim South-Asian Diaspora","This paper delves into the distressing prevalence of gender-based violence
(GBV) and its deep-seated psychological ramifications, particularly among
Muslim South Asian women living in diasporic communities. Despite the gravity
of GBV, these women often face formidable barriers in voicing their experiences
and accessing support. ""Augmented Voices"" emerges as a technological beacon,
harnessing the potential of augmented reality (AR) to bridge the digital and
physical realms through mobile devices, enhancing the visibility of these
often-silenced voices. With its technological motivation firmly anchored in the
convergence of AR and real-world interactions, ""Augmented Voices"" offers a
digital platform where storytelling acts as a catalyst, bringing to the fore
the experiences shared by these women. By superimposing their narratives onto
physical locations via Geographic Information System (GIS) Mapping, the
application ""augments their voices"" in the diaspora, providing a conduit for
expression and solidarity. This project, currently at its developmental stage,
aspires to elevate the stories of GBV victims to a level where their struggles
are not just heard but felt, forging a powerful connection between the user and
the narrative. It is designed to transcend the limitations of conventional
storytelling, creating an ""augmented"" reality where voices that are often muted
by societal constraints can resonate powerfully. The project underscores the
urgent imperative to confront GBV, catalyzing societal transformation and
fostering robust support networks for those in the margins. It is a pioneering
example of how technology can become a formidable ally in the fight for social
justice and the empowerment of the oppressed. Additionally, this paper delves
into the AR workflow illustrating its relevance and contribution to the broader
theme of site-specific AR for social justice.",['Hamida Khatri'],2024-04-23T17:17:52Z,http://arxiv.org/abs/2404.15239v1
Real-time Geometry-Aware Augmented Reality in Minimally Invasive Surgery,"The potential of Augmented Reality (AR) technology to assist minimally
invasive surgeries (MIS) lies in its computational performance and accuracy in
dealing with challenging MIS scenes. Even with the latest hardware and software
technologies, achieving both real-time and accurate augmented information
overlay in MIS is still a formidable task. In this paper, we present a novel
real-time AR framework for MIS that achieves interactive geometric aware
augmented reality in endoscopic surgery with stereo views. Our framework tracks
the movement of the endoscopic camera and simultaneously reconstructs a dense
geometric mesh of the MIS scene. The movement of the camera is predicted by
minimising the re-projection error to achieve a fast tracking performance,
while the 3D mesh is incrementally built by a dense zero mean normalised cross
correlation stereo matching method to improve the accuracy of the surface
reconstruction. Our proposed system does not require any prior template or
pre-operative scan and can infer the geometric information intra-operatively in
real-time. With the geometric information available, our proposed AR framework
is able to interactively add annotations, localisation of tumours and vessels,
and measurement labelling with greater precision and accuracy compared with the
state of the art approaches.","['Long Chen', 'Wen Tang', 'Nigel W. John']",2017-08-03T17:25:38Z,http://arxiv.org/abs/1708.01234v1
Augmented reality navigation system for visual prosthesis,"The visual functions of visual prostheses such as field of view, resolution
and dynamic range, seriously restrict the person's ability to navigate in
unknown environments. Implanted patients still require constant assistance for
navigating from one location to another. Hence, there is a need for a system
that is able to assist them safely during their journey. In this work, we
propose an augmented reality navigation system for visual prosthesis that
incorporates a software of reactive navigation and path planning which guides
the subject through convenient, obstacle-free route. It consists on four steps:
locating the subject on a map, planning the subject trajectory, showing it to
the subject and re-planning without obstacles. We have also designed a
simulated prosthetic vision environment which allows us to systematically study
navigation performance. Twelve subjects participated in the experiment.
Subjects were guided by the augmented reality navigation system and their
instruction was to navigate through different environments until they reached
two goals, cross the door and find an object (bin), as fast and accurately as
possible. Results show how our augmented navigation system help navigation
performance by reducing the time and distance to reach the goals, even
significantly reducing the number of obstacles collisions, compared to other
baseline methods.","['Melani Sanchez-Garcia', 'Alejandro Perez-Yus', 'Ruben Martinez-Cantin', 'Jose J. Guerrero']",2021-09-30T09:41:40Z,http://arxiv.org/abs/2109.14957v1
"Towards Situation Awareness and Attention Guidance in a Multiplayer
  Environment using Augmented Reality and Carcassonne","Augmented reality (AR) games are a rich environment for researching and
testing computational systems that provide subtle user guidance and training.
In particular computer systems that aim to augment a user's situation awareness
benefit from the range of sensors and computing power available in AR headsets.
In this work-in-progress paper, we present a new environment for research into
situation awareness and attention guidance (SAAG): an augmented reality version
of the board game Carcassonne. We also present our initial work in producing a
SAAG pipeline, including the creation of game state encodings, the development
and training of a gameplay AI, and the design of situation modelling and gaze
tracking systems.","['David Kadish', 'Arezoo Sarkheyli-Hägele', 'Jose Font', 'Diederick C. Niehorster', 'Thomas Pederson']",2022-08-18T23:45:13Z,http://arxiv.org/abs/2208.09094v1
"Evaluating Augmented Reality Communication: How Can We Teach Procedural
  Skill in AR?","Augmented reality (AR) has great potential for use in healthcare
applications, especially remote medical training and supervision. In this
paper, we analyze the usage of an AR communication system to teach a medical
procedure, the placement of a central venous catheter (CVC) under ultrasound
guidance. We examine various AR communication and collaboration components,
including gestural communication, volumetric information, annotations,
augmented objects, and augmented screens. We compare how teaching in AR differs
from teaching through videoconferencing-based communication. Our results
include a detailed medical training steps analysis in which we compare how
verbal and visual communication differs between video and AR training. We
identify procedural steps in which medical experts give visual instructions
utilizing AR components. We examine the change in AR usage and interaction over
time and recognize patterns between users. Moreover, AR design recommendations
are given based on post-training interviews.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Neal Sikka', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Puja Sasankan', 'Christian Gütl']",2023-12-14T17:22:22Z,http://arxiv.org/abs/2312.09152v1
"Exploring Interactions with Printed Data Visualizations in Augmented
  Reality","This paper presents a design space of interaction techniques to engage with
visualizations that are printed on paper and augmented through Augmented
Reality. Paper sheets are widely used to deploy visualizations and provide a
rich set of tangible affordances for interactions, such as touch, folding,
tilting, or stacking. At the same time, augmented reality can dynamically
update visualization content to provide commands such as pan, zoom, filter, or
detail on demand. This paper is the first to provide a structured approach to
mapping possible actions with the paper to interaction commands. This design
space and the findings of a controlled user study have implications for future
designs of augmented reality systems involving paper sheets and visualizations.
Through workshops (N=20) and ideation, we identified 81 interactions that we
classify in three dimensions: 1) commands that can be supported by an
interaction, 2) the specific parameters provided by an (inter)action with
paper, and 3) the number of paper sheets involved in an interaction. We tested
user preference and viability of 11 of these interactions with a prototype
implementation in a controlled study (N=12, HoloLens 2) and found that most of
the interactions are intuitive and engaging to use. We summarized interactions
(e.g., tilt to pan) that have strong affordance to complement ""point"" for data
exploration, physical limitations and properties of paper as a medium, cases
requiring redundancy and shortcuts, and other implications for design.","['Wai Tong', 'Zhutian Chen', 'Meng Xia', 'Leo Yu-Ho Lo', 'Linping Yuan', 'Benjamin Bach', 'Huamin Qu']",2022-08-22T21:22:33Z,http://arxiv.org/abs/2208.10603v1
Dynamic X-Ray Vision in Mixed Reality,"X-ray vision, a technique that allows users to see through walls and other
obstacles, is a popular technique for Augmented Reality (AR) and Mixed Reality
(MR). In this paper, we demonstrate a dynamic X-ray vision window that is
rendered in real-time based on the user's current position and changes with
movement in the physical environment. Moreover, the location and transparency
of the window are also dynamically rendered based on the user's eye gaze. We
build this X-ray vision window for a current state-of-the-art MR Head-Mounted
Device (HMD) -- HoloLens 2 by integrating several different features: scene
understanding, eye tracking, and clipping primitive.","['Hung-Jui Guo', 'Jonathan Z. Bakdash', 'Laura R. Marusich', 'Balakrishnan Prabhakaran']",2022-09-15T03:32:10Z,http://arxiv.org/abs/2209.07025v1
"Extended Reality and Internet of Things for Hyper-Connected Metaverse
  Environments","The Metaverse encompasses technologies related to the internet, virtual and
augmented reality, and other domains toward smart interfaces that are
hyper-connected, immersive, and engaging. However, Metaverse applications face
inherent disconnects between virtual and physical components and interfaces.
This work explores how an Extended Metaverse framework can be used to increase
the seamless integration of interoperable agents between virtual and physical
environments. It contributes an early theory and practice toward the synthesis
of virtual and physical smart environments anticipating future designs and
their potential for connected experiences.","['Jie Guan', 'Jay Irizawa', 'Alexis Morris']",2023-01-21T00:28:03Z,http://arxiv.org/abs/2301.08835v1
From medical imaging to virtual reality for archaeology,"The IRMA project aims to design innovative methodologies for research in the
field of historical and archaeological heritage based on a combination of
medical imaging technologies and interactive 3D restitution modalities (virtual
reality, augmented reality, haptics, additive manufacturing). These tools are
based on recent research results from a collaboration between IRISA, Inrap and
the company Image ET and are intended for cultural heritage professionals such
as museums, curators, restorers and archaeologists.","['Théophane Nicolas', 'Ronan Gaugne', 'Bruno Arnaldi', 'Valérie Gouranton']",2023-01-26T09:39:25Z,http://arxiv.org/abs/2301.11006v1
"Investigating Psychological Ownership in a Shared AR Space: Effects of
  Human and Object Reality and Object Controllability","Augmented reality (AR) provides users with a unique social space where
virtual objects are natural parts of the real world. The users can interact
with 3D virtual objects and virtual humans projected onto the physical
environment. This work examines perceived ownership based on the reality of
objects and partners, as well as object controllability in a shared AR setting.
Our formal user study with 28 participants shows a sense of possession,
control, separation, and partner presence affect their perceived ownership of a
shared object. Finally, we discuss the findings and present a conclusion.","['Dongyun Han', 'Donghoon Kim', 'Kangsoo Kim', 'Isaac Cho']",2023-08-26T20:28:20Z,http://arxiv.org/abs/2308.13953v1
Stay in Touch! Shape and Shadow Influence Surface Contact in XR Displays,"The information provided to a person's visual system by extended reality (XR)
displays is not a veridical match to the information provided by the real
world. Due in part to graphical limitations in XR head-mounted displays (HMDs),
which vary by device, our perception of space may be altered. However, we do
not yet know which properties of virtual objects rendered by HMDs --
particularly augmented reality displays -- influence our ability to understand
space. In the current research, we evaluate how immersive graphics affect
spatial perception across three unique XR displays: virtual reality (VR), video
see-through augmented reality (VST AR), and optical see-through augmented
reality (OST AR). We manipulated the geometry of the presented objects as well
as the shading techniques for objects' cast shadows. Shape and shadow were
selected for evaluation as they play an important role in determining where an
object is in space by providing points of contact between an object and its
environment -- be it real or virtual. Our results suggest that a
non-photorealistic (NPR) shading technique, in this case for cast shadows, may
be used to improve depth perception by enhancing perceived surface contact in
XR. Further, the benefit of NPR graphics is more pronounced in AR than in VR
displays. One's perception of ground contact is influenced by an object's
shape, as well. However, the relationship between shape and surface contact
perception is more complicated.","['Haley Adams', 'Holly Gagnon', 'Sarah Creem-Regehr', 'Jeanine Stefanucci', 'Bobby Bodenheimer']",2022-01-06T02:00:41Z,http://arxiv.org/abs/2201.01889v1
Mobile Augmented Reality Applications to Discover New Environments,"Although man has become sedentary over time, his wish to travel the world
remains as strong as ever. The aim of this paper is to show how techniques
based on imagery and Augmented Reality (AR) can prove to be of great help when
discovering a new urban environment and observing the evolution of the natural
environment. The study's support is naturally the Smartphone which in just a
few years has become our most familiar device, which we take with us
practically everywhere we go in our daily lives.","['Nehla Ghouaiel', 'Jean-Marc Cieutat', 'Jean-Pierre Jessel']",2013-11-26T14:00:24Z,http://arxiv.org/abs/1311.6670v1
"Motion model transitions in GPS-IMU sensor fusion for user tracking in
  augmented reality","Finding the position of the user is an important processing step for
augmented reality (AR) applications. This paper investigates the use of
different motion models in order to choose the most suitable one, and
eventually reduce the Kalman filter errors in sensor fusion for such
applications where the accuracy of user tracking is crucial. A Deterministic
Finite Automaton (DFA) was employed using the innovation parameters of the
filter. Results show that the approach presented here reduces the filter error
compared to a static model and prevents filter divergence. The approach was
tested on a simple AR game in order to justify the accuracy and performance of
the algorithm.",['Erkan Bostanci'],2015-12-09T05:51:11Z,http://arxiv.org/abs/1512.02758v1
Augmented Reality Oculus Rift,"This paper covers the whole process of developing an Augmented Reality
Stereoscopig Render Engine for the Oculus Rift. To capture the real world in
form of a camera stream, two cameras with fish-eye lenses had to be installed
on the Oculus Rift DK1 hardware. The idea was inspired by Steptoe
\cite{steptoe2014presence}. After the introduction, a theoretical part covers
all the most neccessary elements to achieve an AR System for the Oculus Rift,
following the implementation part where the code from the AR Stereo Engine is
explained in more detail. A short conclusion section shows some results,
reflects some experiences and in the final chapter some future works will be
discussed. The project can be accessed via the git repository
https://github.com/MaXvanHeLL/ARift.git.","['Markus Höll', 'Nikolaus Heran', 'Vincent Lepetit']",2016-04-29T14:26:55Z,http://arxiv.org/abs/1604.08848v1
"Energy-Efficient Resource Allocation for Mobile Edge Computing-Based
  Augmented Reality Applications","Mobile edge computing is a provisioning solution to enable Augmented Reality
(AR) applications on mobile devices. AR mobile applications have inherent
collaborative properties in terms of data collection in the uplink, computing
at the edge, and data delivery in the downlink. In this letter, these features
are leveraged to propose a novel resource allocation approach over both
communication and computation resources. The approach, implemented via
Successive Convex Approximation (SCA), is seen to yield considerable gains in
mobile energy consumption as compared to conventional independent offloading
across users.","['Ali Al-Shuwaili', 'Osvaldo Simeone']",2016-11-28T17:09:11Z,http://arxiv.org/abs/1611.09243v2
City Planning with Augmented Reality,"We present an early study designed to analyze how city planning and the
health of senior citizens can benefit from the use of augmented reality (AR)
using Microsoft's HoloLens. We also explore whether AR and VR can be used to
help city planners receive real-time feedback from citizens, such as the
elderly, on virtual plans, allowing for informed decisions to be made before
any construction begins.","['Catherine Angelini', 'Adam S. Williams', 'Mathew Kress', 'Edgar Ramos Vieira', ""Newton D'Souza"", 'Naphtali D. Rishe', 'Joseph Medina', 'Francisco R. Ortega']",2020-01-18T02:22:45Z,http://arxiv.org/abs/2001.06578v1
"Augmented Reality with Hololens: Experiential Architectures Embedded in
  the Real World","Early hands-on experiences with the Microsoft Hololens augmented/mixed
reality device are reported and discussed, with a general aim of exploring
basic 3D visualization. A range of usage cases are tested, including data
visualization and immersive data spaces, in-situ visualization of 3D models and
full scale architectural form visualization. Ultimately, the Hololens is found
to provide a remarkable tool for moving from traditional visualization of 3D
objects on a 2D screen, to fully experiential 3D visualizations embedded in the
real world.","['Paul Hockett', 'Tim Ingleby']",2016-10-13T22:32:08Z,http://arxiv.org/abs/1610.04281v1
Feasibility of Corneal Imaging for Handheld Augmented Reality,"Smartphones are a popular device class for mobile Augmented Reality but
suffer from a limited input space. Around-device interaction techniques aim at
extending this input space using various sensing modalities. In this paper we
present our work towards extending the input area of mobile devices using
front-facing device-centered cameras that capture reflections in the cornea. As
current generation mobile devices lack high resolution front-facing cameras, we
study the feasibility of around-device interaction using corneal reflective
imaging based on a high resolution camera. We present a workflow, a technical
prototype and a feasibility evaluation.","['Daniel Schneider', 'Jens Grubert']",2017-09-04T14:00:59Z,http://arxiv.org/abs/1709.00965v1
Efficient Pose Tracking from Natural Features in Standard Web Browsers,"Computer Vision-based natural feature tracking is at the core of modern
Augmented Reality applications. Still, Web-based Augmented Reality typically
relies on location-based sensing (using GPS and orientation sensors) or
marker-based approaches to solve the pose estimation problem.
  We present an implementation and evaluation of an efficient natural feature
tracking pipeline for standard Web browsers using HTML5 and WebAssembly. Our
system can track image targets at real-time frame rates tablet PCs (up to 60
Hz) and smartphones (up to 25 Hz).","['Fabian Göttl', 'Philipp Gagel', 'Jens Grubert']",2018-04-23T13:46:01Z,http://arxiv.org/abs/1804.08424v1
An Augmented Reality Interaction Interface for Autonomous Drone,"Human drone interaction in autonomous navigation incorporates spatial
interaction tasks, including reconstructed 3D map from the drone and human
desired target position. Augmented Reality (AR) devices can be powerful
interactive tools for handling these spatial interactions. In this work, we
build an AR interface that displays the reconstructed 3D map from the drone on
physical surfaces in front of the operator. Spatial target positions can be
further set on the 3D map by intuitive head gaze and hand gesture. The AR
interface is deployed to interact with an autonomous drone to explore an
unknown environment. A user study is further conducted to evaluate the overall
interaction performance.","['Chuhao Liu', 'Shaojie Shen']",2020-08-05T17:02:22Z,http://arxiv.org/abs/2008.02234v1
"SENSAR: A Visual Tool for Intelligent Robots for Collaborative
  Human-Robot Interaction","Establishing common ground between an intelligent robot and a human requires
communication of the robot's intention, behavior, and knowledge to the human to
build trust and assure safety in a shared environment. This paper introduces
SENSAR (Seeing Everything iN Situ with Augmented Reality), an augmented reality
robotic system that enables robots to communicate their sensory and cognitive
data in context over the real-world with rendered graphics, allowing a user to
understand, correct, and validate the robot's perception of the world. Our
system aims to support human-robot interaction research by establishing common
ground where the perceptions of the human and the robot align.","['Andre Cleaver', 'Faizan Muhammad', 'Amel Hassan', 'Elaine Short', 'Jivko Sinapov']",2020-11-09T15:50:32Z,http://arxiv.org/abs/2011.04515v1
"Scene Text Detection for Augmented Reality -- Character Bigram Approach
  to reduce False Positive Rate","Natural scene text detection is an important aspect of scene understanding
and could be a useful tool in building engaging augmented reality applications.
In this work, we address the problem of false positives in text spotting. We
propose improving the performace of sliding window text spotters by looking for
character pairs (bigrams) rather than single characters. An efficient
convolutional neural network is designed and trained to detect bigrams. The
proposed detector reduces false positive rate by 28.16% on the ICDAR 2015
dataset. We demonstrate that detecting bigrams is a computationally inexpensive
way to improve sliding window text spotters.","['Sagar Gubbi', 'Bharadwaj Amrutur']",2020-12-26T08:56:10Z,http://arxiv.org/abs/2101.01054v1
Evaluating Situated Visualization in AR with Eye Tracking,"Augmented reality (AR) technology provides means for embedding visualization
in a real-world context. Such techniques allow situated analyses of live data
in their spatial domain. However, as existing techniques have to be adapted for
this context and new approaches will be developed, the evaluation thereof poses
new challenges for researchers. Apart from established performance measures,
eye tracking has proven to be a valuable means to assess visualizations
qualitatively and quantitatively. We discuss the challenges and opportunities
of eye tracking for the evaluation of situated visualizations. We envision that
an extension of gaze-based evaluation methodology into this field will provide
new insights on how people perceive and interact with visualizations in
augmented reality.","['Kuno Kurzhals', 'Michael Becher', 'Nelusa Pathmanathan', 'Guido Reina']",2022-09-05T09:10:04Z,http://arxiv.org/abs/2209.01846v1
Supporting Electronics Learning through Augmented Reality,"Understanding electronics is a critical area in the maker scene. Many of the
makers' projects require electronics knowledge to connect microcontrollers with
sensors and actuators. Yet, learning electronics is challenging, as internal
component processes remain invisible, and students often fear personal harm or
component damage. Augmented Reality (AR) applications are developed to support
electronics learning and visualize complex processes. This paper reflects on
related work around AR and electronics that characterize open research
challenges around the four characteristics functionality, fidelity, feedback
type, and interactivity.","['Thomas Kosch', 'Julian Rasch', 'Albrecht Schmidt', 'Sebastian Feger']",2022-10-25T07:53:50Z,http://arxiv.org/abs/2210.13820v1
Towards a QoE Model to Evaluate Holographic Augmented Reality Devices,"Augmented reality (AR) technology is developing fast and provides users with
new ways to interact with the real-world surrounding environment. Although the
performance of holographic AR multimedia devices can be measured with
traditional quality-of-service parameters, a quality-of-experience (QoE) model
can better evaluate the device from the perspective of users. As there are
currently no well-recognized models for measuring the QoE of a holographic AR
multimedia device, we present a QoE framework and model it with a fuzzy
inference system to quantitatively evaluate the device.","['Longyu Zhang', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",2022-12-25T15:15:29Z,http://arxiv.org/abs/2212.13842v1
"Digital Twin of a Network and Operating Environment Using Augmented
  Reality","We demonstrate the digital twin of a network, network elements, and operating
environment using machine learning. We achieve network card failure
localization and remote collaboration over 86 km of fiber using augmented
reality.","['Haoshuo Chen', 'Xiaonan Xu', 'Jesse E. Simsarian', 'Mijail Szczerban', 'Rob Harby', 'Roland Ryf', 'Mikael Mazur', 'Lauren Dallachiesa', 'Nicolas K. Fontaine', 'John Cloonan', 'Jim Sandoz', 'David T. Neilson']",2023-03-23T19:37:09Z,http://arxiv.org/abs/2303.15221v1
"Communicating Robot's Intentions while Assisting Users via Augmented
  Reality","This paper explores the challenges faced by assistive robots in effectively
cooperating with humans, requiring them to anticipate human behavior, predict
their actions' impact, and generate understandable robot actions. The study
focuses on a use-case involving a user with limited mobility needing assistance
with pouring a beverage, where tasks like unscrewing a cap or reaching for
objects demand coordinated support from the robot. Yet, anticipating the
robot's intentions can be challenging for the user, which can hinder effective
collaboration. To address this issue, we propose an innovative solution that
utilizes Augmented Reality (AR) to communicate the robot's intentions and
expected movements to the user, fostering a seamless and intuitive interaction.","['Chao Wang', 'Theodoros Stouraitis', 'Anna Belardinelli', 'Stephan Hasler', 'Michael Gienger']",2023-08-21T08:07:56Z,http://arxiv.org/abs/2308.10552v1
"Self-supervised 6-DoF Robot Grasping by Demonstration via Augmented
  Reality Teleoperation System","Most existing 6-DoF robot grasping solutions depend on strong supervision on
grasp pose to ensure satisfactory performance, which could be laborious and
impractical when the robot works in some restricted area. To this end, we
propose a self-supervised 6-DoF grasp pose detection framework via an Augmented
Reality (AR) teleoperation system that can efficiently learn human
demonstrations and provide 6-DoF grasp poses without grasp pose annotations.
Specifically, the system collects the human demonstration from the AR
environment and contrastively learns the grasping strategy from the
demonstration. For the real-world experiment, the proposed system leads to
satisfactory grasping abilities and learning to grasp unknown objects within
three demonstrations.","['Xiwen Dengxiong', 'Xueting Wang', 'Shi Bai', 'Yunbo Zhang']",2024-04-03T21:16:19Z,http://arxiv.org/abs/2404.03067v1
"With or Without Permission: Site-Specific Augmented Reality for Social
  Justice CHI 2024 Workshop Proceedings","This volume represents the proceedings of With or Without Permission:
Site-Specific Augmented Reality for Social Justice CHI 2024 workshop.","['Rafael M. L. Silva', 'Ana María Cárdenas Gasca', 'Joshua A. Fisher', 'Erica Principe Cruz', 'Cinthya Jauregui', 'Amy Lueck', 'Fannie Liu', 'Andrés Monroy-Hernández', 'Kai Lukoff']",2024-04-08T22:02:40Z,http://arxiv.org/abs/2404.05889v3
Live-action Virtual Reality Games,"This paper proposes the concept of ""live-action virtual reality games"" as a
new genre of digital games based on an innovative combination of live-action,
mixed-reality, context-awareness, and interaction paradigms that comprise
tangible objects, context-aware input devices, and embedded/embodied
interactions. Live-action virtual reality games are ""live-action games"" because
a player physically acts out (using his/her real body and senses) his/her
""avatar"" (his/her virtual representation) in the game stage, which is the
mixed-reality environment where the game happens. The game stage is a kind of
""augmented virtuality""; a mixed-reality where the virtual world is augmented
with real-world information. In live-action virtual reality games, players wear
HMD devices and see a virtual world that is constructed using the physical
world architecture as the basic geometry and context information. Physical
objects that reside in the physical world are also mapped to virtual elements.
Live-action virtual reality games keeps the virtual and real-worlds
superimposed, requiring players to physically move in the environment and to
use different interaction paradigms (such as tangible and embodied interaction)
to complete game activities. This setup enables the players to touch physical
architectural elements (such as walls) and other objects, ""feeling"" the game
stage. Players have free movement and may interact with physical objects placed
in the game stage, implicitly and explicitly. Live-action virtual reality games
differ from similar game concepts because they sense and use contextual
information to create unpredictable game experiences, giving rise to emergent
gameplay.","['Luis Valente', 'Esteban Clua', 'Alexandre Ribeiro Silva', 'Bruno Feijó']",2016-01-07T19:30:37Z,http://arxiv.org/abs/1601.01645v1
"A Distributed Software Architecture for Collaborative Teleoperation
  based on a VR Platform and Web Application Interoperability","Augmented Reality and Virtual Reality can provide to a Human Operator (HO) a
real help to complete complex tasks, such as robot teleoperation and
cooperative teleassistance. Using appropriate augmentations, the HO can
interact faster, safer and easier with the remote real world. In this paper, we
present an extension of an existing distributed software and network
architecture for collaborative teleoperation based on networked human-scaled
mixed reality and mobile platform. The first teleoperation system was composed
by a VR application and a Web application. However the 2 systems cannot be used
together and it is impossible to control a distant robot simultaneously. Our
goal is to update the teleoperation system to permit a heterogeneous
collaborative teleoperation between the 2 platforms. An important feature of
this interface is based on different Mobile platforms to control one or many
robots.","['Christophe Domingues', 'Samir Otmane', 'Frédéric Davesne', 'Malik Mallem']",2009-04-14T11:21:47Z,http://arxiv.org/abs/0904.2096v1
"Die Zukunft sehen: Die Chancen und Herausforderungen der Erweiterten und
  Virtuellen Realität für industrielle Anwendungen","Digitalization offers chances as well as risks for industrial companies. This
article describes how the area of Mixed Reality, with its manifestations
Augmented and Virtual Reality, can support industrial applications in the age
of digitalization. Starting from a historical perspective on Augmented and
Virtual Reality, this article surveys recent developments in the domain of
Mixed Reality, relevant for industrial use cases.
  ---
  Die Digitalisierung bietet f\""ur Industrieunternehmen neue Chancen, stellt
diese jedoch auch vor Herausforderungen. Dieser Artikel beleuchtet wie das
Gebiet der vermischten Realit\""at mit seinen Auspr\""agungen der erweiterten
Realit\""at und der virtuellen Realit\""at f\""ur industriellen Anwendungen im
Zeitalter der Digitalisierung Vorteile schaffen kann. Ausgehend von einer
historischen Betrachtung, werden aktuelle Entwicklungen auf dem Gebiet der
erweiterten und virtuellen Realit\""at diskutiert.",['Jens Grubert'],2017-09-04T16:07:35Z,http://arxiv.org/abs/1709.01020v1
"Google Cardboard Dates Augmented Reality : Issues, Challenges and Future
  Opportunities","The Google's frugal Cardboard solution for immersive Virtual Reality
experiences has come a long way in the VR market. The Google Cardboard VR
applications will support us in the fields such as education, virtual tourism,
entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has
introduced support for developing mixed reality applications for Google
Cardboard which can combine Virtual and Augmented Reality to develop exciting
and immersive experiences. In this work, we present a comprehensive review of
Google Cardboard for AR and also highlight its technical and subjective
limitations by conducting a feasibility study through the inspection of a
Desktop computer use-case. Additionally, we recommend the future avenues for
the Google Cardboard in AR. This work also serves as a guide for Android/iOS
developers as there are no published scholarly articles or well documented
studies exclusively on Google Cardboard with both user and developer's
experience captured at one place.","['Ramakrishna Perla', 'Ramya Hebbalaguppe']",2017-06-05T06:26:25Z,http://arxiv.org/abs/1706.03851v1
"Would Gaze-Contingent Rendering Improve Depth Perception in Virtual and
  Augmented Reality?","Near distances are overestimated in virtual reality, and far distances are
underestimated, but an explanation for these distortions remains elusive. One
potential concern is that whilst the eye rotates to look at the virtual scene,
the virtual cameras remain static. Could using eye-tracking to change the
perspective of the virtual cameras as the eye rotates improve depth perception
in virtual reality? This paper identifies 14 distinct perspective distortions
that could in theory occur from keeping the virtual cameras fixed whilst the
eye rotates in the context of near-eye displays. However, the impact of eye
movements on the displayed image depends on the optical, rather than physical,
distance of the display. Since the optical distance of most head-mounted
displays is over 1m, most of these distortions will have only a negligible
effect. The exception are 'gaze-contingent disparities', which will leave near
virtual objects looking displaced from physical objects that are meant to be at
the same distance in augmented reality.",['Paul Linton'],2019-05-24T11:47:08Z,http://arxiv.org/abs/1905.10366v1
"Immersive Insights: A Hybrid Analytics System for Collaborative
  Exploratory Data Analysis","In the past few years, augmented reality (AR) and virtual reality (VR)
technologies have experienced terrific improvements in both accessibility and
hardware capabilities, encouraging the application of these devices across
various domains. While researchers have demonstrated the possible advantages of
AR and VR for certain data science tasks, it is still unclear how these
technologies would perform in the context of exploratory data analysis (EDA) at
large. In particular, we believe it is important to better understand which
level of immersion EDA would concretely benefit from, and to quantify the
contribution of AR and VR with respect to standard analysis workflows.
  In this work, we leverage a Dataspace reconfigurable hybrid reality
environment to study how data scientists might perform EDA in a co-located,
collaborative context. Specifically, we propose the design and implementation
of Immersive Insights, a hybrid analytics system combining high-resolution
displays, table projections, and augmented reality (AR) visualizations of the
data.
  We conducted a two-part user study with twelve data scientists, in which we
evaluated how different levels of data immersion affect the EDA process and
compared the performance of Immersive Insights with a state-of-the-art,
non-immersive data analysis system.","['Marco Cavallo', 'Mishal Dholakia', 'Matous Havlena', 'Kenneth Ocheltree', 'Mark Podlaseck']",2019-10-27T06:44:30Z,http://arxiv.org/abs/1910.12193v1
"Real-Time Detection of Simulator Sickness in Virtual Reality Games Based
  on Players' Psychophysiological Data during Gameplay","Virtual Reality (VR) technology has been proliferating in the last decade,
especially in the last few years. However, Simulator Sickness (SS) still
represents a significant problem for its wider adoption. Currently, the most
common way to detect SS is using the Simulator Sickness Questionnaire (SSQ).
SSQ is a subjective measurement and is inadequate for real-time applications
such as VR games. This research aims to investigate how to use machine learning
techniques to detect SS based on in-game characters' and users' physiological
data during gameplay in VR games. To achieve this, we designed an experiment to
collect such data with three types of games. We trained a Long Short-Term
Memory neural network with the dataset eye-tracking and character movement data
to detect SS in real-time. Our results indicate that, in VR games, our model is
an accurate and efficient way to detect SS in real-time.","['Jialin Wang', 'Hai-Ning Liang', 'Diego Monteiro', 'Wenge Xu', 'Hao Chen', 'Qiwen Chen']",2020-10-13T03:53:07Z,http://arxiv.org/abs/2010.06152v1
"An examination of skill requirements for Augmented Reality and Virtual
  Reality job advertisements","The field of Augmented Reality (AR) and Virtual Reality (VR) has seen massive
growth in recent years. Numerous degree programs have started to redesign their
curricula to meet the high market demand of such job positions. In this paper,
we performed a content analysis of online job postings hosted on Indeed.com and
provided a skill classification framework for AR/VR job positions. Furthermore,
we present a ranking of the relevant skills for the job position. Overall, we
noticed that technical skills like UI/UX design, software design, asset design
and graphics rendering are highly desirable for AR/VR positions. Our findings
regarding prominent skill categories could be beneficial for the human resource
departments as well as enhancing existing course curricula to tailor to the
high market demand.","['Amit Verma', 'Pratibha Purohit', 'Timothy Thornton', 'Kamal Lamsal']",2021-08-10T22:06:10Z,http://arxiv.org/abs/2108.04946v1
"Augmented Reality and Mixed Reality Measurement Under Different
  Environments: A Survey on Head-Mounted Devices","Augmented Reality (AR) and Mixed Reality (MR) have been two of the most
explosive research topics in the last few years. Head-Mounted Devices (HMDs)
are essential intermediums for using AR and MR technology, playing an important
role in the research progress in these two areas. Behavioral research with
users is one way of evaluating the technical progress and effectiveness of
HMDs. In addition, AR and MR technology is dependent upon virtual interactions
with the real environment. Thus, conditions in real environments can be a
significant factor for AR and MR measurements with users. In this paper, we
survey 87 environmental-related HMD papers with measurements from users,
spanning over 32 years. We provide a thorough review of AR- and MR-related user
experiments with HMDs under different environmental factors. Then, we summarize
trends in this literature over time using a new classification method with four
environmental factors, the presence or absence of user feedback in behavioral
experiments, and ten main categories to subdivide these papers (e.g., domain
and method of user assessment). We also categorize characteristics of the
behavioral experiments, showing similarities and differences among papers.","['Hung-Jui Guo', 'Jonathan Z. Bakdash', 'Laura R. Marusich', 'Balakrishnan Prabhakaran']",2022-10-29T02:03:56Z,http://arxiv.org/abs/2210.16463v1
"Comparative Analysis of Change Blindness in Virtual Reality and
  Augmented Reality Environments","Change blindness is a phenomenon where an individual fails to notice
alterations in a visual scene when a change occurs during a brief interruption
or distraction. Understanding this phenomenon is specifically important for the
technique that uses a visual stimulus, such as Virtual Reality (VR) or
Augmented Reality (AR). Previous research had primarily focused on 2D
environments or conducted limited controlled experiments in 3D immersive
environments. In this paper, we design and conduct two formal user experiments
to investigate the effects of different visual attention-disrupting conditions
(Flickering and Head-Turning) and object alternative conditions (Removal, Color
Alteration, and Size Alteration) on change blindness detection in VR and AR
environments. Our results reveal that participants detected changes more
quickly and had a higher detection rate with Flickering compared to
Head-Turning. Furthermore, they spent less time detecting changes when an
object disappeared compared to changes in color or size. Additionally, we
provide a comparison of the results between VR and AR environments.","['DongHoon Kim', 'Dongyun Han', 'Isaac Cho']",2023-08-24T00:08:39Z,http://arxiv.org/abs/2308.12476v3
"OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed
  Reality","One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.","['Aditya Sharma', 'Luke Yoffe', 'Tobias Höllerer']",2024-01-17T04:52:40Z,http://arxiv.org/abs/2401.08973v1
"Toward Better Understanding of Saliency Prediction in Augmented 360
  Degree Videos","Augmented reality (AR) overlays digital content onto the reality. In AR
system, correct and precise estimations of user's visual fixations and head
movements can enhance the quality of experience by allocating more computation
resources on the areas of interest. However, there is inadequate research about
understanding the visual exploration of users when using an AR system or
modeling AR visual attention. To bridge the gap between the saliency prediction
on real-world scene and on scene augmented by virtual information, we construct
the ARVR saliency dataset with 12 diverse videos viewed by 20 people. The
virtual reality (VR) technique is employed to simulate the real-world.
Annotations of object recognition and tracking as augmented contents are
blended into the omnidirectional videos. The saliency annotations of head and
eye movements for both original and augmented videos are collected and together
constitute the ARVR dataset. We also design a model which is capable of solving
the saliency prediction problem in AR. Local block images are extracted to
simulate the viewport and offset the projection distortion. Conspicuous visual
cues in local viewports are extracted to constitute the spatial features. The
optical flow information is estimated as the important temporal feature. We
also consider the interplay between virtual information and reality. The
composition of the augmentation information is distinguished, and the joint
effects of adversarial augmentation and complementary augmentation are
estimated. We generate a graph by taking each block image as one node. Both the
visual saliency mechanism and the characteristics of viewing behaviors are
considered in the computation of edge weights on the graph which are
interpreted as Markov chains. The fraction of the visual attention that is
diverted to each block image is estimated through equilibrium distribution on
of this chain.","['Yucheng Zhu', 'Xiongkuo Min', 'DanDan Zhu', 'Ke Gu', 'Jiantao Zhou', 'Guangtao Zhai', 'Xiaokang Yang', 'Wenjun Zhang']",2019-12-12T14:16:05Z,http://arxiv.org/abs/1912.05971v2
"RealitySummary: On-Demand Mixed Reality Document Enhancement using Large
  Language Models","We introduce RealitySummary, a mixed reality reading assistant that can
enhance any printed or digital document using on-demand text extraction,
summarization, and augmentation. While augmented reading tools promise to
enhance physical reading experiences with overlaid digital content, prior
systems have typically required pre-processed documents, which limits their
generalizability and real-world use cases. In this paper, we explore on-demand
document augmentation by leveraging large language models. To understand
generalizable techniques for diverse documents, we first conducted an
exploratory design study which identified five categories of document
enhancements (summarization, augmentation, navigation, comparison, and
extraction). Based on this, we developed a proof-of-concept system that can
automatically extract and summarize text using Google Cloud OCR and GPT-4, then
embed information around documents using a Microsoft Hololens 2 and Apple
Vision Pro. We demonstrate real-time examples of six specific document
augmentations: 1) summaries, 2) comparison tables, 3) timelines, 4) keyword
lists, 5) summary highlighting, and 6) information cards. Results from a
usability study (N=12) and in-the-wild study (N=11) highlight the potential
benefits of on-demand MR document enhancement and opportunities for future
research.","['Aditya Gunturu', 'Shivesh Jadon', 'Nandi Zhang', 'Jarin Thundathil', 'Wesley Willett', 'Ryo Suzuki']",2024-05-28T21:59:56Z,http://arxiv.org/abs/2405.18620v1
Augmented reality usage for prototyping speed up,"The first part of the article describes our approach for solution of this
problem by means of Augmented Reality. The merging of the real world model and
digital objects allows streamline the work with the model and speed up the
whole production phase significantly. The main advantage of augmented reality
is the possibility of direct manipulation with the scene using a portable
digital camera. Also adding digital objects into the scene could be done using
identification markers placed on the surface of the model. Therefore it is not
necessary to work with special input devices and lose the contact with the real
world model. Adjustments are done directly on the model. The key problem of
outlined solution is the ability of identification of an object within the
camera picture and its replacement with the digital object. The second part of
the article is focused especially on the identification of exact position and
orientation of the marker within the picture. The identification marker is
generalized into the triple of points which represents a general plane in
space. There is discussed the space identification of these points and the
description of representation of their position and orientation be means of
transformation matrix. This matrix is used for rendering of the graphical
objects (e. g. in OpenGL and Direct3D).","['Jiri Stastny', 'David Prochazka', 'Tomas Koubek', 'Jaromir Landa']",2011-03-10T16:00:52Z,http://arxiv.org/abs/1103.2063v1
Augmented Reality in ICT for Minimum Knowledge Loss,"Informatics world digitizes the human beings, with the contribution made by
all the industrial people. In the recent survey it is proved that people are
not accustomed or they are not able to access the electronic devices to its
extreme usage. Also people are more dependent to the technologies and their
day-to-day activities are ruled by the same. In this paper we discuss on one of
the advanced technology which will soon rule the world and make the people are
more creative and at the same time hassle-free. This concept is introduced as
6th sense technology by an IIT, Mumbai student who is presently Ph.D., scholar
in MIT, USA. Similar to this research there is one more research going on under
the title Augmented Reality. This research makes a new association with the
real world to digital world and allows us to share and manipulate the
information directly with our mental thoughts. A college which implements state
of the art technology for teaching and learning, Higher College of Technology,
Muscat, (HCT) tries to identify the opportunities and limitations of
implementing this augmented reality for teaching and learning. The research
team of HCT, here, tries to give two scenarios in which augmented reality can
fit in. Since this research is in the conceptual level we are trying to
illustrate the history of this technology and how it can be adopted in the
teaching environment","['RamKumar Lakshminarayanan', 'RD. Balaji', 'Binod kumar', 'Malathi Balaji']",2013-05-11T12:32:04Z,http://arxiv.org/abs/1305.2500v1
"The Effects of Object Shape, Fidelity, Color, and Luminance on Depth
  Perception in Handheld Mobile Augmented Reality","Depth perception of objects can greatly affect a user's experience of an
augmented reality (AR) application. Many AR applications require depth matching
of real and virtual objects and have the possibility to be influenced by depth
cues. Color and luminance are depth cues that have been traditionally studied
in two-dimensional (2D) objects. However, there is little research
investigating how the properties of three-dimensional (3D) virtual objects
interact with color and luminance to affect depth perception, despite the
substantial use of 3D objects in visual applications. In this paper, we present
the results of a paired comparison experiment that investigates the effects of
object shape, fidelity, color, and luminance on depth perception of 3D objects
in handheld mobile AR. The results of our study indicate that bright colors are
perceived as nearer than dark colors for a high-fidelity, simple 3D object,
regardless of hue. Additionally, bright red is perceived as nearer than any
other color. These effects were not observed for a low-fidelity version of the
simple object or for a more-complex 3D object. High-fidelity objects had more
perceptual differences than low-fidelity objects, indicating that fidelity
interacts with color and luminance to affect depth perception. These findings
reveal how the properties of 3D models influence the effects of color and
luminance on depth perception in handheld mobile AR and can help developers
select colors for their applications.","['Tiffany D. Do', 'Joseph J. LaViola Jr.', 'Ryan P. McMahan']",2020-08-12T18:12:05Z,http://arxiv.org/abs/2008.05505v1
"A Novel Visualization System of Using Augmented Reality in Knee
  Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm","Background and aim: Image registration and alignment are the main limitations
of augmented reality-based knee replacement surgery. This research aims to
decrease the registration error, eliminate outcomes that are trapped in local
minima to improve the alignment problems, handle the occlusion, and maximize
the overlapping parts. Methodology: markerless image registration method was
used for Augmented reality-based knee replacement surgery to guide and
visualize the surgical operation. While weight least square algorithm was used
to enhance stereo camera-based tracking by filling border occlusion in right to
left direction and non-border occlusion from left to right direction. Results:
This study has improved video precision to 0.57 mm~0.61 mm alignment error.
Furthermore, with the use of bidirectional points, for example, forwards and
backwards directional cloud point, the iteration on image registration was
decreased. This has led to improve the processing time as well. The processing
time of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear
that this proposed system has focused on overcoming the misalignment difficulty
caused by movement of patient and enhancing the AR visualization during knee
replacement surgery. The proposed system was reliable and favorable which helps
in eliminating alignment error by ascertaining the optimal rigid transformation
between two cloud points and removing the outliers and non-Gaussian noise. The
proposed augmented reality system helps in accurate visualization and
navigation of anatomy of knee such as femur, tibia, cartilage, blood vessels,
etc.","['Nitish Maharjan', 'Abeer Alsadoon', 'P. W. C. Prasad', 'Salma Abdullah', 'Tarik A. Rashid']",2021-03-13T19:18:16Z,http://arxiv.org/abs/2104.05742v1
"Reducing Gaze Distraction for Real-time Vibration Monitoring Using
  Augmented Reality","Operators want to maintain awareness of the structure being tested while
observing sensor data. Normally the human's gaze shifts to a separate device or
screen during the experiment for data information, missing the structure's
physical response. The human-computer interaction provides valuable data and
information but separates the human from the reality. The sensor data does not
collect experiment safety, quality, and other contextual information of
critical value to the operator. To solve this problem, this research provides
humans with real-time information about vibrations using an Augmented Reality
(AR) application. An application is developed to augment sensor data on top of
the area of interest, which allows the user to perceive real-time changes that
the data may not warn of. This paper presents the results of an experiment that
show how AR can provide a channel for direct sensor feedback while increasing
awareness of reality. In the experiment a researcher attempts to closely follow
a moving sensor with their own sensor while observing the moving sensor's data
with and without AR. The results of the reported experiment indicate that
augmenting the information collected from sensors in real-time narrows the
operator's focus to the structure of interest for more efficient and informed
experimentation.","['Elijah Wyckoff', 'Marlan Ball', 'Fernando Moreu']",2021-10-05T17:37:08Z,http://arxiv.org/abs/2110.02192v2
"Augmented Reality Appendages for Robots: Design Considerations and
  Recommendations for Maximizing Social and Functional Perception","In order to address the limitations of gestural capabilities in physical
robots, researchers in Virtual, Augmented, Mixed Reality Human-Robot
Interaction (VAM-HRI) have been using augmented-reality visualizations that
increase robot expressivity and improve user perception (e.g., social
presence). While a multitude of virtual robot deictic gestures (e.g., pointing
to an object) have been implemented to improve interactions within VAM-HRI,
such systems are often reported to have tradeoffs between functional and social
user perceptions of robots, creating a need for a unified approach that
considers both attributes. We performed a literature analysis that selected
factors that were noted to significantly influence either user perception or
task efficiency and propose a set of design considerations and recommendations
that address those factors by combining anthropomorphic and non-anthropomorphic
virtual gestures based on the motivation of the interaction, visibility of the
target and robot, salience of the target, and distance between the target and
robot. The proposed recommendations provide the VAM-HRI community with starting
points for selecting appropriate gesture types for a multitude of interaction
contexts.","['Ipek Goktan', 'Karen Ly', 'Thomas R. Groechel', 'Maja J. Mataric']",2022-05-13T16:30:53Z,http://arxiv.org/abs/2205.06747v1
"Towards an Intelligent Assistive System Based on Augmented Reality and
  Serious Games","Age-related cognitive impairment is generally characterized by gradual memory
loss and decision-making difficulties. The aim of this study is to investigate
multi level support and suggest relevant helping means for the elderly with
mild cognitive impairment as well as their caregivers as the primary end-users.
This work reports preliminary results on an intelligent assistive system,
achieved through the integration of Internet of Things, augmented reality, and
adaptive fuzzy decision-making methods. The proposed system operates in
different modes, including automated and semi-automated modes. The former helps
the user complete their daily life activities by showing augmented reality
messages or making automatic changes; while the latter allows manual changes
after the real-time assessment of the user's cognitive state based on the
augmented reality serious game score. We have also evaluated the accuracy of
the serious game score with 37 elderly participants and compared it with users'
paper-based cognitive test results. We further noted that there is an
acceptable correlation between the paper-based test and users' serious game
scores. Moreover, we observed that the system response in the semi-automated
mode causes less data loss compared with the automated mode, as the number of
active devices decreases.","['Fatemeh Ghorbani', 'Mahsa Farshi Taghavi', 'Mehdi Delrobaei']",2023-01-06T11:10:04Z,http://arxiv.org/abs/2301.02461v1
"See or Hear? Exploring the Effect of Visual and Audio Hints and
  Gaze-assisted Task Feedback for Visual Search Tasks in Augmented Reality","Augmented reality (AR) is emerging in visual search tasks for increasingly
immersive interactions with virtual objects. We propose an AR approach
providing visual and audio hints along with gaze-assisted instant post-task
feedback for search tasks based on mobile head-mounted display (HMD). The
target case was a book-searching task, in which we aimed to explore the effect
of the hints together with the task feedback with two hypotheses. H1: Since
visual and audio hints can positively affect AR search tasks, the combination
outperforms the individuals. H2: The gaze-assisted instant post-task feedback
can positively affect AR search tasks. The proof-of-concept was demonstrated by
an AR app in HMD and a comprehensive user study (n=96) consisting of two
sub-studies, Study I (n=48) without task feedback and Study II (n=48) with task
feedback. Following quantitative and qualitative analysis, our results
partially verified H1 and completely verified H2, enabling us to conclude that
the synthesis of visual and audio hints conditionally improves the AR visual
search task efficiency when coupled with task feedback.","['Yuchong Zhang', 'Adam Nowak', 'Yueming Xuan', 'Andrzej Romanowski', 'Morten Fjeld']",2023-02-03T12:37:14Z,http://arxiv.org/abs/2302.01690v2
"GBOT: Graph-Based 3D Object Tracking for Augmented Reality-Assisted
  Assembly Guidance","Guidance for assemblable parts is a promising field for augmented reality.
Augmented reality assembly guidance requires 6D object poses of target objects
in real time. Especially in time-critical medical or industrial settings,
continuous and markerless tracking of individual parts is essential to
visualize instructions superimposed on or next to the target object parts. In
this regard, occlusions by the user's hand or other objects and the complexity
of different assembly states complicate robust and real-time markerless
multi-object tracking. To address this problem, we present Graph-based Object
Tracking (GBOT), a novel graph-based single-view RGB-D tracking approach. The
real-time markerless multi-object tracking is initialized via 6D pose
estimation and updates the graph-based assembly poses. The tracking through
various assembly states is achieved by our novel multi-state assembly graph. We
update the multi-state assembly graph by utilizing the relative poses of the
individual assembly parts. Linking the individual objects in this graph enables
more robust object tracking during the assembly process. For evaluation, we
introduce a synthetic dataset of publicly available and 3D printable assembly
assets as a benchmark for future work. Quantitative experiments in synthetic
data and further qualitative study in real test data show that GBOT can
outperform existing work towards enabling context-aware augmented reality
assembly guidance. Dataset and code will be made publically available.","['Shiyu Li', 'Hannah Schieber', 'Niklas Corell', 'Bernhard Egger', 'Julian Kreimeier', 'Daniel Roth']",2024-02-12T14:38:46Z,http://arxiv.org/abs/2402.07677v1
"Achieving Narrative Change Through AR: Displacing the Single Story to
  Create Spatial Justice","The ability of Augmented Reality to overcome the bias of single stories
through multidimensionality is explored in the artifacts of a youth gun
violence prevention project and its goal of narrative change.",['Janice Tisha Samuels'],2024-05-05T15:31:08Z,http://arxiv.org/abs/2405.02971v1
A Standalone Markerless 3D Tracker for Handheld Augmented Reality,"This paper presents an implementation of a markerless tracking technique
targeted to the Windows Mobile Pocket PC platform. The primary aim of this work
is to allow the development of standalone augmented reality applications for
handheld devices based on natural feature tracking. In order to achieve this
goal, a subset of two computer vision libraries was ported to the Pocket PC
platform. They were also adapted to use fixed point math, with the purpose of
improving the overall performance of the routines. The port of these libraries
opens up the possibility of having other computer vision tasks being executed
on mobile platforms. A model based tracking approach that relies on edge
information was adopted. Since it does not require a high processing power, it
is suitable for constrained devices such as handhelds. The OpenGL ES graphics
library was used to perform computer vision tasks, taking advantage of existing
graphics hardware acceleration. An augmented reality application was created
using the implemented technique and evaluations were done regarding tracking
performance and accuracy","['Joao Paulo Lima', 'Veronica Teichrieb', 'Judith Kelner']",2009-02-12T18:25:13Z,http://arxiv.org/abs/0902.2187v1
A Survey on Web-based AR Applications,"Due to the increase of interest in Augmented Reality (AR), the potential uses
of AR are increasing also. It can benefit the user in various fields such as
education, business, medicine, and other. Augmented Reality supports the real
environment with synthetic environment to give more details and meaning to the
objects in the real word. AR refers to a situation in which the goal is to
supplement a user's perception of the real-world through the addition of
virtual objects. This paper is an attempt to make a survey of web-based
Augmented Reality applications and make a comparison among them.","['Behrang Parhizkar', 'Ashraf Abbas M. Al-Modwahi', 'Arash Habibi Lashkari', 'Mohammad Mehdi Bartaripou', 'Hossein Reza Babae']",2011-11-13T07:13:23Z,http://arxiv.org/abs/1111.2993v1
Mobile augmented reality survey: a bottom-up approach,"Augmented Reality (AR) is becoming mobile. Mobile devices have many
constraints but also rich new features that traditional desktop computers do
not have. There are several survey papers on AR, but none is dedicated to
Mobile Augmented Reality (MAR). Our work serves the purpose of closing this
gap. The contents are organized with a bottom-up approach. We first present the
state-of-the-art in system components including hardware platforms, software
frameworks and display devices, follows with enabling technologies such as
tracking and data management. We then survey the latest technologies and
methods to improve run-time performance and energy efficiency for practical
implementation. On top of these, we further introduce the application fields
and several typical MAR applications. Finally we conclude the survey with
several challenge problems, which are under exploration and require great
research efforts in the future.","['Zhanpeng Huang', 'Pan Hui', 'Christoph Peylo', 'Dimitris Chatzopoulos']",2013-09-17T18:13:01Z,http://arxiv.org/abs/1309.4413v2
Low-cost Augmented Reality prototype for controlling network devices,"With the evolution of mobile devices, and smart-phones in particular, comes
the ability to create new experiences that enhance the way we see, interact,
and manipulate objects, within the world that surrounds us. It is now possible
to blend data from our senses and our devices in numerous ways that simply were
not possible before using Augmented Reality technology. In a near future, when
all of the office devices as well as your personal electronic gadgets are on a
common wireless network, operating them using a universal remote controller
would be possible. This paper presents an off-the-shelf, low-cost prototype
that leverages the Augmented Reality technology to deliver a novel and
interactive way of operating office network devices around using a mobile
device. We believe this type of system may provide benefits to controlling
multiple integrated devices and visualizing interconnectivity or utilizing
visual elements to pass information from one device to another, or may be
especially beneficial to control devices when interacting with them physically
may be difficult or pose danger or harm.","['Anh Nguyen', 'Amy Banic']",2014-06-12T04:46:32Z,http://arxiv.org/abs/1406.3117v1
"Towards a Cloud-based Architecture for Visualization and Augmented
  Reality to Support Collaboration in Manufacturing Automation","In this report, we present our work in visualization and augmented reality
technologies supporting collaboration in manufacturing automation. Our approach
is based on (i) analysis based on spatial models of automation environments,
(ii) next-generation controllers based on single board computers, (iii) cloud-,
service- and web-based technologies and (iv) an emphasis on experimental
development using real automation equipment. The contribution of this paper is
the documentation of two new demonstrators, one for distributed viewing of 3D
scans of factory environments, and another for real time augmented reality
display of the status of a manufacturing plant, each based on technologies
under development in our lab and in particular applied to a mini-factory.","['Ian D. Peake', 'Jan Olaf Blech', 'Shyam Nath', 'Jacob Jacky Aharon', 'Argyll McGhie']",2017-11-16T09:19:32Z,http://arxiv.org/abs/1711.05997v1
"Robust Deep-Learning-Based Road-Prediction for Augmented Reality
  Navigation Systems","This paper proposes an approach that predicts the road course from camera
sensors leveraging deep learning techniques. Road pixels are identified by
training a multi-scale convolutional neural network on a large number of
full-scene-labeled night-time road images including adverse weather conditions.
A framework is presented that applies the proposed approach to longer distance
road course estimation, which is the basis for an augmented reality navigation
application. In this framework long range sensor data (radar) and data from a
map database are fused with short range sensor data (camera) to produce a
precise longitudinal and lateral localization and road course estimation. The
proposed approach reliably detects roads with and without lane markings and
thus increases the robustness and availability of road course estimations and
augmented reality navigation. Evaluations on an extensive set of high precision
ground truth data taken from a differential GPS and an inertial measurement
unit show that the proposed approach reaches state-of-the-art performance
without the limitation of requiring existing lane markings.","['Matthias Limmer', 'Julian Forster', 'Dennis Baudach', 'Florian Schüle', 'Roland Schweiger', 'Hendrik P. A. Lensch']",2016-05-31T09:00:33Z,http://arxiv.org/abs/1605.09533v1
A survey on haptic technologies for mobile augmented reality,"Augmented Reality (AR) and Mobile Augmented Reality (MAR) applications have
gained much research and industry attention these days. The mobile nature of
MAR applications limits users' interaction capabilities such as inputs, and
haptic feedbacks. This survey reviews current research issues in the area of
human computer interaction for MAR and haptic devices. The survey first
presents human sensing capabilities and their applicability in AR applications.
We classify haptic devices into two groups according to the triggered sense:
cutaneous/tactile: touch, active surfaces, and mid-air, kinesthetic:
manipulandum, grasp, and exoskeleton. Due to the mobile capabilities of MAR
applications, we mainly focus our study on wearable haptic devices for each
category and their AR possibilities. To conclude, we discuss the future paths
that haptic feedbacks should follow for MAR applications and their challenges.","['Carlos Bermejo', 'Pan Hui']",2017-09-03T11:12:15Z,http://arxiv.org/abs/1709.00698v3
"Analysis of the co-design activity: influence of a mixed artifact and
  contribution of the gestural function in a spatial augmented reality
  environment","Augmented reality provides new possibilities to propose environments where
the designers can take advantage of the physicality of the artifacts while
keeping the versatility of digital environments. Mixed objects can therefore
provide new media in the interactions between stakeholders. Besides, the
increasing interest in user participation in early design phases is limited by
the poor representations or the expensive mock ups to be provided in design
meetings. Therefore, understanding the role of these mixed artifacts by
analyzing and characterizing the interactions is crucial to the development of
both design methods and environments. By focusing on multimodal interactions,
we aim at providing new results in terms of the design process, in particular
by studying the contribution of the gesture in collaborative product
co-creativity sessions but also by understanding the role of these multiple
interactions in an augmented reality environment.","['Maud Poulin', 'Jean-François Boujut', 'Cédric Masclet']",2019-11-15T16:07:35Z,http://arxiv.org/abs/1911.07985v1
"Augmented Reality for Human-Swarm Interaction in a Swarm-Robotic
  Chemistry Simulation","We present a method to register individual members of a robotic swarm in an
augmented reality display while showing relevant information about swarm
dynamics to the user that would be otherwise hidden. Individual swarm members
and clusters of the same group are identified by their color, and by blinking
at a specific time interval that is distinct from the time interval at which
their neighbors blink. We show that this problem is an instance of the graph
coloring problem, which can be solved in a distributed manner in O(log(n))
time. We demonstrate our approach using a swarm chemistry simulation in which
robots simulate individual atoms that form molecules following the rules of
chemistry. Augmented reality is then used to display information about the
internal state of individual swarm members as well as their topological
relationship, corresponding to molecular bonds.","['Sumeet Batra', 'John Klingner', 'Nikolaus Correll']",2019-12-02T17:24:10Z,http://arxiv.org/abs/1912.00951v1
"DAARIA: Driver Assistance by Augmented Reality for Intelligent
  Automobile","Taking into account the drivers' state is a major challenge for designing new
advanced driver assistance systems. In this paper we present a driver
assistance system strongly coupled to the user. DAARIA 1 stands for Driver
Assistance by Augmented Reality for Intelligent Automobile. It is an augmented
reality interface powered by several sensors. The detection has two goals: one
is the position of obstacles and the quantification of the danger represented
by them. The other is the driver's behavior. A suitable visualization metaphor
allows the driver to perceive at any time the location of the relevant hazards
while keeping his eyes on the road. First results show that our method could be
applied to a vehicle but also to aerospace, fluvial or maritime navigation.","['Paul George', 'Indira Thouvenin', 'Vincent Fremont', 'Véronique Cherfaoui']",2012-09-27T07:15:22Z,http://arxiv.org/abs/1209.6140v1
"MVC-3D: Adaptive Design Pattern for Virtual and Augmented Reality
  Systems","In this paper, we present MVC-3D design pattern to develop virtual and
augmented (or mixed) reality interfaces that use new types of sensors,
modalities and implement specific algorithms and simulation models. The
proposed pattern represents the extension of classic MVC pattern by enriching
the View component (interactive View) and adding a specific component
(Library). The results obtained on the development of augmented reality
interfaces showed that the complexity of M, iV and C components is reduced. The
complexity increases only on the Library component (L). This helps the
programmers to well structure their models even if the interface complexity
increases. The proposed design pattern is also used in a design process called
MVC-3D in the loop that enables a seamless evolution from initial prototype to
the final system.","['Samir Benbelkacem', 'Djamel Aouam', 'Nadia Zenati-Henda', 'Abdelkader Bellarbi', 'Ahmed Bouhena', 'Samir Otmane']",2019-03-01T07:38:23Z,http://arxiv.org/abs/1903.00185v1
"Using the Makerspace to Create Educational Open-source Software for
  Electrical Circuits: A Learning Experience","Virtual learning environments are a useful modality for engaging students in
the classroom by affording them a sense of presence and immersion. The
motivation of this project was to create an open-source augmented reality
electrical circuit application for use in lower division engineering courses to
teach students about electricity fundamentals. Softwares that are readily
available for use on virtual and augmented reality devices do not typically
apply to all disciplines and do not necessarily have a pedagogical or
accessibility focus. Considering this lack of appropriate educational
applications for the current virtual and augmented reality devices, a team of
interdisciplinary students was assembled to create such software. With
extensive usability studies, the application was designed for quick adoption
and improve accessibility by providing multimodal access such as voice
assistant, gray scaling for depth perception and daltonize the app. The
software is available as part of VITaL Laboratory, Sonoma State University.","['Dana Conard', 'Blake Vollmer', 'Corbin Shatto', 'Hannah Bowman', 'Sara Kassis']",2019-08-06T05:35:21Z,http://arxiv.org/abs/1908.01963v1
3D landmark detection for augmented reality based otologic procedures,"Ear consists of the smallest bones in the human body and does not contain
significant amount of distinct landmark points that may be used to register a
preoperative CT-scan with the surgical video in an augmented reality framework.
Learning based algorithms may be used to help the surgeons to identify landmark
points. This paper presents a convolutional neural network approach to landmark
detection in preoperative ear CT images and then discusses an augmented reality
system that can be used to visualize the cochlear axis on an otologic surgical
video.","['Raabid Hussain', 'Alain Lalande', 'Kibrom Berihu Girum', 'Caroline Guigou', 'Alexis Bozorg Grayeli']",2019-09-04T09:31:22Z,http://arxiv.org/abs/1909.01647v1
3D Augmented Reality Tangible User Interface using Commodity Hardware,"During the last years, the emerging field of Augmented and Virtual Reality
(AR-VR) has seen tremendous growth. An interface that has also become very
popular for the AR systems is the tangible interface or passive-haptic
interface. Specifically, an interface where users can manipulate digital
information with input devices that are physical objects. This work presents a
low cost Augmented Reality system with a tangible interface that offers
interaction between the real and the virtual world. The system estimates in
real-time the 3D position of a small colored ball (input device), it maps it to
the 3D virtual world and then uses it to control the AR application that runs
in a mobile device. Using the 3D position of our ""input"" device, it allows us
to implement more complicated interactivity compared to a 2D input device.
Finally, we present a simple, fast and robust algorithm that can estimate the
corners of a convex quadrangle. The proposed algorithm is suitable for the fast
registration of markers and significantly improves performance compared to the
state of the art.","['Dimitrios Chamzas', 'Konstantinos Moustakas']",2020-03-02T18:29:58Z,http://arxiv.org/abs/2003.01092v1
"Towards Augmented Reality-driven Human-City Interaction: Current
  Research on Mobile Headsets and Future Challenges","Interaction design for Augmented Reality (AR) is gaining increasing attention
from both academia and industry. This survey discusses 260 articles (68.8% of
articles published between 2015 - 2019) to review the field of human
interaction in connected cities with emphasis on augmented reality-driven
interaction. We provide an overview of Human-City Interaction and related
technological approaches, followed by a review of the latest trends of
information visualization, constrained interfaces, and embodied interaction for
AR headsets. We highlight under-explored issues in interface design and input
techniques that warrant further research, and conjecture that AR with
complementary Conversational User Interfaces (CUIs) is a key enabler for
ubiquitous interaction with immersive systems in smart cities. Our work helps
researchers understand the current potential and future needs of AR in
Human-City Interaction.","['Lik Hang Lee', 'Tristan Braud', 'Simo Hosio', 'Pan Hui']",2020-07-17T19:47:02Z,http://arxiv.org/abs/2007.09207v2
Tackling problems of marker-based augmented reality under water,"Underwater sites are a harsh environment for augmented reality applications.
Obstacles that must be battled include poor visibility conditions, difficult
navigation, and hard manipulation with devices under water. This chapter
focuses on the problem of localizing a device under water using markers. It
discusses various filters that enhance and improve images recorded under water,
and their impact on marker-based tracking. It presents various combinations of
10 image improving algorithms and 4 marker detecting algorithms, and tests
their performance in real situations. All solutions are designed to run
real-time on mobile devices to provide a solid basis for augmented reality.
Usability of this solution is evaluated on locations in Mediterranean Sea. It
is shown that image improving algorithms with carefully chosen parameters can
reduce the problems with visibility under water and improve the detection of
markers. The best results are obtained with marker detecting algorithms that
are specifically designed for underwater environments.","['Jan Čejka', 'Fotis Liarokapis']",2020-10-11T09:54:13Z,http://arxiv.org/abs/2010.11691v1
"HAVEN: A Unity-based Virtual Robot Environment to Showcase HRI-based
  Augmented Reality","Due to the COVID-19 pandemic, conducting Human-Robot Interaction (HRI)
studies in person is not permissible due to social distancing practices to
limit the spread of the virus. Therefore, a virtual reality (VR) simulation
with a virtual robot may offer an alternative to real-life HRI studies. Like a
real intelligent robot, a virtual robot can utilize the same advanced
algorithms to behave autonomously. This paper introduces HAVEN (HRI-based
Augmentation in a Virtual robot Environment using uNity), a VR simulation that
enables users to interact with a virtual robot. The goal of this system design
is to enable researchers to conduct HRI Augmented Reality studies using a
virtual robot without being in a real environment. This framework also
introduces two common HRI experiment designs: a hallway passing scenario and
human-robot team object retrieval scenario. Both reflect HAVEN's potential as a
tool for future AR-based HRI studies.","['Andre Cleaver', 'Darren Tang', 'Victoria Chen', 'Jivko Sinapov']",2020-11-06T16:34:17Z,http://arxiv.org/abs/2011.03464v1
"Design and Test of an adaptive augmented reality interface to manage
  systems to assist critical missions","We present a user interface (UI) based on augmented reality (AR) with
head-mounted display (HMD) for improving situational awareness during critical
operation and improve human efficiency on operations. The UI displays
contextual information as well as accepts orders given from the headset to
control unmanned aerial vehicles (UAVs) for assisting the rescue team. We
established experiments where people had been put in a stressful situation and
are asked to resolve a complex mission using a headset and a computer.
Comparing both technologies, our results show that augmented reality has the
potential to be an important tool to help those involved in the emergency
situation.","['Dany Naser Addin', 'Benoit Ozell']",2021-03-25T22:28:37Z,http://arxiv.org/abs/2103.14160v1
"A multi-plane augmented reality head-up display system based on volume
  holographic optical elements with large area","The traditional head-up display (HUD) system has the disadvantages of a small
area and a single display plane, here we propose and design an augmented
reality (AR) HUD system with multi-plane, large area, high diffraction
efficiency and a single picture generation unit (PGU) based on holographic
optical elements (HOEs). Since volume HOEs have excellent angle selectivity and
wavelength selectivity, HOEs of different wavelengths can be designed to
display images in different planes. Experimental and simulated results verify
the feasibility of this method. Experimental results show that the diffraction
efficiencies of the red, green and blue HOEs are 75.2%, 73.1% and 67.5%. And
the size of HOEs is 20cm*15cm. Moreover, the three HOEs of red, green and blue
display images at different depths of 150cm, 500cm and 1000cm, respectively. In
addition, the field of view (FOV) and eye-box (EB) of the system are
12{\deg}*10{\deg} and 9.5cm*11.2cm. Furthermore, the light transmittance of the
system has reached 60%. It is believed that this technique can be applied to
the augmented reality navigation display of vehicles and aviation.","['Zhenlv Lv', 'Juan Liu', 'Liangfa Xu']",2021-04-12T12:24:25Z,http://arxiv.org/abs/2104.14315v1
Surgical navigation systems based on augmented reality technologies,"This study considers modern surgical navigation systems based on augmented
reality technologies. Augmented reality glasses are used to construct holograms
of the patient's organs from MRI and CT data, subsequently transmitted to the
glasses. This, in addition to seeing the actual patient, the surgeon gains
visualization inside the patient's body (bones, soft tissues, blood vessels,
etc.). The solutions developed at Peter the Great St. Petersburg Polytechnic
University allow reducing the invasiveness of the procedure and preserving
healthy tissues. This also improves the navigation process, making it easier to
estimate the location and size of the tumor to be removed. We describe the
application of developed systems to different types of surgical operations
(removal of a malignant brain tumor, removal of a cyst of the cervical spine).
We consider the specifics of novel navigation systems designed for anesthesia,
for endoscopic operations. Furthermore, we discuss the construction of novel
visualization systems for ultrasound machines. Our findings indicate that the
technologies proposed show potential for telemedicine.","['Vladimir Ivanov', 'Anton Krivtsov', 'Sergey Strelkov', 'Dmitry Gulyaev', 'Denis Godanyuk', 'Nikolay Kalakutsky', 'Artyom Pavlov', 'Marina Petropavloskaya', 'Alexander Smirnov', 'Andrew Yaremenko']",2021-05-13T11:09:12Z,http://arxiv.org/abs/2106.00727v1
"Simplifying Robot Programming using Augmented Reality and End-User
  Development","Robots are widespread across diverse application contexts. Teaching robots to
perform tasks, in their respective contexts, demands a high domain and
programming expertise. However, robot programming faces high entry barriers due
to the complexity of robot programming itself. Even for experts robot
programming is a cumbersome and error-prone task where faulty robot programs
can be created, causing damage when being executed on a real robot. To simplify
the process of robot programming, we combine Augmented Reality (AR) with
principles of end-user development. By combining them, the real environment is
extended with useful virtual artifacts that can enable experts as well as
non-professionals to perform complex robot programming tasks. Therefore, Simple
Programming Environment in Augmented Reality with Enhanced Debugging (SPEARED)
was developed as a prototype for an AR-assisted robot programming environment.
SPEARED makes use of AR to project a robot as well as a programming environment
onto the target working space. To evaluate our approach, expert interviews with
domain experts from the area of industrial automation, robotics, and AR were
performed. The experts agreed that SPEARED has the potential to enrich and ease
current robot programming processes.","['Enes Yigitbas', 'Ivan Jovanovikj', 'Gregor Engels']",2021-06-15T07:57:48Z,http://arxiv.org/abs/2106.07944v1
ARShoe: Real-Time Augmented Reality Shoe Try-on System on Smartphones,"Virtual try-on technology enables users to try various fashion items using
augmented reality and provides a convenient online shopping experience.
However, most previous works focus on the virtual try-on for clothes while
neglecting that for shoes, which is also a promising task. To this concern,
this work proposes a real-time augmented reality virtual shoe try-on system for
smartphones, namely ARShoe. Specifically, ARShoe adopts a novel multi-branch
network to realize pose estimation and segmentation simultaneously. A solution
to generate realistic 3D shoe model occlusion during the try-on process is
presented. To achieve a smooth and stable try-on effect, this work further
develop a novel stabilization method. Moreover, for training and evaluation, we
construct the very first large-scale foot benchmark with multiple virtual shoe
try-on task-related labels annotated. Exhaustive experiments on our newly
constructed benchmark demonstrate the satisfying performance of ARShoe.
Practical tests on common smartphones validate the real-time performance and
stabilization of the proposed approach.","['Shan An', 'Guangfu Che', 'Jinghao Guo', 'Haogang Zhu', 'Junjie Ye', 'Fangru Zhou', 'Zhaoqi Zhu', 'Dong Wei', 'Aishan Liu', 'Wei Zhang']",2021-08-24T03:54:45Z,http://arxiv.org/abs/2108.10515v1
"SceneAR: Scene-based Micro Narratives for Sharing and Remixing in
  Augmented Reality","Short-form digital storytelling has become a popular medium for millions of
people to express themselves. Traditionally, this medium uses primarily 2D
media such as text (e.g., memes), images (e.g., Instagram), gifs (e.g., Giphy),
and videos (e.g., TikTok, Snapchat). To expand the modalities from 2D to 3D
media, we present SceneAR, a smartphone application for creating sequential
scene-based micro narratives in augmented reality (AR). What sets SceneAR apart
from prior work is the ability to share the scene-based stories as AR content
-- no longer limited to sharing images or videos, these narratives can now be
experienced in people's own physical environments. Additionally, SceneAR
affords users the ability to remix AR, empowering them to build-upon others'
creations collectively. We asked 18 people to use SceneAR in a 3-day study.
Based on user interviews, analysis of screen recordings, and the stories they
created, we extracted three themes. From those themes and the study overall, we
derived six strategies for designers interested in supporting short-form AR
narratives.","['Mengyu Chen', 'Andrés Monroy-Hernández', 'Misha Sra']",2021-08-28T14:44:58Z,http://arxiv.org/abs/2108.12661v1
FaceEraser: Removing Facial Parts for Augmented Reality,"Our task is to remove all facial parts (e.g., eyebrows, eyes, mouth and
nose), and then impose visual elements onto the ``blank'' face for augmented
reality. Conventional object removal methods rely on image inpainting
techniques (e.g., EdgeConnect, HiFill) that are trained in a self-supervised
manner with randomly manipulated image pairs. Specifically, given a set of
natural images, randomly masked images are used as inputs and the raw images
are treated as ground truths. Whereas, this technique does not satisfy the
requirements of facial parts removal, as it is hard to obtain ``ground-truth''
images with real ``blank'' faces. To address this issue, we propose a novel
data generation technique to produce paired training data that well mimic the
``blank'' faces. In the mean time, we propose a novel network architecture for
improved inpainting quality for our task. Finally, we demonstrate various
face-oriented augmented reality applications on top of our facial parts removal
model. The source codes are released at
\href{https://github.com/duxingren14/FaceEraser}{duxingren14/FaceEraser} on
github for research purposes.","['Miao Hua', 'Lijie Liu', 'Ziyang Cheng', 'Qian He', 'Bingchuan Li', 'Zili Yi']",2021-09-22T14:30:12Z,http://arxiv.org/abs/2109.10760v2
An immersive Open Source environment using Godot,"We present a sample implementation of a Virtual and Augmented Reality
immersive environment based on Free and Libre Open Source Hardware and Software
and the HTC Vive system, used to enhance the immersive experience of the user
and to track her/his movements. The sense of immersion has increased and
stimulated using a footplate and a Tibetan bridge, connected to the virtual
world as Augmented Reality applications and implemented through an Arduino
board, thereby adopting a low cost, open source hardware and software approach.
The proposed architecture is relatively affordable from the cost point of view,
easy to implement, configure and adapt to different contexts. It can be of
great help for organizing laboratory classes for young students to afford the
implementation of virtual worlds and Augmented Reality applications.","['Francesca Santucci', 'Federico Frenguelli', 'Alessandro De Angelis', 'Ilaria Cuccaro', 'Damiano Perri', 'Marco Simonetti']",2021-11-03T01:57:54Z,http://arxiv.org/abs/2111.01974v1
"Implementing augmented reality technology to measure structural changes
  across time","In recent years, augmented reality (AR) technology has been increasingly
employed in structural health monitoring (SHM). In the case of conditions
following a seismic event, inspections are conducted to evaluate the
progression of the damage pattern quantitatively and efficiently respond if the
displacement pattern is determined to be unsafe. Additionally, quantification
of nearby structural changes over short-term and long-term periods can provide
building inspectors with information to improve safety. This paper proposes the
Time Machine Measure (TMM) application on an Augmented Reality (AR)
Head-Mounted-Device (HMD) platform. The main function of the TMM application is
to restore the saved meshes of a past environment and overlay them onto the
real environment so that inspectors can intuitively measure structural
deformation and other movement across time. The proposed TMM application was
verified by experiments meant to simulate a real-world inspection.","['Jiaqi Xu', 'Elijah Wyckoff', 'John-Wesley Hanson', 'Fernando Moreu', 'Derek Doyle']",2021-11-03T23:08:25Z,http://arxiv.org/abs/2111.02555v1
"Integrating Artificial Intelligence and Augmented Reality in Robotic
  Surgery: An Initial dVRK Study Using a Surgical Education Scenario","Robot-assisted surgery has become progressively more and more popular due to
its clinical advantages. In the meanwhile, the artificial intelligence and
augmented reality in robotic surgery are developing rapidly and receive lots of
attention. However, current methods have not discussed the coherent integration
of AI and AR in robotic surgery. In this paper, we develop a novel system by
seamlessly merging artificial intelligence module and augmented reality
visualization to automatically generate the surgical guidance for robotic
surgery education. Specifically, we first leverage reinforcement leaning to
learn from expert demonstration and then generate 3D guidance trajectory,
providing prior context information of the surgical procedure. Along with other
information such as text hint, the 3D trajectory is then overlaid in the stereo
view of dVRK, where the user can perceive the 3D guidance and learn the
procedure. The proposed system is evaluated through a preliminary experiment on
surgical education task peg-transfer, which proves its feasibility and
potential as the next generation of robot-assisted surgery education solution.","['Yonghao Long', 'Jianfeng Cao', 'Anton Deguet', 'Russell H. Taylor', 'Qi Dou']",2022-01-02T17:34:10Z,http://arxiv.org/abs/2201.00383v2
"Modern Augmented Reality: Applications, Trends, and Future Directions","Augmented reality (AR) is one of the relatively old, yet trending areas in
the intersection of computer vision and computer graphics with numerous
applications in several areas, from gaming and entertainment, to education and
healthcare. Although it has been around for nearly fifty years, it has seen a
lot of interest by the research community in the recent years, mainly because
of the huge success of deep learning models for various computer vision and AR
applications, which made creating new generations of AR technologies possible.
This work tries to provide an overview of modern augmented reality, from both
application-level and technical perspective. We first give an overview of main
AR applications, grouped into more than ten categories. We then give an
overview of around 100 recent promising machine learning based works developed
for AR systems, such as deep learning works for AR shopping (clothing, makeup),
AR based image filters (such as Snapchat's lenses), AR animations, and more. In
the end we discuss about some of the current challenges in AR domain, and the
future directions in this area.","['Shervin Minaee', 'Xiaodan Liang', 'Shuicheng Yan']",2022-02-18T22:12:37Z,http://arxiv.org/abs/2202.09450v2
"Development of Augmented Reality Application for Made-to-Order Furniture
  Industry in Pampanga, Philippines","The focus of the study was to develop a mobile application utilizing
marker-less augmented reality for specific made-to-order products to support
furniture and fixtures businesses. The study implemented mixed-methodology to
properly identify the various stakeholders' considerations in developing the
application. Interviews with key informants were conducted to ensure that the
features were appropriate for the intended user needs, and selected ISO
standards were used as evaluation criteria. The results indicate that the
mobile application with marker-less AR technology was found to be highly
acceptable by three evaluators (i.e., customers, owners, and IT experts). The
study also highlighted the use of AR-related technology in this case, where
marker-less has the potential to improve customer purchasing experience even
further. Future studies may include using newer technologies to further improve
the application. The study suggests that Augmented Reality technology could be
used to connect specific businesses directly to consumers regardless of setting
or context.","['Jaymark A. Yambao', 'John Paul P. Miranda', 'Earl Lawrence B. Pelayo']",2022-08-13T11:40:25Z,http://arxiv.org/abs/2208.06632v1
"A systematic review of structural equation modeling in augmented reality
  applications","The purpose of this study is to present a comprehensive review of the use of
structural equation modeling (SEM) in augmented reality (AR) studies in the
context of the COVID-19 pandemic. IEEE Xplore Scopus, Wiley Online Library,
Emerald Insight, and ScienceDirect are the main five data sources for data
collection from Jan 2020 to May 2021. The results showed that a variety of
external factors were used to construct the SEM models rather than using the
parsimonious ones. The reports showed a fair balance between the direct and
indirect methods to contact participants. Despite the COVID-19 pandemic, few
publications addressed the issue of data collection and evaluation methods,
whereas video demonstrations of the augmented reality (AR) apps were utilized","['Vinh The Nguyen', 'Chuyen Thi Hong Nguyen']",2023-01-25T03:21:15Z,http://arxiv.org/abs/2301.11811v1
"Has the Virtualization of the Face Changed Facial Perception? A Study of
  the Impact of Photo Editing and Augmented Reality on Facial Perception","Augmented reality and other photo editing filters are popular methods used to
modify faces online. Considering the important role of facial perception in
communication, how do we perceive this increasing number of modified faces? In
this paper we present the results of six surveys that measure familiarity with
different styles of facial filters, perceived strangeness of faces edited with
different filters, and ability to discern whether images are filtered. Our
results demonstrate that faces modified with more traditional face filters are
perceived similarly to unmodified faces, and faces filtered with augmented
reality filters are perceived differently from unmodified faces. We discuss
possible explanations for these results, including a societal adjustment to
traditional photo editing techniques or the inherent differences in the
different types of filters. We conclude with a discussion of how to build
online spaces more responsibly based on our results.","['Louisa Conwill', 'Sam English Anthony', 'Walter J. Scheirer']",2023-03-01T16:09:11Z,http://arxiv.org/abs/2303.00612v3
BrickPal: Augmented Reality-based Assembly Instructions for Brick Models,"The assembly instruction is a mandatory component of Lego-like brick sets.The
conventional production of assembly instructions requires a considerable amount
of manual fine-tuning, which is intractable for casual users and customized
brick sets.Moreover, the traditional paper-based instructions lack
expressiveness and interactivity.To tackle the two problems above, we present
BrickPal, an augmented reality-based system, which visualizes assembly
instructions in an augmented reality head-mounted display. It utilizes Natural
Language Processing (NLP) techniques to generate plausible assembly sequences,
and provide real-time guidance in the AR headset.Our user study demonstrates
BrickPal's effectiveness at assisting users in brick assembly compared to
traditional assembly methods. Additionally, the NLP algorithm-generated
assembly sequences achieve the same usability with manually adapted sequences.","['Yao Shi', 'Xiaofeng Zhang', 'Ran zhang', 'Zhou Yang', 'Xiao Tang', 'Hongni Ye', 'Yi Wu']",2023-07-06T17:42:56Z,http://arxiv.org/abs/2307.03162v1
"Improving Human Legibility in Collaborative Robot Tasks through
  Augmented Reality and Workspace Preparation","Understanding the intentions of human teammates is critical for safe and
effective human-robot interaction. The canonical approach for human-aware robot
motion planning is to first predict the human's goal or path, and then
construct a robot plan that avoids collision with the human. This method can
generate unsafe interactions if the human model and subsequent predictions are
inaccurate. In this work, we present an algorithmic approach for both arranging
the configuration of objects in a shared human-robot workspace, and projecting
``virtual obstacles'' in augmented reality, optimizing for legibility in a
given task. These changes to the workspace result in more legible human
behavior, improving robot predictions of human goals, thereby improving task
fluency and safety. To evaluate our approach, we propose two user studies
involving a collaborative tabletop task with a manipulator robot, and a
warehouse navigation task with a mobile robot.","['Yi-Shiuan Tung', 'Matthew B. Luebbers', 'Alessandro Roncone', 'Bradley Hayes']",2023-11-09T18:18:28Z,http://arxiv.org/abs/2311.05562v1
"Augmented Reality Technology in Teaching about Physics: A systematic
  review of opportunities and challenges","The use of augmented reality (AR) allows for the integration of digital
information onto our perception of the physical world. In this article, we
present a comprehensive review of previously published literature on the
implementation of augmented reality in physics education, at the school and the
university level. Our review includes an analysis of 96 papers from the Scopus
and Eric databases, all of which were published between January 1st, 2012 and
January 1st, 2023. We evaluated how AR has been used for facilitating learning
about physics. Potential AR-based learning activities for different physics
topics have been summarized and opportunities, as well as challenges associated
with AR-based learning of physics have been reported. It has been shown that AR
technologies may facilitate physics learning by: providing complementary
visualizations, optimizing cognitive load, allowing for haptic learning,
reducing task completion time and promoting collaborative inquiry. The
potential disadvantages of using AR in physics teaching are mainly related to
the shortcomings of software and hardware technologies (e.g., camera freeze,
visualization delay) and extraneous cognitive load (e.g., paying more attention
to secondary details than to constructing target knowledge).","['A. Vidak', 'I. Movre Šapić', 'V. Mešić', 'V. Gomzi']",2023-11-30T09:35:36Z,http://arxiv.org/abs/2311.18392v1
"I Did Not Notice: A Comparison of Immersive Analytics with Augmented and
  Virtual Reality","Immersive environments enable users to engage in embodied interaction,
enhancing the sensemaking processes involved in completing tasks such as
immersive analytics. Previous comparative studies on immersive analytics using
augmented and virtual realities have revealed that users employ different
strategies for data interpretation and text-based analytics depending on the
environment. Our study seeks to investigate how augmented and virtual reality
influences sensemaking processes in quantitative immersive analytics. Our
results, derived from a diverse group of participants, indicate that users
demonstrate comparable performance in both environments. However, it was
observed that users exhibit a higher tolerance for cognitive load in VR and
travel further in AR. Based on our findings, we recommend providing users with
the option to switch between AR and VR, thereby enabling them to select an
environment that aligns with their preferences and task requirements.","['Xiaoyan Zhou', 'Anil Ufuk Batmaz', 'Adam S. Williams', 'Dylan Schreiber', 'Francisco Ortega']",2024-04-04T21:39:49Z,http://arxiv.org/abs/2404.03814v1
"Deep Learning-based Point Cloud Registration for Augmented
  Reality-guided Surgery","Point cloud registration aligns 3D point clouds using spatial
transformations. It is an important task in computer vision, with applications
in areas such as augmented reality (AR) and medical imaging. This work explores
the intersection of two research trends: the integration of AR into
image-guided surgery and the use of deep learning for point cloud registration.
The main objective is to evaluate the feasibility of applying deep
learning-based point cloud registration methods for image-to-patient
registration in augmented reality-guided surgery. We created a dataset of point
clouds from medical imaging and corresponding point clouds captured with a
popular AR device, the HoloLens 2. We evaluate three well-established deep
learning models in registering these data pairs. While we find that some deep
learning methods show promise, we show that a conventional registration
pipeline still outperforms them on our challenging dataset.","['Maximilian Weber', 'Daniel Wild', 'Jens Kleesiek', 'Jan Egger', 'Christina Gsaxner']",2024-05-06T09:41:31Z,http://arxiv.org/abs/2405.03314v1
"3DMAP-VR, a project to visualize 3-dimensional models of astrophysical
  phenomena in virtual reality","In this research note, we present 3DMAP-VR,(3-Dimensional Modeling of
Astrophysical Phenomena in Virtual Reality), a project aimed at visualizing 3D
MHD models of astrophysical simulations, using virtual reality sets of
equipment. The models account for all the relevant physical processes in
astrophysical phenomena: gravity, magnetic-field-oriented thermal conduction,
energy losses due to radiation, gas viscosity, deviations from proton-electron
temperature equilibration, deviations from the ionization equilibrium, cosmic
rays acceleration, etc.. We realized an excellent synergy between our 3DMAP-VR
project and Sketchfab (one of the largest open access platforms to publish and
share 3D virtual reality and augmented reality content) to promote a wide
dissemination of results for both scientific and public outreach purposes.","['Salvatore Orlando', 'Ignazio Pillitteri', 'Fabrizio Bocchino', 'Laura Daricello', 'Laura Leonardi']",2019-12-01T07:38:11Z,http://arxiv.org/abs/1912.02649v1
"An Explore of Virtual Reality for Awareness of the Climate Change
  Crisis: A Simulation of Sea Level Rise","Virtual Reality (VR) technology has been shown to achieve remarkable results
in multiple fields. Due to the nature of the immersive medium of Virtual
Reality it logically follows that it can be used as a high-quality educational
tool as it offers potentially a higher bandwidth than other mediums such as
text, pictures and videos. This short paper illustrates the development of a
climate change educational awareness application for virtual reality to
simulate virtual scenes of local scenery and sea level rising until 2100 using
prediction data. The paper also reports on the current in progress work of
porting the system to Augmented Reality (AR) and future work to evaluate the
system.","['Zixiang Xu', 'Abraham G. Campbell', 'Soumyabrata Dev', 'Yuan Liang']",2022-05-03T16:02:31Z,http://arxiv.org/abs/2205.01583v1
The Value Chain of Education Metaverse,"Since the end of 2021, the Metaverse has been booming. Many unknown
possibilities are gradually being realized, but many people only determined
that they use Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality
(MR) in the Metaverse. It is even considered that as long as the above
realities (VR, AR, MR) are used, it is equal to the Metaverse. However, this is
not true, for Reality-based display tools are only one of the presentation
methods of the Metaverse. If we cannot return to the three main characteristics
of the Metaverse: ""digital avatars,"" a decentralized ""consensus value system,""
and ""Immersive experience,"" the practice and imagination of the Metaverse will
become very narrow. Since 2022, the concept of Metaverse has also been widely
used in classroom teaching to integrate into teaching activities. Therefore, to
prevent teachers and students from understanding the Metaverse not only in the
""Using VR, AR, MR is equivalent to Metaverse"" but also pay more attention to
the other two characteristics of the Metaverse: ""digital avatars"" and a
decentralized ""consensus value system.""",['Yun-Cheng Tsai'],2022-11-07T15:28:06Z,http://arxiv.org/abs/2211.05833v2
Traffic Characteristics of Extended Reality,"This tutorial paper analyzes the traffic characteristics of immersive
experiences with extended reality (XR) technologies, including Augmented
reality (AR), virtual reality (VR), and mixed reality (MR). The current trend
in XR applications is to offload the computation and rendering to an external
server and use wireless communications between the XR head-mounted display
(HMD) and the access points. This paradigm becomes essential owing to (1) its
high flexibility (in terms of user mobility) compared to remote rendering
through a wired connection, and (2) the high computing power available on the
server compared to local rendering (on HMD). The requirements to facilitate a
pleasant XR experience are analyzed in three aspects: capacity (throughput),
latency, and reliability. For capacity, two VR experiences are analyzed: a
human eye-like experience and an experience with the Oculus Quest 2 HMD. For
latency, the key components of the motion-to-photon (MTP) delay are discussed.
For reliability, the maximum packet loss rate (or the minimum packet delivery
rate) is studied for different XR scenarios. Specifically, the paper reviews
optimization techniques that were proposed to reduce the latency, conserve the
bandwidth, extend the scalability, and/or increase the reliability to satisfy
the stringent requirements of the emerging XR applications.","['Abdullah Alnajim', 'Seyedmohammad Salehi', 'Chien-Chung Shen', 'Malcolm Smith']",2023-04-16T22:17:29Z,http://arxiv.org/abs/2304.07908v1
"Evolution of 3GPP Standards Towards True Extended Reality (XR) Support
  in 6G Networks","Extended reality (XR) is a key innovation of 5G-advanced and beyond networks.
The diverse XR use-cases, including virtual reality, augmented reality, and
mixed reality, transform the way humans interact with surrounding environments.
Thus, XR technology enables true immersive experiences of novel services
spanning, e.g., e-commerce, healthcare, and education, respectively. However,
the efficient support of XR services over existing and future cellular systems
is highly challenging and requires multiple radio design improvements, due to
the unique XR traffic and performance characteristics. Thus, this article
surveys the state-of-art 3GPP standardization activities (release-18) for
integrating the XR service class into the 5G-advanced specifications,
highlighting the major XR performance challenges. Furthermore, the paper
introduces valuable insights and research directions for supporting true XR
services over the next-generation 6G networks, where multiple novel radio
design mindsets and protocol enhancements are proposed and evaluated using
extensive system level simulations, including solutions for application-native
dynamic performance reporting, traffic-dependent control channel design,
collaborative device aggregation for XR capacity boosting and offload,
respectively.","['Ali A. Esswie', 'Morris Repeta']",2023-06-06T20:57:35Z,http://arxiv.org/abs/2306.04012v1
Unveiling the Era of Spatial Computing,"The evolution of User Interfaces marks a significant transition from
traditional command-line interfaces to more intuitive graphical and touch-based
interfaces, largely driven by the emergence of personal computing devices. The
advent of spatial computing and Extended Reality technologies further pushes
the boundaries, promising a fusion of physical and digital realms through
interactive environments. This paper delves into the progression from All
Realities technologies encompassing Augmented Reality, Virtual Reality, and
Mediated Reality to spatial computing, highlighting their conceptual
differences and applications. We explore enabling technologies such as
Artificial Intelligence, the Internet of Things, 5G, cloud and edge computing,
and blockchain that underpin the development of spatial computing. We further
scrutinize the initial forays into commercial spatial computing devices, with a
focus on Apple's Vision Pro, evaluating its technological advancements
alongside the challenges it faces. Through this examination, we aim to provide
insights into the potential of spatial computing to revolutionize our
interaction with digital information and the physical world.",['Hanzhong Cao'],2024-05-11T03:44:45Z,http://arxiv.org/abs/2405.06895v1
On the Emergence of Symmetrical Reality,"Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.","['Zhenliang Zhang', 'Zeyu Zhang', 'Ziyuan Jiao', 'Yao Su', 'Hangxin Liu', 'Wei Wang', 'Song-Chun Zhu']",2024-01-26T16:09:39Z,http://arxiv.org/abs/2401.15132v1
"That Doesn't Go There: Attacks on Shared State in Multi-User Augmented
  Reality Applications","Augmented Reality (AR) is expected to become a pervasive component in
enabling shared virtual experiences. In order to facilitate collaboration among
multiple users, it is crucial for multi-user AR applications to establish a
consensus on the ""shared state"" of the virtual world and its augmentations,
through which they interact within augmented reality spaces. Current methods to
create and access shared state collect sensor data from devices (e.g., camera
images), process them, and integrate them into the shared state. However, this
process introduces new vulnerabilities and opportunities for attacks.
Maliciously writing false data to ""poison"" the shared state is a major concern
for the security of the downstream victims that depend on it. Another type of
vulnerability arises when reading the shared state; by providing false inputs,
an attacker can view hologram augmentations at locations they are not allowed
to access. In this work, we demonstrate a series of novel attacks on multiple
AR frameworks with shared states, focusing on three publicly-accessible
frameworks. We show that these frameworks, while using different underlying
implementations, scopes, and mechanisms to read from and write to the shared
state, have shared vulnerability to a unified threat model. Our evaluation of
these state-of-art AR applications demonstrates reliable attacks both on
updating and accessing shared state across the different systems. To defend
against such threats, we discuss a number of potential mitigation strategies
that can help enhance the security of multi-user AR applications.","['Carter Slocum', 'Yicheng Zhang', 'Erfan Shayegani', 'Pedram Zaree', 'Nael Abu-Ghazaleh', 'Jiasi Chen']",2023-08-17T18:33:23Z,http://arxiv.org/abs/2308.09146v2
Virtual Augmented Reality for Atari Reinforcement Learning,"Reinforcement Learning (RL) has achieved significant milestones in the gaming
domain, most notably Google DeepMind's AlphaGo defeating human Go champion Ken
Jie. This victory was also made possible through the Atari Learning Environment
(ALE): The ALE has been foundational in RL research, facilitating significant
RL algorithm developments such as AlphaGo and others. In current Atari video
game RL research, RL agents' perceptions of its environment is based on raw
pixel data from the Atari video game screen with minimal image preprocessing.
Contrarily, cutting-edge ML research, external to the Atari video game RL
research domain, is focusing on enhancing image perception. A notable example
is Meta Research's ""Segment Anything Model"" (SAM), a foundation model capable
of segmenting images without prior training (zero-shot). This paper addresses a
novel methodical question: Can state-of-the-art image segmentation models such
as SAM improve the performance of RL agents playing Atari video games? The
results suggest that SAM can serve as a ""virtual augmented reality"" for the RL
agent, boosting its Atari video game playing performance under certain
conditions. Comparing RL agent performance results from raw and augmented pixel
inputs provides insight into these conditions. Although this paper was limited
by computational constraints, the findings show improved RL agent performance
for augmented pixel inputs and can inform broader research agendas in the
domain of ""virtual augmented reality for video game playing RL agents"".",['Christian A. Schiller'],2023-10-12T19:42:42Z,http://arxiv.org/abs/2310.08683v1
Human movement augmentation and how to make it a reality,"Augmenting the body with artificial limbs controlled concurrently to the
natural limbs has long appeared in science fiction, but recent technological
and neuroscientific advances have begun to make this vision possible. By
allowing individuals to achieve otherwise impossible actions, this movement
augmentation could revolutionize medical and industrial applications and
profoundly change the way humans interact with their environment. Here, we
construct a movement augmentation taxonomy through what is augmented and how it
is achieved. With this framework, we analyze augmentation that extends the
number of degrees-of-freedom, discuss critical features of effective
augmentation such as physiological control signals, sensory feedback and
learning, and propose a vision for the field.","['Jonathan Eden', 'Mario Bräcklein', 'Jaime Ibáñez Pereda', 'Deren Yusuf Barsakcioglu', 'Giovanni Di Pino', 'Dario Farina', 'Etienne Burdet', 'Carsten Mehring']",2021-06-15T13:36:27Z,http://arxiv.org/abs/2106.08129v1
Augmenting Static Visualizations with PapARVis Designer,"This paper presents an authoring environment for augmenting static
visualizations with virtual content in augmented reality. Augmenting static
visualizations can leverage the best of both physical and digital worlds, but
its creation currently involves different tools and devices, without any means
to explicitly design and debug both static and virtual content simultaneously.
To address these issues, we design an environment that seamlessly integrates
all steps of a design and deployment workflow through its main features: i) an
extension to Vega, ii) a preview, and iii) debug hints that facilitate valid
combinations of static and augmented content. We inform our design through a
design space with four ways to augment static visualizations. We demonstrate
the expressiveness of our tool through examples, including books, posters,
projections, wall-sized visualizations. A user study shows high user
satisfaction of our environment and confirms that participants can create
augmented visualizations in an average of 4.63 minutes.","['Chen Zhu-Tian', 'Wai Tong', 'Qianwen Wang', 'Benjamin Bach', 'Huamin Qu']",2023-10-07T14:33:33Z,http://arxiv.org/abs/2310.04826v2
"Augmenting the thermal flux experiment: a mixed reality approach with
  the HoloLens","In the field of Virtual Reality (VR) and Augmented Reality (AR) technologies
have made huge progress during the last years and also reached the field of
education. The virtuality continuum, ranging from pure virtuality on one side
to the real world on the other has been successfully covered by the use of
immersive technologies like head-mounted displays, which allow to embed virtual
objects into the real surroundings, leading to a Mixed Reality (MR) experience.
In such an environment digital and real objects do not only co-exist, but
moreover are also able to interact with each other in real-time. These concepts
can be used to merge human perception of reality with digitally visualized
sensor data and thereby making the invisible visible. As a first example, in
this paper we introduce alongside the basic idea of this column an
MR-experiment in thermodynamics for a laboratory course for freshman students
in physics or other science and engineering subjects which uses physical data
from mobile devices for analyzing and displaying physical phenomena to
students.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-09-05T11:52:18Z,http://arxiv.org/abs/1709.01342v1
"Effects of Head-locked Augmented Reality on User's performance and
  perceived workload","An augmented reality (AR) environment includes a set of digital elements with
which the users interact while performing certain tasks. Recent AR head-mounted
displays allow users to select how these elements are presented. However, few
studies have been conducted to examine the effect of the way of presenting
augmented content on user performance and workload. This study aims to evaluate
two methods of presenting augmented content - world-locked and head-locked
modes in a data entry task. A total of eighteen participants performed the data
entry task in this study. The effectiveness of each mode is evaluated in terms
of task performance, muscle activity, perceived workload, and usability. The
results show that the task completion time is shorter and the typing speed is
significantly faster in the head-locked mode while the world-locked mode
achieved higher scores in terms of preference. The findings of this study can
be applied to AR user interfaces to improve content presentation and enhance
the user experience.","['Yalda Ghasemi', 'Ankit Singh', 'Myunghee Kim', 'Andrew Johnson', 'Heejin Jeong']",2021-06-26T17:35:39Z,http://arxiv.org/abs/2106.14068v1
PokAR: Facilitating Poker Play Through Augmented Reality,"We introduce PokAR, an augmented reality (AR) application to facilitate poker
play. PokAR aims to alleviate three difficulties of traditional poker by
leveraging AR technology: (1) need to have physical poker chips, (2) complex
rules of poker, (3) slow game pace caused by laborious tasks. Despite the
potential benefits of AR in poker, not much research has been done in the
field. In fact, PokAR is the first application to enable AR poker on a mobile
device without requiring extra costly equipment. This has been done by creating
a Snapchat Lens which can be used on most mobile devices. We evaluated this
application by instructing 4 participant dyads to use PokAR to engage in poker
play and respond to survey questions about their experience. We found that most
PokAR features were positively received, AR did not significantly improve nor
hinder socialization, PokAR slightly increased the game pace, and participants
had an overall enjoyable experience with the Lens. These findings led to three
major conclusions: (1) AR has the potential to augment and simplify traditional
table games, (2) AR should not be used to replace traditional experiences, only
augment them, (3) Future work includes additional features like increased
tactility and statistical annotations.","['Adam Gamba', 'Andrés Monroy-Hernández']",2023-01-02T02:32:26Z,http://arxiv.org/abs/2301.00505v1
"Augmented Reality Applied to LEGO Construction: AR-based Building
  Instructions with High Accuracy & Precision and Realistic Object-Hand
  Occlusions","BRICKxAR is a novel Augmented Reality (AR) instruction method for
construction toys such as LEGO. With BRICKxAR, physical LEGO construction is
guided by virtual bricks. Compared with the state-of-the-art, accuracy of the
virtual - physical model alignment is significantly improved through a new
design of marker-based registration, which can achieve an average error less
than 1mm throughout the model. Realistic object occlusion is accomplished to
reveal the true spatial relationship between physical and virtual bricks. LEGO
players' hand detection and occlusion are realized to visualize the correct
spatial relationship between real hands and virtual bricks, and allow virtual
bricks to be ""grasped"" by real hands. The integration of these features makes
AR instructions possible for small-parts assembly, validated through a working
AR prototype for constructing LEGO Arc de Triomphe, quantitative measures of
the accuracies of registration and occlusions, and heuristic evaluation of AR
instruction features.",['Wei Yan'],2019-07-29T17:44:14Z,http://arxiv.org/abs/1907.12549v4
Online and Offline Robot Programming via Augmented Reality Workspaces,"Robot programming methods for industrial robots are time consuming and often
require operators to have knowledge in robotics and programming. To reduce
costs associated with reprogramming, various interfaces using augmented reality
have recently been proposed to provide users with more intuitive means of
controlling robots in real-time and programming them without having to code.
However, most solutions require the operator to be close to the real robot's
workspace which implies either removing it from the production line or shutting
down the whole production line due to safety hazards. We propose a novel
augmented reality interface providing the users with the ability to model a
virtual representation of a workspace which can be saved and reused to program
new tasks or adapt old ones without having to be co-located with the real
robot. Similar to previous interfaces, the operators then have the ability to
program robot tasks or control the robot in real-time by manipulating a virtual
robot. We evaluate the intuitiveness and usability of the proposed interface
with a user study where 18 participants programmed a robot manipulator for a
disassembly task.","['Yong Joon Thoo', 'Jérémy Maceiras', 'Philip Abbet', 'Mattia Racca', 'Hakan Girgin', 'Sylvain Calinon']",2021-07-05T09:15:13Z,http://arxiv.org/abs/2107.01884v2
Introspectibles: Tangible Interaction to Foster Introspection,"Digital devices are now ubiquitous and have the potential to be used to
support positive changes in human lives and promote psychological well-being.
This paper presents three interactive systems that we created focusing on
introspection activities, leveraging tangible interaction and spatial augmented
reality. More specifically, we describe anthropomorphic augmented avatars that
display the users' inner states using physiological sensors. We also present a
first prototype of an augmented sandbox specifically dedicated to promoting
mindfulness activities.","['Renaud Gervais', 'Joan Sol Roo', 'Jérémy Frey', 'Martin Hachet']",2016-03-15T07:36:10Z,http://arxiv.org/abs/1603.04581v1
"Deep Neural Network and Data Augmentation Methodology for off-axis iris
  segmentation in wearable headsets","A data augmentation methodology is presented and applied to generate a large
dataset of off-axis iris regions and train a low-complexity deep neural
network. Although of low complexity the resulting network achieves a high level
of accuracy in iris region segmentation for challenging off-axis eye-patches.
Interestingly, this network is also shown to achieve high levels of performance
for regular, frontal, segmentation of iris regions, comparing favorably with
state-of-the-art techniques of significantly higher complexity. Due to its
lower complexity, this network is well suited for deployment in embedded
applications such as augmented and mixed reality headsets.","['Viktor Varkarakis', 'Shabab Bazrafkan', 'Peter Corcoran']",2019-03-01T16:17:00Z,http://arxiv.org/abs/1903.00389v1
"Hologram: Realtime Holographic Overlays via LiDAR Augmented
  Reconstruction","Guided by the hologram technology of the infamous Star Wars franchise, I
present an application that creates real-time holographic overlays using LiDAR
augmented 3D reconstruction. Prior attempts involve SLAM or NeRFs which either
require highly calibrated scenes, incur steep computation costs, or fail to
render dynamic scenes. I propose 3 high-fidelity reconstruction tools that can
run on a portable device, such as a iPhone 14 Pro, which can allow for metric
accurate facial reconstructions. My systems enable interactive and immersive
holographic experiences that can be used for a wide range of applications,
including augmented reality, telepresence, and entertainment.",['Ekansh Agrawal'],2024-05-12T06:35:10Z,http://arxiv.org/abs/2405.07178v1
Impact of XR on Mental Health: Are we Playing with Fire?,"Extended reality (XR) technology has the incredible potential to
revolutionize mental health treatment and support, bringing a whole new
dimension to the field. Through the use of immersive virtual and augmented
reality experiences, individuals can enter entirely new worlds and realities
that provide a safe and controlled space for therapy and self-exploration.
Whether it's stepping into a calming natural environment, practicing social
interactions or confronting past traumas in a controlled environment, extended
reality offers endless possibilities. Engaging these virtual realities,
individuals can gain a deeper understanding of themselves and their emotions,
learn coping strategies, and practice important life skills in a way that is
both engaging and effective. The wonders of extended reality for mental health
are truly awe-inspiring and offer a powerful tool for improving the well-being
of individuals around the world. However, we should remember, everything has
its disadvantages, and XR is no different. While XR is a revolution, the human
brain is very complex, fragile and unique (like with fingerprints, no two
people have the same brain anatomy), leading to varying conditions, results,
experiences and consequences. This article presents insights and information on
how immersive interactive digital experiences can shape our minds and
behaviors. Research to date suggests that XR experiences can change regions of
the brain responsible for attention and visuospatial skills.",['Benjamin Kenwright'],2023-04-04T09:06:18Z,http://arxiv.org/abs/2304.01648v1
Augmented Reality for Depth Cues in Monocular Minimally Invasive Surgery,"One of the major challenges in Minimally Invasive Surgery (MIS) such as
laparoscopy is the lack of depth perception. In recent years, laparoscopic
scene tracking and surface reconstruction has been a focus of investigation to
provide rich additional information to aid the surgical process and compensate
for the depth perception issue. However, robust 3D surface reconstruction and
augmented reality with depth perception on the reconstructed scene are yet to
be reported. This paper presents our work in this area. First, we adopt a
state-of-the-art visual simultaneous localization and mapping (SLAM) framework
- ORB-SLAM - and extend the algorithm for use in MIS scenes for reliable
endoscopic camera tracking and salient point mapping. We then develop a robust
global 3D surface reconstruction frame- work based on the sparse point clouds
extracted from the SLAM framework. Our approach is to combine an outlier
removal filter within a Moving Least Squares smoothing algorithm and then
employ Poisson surface reconstruction to obtain smooth surfaces from the
unstructured sparse point cloud. Our proposed method has been quantitatively
evaluated compared with ground-truth camera trajectories and the organ model
surface we used to render the synthetic simulation videos. In vivo laparoscopic
videos used in the tests have demonstrated the robustness and accuracy of our
proposed framework on both camera tracking and surface reconstruction,
illustrating the potential of our algorithm for depth augmentation and
depth-corrected augmented reality in MIS with monocular endoscopes.","['Long Chen', 'Wen Tang', 'Nigel W. John', 'Tao Ruan Wan', 'Jian Jun Zhang']",2017-03-01T18:01:52Z,http://arxiv.org/abs/1703.01243v1
Immersive Augmented Reality Training for Complex Manufacturing Scenarios,"In the complex manufacturing sector a considerable amount of resources are
focused on developing new skills and training workers. In that context,
increasing the effectiveness of those processes and reducing the investment
required is an outstanding issue. In this paper we present an experiment that
shows how modern Human Computer Interaction (HCI) metaphors such as
collaborative mixed-reality can be used to transmit procedural knowledge and
could eventually replace other forms of face-to-face training. We implement a
real-time Immersive Augmented Reality (IAR) setup with see-through cameras that
allows for collaborative interactions that can simulate conventional forms of
training. The obtained results indicate that people who took the IAR training
achieved the same performance than people in the conventional face-to-face
training condition. These results, their implications for future training and
the use of HCI paradigms in this context are discussed in this paper.","['Mar Gonzalez-Franco', 'Julio Cermeron', 'Katie Li', 'Rodrigo Pizarro', 'Jacob Thorn', 'Windo Hutabarat', 'Ashutosh Tiwari', 'Pablo Bermell-Garcia']",2016-02-05T07:50:25Z,http://arxiv.org/abs/1602.01944v2
"Unleashing the Potentials of Immersive Augmented Reality for Software
  Engineering","In immersive augmented reality (IAR), users can wear a head-mounted display
to see computer-generated images superimposed to their view of the world. IAR
was shown to be beneficial across several domains, e.g., automotive, medicine,
gaming and engineering, with positive impacts on, e.g., collaboration and
communication. We think that IAR bears a great potential for software
engineering but, as of yet, this research area has been neglected. In this
vision paper, we elicit potentials and obstacles for the use of IAR in software
engineering. We identify possible areas that can be supported with IAR
technology by relating commonly discussed IAR improvements to typical software
engineering tasks. We further demonstrate how innovative use of IAR technology
may fundamentally improve typical activities of a software engineer through a
comprehensive series of usage scenarios outlining practical application.
Finally, we reflect on current limitations of IAR technology based on our
scenarios and sketch research activities necessary to make our vision a
reality. We consider this paper to be relevant to academia and industry alike
in guiding the steps to innovative research and applications for IAR in
software engineering.","['Leonel Merino', 'Mircea Lungu', 'Christoph Seidl']",2020-01-05T12:22:28Z,http://arxiv.org/abs/2001.01223v1
A Review of Augmented Reality Applications for Building Evacuation,"Evacuation is one of the main disaster management solutions to reduce the
impact of man-made and natural threats on building occupants. To date, several
modern technologies and gamification concepts, e.g. immersive virtual reality
and serious games, have been used to enhance building evacuation preparedness
and effectiveness. Those tools have been used both to investigate human
behavior during building emergencies and to train building occupants on how to
cope with building evacuations.
  Augmented Reality (AR) is novel technology that can enhance this process
providing building occupants with virtual contents to improve their evacuation
performance. This work aims at reviewing existing AR applications developed for
building evacuation. This review identifies the disasters and types of building
those tools have been applied for. Moreover, the application goals, hardware
and evacuation stages affected by AR are also investigated in the review.
Finally, this review aims at identifying the challenges to face for further
development of AR evacuation tools.",['Lovreglio Ruggiero'],2018-04-10T07:50:14Z,http://arxiv.org/abs/1804.04186v1
"Concepts for End-to-end Augmented Reality based Human-Robot Interaction
  Systems","The field of Augmented Reality (AR) based Human Robot Interaction (HRI) has
progressed significantly since its inception more than two decades ago. With
more advanced devices, particularly head-mounted displays (HMD), freely
available programming environments and better connectivity, the possible
application space expanded significantly. Here we present concepts and systems
currently being developed at our lab to enable a truly end-to-end application
of AR in HRI, from setting up the working environment of the robot, through
programming and finally interaction with the programmed robot. Relevant papers
by other authors will also be overviewed. We demonstrate the use of such
technologies with systems not inherently designed to be collaborative, namely
industrial manipulators. By trying to make such industrial systems
easily-installable, collaborative and interactive, the vision of universal
robot co-workers can be pushed one step closer to reality. The main goal of the
paper is to provide a short overview of the capabilities of HMD-based HRI to
researchers unfamiliar with the concepts. For researchers already using such
techniques, the hope is to perhaps introduce some new ideas and to broaden the
field of research.","['David Puljiz', 'Björn Hein']",2019-10-10T11:47:10Z,http://arxiv.org/abs/1910.04494v1
"A Survey Study to Understand Industry Vision for Virtual and Augmented
  Reality Applications in Design and Construction","With advances in Building Information Modeling (BIM), Virtual Reality (VR)
and Augmented Reality (AR) technologies have many potential applications in the
Architecture, Engineering, and Construction (AEC) industry. However, the AEC
industry, relative to other industries, has been slow in adopting AR/VR
technologies, partly due to lack of feasibility studies examining the actual
cost of implementation versus an increase in profit. The main objectives of
this paper are to understand the industry trends in adopting AR/VR technologies
and identifying gaps between AEC research and industry practices. The
identified gaps can lead to opportunities for developing new tools and finding
new use cases. To achieve these goals, two rounds of a survey at two different
time periods (a year apart) were conducted. Responses from 158 industry experts
and researchers were analyzed to assess the current state, growth, and saving
opportunities for AR/VR technologies for the AEC industry. The authors used
t-test for hypothesis testing. The findings show a significant increase in
AR/VR utilization in the AEC industry over the past year from 2017 to 2018. The
industry experts also anticipate strong growth in the use of AR/VR technologies
over the next 5 to 10 years.","['Mojtaba Noghabaei', 'Arsalan Heydarian', 'Vahid Balali', 'Kevin Han']",2020-05-06T13:16:05Z,http://arxiv.org/abs/2005.02795v1
"Virtual, Augmented, and Mixed Reality for Human-Robot Interaction: A
  Survey and Virtual Design Element Taxonomy","Virtual, Augmented, and Mixed Reality for Human-Robot Interaction (VAM-HRI)
has been gaining considerable attention in research in recent years. However,
the HRI community lacks a set of shared terminology and framework for
characterizing aspects of mixed reality interfaces, presenting serious problems
for future research. Therefore, it is important to have a common set of terms
and concepts that can be used to precisely describe and organize the diverse
array of work being done within the field. In this paper, we present a novel
taxonomic framework for different types of VAM-HRI interfaces, composed of four
main categories of virtual design elements (VDEs). We present and justify our
taxonomy and explain how its elements have been developed over the last 30
years as well as the current directions VAM-HRI is headed in the coming decade.","['Michael Walker', 'Thao Phung', 'Tathagata Chakraborti', 'Tom Williams', 'Daniel Szafir']",2022-02-23T00:39:44Z,http://arxiv.org/abs/2202.11249v1
"Sharing Construction Safety Inspection Experiences and Site-Specific
  Knowledge through XR-Augmented Visual Assistance","Early identification of on-site hazards is crucial for accident prevention in
the construction industry. Currently, the construction industry relies on
experienced safety advisors (SAs) to identify site hazards and generate
mitigation measures to guide field workers. However, more than half of the site
hazards remain unrecognized due to the lack of field experience or
site-specific knowledge of some SAs. To address these limitations, this study
proposed an Extended Reality (XR)-augmented visual assistance framework,
including Virtual Reality (VR) and Augmented Reality (AR), that enables
capturing and transferring subconscious inspection strategies between workers
or workers/machines for a construction safety inspection. The purpose is to
enhance SA's training and real-time situational awareness for identifying
on-site hazards while reducing their mental workloads.","['Pengkun Liu', 'Jinding Xing', 'Ruoxin Xiong', 'Pingbo Tang']",2022-05-31T14:37:22Z,http://arxiv.org/abs/2205.15833v1
"GazePair: Efficient Pairing of Augmented Reality Devices Using Gaze
  Tracking","As Augmented Reality (AR) devices become more prevalent and commercially
viable, the need for quick, efficient, and secure schemes for pairing these
devices has become more pressing. Current methods to securely exchange
holograms require users to send this information through large data centers,
creating security and privacy concerns. Existing techniques to pair these
devices on a local network and share information fall short in terms of
usability and scalability. These techniques either require hardware not
available on AR devices, intricate physical gestures, removal of the device
from the head, do not scale to multiple pairing partners, or rely on methods
with low entropy to create encryption keys. To that end, we propose a novel
pairing system, called GazePair, that improves on all existing local pairing
techniques by creating an efficient, effective, and intuitive pairing protocol.
GazePair uses eye gaze tracking and a spoken key sequence cue (KSC) to generate
identical, independently generated symmetric encryption keys with 64 bits of
entropy. GazePair also achieves improvements in pairing success rates and times
over current methods. Additionally, we show that GazePair can extend to
multiple users. Finally, we assert that GazePair can be used on any Mixed
Reality (MR) device equipped with eye gaze tracking.","['Matthew Corbett', 'Jiacheng Shang', 'Bo Ji']",2023-03-13T18:32:32Z,http://arxiv.org/abs/2303.07404v1
"Augmented Reality in Service of Human Operations on the Moon: Insights
  from a Virtual Testbed","Future astronauts living and working on the Moon will face extreme
environmental conditions impeding their operational safety and performance.
While it has been suggested that Augmented Reality (AR) Head-Up Displays (HUDs)
could potentially help mitigate some of these adversities, the applicability of
AR in the unique lunar context remains underexplored. To address this
limitation, we have produced an accurate representation of the lunar setting in
virtual reality (VR) which then formed our testbed for the exploration of
prospective operational scenarios with aerospace experts. Herein we present
findings based on qualitative reflections made by the first 6 study
participants. AR was found instrumental in several use cases, including the
support of navigation and risk awareness. Major design challenges were likewise
identified, including the importance of redundancy and contextual
appropriateness. Drawing on these findings, we conclude by outlining directions
for future research aimed at developing AR-based assistive solutions tailored
to the lunar setting.","['Leonie Becker', 'Tommy Nilsson', 'Paul Topf Aguiar de Medeiros', 'Flavie Rometsch']",2023-03-19T15:32:14Z,http://arxiv.org/abs/2303.10686v1
"Performance Evaluation of Transport Protocols and Roadmap to a
  High-Performance Transport Design for Immersive Applications","Immersive technologies such as virtual reality (VR), augmented reality (AR),
and holograms will change users' digital experience. These immersive
technologies have a multitude of applications, including telesurgeries,
teleconferencing, Internet shopping, computer games, etc. Holographic-type
communication (HTC) is a type of augmented reality media that provides an
immersive experience to Internet users. However, HTC has different
characteristics and network requirements, and the existing network architecture
and transport protocols may not be able to cope with the stringent network
requirements of HTC. Therefore, in this paper, we provide an in-depth and
critical study of the transport protocols for HTC. We also discuss the
characteristics and the network requirements for HTC. Based on the performance
evaluation of the existing transport protocols, we propose a roadmap to design
new high-performance transport protocols for immersive applications.","['Inayat Ali', 'Seungwoo Hong', 'Pyung-koo Park', 'Tae Yeon Kim']",2023-06-29T05:31:02Z,http://arxiv.org/abs/2306.16692v2
"HoloBots: Augmenting Holographic Telepresence with Mobile Robots for
  Tangible Remote Collaboration in Mixed Reality","This paper introduces HoloBots, a mixed reality remote collaboration system
that augments holographic telepresence with synchronized mobile robots. Beyond
existing mixed reality telepresence, HoloBots lets remote users not only be
visually and spatially present, but also physically engage with local users and
their environment. HoloBots allows the users to touch, grasp, manipulate, and
interact with the remote physical environment as if they were co-located in the
same shared space. We achieve this by synchronizing holographic user motion
(Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond
the existing physical telepresence, HoloBots contributes to an exploration of
broader design space, such as object actuation, virtual hand physicalization,
world-in-miniature exploration, shared tangible interfaces, embodied guidance,
and haptic communication. We evaluate our system with twelve participants by
comparing it with hologram-only and robot-only conditions. Both quantitative
and qualitative results confirm that our system significantly enhances the
level of co-presence and shared experience, compared to the other conditions.","['Keiichi Ihara', 'Mehrad Faridan', 'Ayumi Ichikawa', 'Ikkaku Kawaguchi', 'Ryo Suzuki']",2023-07-30T03:20:12Z,http://arxiv.org/abs/2307.16114v1
"Augmented Reality User Interface for Command, Control, and Supervision
  of Large Multi-Agent Teams","Multi-agent human-robot teaming allows for the potential to gather
information about various environments more efficiently by exploiting and
combining the strengths of humans and robots. In industries like defense,
search and rescue, first-response, and others alike, heterogeneous human-robot
teams show promise to accelerate data collection and improve team safety by
removing humans from unknown and potentially hazardous situations. This work
builds upon AugRE, an Augmented Reality (AR) based scalable human-robot teaming
framework. It enables users to localize and communicate with 50+ autonomous
agents. Through our efforts, users are able to command, control, and supervise
agents in large teams, both line-of-sight and non-line-of-sight, without the
need to modify the environment prior and without requiring users to use typical
hardware (i.e. joysticks, keyboards, laptops, tablets, etc.) in the field. The
demonstrated work shows early indications that combining these AR-HMD-based
user interaction modalities for command, control, and supervision will help
improve human-robot team collaboration, robustness, and trust.","['Frank Regal', 'Chris Suarez', 'Fabian Parra', 'Mitch Pryor']",2024-01-11T04:57:08Z,http://arxiv.org/abs/2401.05665v1
"Classifying Objects in 3D Point Clouds Using Recurrent Neural Network: A
  GRU LSTM Hybrid Approach","Accurate classification of objects in 3D point clouds is a significant
problem in several applications, such as autonomous navigation and
augmented/virtual reality scenarios, which has become a research hot spot. In
this paper, we presented a deep learning strategy for 3D object classification
in augmented reality. The proposed approach is a combination of the GRU and
LSTM. LSTM networks learn longer dependencies well, but due to the number of
gates, it takes longer to train; on the other hand, GRU networks have a weaker
performance than LSTM, but their training speed is much higher than GRU, which
is The speed is due to its fewer gates. The proposed approach used the
combination of speed and accuracy of these two networks. The proposed approach
achieved an accuracy of 0.99 in the 4,499,0641 points dataset, which includes
eight classes (unlabeled, man-made terrain, natural terrain, high vegetation,
low vegetation, buildings, hardscape, scanning artifacts, cars). Meanwhile, the
traditional machine learning approaches could achieve a maximum accuracy of
0.9489 in the best case. Keywords: Point Cloud Classification, Virtual Reality,
Hybrid Model, GRULSTM, GRU, LSTM","['Ramin Mousa', 'Mitra Khezli', 'Mohamadreza Azadi', 'Vahid Nikoofard', 'Saba Hesaraki']",2024-03-09T16:05:31Z,http://arxiv.org/abs/2403.05950v2
"Designing Wearable Augmented Reality Concepts to Support Scalability in
  Autonomous Vehicle-Pedestrian Interaction","Wearable augmented reality (AR) offers new ways for supporting the
interaction between autonomous vehicles (AVs) and pedestrians due to its
ability to integrate timely and contextually relevant data into the user's
field of view. This article presents novel wearable AR concepts that assist
crossing pedestrians in multi-vehicle scenarios where several AVs frequent the
road from both directions. Three concepts with different communication
approaches for signaling responses from multiple AVs to a crossing request, as
well as a conventional pedestrian push button, were simulated and tested within
a virtual reality environment. The results showed that wearable AR is a
promising way to reduce crossing pedestrians' cognitive load when the design
offers both individual AV responses and a clear signal to cross. The
willingness of pedestrians to adopt a wearable AR solution, however, is subject
to different factors, including costs, data privacy, technical defects,
liability risks, maintenance duties, and form factors. We further found that
all participants favored sending a crossing request to AVs rather than waiting
for the vehicles to detect their intentions-pointing to an important gap and
opportunity in the current AV-pedestrian interaction literature.","['Tram Thi Minh Tran', 'Callum Parker', 'Yiyuan Wang', 'Martin Tomitsch']",2024-03-08T23:47:29Z,http://arxiv.org/abs/2403.07006v1
"Simulating Wearable Urban Augmented Reality Experiences in VR: Lessons
  Learnt from Designing Two Future Urban Interfaces","Augmented reality (AR) has the potential to fundamentally change how people
engage with increasingly interactive urban environments. However, many
challenges exist in designing and evaluating these new urban AR experiences,
such as technical constraints and safety concerns associated with outdoor AR.
We contribute to this domain by assessing the use of virtual reality (VR) for
simulating wearable urban AR experiences, allowing participants to interact
with future AR interfaces in a realistic, safe and controlled setting. This
paper describes two wearable urban AR applications (pedestrian navigation and
autonomous mobility) simulated in VR. Based on a thematic analysis of interview
data collected across the two studies, we found that the VR simulation
successfully elicited feedback on the functional benefits of AR concepts and
the potential impact of urban contextual factors, such as safety concerns,
attentional capacity, and social considerations. At the same time, we
highlighted the limitations of this approach in terms of assessing the AR
interface's visual quality and providing exhaustive contextual information. The
paper concludes with recommendations for simulating wearable urban AR
experiences in VR.","['Tram Thi Minh Tran', 'Callum Parker', 'Marius Hoggenmüller', 'Luke Hespanhol', 'Martin Tomitsch']",2024-03-18T00:05:16Z,http://arxiv.org/abs/2403.11377v1
"Video2MR: Automatically Generating Mixed Reality 3D Instructions by
  Augmenting Extracted Motion from 2D Videos","This paper introduces Video2MR, a mixed reality system that automatically
generates 3D sports and exercise instructions from 2D videos. Mixed reality
instructions have great potential for physical training, but existing works
require substantial time and cost to create these 3D experiences. Video2MR
overcomes this limitation by transforming arbitrary instructional videos
available online into MR 3D avatars with AI-enabled motion capture
(DeepMotion). Then, it automatically enhances the avatar motion through the
following augmentation techniques: 1) contrasting and highlighting differences
between the user and avatar postures, 2) visualizing key trajectories and
movements of specific body parts, 3) manipulation of time and speed using body
motion, and 4) spatially repositioning avatars for different perspectives.
Developed on Hololens 2 and Azure Kinect, we showcase various use cases,
including yoga, dancing, soccer, tennis, and other physical exercises. The
study results confirm that Video2MR provides more engaging and playful learning
experiences, compared to existing 2D video instructions.","['Keiichi Ihara', 'Kyzyl Monteiro', 'Mehrad Faridan', 'Rubaiat Habib Kazi', 'Ryo Suzuki']",2024-05-28T20:19:38Z,http://arxiv.org/abs/2405.18565v1
Contextual Scene Augmentation and Synthesis via GSACNet,"Indoor scene augmentation has become an emerging topic in the field of
computer vision and graphics with applications in augmented and virtual
reality. However, current state-of-the-art systems using deep neural networks
require large datasets for training. In this paper we introduce GSACNet, a
contextual scene augmentation system that can be trained with limited scene
priors. GSACNet utilizes a novel parametric data augmentation method combined
with a Graph Attention and Siamese network architecture followed by an
Autoencoder network to facilitate training with small datasets. We show the
effectiveness of our proposed system by conducting ablation and comparative
studies with alternative systems on the Matterport3D dataset. Our results
indicate that our scene augmentation outperforms prior art in scene synthesis
with limited scene priors available.","['Mohammad Keshavarzi', 'Flaviano Christian Reyes', 'Ritika Shrivastava', 'Oladapo Afolabi', 'Luisa Caldas', 'Allen Y. Yang']",2021-03-29T06:47:01Z,http://arxiv.org/abs/2103.15369v1
Mid-Air Haptic Bio-Holograms in Mixed Reality,"We present a prototype demonstrator that integrates three technologies, mixed
reality head-mounted displays, wearable bio-sensors, and mid-air haptic
projectors to deliver an interactive tactile experience with a bio-hologram.
Users of this prototype are able to see, touch and feel a hologram of a heart
that is beating at the same rhythm as their own. The demo uses an Ultrahaptics
device, a Magic Leap One Mixed Reality headset, and an Apple Watch that
measures the wearer's heart rate, all synchronized and networked together such
that updates from the wristband dynamically change the haptic feedback and the
animation speed of the beating heart thus creating a more personalised
experience.","['Ted Romanus', 'Sam Frish', 'Mykola Maksymenko', 'William Frier', 'Loïc Corenthy', 'Orestis Georgiou']",2020-01-06T09:04:51Z,http://arxiv.org/abs/2001.01441v1
Keep It Real: a Window to Real Reality in Virtual Reality,"This paper proposed a new interaction paradigm in the virtual reality (VR)
environments, which consists of a virtual mirror or window projected onto a
virtual surface, representing the correct perspective geometry of a mirror or
window reflecting the real world. This technique can be applied to various
videos, live streaming apps, augmented and virtual reality settings to provide
an interactive and immersive user experience. To support such a
perspective-accurate representation, we implemented computer vision algorithms
for feature detection and correspondence matching. To constrain the solutions,
we incorporated an automatically tuning scaling factor upon the homography
transform matrix such that each image frame follows a smooth transition with
the user in sight. The system is a real-time rendering framework where users
can engage their real-life presence with the virtual space.",['Baihan Lin'],2020-04-21T21:33:14Z,http://arxiv.org/abs/2004.10313v3
"Real-time Collaboration Between Mixed Reality Users in Geo-referenced
  Virtual Environment","Collaboration using mixed reality technology is an active area of research,
where significant research is done to virtually bridge physical distances.
There exist a diverse set of platforms and devices that can be used for a
mixed-reality collaboration, and is largely focused for indoor scenarios,
where, a stable tracking can be assumed. We focus on supporting collaboration
between VR and AR users, where AR user is mobile outdoors, and VR user is
immersed in true-sized digital twin. This cross-platform solution requires new
user experiences for interaction, accurate modelling of the real-world, and
working with noisy outdoor tracking sensor such as GPS. In this paper, we
present our results and observations of real-time collaboration between
cross-platform users, in the context of a geo-referenced virtual environment.
We propose a solution for using GPS measurement in VSLAM to localize the AR
user in an outdoor environment. The client applications enable VR and AR user
to collaborate across the heterogeneous platforms seamlessly. The user can
place or load dynamic contents tagged to a geolocation and share their
experience with remote users in real-time.","['Shubham Singh', 'Zengou Ma', 'Daniele Giunchi', 'Anthony Steed']",2020-10-02T14:23:39Z,http://arxiv.org/abs/2010.01023v1
Implementing Virtual Reality for Teleoperation of a Humanoid Robot,"Our research explores the potential of a humanoid robot for work in
unpredictable environments, but controlling a humanoid robot remains a very
difficult problem. In our previous work, we designed a prototype virtual
reality (VR) interface to allow an operator to command a humanoid robot.
However, while usable, the initial interface was not sufficient for commanding
the robot to perform the tasks; for example, in some cases, there was a lack of
precision available for robot control. The interface was overly cumbersome in
some areas as well. In this paper, we discuss numerous additions, inspired by
traditional interfaces and virtual reality video games, to our prior
implementation, providing additional ways to visualize and command a humanoid
robot to perform difficult tasks within a virtual world.","['Jordan Allspaw', 'Gregory LeMasurier', 'Holly Yanco']",2021-04-23T21:44:25Z,http://arxiv.org/abs/2104.11826v1
"eXtended Reality for Autism Interventions: The importance of Mediation
  and Sensory-Based Approaches","eXtended Reality (XR) autism research, ranging from Augmented Reality to
Virtual Reality, focuses on socio-emotional abilities and high-functioning
autism. However common autism interventions address the entire spectrum over
social, sensory and mediation issues. To bridge the gap between autism research
and real interventions, we compared existing literature on XR and autism with
stakeholders' needs obtained by interviewing 34 skateholders, mainly
practitioners. It allow us first to suggest XR use cases that could better
support practitioners' interventions, and second to derive design guidelines
accordingly. Findings demonstrate that collaborative XR sensory-based and
mediation approaches would benefit the entire spectrum, and encourage to
consider the overall intervention context when designing XR protocols.","['Valentin Bauer', 'Tifanie Bouchara', 'Patrick Bourdot']",2021-06-30T11:11:23Z,http://arxiv.org/abs/2106.15983v1
HoloLens 2 Technical Evaluation as Mixed Reality Guide,"Mixed Reality (MR) is an evolving technology lying in the continuum spanned
by related technologies such as Virtual Reality (VR) and Augmented Reality
(AR), and creates an exciting way of interacting with people and the
environment. This technology is fast becoming a tool used by many people,
potentially improving living environments and work efficiency. Microsoft
HoloLens has played an important role in the progress of MR, from the first
generation to the second generation. In this paper, we systematically evaluate
the functions of applicable functions in HoloLens 2. These evaluations can
serve as a performance benchmark that can help people who need to use this
instrument for research or applications in the future. The detailed tests and
the performance evaluation of the different functionalities show the usability
and possible limitations of each function. We mainly divide the experiment into
the existing functions of the HoloLens 1, the new functions of the HoloLens 2,
and the use of research mode. This research results will be useful for MR
researchers who want to use HoloLens 2 as a research tool to design their own
MR applications.","['Hung-Jui Guo', 'Balakrishnan Prabhakaran']",2022-07-19T21:19:23Z,http://arxiv.org/abs/2207.09554v1
"Virtual Reality Therapy for the Psychological Well-being of Palliative
  Care Patients in Hong Kong","In this paper we introduce novel Virtual Reality (VR) and Augmented Reality
(AR) treatments to improve the psychological well being of patients in
palliative care, based on interviews with a clinical psychologist who has
successfully implemented VR assisted interventions on palliative care patients
in the Hong Kong hospital system. Our VR and AR assisted interventions are
adaptations of traditional palliative care therapies which simultaneously
facilitate patients communication with family and friends while isolated in
hospital due to physical weakness and COVID-19 related restrictions. The first
system we propose is a networked, metaverse platform for palliative care
patients to create customized virtual environments with therapists, family and
friends which function as immersive and collaborative versions of 'life review'
and 'reminiscence therapy'. The second proposed system will investigate the use
of Mixed Reality telepresence and haptic touch in an AR environment, which will
allow palliative care patients to physically feel friends and family in a
virtual space, adding to the sense of presence and immersion in that
environment.","['Daniel Eckhoff', 'Royce Ng', 'Alvaro Cassinelli']",2022-07-24T14:31:52Z,http://arxiv.org/abs/2207.11754v1
"Investigating Input Modality and Task Geometry on Precision-first 3D
  Drawing in Virtual Reality","Accurately drawing non-planar 3D curves in immersive Virtual Reality (VR) is
indispensable for many precise 3D tasks. However, due to lack of physical
support, limited depth perception, and the non-planar nature of 3D curves, it
is challenging to adjust mid-air strokes to achieve high precision. Instead of
creating new interaction techniques, we investigated how task geometric shapes
and input modalities affect precision-first drawing performance in a
within-subject study (n = 12) focusing on 3D target tracing in commercially
available VR headsets. We found that compared to using bare hands, VR
controllers and pens yield nearly 30% of precision gain, and that the tasks
with large curvature, forward-backward or left-right orientations perform best.
We finally discuss opportunities for designing novel interaction techniques for
precise 3D drawing. We believe that our work will benefit future research
aiming to create usable toolboxes for precise 3D drawing.","['Chen Chen', 'Matin Yarmand', 'Zhuoqun Xu', 'Varun Singh', 'Yang Zhang', 'Nadir Weibel']",2022-10-21T21:56:43Z,http://arxiv.org/abs/2210.12270v1
Extended-XRI Body Interfaces for Hyper-Connected Metaverse Environments,"Hybrid mixed-reality (XR) internet-of-things (IoT) research, here called XRI,
aims at a strong integration between physical and virtual objects,
environments, and agents wherein IoT-enabled edge devices are deployed for
sensing, context understanding, networked communication and control of device
actuators. Likewise, as augmented reality systems provide an immersive overlay
on the environments, and virtual reality provides fully immersive environments,
the merger of these domains leads to immersive smart spaces that are
hyper-connected, adaptive and dynamic components that anchor the metaverse to
real-world constructs. Enabling the human-in-the-loop to remain engaged and
connected across these virtual-physical hybrid environments requires advances
in user interaction that are multi-dimensional. This work investigates the
potential to transition the user interface to the human body as an
extended-reality avatar with hybrid extended-body interfaces that can interact
both with the physical and virtual sides of the metaverse. It contributes: i)
an overview of metaverses, XRI, and avatarization concepts, ii) a taxonomy
landscape for extended XRI body interfaces, iii) an architecture and potential
interactions for XRI body designs, iv) a prototype XRI body implementation
based on the architecture, v) a design-science evaluation, toward enabling
future design research directions.","['Jie Guan', 'Alexis Morris']",2023-06-01T19:11:18Z,http://arxiv.org/abs/2306.01096v1
Poster: Enabling Flexible Edge-assisted XR,"Extended reality (XR) is touted as the next frontier of the digital future.
XR includes all immersive technologies of augmented reality (AR), virtual
reality (VR), and mixed reality (MR). XR applications obtain the real-world
context of the user from an underlying system, and provide rich, immersive, and
interactive virtual experiences based on the user's context in real-time. XR
systems process streams of data from device sensors, and provide
functionalities including perceptions and graphics required by the
applications. These processing steps are computationally intensive, and the
challenge is that they must be performed within the strict latency requirements
of XR. This poses limitations on the possible XR experiences that can be
supported on mobile devices with limited computing resources.
  In this XR context, edge computing is an effective approach to address this
problem for mobile users. The edge is located closer to the end users and
enables processing and storing data near them. In addition, the development of
high bandwidth and low latency network technologies such as 5G facilitates the
application of edge computing for latency-critical use cases [4, 11]. This work
presents an XR system for enabling flexible edge-assisted XR.","['Jin Heo', 'Ketan Bhardwaj', 'Ada Gavrilovska']",2023-09-08T18:34:34Z,http://arxiv.org/abs/2309.04548v1
"Poster: Real-Time Object Substitution for Mobile Diminished Reality with
  Edge Computing","Diminished Reality (DR) is considered as the conceptual counterpart to
Augmented Reality (AR), and has recently gained increasing attention from both
industry and academia. Unlike AR which adds virtual objects to the real world,
DR allows users to remove physical content from the real world. When combined
with object replacement technology, it presents an further exciting avenue for
exploration within the metaverse. Although a few researches have been conducted
on the intersection of object substitution and DR, there is no real-time object
substitution for mobile diminished reality architecture with high quality. In
this paper, we propose an end-to-end architecture to facilitate immersive and
real-time scene construction for mobile devices with edge computing.","['Hongyu Ke', 'Haoxin Wang']",2023-10-23T02:47:25Z,http://arxiv.org/abs/2310.14511v1
"Seamless Virtual Reality with Integrated Synchronizer and Synthesizer
  for Autonomous Driving","Virtual reality (VR) is a promising data engine for autonomous driving (AD).
However, data fidelity in this paradigm is often degraded by VR inconsistency,
for which the existing VR approaches become ineffective, as they ignore the
inter-dependency between low-level VR synchronizer designs (i.e., data
collector) and high-level VR synthesizer designs (i.e., data processor). This
paper presents a seamless virtual reality SVR platform for AD, which mitigates
such inconsistency, enabling VR agents to interact with each other in a shared
symbiotic world. The crux to SVR is an integrated synchronizer and synthesizer
IS2 design, which consists of a drift-aware lidar-inertial synchronizer for VR
colocation and a motion-aware deep visual synthesis network for augmented
reality image generation. We implement SVR on car-like robots in two sandbox
platforms, achieving a cm-level VR colocalization accuracy and 3.2% VR image
deviation, thereby avoiding missed collisions or model clippings. Experiments
show that the proposed SVR reduces the intervention times, missed turns, and
failure rates compared to other benchmarks. The SVR-trained neural network can
handle unseen situations in real-world environments, by leveraging its
knowledge learnt from the VR space.","['He Li', 'Ruihua Han', 'Zirui Zhao', 'Wei Xu', 'Qi Hao', 'Shuai Wang', 'Chengzhong Xu']",2024-03-06T08:37:36Z,http://arxiv.org/abs/2403.03541v1
"Towards Massive Interaction with Generalist Robotics: A Systematic
  Review of XR-enabled Remote Human-Robot Interaction Systems","The rising interest of generalist robots seek to create robots with
versatility to handle multiple tasks in a variety of environments, and human
will interact with such robots through immersive interfaces. In the context of
human-robot interaction (HRI), this survey provides an exhaustive review of the
applications of extended reality (XR) technologies in the field of remote HRI.
We developed a systematic search strategy based on the PRISMA methodology. From
the initial 2,561 articles selected, 100 research papers that met our inclusion
criteria were included. We categorized and summarized the domain in detail,
delving into XR technologies, including augmented reality (AR), virtual reality
(VR), and mixed reality (MR), and their applications in facilitating intuitive
and effective remote control and interaction with robotic systems. The survey
highlights existing articles on the application of XR technologies, user
experience enhancement, and various interaction designs for XR in remote HRI,
providing insights into current trends and future directions. We also
identified potential gaps and opportunities for future research to improve
remote HRI systems through XR technology to guide and inform future XR and
robotics research.","['Xian Wang', 'Luyao Shen', 'Lik-Hang Lee']",2024-03-18T00:22:30Z,http://arxiv.org/abs/2403.11384v3
Authoring and Living Next-Generation Location-Based Experiences,"Authoring location-based experiences involving multiple participants,
collaborating or competing in both indoor and outdoor mixed realities, is
extremely complex and bound to serious technical challenges. In this work, we
present the first results of the MAGELLAN European project and how these
greatly simplify this creative process using novel authoring, augmented reality
(AR) and indoor geolocalisation techniques.","['Olivier Balet', 'Boriana Koleva', 'Jens Grubert', 'Kwang Moo Yi', 'Marco Gunia', 'Angelos Katsis', 'Julien Castet']",2017-09-05T09:04:05Z,http://arxiv.org/abs/1709.01293v1
"Immersion on the Edge: A Cooperative Framework for Mobile Immersive
  Computing","Immersive computing (IC) technologies such as virtual reality and augmented
reality are gaining tremendous popularity. In this poster, we present CoIC, a
Cooperative framework for mobile Immersive Computing. The design of CoIC is
based on a key insight that IC tasks among different applications or users
might be similar or redundant. CoIC enhances the performance of mobile IC
applications by caching and sharing computation-intensive IC results on the
edge. Our preliminary evaluation results on an AR application show that CoIC
can reduce the recognition and rendering latency by up to 52.28% and 75.86%
respectively on current mobile devices.","['Zeqi Lai', 'Yong Cui', 'Ziyi Wang', 'Xiaoyu Hu']",2018-07-12T12:32:59Z,http://arxiv.org/abs/1807.04572v1
Exploiting Social Networks. Technological Trends (Habilitation Thesis),"The habilitation thesis presents two main directions:
  1. Exploiting data from social networks (Twitter, Facebook, Flickr, etc.) -
creating resources for text and image processing (classification, retrieval,
credibility, diversification, etc.);
  2. Creating applications with new technologies : augmented reality
(eLearning, games, smart museums, gastronomy, etc.), virtual reality (eLearning
and games), speech processing with Amazon Alexa (eLearning, entertainment, IoT,
etc.).
  The work was validated with good results in evaluation campaigns like CLEF
(Question Answering, Image CLEF, LifeCLEF, etc.), SemEval (Sentiment and
Emotion in text, Anorexia, etc.).",['Adrian Iftene'],2020-04-30T06:01:51Z,http://arxiv.org/abs/2004.14386v1
"Augmenting reality: On the shared history of perceptual illusion and
  video projection mapping","Perceptual illusions based on the spatial correspondence between objects and
displayed images have been pursued by artists and scientists since the 15th
century, mastering optics to create crucial techniques as the linear
perspective and devices as the Magic Lantern. Contemporary video projection
mapping inherits and further extends this drive to produce perceptual illusions
in space by incorporating the required real time capabilities for dynamically
superposing the imaginary onto physical objects under fluid real world
conditions. A critical milestone has been reached in the creation of the
technical possibilities for all encompassing, untethered synthetic reality
experiences available to the plain senses, where every surface may act as a
screen and the relation to everyday objects is open to alterations.",['Alvaro Pastor'],2020-05-28T22:07:29Z,http://arxiv.org/abs/2005.14317v1
Towards Immersive Virtual Reality Simulations of Bionic Vision,"Bionic vision is a rapidly advancing field aimed at developing visual
neuroprostheses ('bionic eyes') to restore useful vision to people who are
blind. However, a major outstanding challenge is predicting what people 'see'
when they use their devices. The limited field of view of current devices
necessitates head movements to scan the scene, which is difficult to simulate
on a computer screen. In addition, many computational models of bionic vision
lack biological realism. To address these challenges, we propose to embed
biologically realistic models of simulated prosthetic vision (SPV) in immersive
virtual reality (VR) so that sighted subjects can act as 'virtual patients' in
real-world tasks.","['Justin Kasowski', 'Nathan Wu', 'Michael Beyeler']",2021-02-21T20:38:20Z,http://arxiv.org/abs/2102.10678v1
Towards Immersive Humanitarian Visualizations,"This paper introduces immersive humanitarian visualization as a promising
research area in information visualization. Humanitarian visualizations are
data visualizations designed to promote human welfare. This paper explains why
immersive display technologies taken broadly (e.g, virtual reality, augmented
reality, ambient displays and physical representations) open up a range of
opportunities for humanitarian visualization. In particular, immersive displays
offer ways to make remote and hidden human suffering more salient. They also
offer ways to communicate quantitative facts together with qualitative
information and visceral experiences, in order to provide a holistic
understanding of humanitarian issues that could support more informed
humanitarian decisions. But despite some promising preliminary work, immersive
humanitarian visualization has not taken off as a research topic yet. The goal
of this paper is to encourage, motivate, and inspire future research in this
area.",['Pierre Dragicevic'],2022-04-04T08:35:49Z,http://arxiv.org/abs/2204.01313v1
Privacy concerns from variances in spatial navigability in VR,"Current Virtual Reality (VR) input devices make it possible to navigate a
virtual environment and record immersive, personalized data regarding the
user's movement and specific behavioral habits, which brings the question of
the user's privacy concern to the forefront. In this article, the authors
propose to investigate Machine Learning driven learning algorithms that try to
learn with human users co-operatively and can be used to countermand existing
privacy concerns in VR but could also be extended to Augmented Reality (AR)
platforms.","['Aryabrata Basu', 'Mohammad Jahed Murad Sunny', 'Jayasri Sai Nikitha Guthula']",2023-02-06T01:48:59Z,http://arxiv.org/abs/2302.02525v1
"PWR-Align: Leveraging Part-Whole Relationships for Part-wise Rigid Point
  Cloud Registration in Mixed Reality Applications","We present an efficient and robust point cloud registration (PCR) workflow
for part-wise rigid point cloud alignment using the Microsoft HoloLens 2. Point
Cloud Registration (PCR) is an important problem in Augmented and Mixed Reality
use cases, and we present a study for a special class of non-rigid
transformations. Many commonly encountered objects are composed of rigid parts
that move relative to one another about joints resulting in non-rigid
deformation of the whole object such as robots with manipulators, and machines
with hinges. The workflow presented allows us to register the point cloud with
various configurations of the point cloud.","['Manorama Jha', 'Bhaskar Banerjee']",2023-06-11T16:36:31Z,http://arxiv.org/abs/2306.06717v1
"Augmenting Security and Privacy in the Virtual Realm: An Analysis of
  Extended Reality Devices","In this work, we present a device-centric analysis of security and privacy
attacks and defenses on Extended Reality (XR) devices, highlighting the need
for robust and privacy-aware security mechanisms. Based on our analysis, we
present future research directions and propose design considerations to help
ensure the security and privacy of XR devices.","['Derin Cayir', 'Abbas Acar', 'Riccardo Lazzeretti', 'Marco Angelini', 'Mauro Conti', 'Selcuk Uluagac']",2024-02-05T15:45:11Z,http://arxiv.org/abs/2402.03114v1
"Enhancing Autonomous Vehicle Design and Testing: A Comprehensive Review
  of AR and VR Integration","This comprehensive literature review explores the potential of Augmented
Reality and Virtual Reality technologies to enhance the design and testing of
autonomous vehicles. By analyzing existing research, the review aims to
identify how AR and VR can be leveraged to improve various aspects of
autonomous vehicle development, including: creating more realistic and
comprehensive testing environments, facilitating the design of user centered
interfaces, and safely evaluating driver behavior in complex scenarios.
Ultimately, the review highlights AR and VR utilization as a key driver in the
development of adaptable testing environments, fostering more dependable
autonomous vehicle technology, and ultimately propelling significant
advancements within the field.","['Emanuella Ejichukwu', 'Lauren Tong', 'Gadir Hazime', 'Bochen Jia']",2024-04-29T18:04:30Z,http://arxiv.org/abs/2404.19021v1
"Towards Quality of Experience Determination for Video in Augmented
  Binocular Vision Scenarios","With the continuous growth in the consumer markets of mobile smartphones and
increasingly in augmented reality wearable devices, several avenues of research
investigate the relationships between the quality perceived by mobile users and
the delivery mechanisms at play to support a high quality of experience for
mobile users. In this paper, we present the first study that evaluates the
relationships of mobile movie quality and the viewer-perceived quality thereof
in an augmented reality setting with see-through devices. We find that
participants tend to overestimate the video quality and exhibit a significant
variation of accuracy that leans onto the movie content and its dynamics. Our
findings, thus, can broadly impact future media adaptation and delivery
mechanisms for this new display format of mobile multimedia.",['Patrick Seeling'],2014-06-04T00:14:06Z,http://arxiv.org/abs/1406.0912v3
"Using CNNs For Users Segmentation In Video See-Through Augmented
  Virtuality","In this paper, we present preliminary results on the use of deep learning
techniques to integrate the users self-body and other participants into a
head-mounted video see-through augmented virtuality scenario. It has been
previously shown that seeing users bodies in such simulations may improve the
feeling of both self and social presence in the virtual environment, as well as
user performance. We propose to use a convolutional neural network for real
time semantic segmentation of users bodies in the stereoscopic RGB video
streams acquired from the perspective of the user. We describe design issues as
well as implementation details of the system and demonstrate the feasibility of
using such neural networks for merging users bodies in an augmented virtuality
simulation.","['Pierre-Olivier Pigny', 'Lionel Dominjon']",2020-01-02T15:22:36Z,http://arxiv.org/abs/2001.00487v1
Study of Gesture Recognition methods and augmented reality,"With the growing technology, we humans always need something that stands out
from the other thing. Gestures are most desirable source to Communicate with
the Machines. Human Computer Interaction finds its importance when it comes to
working with the Human gestures to control the computer applications. Usually
we control the applications using mouse, keyboard, laser pointers etc. but,
with the recent advent in the technology it has even left behind their usage by
introducing more efficient techniques to control applications. There are many
Gesture Recognition techniques that have been implemented using image
processing in the past.
  However recognizing the gestures in the noisy background has always been a
difficult task to achieve. In the proposed system, we are going to use one such
technique called Augmentation in Image processing to control Media Player. We
will recognize Gestures using which we are going to control the operations on
Media player. Augmentation usually is one step ahead when it comes to virtual
reality. It has no restrictions on the background. Moreover it also does not
rely on certain things like gloves, color pointers etc. for recognizing the
gesture. This system mainly appeals to those users who always looks out for a
better option that makes their interaction with computer more simpler or
easier.","['Sandeep Vasave', 'Amol Plave']",2014-11-19T07:52:04Z,http://arxiv.org/abs/1411.5137v1
ARcall: Real-Time AR Communication using Smartphones and Smartglasses,"Augmented Reality (AR) smartglasses are increasingly regarded as the next
generation personal computing platform. However, there is a lack of
understanding about how to design communication systems using them. We present
ARcall, a novel Augmented Reality-based real-time communication system that
enables an immersive, delightful, and privacy-preserving experience between a
smartphone user and a smartglasses wearer. ARcall allows a remote friend
(Friend) to send and project AR content to a smartglasses wearer (Wearer). The
ARcall system was designed with the practical limits of existing AR glasses in
mind, including shorter battery life and a reduced field of view. We conduct a
qualitative evaluation of the three main components of ARcall: Drop-In,
ARaction, and Micro-Chat. Our results provide novel insights for building
future AR-based communication methods, including, the importance of context
priming, user control over AR content placement, and the feeling of co-presence
while conversing.","['Hemant Bhaskar Surale', 'Yu Jiang Tham', 'Brian A. Smith', 'Rajan Vaish']",2022-03-08T19:32:16Z,http://arxiv.org/abs/2203.04358v1
"Deceiving Audio Design in Augmented Environments : A Systematic Review
  of Audio Effects in Augmented Reality","Recently, a lot of works show promising directions for audio design in
augmented reality (AR). These works are mainly focused on how to improve user
experience and make AR more realistic. But even though these improvements seem
promising, these new possibilities could also be used as an input for
manipulative design. This survey aims to analyze all recent discoveries in
audio development regarding AR and argue what kind of ""manipulative"" effect
this could have on the user. It can be concluded that even though there are
many works explaining the effects of audio design in AR, very few works point
out the risk of harm or manipulation toward the user. Future works could
contain more awareness of this problem or maybe even","['Esmée Henrieke Anne de Haas', 'Lik-Hang Lee']",2022-09-03T08:33:19Z,http://arxiv.org/abs/2209.01367v1
"An Augmented Reality Application and User Study for Understanding and
  Learning Spatial Transformation Matrices","Understanding spatial transformations and their mathematical representations
are essential in computer-aided design, robotics, etc. This research has
developed and tested an Augmented Reality (AR) application (BRICKxAR/T) to
enhance students' learning of spatial transformation matrices. BRICKxAR/T
leverages AR features, including information augmentation, physical-virtual
object interplay, and embodied learning, to create a novel and effective
visualization experience for learning. BRICKxAR T has been evaluated as a
learning intervention using LEGO models as example physical and virtual
manipulatives in a user study to assess students' learning gains. The study
compared AR (N=29) vs. non-AR (N=30) learning workshops with pre- and
post-tests on Purdue Visualization of Rotations Test and math questions.
Students' math scores significantly improved after participating in both
workshops with the AR workshop tending to show greater improvements. The
post-workshop survey showed students were inclined to think BRICKxAR/T an
interesting and useful application, and they spent more time learning in AR
than non-AR.","['Zohreh Shaghaghian', 'Heather Burte', 'Dezhen Song', 'Wei Yan']",2022-11-16T22:39:33Z,http://arxiv.org/abs/2212.00110v1
"Evaluation of AI-Supported Input Methods in Augmented Reality
  Environment","Augmented Reality (AR) solutions are providing tools that could improve
applications in the medical and industrial fields. Augmentation can provide
additional information in training, visualization, and work scenarios, to
increase efficiency, reliability, and safety, while improving communication
with other devices and systems on the network. Unfortunately, tasks in these
fields often require both hands to execute, reducing the variety of input
methods suitable to control AR applications. People with certain physical
disabilities, where they are not able to use their hands, are also negatively
impacted when using these devices. The goal of this work is to provide novel
hand-free interfacing methods, using AR technology, in association with AI
support approaches to produce an improved Human-Computer interaction solution.","['Akos Nagy', 'Thomas Lagkas', 'Panagiotis Sarigiannidis', 'Vasileios Argyriou']",2023-06-29T17:34:42Z,http://arxiv.org/abs/2306.17132v1
"Semi-Supervised Semantic Depth Estimation using Symbiotic Transformer
  and NearFarMix Augmentation","In computer vision, depth estimation is crucial for domains like robotics,
autonomous vehicles, augmented reality, and virtual reality. Integrating
semantics with depth enhances scene understanding through reciprocal
information sharing. However, the scarcity of semantic information in datasets
poses challenges. Existing convolutional approaches with limited local
receptive fields hinder the full utilization of the symbiotic potential between
depth and semantics. This paper introduces a dataset-invariant semi-supervised
strategy to address the scarcity of semantic information. It proposes the Depth
Semantics Symbiosis module, leveraging the Symbiotic Transformer for achieving
comprehensive mutual awareness by information exchange within both local and
global contexts. Additionally, a novel augmentation, NearFarMix is introduced
to combat overfitting and compensate both depth-semantic tasks by strategically
merging regions from two images, generating diverse and structurally consistent
samples with enhanced control. Extensive experiments on NYU-Depth-V2 and KITTI
datasets demonstrate the superiority of our proposed techniques in indoor and
outdoor environments.","['Md Awsafur Rahman', 'Shaikh Anowarul Fattah']",2023-08-28T08:33:45Z,http://arxiv.org/abs/2308.14400v1
"Penn & Slavery Project's Augmented Reality Tour: Augmenting a Campus to
  Reveal a Hidden History","In 2006 and 2016, the University of Pennsylvania denied any ties to slavery.
In 2017, a group of undergraduate researchers, led by Professor Kathleen Brown,
investigated this claim. Initial research, focused on 18th century faculty and
trustees who owned slaves, revealed deep connections between the university's
history and the institution of slavery. These findings, and discussions amongst
the researchers shaped the Penn and Slavery Project's goal of redefining
complicity beyond ownership. Breanna Moore's contributions in PSP's second
semester expanded the project's focus to include generational wealth gaps. In
2018, VanJessica Gladney served as the PSP's Public History Fellow and spread
the project outreach in the greater Philadelphia area. That year, the PSP team
began to design an augmented reality app as a Digital Interruption and an
attempt to display the truth about Penn's history on its campus. Unfortunately,
PSP faced delays due to COVID 19. Despite setbacks, the project persisted,
engaging with activists and the wider community to confront historical
injustices and modern inequalities.","['VanJessica Gladney', 'Breanna Moore', 'Kathleen Brown']",2024-04-22T17:32:52Z,http://arxiv.org/abs/2404.14379v1
"Gamification for Education of the Digitally Native Generation by Means
  of Virtual Reality, Augmented Reality, Machine Learning, and Brain-Computing
  Interfaces in Museums","Particularly close attention is being paid today among researchers in social
science disciplines to aspects of learning in the digital age, especially for
the Digitally Native Generation. In the context of museums, the question is:
how can rich learning experiences be provided for increasingly technologically
advanced young visitors in museums? Which high-tech platforms and solutions do
museums need to focus on? At the same time, the software games business is
growing fast and now finding its way into non-entertainment contexts, helping
to deliver substantial benefits, particularly in education, training, research,
and health. This article outlines some aspects facing Digitally Native learners
in museums through an analysis of several radically new key technologies:
Interactivity, Wearables, Virtual Reality, and Augmented Reality. Special
attention is paid to use cases for application of games-based scenarios via
these technologies in non-leisure contexts and specifically for educational
purposes in museums.","['Olga Barkova', 'Natalia Pysarevska', 'Oleg Allenin', 'Serhii Hamotsky', 'Nikita Gordienko', 'Vladyslav Sarnatskyi', 'Vadym Ovcharenko', 'Mariia Tkachenko', 'Yurii Gordienko', 'Sergei Stirenko']",2018-06-20T17:03:52Z,http://arxiv.org/abs/1806.07842v1
"Design, Assembly, Calibration, and Measurement of an Augmented Reality
  Haploscope","A haploscope is an optical system which produces a carefully controlled
virtual image. Since the development of Wheatstone's original stereoscope in
1838, haploscopes have been used to measure perceptual properties of human
stereoscopic vision. This paper presents an augmented reality (AR) haploscope,
which allows the viewing of virtual objects superimposed against the real
world. Our lab has used generations of this device to make a careful series of
perceptual measurements of AR phenomena, which have been described in
publications over the previous 8 years. This paper systematically describes the
design, assembly, calibration, and measurement of our AR haploscope. These
methods have been developed and improved in our lab over the past 10 years.
Despite the fact that 180 years have elapsed since the original report of
Wheatstone's stereoscope, we have not previously found a paper that describes
these kinds of details.","['Nate Phillips', 'Kristen Massey', 'Mohammed Safayet Arefin', 'J. Edward Swan II']",2019-08-21T23:32:57Z,http://arxiv.org/abs/1908.08532v1
"A survey on applications of augmented, mixed and virtual reality for
  nature and environment","Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are
technologies of great potential due to the engaging and enriching experiences
they are capable of providing. Their use is rapidly increasing in diverse
fields such as medicine, manufacturing or entertainment. However, the
possibilities that AR, VR and MR offer in the area of environmental
applications are not yet widely explored. In this paper we present the outcome
of a survey meant to discover and classify existing AR/VR/MR applications that
can benefit the environment or increase awareness on environmental issues. We
performed an exhaustive search over several online publication access platforms
and past proceedings of major conferences in the fields of AR/VR/MR. Identified
relevant papers were filtered based on novelty, technical soundness, impact and
topic relevance, and classified into different categories. Referring to the
selected papers, we discuss how the applications of each category are
contributing to environmental protection, preservation and sensitization
purposes. We further analyse these approaches as well as possible future
directions in the scope of existing and upcoming AR/VR/MR enabling
technologies.","['Jason Rambach', 'Gergana Lilligreen', 'Alexander Schäfer', 'Ramya Bankanal', 'Alexander Wiebel', 'Didier Stricker']",2020-08-27T09:59:27Z,http://arxiv.org/abs/2008.12024v2
"Virtual and Augmented Reality-Based Assistive Interfaces for Upper-limb
  Prosthesis Control and Rehabilitation","Functional upper-limb prosthetic training can improve users performance in
controlling prostheses and has been incorporated into occupational therapy for
individuals in need. In recent years, virtual reality (VR) and augmented
reality (AR) technologies have been shown to be promising avenues to improve
the convenience of rehabilitative prosthesis training systems. However, it is
uncertain if the comprehensive efficacy and effectiveness of VR or AR assistive
tools are adequate compared to conventional prosthetic tools and if not,
whether enhancements can be made through incorporation of other technical
paradigms.
  This work first presents a mixed reality system we developed for prosthesis
control and training. Five able-bodied subjects are involved to perform
three-dimensional object manipulation tasks in analogous AR and VR
environments. Multiple evaluation metrics are applied to assess subjects
performances within the two paradigms. Based on the comparative analysis, we
find that VR-based environment promotes more efficient motion along with higher
task completion rate and path efficiency while AR paradigm allows subjects to
perform motor tasks with shorter time consumed. Another study is conducted to
evaluate the efficiency and feasibility of AR-facilitated prosthesis control
system compared to that in real-world and if any technical additions can be
applied to improve the AR-based system. Three able-bodied subjects were engaged
in the experiment to perform object manipulation tasks in a) physical
environment, b) AR-without-bypass environment, and c) AR-with-bypass
environment. Based on the results obtained from the assessment, we conclude
that while our AR-based system modestly lags behind the effectiveness of
physical systems, the study conducted using a bypass prosthesis suggests that
AR system has the potential to improve the efficacy of prosthesis control.",['Yinghe Sun'],2022-04-28T03:26:12Z,http://arxiv.org/abs/2205.02227v1
"Mixed Reality Communication for Medical Procedures: Teaching the
  Placement of a Central Venous Catheter","Medical procedures are an essential part of healthcare delivery, and the
acquisition of procedural skills is a critical component of medical education.
Unfortunately, procedural skill is not evenly distributed among medical
providers. Skills may vary within departments or institutions, and across
geographic regions, depending on the provider's training and ongoing
experience. We present a mixed reality real-time communication system to
increase access to procedural skill training and to improve remote emergency
assistance. Our system allows a remote expert to guide a local operator through
a medical procedure. RGBD cameras capture a volumetric view of the local scene
including the patient, the operator, and the medical equipment. The volumetric
capture is augmented onto the remote expert's view to allow the expert to
spatially guide the local operator using visual and verbal instructions. We
evaluated our mixed reality communication system in a study in which experts
teach the ultrasound-guided placement of a central venous catheter (CVC) to
students in a simulation setting. The study compares state-of-the-art video
communication against our system. The results indicate that our system enhances
and offers new possibilities for visual communication compared to video
teleconference-based training.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka', 'David Li', 'Christian Gütl']",2023-12-14T03:11:20Z,http://arxiv.org/abs/2312.08624v1
Interactive Multi-User 3D Visual Analytics in Augmented Reality,"This publication reports on a research project in which we set out to explore
the advantages and disadvantages augmented reality (AR) technology has for
visual data analytics. We developed a prototype of an AR data analytics
application, which provides users with an interactive 3D interface, hand
gesture-based controls and multi-user support for a shared experience, enabling
multiple people to collaboratively visualize, analyze and manipulate data with
high dimensional features in 3D space. Our software prototype, called DataCube,
runs on the Microsoft HoloLens - one of the first true stand-alone AR headsets,
through which users can see computer-generated images overlaid onto real-world
objects in the user's physical environment. Using hand gestures, the users can
select menu options, control the 3D data visualization with various filtering
and visualization functions, and freely arrange the various menus and virtual
displays in their environment. The shared multi-user experience allows all
participating users to see and interact with the virtual environment, changes
one user makes will become visible to the other users instantly. As users
engage together they are not restricted from observing the physical world
simultaneously and therefore they can also see non-verbal cues such as
gesturing or facial reactions of other users in the physical environment. The
main objective of this research project was to find out if AR interfaces and
collaborative analysis can provide an effective solution for data analysis
tasks, and our experience with our prototype system confirms this.","['Wanze Xie', 'Yining Liang', 'Janet Johnson', 'Andrea Mower', 'Samuel Burns', 'Colleen Chelini', 'Paul D Alessandro', 'Nadir Weibel', 'Jürgen P. Schulze']",2020-02-13T01:35:56Z,http://arxiv.org/abs/2002.05305v1
ArK: Augmented Reality with Knowledge Interactive Emergent Ability,"Despite the growing adoption of mixed reality and interactive AI agents, it
remains challenging for these systems to generate high quality 2D/3D scenes in
unseen environments. The common practice requires deploying an AI agent to
collect large amounts of data for model training for every new task. This
process is costly, or even impossible, for many domains. In this study, we
develop an infinite agent that learns to transfer knowledge memory from general
foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene
understanding and generation in the physical or virtual world. The heart of our
approach is an emerging mechanism, dubbed Augmented Reality with Knowledge
Inference Interaction (ArK), which leverages knowledge-memory to generate
scenes in unseen physical world and virtual reality environments. The knowledge
interactive emergent ability (Figure 1) is demonstrated as the observation
learns i) micro-action of cross-modality: in multi-modality models to collect a
large amount of relevant knowledge memory data for each interaction task (e.g.,
unseen scene understanding) from the physical reality; and ii) macro-behavior
of reality-agnostic: in mix-reality environments to improve interactions that
tailor to different characterized roles, target variables, collaborative
information, and so on. We validate the effectiveness of ArK on the scene
generation and editing tasks. We show that our ArK approach, combined with
large foundation models, significantly improves the quality of generated 2D/3D
scenes, compared to baselines, demonstrating the potential benefit of
incorporating ArK in generative AI for applications such as metaverse and
gaming simulation.","['Qiuyuan Huang', 'Jae Sung Park', 'Abhinav Gupta', 'Paul Bennett', 'Ran Gong', 'Subhojit Som', 'Baolin Peng', 'Owais Khan Mohammed', 'Chris Pal', 'Yejin Choi', 'Jianfeng Gao']",2023-05-01T17:57:01Z,http://arxiv.org/abs/2305.00970v1
"Towards Predictions of the Image Quality of Experience for Augmented
  Reality Scenarios","Augmented Reality (AR) devices are commonly head-worn to overlay
context-dependent information into the field of view of the device operators.
One particular scenario is the overlay of still images, either in a traditional
fashion, or as spherical, i.e., immersive, content. For both media types, we
evaluate the interplay of user ratings as Quality of Experience (QoE) with (i)
the non-referential BRISQUE objective image quality metric and (ii) human
subject dry electrode EEG signals gathered with a commercial device.
Additionally, we employ basic machine learning approaches to assess the
possibility of QoE predictions based on rudimentary subject data. Corroborating
prior research for the overall scenario, we find strong correlations for both
approaches with user ratings as Mean Opinion Scores, which we consider as QoE
metric. In prediction scenarios based on data subsets, we find good performance
for the objective metric as well as the EEG-based approach. While the objective
metric can yield high QoE prediction accuracies overall, it is limited i its
application for individual subjects. The subject-based EEG approach, on the
other hand, enables good predictability of the QoE for both media types, but
with better performance for regular content. Our results can be employed in
practical scenarios by content and network service providers to optimize the
user experience in augmented reality scenarios.","['Brian Bauman', 'Patrick Seeling']",2017-05-02T18:17:50Z,http://arxiv.org/abs/1705.01123v1
"Physics holo.lab learning experience: Using Smartglasses for Augmented
  Reality labwork to foster the concepts of heat conduction","Fundamental concepts of thermodynamics rely on abstract physical quantities
such as energy, heat and entropy, which play an important role in the process
of interpreting thermal phenomena and statistical mechanics. However, these
quantities are not covered by human (visual) perception and thus, an intuitive
understanding often is lacking. Today immersive technologies like head-mounted
displays of the newest generation, especially HoloLens, allow for high quality
augmented reality learning experiences, which can overcome this perception gap
and simultaneously avoid a split attention effect. In a mixed reality (MR)
scenario as presented in this paper---which we call a holo.lab---human
perception can be extended to the thermal regime by presenting false-color
representations of the temperature of objects as a virtual augmentation
directly on the real object itself in real-time. Direct feedback to
experimental actions of the users in form of different representations allows
for immediate comparison to theoretical principles and predictions and
therefore is supposed to intensify the theory-experiment interactions and to
increase the conceptual understanding. We tested this technology for an
experiment on thermal conduction of metals in the framework of undergraduate
laboratories. A pilot study with treatment and control groups (N = 59) showed a
small positive effect of MR on students' performance measured with a
standardized concept test for thermodynamics, indicating an improvement of the
understanding of the underlying physical concepts.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Klein', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-11-14T12:50:35Z,http://arxiv.org/abs/1711.05087v2
"An embedded deep learning system for augmented reality in firefighting
  applications","Firefighting is a dynamic activity, in which numerous operations occur
simultaneously. Maintaining situational awareness (i.e., knowledge of current
conditions and activities at the scene) is critical to the accurate
decision-making necessary for the safe and successful navigation of a fire
environment by firefighters. Conversely, the disorientation caused by hazards
such as smoke and extreme heat can lead to injury or even fatality. This
research implements recent advancements in technology such as deep learning,
point cloud and thermal imaging, and augmented reality platforms to improve a
firefighter's situational awareness and scene navigation through improved
interpretation of that scene. We have designed and built a prototype embedded
system that can leverage data streamed from cameras built into a firefighter's
personal protective equipment (PPE) to capture thermal, RGB color, and depth
imagery and then deploy already developed deep learning models to analyze the
input data in real time. The embedded system analyzes and returns the processed
images via wireless streaming, where they can be viewed remotely and relayed
back to the firefighter using an augmented reality platform that visualizes the
results of the analyzed inputs and draws the firefighter's attention to objects
of interest, such as doors and windows otherwise invisible through smoke and
flames.","['Manish Bhattarai', 'Aura Rose Jensen-Curtis', 'Manel MartíNez-Ramón']",2020-09-22T16:55:44Z,http://arxiv.org/abs/2009.10679v1
"Modeling an Augmented Reality Game Environment to Enhance Behavior of
  ADHD Patients","The paper generically models an augmented reality game-based environment to
project the gamification of an online cognitive behavioral therapist that
performs instant measurements for patients with a predefined Attention Deficit
Hyperactivity Disorder (ADHD). ADHD is one of the most common
neurodevelopmental disorders in which patients have difficulties related to
inattention, hyperactivity, and impulsivity. Those patients are in need for a
psychological therapy; the use of cognitive behavioral therapy as a
firmly-established treatment is to help in enhancing the way they think and
behave. A major limitation in traditional cognitive behavioral therapies is
that therapists may face difficulty to optimize patients' neuropsychological
stimulus following a specified treatment plan, i.e., therapists struggle to
draw clear images when stimulating patients' mindset to a point where they
should be. Other limitations recognized here include availability,
accessibility and level-of-experience of the therapists. Therefore, the paper
present a gamification model, we term as ""AR-Therapist,"" in order to take
advantages of augmented reality developments to engage patients in both real
and virtual game-based environments. The model provides an on-time measurements
of patients' progress throughout the treatment sessions which, in result,
overcomes limitations observed in traditional cognitive behavioral therapies.","['Saad Alqithami', 'Musaad Alzahrani', 'Abdulkareem Alzahrani', 'Ahmed Mostafa']",2019-11-04T01:57:13Z,http://arxiv.org/abs/1911.01003v1
"On-the-fly Augmented Reality for Orthopaedic Surgery Using a Multi-Modal
  Fiducial","Fluoroscopic X-ray guidance is a cornerstone for percutaneous orthopaedic
surgical procedures. However, two-dimensional observations of the
three-dimensional anatomy suffer from the effects of projective simplification.
Consequently, many X-ray images from various orientations need to be acquired
for the surgeon to accurately assess the spatial relations between the
patient's anatomy and the surgical tools. In this paper, we present an
on-the-fly surgical support system that provides guidance using augmented
reality and can be used in quasi-unprepared operating rooms. The proposed
system builds upon a multi-modality marker and simultaneous localization and
mapping technique to co-calibrate an optical see-through head mounted display
to a C-arm fluoroscopy system. Then, annotations on the 2D X-ray images can be
rendered as virtual objects in 3D providing surgical guidance. We
quantitatively evaluate the components of the proposed system, and finally,
design a feasibility study on a semi-anthropomorphic phantom. The accuracy of
our system was comparable to the traditional image-guided technique while
substantially reducing the number of acquired X-ray images as well as procedure
time. Our promising results encourage further research on the interaction
between virtual and real objects, that we believe will directly benefit the
proposed method. Further, we would like to explore the capabilities of our
on-the-fly augmented reality support system in a larger study directed towards
common orthopaedic interventions.","['Sebastian Andress', 'Alex Johnson', 'Mathias Unberath', 'Alexander Winkler', 'Kevin Yu', 'Javad Fotouhi', 'Simon Weidert', 'Greg Osgood', 'Nassir Navab']",2018-01-04T22:02:33Z,http://arxiv.org/abs/1801.01560v1
"The Impact of Augmented-Reality Head-Mounted Displays on Users' Movement
  Behavior: An Exploratory Study","The augmented-reality head-mounted display (e.g., Microsoft HoloLens) is one
of the most innovative technologies in multimedia and human-computer
interaction in recent years. Despite the emerging research of its applications
on engineering, education, medicines, to name a few, its impact on users'
movement behavior is still underexplored. The movement behavior, especially for
office workers with sedentary lifestyles, is related to many chronic
conditions. Unlike the traditional screens, the augmented-reality head-mounted
display (AR-HMD) could enable mobile virtual screens, which might impact on
users' movement behavior. In this paper, we present our initial study to
explore the impact of AR-HMDs on users' movement behavior. We compared the
differences of macro-movements (e.g., sit-stand transitions) and
micro-movements (e.g., moving the head) between two experimental modes (i.e.,
spatial-mapping and tag-along) with a dedicated trivial quiz task using
HoloLens. The study reveals interesting findings: strong evidence supports that
participants had more head-movements in the tag-along mode where higher
simplicity and freedom of moving the virtual screen were given; body
position/direction changes show the same effect with moderate evidence, while
sit-stand transitions show no difference between the two modes with weak
evidence. Our results imply several design considerations and research
opportunities for future work on the ergonomics of AR-HMDs in the perspective
of health.","['Yunlong Wang', 'Harald Reiterer']",2019-05-24T16:20:08Z,http://arxiv.org/abs/1905.10315v2
"Augmented reality as a tool for open science platform by research
  collaboration in virtual teams","The provision of open science is defined as a general policy aimed at
overcoming the barriers that hinder the implementation of the European Research
Area (ERA). An open science foundation seeks to capture all the elements needed
for the functioning of ERA: research data, scientific instruments, ICT services
(connections, calculations, platforms, and specific studies such as portals).
Managing shared resources for the community of scholars maximizes the benefits
to society. In the field of digital infrastructure, this has already
demonstrated great benefits. It is expected that applying this principle to an
open science process will improve management by funding organizations in
collaboration with stakeholders through mechanisms such as public consultation.
This will increase the perception of joint ownership of the infrastructure. It
will also create clear and non-discriminatory access rules, along with a sense
of joint ownership that stimulates a higher level of participation,
collaboration and social reciprocity. The article deals with the concept of
open science. The concept of the European cloud of open science and its
structure are presented. According to the study, it has been shown that the
structure of the cloud of open science includes an augmented reality as an
open-science platform. An example of the practical application of this tool is
the general description of MaxWhere, developed by Hungarian scientists, and is
a platform of aggregates of individual 3D spaces.","['Mariya P. Shyshkina', 'Maiia V. Marienko']",2020-02-28T07:32:07Z,http://arxiv.org/abs/2003.07687v1
Object Detection in the Context of Mobile Augmented Reality,"In the past few years, numerous Deep Neural Network (DNN) models and
frameworks have been developed to tackle the problem of real-time object
detection from RGB images. Ordinary object detection approaches process
information from the images only, and they are oblivious to the camera pose
with regard to the environment and the scale of the environment. On the other
hand, mobile Augmented Reality (AR) frameworks can continuously track a
camera's pose within the scene and can estimate the correct scale of the
environment by using Visual-Inertial Odometry (VIO). In this paper, we propose
a novel approach that combines the geometric information from VIO with semantic
information from object detectors to improve the performance of object
detection on mobile devices. Our approach includes three components: (1) an
image orientation correction method, (2) a scale-based filtering approach, and
(3) an online semantic map. Each component takes advantage of the different
characteristics of the VIO-based AR framework. We implemented the AR-enhanced
features using ARCore and the SSD Mobilenet model on Android phones. To
validate our approach, we manually labeled objects in image sequences taken
from 12 room-scale AR sessions. The results show that our approach can improve
on the accuracy of generic object detectors by 12% on our dataset.","['Xiang Li', 'Yuan Tian', 'Fuyao Zhang', 'Shuxue Quan', 'Yi Xu']",2020-08-15T05:15:00Z,http://arxiv.org/abs/2008.06655v1
"AEGIS: A real-time multimodal augmented reality computer vision based
  system to assist facial expression recognition for individuals with autism
  spectrum disorder","The ability to interpret social cues comes naturally for most people, but for
those living with Autism Spectrum Disorder (ASD), some experience a deficiency
in this area. This paper presents the development of a multimodal augmented
reality (AR) system which combines the use of computer vision and deep
convolutional neural networks (CNN) in order to assist individuals with the
detection and interpretation of facial expressions in social settings. The
proposed system, which we call AEGIS (Augmented-reality Expression Guided
Interpretation System), is an assistive technology deployable on a variety of
user devices including tablets, smartphones, video conference systems, or
smartglasses, showcasing its extreme flexibility and wide range of use cases,
to allow integration into daily life with ease. Given a streaming video camera
source, each real-world frame is passed into AEGIS, processed for facial
bounding boxes, and then fed into our novel deep convolutional time windowed
neural network (TimeConvNet). We leverage both spatial and temporal information
in order to provide an accurate expression prediction, which is then converted
into its corresponding visualization and drawn on top of the original video
frame. The system runs in real-time, requires minimal set up and is simple to
use. With the use of AEGIS, we can assist individuals living with ASD to learn
to better identify expressions and thus improve their social experiences.","['James Ren Hou Lee', 'Alexander Wong']",2020-10-22T17:20:38Z,http://arxiv.org/abs/2010.11884v1
"An Unsupervised Approach towards Varying Human Skin Tone Using
  Generative Adversarial Networks","With the increasing popularity of augmented and virtual reality, retailers
are now focusing more towards customer satisfaction to increase the amount of
sales. Although augmented reality is not a new concept but it has gained much
needed attention over the past few years. Our present work is targeted towards
this direction which may be used to enhance user experience in various virtual
and augmented reality based applications. We propose a model to change skin
tone of a person. Given any input image of a person or a group of persons with
some value indicating the desired change of skin color towards fairness or
darkness, this method can change the skin tone of the persons in the image.
This is an unsupervised method and also unconstrained in terms of pose,
illumination, number of persons in the image etc. The goal of this work is to
reduce the time and effort which is generally required for changing the skin
tone using existing applications (e.g., Photoshop) by professionals or novice.
To establish the efficacy of this method we have compared our result with that
of some popular photo editor and also with the result of some existing
benchmark method related to human attribute manipulation. Rigorous experiments
on different datasets show the effectiveness of this method in terms of
synthesizing perceptually convincing outputs.","['Debapriya Roy', 'Diganta Mukherjee', 'Bhabatosh Chanda']",2020-10-30T06:27:03Z,http://arxiv.org/abs/2010.16092v1
"Multicenter Assessment of Augmented Reality Registration Methods for
  Image-guided Interventions","Purpose: To evaluate manual and automatic registration times as well as
accuracy with augmented reality during alignment of a holographic 3-dimensional
(3D) model onto the real-world environment.
  Method: 18 participants in various stages of clinical training across two
academic centers registered a 3D CT phantom model onto a CT grid using the
HoloLens 2 augmented reality headset 3 consecutive times. Registration times
and accuracy were compared among different registration methods (hand gesture,
Xbox controller, and automatic registration), levels of clinical experience,
and consecutive attempts. Registration times were also compared with prior
HoloLens 1 data.
  Results: Mean aggregate manual registration times were 27.7, 24.3, and 72.8
seconds for one-handed gesture, two-handed gesture, and Xbox controller,
respectively; mean automatic registration time was 5.3s (ANOVA p<0.0001). No
significant difference in registration times was found among attendings,
residents and fellows, and medical students (p>0.05). Significant improvements
in registration times were detected across consecutive attempts using hand
gestures (p<0.01). Compared with previously reported HoloLens 1 experience,
hand gesture registration times were 81.7% faster (p<0.05). Registration
accuracies were not significantly different across manual registration methods,
measuring at 5.9, 9.5, and 8.6 mm with one-handed gesture, two-handed gesture,
and Xbox controller, respectively (p>0.05).
  Conclusions: Manual registration times decreased significantly with updated
hand gesture maneuvers on HoloLens 2 versus HoloLens 1, approaching the
registration times of automatic registration and outperforming Xbox controller
mediated registration. These results will encourage wider clinical integration
of HoloLens 2 in procedural medical care.","['Ningcheng Li', 'Jonathan Wakim', 'Yilun Koethe', 'Timothy Huber', 'Terence Gade', 'Stephen Hunt', 'Brian Park']",2020-12-03T23:05:48Z,http://arxiv.org/abs/2012.02319v1
Seeing Thru Walls: Visualizing Mobile Robots in Augmented Reality,"We present an approach for visualizing mobile robots through an Augmented
Reality headset when there is no line-of-sight visibility between the robot and
the human. Three elements are visualized in Augmented Reality: 1) Robot's 3D
model to indicate its position, 2) An arrow emanating from the robot to
indicate its planned movement direction, and 3) A 2D grid to represent the
ground plane. We conduct a user study with 18 participants, in which each
participant are asked to retrieve objects, one at a time, from stations at the
two sides of a T-junction at the end of a hallway where a mobile robot is
roaming. The results show that visualizations improved the perceived safety and
efficiency of the task and led to participants being more comfortable with the
robot within their personal spaces. Furthermore, visualizing the motion intent
in addition to the robot model was found to be more effective than visualizing
the robot model alone. The proposed system can improve the safety of automated
warehouses by increasing the visibility and predictability of robots.","['Morris Gu', 'Akansel Cosgun', 'Wesley P. Chan', 'Tom Drummond', 'Elizabeth Croft']",2021-04-08T06:54:37Z,http://arxiv.org/abs/2104.03547v2
"Augmented Reality and Gamification: A Framework for Developing
  Supplementary Learning Tool","The main purpose of the study is to develop a supplementary learning tool
framework by the use of a dynamic mobile application using Unity AR and Vuforia
for Senior High School (SHS) students and teachers to help the learning process
in SHS Earth Science. The researchers will be using the Software Development
Life Cycle (SDLC) Model of methodology to ensure the quality of the software as
well as the correctness of the development process. The expected result of the
study is that Augmented Reality and Gamification will now be used as a
supplementary learning tool in SHS Earth Science. Augmented Reality and
Gamification can now be used as a supplementary learning tool in SHS Earth
Science using the designed framework. Future studies will focus on the
development of the framework and the mobile application. Since the system has a
lot of potential in the education sector and due to the effects of COVID-19,
the software will serve as a pioneer to show that a supplementary tool will
help students learn logically and entertainingly especially since schools
nowadays are transitioning with either distance learning or blended learning.",['Carlo H. Godoy Jr'],2021-08-07T17:03:00Z,http://arxiv.org/abs/2108.03487v1
"A Review of Augmented Reality Apps for an AR-Based STEM Education
  Framework","Within the past two decades, Augmented Reality (AR) applications have
received increased attention. Augmented Reality is now widely used in the
education sector at level K to 12. AR is expected to be generally adopted in
two to three years in higher education and four to five years in K to 12.
Applying AR technology in the education sector especially in STEM subjects, can
result in having a smart campus. In adopting a SMART Campus strategy, education
practitioners must address many intrinsic issues in science, technology,
engineering, and mathematics (STEM) research. For example, in physics, there
are expensive or insufficient laboratory systems, system faults, and difficulty
simulating other experimental circumstances. In technology, many schools do not
have enough computers. In engineering, there are only a few instructors who are
knowledgeable in computer aided design (CAD). In mathematics, few teachers
incorporate technology into their lessons often because they believe it is
still better to teach through the traditional methods. Hence, In this paper we
discuss how AR is being used now in different learning areas in STEM to open
new doors to researchers and teachers as they transition their schools into
SMART campuses with the use of AR apps. Aligned with this, a suggested
framework for school administrators and policymakers is proposed based on a
review of the positive benefits of different AR apps.",['Carlo H. Godoy Jr.'],2022-01-19T08:30:26Z,http://arxiv.org/abs/2203.07024v1
"Deep Learning and Handheld Augmented Reality Based System for Optimal
  Data Collection in Fault Diagnostics Domain","Compared to current AI or robotic systems, humans navigate their environment
with ease, making tasks such as data collection trivial. However, humans find
it harder to model complex relationships hidden in the data. AI systems,
especially deep learning (DL) algorithms, impressively capture those complex
relationships. Symbiotically coupling humans and computational machines'
strengths can simultaneously minimize the collected data required and build
complex input-to-output mapping models. This paper enables this coupling by
presenting a novel human-machine interaction framework to perform fault
diagnostics with minimal data. Collecting data for diagnosing faults for
complex systems is difficult and time-consuming. Minimizing the required data
will increase the practicability of data-driven models in diagnosing faults.
The framework provides instructions to a human user to collect data that
mitigates the difference between the data used to train and test the fault
diagnostics model. The framework is composed of three components: (1) a
reinforcement learning algorithm for data collection to develop a training
dataset, (2) a deep learning algorithm for diagnosing faults, and (3) a
handheld augmented reality application for data collection for testing data.
The proposed framework has provided above 100\% precision and recall on a novel
dataset with only one instance of each fault condition. Additionally, a
usability study was conducted to gauge the user experience of the handheld
augmented reality application, and all users were able to follow the provided
steps.","['Ryan Nguyen', 'Rahul Rai']",2022-06-15T19:15:26Z,http://arxiv.org/abs/2206.07772v1
"Arigatō: Effects of Adaptive Guidance on Engagement and Performance in
  Augmented Reality Learning Environments","Experiential learning (ExL) is the process of learning through experience or
more specifically ""learning through reflection on doing"". In this paper, we
propose a simulation of these experiences, in Augmented Reality (AR),
addressing the problem of language learning. Such systems provide an excellent
setting to support ""adaptive guidance"", in a digital form, within a real
environment. Adaptive guidance allows the instructions and learning content to
be customised for the individual learner, thus creating a unique learning
experience. We developed an adaptive guidance AR system for language learning,
we call Arigat\=o (Augmented Reality Instructional Guidance & Tailored
Omniverse), which offers immediate assistance, resources specific to the
learner's needs, manipulation of these resources, and relevant feedback.
Considering guidance, we employ this prototype to investigate the effect of the
amount of guidance (fixed vs. adaptive-amount) and the type of guidance (fixed
vs. adaptive-associations) on the engagement and consequently the learning
outcomes of language learning in an AR environment. The results for the amount
of guidance show that compared to the adaptive-amount, the fixed-amount of
guidance group scored better in the immediate and delayed (after 7 days) recall
tests. However, this group also invested a significantly higher mental effort
to complete the task. The results for the type of guidance show that the
adaptive-associations group outperforms the fixed-associations group in the
immediate, delayed (after 7 days) recall tests, and learning efficiency. The
adaptive-associations group also showed significantly lower mental effort and
spent less time to complete the task.","['Maheshya Weerasinghe', 'Aaron Quigley', 'Klen Čopič Pucihar', 'Alice Toniolo', 'Angela Miguel', 'Matjaž Kljun']",2022-07-02T11:20:13Z,http://arxiv.org/abs/2207.00798v1
"Augmented Reality's Potential for Identifying and Mitigating Home
  Privacy Leaks","Users face various privacy risks in smart homes, yet there are limited ways
for them to learn about the details of such risks, such as the data practices
of smart home devices and their data flow. In this paper, we present Privacy
Plumber, a system that enables a user to inspect and explore the privacy
""leaks"" in their home using an augmented reality tool. Privacy Plumber allows
the user to learn and understand the volume of data leaving the home and how
that data may affect a user's privacy -- in the same physical context as the
devices in question, because we visualize the privacy leaks with augmented
reality. Privacy Plumber uses ARP spoofing to gather aggregate network traffic
information and presents it through an overlay on top of the device in an
smartphone app. The increased transparency aims to help the user make privacy
decisions and mend potential privacy leaks, such as instruct Privacy Plumber on
what devices to block, on what schedule (i.e., turn off Alexa when sleeping),
etc. Our initial user study with six participants demonstrates participants'
increased awareness of privacy leaks in smart devices, which further
contributes to their privacy decisions (e.g., which devices to block).","['Stefany Cruz', 'Logan Danek', 'Shinan Liu', 'Christopher Kraemer', 'Zixin Wang', 'Nick Feamster', 'Danny Yuxing Huang', 'Yaxing Yao', 'Josiah Hester']",2023-01-27T21:36:23Z,http://arxiv.org/abs/2301.11998v1
"Task-oriented and Semantics-aware Communication Framework for
  Avatar-centric Augmented Reality","Upon the advent of the emerging metaverse and its related applications in
Augmented Reality (AR), the current bit-oriented network struggles to support
real-time changes for the vast amount of associated information, hindering its
development. Thus, a critical revolution in the Sixth Generation (6G) networks
is envisioned through the joint exploitation of information context and its
importance to the task, leading to a communication paradigm shift towards
semantic and effectiveness levels. However, current research has not yet
proposed any explicit and systematic communication framework for AR
applications that incorporate these two levels. To fill this research gap, this
paper presents a task-oriented and semantics-aware communication framework for
augmented reality (TSAR) to enhance communication efficiency and effectiveness
in 6G. Specifically, we first analyse the traditional wireless AR point cloud
communication framework and then summarize our proposed semantic information
along with the end-to-end wireless communication. We then detail the design
blocks of the TSAR framework, covering both semantic and effectiveness levels.
Finally, numerous experiments have been conducted to demonstrate that, compared
to the traditional point cloud communication framework, our proposed TSAR
significantly reduces wireless AR application transmission latency by 95.6%,
while improving communication effectiveness in geometry and color aspects by up
to 82.4% and 20.4%, respectively.","['Zhe Wang', 'Yansha Deng', 'A. Hamid Aghvami']",2023-06-27T13:41:54Z,http://arxiv.org/abs/2306.15470v3
"DualStream: Spatially Sharing Selves and Surroundings using Mobile
  Devices and Augmented Reality","In-person human interaction relies on our spatial perception of each other
and our surroundings. Current remote communication tools partially address each
of these aspects. Video calls convey real user representations but without
spatial interactions. Augmented and Virtual Reality (AR/VR) experiences are
immersive and spatial but often use virtual environments and characters instead
of real-life representations. Bridging these gaps, we introduce DualStream, a
system for synchronous mobile AR remote communication that captures, streams,
and displays spatial representations of users and their surroundings.
DualStream supports transitions between user and environment representations
with different levels of visuospatial fidelity, as well as the creation of
persistent shared spaces using environment snapshots. We demonstrate how
DualStream can enable spatial communication in real-world contexts, and support
the creation of blended spaces for collaboration. A formative evaluation of
DualStream revealed that users valued the ability to interact spatially and
move between representations, and could see DualStream fitting into their own
remote communication practices in the near future. Drawing from these findings,
we discuss new opportunities for designing more widely accessible spatial
communication tools, centered around the mobile phone.","['Rishi Vanukuru', 'Suibi Che-Chuan Weng', 'Krithik Ranjan', 'Torin Hopkins', 'Amy Banic', 'Mark D. Gross', 'Ellen Yi-Luen Do']",2023-09-02T06:38:33Z,http://arxiv.org/abs/2309.00842v1
"Using Augmented Reality to Assess and Modify Mobile Manipulator Surface
  Repair Plans","Industrial robotics are redefining inspection and maintenance routines across
multiple sectors, enhancing safety, efficiency, and environmental
sustainability. In outdoor industrial facilities, it is crucial to inspect and
repair complex surfaces affected by corrosion. To address this challenge,
mobile manipulators have been developed to navigate these facilities, identify
corroded areas, and apply protective coatings. However, given that this
technology is still in its infancy and the consequences of improperly coating
essential equipment can be significant, human oversight is necessary to review
the robot's corrosion identification and repair plan. We present a practical
and scalable Augmented Reality (AR)-based system designed to empower
non-experts to visualize, modify, and approve robot-generated surface corrosion
repair plans in real-time. Built upon an AR-based human-robot interaction
framework, Augmented Robot Environment (AugRE), we developed a comprehensive AR
application module called Situational Task Accept and Repair (STAR). STAR
allows users to examine identified corrosion images, point cloud data, and
robot navigation objectives overlaid on the physical environment within these
industrial environments. Users are able to additionally make adjustments to the
robot repair plan in real-time using interactive holographic volumes, excluding
critical nearby equipment that might be at risk of coating overspray. We
demonstrate the entire system using a Microsoft HoloLens 2 and a dual-arm
mobile manipulator. Our future research will focus on evaluating user
experience, system robustness, and real-world validation.","['Frank Regal', 'Steven Swanbeck', 'Fabian Parra', 'Jared Rosenbaum', 'Mitch Pryor']",2023-11-02T04:34:57Z,http://arxiv.org/abs/2311.00988v1
"Exploring the Pathways of Adaptation an Avatar 3D Animation Procedures
  and Virtual Reality Arenas in Research of Human Courtship Behaviour and
  Sexual Reactivity in Psychological Research","There are many reasons for utilising 3D animation and virtual reality in
sexuality research. Apart from providing a mean with which to (re)experience
certain situations there are four main advantages: a) bespoke animated stimuli
can be created and customized, which is especially important when researching
paraphilia and sexual preferences, b) stimulus production is less expensive and
easier to produce compared to real world stimuli, c) virtual reality allows us
to capture data such as physiological reasons to stimuli, that we would not be
able to otherwise (without resorting to self-report measures which are
especially problematic in this research domain), d) ethical, legal, and health
and safety issues are less complex since neither physical nor psychological
harm is caused to animated characters allowing for the safe presentation of
stimuli involving vulnerable targets. The animation sub-group has been
exploring so far several production quality levels and various animation
procedures in a number of available software. The aim is to develop static as
well as dynamic, interactive sexual stimuli for sexual diagnostic and
therapeutic purposes. We are aware of number of ethical issues related to the
use of virtual reality in proposed research are analysed in this chapter.","['Jakub Binter', 'Kateřina Klapilová', 'Tereza Zikánová', 'Tommy Nilsson', 'Klára Bártová', 'Lucie Krejcová', 'Renata Androvicová', 'Jitka Lindová', 'Denisa Prušová', 'Timothy Wells', 'Daniel Riha']",2016-11-06T18:27:09Z,http://arxiv.org/abs/1611.01817v1
"Toward the Internet of No Things: The Role of O2O Communications and
  Extended Reality","Future fully interconnected virtual reality (VR) systems and the Tactile
Internet diminish the boundary between virtual (online) and real (offline)
worlds, while extending the digital and physical capabilities of humans via
edge computing and teleoperated robots, respectively. In this paper, we focus
on the Internet of No Things as an extension of immersive VR from virtual to
real environments, where human-intended Internet services - either digital or
physical - appear when needed and disappear when not needed. We first introduce
the concept of integrated online-to-offline (O2O) communications, which treats
online and offline channels as complementary to bridge the virtual and physical
worlds and provide O2O multichannel experiences. We then elaborate on the
emerging extended reality (XR), which brings the different forms of
virtual/augmented/mixed reality together to realize the entire
reality-virtuality continuum and, more importantly, supports human-machine
interaction as envisioned by the Tactile Internet, while posing challenges to
conventional handhelds, e.g., smartphones. Building on the so-called
invisible-to-visible (I2V) technology concept, we present our extrasensory
perception network (ESPN) and investigate how O2O communications and XR can be
combined for the nonlocal extension of human ""sixth-sense"" experiences in space
and time. We conclude by putting our ideas in perspective of the 6G vision.","['Martin Maier', 'Amin Ebrahimzadeh']",2019-06-16T17:41:23Z,http://arxiv.org/abs/1906.06738v1
Scalable sim-to-real transfer of soft robot designs,"The manual design of soft robots and their controllers is notoriously
challenging, but it could be augmented---or, in some cases, entirely
replaced---by automated design tools. Machine learning algorithms can
automatically propose, test, and refine designs in simulation, and the most
promising ones can then be manufactured in reality (sim2real). However, it is
currently not known how to guarantee that behavior generated in simulation can
be preserved when deployed in reality. Although many previous studies have
devised training protocols that facilitate sim2real transfer of control
polices, little to no work has investigated the simulation-reality gap as a
function of morphology. This is due in part to an overall lack of tools capable
of systematically designing and rapidly manufacturing robots. Here we introduce
a low cost, open source, and modular soft robot design and construction kit,
and use it to simulate, fabricate, and measure the simulation-reality gap of
minimally complex yet soft, locomoting machines. We prove the scalability of
this approach by transferring an order of magnitude more robot designs from
simulation to reality than any other method. The kit and its instructions can
be found here: https://github.com/skriegman/sim2real4designs","['Sam Kriegman', 'Amir Mohammadi Nasab', 'Dylan Shah', 'Hannah Steele', 'Gabrielle Branin', 'Michael Levin', 'Josh Bongard', 'Rebecca Kramer-Bottiglio']",2019-11-23T01:11:29Z,http://arxiv.org/abs/1911.10290v1
"User Experience of Reading in Virtual Reality -- Finding Values for Text
  Distance, Size and Contrast","Virtual Reality (VR) has an increasing impact on the market in many fields,
from education and medicine to engineering and entertainment, by creating
different applications that replicate or in the case of augmentation enhance
real-life scenarios. Intending to present realistic environments, VR
applications are including text that we are surrounded by every day. However,
text can only add value to the virtual environment if it is designed and
created in such a way that users can comfortably read it. With the aim to
explore what values for text parameters users find comfortable while reading in
virtual reality, a study was conducted allowing participants to manipulate text
parameters such as font size, distance, and contrast. Therefore two different
standalone virtual reality devices were used, Oculus Go and Quest, together
with three different text samples: Short (2 words), medium (21 words), and long
(51 words). Participants had the task of setting text parameters to the best
and worst possible value. Additionally, participants were asked to rate their
experience of reading in virtual reality. Results report mean values for
angular size (the combination of distance and font size) and color contrast
depending on the different device used as well as the varying text length, for
both tasks. Significant differences were found for values of angular size,
depending on the length of the displayed text. However, different device types
had no significant influence on text parameters but on the experiences reported
using the self-assessment manikin (SAM) scale.","['Tanja Kojić', 'Danish Ali', 'Robert Greinacher', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2020-04-03T13:14:42Z,http://arxiv.org/abs/2004.01545v1
"Standardization of Extended Reality (XR) over 5G and 5G-Advanced 3GPP
  New Radio","Extended Reality (XR) is one of the major innovations to be introduced in
5G/5G-Advanced communication systems. A combination of augmented reality,
virtual reality, and mixed reality, supplemented by cloud gaming, revisits the
way how humans interact with computers, networks, and each other. However,
efficient support of XR services imposes new challenges for existing and future
wireless networks. This article presents a tutorial on integrating support for
the XR into the 3GPP New Radio (NR), summarizing a range of activities handled
within various 3GPP Service and Systems Aspects (SA) and Radio Access Networks
(RAN) groups. The article also delivers a case study evaluating the performance
of different XR services in state-of-the-art NR Release 17. The paper concludes
with a vision of further enhancements to better support XR in future NR
releases and outlines open problems in this area.","['Margarita Gapeyenko', 'Vitaly Petrov', 'Stefano Paris', 'Andrea Marcano', 'Klaus I. Pedersen']",2022-03-04T11:17:34Z,http://arxiv.org/abs/2203.02242v3
Analyzing Performance Issues of Virtual Reality Applications,"Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)
and Mixed Reality (MR). XR is an emerging technology that simulates a realistic
environment for users. XR techniques have provided revolutionary user
experiences in various application scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). Due to the
high computational cost of rendering real-time animation in limited-resource
devices and constant interaction with user activity, XR applications often face
performance bottlenecks, and these bottlenecks create a negative impact on the
user experience of XR software. Thus, performance optimization plays an
essential role in many industry-standard XR applications. Even though
identifying performance bottlenecks in traditional software (e.g., desktop
applications) is a widely explored topic, those approaches cannot be directly
applied within XR software due to the different nature of XR applications.
Moreover, XR applications developed in different frameworks such as Unity and
Unreal Engine show different performance bottleneck patterns and thus,
bottleneck patterns of Unity projects can't be applied for Unreal Engine
(UE)-based XR projects. To fill the knowledge gap for XR performance
optimizations of Unreal Engine-based XR projects, we present the first
empirical study on performance optimizations from seven UE XR projects, 78 UE
XR discussion issues and three sources of UE documentation. Our analysis
identified 14 types of performance bugs, including 12 types of bugs related to
UE settings issues and two types of CPP source code-related issues. To further
assist developers in detecting performance bugs based on the identified bug
patterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect
performance bugs in both configuration files and source code.","['Jason Hogan', 'Aaron Salo', 'Dhia Elhaq Rzig', 'Foyzul Hassan', 'Bruce Maxim']",2022-11-03T17:27:36Z,http://arxiv.org/abs/2211.02013v1
"Painterly Reality: Enhancing Audience Experience with Paintings through
  Interactive Art","Perceiving paintings entails more than merely engaging the audience's eyes
and brains; their perceptions and experiences of a painting can be intricately
connected with body movement. This paper proposes an interactive art approach
entitled ""Painterly Reality"" that facilitates the perception and interaction
with paintings in a three-dimensional manner. Its objective is to promote
bodily engagement with the painting (i.e., embedded body embodiment and its
movement and interaction) to enhance the audience's experience, while
maintaining its essence. Unlike two-dimensional interactions, this approach
constructs the Painterly Reality by capturing the audience's body embodiment in
real-time and embedding into a three-dimensional painterly world derived from a
given painting input. Through their body embodiment, the audience can navigate
the painterly world and play with the magical realism (i.e., interactive
painterly objects), fostering meaningful experiences via interactions. The
Painterly Reality is subsequently projected through an Augmented Reality Mirror
as a live painting and displayed in front of the audience. Hence, the audience
can gain enhanced experiences through bodily engagement while simultaneously
viewing and appreciating the live painting. The paper implements the proposed
approach as an interactive artwork, entitled ""Everyday Conjunctive,"" with Fong
Tse Ka's painting and installs in a local museum, which successfully enhances
audience experience through bodily engagement.","['Aven Le Zhou', 'Kang Zhang', 'David Yip']",2023-12-02T08:33:34Z,http://arxiv.org/abs/2312.01067v1
"Exploring the current applications and potential of extended reality for
  environmental sustainability in manufacturing","In response to the transformation towards Industry 5.0, there is a growing
call for manufacturing systems that prioritize environmental sustainability,
alongside the emerging application of digital tools. Extended Reality (XR) -
including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) -
is one of the technologies identified as an enabler for Industry 5.0. XR could
potentially also be a driver for more sustainable manufacturing: however, its
potential environmental benefits have received limited attention. This paper
aims to explore the current manufacturing applications and research within the
field of XR technology connected to the environmental sustainability principle.
The objectives of this paper are two-fold: (1) Identify the currently explored
use cases of XR technology in literature and research, addressing environmental
sustainability in manufacturing; (2) Provide guidance and references for
industry and companies to use cases, toolboxes, methodologies, and workflows
for implementing XR in environmental sustainable manufacturing practices. Based
on the categorization of sustainability indicators, developed by the National
Institute of Standards and Technology (NIST), the authors analyzed and mapped
the current literature, with criteria of pragmatic XR use cases for
manufacturing. The exploration resulted in a mapping of the current
applications and use cases of XR technology within manufacturing that has the
potential to drive environmental sustainability. The results are presented as
stated use-cases with reference to the literature, contributing as guidance and
inspiration for future researchers or implementations in industry, using XR as
a driver for environmental sustainability. Furthermore, the authors open up the
discussion for future work and research to increase the attention of XR as a
driver for environmental sustainability.","['Huizhong Cao', 'Henrik Söderlund', 'Mélanie Derspeisse', 'Björn Johansson']",2023-12-29T13:18:01Z,http://arxiv.org/abs/2312.17595v1
Structural Imbalance Aware Graph Augmentation Learning,"Graph machine learning (GML) has made great progress in node classification,
link prediction, graph classification and so on. However, graphs in reality are
often structurally imbalanced, that is, only a few hub nodes have a denser
local structure and higher influence. The imbalance may compromise the
robustness of existing GML models, especially in learning tail nodes. This
paper proposes a selective graph augmentation method (SAug) to solve this
problem. Firstly, a Pagerank-based sampling strategy is designed to identify
hub nodes and tail nodes in the graph. Secondly, a selective augmentation
strategy is proposed, which drops the noisy neighbors of hub nodes on one side,
and discovers the latent neighbors and generates pseudo neighbors for tail
nodes on the other side. It can also alleviate the structural imbalance between
two types of nodes. Finally, a GNN model will be retrained on the augmented
graph. Extensive experiments demonstrate that SAug can significantly improve
the backbone GNNs and achieve superior performance to its competitors of graph
augmentation methods and hub/tail aware methods.","['Zulong Liu', 'Kejia-Chen', 'Zheng Liu']",2023-03-24T02:13:32Z,http://arxiv.org/abs/2303.13757v1
"Dynamical probability, particle trajectories and completion of
  traditional quantum mechanics","Maintaining the position that the wave function $\psi$ provides a complete
description of state, the traditional formalism of quantum mechanics is
augmented by introducing continuous trajectories for particles which are sample
paths of a stochastic process determined (including the underlying probability
space) by $\psi$. In the resulting formalism, problems relating to measurements
and objective reality are solved as in Bohmian mechanics (without sharing its
weak points). The pitfalls of Nelson's stochastic mechanics are also avoided.",['Tulsi Dass'],2005-05-25T11:32:52Z,http://arxiv.org/abs/quant-ph/0505190v1
"Augmented Reality, Cyber-Physical Systems, and Feedback Control for
  Additive Manufacturing: A Review","Our objective in this paper is to review the application of feedback ideas in
the area of additive manufacturing. Both the application of feedback control to
the 3D printing process, and the application of feedback theory to enable users
to interact better with machines, are reviewed. Where appropriate,
opportunities for future work are highlighted.","['Hugo Lhachemi', 'Ammar Malik', 'Robert Shorten']",2019-03-05T13:21:44Z,http://arxiv.org/abs/1903.01808v1
Recent Advances in 3D Object and Hand Pose Estimation,"3D object and hand pose estimation have huge potentials for Augmented
Reality, to enable tangible interfaces, natural interfaces, and blurring the
boundaries between the real and virtual worlds. In this chapter, we present the
recent developments for 3D object and hand pose estimation using cameras, and
discuss their abilities and limitations and the possible future development of
the field.",['Vincent Lepetit'],2020-06-10T16:25:28Z,http://arxiv.org/abs/2006.05927v1
Enabling immersive experiences in challenging network conditions,"Immersive experiences, such as remote collaboration and augmented and virtual
reality, require delivery of large volumes of data with consistent ultra-low
latency across wireless networks in fluctuating network conditions. We describe
the high-level design behind a data delivery solution that meets these
requirements and provide synthetic simulations and test results running in
network conditions based on real-world measurements demonstrating the efficacy
of the solution.","['Pooja Aggarwal', 'Michael Luby', 'Lorenz Minder']",2023-04-07T16:55:52Z,http://arxiv.org/abs/2304.03732v1
"Ergonomic-Centric Holography: Optimizing Realism,Immersion, and Comfort
  for Holographic Display","We introduce ergonomic-centric holography, an algorithmic framework that
simultaneously optimizes for realistic incoherent defocus, unrestricted pupil
movements in the eye box, and high-order diffractions for filtering-free
holography. The proposed method outperforms prior algorithms on holographic
display prototypes operating in unfiltered and pupil-mimicking modes, offering
the potential to enhance next-generation virtual and augmented reality
experiences.","['Liang Shi', 'DongHun Ryu', 'Wojciech Matusik']",2023-06-13T21:08:05Z,http://arxiv.org/abs/2306.08138v2
"Augmenting the fine beam tube: From hybrid measurements to magnetic
  field visualization","We present an Augmented Reality (AR) enhanced and networked fine beam tube
experiment for undergraduate physics education. In order to determine the
charge-to-mass ratio of the electron students are able to record all
measurement values digitally within the AR-environment wearing a head-mounted
AR device. Besides more accurate determination of $\frac{e}{m_e}$ it offers the
possibility to overlay additional data such as a magnetic field visualization
or formulas in order to foster the students' understanding of relations between
experiment and theory.","['Oliver Bodensiek', 'Doerte Sonntag', 'Nils Wendorff', 'Georgia Albuquerque', 'Marcus Magnor']",2019-01-03T08:01:52Z,http://arxiv.org/abs/1901.00859v1
Binaural Audio Source Remixing with Microphone Array Listening Devices,"Augmented listening devices, such as hearing aids and augmented reality
headsets, enhance human perception by changing the sounds that we hear.
Microphone arrays can improve the performance of listening systems in noisy
environments, but most array-based listening systems are designed to isolate a
single sound source from a mixture. This work considers a source-remixing
filter that alters the relative level of each source independently. Remixing
rather than separating sounds can help to improve perceptual transparency: it
causes less distortion to the signal spectrum and especially to the interaural
cues that humans use to localize sounds in space.","['Ryan M. Corey', 'Andrew C. Singer']",2020-04-24T19:34:38Z,http://arxiv.org/abs/2004.11956v1
Reimagining Retrieval Augmented Language Models for Answering Queries,"We present a reality check on large language models and inspect the promise
of retrieval augmented language models in comparison. Such language models are
semi-parametric, where models integrate model parameters and knowledge from
external data sources to make their predictions, as opposed to the parametric
nature of vanilla large language models. We give initial experimental findings
that semi-parametric architectures can be enhanced with views, a query
analyzer/planner, and provenance to make a significantly more powerful system
for question answering in terms of accuracy and efficiency, and potentially for
other NLP tasks","['Wang-Chiew Tan', 'Yuliang Li', 'Pedro Rodriguez', 'Richard James', 'Xi Victoria Lin', 'Alon Halevy', 'Scott Yih']",2023-06-01T18:08:51Z,http://arxiv.org/abs/2306.01061v1
"Beyond Screens: Supporting Co-located Augmented Reality Experiences with
  Smart Home Devices","We introduce Spooky Spirits, an AR game that makes novel use of everyday
smart home devices to support co-located play. Recent exploration of co-located
AR experiences consists mainly of digital visual augmentations on mobile or
head-mounted screens. In this work, we leverage widely adopted smart lightbulbs
to expand AR capabilities beyond the digital and into the physical world,
further leveraging the physicality of users' shared environment.","['Ava Robinson', 'Yu Jiang Tham', 'Rajan Vaish', 'Andrés Monroy-Hernández']",2023-09-01T05:20:35Z,http://arxiv.org/abs/2309.00256v1
Registration made easy -- standalone orthopedic navigation with HoloLens,"In surgical navigation, finding correspondence between preoperative plan and
intraoperative anatomy, the so-called registration task, is imperative. One
promising approach is to intraoperatively digitize anatomy and register it with
the preoperative plan. State-of-the-art commercial navigation systems implement
such approaches for pedicle screw placement in spinal fusion surgery. Although
these systems improve surgical accuracy, they are not gold standard in clinical
practice. Besides economical reasons, this may be due to their difficult
integration into clinical workflows and unintuitive navigation feedback.
Augmented Reality has the potential to overcome these limitations.
Consequently, we propose a surgical navigation approach comprising
intraoperative surface digitization for registration and intuitive holographic
navigation for pedicle screw placement that runs entirely on the Microsoft
HoloLens. Preliminary results from phantom experiments suggest that the method
may meet clinical accuracy requirements.","['Florentin Liebmann', 'Simon Roner', 'Marco von Atzigen', 'Florian Wanivenhaus', 'Caroline Neuhaus', 'José Spirig', 'Davide Scaramuzza', 'Reto Sutter', 'Jess Snedeker', 'Mazda Farshad', 'Philipp Fürnstahl']",2020-01-17T09:22:21Z,http://arxiv.org/abs/2001.06209v1
"Parametric Modelling Within Immersive Environments: Building a Bridge
  Between Existing Tools and Virtual Reality Headsets","Even though architectural modelling radically evolved over the course of its
history, the current integration of Augmented Reality (AR) and Virtual
Reality(VR) components in the corresponding design tasks is mostly limited to
enhancing visualisation. Little to none of these tools attempt to tackle the
challenge of modelling within immersive environments, that calls for new input
modalities in order to move away from the traditional mouse and keyboard
combination. In fact, relying on 2D devices for 3D manipulations does not seem
to be effective as it does not offer the same degrees of freedom. We therefore
present a solution that brings VR modelling capabilities to Grasshopper, a
popular parametric design tool. Together with its associated proof-of-concept
application, our extension offers a glimpse at new perspectives in that field.
By taking advantage of them,one can edit geometries with real-time feedback on
the generated models, without ever leaving the virtual environment. The
distinctive characteristics of VR applications provide a range of benefits
without obstructing design activities. The designer can indeed experience the
architectural models at full scale from a realistic point-of-view and truly
feels immersed right next to them.","['Adrien Coppens', 'Tom Mens', 'Mohamed-Anis Gallas']",2019-06-13T07:58:48Z,http://arxiv.org/abs/1906.05532v1
"Walking Through an Exploded Star: Rendering Supernova Remnant Cassiopeia
  A into Virtual Reality","NASA and other astrophysical data of the Cassiopeia A supernova remnant have
been rendered into a three-dimensional virtual reality (VR) and augmented
reality (AR) program, the first of its kind. This data-driven experience of a
supernova remnant allows viewers to walk inside the leftovers from the
explosion of a massive star, select the parts of the supernova remnant to
engage with, and access descriptive texts on what the materials are. The basis
of this program is a unique 3D model of the 340-year old remains of a stellar
explosion, made by combining data from the NASA Chandra X-ray Observatory,
Spitzer Space Telescope, and ground-based facilities. A collaboration between
the Smithsonian Astrophysical Observatory and Brown University allowed the 3D
astronomical data collected on Cassiopeia A to be featured in the VR/AR
program, which is an innovation in digital technologies with public, education,
and research-based impacts.","['Kimberly K. Arcand', 'Elaine Jiang', 'Sara Price', 'Megan Watzke', 'Tom Sgouros', 'Peter Edmonds']",2018-12-15T05:29:04Z,http://arxiv.org/abs/1812.06237v1
"Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative
  Information Analysis","Immersive environments have gradually become standard for visualizing and
analyzing large or complex datasets that would otherwise be cumbersome, if not
impossible, to explore through smaller scale computing devices. However, this
type of workspace often proves to possess limitations in terms of interaction,
flexibility, cost and scalability.
  In this paper we introduce a novel immersive environment called Dataspace,
which features a new combination of heterogeneous technologies and methods of
interaction towards creating a better team workspace. Dataspace provides 15
high-resolution displays that can be dynamically reconfigured in space through
robotic arms, a central table where information can be projected, and a unique
integration with augmented reality (AR) and virtual reality (VR) headsets and
other mobile devices. In particular, we contribute novel interaction
methodologies to couple the physical environment with AR and VR technologies,
enabling visualization of complex types of data and mitigating the scalability
issues of existing immersive environments.
  We demonstrate through four use cases how this environment can be effectively
used across different domains and reconfigured based on user requirements.
  Finally, we compare Dataspace with existing technologies, summarizing the
trade-offs that should be considered when attempting to build better
collaborative workspaces for the future.","['Marco Cavallo', 'Mishal Dholakia', 'Matous Havlena', 'Kenneth Ocheltree', 'Mark Podlaseck']",2019-03-08T23:53:10Z,http://arxiv.org/abs/1903.03700v1
3D Virtual Garment Modeling from RGB Images,"We present a novel approach that constructs 3D virtual garment models from
photos. Unlike previous methods that require photos of a garment on a human
model or a mannequin, our approach can work with various states of the garment:
on a model, on a mannequin, or on a flat surface. To construct a complete 3D
virtual model, our approach only requires two images as input, one front view
and one back view. We first apply a multi-task learning network called JFNet
that jointly predicts fashion landmarks and parses a garment image into
semantic parts. The predicted landmarks are used for estimating sizing
information of the garment. Then, a template garment mesh is deformed based on
the sizing information to generate the final 3D model. The semantic parts are
utilized for extracting color textures from input images. The results of our
approach can be used in various Virtual Reality and Mixed Reality applications.","['Yi Xu', 'Shanglin Yang', 'Wei Sun', 'Li Tan', 'Kefeng Li', 'Hui Zhou']",2019-07-31T21:47:52Z,http://arxiv.org/abs/1908.00114v1
"Enabling Humans to Plan Inspection Paths Using a Virtual Reality
  Interface","In this work, we investigate whether humans can manually generate
high-quality robot paths for optical inspections. Typically, automated
algorithms are used to solve the inspection planning problem. The use of
automated algorithms implies that specialized knowledge from users is needed to
set up the algorithm. We aim to replace this need for specialized experience,
by entrusting a non-expert human user with the planning task. We augment this
user with intuitive visualizations and interactions in virtual reality. To
investigate if humans can generate high-quality inspection paths, we perform a
user study in which users from different experience categories, generate
inspection paths with the proposed virtual reality interface. From our study,
it can be concluded that users without experience can generate high-quality
inspection paths: The median inspection quality of user generated paths ranged
between 66-81\% of the quality of a state-of-the-art automated algorithm on
various inspection planning scenarios. We noticed however, a sizable variation
in the performance of users, which is a result of some typical user behaviors.
These behaviors are discussed, and possible solutions are provided.","['Boris Bogaerts', 'Seppe Sels', 'Steve Vanlanduit', 'Rudi Penne']",2019-09-13T08:15:36Z,http://arxiv.org/abs/1909.06077v1
Exploration of Hands-free Text Entry Techniques For Virtual Reality,"Text entry is a common activity in virtual reality (VR) systems. There is a
limited number of available hands-free techniques, which allow users to carry
out text entry when users' hands are busy such as holding items or hand-based
devices are not available. The most used hands-free text entry technique is
DwellType, where a user selects a letter by dwelling over it for a specific
period. However, its performance is limited due to the fixed dwell time for
each character selection. In this paper, we explore two other hands-free text
entry mechanisms in VR: BlinkType and NeckType, which leverage users' eye
blinks and neck's forward and backward movements to select letters. With a user
study, we compare the performance of the two techniques with DwellType. Results
show that users can achieve an average text entry rate of 13.47, 11.18 and
11.65 words per minute with BlinkType, NeckType, and DwellType, respectively.
Users' subjective feedback shows BlinkType as the preferred technique for text
entry in VR.","['Xueshi Lu', 'Difeng Yu', 'Hai-Ning Liang', 'Wenge Xu', 'Yuzheng Chen', 'Xiang Li', 'Khalad Hasan']",2020-10-07T07:59:31Z,http://arxiv.org/abs/2010.03247v1
"Extended Reality (XR) Remote Research: a Survey of Drawbacks and
  Opportunities","Extended Reality (XR) technology - such as virtual and augmented reality - is
now widely used in Human Computer Interaction (HCI), social science and
psychology experimentation. However, these experiments are predominantly
deployed in-lab with a co-present researcher. Remote experiments, without
co-present researchers, have not flourished, despite the success of remote
approaches for non-XR investigations. This paper summarises findings from a
30-item survey of 46 XR researchers to understand perceived limitations and
benefits of remote XR experimentation. Our thematic analysis identifies
concerns common with non-XR remote research, such as participant recruitment,
as well as XR-specific issues, including safety and hardware variability. We
identify potential positive affordances of XR technology, including leveraging
data collection functionalities builtin to HMDs (e.g. hand, gaze tracking) and
the portability and reproducibility of an experimental setting. We suggest that
XR technology could be conceptualised as an interactive technology and a
capable data-collection device suited for remote experimentation.","['Jack Ratcliffe', 'Francesco Soave', 'Nick Bryan-Kinns', 'Laurissa Tokarchuk', 'Ildar Farkhatdinov']",2021-01-20T10:02:29Z,http://arxiv.org/abs/2101.08046v1
"Congruence and Plausibility, not Presence?! Pivotal Conditions for XR
  Experiences and Effects, a Novel Model","Presence often is considered the most important quale describing the
subjective feeling of being in a computer-generated and/or computer-mediated
virtual environment. The identification and separation of orthogonal presence
components, i.e., the place illusion and the plausibility illusion, has been an
accepted theoretical model describing Virtual Reality (VR) experiences for some
time. This perspective article challenges this presence-oriented VR theory.
First, we argue that a place illusion cannot be the major construct to describe
the much wider scope of Virtual, Augmented, and Mixed Reality (VR, AR, MR: or
XR for short). Second, we argue that there is no plausibility illusion but
merely plausibility, and we derive the place illusion caused by congruent and
plausible generation of spatial cues, and similarly for all the current model's
so-defined illusions. Finally, we propose congruence and plausibility to become
the central essential conditions in a novel theoretical model describing XR
experiences and effects.","['Marc Erich Latoschik', 'Carolin Wienrich']",2021-04-10T19:25:17Z,http://arxiv.org/abs/2104.04846v5
Semi-Autonomous Planning and Visualization in Virtual Reality,"Virtual reality (VR) interfaces for robots provide a three-dimensional (3D)
view of the robot in its environment, which allows people to better plan
complex robot movements in tight or cluttered spaces. In our prior work, we
created a VR interface to allow for the teleoperation of a humanoid robot. As
detailed in this paper, we have now focused on a human-in-the-loop planner
where the operator can send higher level manipulation and navigation goals in
VR through functional waypoints, visualize the results of a robot planner in
the 3D virtual space, and then deny, alter or confirm the plan to send to the
robot. In addition, we have adapted our interface to also work for a mobile
manipulation robot in addition to the humanoid robot. For a video demonstration
please see the accompanying video at https://youtu.be/wEHZug_fxrA.","['Gregory LeMasurier', 'Jordan Allspaw', 'Holly A. Yanco']",2021-04-23T21:48:05Z,http://arxiv.org/abs/2104.11827v1
"Latency and Information Freshness in Multipath Communications for
  Virtual Reality","Wireless Virtual Reality (VR) and Augmented Reality (AR) will contribute to
people increasingly working and socializing remotely. However, the VR/AR
experience is very susceptible to various delays and timing discrepancies,
which can lead to motion sickness and discomfort. This paper models and
exploits the existence of multiple paths and redundancy to improve the timing
performance of wireless VR communications. We consider Multiple Description
Coding (MDC), a scheme where the video stream is encoded in Q streams (Q = 2 in
this paper) known as descriptors and delivered independently over multiple
paths. We also consider an alternating scheme, that simply switches between the
paths. We analyze the full distribution of two relevant metrics: the packet
delay and the Peak Age of Information (PAoI), which measures the freshness of
the information at the receiver. The results show interesting trade-offs
between picture quality, frame rate, and latency: full duplication results in
fewer lost frames, but a higher latency than schemes with less redundancy. Even
the simple alternating scheme can outperform duplication in terms of PAoI, but
MDC can exploit the independent decodability of the descriptors to deliver a
basic version of the frames faster, while still getting the full-quality frames
with a slightly higher delay.","['Federico Chiariotti', 'Beatriz Soret', 'Petar Popovski']",2021-06-10T10:47:54Z,http://arxiv.org/abs/2106.05652v1
"Virtual Reality Digital Twin and Environment for Troubleshooting
  Lunar-based Infrastructure Assembly Failures","Humans and robots will need to collaborate in order to create a sustainable
human lunar presence by the end of the 2020s. This includes cases in which a
human will be required to teleoperate an autonomous rover that has encountered
an instrument assembly failure. To aid teleoperators in the troubleshooting
process, we propose a virtual reality digital twin placed in a simulated
environment. Here, the operator can virtually interact with a digital version
of the rover and mechanical arm that uses the same controls and kinematic
model. The user can also adopt the egocentric (a first person view through
using stereoscopic passthrough) and exocentric (a third person view where the
operator can virtually walk around the environment and rover as if they were on
site) view. We also discuss our metrics for evaluating the differences between
our digital and physical robot, as well as the experimental concept based on
real and applicable missions, and future work that would compare our platform
to traditional troubleshooting methods.","['Phaedra S. Curlin', 'Madaline A. Muniz', 'Mason M. Bell', 'Alexis A. Muniz', 'Jack O. Burns']",2022-03-05T19:36:16Z,http://arxiv.org/abs/2203.02810v1
"When Internet of Things meets Metaverse: Convergence of Physical and
  Cyber Worlds","In recent years, the Internet of Things (IoT) is studied in the context of
the Metaverse to provide users immersive cyber-virtual experiences in mixed
reality environments. This survey introduces six typical IoT applications in
the Metaverse, including collaborative healthcare, education, smart city,
entertainment, real estate, and socialization. In the IoT-inspired Metaverse,
we also comprehensively survey four pillar technologies that enable augmented
reality (AR) and virtual reality (VR), namely, responsible artificial
intelligence (AI), high-speed data communications, cost-effective mobile edge
computing (MEC), and digital twins. According to the physical-world demands, we
outline the current industrial efforts and seven key requirements for building
the IoT-inspired Metaverse: immersion, variety, economy, civility,
interactivity, authenticity, and independence. In addition, this survey
describes the open issues in the IoT-inspired Metaverse, which need to be
addressed to eventually achieve the convergence of physical and cyber worlds.","['Kai Li', 'Yingping Cui', 'Weicai Li', 'Tiejun Lv', 'Xin Yuan', 'Shenghong Li', 'Wei Ni', 'Meryem Simsek', 'Falko Dressler']",2022-08-29T11:17:54Z,http://arxiv.org/abs/2208.13501v1
"Point Cloud Registration of non-rigid objects in sparse 3D Scans with
  applications in Mixed Reality","Point Cloud Registration is the problem of aligning the corresponding points
of two 3D point clouds referring to the same object. The challenges include
dealing with noise and partial match of real-world 3D scans. For non-rigid
objects, there is an additional challenge of accounting for deformations in the
object shape that happen to the object in between the two 3D scans. In this
project, we study the problem of non-rigid point cloud registration for use
cases in the Augmented/Mixed Reality domain. We focus our attention on a
special class of non-rigid deformations that happen in rigid objects with parts
that move relative to one another about joints, for example, robots with hands
and machines with hinges. We propose an efficient and robust point-cloud
registration workflow for such objects and evaluate it on real-world data
collected using Microsoft Hololens 2, a leading Mixed Reality device.",['Manorama Jha'],2022-12-07T18:54:32Z,http://arxiv.org/abs/2212.03856v2
"ChromaCorrect: Prescription Correction in Virtual Reality Headsets
  through Perceptual Guidance","A large portion of today's world population suffer from vision impairments
and wear prescription eyeglasses. However, eyeglasses causes additional bulk
and discomfort when used with augmented and virtual reality headsets, thereby
negatively impacting the viewer's visual experience. In this work, we remedy
the usage of prescription eyeglasses in Virtual Reality (VR) headsets by
shifting the optical complexity completely into software and propose a
prescription-aware rendering approach for providing sharper and immersive VR
imagery. To this end, we develop a differentiable display and visual perception
model encapsulating display-specific parameters, color and visual acuity of
human visual system and the user-specific refractive errors. Using this
differentiable visual perception model, we optimize the rendered imagery in the
display using stochastic gradient-descent solvers. This way, we provide
prescription glasses-free sharper images for a person with vision impairments.
We evaluate our approach on various displays, including desktops and VR
headsets, and show significant quality and contrast improvements for users with
vision impairments.","['Ahmet Güzel', 'Jeanne Beyazian', 'Praneeth Chakravarthula', 'Kaan Akşit']",2022-12-08T13:30:17Z,http://arxiv.org/abs/2212.04264v1
"Imitation Learning based Auto-Correction of Extrinsic Parameters for A
  Mixed-Reality Setup","In this paper, we discuss an imitation learning based method for reducing the
calibration error for a mixed reality system consisting of a vision sensor and
a projector. Unlike a head mounted display, in this setup, augmented
information is available to a human subject via the projection of a scene into
the real world. Inherently, the camera and projector need to be calibrated as a
stereo setup to project accurate information in 3D space. Previous calibration
processes require multiple recording and parameter tuning steps to achieve the
desired calibration, which is usually time consuming process. In order to avoid
such tedious calibration, we train a CNN model to iteratively correct the
extrinsic offset given a QR code and a projected pattern. We discuss the
overall system setup, data collection for training, and results of the
auto-correction model.","['Shubham Sonawani', 'Yifan Zhou', 'Heni Ben Amor']",2022-12-16T21:34:33Z,http://arxiv.org/abs/2212.08720v1
"Virtual reality for the analysis and visualization of scientific
  numerical models","The complexity of the data generated by (magneto)-hydrodynamic (HD/MHD)
simulations requires advanced tools for their analysis and visualization. The
dramatic improvements in virtual reality (VR) technologies have inspired us to
seek the long-term goal of creating VR tools for scientific model analysis and
visualization that would allow researchers to study and perform data analysis
on their models within an immersive environment. Here, we report the results
obtained at INAF-Osservatorio Astronomico di Palermo in the development of
these tools, which would allow for the exploration of 3D models interactively,
resulting in highly detailed analysis that cannot be performed with traditional
data visualization and analysis platforms. Additionally, these VR-based tools
offer the ability to produce high-impact VR content for efficient audience
engagement and awareness.","['S. Orlando', 'M. Miceli', 'U. Lo Cicero', 'S. Ustamujic']",2023-01-26T08:42:03Z,http://arxiv.org/abs/2301.11334v1
"A Fusion Model: Towards a Virtual, Physical and Cognitive Integration
  and its Principles","Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital
twin, Metaverse and other related digital technologies have attracted much
attention in recent years. These new emerging technologies are changing the
world significantly. This research introduces a fusion model, i.e. Fusion
Universe (FU), where the virtual, physical, and cognitive worlds are merged
together. Therefore, it is crucial to establish a set of principles for the
fusion model that is compatible with our physical universe laws and principles.
This paper investigates several aspects that could affect immersive and
interactive experience; and proposes the fundamental principles for Fusion
Universe that can integrate physical and virtual world seamlessly.","['Hao Lan Zhang', 'Yun Xue', 'Yifan Lu', 'Sanghyuk Lee']",2023-05-17T06:34:22Z,http://arxiv.org/abs/2305.09992v1
"Who's Watching Me?: Exploring the Impact of Audience Familiarity on
  Player Performance, Experience, and Exertion in Virtual Reality Exergames","Familiarity with audiences plays a significant role in shaping individual
performance and experience across various activities in everyday life. This
study delves into the impact of familiarity with non-playable character (NPC)
audiences on player performance and experience in virtual reality (VR)
exergames. By manipulating of NPC appearance (face and body shape) and voice
familiarity, we explored their effect on game performance, experience, and
exertion. The findings reveal that familiar NPC audiences have a positive
impact on performance, creating a more enjoyable gaming experience, and leading
players to perceive less exertion. Moreover, individuals with higher levels of
self-consciousness exhibit heightened sensitivity to the familiarity with NPC
audiences. Our results shed light on the role of familiar NPC audiences in
enhancing player experiences and provide insights for designing more engaging
and personalized VR exergame environments.","['Zixuan Guo', 'Wenge Xu', 'Jialin Zhang', 'Hongyu Wang', 'Cheng-Hung Lo', 'Hai-Ning Liang']",2023-10-23T12:37:02Z,http://arxiv.org/abs/2310.14867v1
"Smell of Fire Increases Behavioural Realism in Virtual Reality: A Case
  Study on a Recreated MGM Grand Hotel Fire","Virtual reality allows creating highly immersive visual and auditory
experiences, making users feel physically present in the environment. This
makes it an ideal platform to simulate dangerous scenarios, including fire
evacuation, and study human behaviour without exposing users to harmful
elements. However, human perception of the surroundings is based on the
integration of multiple sensory cues (visual, auditory, tactile, or/and
olfactory) present in the environment. When some of the sensory stimuli are
missing in the virtual experience, it can break the illusion of being there in
the environment and could lead to actions that deviate from normal behaviour.
In this work, we added an olfactory cue in a well-documented historic hotel
fire scenario that was recreated in VR, and examined the effects of the
olfactory cue on human behaviour. We conducted a between subject study on 40
naive participants. Our results show that the addition of the olfactory cue
could increase behavioural realism. We found that 80% of the studied actions
for the VR with olfactory cue condition matched the ones performed by the
survivors. In comparison, only 40% of the participants' actions for VR only
condition were similar to the survivors.","['Humayun Khan', 'Daniel Nilsson']",2023-11-13T20:55:56Z,http://arxiv.org/abs/2311.09246v1
"Testing Human-Robot Interaction in Virtual Reality: Experience from a
  Study on Speech Act Classification","In recent years, an increasing number of Human-Robot Interaction (HRI)
approaches have been implemented and evaluated in Virtual Reality (VR), as it
allows to speed-up design iterations and makes it safer for the final user to
evaluate and master the HRI primitives. However, identifying the most suitable
VR experience is not straightforward. In this work, we evaluate how, in a smart
agriculture scenario, immersive and non-immersive VR are perceived by users
with respect to a speech act understanding task. In particular, we collect
opinions and suggestions from the 81 participants involved in both experiments
to highlight the strengths and weaknesses of these different experiences.","['Sara Kaszuba', 'Sandeep Reddy Sabbella', 'Francesco Leotta', 'Pascal Serrarens', 'Daniele Nardi']",2024-01-09T13:08:13Z,http://arxiv.org/abs/2401.04534v1
CMC: Few-shot Novel View Synthesis via Cross-view Multiplane Consistency,"Neural Radiance Field (NeRF) has shown impressive results in novel view
synthesis, particularly in Virtual Reality (VR) and Augmented Reality (AR),
thanks to its ability to represent scenes continuously. However, when just a
few input view images are available, NeRF tends to overfit the given views and
thus make the estimated depths of pixels share almost the same value. Unlike
previous methods that conduct regularization by introducing complex priors or
additional supervisions, we propose a simple yet effective method that
explicitly builds depth-aware consistency across input views to tackle this
challenge. Our key insight is that by forcing the same spatial points to be
sampled repeatedly in different input views, we are able to strengthen the
interactions between views and therefore alleviate the overfitting problem. To
achieve this, we build the neural networks on layered representations
(\textit{i.e.}, multiplane images), and the sampling point can thus be
resampled on multiple discrete planes. Furthermore, to regularize the unseen
target views, we constrain the rendered colors and depths from different input
views to be the same. Although simple, extensive experiments demonstrate that
our proposed method can achieve better synthesis quality over state-of-the-art
methods.","['Hanxin Zhu', 'Tianyu He', 'Zhibo Chen']",2024-02-26T09:04:04Z,http://arxiv.org/abs/2402.16407v1
"Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D
  Spatio-Temporal Data in Mixed Reality","Tangible interfaces in mixed reality (MR) environments allow for intuitive
data interactions. Tangible cubes, with their rich interaction affordances,
high maneuverability, and stable structure, are particularly well-suited for
exploring multi-dimensional data types. However, the design potential of these
cubes is underexplored. This study introduces a design space for tangible cubes
in MR, focusing on interaction space, visualization space, sizes, and
multiplicity. Using spatio-temporal data, we explored the interaction
affordances of these cubes in a workshop (N=24). We identified unique
interactions like rotating, tapping, and stacking, which are linked to
augmented reality (AR) visualization commands. Integrating user-identified
interactions, we created a design space for tangible-cube interactions and
visualization. A prototype visualizing global health spending with small cubes
was developed and evaluated, supporting both individual and combined cube
manipulation. This research enhances our grasp of tangible interaction in MR,
offering insights for future design and application in diverse data contexts.","['Shuqi He', 'Haonan Yao', 'Luyan Jiang', 'Kaiwen Li', 'Nan Xiang', 'Yue Li', 'Hai-Ning Liang', 'Lingyun Yu']",2024-03-11T16:47:39Z,http://arxiv.org/abs/2403.06891v1
"Meta-Object: Interactive and Multisensory Virtual Object Learned from
  the Real World for the Post-Metaverse","With the proliferation of wearable Augmented Reality/Virtual Reality (AR/VR)
devices, ubiquitous virtual experiences seamlessly integrate into daily life
through metaverse platforms. To support immersive metaverse experiences akin to
reality, we propose a next-generation virtual object, a meta-object, a
property-embedded virtual object that contains interactive and multisensory
characteristics learned from the real world. Current virtual objects differ
significantly from real-world objects due to restricted sensory feedback based
on limited physical properties. To leverage meta-objects in the metaverse,
three key components are needed: meta-object modeling and property embedding,
interaction-adaptive multisensory feedback, and an intelligence
simulation-based post-metaverse platform. Utilizing meta-objects that enable
both on-site and remote users to interact as if they were engaging with real
objects could contribute to the advent of the post-metaverse era through
wearable AR/VR devices.","['Dooyoung Kim', 'Taewook Ha', 'Jinseok Hong', 'Seonji Kim', 'Selin Choi', 'Heejeong Ko', 'Woontack Woo']",2024-04-26T06:22:21Z,http://arxiv.org/abs/2404.17179v2
From Virtual Gains to Real Pains: Potential Harms of Immersive Exergames,"Digitalization and virtualization are parts of our everyday lives in almost
all aspects ranging from work, education, and communication to entertainment. A
novel step in this direction is the widespread interest in extended reality
(XR) [2]. The newest consumer-ready head-mounted displays (HMD) such as Meta
Quest 3 or Apple Vision Pro, have reached unprecedented levels of visual
fidelity, interaction capabilities, and computational power. The built-in
pass-through features of these headsets enable both virtual reality (VR) and
augmented reality (AR) with the same devices. However, the immersive nature of
these experiences is not the only groundbreaking difference from established
forms of media.","['Sebastian Cmentowski', 'Sukran Karaosmanoglu', 'Frank Steinicke']",2024-04-23T17:48:59Z,http://arxiv.org/abs/2405.05915v1
Ubiquitous Talker: Spoken Language Interaction with Real World Objects,"Augmented reality is a research area that tries to embody an electronic
information space within the real world, through computational devices. A
crucial issue within this area, is the recognition of real world objects or
situations.
  In natural language processing, it is much easier to determine
interpretations of utterances, even if they are ill-formed, when the context or
situation is fixed. We therefore introduce robust, natural language processing
into a system of augmented reality with situation awareness. Based on this
idea, we have developed a portable system, called the Ubiquitous Talker. This
consists of an LCD display that reflects the scene at which a user is looking
as if it is a transparent glass, a CCD camera for recognizing real world
objects with color-bar ID codes, a microphone for recognizing a human voice and
a speaker which outputs a synthesized voice. The Ubiquitous Talker provides its
user with some information related to a recognized object, by using the display
and voice. It also accepts requests or questions as voice inputs. The user
feels as if he/she is talking with the object itself through the system.","['Katashi Nagao', 'Jun Rekimoto']",1995-05-23T07:14:19Z,http://arxiv.org/abs/cmp-lg/9505038v1
When Augmented Reality Meets Big Data,"With computing and sensing woven into the fabric of everyday life, we live in
an era where we are awash in a flood of data from which we can gain rich
insights. Augmented reality (AR) is able to collect and help analyze the
growing torrent of data about user engagement metrics within our personal
mobile and wearable devices. This enables us to blend information from our
senses and the digitalized world in a myriad of ways that was not possible
before. AR and big data have a logical maturity that inevitably converge them.
The tread of harnessing AR and big data to breed new interesting applications
is starting to have a tangible presence. In this paper, we explore the
potential to capture value from the marriage between AR and big data
technologies, following with several challenges that must be addressed to fully
realize this potential.","['Zhanpeng Huang', 'Pan Hui', 'Christoph Peylo']",2014-07-27T13:21:10Z,http://arxiv.org/abs/1407.7223v1
"Preprint A Game Based Assistive Tool for Rehabilitation of Dysphonic
  Patients","This is the preprint version of our paper on 3rd International Workshop on
Virtual and Augmented Assistive Technology (VAAT) at IEEE Virtual Reality 2015
(VR2015). An assistive training tool for rehabilitation of dysphonic patients
is designed and developed according to the practical clinical needs. The
assistive tool employs a space flight game as the attractive logic part, and
microphone arrays as input device, which is getting rid of ambient noise by
setting a specific orientation. The therapist can guide the patient to play the
game as well as the voice training simultaneously side by side, while not
interfere the patient voice. The voice information can be recorded and
extracted for evaluating the long-time rehabilitation progress. This paper
outlines a design science approach for the development of an initial useful
software prototype of such a tool, considering 'Intuitive', 'Entertainment',
'Incentive' as main design factors.","['Zhihan Lv', 'Chantal Esteve', 'Javier Chirivella', 'Pablo Gagliardo']",2015-04-04T17:43:54Z,http://arxiv.org/abs/1504.01030v2
"WalkieLokie: Relative Positioning for Augmented Reality Using a Dummy
  Acoustic Speaker","We propose and implement a novel relative positioning system, WalkieLokie, to
enable more kinds of Augmented Reality applications, e.g., virtual shopping
guide, virtual business card sharing. WalkieLokie calculates the distance and
direction between an inquiring user and the corresponding target. It only
requires a dummy speaker binding to the target and broadcasting inaudible
acoustic signals. Then the user walking around can obtain the position using a
smart device. The key insight is that when a user walks, the distance between
the smart device and the speaker changes; and the pattern of displacement
(variance of distance) corresponds to the relative position. We use a
second-order phase locked loop to track the displacement and further estimate
the position. To enhance the accuracy and robustness of our strategy, we
propose a synchronization mechanism to synthesize all estimation results from
different timeslots. We show that the mean error of ranging and direction
estimation is 0.63m and 2.46 degrees respectively, which is accurate even in
case of virtual business card sharing. Furthermore, in the shopping mall where
the environment is quite severe, we still achieve high accuracy of positioning
one dummy speaker, and the mean position error is 1.28m.","['Wenchao Huang', 'Yan Xiong', 'Xiang-Yang Li', 'Yiqing Hu', 'Xufei Mao', 'Panlong Yang']",2015-08-22T08:15:04Z,http://arxiv.org/abs/1508.05477v1
"3D Character Customization for Non-Professional Users in Handheld
  Augmented Reality","In gaming, customizing individual characters, can create personal bonds
between players and their characters. Hence, character customization is a
standard component in many games. While mobile Augmented Reality (AR) games
become popular, to date, no 3D character editor for AR games exists. We
investigate the feasibility of 3D character customization for smartphone-based
AR in an iterative design process.
  Specifically, we present findings from creating AR prototypes in a handheld
AR setting. In a first user study, we found that a tangible AR prototype
resulted in higher hedonistic measures than a camera-based approach. In a
follow up study, we compared the tangible AR prototype with a non-AR
touchscreen version for selection, scaling, translation and rotation tasks in a
3D character customization setting. The tangible AR version resulted in
significantly better results for stimulation and novelty measures than the
non-AR version. At the same time, it maintained a proficient level in pragmatic
measures such as accuracy and efficiency.","['Iris Seidinger', 'Jens Grubert']",2016-07-22T08:00:55Z,http://arxiv.org/abs/1607.06587v1
"Automated capture and delivery of assistive task guidance with an
  eyewear computer: The GlaciAR system","In this paper we describe and evaluate a mixed reality system that aims to
augment users in task guidance applications by combining automated and
unsupervised information collection with minimally invasive video guides. The
result is a self-contained system that we call GlaciAR (Glass-enabled
Contextual Interactions for Augmented Reality), that operates by extracting
contextual interactions from observing users performing actions. GlaciAR is
able to i) automatically determine moments of relevance based on a head motion
attention model, ii) automatically produce video guidance information, iii)
trigger these video guides based on an object detection method, iv) learn
without supervision from observing multiple users and v) operate fully on-board
a current eyewear computer (Google Glass). We describe the components of
GlaciAR together with evaluations on how users are able to use the system to
achieve three tasks. We see this work as a first step toward the development of
systems that aim to scale up the notoriously difficult authoring problem in
guidance systems and where people's natural abilities are enhanced via
minimally invasive visual guidance.","['Teesid Leelasawassuk', 'Dima Damen', 'Walterio Mayol-Cuevas']",2016-12-29T01:10:54Z,http://arxiv.org/abs/1701.02586v1
"Towards an Understanding of the Effects of Augmented Reality Games on
  Disaster Management","Location-based augmented reality games have entered the mainstream with the
nearly overnight success of Niantic's Pok\'emon Go. Unlike traditional video
games, the fact that players of such games carry out actions in the external,
physical world to accomplish in-game objectives means that the large-scale
adoption of such games motivate people, en masse, to do things and go places
they would not have otherwise done in unprecedented ways. The social
implications of such mass-mobilisation of individual players are, in general,
difficult to anticipate or characterise, even for the short-term. In this work,
we focus on disaster relief, and the short- and long-term implications that a
proliferation of AR games like Pok\'emon Go, may have in disaster-prone regions
of the world. We take a distributed cognition approach and focus on one natural
disaster-prone region of New Zealand, the city of Wellington.",['Markus Luczak-Roesch'],2017-02-21T22:52:43Z,http://arxiv.org/abs/1702.06610v1
"Semantic Augmented Reality Environment with Material-Aware Physical
  Interactions","In Augmented Reality (AR) environment, realistic interactions between the
virtual and real objects play a crucial role in user experience. Much of recent
advances in AR has been largely focused on developing geometry-aware
environment, but little has been done in dealing with interactions at the
semantic level. High-level scene understanding and semantic descriptions in AR
would allow effective design of complex applications and enhanced user
experience. In this paper, we present a novel approach and a prototype system
that enables the deeper understanding of semantic properties of the real world
environment, so that realistic physical interactions between the real and the
virtual objects can be generated. A material-aware AR environment has been
created based on the deep material learning using a fully convolutional network
(FCN). The state-of-the-art dense Simultaneous Localisation and Mapping (SLAM)
has been used for the semantic mapping. Together with efficient accelerated 3D
ray casting, natural and realistic physical interactions are generated for
interactive AR games. Our approach has significant impact on the future
development of advanced AR systems and applications.","['Long Chen', 'Karl Francis', 'Wen Tang']",2017-08-03T16:52:14Z,http://arxiv.org/abs/1708.01208v3
"VisAR: Bringing Interactivity to Static Data Visualizations through
  Augmented Reality","Static visualizations have analytic and expressive value. However, many
interactive tasks cannot be completed using static visualizations. As datasets
grow in size and complexity, static visualizations start losing their analytic
and expressive power for interactive data exploration. Despite this limitation
of static visualizations, there are still many cases where visualizations are
limited to being static (e.g., visualizations on presentation slides or
posters). We believe in many of these cases, static visualizations will benefit
from allowing users to perform interactive tasks on them. Inspired by the
introduction of numerous commercial personal augmented reality (AR) devices, we
propose an AR solution that allows interactive data exploration of datasets on
static visualizations. In particular, we present a prototype system named VisAR
that uses the Microsoft Hololens to enable users to complete interactive tasks
on static visualizations.","['Taeheon Kim', 'Bahador Saket', 'Alex Endert', 'Blair MacIntyre']",2017-08-04T04:54:24Z,http://arxiv.org/abs/1708.01377v1
"Multi-Path Cooperative Communications Networks for Augmented and Virtual
  Reality Transmission","Augmented and/or virtual reality (AR/VR) are emerging as one of the main
applications in future fifth generation (5G) networks. To meet the requirements
of lower latency and massive data transmission in AR/VR applications, a
solution with software-defined networking (SDN) architecture is proposed for 5G
small cell networks. On this basis, a multi-path cooperative route (MCR) scheme
is proposed to facilitate the AR/VR wireless transmissions in 5G small cell
networks, in which the delay of MCR scheme is analytically studied.
Furthermore, a service effective energy optimal (SEEO) algorithm is developed
for AR/VR wireless transmission in 5G small cell networks. Simulation results
indicate that both the delay and service effective energy (SEE) of the proposed
MCR scheme outperform the delay and SEE of the conventional single path route
scheme in 5G small cell networks.","['Xiaohu Ge', 'Linghui Pan', 'Qiang Li', 'Guoqiang Mao', 'Song Tu']",2017-10-31T14:08:56Z,http://arxiv.org/abs/1710.11486v1
"Distributed Augmented Reality with 3D Lung Dynamics -- A Planning Tool
  Concept","Augmented Reality (AR) systems add visual information to the world by using
advanced display techniques. The advances in miniaturization and reduced costs
make some of these systems feasible for applications in a wide set of fields.
We present a potential component of the cyber infrastructure for the operating
room of the future; a distributed AR based software-hardware system that allows
real-time visualization of 3D lung dynamics superimposed directly on the
patient's body. Several emergency events (e.g. closed and tension pneumothorax)
and surgical procedures related to the lung (e.g. lung transplantation, lung
volume reduction surgery, surgical treatment of lung infections, lung cancer
surgery) could benefit from the proposed prototype.","['Felix G. Hamza-Lup', 'Anand P. Santhanam', 'Celina Imielinska', 'Sanford Meeks', 'Jannick P. Rolland']",2018-11-29T04:10:23Z,http://arxiv.org/abs/1811.11953v1
Recognizing and tracking outdoor objects by using ARToolKit markers,"We created an augmented reality platform for spatial exploration that
recognizes buildings facades and displays various multimedia for different time
points. In order to provide the user with the best user experience fast
recognition and stable tracking are the key elements of any augmented reality
app. In an outdoor environment, lighting, reflective surfaces and occlusion can
drastically affect the user experience. In a setup where these conditions are
similar, marker creation methodology and the app parameters are key. In this
paper we focus on resizing the photo prior marker creating and the importance
of camera calibration and resolution and their effect on the recognition speed
and quality of tracking outdoor objects.","['Blagoj Nenovski', 'Igor Nedelkovski']",2020-01-04T12:56:59Z,http://arxiv.org/abs/2001.01073v1
"Developing an Augmented Reality Tourism App through User-Centred Design
  (Extended Version)","Augmented Reality (AR) bridges the gap between the physical and virtual
world. Through overlaying graphics on natural environments, users can immerse
themselves in a tailored environment. This offers great benefits to mobile
tourism, where points of interest (POIs) can be annotated on a smartphone
screen. While a variety of apps currently exist, usability issues can
discourage users from embracing AR. Interfaces can become cluttered with icons,
with POI occlusion posing further challenges. In this paper, we use
user-centred design (UCD) to develop an AR tourism app. We solicit requirements
through a synthesis of domain analysis, tourist observation and semi-structured
interviews. Whereas previous user-centred work has designed mock-ups, we
iteratively develop a full Android app. This includes overhead maps and route
navigation, in addition to a detailed AR browser. The final product is
evaluated by 20 users, who participate in a tourism task in a UK city. Users
regard the system as usable and intuitive, and suggest the addition of further
customisation. We finish by critically analysing the challenges of a
user-centred methodology.","['Meredydd Williams', 'Kelvin K. K. Yao', 'Jason R. C. Nurse']",2020-01-29T23:35:32Z,http://arxiv.org/abs/2001.11131v1
"ToARist: An Augmented Reality Tourism App created through User-Centred
  Design","Through Augmented Reality (AR), virtual graphics can transform the physical
world. This offers benefits to mobile tourism, where points of interest (POIs)
can be annotated on a smartphone screen. Although several of these applications
exist, usability issues can discourage adoption. User-centred design (UCD)
solicits frequent feedback, often contributing to usable products. While AR
mock-ups have been constructed through UCD, we develop a novel and functional
tourism app. We solicit requirements through a synthesis of domain analysis,
tourist observation and semi-structured interviews. Through four rounds of
iterative development, users test and refine the app. The final product, dubbed
ToARist, is evaluated by 20 participants, who engage in a tourism task around a
UK city. Users regard the system as usable, but find technical issues can
disrupt AR. We finish by reflecting on our design and critiquing the challenges
of a strict user-centred methodology.","['Meredydd Williams', 'Kelvin K. K. Yao', 'Jason R. C. Nurse']",2018-07-16T09:44:54Z,http://arxiv.org/abs/1807.05759v1
"The Problems of Personnel Training for STEM Education in the Modern
  Innovative Learning and Research Environment","The aim of the article is to describe the problems of personnel training that
arise in view of extension of the STEM approach to education, development of
innovative technologies, in particular, virtualization, augmented reality, the
use of ICT outsourcing in educational systems design. The object of research is
the process of formation and development of the educational and scientific
envi- ronment of educational institution. The subject of the study is the
formation and development of the cloud-based learning and research environment
for STEM education. The methods of research are: the analysis of publications
on the prob- lem, generalization of domestic and foreign experience,
theoretical analysis, sys- tem analysis, systematization and generalization of
research facts and laws for the development and design of the model of the
cloud-based learning environ- ment, substantiation of the main conclusions. The
results of the research are the next: the concepts and the model of the
cloud-based environment of STEM edu- cation is substantiated, the problems of
personnel training at the present stage are outlined.",['Mariya Shyshkina'],2018-07-23T12:36:17Z,http://arxiv.org/abs/1807.08562v1
"Dynamic Environment Mapping for Augmented Reality Applications on Mobile
  Devices","Augmented Reality is a topic of foremost interest nowadays. Its main goal is
to seamlessly blend virtual content in real-world scenes. Due to the lack of
computational power in mobile devices, rendering a virtual object with
high-quality, coherent appearance and in real-time, remains an area of active
research. In this work, we present a novel pipeline that allows for coupled
environment acquisition and virtual object rendering on a mobile device
equipped with a depth sensor. While keeping human interaction to a minimum, our
system can scan a real scene and project it onto a two-dimensional environment
map containing RGB+Depth data. Furthermore, we define a set of criteria that
allows for an adaptive update of the environment map to account for dynamic
changes in the scene. Then, under the assumption of diffuse surfaces and
distant illumination, our method exploits an analytic expression for the
irradiance in terms of spherical harmonic coefficients, which leads to a very
efficient rendering algorithm. We show that all the processes in our pipeline
can be executed while maintaining an average frame rate of 31Hz on a mobile
device.","['Rafael Monroy', 'Matis Hudon', 'Aljosa Smolic']",2018-09-21T14:10:55Z,http://arxiv.org/abs/1809.08134v1
An Edge-Computing Based Architecture for Mobile Augmented Reality,"In order to mitigate the long processing delay and high energy consumption of
mobile augmented reality (AR) applications, mobile edge computing (MEC) has
been recently proposed and is envisioned as a promising means to deliver better
quality of experience (QoE) for AR consumers. In this article, we first present
a comprehensive AR overview, including the indispensable components of general
AR applications, fashionable AR devices, and several existing techniques for
overcoming the thorny latency and energy consumption problems. Then, we propose
a novel hierarchical computation architecture by inserting an edge layer
between the conventional user layer and cloud layer. Based on the proposed
architecture, we further develop an innovated operation mechanism to improve
the performance of mobile AR applications. Three key technologies are also
discussed to further assist the proposed AR architecture. Simulation results
are finally provided to verify that our proposals can significantly improve the
latency and energy performance as compared against existing baseline schemes.","['Jinke Ren', 'Yinghui He', 'Guan Huang', 'Guanding Yu', 'Yunlong Cai', 'Zhaoyang Zhang']",2018-10-05T03:53:10Z,http://arxiv.org/abs/1810.02509v2
"Exploring Stereovision-Based 3-D Scene Reconstruction for Augmented
  Reality","Three-dimensional (3-D) scene reconstruction is one of the key techniques in
Augmented Reality (AR), which is related to the integration of image processing
and display systems of complex information. Stereo matching is a computer
vision based approach for 3-D scene reconstruction. In this paper, we explore
an improved stereo matching network, SLED-Net, in which a Single Long
Encoder-Decoder is proposed to replace the stacked hourglass network in PSM-Net
for better contextual information learning. We compare SLED-Net to
state-of-the-art methods recently published, and demonstrate its superior
performance on Scene Flow and KITTI2015 test sets.","['Guang-Yu Nie', 'Yun Liu', 'Cong Wang', 'Yue Liu', 'Yongtian Wang']",2019-02-17T13:09:16Z,http://arxiv.org/abs/1902.06255v1
"Development of Head-Mounted Projection Displays for Distributed,
  Collaborative, Augmented Reality Applications","Distributed systems technologies supporting 3D visualization and social
collaboration will be increasing in frequency and type over time. An emerging
type of head-mounted display referred to as the head-mounted projection display
(HMPD) was recently developed that only requires ultralight optics (i.e., less
than 8 g per eye) that enables immersive multiuser, mobile augmented reality 3D
visualization, as well as remote 3D collaborations. In this paper a review of
the development of lightweight HMPD technology is provided, together with
insight into what makes this technology timely and so unique. Two novel
emerging HMPD-based technologies are then described: a teleportal HMPD(T-HMPD)
enabling face-to-face communication and visualization of shared 3D virtual
objects, and a mobile HMPD (M-HMPD) designed for outdoor wearable visualization
and communication. Finally, the use of HMPD in medical visualization and
training, as well as in infospaces, two applications developed in the ODA and
MIND labs respectively, are discussed.","['Jannick P. Rolland', 'Frank Biocca', 'Felix G. Hamza-Lup', 'Yanggang Ha', 'Ricardo Martins']",2019-02-20T20:32:51Z,http://arxiv.org/abs/1902.07769v1
BlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs,"We present BlazeFace, a lightweight and well-performing face detector
tailored for mobile GPU inference. It runs at a speed of 200-1000+ FPS on
flagship devices. This super-realtime performance enables it to be applied to
any augmented reality pipeline that requires an accurate facial region of
interest as an input for task-specific models, such as 2D/3D facial keypoint or
geometry estimation, facial features or expression classification, and face
region segmentation. Our contributions include a lightweight feature extraction
network inspired by, but distinct from MobileNetV1/V2, a GPU-friendly anchor
scheme modified from Single Shot MultiBox Detector (SSD), and an improved tie
resolution strategy alternative to non-maximum suppression.","['Valentin Bazarevsky', 'Yury Kartynnik', 'Andrey Vakunov', 'Karthik Raveendran', 'Matthias Grundmann']",2019-07-11T08:40:08Z,http://arxiv.org/abs/1907.05047v2
"EyeSec: A Retrofittable Augmented Reality Tool for Troubleshooting
  Wireless Sensor Networks in the Field","Wireless Sensor Networks (WSNs) often lack interfaces for remote debugging.
Thus, fault diagnosis and troubleshooting are conducted at the deployment site.
Currently, WSN operators lack dedicated tools that aid them in this process.
Therefore, we introduce EyeSec, a tool for WSN monitoring and maintenance in
the field. An Augmented Reality Device (AR Device) identifies sensor nodes
using optical markers. Portable Sniffer Units capture network traffic and
extract information. With those data, the AR Device network topology and data
flows between sensor nodes are visualized. Unlike previous tools, EyeSec is
fully portable, independent of any given infrastructure and does not require
dedicated and expensive AR hardware. Using passive inspection only, it can be
retrofitted to already deployed WSNs. We implemented a proof of concept on
low-cost embedded hardware and commodity smart phones and demonstrate the usage
of EyeSec within a WSN test bed using the 6LoWPAN transmission protocol.","['Martin Striegel', 'Carsten Rolfes', 'Johann Heyszl', 'Fabian Helfert', 'Maximilian Hornung', 'Georg Sigl']",2019-07-08T14:11:49Z,http://arxiv.org/abs/1907.12364v1
Closing the Reality Gap with Unsupervised Sim-to-Real Image Translation,"Deep learning approaches have become the standard solution to many problems
in computer vision and robotics, but obtaining sufficient training data in high
enough quality is challenging, as human labor is error prone, time consuming,
and expensive. Solutions based on simulation have become more popular in recent
years, but the gap between simulation and reality is still a major issue. In
this paper, we introduce a novel method for augmenting synthetic image data
through unsupervised image-to-image translation by applying the style of real
world images to simulated images with open source frameworks. The generated
dataset is combined with conventional augmentation methods and is then applied
to a neural network model running in real-time on autonomous soccer robots. Our
evaluation shows a significant improvement compared to models trained on images
generated entirely in simulation.","['Jan Blumenkamp', 'Andreas Baude', 'Tim Laue']",2019-11-04T23:17:03Z,http://arxiv.org/abs/1911.01529v2
Semantic-Aware Label Placement for Augmented Reality in Street View,"In an augmented reality (AR) application, placing labels in a manner that is
clear and readable without occluding the critical information from the
real-world can be a challenging problem. This paper introduces a label
placement technique for AR used in street view scenarios. We propose a
semantic-aware task-specific label placement method by identifying potentially
important image regions through a novel feature map, which we refer to as
guidance map. Given an input image, its saliency information, semantic
information and the task-specific importance prior are integrated into the
guidance map for our labeling task. To learn the task prior, we created a label
placement dataset with the users' labeling preferences, as well as use it for
evaluation. Our solution encodes the constraints for placing labels in an
optimization problem to obtain the final label layout, and the labels will be
placed in appropriate positions to reduce the chances of overlaying important
real-world objects in street view AR scenarios. The experimental validation
shows clearly the benefits of our method over previous solutions in the AR
street view navigation and similar applications.","['Jianqing Jia', 'Semir Elezovikj', 'Heng Fan', 'Shuojin Yang', 'Jing Liu', 'Wei Guo', 'Chiu C. Tan', 'Haibin Ling']",2019-12-15T20:29:37Z,http://arxiv.org/abs/1912.07105v1
"Augmented-Reality-Based Visualization of Navigation Data of Mobile
  Robots on the Microsoft Hololens -- Possibilities and Limitations","The demand for mobile robots has rapidly increased in recent years due to the
flexibility and high variety of application fields comparing to static robots.
To deal with complex tasks such as navigation, they work with high amounts of
different sensor data making it difficult to operate with for non-experts. To
enhance user understanding and human robot interaction, we propose an approach
to visualize the navigation stack within a cutting edge 3D Augmented Reality
device -- the Microsoft Hololens. Therefore, relevant navigation stack data
including laser scan, environment map and path planing data are visualized in
3D within the head mounted device. Based on that prototype, we evaluate the
Hololens in terms of computational capabilities and limitations for dealing
with huge amount of real-time data. Results show that the Hololens is capable
of a proper visualization of huge amounts of sensor data. We demonstrate a
proper visualization of navigation stack data in 3D within the Hololens.
However, there are limitations when transferring and displaying different kinds
of data simultaneously.","['Linh Kästner', 'Jens Lambrecht']",2019-12-27T14:13:15Z,http://arxiv.org/abs/1912.12109v1
"Development of Interactive Instructional Model Using Augmented Reality
  based on Edutainment to Enhance Emotional Quotient","The research aims to develop an interactive instructional model using
augmented reality based on edutainment to enhance emotional quotient and
evaluate the model. Two phases of the research will be carried out: a
development and an evaluation of the model. Samples are experts in the field of
IT, child psychology, and 7th grade curriculum management. Ten experts are
selected by purposive sampling method. The obtained data are analyzed using
mean and standard deviation. The research result demonstrates the following
findings: 1) The results of this research show that Model consists of 3
elements: IIAR, EduLA, and EQ. EQ is a means to assess EQ based on Time Series
Experimental Design using 2 kinds of tools; i.e. EQ Assessment by programs in
tablet computers, and EQ Assessment by behavioral observation. 2) The ten
experts have evaluated the model and commented that the developed model showed
high suitability.","['Nuttakan Pakprod', 'Panita Wannapiroon']",2014-02-17T09:46:16Z,http://arxiv.org/abs/1402.3942v1
"An Improved Tracking using IMU and Vision Fusion for Mobile Augmented
  Reality Applications","Mobile Augmented Reality (MAR) is becoming an important cyber-physical system
application given the ubiquitous availability of mobile phones. With the need
to operate in unprepared environments, accurate and robust registration and
tracking has become an important research problem to solve. In fact, when MAR
is used for tele-interactive applications involving large distances, say from
an accident site to insurance office, tracking at both the ends is desirable
and further it is essential to appropriately fuse inertial and vision sensors
data. In this paper, we present results and discuss some insights gained in
marker-less tracking during the development of a prototype pertaining to an
example use case related to breakdown or damage assessment of a vehicle. The
novelty of this paper is in bringing together different components and modules
with appropriate enhancements towards a complete working system.","['Kriti Kumar', 'Ashley Varghese', 'Pavan K Reddy', 'N Narendra', 'Prashanth Swamy', 'M Girish Chandra', 'P Balamuralidhar']",2014-11-10T06:15:42Z,http://arxiv.org/abs/1411.2335v1
Adaptive User Perspective Rendering for Handheld Augmented Reality,"Handheld Augmented Reality commonly implements some variant of magic lens
rendering, which turns only a fraction of the user's real environment into AR
while the rest of the environment remains unaffected. Since handheld AR devices
are commonly equipped with video see-through capabilities, AR magic lens
applications often suffer from spatial distortions, because the AR environment
is presented from the perspective of the camera of the mobile device. Recent
approaches counteract this distortion based on estimations of the user's head
position, rendering the scene from the user's perspective. To this end,
approaches usually apply face-tracking algorithms on the front camera of the
mobile device. However, this demands high computational resources and therefore
commonly affects the performance of the application beyond the already high
computational load of AR applications. In this paper, we present a method to
reduce the computational demands for user perspective rendering by applying
lightweight optical flow tracking and an estimation of the user's motion before
head tracking is started. We demonstrate the suitability of our approach for
computationally limited mobile devices and we compare it to device perspective
rendering, to head tracked user perspective rendering, as well as to fixed
point of view user perspective rendering.","['Peter Mohr', 'Markus Tatzgern', 'Jens Grubert', 'Dieter Schmalstieg', 'Denis Kalkofen']",2017-03-22T21:56:58Z,http://arxiv.org/abs/1703.07869v1
Interaction Methods for Smart Glasses,"Since the launch of Google Glass in 2014, smart glasses have mainly been
designed to support micro-interactions. The ultimate goal for them to become an
augmented reality interface has not yet been attained due to an encumbrance of
controls. Augmented reality involves superimposing interactive computer
graphics images onto physical objects in the real world. This survey reviews
current research issues in the area of human computer interaction for smart
glasses. The survey first studies the smart glasses available in the market and
afterwards investigates the interaction methods proposed in the wide body of
literature. The interaction methods can be classified into hand-held, touch,
and touchless input. This paper mainly focuses on the touch and touchless
input. Touch input can be further divided into on-device and on-body, while
touchless input can be classified into hands-free and freehand. Next, we
summarize the existing research efforts and trends, in which touch and
touchless input are evaluated by a total of eight interaction goals. Finally,
we discuss several key design challenges and the possibility of multi-modal
input for smart glasses.","['Lik-Hang Lee', 'Pan Hui']",2017-07-31T05:49:03Z,http://arxiv.org/abs/1707.09728v1
"The Helping Hand: An Assistive Manipulation Framework Using Augmented
  Reality and a Tongue-Drive Interfaces","A human-in-the-loop system is proposed to enable collaborative manipulation
tasks for person with physical disabilities. Studies show that the cognitive
burden of subject reduces with increased autonomy of assistive system. Our
framework obtains high-level intent from the user to specify manipulation
tasks. The system processes sensor input to interpret the user's environment.
Augmented reality glasses provide ego-centric visual feedback of the
interpretation and summarize robot affordances on a menu. A tongue drive system
serves as the input modality for triggering a robotic arm to execute the tasks.
Assistance experiments compare the system to Cartesian control and to
state-of-the-art approaches. Our system achieves competitive results with
faster completion time by simplifying manipulation tasks.","['Fu-Jen Chu', 'Ruinian Xu', 'Zhenxuan Zhang', 'Patricio A. Vela', 'Maysam Ghovanloo']",2018-02-01T19:24:22Z,http://arxiv.org/abs/1802.00463v2
"MaskFusion: Real-Time Recognition, Tracking and Reconstruction of
  Multiple Moving Objects","We present MaskFusion, a real-time, object-aware, semantic and dynamic RGB-D
SLAM system that goes beyond traditional systems which output a purely
geometric map of a static scene. MaskFusion recognizes, segments and assigns
semantic class labels to different objects in the scene, while tracking and
reconstructing them even when they move independently from the camera.
  As an RGB-D camera scans a cluttered scene, image-based instance-level
semantic segmentation creates semantic object masks that enable real-time
object recognition and the creation of an object-level representation for the
world map. Unlike previous recognition-based SLAM systems, MaskFusion does not
require known models of the objects it can recognize, and can deal with
multiple independent motions. MaskFusion takes full advantage of using
instance-level semantic segmentation to enable semantic labels to be fused into
an object-aware map, unlike recent semantics enabled SLAM systems that perform
voxel-level semantic segmentation. We show augmented-reality applications that
demonstrate the unique features of the map output by MaskFusion:
instance-aware, semantic and dynamic.","['Martin Rünz', 'Maud Buffier', 'Lourdes Agapito']",2018-04-24T18:15:15Z,http://arxiv.org/abs/1804.09194v2
"Occluded object reconstruction for first responders with augmented
  reality glasses using conditional generative adversarial networks","Firefighters suffer a variety of life-threatening risks, including
line-of-duty deaths, injuries, and exposures to hazardous substances. Support
for reducing these risks is important. We built a partially occluded object
reconstruction method on augmented reality glasses for first responders. We
used a deep learning based on conditional generative adversarial networks to
train associations between the various images of flammable and hazardous
objects and their partially occluded counterparts. Our system then
reconstructed an image of a new flammable object. Finally, the reconstructed
image was superimposed on the input image to provide ""transparency"". The system
imitates human learning about the laws of physics through experience by
learning the shape of flammable objects and the flame characteristics.","['Kyongsik Yun', 'Thomas Lu', 'Edward Chow']",2018-04-20T23:56:10Z,http://arxiv.org/abs/1805.00322v1
CloudAR: A Cloud-based Framework for Mobile Augmented Reality,"Computation capabilities of recent mobile devices enable natural feature
processing for Augmented Reality (AR). However, mobile AR applications are
still faced with scalability and performance challenges. In this paper, we
propose CloudAR, a mobile AR framework utilizing the advantages of cloud and
edge computing through recognition task offloading. We explore the design space
of cloud-based AR exhaustively and optimize the offloading pipeline to minimize
the time and energy consumption. We design an innovative tracking system for
mobile devices which provides lightweight tracking in 6 degree of freedom
(6DoF) and hides the offloading latency from users' perception. We also design
a multi-object image retrieval pipeline that executes fast and accurate image
recognition tasks on servers. In our evaluations, the mobile AR application
built with the CloudAR framework runs at 30 frames per second (FPS) on average
with precise tracking of only 1~2 pixel errors and image recognition of at
least 97% accuracy. Our results also show that CloudAR outperforms one of the
leading commercial AR framework in several performance metrics.","['Wenxiao Zhang', 'Sikun Lin', 'Farshid Hassani Bijarbooneh', 'Hao Fei Cheng', 'And Pan Hui']",2018-05-08T14:38:58Z,http://arxiv.org/abs/1805.03060v1
Sensorless Hand Guidance using Microsoft Hololens,"Hand guidance of robots has proven to be a useful tool both for programming
trajectories and in kinesthetic teaching. However hand guidance is usually
relegated to robots possessing joint-torque sensors (JTS). Here we propose to
extend hand guidance to robots lacking those sensors through the use of an
Augmented Reality (AR) device, namely Microsoft's Hololens. Augmented reality
devices have been envisioned as a helpful addition to ease both robot
programming and increase situational awareness of humans working in close
proximity to robots. We reference the robot by using a registration algorithm
to match a robot model to the spatial mesh. The in-built hand tracking
capabilities are then used to calculate the position of the hands relative to
the robot. By decomposing the hand movements into orthogonal rotations we
achieve a completely sensorless hand guidance without any need to build a
dynamic model of the robot itself. We did the first tests our approach on a
commonly used industrial manipulator, the KUKA KR-5.","['David Puljiz', 'Erik Stöhr', 'Katharina S. Riesterer', 'Björn Hein', 'Torsten Kröger']",2019-01-15T16:55:43Z,http://arxiv.org/abs/1901.04933v1
Metasurfaces for near-eye augmented reality,"Augmented reality (AR) has the potential to revolutionize the way in which
information is presented by overlaying virtual information onto a person's
direct view of their real-time surroundings. By placing the display on the
surface of the eye, a contact lens display (CLD) provides a versatile solution
for compact AR. However, an unaided human eye cannot visualize patterns on the
CLD simply because of the limited accommodation of the eye. Here, we introduce
a holographic display technology that casts virtual information directly to the
retina so that the eye sees it while maintaining the visualization of the
real-world intact. The key to our design is to introduce metasurfaces to create
a phase distribution that projects virtual information in a pixel-by-pixel
manner. Unlike conventional holographic techniques, our metasurface-based
technique is able to display arbitrary patterns using a single passive
hologram. With a small form-factor, the designed metasurface empowers near-eye
AR excluding the need of extra optical elements, such as a spatial light
modulator, for dynamic image control.","['Shoufeng Lan', 'Xueyue Zhang', 'Mohammad Taghinejad', 'Sean Rodrigues', 'Kyu-Tae Lee', 'Zhaocheng Liu', 'Wenshan Cai']",2019-01-18T20:02:35Z,http://arxiv.org/abs/1901.06408v1
A Projection-based Augmented Reality for Elderly People with Dementia,"As aging societies grow, researchers are actively studying care systems
concerning the life and diseases of the elderly. Among these diseases, dementia
makes it difficult to maintain daily life due to the degradation of cognitive
functioning, memory, and reasoning, as well as the ability to perform actions.
Moreover, dementia does not have a perfect cure, though therapy and care can
slow its onset and provide patients with physical and mental support. In this
paper, we developed a projection-based augmented reality system robot that can
cover 360 degrees of space. We also propose an application that supports
continuous monitoring of dementia patients to address the difficulties they
face in daily life. The system is also designed to provide therapy
applications, such as entertainment and spatial art, to provide mental care
aids for the patients.","['Hyocheol Ro', 'Yoon Jung Park', 'Tack-Don Han']",2019-08-16T16:27:51Z,http://arxiv.org/abs/1908.06046v1
"Enabling Intuitive Human-Robot Teaming Using Augmented Reality and
  Gesture Control","Human-robot teaming offers great potential because of the opportunities to
combine strengths of heterogeneous agents. However, one of the critical
challenges in realizing an effective human-robot team is efficient information
exchange - both from the human to the robot as well as from the robot to the
human. In this work, we present and analyze an augmented reality-enabled,
gesture-based system that supports intuitive human-robot teaming through
improved information exchange. Our proposed system requires no external
instrumentation aside from human-wearable devices and shows promise of
real-world applicability for service-oriented missions. Additionally, we
present preliminary results from a pilot study with human participants, and
highlight lessons learned and open research questions that may help direct
future development, fielding, and experimentation of autonomous HRI systems.","['Jason M. Gregory', 'Christopher Reardon', 'Kevin Lee', 'Geoffrey White', 'Ki Ng', 'Caitlyn Sims']",2019-09-13T19:18:52Z,http://arxiv.org/abs/1909.06415v1
"Multi-user Augmented Reality Application for Video Communication in
  Virtual Space","Communication is the most useful tool to impart knowledge, understand ideas,
clarify thoughts and expressions, organize plan and manage every single
day-to-day activity. Although there are different modes of communication,
physical barrier always affects the clarity of the message due to the absence
of body language and facial expressions. These barriers are overcome by video
calling, which is technically the most advance mode of communication at
present. The proposed work concentrates around the concept of video calling in
a more natural and seamless way using Augmented Reality (AR). AR can be helpful
in giving the users an experience of physical presence in each other's
environment. Our work provides an entirely new platform for video calling,
wherein the users can enjoy the privilege of their own virtual space to
interact with the individual's environment. Moreover, there is no limitation of
sharing the same screen space. Any number of participants can be accommodated
over a single conference without having to compromise the screen size.","['Kumar Mridul', 'M. Ramanathan', 'Kunal Ahirwar', 'Mansi Sharma']",2019-09-20T14:32:54Z,http://arxiv.org/abs/1909.09529v1
Negotiation-based Human-Robot Collaboration via Augmented Reality,"Effective human-robot collaboration (HRC) requires extensive communication
among the human and robot teammates, because their actions can potentially
produce conflicts, synergies, or both. We develop a novel augmented reality
(AR) interface to bridge the communication gap between human and robot
teammates. Building on our AR interface, we develop an AR-mediated,
negotiation-based (ARN) framework for HRC. We have conducted experiments both
in simulation and on real robots in an office environment, where multiple
mobile robots work on delivery tasks. The robots could not complete the tasks
on their own, but sometimes need help from their human teammate, rendering
human-robot collaboration necessary. Results suggest that ARN significantly
reduced the human-robot team's task completion time compared to a non-AR
baseline approach.","['Kishan Chandan', 'Vidisha Kudalkar', 'Xiang Li', 'Shiqi Zhang']",2019-09-24T23:34:36Z,http://arxiv.org/abs/1909.11227v3
"Augmented Reality on the Large Scene Based on a Markerless Registration
  Framework","In this paper, a mobile camera positioning method based on forward and
inverse kinematics of robot is proposed, which can realize far point
positioning of imaging position and attitude tracking in large scene
enhancement. Orbit precision motion through the framework overhead cameras and
combining with the ground system of sensor array object such as mobile robot
platform of various sensors, realize the good 3 d image registration, solve any
artifacts that is mobile robot in the large space position initialization
problem, effectively implement the large space no marks augmented reality,
human-computer interaction, and information summary. Finally, the feasibility
and effectiveness of the method are verified by experiments.","['Zhen Ma', 'He Xu', 'Yonghui Zhang', 'Junlong Chen', 'Dongbo Zhao', 'Siqing Chen']",2020-03-03T00:01:05Z,http://arxiv.org/abs/2003.01256v2
PointAR: Efficient Lighting Estimation for Mobile Augmented Reality,"We propose an efficient lighting estimation pipeline that is suitable to run
on modern mobile devices, with comparable resource complexities to
state-of-the-art mobile deep learning models. Our pipeline, PointAR, takes a
single RGB-D image captured from the mobile camera and a 2D location in that
image, and estimates 2nd order spherical harmonics coefficients. This estimated
spherical harmonics coefficients can be directly utilized by rendering engines
for supporting spatially variant indoor lighting, in the context of augmented
reality. Our key insight is to formulate the lighting estimation as a point
cloud-based learning problem directly from point clouds, which is in part
inspired by the Monte Carlo integration leveraged by real-time spherical
harmonics lighting. While existing approaches estimate lighting information
with complex deep learning pipelines, our method focuses on reducing the
computational complexity. Through both quantitative and qualitative
experiments, we demonstrate that PointAR achieves lower lighting estimation
errors compared to state-of-the-art methods. Further, our method requires an
order of magnitude lower resource, comparable to that of mobile-specific DNNs.","['Yiqin Zhao', 'Tian Guo']",2020-03-30T19:13:26Z,http://arxiv.org/abs/2004.00006v4
Congestion-aware Evacuation Routing using Augmented Reality Devices,"We present a congestion-aware routing solution for indoor evacuation, which
produces real-time individual-customized evacuation routes among multiple
destinations while keeping tracks of all evacuees' locations. A population
density map, obtained on-the-fly by aggregating locations of evacuees from
user-end Augmented Reality (AR) devices, is used to model the congestion
distribution inside a building. To efficiently search the evacuation route
among all destinations, a variant of A* algorithm is devised to obtain the
optimal solution in a single pass. In a series of simulated studies, we show
that the proposed algorithm is more computationally optimized compared to
classic path planning algorithms; it generates a more time-efficient evacuation
route for each individual that minimizes the overall congestion. A complete
system using AR devices is implemented for a pilot study in real-world
environments, demonstrating the efficacy of the proposed approach.","['Zeyu Zhang', 'Hangxin Liu', 'Ziyuan Jiao', 'Yixin Zhu', 'Song-Chun Zhu']",2020-04-25T22:54:35Z,http://arxiv.org/abs/2004.12246v1
"Deep Residual Network based food recognition for enhanced Augmented
  Reality application","Deep neural network based learning approaches is widely utilized for image
classification or object detection based problems with remarkable outcomes.
Realtime Object state estimation of objects can be used to track and estimate
the features that the object of the current frame possesses without causing any
significant delay and misclassification. A system that can detect the features
of such objects in the present state from camera images can be used to enhance
the application of Augmented Reality for improving user experience and
delivering information in a much perceptual way. The focus behind this paper is
to determine the most suitable model to create a low-latency assistance AR to
aid users by providing them nutritional information about the food that they
consume in order to promote healthier life choices. Hence the dataset has been
collected and acquired in such a manner, and we conduct various tests in order
to identify the most suitable DNN in terms of performance and complexity and
establish a system that renders such information realtime to the user.","['Siddarth S', 'Sainath G', 'Vignesh S']",2020-05-08T21:08:58Z,http://arxiv.org/abs/2005.04292v2
"A Novel Approach of using AR and Smart Surgical Glasses Supported Trauma
  Care","BACKGROUND: Augmented reality (AR) is gaining popularity in varying field
such as computer gaming and medical education fields. However, still few of
applications in real surgeries. Orthopedic surgical applications are currently
limited and underdeveloped. - METHODS: The clinic validation was prepared with
the currently available AR equipment and software. A total of 1 Vertebroplasty,
2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal Femoral Fracture, 1 CRIF
for distal radius fracture and 2 ORIF for Tibia Fracture cases were performed
with fluoroscopy combined with AR smart surgical glasses system. - RESULTS: A
total of 1 Vertebroplasty, 2 ORIF Pelvis fracture, 1 ORIF with PFN for Proximal
Femoral Fracture, 1 CRIF for distal radius fracture and 2 ORIF for Tibia
Fracture cases are performed to evaluate the benefits of AR surgery. Among the
AR surgeries, surgeons wear the smart surgical are lot reduce of eyes of turns
to focus on the monitors. This paper shows the potential ability of augmented
reality technology for trauma surgery.","['Anurag Lal', 'Ming-Hsien Hu', 'Pei-Yuan Lee', 'Min Liang Wang']",2020-05-25T06:03:30Z,http://arxiv.org/abs/2005.11935v1
"What the HoloLens Maps Is Your Workspace: Fast Mapping and Set-up of
  Robot Cells via Head Mounted Displays and Augmented Reality","Classical methods of modelling and mapping robot work cells are time
consuming, expensive and involve expert knowledge. We present a novel approach
to mapping and cell setup using modern Head Mounted Displays (HMDs) that
possess self-localisation and mapping capabilities. We leveraged these
capabilities to create a point cloud of the environment and build an OctoMap -
a voxel occupancy grid representation of the robot's workspace for path
planning. Through the use of Augmented Reality (AR) interactions, the user can
edit the created Octomap and add security zones. We perform comprehensive tests
of the HoloLens' depth sensing capabilities and the quality of the resultant
point cloud. A high-end laser scanner is used to provide the ground truth for
the evaluation of the point cloud quality. The amount of false-positive and
false-negative voxels in the OctoMap are also tested.","['David Puljiz', 'Franziska Krebs', 'Fabian Bösing', 'Björn Hein']",2020-05-26T12:13:03Z,http://arxiv.org/abs/2005.12651v1
Instant 3D Object Tracking with Applications in Augmented Reality,"Tracking object poses in 3D is a crucial building block for Augmented Reality
applications. We propose an instant motion tracking system that tracks an
object's pose in space (represented by its 3D bounding box) in real-time on
mobile devices. Our system does not require any prior sensory calibration or
initialization to function. We employ a deep neural network to detect objects
and estimate their initial 3D pose. Then the estimated pose is tracked using a
robust planar tracker. Our tracker is capable of performing relative-scale
9-DoF tracking in real-time on mobile devices. By combining use of CPU and GPU
efficiently, we achieve 26-FPS+ performance on mobile devices.","['Adel Ahmadyan', 'Tingbo Hou', 'Jianing Wei', 'Liangkai Zhang', 'Artsiom Ablavatski', 'Matthias Grundmann']",2020-06-23T17:48:29Z,http://arxiv.org/abs/2006.13194v1
"Towards Secure and Usable Authentication for Augmented and Virtual
  Reality Head-Mounted Displays","Immersive technologies, including augmented and virtual reality (AR & VR)
devices, have enhanced digital communication along with a considerable increase
in digital threats. Thus, authentication becomes critical in AR & VR
technology, particularly in shared spaces. In this paper, we propose applying
the ZeTA protocol that allows secure authentication even in shared spaces for
the AR & VR context. We explain how it can be used with the available
interaction methods provided by Head-Mounted Displays. In future work, our
research goal is to evaluate different designs of ZeTA (e.g., interaction
modes) concerning their usability and users' risk perception regarding their
security - while using a cross-cultural approach.","['Reyhan Duezguen', 'Peter Mayer', 'Sanchari Das', 'Melanie Volkamer']",2020-07-22T20:34:14Z,http://arxiv.org/abs/2007.11663v2
Human-Robot Interaction in a Shared Augmented Reality Workspace,"We design and develop a new shared Augmented Reality (AR) workspace for
Human-Robot Interaction (HRI), which establishes a bi-directional communication
between human agents and robots. In a prototype system, the shared AR workspace
enables a shared perception, so that a physical robot not only perceives the
virtual elements in its own view but also infers the utility of the human
agent--the cost needed to perceive and interact in AR--by sensing the human
agent's gaze and pose. Such a new HRI design also affords a shared
manipulation, wherein the physical robot can control and alter virtual objects
in AR as an active agent; crucially, a robot can proactively interact with
human agents, instead of purely passively executing received commands. In
experiments, we design a resource collection game that qualitatively
demonstrates how a robot perceives, processes, and manipulates in AR and
quantitatively evaluates the efficacy of HRI using the shared AR workspace. We
further discuss how the system can potentially benefit future HRI studies that
are otherwise challenging.","['Shuwen Qiu', 'Hangxin Liu', 'Zeyu Zhang', 'Yixin Zhu', 'Song-Chun Zhu']",2020-07-24T17:18:30Z,http://arxiv.org/abs/2007.12656v2
"Adapting Nielsen's Usability Heuristics to the Context of Mobile
  Augmented Reality","Augmented reality (AR) is an emerging technology in mobile app design during
recent years. However, usability challenges in these apps are prominent. There
are currently no established guidelines for designing and evaluating
interactions in AR as there are in traditional user interfaces. In this work,
we aimed to examine the usability of current mobile AR applications and
interpreting classic usability heuristics in the context of mobile AR.
Particularly, we focused on AR home design apps because of their popularity and
ability to incorporate important mobile AR interaction schemas. Our findings
indicated that it is important for the designers to consider the unfamiliarity
of AR technology to the vast users and to take technological limitations into
consideration when designing mobile AR apps. Our work serves as a first step
for establishing more general heuristics and guidelines for mobile AR.","['Audrey Labrie', 'Jinghui Cheng']",2020-08-07T13:32:18Z,http://arxiv.org/abs/2008.03174v1
"Identifying Usability Issues of Software Analytics Applications in
  Immersive Augmented Reality","Software analytics in augmented reality (AR) is said to have great potential.
One reason why this potential is not yet fully exploited may be usability
problems of the AR user interfaces. We present an iterative and qualitative
usability evaluation with 15 subjects of a state-of-the-art application for
software analytics in AR. We could identify and resolve numerous usability
issues. Most of them were caused by applying conventional user interface
elements, such as dialog windows, buttons, and scrollbars. The used city
visualization, however, did not cause any usability issues. Therefore, we argue
that future work should focus on making conventional user interface elements in
AR obsolete by integrating their functionality into the immersive
visualization.","['David Baum', 'Stefan Bechert', 'Ulrich Eisenecker', 'Isabelle Meichsner', 'Richard Müller']",2020-08-13T20:15:28Z,http://arxiv.org/abs/2008.06099v1
iviz: A ROS Visualization App for Mobile Devices,"In this work, we introduce iviz, a mobile application for visualizing ROS
data. In the last few years, the popularity of ROS has grown enormously, making
it the standard platform for open source robotic programming. A key reason for
this success is the availability of polished, general-purpose modules for many
tasks, such as localization, mapping, path planning, and quite importantly,
data visualization. However, the availability of the latter is generally
restricted to PCs with the Linux operating system. Thus, users that want to see
what is happening in the system with a smartphone or a tablet are stuck with
solutions such as screen mirroring or using web browser versions of rviz, which
are difficult to interact with from a mobile interface. More importantly, this
makes newer visualization modalities such as Augmented Reality impossible. Our
application iviz, based on the Unity engine, addresses these issues by
providing a visualization platform designed from scratch to be usable in mobile
platforms, such as iOS, Android, and UWP, and including native support for
Augmented Reality for all three platforms. If desired, it can also be used in a
PC with Linux, Windows, or macOS without any changes.","['Antonio Zea', 'Uwe D. Hanebeck']",2020-08-28T16:24:04Z,http://arxiv.org/abs/2008.12725v1
"Augmented Reality-Based Advanced Driver-Assistance System for Connected
  Vehicles","With the development of advanced communication technology, connected vehicles
become increasingly popular in our transportation systems, which can conduct
cooperative maneuvers with each other as well as road entities through
vehicle-to-everything communication. A lot of research interests have been
drawn to other building blocks of a connected vehicle system, such as
communication, planning, and control. However, less research studies were
focused on the human-machine cooperation and interface, namely how to visualize
the guidance information to the driver as an advanced driver-assistance system
(ADAS). In this study, we propose an augmented reality (AR)-based ADAS, which
visualizes the guidance information calculated cooperatively by multiple
connected vehicles. An unsignalized intersection scenario is adopted as the use
case of this system, where the driver can drive the connected vehicle crossing
the intersection under the AR guidance, without any full stop at the
intersection. A simulation environment is built in Unity game engine based on
the road network of San Francisco, and human-in-the-loop (HITL) simulation is
conducted to validate the effectiveness of our proposed system regarding travel
time and energy consumption.","['Ziran Wang', 'Kyungtae Han', 'Prashant Tiwari']",2020-08-31T06:14:28Z,http://arxiv.org/abs/2008.13381v1
"Design of achromatic augmented reality visors based on composite
  metasurfaces","A compact near-eye visor (NEV) system that can guide light from a display to
the eye could transform augmented reality (AR) technology. Unfortunately,
existing implementations of such an NEV either suffer from small field of view
or chromatic aberrations. See-through quality and bulkiness further make the
overall performance of the visors unsuitable for a seamless user experience.
Metasurfaces are an emerging class of nanophotonic elements that can
dramatically reduce the size of optical elements while enhancing functionality.
In this paper, we present a design of composite metasurfaces for an
ultra-compact NEV. We simulate the performance of a proof-of-principle visor
corrected for chromatic aberrations while providing a large display field of
view (>77{\deg} both horizontally and vertically), good see-through quality
(>70% transmission and less than a wavelength root mean-square (RMS) wavefront
error over the whole visible wavelength range), as needed for an immersive AR
experience.","['Elyas Bayati', 'Andrew Wolfram', 'Shane Colburn', 'Luocheng Huang', 'Arka Majumdar']",2020-10-01T20:04:34Z,http://arxiv.org/abs/2010.00668v1
"Ego-Motion Alignment from Face Detections for Collaborative Augmented
  Reality","Sharing virtual content among multiple smart glasses wearers is an essential
feature of a seamless Collaborative Augmented Reality experience. To enable the
sharing, local coordinate systems of the underlying 6D ego-pose trackers,
running independently on each set of glasses, have to be spatially and
temporally aligned with respect to each other. In this paper, we propose a
novel lightweight solution for this problem, which is referred as ego-motion
alignment. We show that detecting each other's face or glasses together with
tracker ego-poses sufficiently conditions the problem to spatially relate local
coordinate systems. Importantly, the detected glasses can serve as reliable
anchors to bring sufficient accuracy for the targeted practical use. The
proposed idea allows us to abandon the traditional visual localization step
with fiducial markers or scene points as anchors. A novel closed form minimal
solver which solves a Quadratic Eigenvalue Problem is derived and its
refinement with Gaussian Belief Propagation is introduced. Experiments validate
the presented approach and show its high practical potential.","['Branislav Micusik', 'Georgios Evangelidis']",2020-10-05T16:57:48Z,http://arxiv.org/abs/2010.02153v1
Making Mobile Augmented Reality Applications Accessible,"Augmented Reality (AR) technology creates new immersive experiences in
entertainment, games, education, retail, and social media. AR content is often
primarily visual and it is challenging to enable access to it non-visually due
to the mix of virtual and real-world content. In this paper, we identify common
constituent tasks in AR by analyzing existing mobile AR applications for iOS,
and characterize the design space of tasks that require accessible
alternatives. For each of the major task categories, we create prototype
accessible alternatives that we evaluate in a study with 10 blind participants
to explore their perceptions of accessible AR. Our study demonstrates that
these prototypes make AR possible to use for blind users and reveals a number
of insights to move forward. We believe our work sets forth not only exemplars
for developers to create accessible AR applications, but also a roadmap for
future research to make AR comprehensively accessible.","['Jaylin Herskovitz', 'Jason Wu', 'Samuel White', 'Amy Pavel', 'Gabriel Reyes', 'Anhong Guo', 'Jeffrey P. Bigham']",2020-10-12T21:23:27Z,http://arxiv.org/abs/2010.06035v1
"Developing Augmented Reality based Gaming Model to Teach Ethical
  Education in Primary Schools","Education sector is adopting new technologies for both teaching and learning
pedagogy. Augmented Reality (AR) is a new technology that can be used in the
educational pedagogy to enhance the engagement with students. Students interact
with AR-based educational material for more visualization and explanation.
Therefore, the use of AR in education is becoming more popular. However, most
researches narrate the use of AR technologies in the field of English, Maths,
Science, Culture, Arts, and History education but the absence of ethical
education is visible. In our paper, we design the system and develop an
AR-based mobile game model in the field of Ethical education for pre-primary
students. Students from pre-primary require more interactive lessons than
theoretical concepts. So, we use AR technology to develop a game which offers
interactive procedures where students can learn with fun and engage with the
context. Finally, we develop a prototype that works with our research
objective. We conclude our paper with future works.",['Mohammad Ali'],2020-10-29T04:01:32Z,http://arxiv.org/abs/2010.15346v1
"Collaborative Augmented Reality on Smartphones via Life-long City-scale
  Maps","In this paper we present the first published end-to-end production
computer-vision system for powering city-scale shared augmented reality
experiences on mobile devices. In doing so we propose a new formulation for an
experience-based mapping framework as an effective solution to the key issues
of city-scale SLAM scalability, robustness, map updates and all-time
all-weather performance required by a production system. Furthermore, we
propose an effective way of synchronising SLAM systems to deliver seamless
real-time localisation of multiple edge devices at the same time. All this in
the presence of network latency and bandwidth limitations. The resulting system
is deployed and tested at scale in San Francisco where it delivers AR
experiences in a mapped area of several hundred kilometers. To foster further
development of this area we offer the data set to the public, constituting the
largest of this kind to date.","['Lukas Platinsky', 'Michal Szabados', 'Filip Hlasek', 'Ross Hemsley', 'Luca Del Pero', 'Andrej Pancik', 'Bryan Baum', 'Hugo Grimmett', 'Peter Ondruska']",2020-11-10T19:45:06Z,http://arxiv.org/abs/2011.05370v1
"Implementing the Cognition Level for Industry 4.0 by integrating
  Augmented Reality and Manufacturing Execution Systems","In the current industrial practices, the exponential growth in terms of
availability and affordability of sensors, data acquisition systems, and
computer networks forces factories to move toward implementing high integrating
Cyber-Physical Systems (CPS) with production, logistics, and services. This
transforms today's factories into Industry 4.0 factories with significant
economic potential. Industry 4.0, also known as the fourth Industrial
Revolution, levers on the integration of cyber technologies, the Internet of
Things, and Services. This paper proposes an Augmented Reality (AR)-based
system that creates a Cognition Level that integrates existent Manufacturing
Execution Systems (MES) to CPS. The idea is to highlight the opportunities
offered by AR technologies to CPS by describing an application scenario. The
system, analyzed in a real factory, shows its capacity to integrate physical
and digital worlds strongly. Furthermore, the conducted survey (based on the
Situation Awareness Global Assessment Technique method) reveals significant
advantages in terms of production monitoring, progress, and workers' Situation
Awareness in general.","['Alfonso Di Pace', 'Giuseppe Fenza', 'Mariacristina Gallo', 'Vincenzo Loia', 'Aldo Meglio', 'Francesco Orciuoli']",2020-11-18T21:53:13Z,http://arxiv.org/abs/2011.10482v1
"Beyond LunAR: An augmented reality UI for deep-space exploration
  missions","As space exploration efforts shift to deep space missions, new challenges
emerge regarding astronaut communication and task completion. While the round
trip propagation delay for lunar communications is 2.6 seconds, the time delay
increases to nearly 22 minutes for Mars missions. This creates a need for
astronaut independence from earth-based assistance, and places greater
significance upon the limited communications that are able to be delivered. To
address this issue, we prototyped an augmented reality user interface for the
new xEMU spacesuit, intended for use on planetary surface missions. This user
interface assists with functions that would usually be completed by flight
controllers in Mission Control, or are currently completed in manners that are
unnecessarily difficult. We accomplish this through features such as AR
model-based task instruction, sampling task assistance, note taking, and
telemetry monitoring and display.","['Sarah Radway', 'Anthony Luo', 'Carmine Elvezio', 'Jenny Cha', 'Sophia Kolak', 'Elijah Zulu', 'Sad Adib']",2020-11-30T04:16:19Z,http://arxiv.org/abs/2011.14535v1
"Sonic Sculpture: Activating Engagement with Head-Mounted Augmented
  Reality","This work examines how head-mounted AR can be used to build an interactive
sonic landscape to engage with a public sculpture. We describe a sonic artwork,
""Listening To Listening"", that has been designed to accompany a real-world
sculpture with two prototype interaction schemes. Our artwork is created for
the HoloLens platform so that users can have an individual experience in a
mixed reality context. Personal head-mounted AR systems have recently become
available and practical for integration into public art projects, however
research into sonic sculpture works has yet to account for the affordances of
current portable and mainstream AR systems. In this work, we take advantage of
the HoloLens' spatial awareness to build sonic spaces that have a precise
spatial relationship to a given sculpture and where the sculpture itself is
modelled in the augmented scene as an ""invisible hologram"". We describe the
artistic rationale for our artwork, the design of the two interaction schemes,
and the technical and usability feedback that we have obtained from
demonstrations during iterative development.","['Charles Patrick Martin', 'Zeruo Liu', 'Yichen Wang', 'Wennan He', 'Henry Gardner']",2020-12-03T22:35:49Z,http://arxiv.org/abs/2012.02311v1
"A Survey on Synchronous Augmented, Virtual and Mixed Reality Remote
  Collaboration Systems","Remote collaboration systems have become increasingly important in today's
society, especially during times where physical distancing is advised.
Industry, research and individuals face the challenging task of collaborating
and networking over long distances. While video and teleconferencing are
already widespread, collaboration systems in augmented, virtual, and mixed
reality are still a niche technology. We provide an overview of recent
developments of synchronous remote collaboration systems and create a taxonomy
by dividing them into three main components that form such systems:
Environment, Avatars, and Interaction. A thorough overview of existing systems
is given, categorising their main contributions in order to help researchers
working in different fields by providing concise information about specific
topics such as avatars, virtual environment, visualisation styles and
interaction. The focus of this work is clearly on synchronised collaboration
from a distance. A total of 82 unique systems for remote collaboration are
discussed, including more than 100 publications and 25 commercial systems.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2021-02-11T13:33:51Z,http://arxiv.org/abs/2102.05998v1
"Multi-view data capture for dynamic object reconstruction using handheld
  augmented reality mobiles","We propose a system to capture nearly-synchronous frame streams from multiple
and moving handheld mobiles that is suitable for dynamic object 3D
reconstruction. Each mobile executes Simultaneous Localisation and Mapping
on-board to estimate its pose, and uses a wireless communication channel to
send or receive synchronisation triggers. Our system can harvest frames and
mobile poses in real time using a decentralised triggering strategy and a
data-relay architecture that can be deployed either at the Edge or in the
Cloud. We show the effectiveness of our system by employing it for 3D skeleton
and volumetric reconstructions. Our triggering strategy achieves equal
performance to that of an NTP-based synchronisation approach, but offers higher
flexibility, as it can be adjusted online based on application needs. We
created a challenging new dataset, namely 4DM, that involves six handheld
augmented reality mobiles recording an actor performing sports actions
outdoors. We validate our system on 4DM, analyse its strengths and limitations,
and compare its modules with alternative ones.","['M. Bortolon', 'L. Bazzanella', 'F. Poiesi']",2021-03-14T10:26:50Z,http://arxiv.org/abs/2103.07883v2
Neural Networks for Semantic Gaze Analysis in XR Settings,"Virtual-reality (VR) and augmented-reality (AR) technology is increasingly
combined with eye-tracking. This combination broadens both fields and opens up
new areas of application, in which visual perception and related cognitive
processes can be studied in interactive but still well controlled settings.
However, performing a semantic gaze analysis of eye-tracking data from
interactive three-dimensional scenes is a resource-intense task, which so far
has been an obstacle to economic use. In this paper we present a novel approach
which minimizes time and information necessary to annotate volumes of interest
(VOIs) by using techniques from object recognition. To do so, we train
convolutional neural networks (CNNs) on synthetic data sets derived from
virtual models using image augmentation techniques. We evaluate our method in
real and virtual environments, showing that the method can compete with
state-of-the-art approaches, while not relying on additional markers or
preexisting databases but instead offering cross-platform use.","['Lena Stubbemann', 'Dominik Dürrschnabel', 'Robert Refflinghaus']",2021-03-18T18:05:01Z,http://arxiv.org/abs/2103.10451v1
"Virtual Barriers in Augmented Reality for Safe and Effective Human-Robot
  Cooperation in Manufacturing","Safety is a fundamental requirement in any human-robot collaboration
scenario. To ensure the safety of users for such scenarios, we propose a novel
Virtual Barrier system facilitated by an augmented reality interface. Our
system provides two kinds of Virtual Barriers to ensure safety: 1) a Virtual
Person Barrier which encapsulates and follows the user to protect them from
colliding with the robot, and 2) Virtual Obstacle Barriers which users can
spawn to protect objects or regions that the robot should not enter. To enable
effective human-robot collaboration, our system includes an intuitive robot
programming interface utilizing speech commands and hand gestures, and features
the capability of automatic path re-planning when potential collisions are
detected as a result of a barrier intersecting the robot's planned path. We
compared our novel system with a standard 2D display interface through a user
study, where participants performed a task mimicking an industrial
manufacturing procedure. Results show that our system increases the user's
sense of safety and task efficiency, and makes the interaction more intuitive.","['Khoa Cong Hoang', 'Wesley P. Chan', 'Steven Lay', 'Akansel Cosgun', 'Elizabeth Croft']",2021-04-12T05:36:49Z,http://arxiv.org/abs/2104.05211v1
Enhanced LSTM-based Service Decomposition for Mobile Augmented Reality,"Undoubtedly, Mobile Augmented Reality (MAR) applications for 5G and Beyond
wireless networks are witnessing a notable attention recently. However, they
require significant computational and storage resources at the end device
and/or the network via Edge Cloud (EC) support. In this work, a MAR service is
considered under the lenses of microservices where MAR service components can
be decomposed and anchored at different locations ranging from the end device
to different ECs in order to optimize the overall service and network
efficiency. To this end, we propose a mobility aware MAR service decomposition
using a Long Short Term Memory (LSTM) deep neural network to provide efficient
pro-active decision making in real-time. More specifically, the LSTM deep
neural network is trained with optimal solutions derived from a mathematical
programming formulation in an offline manner. Then, decision making at the
inference stage is used to optimize service decomposition of MAR services. A
wide set of numerical investigations reveal that the mobility aware LSTM deep
neural network manage to outperform recently proposed schemes in terms of both
decision making quality as well as computational time.","['Zhaohui Huang', 'Vasilis Friderikos']",2021-04-15T10:19:45Z,http://arxiv.org/abs/2104.07351v1
"A Novel Edge Detection Operator for Identifying Buildings in Augmented
  Reality Applications","Augmented Reality is an environment-enhancing technology, widely applied in
many domains, such as tourism and culture. One of the major challenges in this
field is precise detection and extraction of building information through
Computer Vision techniques. Edge detection is one of the building blocks
operations for many feature extraction solutions in Computer Vision. AR systems
use edge detection for building extraction or for extraction of facade details
from buildings. In this paper, we propose a novel filter operator for edge
detection that aims to extract building contours or facade features better. The
proposed filter gives more weight for finding vertical and horizontal edges
that is an important feature for our aim.","['Ciprian Orhei', 'Silviu Vert', 'Radu Vasiu']",2021-06-02T10:06:50Z,http://arxiv.org/abs/2106.01055v1
"Small Object Detection for Near Real-Time Egocentric Perception in a
  Manual Assembly Scenario","Detecting small objects in video streams of head-worn augmented reality
devices in near real-time is a huge challenge: training data is typically
scarce, the input video stream can be of limited quality, and small objects are
notoriously hard to detect. In industrial scenarios, however, it is often
possible to leverage contextual knowledge for the detection of small objects.
Furthermore, CAD data of objects are typically available and can be used to
generate synthetic training data. We describe a near real-time small object
detection pipeline for egocentric perception in a manual assembly scenario: We
generate a training data set based on CAD data and realistic backgrounds in
Unity. We then train a YOLOv4 model for a two-stage detection process: First,
the context is recognized, then the small object of interest is detected. We
evaluate our pipeline on the augmented reality device Microsoft Hololens 2.","['Hooman Tavakoli', 'Snehal Walunj', 'Parsha Pahlevannejad', 'Christiane Plociennik', 'Martin Ruskowski']",2021-06-11T13:59:44Z,http://arxiv.org/abs/2106.06403v1
"Exploring the Effect of Visual Cues on Eye Gaze During AR-Guided Picking
  and Assembly Tasks","In this paper, we present an analysis of eye gaze patterns pertaining to
visual cues in augmented reality (AR) for head-mounted displays (HMDs). We
conducted an experimental study involving a picking and assembly task, which
was guided by different visual cues. We compare these visual cues along
multiple dimensions (in-view vs. out-of-view, static vs. dynamic, sequential
vs. simultaneous) and analyze quantitative metrics such as gaze distribution,
gaze duration, and gaze path distance. Our results indicate that visual cues in
AR significantly affect eye gaze patterns. Specifically, we show that the
effect varies depending on the type of visual cue. We discuss these empirical
results with respect to visual attention theory.","['Arne Seeliger', 'Gerrit Merz', 'Christian Holz', 'Stefan Feuerriegel']",2021-08-10T13:19:39Z,http://arxiv.org/abs/2108.04669v1
"Soccer line mark segmentation and classification with stochastic
  watershed transform","Augmented reality applications are beginning to change the way sports are
broadcast, providing richer experiences and valuable insights to fans. The
first step of augmented reality systems is camera calibration, possibly based
on detecting the line markings of the playing field. Most existing proposals
for line detection rely on edge detection and Hough transform, but radial
distortion and extraneous edges cause inaccurate or spurious detections of line
markings. We propose a novel strategy to automatically and accurately segment
and classify line markings. First, line points are segmented thanks to a
stochastic watershed transform that is robust to radial distortions, since it
makes no assumptions about line straightness, and is unaffected by the presence
of players or the ball. The line points are then linked to primitive structures
(straight lines and ellipses) thanks to a very efficient procedure that makes
no assumptions about the number of primitives that appear in each image. The
strategy has been tested on a new and public database composed by 60 annotated
images from matches in five stadiums. The results obtained have proven that the
proposed strategy is more robust and accurate than existing approaches,
achieving successful line mark detection even in challenging conditions.","['Daniel Berjón', 'Carlos Cuevas', 'Narciso García']",2021-08-14T00:51:12Z,http://arxiv.org/abs/2108.06432v2
"SketchMeHow: Interactive Projection Guided Task Instruction with User
  Sketches","In this work, we propose an interactive general instruction framework
SketchMeHow to guidance the common users to complete the daily tasks in
real-time. In contrast to the conventional augmented reality-based instruction
systems, the proposed framework utilizes the user sketches as system inputs to
acquire the users' production intentions from the drawing interfaces. Given the
user sketches, the designated task instruction can be analyzed based on the
sub-task division and spatial localization for each task. The projector-camera
system is adopted in the projection guidance to the end-users with the spatial
augmented reality technology. To verify the proposed framework, we conducted
two case studies of domino arrangement and bento production. From our user
studies, the proposed systems can help novice users complete the tasks
efficiently with user satisfaction. We believe the proposed SketchMeHow can
broaden the research topics in sketch-based real-world applications in
human-computer interaction.","['Haoran Xie', 'Yichen Peng', 'Hange Wang', 'Kazunori Miyata']",2021-09-07T12:00:54Z,http://arxiv.org/abs/2109.03013v1
ARROCH: Augmented Reality for Robots Collaborating with a Human,"Human-robot collaboration frequently requires extensive communication, e.g.,
using natural language and gestures. Augmented reality (AR) has provided an
alternative way of bridging the communication gap between robots and people.
However, most current AR-based human-robot communication methods are
unidirectional, focusing on how the human adapts to robot behaviors, and are
limited to single-robot domains. In this paper, we develop AR for Robots
Collaborating with a Human (ARROCH), a novel algorithm and system that supports
bidirectional, multi-turn, human-multi-robot communication in indoor multi-room
environments. The human can see through obstacles to observe the robots'
current states and intentions, and provide feedback, while the robots'
behaviors are then adjusted toward human-multi-robot teamwork. Experiments have
been conducted with real robots and human participants using collaborative
delivery tasks. Results show that ARROCH outperformed a standard non-AR
approach in both user experience and teamwork efficiency. In addition, we have
developed a novel simulation environment using Unity (for AR and human
simulation) and Gazebo (for robot simulation). Results in simulation
demonstrate ARROCH's superiority over AR-based baselines in human-robot
collaboration.","['Kishan Chandan', 'Vidisha Kudalkar', 'Xiang Li', 'Shiqi Zhang']",2021-09-21T18:46:19Z,http://arxiv.org/abs/2109.10400v2
"Learning Geometric Transformations for Parametric Design: An Augmented
  Reality (AR)-Powered Approach","Despite the remarkable development of parametric modeling methods for
architectural design, a significant problem still exists, which is the lack of
knowledge and skill regarding the professional implementation of parametric
design in architectural modeling. Considering the numerous advantages of
digital/parametric modeling in rapid prototyping and simulation most
instructors encourage students to use digital modeling even from the early
stages of design; however, an appropriate context to learn the basics of
digital design thinking is rarely provided in architectural pedagogy. This
paper presents an educational tool, specifically an Augmented Reality (AR)
intervention, to help students understand the fundamental concepts of
para-metric modeling before diving into complex parametric modeling platforms.
The goal of the AR intervention is to illustrate geometric transformation and
the associated math functions so that students learn the mathematical logic
behind the algorithmic thinking of parametric modeling. We have developed
BRICKxAR_T, an educational AR prototype, that intends to help students learn
geometric transformations in an immersive spatial AR environment. A LEGO set is
used within the AR intervention as a physical manipulative to support physical
interaction and im-prove spatial skill through body gesture.","['Zohreh Shaghaghian', 'Heather Burte', 'Dezhen Song', 'Wei Yan']",2021-09-02T03:49:01Z,http://arxiv.org/abs/2109.10899v1
"Here To Stay: Measuring Hologram Stability in Markerless Smartphone
  Augmented Reality","Markerless augmented reality (AR) has the potential to provide engaging
experiences and improve outcomes across a wide variety of industries; the
overlaying of virtual content, or holograms, onto a view of the real world
without the need for predefined markers provides great convenience and
flexibility. However, unwanted hologram movement frequently occurs in
markerless smartphone AR due to challenging visual conditions or device
movement, and resulting error in device pose tracking. We develop a method for
measuring hologram positional errors on commercial smartphone markerless AR
platforms, implement it as an open-source AR app, HoloMeasure, and use the app
to conduct systematic quantitative characterizations of hologram stability
across 6 different user actions, 3 different smartphone models, and over 200
different environments. Our study demonstrates significant levels of spatial
instability in holograms in all but the simplest settings, and underscores the
need for further enhancements to pose tracking algorithms for smartphone-based
markerless AR.","['Tim Scargill', 'Jiasi Chen', 'Maria Gorlatova']",2021-09-29T23:15:43Z,http://arxiv.org/abs/2109.14757v1
"RescueAR: Augmented Reality Supported Collaboration for UAV Driven
  Emergency Response Systems","Emergency response events are fast-paced, noisy, and they require teamwork to
accomplish the mission. Furthermore, the increasing deployment of Unmanned
Aerial Vehicles (UAVs) alongside emergency responders, demands a new form of
partnership between humans and UAVs. Traditional radio-based information
exchange between humans during an emergency response suffers from a lack of
visualization and often results in miscommunication. This paper presents a
novel collaboration platform: RescueAR, which utilizes the paradigm of
Location-based Augmented Reality to geotag, share, and visualize information.
RescueAR aims to support the two-way communication between humans and UAVs,
facilitate collaboration across diverse responders, and visualize scene
information relevant to the rescue team's role. According to our feasibility
study, a user study, followed by a focus group session with police officers,
RescueAR can support rescue teams in developing the spatial cognition of the
scene, facilitate the exchange of geolocation information, and complement
existing communication tools during the UAV-supported emergency response.","['Ankit Agrawal', 'Jane Cleland-Huang']",2021-10-01T02:52:18Z,http://arxiv.org/abs/2110.00180v1
"Effect of Visual Cues on Pointing Tasks in Co-located Augmented Reality
  Collaboration","Visual cues are essential in computer-mediated communication. It is
especially important when communication happens in a collaboration scenario
that requires focusing several users' attention on aspecific object among other
similar ones. This paper explores the effect of visual cues on pointing tasks
in co-located Augmented Reality (AR) collaboration. A user study (N = 32, 16
pairs) was conducted to compare two types of visual cues: Pointing Line (PL)and
Moving Track (MT). Both are head-based visual techniques.Through a series of
collaborative pointing tasks on objects with different states (static and
dynamic) and density levels (low, mediumand high), the results showed that PL
was better on task performance and usability, but MT was rated higher on social
presenceand user preference. Based on our results, some design implicationsare
provided for pointing tasks in co-located AR collaboration.","['Lei Chen', 'Yilin Liu', 'Yue Li', 'Lingyun Yu', 'BoYu Gao', 'Maurizio Caon', 'Yong Yue', 'Hai-Ning Liang']",2021-10-08T11:44:15Z,http://arxiv.org/abs/2110.04045v1
"An Augmented Reality Platform for Introducing Reinforcement Learning to
  K-12 Students with Robots","Interactive reinforcement learning, where humans actively assist during an
agent's learning process, has the promise to alleviate the sample complexity
challenges of practical algorithms. However, the inner workings and state of
the robot are typically hidden from the teacher when humans provide feedback.
To create a common ground between the human and the learning robot, in this
paper, we propose an Augmented Reality (AR) system that reveals the hidden
state of the learning to the human users. This paper describes our system's
design and implementation and concludes with a discussion on two directions for
future work which we are pursuing: 1) use of our system in AI education
activities at the K-12 level; and 2) development of a framework for an AR-based
human-in-the-loop reinforcement learning, where the human teacher can see
sensory and cognitive representations of the robot overlaid in the real world.","['Ziyi Zhang', 'Samuel Micah Akai-Nettey', 'Adonai Addo', 'Chris Rogers', 'Jivko Sinapov']",2021-10-10T03:51:39Z,http://arxiv.org/abs/2110.04697v1
Developing a Lecture Video Recording System Using Augmented Reality,"Assistive technology is a prerequisite for making a high-quality lecture
video. It is therefore imperative to edit the lecture video after recording. In
this study, we aim to reduce the cumbersome task of lecture video editing by
developing a system that enables the addition of visual effects in the video
while recording. In particular, we use augmented reality (AR) technology to
digitize and display in real-time lecture materials, assistant agents, and
other recording contents used by the lecturer. Our system realizes such a
mechanism as a lecture recording environment. In addition, our system based on
AR technology can support the work of the lecturer, which is difficult to do by
oneself while conducting the lecture, using the information of the lecturer's
position and the progress of the lecture. We evaluated the system functionality
and performance, and verified the system's correct behavior. If the burden of
making lecture videos can be reduced, the lecturer will be able to devote more
time to improving the quality of lecture contents, which is expected to
contribute to the improvement of lectures.","['Yuma Ito', 'Masato Kikuchi', 'Tadachika Ozono', 'Toramatsu Shintani']",2021-10-11T06:32:27Z,http://arxiv.org/abs/2110.05955v1
"State of the Art of Augmented Reality (AR) Capabilities for Civil
  Infrastructure Applications","Augmented Reality (AR) is a technology superimposing interactional virtual
objects onto a real environment. Since the beginning of the millennium, AR
technologies have shown rapid growth, with significant research publications in
engineering and science. However, the civil infrastructure community has
minimally implemented AR technologies to date. One of the challenges that civil
engineers face when understanding and using AR is the lack of a classification
of AR in the context of capabilities for civil infrastructure applications.
Practitioners in civil infrastructure, like most engineering fields, prioritize
understanding the level of maturity of a new technology before considering its
adoption and field implementation. This paper compares the capabilities of
sixteen AR Head-Mounted Devices (HMDs) available in the market since 2017,
ranking them in terms of performance for civil infrastructure implementations.
Finally, the authors recommend a development framework for practical AR
interfaces with civil infrastructure and operations.","['Jiaqi Xu', 'Derek Doyle', 'Fernando Moreu']",2021-10-17T01:34:59Z,http://arxiv.org/abs/2110.08698v1
"Exploring Augmented Reality Games in Accessible Learning: A Systematic
  Review","Augmented Reality (AR) learning games, on average, have been shown to have a
positive impact on student learning. However, the exploration of AR learning
games in special education settings, where accessibility is a concern, has not
been well explored. Thus, the purpose of this study is to explore the use of AR
games in accessible learning applications and to provide a comprehensive
understanding of its advantages over traditional learning approaches. In this
paper, we present our systematic review of previous studies included in major
databases in the past decade. We explored the characteristics of user
evaluation, learning effects on students, and features of implemented systems
mentioned in the literature. The results showed that AR game applications can
promote students learning activities from three perspectives: cognitive,
affective, and retention. We also found there were still several drawbacks to
current AR learning game designs for special needs despite the positive effects
associated with AR game use. Based on our findings, we propose potential design
strategies for future AR learning games for accessible education.","['Minghao Cai', 'Gokce Akcayir', 'Carrie Demmans Epp']",2021-11-16T04:03:51Z,http://arxiv.org/abs/2111.08214v1
FaceAtlasAR: Atlas of Facial Acupuncture Points in Augmented Reality,"Acupuncture is a technique in which practitioners stimulate specific points
on the body. Those points, called acupuncture points (or acupoints),
anatomically define areas on the skin relative to specific landmarks on the
body. However, mapping the acupoints to individuals could be challenging for
inexperienced acupuncturists. In this project, we proposed a system to localize
and visualize facial acupoints for individuals in an augmented reality (AR)
context. This system combines a face alignment model and a hair segmentation
model to provide dense reference points for acupoints localization in real-time
(60FPS). The localization process takes the proportional bone (B-cun or
skeletal) measurement method, which is commonly operated by specialists;
however, in the real practice, operators sometimes find it inaccurate due to
skill-related error. With this system, users, even without any skills, can
locate the facial acupoints as a part of the self-training or self-treatment
process.","['Menghe Zhang', 'Jurgen Schulze', 'Dong Zhang']",2021-11-29T18:00:25Z,http://arxiv.org/abs/2111.14755v2
"Semantic Interaction in Augmented Reality Environments for Microsoft
  HoloLens","Augmented Reality is a promising technique for human-machine interaction.
Especially in robotics, which always considers systems in their environment, it
is highly beneficial to display visualizations and receive user input directly
in exactly that environment. We explore this idea using the Microsoft HoloLens,
with which we capture indoor environments and display interaction cues with
known object classes. The 3D mesh recorded by the HoloLens is annotated
on-line, as the user moves, with semantic classes using a projective approach,
which allows us to use a state-of-the-art 2D semantic segmentation method. The
results are fused onto the mesh; prominent object segments are identified and
displayed in 3D to the user. Finally, the user can trigger actions by gesturing
at the object. We both present qualitative results and analyze the accuracy and
performance of our method in detail on an indoor dataset.","['Peer Schüett', 'Max Schwarz', 'Sven Behnke']",2021-11-18T14:58:04Z,http://arxiv.org/abs/2112.05846v1
Stay Positive: Non-Negative Image Synthesis for Augmented Reality,"In applications such as optical see-through and projector augmented reality,
producing images amounts to solving non-negative image generation, where one
can only add light to an existing image. Most image generation methods,
however, are ill-suited to this problem setting, as they make the assumption
that one can assign arbitrary color to each pixel. In fact, naive application
of existing methods fails even in simple domains such as MNIST digits, since
one cannot create darker pixels by adding light. We know, however, that the
human visual system can be fooled by optical illusions involving certain
spatial configurations of brightness and contrast. Our key insight is that one
can leverage this behavior to produce high quality images with negligible
artifacts. For example, we can create the illusion of darker patches by
brightening surrounding pixels. We propose a novel optimization procedure to
produce images that satisfy both semantic and non-negativity constraints. Our
approach can incorporate existing state-of-the-art methods, and exhibits strong
performance in a variety of tasks including image-to-image translation and
style transfer.","['Katie Luo', 'Guandao Yang', 'Wenqi Xian', 'Harald Haraldsson', 'Bharath Hariharan', 'Serge Belongie']",2022-02-01T18:55:11Z,http://arxiv.org/abs/2202.00659v1
Context-Based MEC Platform for Augmented-Reality Services in 5G Networks,"Augmented reality (AR) has drawn great attention in recent years. However,
current AR devices have drawbacks, e.g., weak computation ability and large
power consumption. To solve the problem, mobile edge computing (MEC) can be
introduced as a key technology to offload data and computation from AR devices
to MEC servers via 5th Generation Mobile Communication Technology (5G)
networks. To this end, a context-based MEC platform for AR services in 5G
networks is proposed in this paper. On the platform, MEC is employed as a data
processing center while AR devices are simplified as universal input/output
devices, which overcomes their limitations and achieves better user experience.
Moreover, the proof-of-concept (PoC) hardware prototype of the platform, and
two typical use cases providing AR services of navigation and face recognition
respectively are implemented to demonstrate the feasibility and effectiveness
of the platform. Finally, the performance of the platform is also numerically
evaluated, and the results validate the system design and agree well with the
design expectations.","['Yue Wang', 'Tao Yu', 'Kei Sakaguchi']",2022-02-03T14:07:20Z,http://arxiv.org/abs/2202.01600v1
Experimental Augmented Reality User Experience,"Augmented Reality (AR) is an emerging field ripe for experimentation,
especially when it comes to developing the kinds of applications and
experiences that will drive mass adoption of the technology. While we aren't
aware of any current consumer product that realize a wearable, wide Field of
View (FoV), AR Head Mounted Display (HMD), such devices will certainly come. In
order for these sophisticated, likely high-cost hardware products to succeed,
it is important they provide a high quality user experience. To that end, we
prototyped 4 experimental applications for wide FoV displays that will likely
exist in the future. Given current AR HMD limitations, we used a AR simulator
built on web technology and VR headsets to demonstrate these applications,
allowing users and designers to peer into the future.","['Josef Spjut', 'Fengyuan Zhu', 'Xiaolei Huang', 'Yichen Shou', 'Ben Boudaoud', 'Omer Shapira', 'Morgan McGuire']",2022-02-10T19:05:46Z,http://arxiv.org/abs/2202.06726v1
"Medicinal Boxes Recognition on a Deep Transfer Learning Augmented
  Reality Mobile Application","Taking medicines is a fundamental aspect to cure illnesses. However, studies
have shown that it can be hard for patients to remember the correct posology.
More aggravating, a wrong dosage generally causes the disease to worsen.
Although, all relevant instructions for a medicine are summarized in the
corresponding patient information leaflet, the latter is generally difficult to
navigate and understand. To address this problem and help patients with their
medication, in this paper we introduce an augmented reality mobile application
that can present to the user important details on the framed medicine. In
particular, the app implements an inference engine based on a deep neural
network, i.e., a densenet, fine-tuned to recognize a medicinal from its
package. Subsequently, relevant information, such as posology or a simplified
leaflet, is overlaid on the camera feed to help a patient when taking a
medicine. Extensive experiments to select the best hyperparameters were
performed on a dataset specifically collected to address this task; ultimately
obtaining up to 91.30\% accuracy as well as real-time capabilities.","['Danilo Avola', 'Luigi Cinque', 'Alessio Fagioli', 'Gian Luca Foresti', 'Marco Raoul Marini', 'Alessio Mecca', 'Daniele Pannone']",2022-03-26T09:21:56Z,http://arxiv.org/abs/2203.14031v1
"BIMxAR: BIM-Empowered Augmented Reality for Learning Architectural
  Representations","Literature review shows limited research investigating the utilization of
Augmented Reality (AR) to improve learning and understanding architectural
representations, specifically section views. In this study, we present an AR
system prototype (BIMxAR), its new and accurate building-scale registration
method, and its novel visualization features that facilitate the comprehension
of building construction systems, materials configuration, and 3D section views
of complex structures through the integration of AR, Building Information
Modeling (BIM), and physical buildings. A pilot user study found improvements
after students studied building section views in a physical building with AR,
though not statistically significant, in terms of scores of the Santa Barbara
Solids Test (SBST) and the Architectural Representations Test (ART). When
incorporating time as a performance factor, the ART timed scores show a
significant improvement in the posttest session. BIMxAR has the potential to
enhance the students spatial abilities, particularly in understanding buildings
and complex section views.","['Ziad Ashour', 'Zohreh Shaghaghian', 'Wei Yan']",2022-04-07T04:32:43Z,http://arxiv.org/abs/2204.03207v2
"Understanding AR Activism: An Interview Study with Creators of Augmented
  Reality Experiences for Social Change","The rise of consumer augmented reality (AR) technology has opened up new
possibilities for interventions intended to disrupt and subvert cultural
conventions. From defacing corporate logos to erecting geofenced digital
monuments, more and more people are creating AR experiences for social causes.
We sought to understand this new form of activism, including why people use AR
for these purposes, opportunities and challenges in using it, and how well it
can support activist goals. We conducted semi-structured interviews with twenty
people involved in projects that used AR for a social cause across six
different countries. We found that AR can overcome physical world limitations
of activism to convey immersive, multilayered narratives that aim to reveal
invisible histories and perspectives. At the same time, people experienced
challenges in creating, maintaining, and distributing their AR experiences to
audiences. We discuss open questions and opportunities for creating AR tools
and experiences for social change.","['Rafael M. L. Silva', 'Erica Principe Cruz', 'Daniela K. Rosner', 'Dayton Kelly', 'Andrés Monroy-Hernández', 'Fannie Liu']",2022-04-21T00:12:55Z,http://arxiv.org/abs/2204.09821v1
"Microvision: Static analysis-based approach to visualizing microservices
  in augmented reality","Microservices are supporting digital transformation; however, fundamental
tools and system perspectives are missing to better observe, understand, and
manage these systems, their properties, and their dependencies. Microservices
architecture leans toward decentralization, which yields many advantages to
system operation; it, however, brings challenges to their development.
Microservices lack a system-centric perspective to better cope with system
evolution and quality assessment. In this work, we explore
microservice-specific architecture reconstruction based on static analysis.
Such reconstruction typically results in system models to visualize selected
system-centric perspectives. Conventional models are limited in utility when
the service cardinality is high. We consider an alternative data visualization
using 3D space using augmented reality. To begin testing the feasibility of
deriving such perspectives from microservice systems, we developed and
implemented prototype tools for software architecture reconstruction and
visualization of compared perspectives.","['Tomas Cerny', 'Amr S. Abdelfattah', 'Vincent Bushong', 'Abdullah Al Maruf', 'Davide Taibi']",2022-07-06T21:19:19Z,http://arxiv.org/abs/2207.02974v2
"A Brief Note on Building Augmented Reality Models for Scientific
  Visualization","Augmented reality (AR) has revolutionized the video game industry by
providing interactive, three-dimensional visualization. Interestingly, AR
technology has only been sparsely used in scientific visualization. This is, at
least in part, due to the significant technical challenges previously
associated with creating and accessing such models. To ease access to AR for
the scientific community, we introduce a novel visualization pipeline with
which they can create and render AR models. We demonstrate our pipeline by
means of finite element results, but note that our pipeline is generally
applicable to data that may be represented through meshed surfaces.
Specifically, we use two open-source software packages, ParaView and Blender.
The models are then rendered through the <model-viewer> platform, which we
access through Android and iOS smartphones. To demonstrate our pipeline, we
build AR models from static and time-series results of finite element
simulations discretized with continuum, shell, and beam elements. Moreover, we
openly provide python scripts to automate this process. Thus, others may use
our framework to create and render AR models for their own research and
teaching activities.","['Mrudang Mathur', 'Josef M. Brozovich', 'Manuel K. Rausch']",2022-07-30T19:51:29Z,http://arxiv.org/abs/2208.02022v1
Evaluating Cardiovascular Surgical Planning in Mobile Augmented Reality,"Advanced surgical procedures for congenital heart diseases (CHDs) require
precise planning before the surgeries. The conventional approach utilizes
3D-printing and cutting physical heart models, which is a time and resource
intensive process. While rapid advances in augmented reality (AR) technologies
have the potential to streamline surgical planning, there is limited research
that evaluates such AR approaches with medical experts. This paper presents an
evaluation with 6 experts, 4 cardiothoracic surgeons, and 2 cardiologists, from
Children's Healthcare of Atlanta (CHOA) Heart Center to validate the usability
and technical innovations of CardiacAR, a prototype mobile AR surgical planning
application. Potential future improvements based on user feedback are also
proposed to further improve the design of CardiacAR and broaden its access.","['Haoyang Yang', 'Pratham Darrpan Mehta', 'Jonathan Leo', 'Zhiyan Zhou', 'Megan Dass', 'Anish Upadhayay', 'Timothy C. Slesnick', 'Fawwaz Shaw', 'Amanda Randles', 'Duen Horng Chau']",2022-08-22T22:27:59Z,http://arxiv.org/abs/2208.10639v1
"Design and Implementation of a Human-Robot Joint Action Framework using
  Augmented Reality and Eye Gaze","When humans work together to complete a joint task, each person builds an
internal model of the situation and how it will evolve. Efficient collaboration
is dependent on how these individual models overlap to form a shared mental
model among team members, which is important for collaborative processes in
human-robot teams. The development and maintenance of an accurate shared mental
model requires bidirectional communication of individual intent and the ability
to interpret the intent of other team members. To enable effective human-robot
collaboration, this paper presents a design and implementation of a novel joint
action framework in human-robot team collaboration, utilizing augmented reality
(AR) technology and user eye gaze to enable bidirectional communication of
intent. We tested our new framework through a user study with 37 participants,
and found that our system improves task efficiency, trust, as well as task
fluency. Therefore, using AR and eye gaze to enable bidirectional communication
is a promising mean to improve core components that influence collaboration
between humans and robots.","['Wesley P. Chan', 'Morgan Crouch', 'Khoa Hoang', 'Charlie Chen', 'Nicole Robinson', 'Elizabeth Croft']",2022-08-25T03:48:12Z,http://arxiv.org/abs/2208.11856v1
"You Can't Hide Behind Your Headset: User Profiling in Augmented and
  Virtual Reality","Virtual and Augmented Reality (VR, AR) are increasingly gaining traction
thanks to their technical advancement and the need for remote connections,
recently accentuated by the pandemic. Remote surgery, telerobotics, and virtual
offices are only some examples of their successes. As users interact with
VR/AR, they generate extensive behavioral data usually leveraged for measuring
human behavior. However, little is known about how this data can be used for
other purposes.
  In this work, we demonstrate the feasibility of user profiling in two
different use-cases of virtual technologies: AR everyday application ($N=34$)
and VR robot teleoperation ($N=35$). Specifically, we leverage machine learning
to identify users and infer their individual attributes (i.e., age, gender). By
monitoring users' head, controller, and eye movements, we investigate the ease
of profiling on several tasks (e.g., walking, looking, typing) under different
mental loads. Our contribution gives significant insights into user profiling
in virtual environments.","['Pier Paolo Tricomi', 'Federica Nenna', 'Luca Pajola', 'Mauro Conti', 'Luciano Gamberini']",2022-09-22T08:29:32Z,http://arxiv.org/abs/2209.10849v1
"Towards Semi-automatic Detection and Localization of Indoor
  Accessibility Issues using Mobile Depth Scanning and Computer Vision","To help improve the safety and accessibility of indoor spaces, researchers
and health professionals have created assessment instruments that enable
homeowners and trained experts to audit and improve homes. With advances in
computer vision, augmented reality (AR), and mobile sensors, new approaches are
now possible. We introduce RASSAR (Room Accessibility and Safety Scanning in
Augmented Reality), a new proof-of-concept prototype for semi-automatically
identifying, categorizing, and localizing indoor accessibility and safety
issues using LiDAR + camera data, machine learning, and AR. We present an
overview of the current RASSAR prototype and a preliminary evaluation in a
single home.","['Xia Su', 'Kaiming Cheng', 'Han Zhang', 'Jaewook Lee', 'Jon E. Froehlich']",2022-10-05T20:07:05Z,http://arxiv.org/abs/2210.02533v1
"Learning Visualization Policies of Augmented Reality for Human-Robot
  Collaboration","In human-robot collaboration domains, augmented reality (AR) technologies
have enabled people to visualize the state of robots. Current AR-based
visualization policies are designed manually, which requires a lot of human
efforts and domain knowledge. When too little information is visualized, human
users find the AR interface not useful; when too much information is
visualized, they find it difficult to process the visualized information. In
this paper, we develop a framework, called VARIL, that enables AR agents to
learn visualization policies (what to visualize, when, and how) from
demonstrations. We created a Unity-based platform for simulating warehouse
environments where human-robot teammates collaborate on delivery tasks. We have
collected a dataset that includes demonstrations of visualizing robots' current
and planned behaviors. Results from experiments with real human participants
show that, compared with competitive baselines from the literature, our learned
visualization strategies significantly increase the efficiency of human-robot
teams, while reducing the distraction level of human users. VARIL has been
demonstrated in a built-in-lab mock warehouse.","['Kishan Chandan', 'Jack Albertson', 'Shiqi Zhang']",2022-11-13T22:03:20Z,http://arxiv.org/abs/2211.07028v1
Mobile Augmented Reality with Federated Learning in the Metaverse,"The Metaverse is deemed the next evolution of the Internet and has received
much attention recently. Metaverse applications via mobile augmented reality
(MAR) require rapid and accurate object detection to mix digital data with the
real world. As mobile devices evolve, their computational capabilities are
increasing, and thus their computational resources can be leveraged to train
machine learning models. In light of the increasing concerns of user privacy
and data security, federated learning (FL) has become a promising distributed
learning framework for privacy-preserving analytics. In this article, FL and
MAR are brought together in the Metaverse. We discuss the necessity and
rationality of the combination of FL and MAR. The prospective technologies that
support FL and MAR in the Metaverse are also discussed. In addition, existing
challenges that prevent the fulfillment of FL and MAR in the Metaverse and
several application scenarios are presented. Finally, three case studies of
Metaverse FL-MAR systems are demonstrated.","['Xinyu Zhou', 'Jun Zhao']",2022-12-16T07:53:05Z,http://arxiv.org/abs/2212.08324v2
Exploring Text Selection in Augmented Reality Systems,"Text selection is a common and essential activity during text interaction in
all interactive systems. As Augmented Reality (AR) head-mounted displays (HMDs)
become more widespread, they will need to provide effective interaction
techniques for text selection that ensure users can complete a range of text
manipulation tasks (e.g., to highlight, copy, and paste text, send instant
messages, and browse the web). As a relatively new platform, text selection in
AR is largely unexplored and the suitability of interaction techniques
supported by current AR HMDs for text selection tasks is unclear. This research
aims to fill this gap and reports on an experiment with 12 participants, which
compares the performance and usability (user experience and workload) of four
possible techniques (Hand+Pinch, Hand+Dwell, Head+Pinch, and Head+Dwell). Our
results suggest that Head+Dwell should be the default selection technique, as
it is relatively fast, has the lowest error rate and workload, and has the
highest-rated user experience and social acceptance.","['Xinyi Liu', 'Xuanru Meng', 'Becky Spittle', 'Wenge Xu', 'BoYu Gao', 'Hai-Ning Liang']",2022-12-29T15:09:57Z,http://arxiv.org/abs/2212.14336v1
"Multi-Camera Lighting Estimation for Photorealistic Front-Facing Mobile
  Augmented Reality","Lighting understanding plays an important role in virtual object composition,
including mobile augmented reality (AR) applications. Prior work often targets
recovering lighting from the physical environment to support photorealistic AR
rendering. Because the common workflow is to use a back-facing camera to
capture the physical world for overlaying virtual objects, we refer to this
usage pattern as back-facing AR. However, existing methods often fall short in
supporting emerging front-facing mobile AR applications, e.g., virtual try-on
where a user leverages a front-facing camera to explore the effect of various
products (e.g., glasses or hats) of different styles. This lack of support can
be attributed to the unique challenges of obtaining 360$^\circ$ HDR environment
maps, an ideal format of lighting representation, from the front-facing camera
and existing techniques. In this paper, we propose to leverage dual-camera
streaming to generate a high-quality environment map by combining multi-view
lighting reconstruction and parametric directional lighting estimation. Our
preliminary results show improved rendering quality using a dual-camera setup
for front-facing AR compared to a commercial solution.","['Yiqin Zhao', 'Sean Fanello', 'Tian Guo']",2023-01-15T16:52:59Z,http://arxiv.org/abs/2301.06143v1
"Evaluating Digital Work Instructions with Augmented Reality versus
  Paper-based Documents for Manual, Object-Specific Repair Tasks in a Case
  Study with Experienced Workers","Manual repair tasks in the industry of maintenance, repair, and overhaul
require experience and object-specific information. Today, many of these repair
tasks are still performed and documented with inefficient paper documents.
Cognitive assistance systems have the potential to reduce costs, errors, and
mental workload by providing all required information digitally. In this case
study, we present an assistance system for object-specific repair tasks for
turbine blades. The assistance system provides digital work instructions and
uses augmented reality to display spatial information. In a user study with ten
experienced metalworkers performing a familiar repair task, we compare time to
task completion, subjective workload, and system usability of the new
assistance system to their established paper-based workflow. All participants
stated that they preferred the assistance system over the paper documents. The
results of the study show that the manual repair task can be completed 21 %
faster and with a 26 % lower perceived workload using the assistance system.","['Leon Eversberg', 'Jens Lambrecht']",2023-01-18T14:46:07Z,http://arxiv.org/abs/2301.07570v2
"Evaluating the Possibility of Integrating Augmented Reality and Internet
  of Things Technologies to Help Patients with Alzheimer's Disease","People suffering from Alzheimer's disease (AD) and their caregivers seek
different approaches to cope with memory loss. Although AD patients want to
live independently, they often need help from caregivers. In this situation,
caregivers may attach notes on every single object or take out the contents of
a drawer to make them visible before leaving the patient alone at home. This
study reports preliminary results on an Ambient Assisted Living (AAL) real-time
system, achieved through the Internet of Things (IoT) and Augmented Reality
(AR) concepts, aimed at helping people suffering from AD. The system has two
main sections: the smartphone or windows application allows caregivers to
monitor patients' status at home and be notified if patients are at risk. The
second part allows patients to use smart glasses to recognize QR codes in the
environment and receive information related to tags in the form of audio, text,
or three-dimensional image. This work presents preliminary results and
investigates the possibility of implementing such a system.","['Fatemeh Ghorbani', 'Mohammad Kia', 'Mehdi Delrobaei', 'Quazi Rahman']",2023-01-20T20:39:32Z,http://arxiv.org/abs/2301.08795v1
"Resource Allocation of Federated Learning Assisted Mobile Augmented
  Reality System in the Metaverse","Metaverse has become a buzzword recently. Mobile augmented reality (MAR) is a
promising approach to providing users with an immersive experience in the
Metaverse. However, due to limitations of bandwidth, latency and computational
resources, MAR cannot be applied on a large scale in the Metaverse yet.
Moreover, federated learning, with its privacy-preserving characteristics, has
emerged as a prospective distributed learning framework in the future Metaverse
world. In this paper, we propose a federated learning assisted MAR system via
non-orthogonal multiple access for the Metaverse. Additionally, to optimize a
weighted sum of energy, latency and model accuracy, a resource allocation
algorithm is devised by setting appropriate transmission power, CPU frequency
and video frame resolution for each user. Experimental results demonstrate that
our proposed algorithm achieves an overall good performance compared to a
random algorithm and greedy algorithm.","['Xinyu Zhou', 'Yang Li', 'Jun Zhao']",2023-01-28T04:14:43Z,http://arxiv.org/abs/2301.12085v2
Explainable Human-Robot Training and Cooperation with Augmented Reality,"The current spread of social and assistive robotics applications is
increasingly highlighting the need for robots that can be easily taught and
interacted with, even by users with no technical background. Still, it is often
difficult to grasp what such robots know or to assess if a correct
representation of the task is being formed. Augmented Reality (AR) has the
potential to bridge this gap. We demonstrate three use cases where AR design
elements enhance the explainability and efficiency of human-robot interaction:
1) a human teaching a robot some simple kitchen tasks by demonstration, 2) the
robot showing its plan for solving novel tasks in AR to a human for validation,
and 3) a robot communicating its intentions via AR while assisting people with
limited mobility during daily activities.","['Chao Wang', 'Anna Belardinelli', 'Stephan Hasler', 'Theodoros Stouraitis', 'Daniel Tanneberg', 'Michael Gienger']",2023-02-02T12:07:34Z,http://arxiv.org/abs/2302.01039v1
"Comparing 2D and Augmented Reality Visualizations for Microservice
  System Understandability: A Controlled Experiment","Microservice-based systems are often complex to understand, especially when
their sizes grow. Abstracted views help practitioners with the system
understanding from a certain perspective. Recent advancement in interactive
data visualization begs the question of whether established software
engineering models to visualize system design remain the most suited approach
for the service-oriented design of microservices. Our recent work proposed
presenting a 3D visualization for microservices in augmented reality. This
paper analyzes whether such an approach brings any benefits to practitioners
when dealing with selected architectural questions related to system design
quality. For this purpose, we conducted a controlled experiment involving 20
participants investigating their performance in identifying service dependency,
service cardinality, and bottlenecks. Results show that the 3D enables novices
to perform as well as experts in the detection of service dependencies,
especially in large systems, while no differences are reported for the
identification of service cardinality and bottlenecks. We recommend industry
and researchers to further investigate AR for microservice architectural
analysis, especially to ease the onboarding of new developers in
microservice~projects.","['Amr S. Abdelfattah', 'Tomas Cerny', 'Davide Taibi', 'Sira Vegas']",2023-03-03T23:50:14Z,http://arxiv.org/abs/2303.02268v1
The Dark Side of Augmented Reality: Exploring Manipulative Designs in AR,"Augmented Reality (AR) applications are becoming more mainstream, with
successful examples in the mobile environment like Pokemon GO. Current
malicious techniques can exploit these environments' immersive and mixed nature
(physical-virtual) to trick users into providing more personal information,
i.e., dark patterns. Dark patterns are deceiving techniques (e.g., interface
tricks) designed to influence individuals' behavioural decisions. However,
there are few studies regarding dark patterns' potential issues in AR
environments. In this work, using scenario construction to build our
prototypes, we investigate the potential future approaches that dark patterns
can have. We use VR mockups in our user study to analyze the effects of dark
patterns in AR. Our study indicates that dark patterns are effective in
immersive scenarios, and the use of novel techniques such as `haptic grabbing'
to drag participants' attention can influence their movements. Finally, we
discuss the impact of such malicious techniques and what techniques can
mitigate them.","['Xian Wang', 'Lik-Hang Lee', 'Carlos Bermejo Fernandez', 'Pan Hui']",2023-03-06T02:40:43Z,http://arxiv.org/abs/2303.02843v2
"Enhancing Human-robot Collaboration by Exploring Intuitive Augmented
  Reality Design Representations","As the use of Augmented Reality (AR) to enhance interactions between human
agents and robotic systems in a work environment continues to grow, robots must
communicate their intents in informative yet straightforward ways. This
improves the human agent's feeling of trust and safety in the work environment
while also reducing task completion time. To this end, we discuss a set of
guidelines for the systematic design of AR interfaces for Human-Robot
Interaction (HRI) systems. Furthermore, we develop design frameworks that would
ride on these guidelines and serve as a base for researchers seeking to explore
this direction further. We develop a series of designs for visually
representing the robot's planned path and reactions, which we evaluate by
conducting a user survey involving 14 participants. Subjects were given
different design representations to review and rate based on their
intuitiveness and informativeness. The collated results showed that our design
representations significantly improved the participants' ease of understanding
the robot's intents over the baselines for the robot's proposed navigation
path, planned arm trajectory, and reactions.","['Chrisantus Eze', 'Christopher Crick']",2023-03-09T19:03:59Z,http://arxiv.org/abs/2303.05539v1
"Adaptive Local Adversarial Attacks on 3D Point Clouds for Augmented
  Reality","As the key technology of augmented reality (AR), 3D recognition and tracking
are always vulnerable to adversarial examples, which will cause serious
security risks to AR systems. Adversarial examples are beneficial to improve
the robustness of the 3D neural network model and enhance the stability of the
AR system. At present, most 3D adversarial attack methods perturb the entire
point cloud to generate adversarial examples, which results in high
perturbation costs and difficulty in reconstructing the corresponding real
objects in the physical world. In this paper, we propose an adaptive local
adversarial attack method (AL-Adv) on 3D point clouds to generate adversarial
point clouds. First, we analyze the vulnerability of the 3D network model and
extract the salient regions of the input point cloud, namely the vulnerable
regions. Second, we propose an adaptive gradient attack algorithm that targets
vulnerable regions. The proposed attack algorithm adaptively assigns different
disturbances in different directions of the three-dimensional coordinates of
the point cloud. Experimental results show that our proposed method AL-Adv
achieves a higher attack success rate than the global attack method.
Specifically, the adversarial examples generated by the AL-Adv demonstrate good
imperceptibility and small generation costs.","['Weiquan Liu', 'Shijun Zheng', 'Cheng Wang']",2023-03-12T11:52:02Z,http://arxiv.org/abs/2303.06641v1
"Investigating the Characteristics and Performance of Augmented Reality
  Applications on Head-Mounted Displays: A Study of the Hololens Application
  Store","Augmented Reality (AR) based on Head-Mounted Displays (HMDs) has gained
significant traction over the recent years. Nevertheless, it remains unclear
what AR HMD-based applications have been developed over the years and what
their system performance is when they are run on HMDs. In this paper, we aim to
shed light into this direction. Our study focuses on the applications available
on the Microsoft Hololens application store given the wide use of the Hololens
headset. Our study has two major parts: (i) we collect metadata about the
applications available on the Microsoft Hololens application store to
understand their characteristics (e.g., categories, pricing, permissions
requested, hardware and software compatibility); and (ii) we interact with
these applications while running on a Hololens 2 headset and collect data about
systems-related metrics (e.g., memory and storage usage, time spent on CPU and
GPU related operations) to investigate the systems performance of applications.
Our study has resulted in several interesting findings, which we share with the
research community.","['Pubudu Wijesooriya', 'Sheikh Muhammad Farjad', 'Nikolaos Stergiou', 'Spyridon Mastorakis']",2023-03-13T23:18:17Z,http://arxiv.org/abs/2303.07523v1
"HoloTouch: Interacting with Mixed Reality Visualizations Through
  Smartphone Proxies","We contribute interaction techniques for augmenting mixed reality (MR)
visualizations with smartphone proxies. By combining head-mounted displays
(HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D
charts with precise touch input, haptics feedback, high-resolution 2D graphics,
and physical manipulation. Our approach aims to complement both MR and physical
visualizations. Most current MR visualizations suffer from unreliable tracking,
low visual resolution, and imprecise input. Data physicalizations on the other
hand, although allowing for natural physical manipulation, are limited in
dynamic and interactive modification. We demonstrate how mobile devices such as
smartphones or tablets can serve as physical proxies for MR data interactions,
creating dynamic visualizations that support precise manipulation and rich
input and output. We describe 6 interaction techniques that leverage the
combined physicality, sensing, and output capabilities of HMDs and smartphones,
and demonstrate those interactions via a prototype system. Based on an
evaluation, we outline opportunities for combining the advantages of both MR
and physical charts.","['Neil Chulpongsatorn', 'Wesley Willett', 'Ryo Suzuki']",2023-03-15T20:19:13Z,http://arxiv.org/abs/2303.08916v1
"Supporting Piggybacked Co-Located Leisure Activities via Augmented
  Reality","Technology, especially the smartphone, is villainized for taking meaning and
time away from in-person interactions and secluding people into ""digital
bubbles"". We believe this is not an intrinsic property of digital gadgets, but
evidence of a lack of imagination in technology design. Leveraging augmented
reality (AR) toward this end allows us to create experiences for multiple
people, their pets, and their environments. In this work, we explore the design
of AR technology that ""piggybacks"" on everyday leisure to foster co-located
interactions among close ties (with other people and pets. We designed,
developed, and deployed three such AR applications, and evaluated them through
a 41-participant and 19-pet user study. We gained key insights about the
ability of AR to spur and enrich interaction in new channels, the importance of
customization, and the challenges of designing for the physical aspects of AR
devices (e.g., holding smartphones). These insights guide design implications
for the novel research space of co-located AR.","['Samantha Reig', 'Erica Principe Cruz', 'Melissa M. Powers', 'Jennifer He', 'Timothy Chong', 'Yu Jiang Tham', 'Sven Kratz', 'Ava Robinson', 'Brian A. Smith', 'Rajan Vaish', 'Andrés Monroy-Hernández']",2023-03-19T03:09:08Z,http://arxiv.org/abs/2303.10546v1
"Augmented reality as a Thirdspace: Simultaneous experience of the
  physical and virtual","With the proliferation of devices that display augmented reality (AR), now is
the time for scholars and practitioners to evaluate and engage critically with
emerging applications of the medium. AR mediates the way users see their
bodies, hear their environment and engage with places. Applied in various
forms, including social media, e-commerce, gaming, enterprise and art, the
medium facilitates a hybrid experience of physical and digital spaces. This
article employs a model of real-and-imagined space from geographer Edward Soja
to examine how the user of an AR app navigates the two intertwined spaces of
physical and digital, experiencing what Soja calls a 'Third-space'. The article
illustrates the potential for headset-based AR to engender such a Thirdspace
through the author's practice-led research project, the installation Through
the Wardrobe. This installation demonstrates how AR has the potential to shift
the way that users view and interact with their world with artistic
applications providing an opportunity to question assumptions of social norms,
identity and uses of physical space.",['Rob Eagle'],2023-03-21T22:46:22Z,http://arxiv.org/abs/2303.13550v1
Cross-View Visual Geo-Localization for Outdoor Augmented Reality,"Precise estimation of global orientation and location is critical to ensure a
compelling outdoor Augmented Reality (AR) experience. We address the problem of
geo-pose estimation by cross-view matching of query ground images to a
geo-referenced aerial satellite image database. Recently, neural network-based
methods have shown state-of-the-art performance in cross-view matching.
However, most of the prior works focus only on location estimation, ignoring
orientation, which cannot meet the requirements in outdoor AR applications. We
propose a new transformer neural network-based model and a modified triplet
ranking loss for joint location and orientation estimation. Experiments on
several benchmark cross-view geo-localization datasets show that our model
achieves state-of-the-art performance. Furthermore, we present an approach to
extend the single image query-based geo-localization approach by utilizing
temporal information from a navigation pipeline for robust continuous
geo-localization. Experimentation on several large-scale real-world video
sequences demonstrates that our approach enables high-precision and stable AR
insertion.","['Niluthpol Chowdhury Mithun', 'Kshitij Minhas', 'Han-Pang Chiu', 'Taragay Oskiper', 'Mikhail Sizintsev', 'Supun Samarasekera', 'Rakesh Kumar']",2023-03-28T01:58:03Z,http://arxiv.org/abs/2303.15676v1
"Inside-out Infrared Marker Tracking via Head Mounted Displays for Smart
  Robot Programming","Intuitive robot programming through use of tracked smart input devices relies
on fixed, external tracking systems, most often employing infra-red markers.
Such an approach is frequently combined with projector-based augmented reality
for better visualisation and interface. The combined system, although providing
an intuitive programming platform with short cycle times even for inexperienced
users, is immobile, expensive and requires extensive calibration. When faced
with a changing environment and large number of robots it becomes sorely
impractical. Here we present our work on infra-red marker tracking using the
Microsoft HoloLens head-mounted display. The HoloLens can map the environment,
register the robot on-line, and track smart devices equipped with infra-red
markers in the robot coordinate system. We envision our work to provide the
basis to transfer many of the paradigms developed over the years for systems
requiring a projector and a tracked input device into a highly-portable system
that does not require any calibration or special set-up. We test the quality of
the marker-tracking in an industrial robot cell and compare our tracking with a
ground truth obtained via an ART-3 tracking system.","['David Puljiz', 'Alexandru-George Vasilache', 'Michael Mende', 'Björn Hein']",2023-03-28T14:45:03Z,http://arxiv.org/abs/2303.16017v1
"Exploring the Design Space of Employing AI-Generated Content for
  Augmented Reality Display","As the two most common display content of Augmented Reality (AR), the
creation process of image and text often requires a human to execute. However,
due to the rapid advances in Artificial Intelligence (AI), today the media
content can be automatically generated by software. The ever-improving quality
of AI-generated content (AIGC) has opened up new scenarios employing such
content, which is expected to be applied in AR. In this paper, we attempt to
explore the design space for projecting AI-generated image and text into an AR
display. Specifically, we perform an exploratory study and suggest a
``user-function-environment'' design thinking by building a preliminary
prototype and conducting focus groups based on it. With the early insights
presented, we point out the design space and potential applications for
combining AIGC and AR.","['Yongquan Hu', 'Mingyue Yuan', 'Kaiqi Xian', 'Don Samitha Elvitigala', 'Aaron Quigley']",2023-03-29T11:07:27Z,http://arxiv.org/abs/2303.16593v1
"Multimodal Grounding for Embodied AI via Augmented Reality Headsets for
  Natural Language Driven Task Planning","Recent advances in generative modeling have spurred a resurgence in the field
of Embodied Artificial Intelligence (EAI). EAI systems typically deploy large
language models to physical systems capable of interacting with their
environment. In our exploration of EAI for industrial domains, we successfully
demonstrate the feasibility of co-located, human-robot teaming. Specifically,
we construct an experiment where an Augmented Reality (AR) headset mediates
information exchange between an EAI agent and human operator for a variety of
inspection tasks. To our knowledge the use of an AR headset for multimodal
grounding and the application of EAI to industrial tasks are novel
contributions within Embodied AI research. In addition, we highlight potential
pitfalls in EAI's construction by providing quantitative and qualitative
analysis on prompt robustness.","['Selma Wanna', 'Fabian Parra', 'Robert Valner', 'Karl Kruusamäe', 'Mitch Pryor']",2023-04-26T16:44:19Z,http://arxiv.org/abs/2304.13676v1
"ARDIE: AR, Dialogue, and Eye Gaze Policies for Human-Robot Collaboration","Human-robot collaboration (HRC) has become increasingly relevant in
industrial, household, and commercial settings. However, the effectiveness of
such collaborations is highly dependent on the human and robots' situational
awareness of the environment. Improving this awareness includes not only
aligning perceptions in a shared workspace, but also bidirectionally
communicating intent and visualizing different states of the environment to
enhance scene understanding. In this paper, we propose ARDIE (Augmented Reality
with Dialogue and Eye Gaze), a novel intelligent agent that leverages
multi-modal feedback cues to enhance HRC. Our system utilizes a decision
theoretic framework to formulate a joint policy that incorporates interactive
augmented reality (AR), natural language, and eye gaze to portray current and
future states of the environment. Through object-specific AR renders, the human
can visualize future object interactions to make adjustments as needed,
ultimately providing an interactive and efficient collaboration between humans
and robots.","['Chelsea Zou', 'Kishan Chandan', 'Yan Ding', 'Shiqi Zhang']",2023-05-08T13:01:27Z,http://arxiv.org/abs/2305.04685v1
"Digital Twin-Based 3D Map Management for Edge-Assisted Mobile Augmented
  Reality","In this paper, we design a 3D map management scheme for edge-assisted mobile
augmented reality (MAR) to support the pose estimation of individual MAR
device, which uploads camera frames to an edge server. Our objective is to
minimize the pose estimation uncertainty of the MAR device by periodically
selecting a proper set of camera frames for uploading to update the 3D map. To
address the challenges of the dynamic uplink data rate and the time-varying
pose of the MAR device, we propose a digital twin (DT)-based approach to 3D map
management. First, a DT is created for the MAR device, which emulates 3D map
management based on predicting subsequent camera frames. Second, a model-based
reinforcement learning (MBRL) algorithm is developed, utilizing the data
collected from both the actual and the emulated data to manage the 3D map. With
extensive emulated data provided by the DT, the MBRL algorithm can quickly
provide an adaptive map management policy in a highly dynamic environment.
Simulation results demonstrate that the proposed DT-based 3D map management
outperforms benchmark schemes by achieving lower pose estimation uncertainty
and higher data efficiency in dynamic environments.","['Conghao Zhou', 'Jie Gao', 'Mushu Li', 'Nan Cheng', 'Xuemin Shen', 'Weihua Zhuang']",2023-05-26T01:38:45Z,http://arxiv.org/abs/2305.16571v1
"Look-Ahead Task Offloading for Multi-User Mobile Augmented Reality in
  Edge-Cloud Computing","Mobile augmented reality (MAR) blends a real scenario with overlaid virtual
content, which has been envisioned as one of the ubiquitous interfaces to the
Metaverse. Due to the limited computing power and battery life of MAR devices,
it is common to offload the computation tasks to edge or cloud servers in close
proximity. However, existing offloading solutions developed for MAR tasks
suffer from high migration overhead, poor scalability, and short-sightedness
when applied in provisioning multi-user MAR services. To address these issues,
a MAR service-oriented task offloading scheme is designed and evaluated in
edge-cloud computing networks. Specifically, the task interdependency of MAR
applications is firstly analyzed and modeled by using directed acyclic graphs.
Then, we propose a look-ahead offloading scheme based on a modified Monte Carlo
tree (MMCT) search, which can run several multi-step executions in advance to
get an estimate of the long-term effect of immediate action. Experiment results
show that the proposed offloading scheme can effectively improve the quality of
service (QoS) in provisioning multi-user MAR services, compared to four
benchmark schemes. Furthermore, it is also shown that the proposed solution is
stable and suitable for applications in a highly volatile environment.","['Ruxiao Chen', 'Shuaishuai Guo']",2023-05-31T05:03:40Z,http://arxiv.org/abs/2305.19558v1
"Real-Time Onboard Object Detection for Augmented Reality: Enhancing
  Head-Mounted Display with YOLOv8","This paper introduces a software architecture for real-time object detection
using machine learning (ML) in an augmented reality (AR) environment. Our
approach uses the recent state-of-the-art YOLOv8 network that runs onboard on
the Microsoft HoloLens 2 head-mounted display (HMD). The primary motivation
behind this research is to enable the application of advanced ML models for
enhanced perception and situational awareness with a wearable, hands-free AR
platform. We show the image processing pipeline for the YOLOv8 model and the
techniques used to make it real-time on the resource-limited edge computing
platform of the headset. The experimental results demonstrate that our solution
achieves real-time processing without needing offloading tasks to the cloud or
any other external servers while retaining satisfactory accuracy regarding the
usual mAP metric and measured qualitative performance","['Mikołaj Łysakowski', 'Kamil Żywanowski', 'Adam Banaszczyk', 'Michał R. Nowicki', 'Piotr Skrzypczyński', 'Sławomir K. Tadeja']",2023-06-06T09:35:45Z,http://arxiv.org/abs/2306.03537v1
"ARCOR2: Framework for Collaborative End-User Management of Industrial
  Robotic Workplaces using Augmented Reality","This paper presents a novel framework enabling end-users to perform the
management of complex robotic workplaces using a tablet and augmented reality.
The framework allows users to commission the workplace comprising different
types of robots, machines, or services irrespective of the vendor, set
task-important points in space, specify program steps, generate a code, and
control its execution. More users can collaborate simultaneously, for instance,
within a large-scale workplace. Spatially registered visualization and
programming enable a fast and easy understanding of workplace processes, while
high precision is achieved by combining kinesthetic teaching with specific
graphical tools for relative manipulation of poses. A visually defined program
is for execution translated into Python representation, allowing efficient
involvement of experts. The system was designed and developed in cooperation
with a system integrator based on an offline printed circuit board testing use
case, and its user interface was evaluated multiple times during the
development. The latest evaluation was performed by three experts and indicates
the high potential of the solution.","['Michal Kapinus', 'Zdeněk Materna', 'Daniel Bambušek', 'Vítězslav Beran', 'Pavel Smrž']",2023-06-14T12:20:23Z,http://arxiv.org/abs/2306.08464v1
"A Design Approach and Prototype Implementation for Factory Monitoring
  Based on Virtual and Augmented Reality at the Edge of Industry 4.0","Virtual and augmented reality are currently enjoying a great deal of
attention from the research community and the industry towards their adoption
within industrial spaces and processes. However, the current design and
implementation landscape is still very fluid, while the community as a whole
has not yet consolidated into concrete design directions, other than basic
patterns. Other open issues include the choice over a cloud or edge-based
architecture when designing such systems. Within this work, we present our
approach for a monitoring intervention inside a factory space utilizing both VR
and AR, based primarily on edge computing, while also utilizing the cloud. We
discuss its main design directions, as well as a basic ontology to aid in
simple description of factory assets. In order to highlight the design aspects
of our approach, we present a prototype implementation, based on a use case
scenario in a factory site, within the context of the ENERMAN H2020 project.","['Christos Anagnostopoulos', 'Georgios Mylonas', 'Apostolos P. Fournaris', 'Christos Koulamas']",2023-06-16T08:50:08Z,http://arxiv.org/abs/2306.09692v1
"Assessing Augmented Reality Selection Techniques for Passengers in
  Moving Vehicles: A Real-World User Study","Nowadays, cars offer many possibilities to explore the world around you by
providing location-based information displayed on a 2D-Map. However, this
information is often only available to front-seat passengers while being
restricted to in-car displays. To propose a more natural way of interacting
with the environment, we implemented an augmented reality head-mounted display
to overlay points of interest onto the real world. We aim to compare multiple
selection techniques for digital objects located outside a moving car by
investigating head gaze with dwell time, head gaze with hardware button, eye
gaze with hardware button, and hand pointing with gesture confirmation. Our
study was conducted in a moving car under real-world conditions (N=22), with
significant results indicating that hand pointing usage led to slower and less
precise content selection while eye gaze was preferred by participants and
performed on par with the other techniques.","['Robin Connor Schramm', 'Markus Sasalovici', 'Axel Hildebrand', 'Ulrich Schwanecke']",2023-07-12T13:58:11Z,http://arxiv.org/abs/2307.06173v1
Design Patterns for Situated Visualization in Augmented Reality,"Situated visualization has become an increasingly popular research area in
the visualization community, fueled by advancements in augmented reality (AR)
technology and immersive analytics. Visualizing data in spatial proximity to
their physical referents affords new design opportunities and considerations
not present in traditional visualization, which researchers are now beginning
to explore. However, the AR research community has an extensive history of
designing graphics that are displayed in highly physical contexts. In this
work, we leverage the richness of AR research and apply it to situated
visualization. We derive design patterns which summarize common approaches of
visualizing data in situ. The design patterns are based on a survey of 293
papers published in the AR and visualization communities, as well as our own
expertise. We discuss design dimensions that help to describe both our patterns
and previous work in the literature. This discussion is accompanied by several
guidelines which explain how to apply the patterns given the constraints
imposed by the real world. We conclude by discussing future research directions
that will help establish a complete understanding of the design of situated
visualization, including the role of interactivity, tasks, and workflows.","['Benjamin Lee', 'Michael Sedlmair', 'Dieter Schmalstieg']",2023-07-18T11:34:28Z,http://arxiv.org/abs/2307.09157v2
"Accessibility and Inclusiveness of New Information and Communication
  Technologies for Disabled Users and Content Creators in the Metaverse","Despite the proliferation of Blockchain Metaverse projects, the inclusion of
physically disabled individuals in the Metaverse remains distant, with limited
standards and regulations in place. However, the article proposes a concept of
the Metaverse that leverages emerging technologies, such as Virtual and
Augmented Reality, and the Internet of Things, to enable greater engagement of
disabled creatives. This approach aims to enhance inclusiveness in the
Metaverse landscape. Based on the findings, the paper concludes that the active
involvement of physically disabled individuals in the design and development of
Metaverse platforms is crucial for promoting inclusivity. The proposed
framework for accessibility and inclusiveness in Virtual, Augmented, and Mixed
realities of decentralised Metaverses provides a basis for the meaningful
participation of disabled creatives. The article emphasises the importance of
addressing the mechanisms for art production by individuals with disabilities
in the emerging Metaverse landscape. Additionally, it highlights the need for
further research and collaboration to establish standards and regulations that
facilitate the inclusion of physically disabled individuals in Metaverse
projects.","['Petar Radanliev', 'David De Roure', 'Peter Novitzky', 'Ivo Sluganovic']",2023-08-01T18:39:12Z,http://arxiv.org/abs/2308.01925v1
"Using Abstract Tangible Proxy Objects for Interaction in Optical
  See-through Augmented Reality","Interaction with virtual objects displayed in Optical See-through Augmented
Reality is still mostly done with controllers or hand gestures. A much more
intuitive way of interacting with virtual content is to use physical proxy
objects to interact with the virtual objects. Here, the virtual model is
superimposed on a physical object, which can then be touched and moved to
interact with the virtual object. Since it is not possible to use an exact
replica as a tangible proxy object for every use case, we conducted a study to
determine the extent to which the shape of the physical object can deviate from
the shape of the virtual object without massively impacting performance and
usability, as well as the sense of presence. Our study, in which we
investigated different levels of abstraction for a sofa model, shows that the
physical proxy object can be abstracted to a certain degree. At the same time,
our results indicate that the physical object must have at least a similar
shape as the virtual object in order to serve as a suitable proxy.","['Denise Kahl', 'Antonio Krüger']",2023-08-10T19:34:55Z,http://arxiv.org/abs/2308.05836v1
Augmented Reality in Higher Education: a Case Study in Medical Education,"During lockdown, we piloted a variety of augmented reality (AR) experiences
in collaboration with subject matter experts from different fields aiming at
creating remote teaching and training experiences. In this paper, we present a
case study on how AR can be used as a teaching aid for medical education with
pertinent focus on remote and social distanced learning. We describe the
process of creating an AR experience that can enhance the knowledge and
understanding of anatomy for medical students. The Anatomy Experience is an AR
enhanced learning experience developed in collaboration with the Medical School
of the University of Edinburgh aiming to assist medical students understand the
complex geometry of different parts of the human body. After conducting a focus
group study with medical students, trainees, and trainers, we received very
positive feedback on the Anatomy Experience and its effects on understanding
anatomy, enriching the learning process, and using it as a tool for anatomy
teaching.","['Danai Korre', 'Andrew Sherlock']",2023-08-30T18:11:58Z,http://arxiv.org/abs/2308.16248v1
Chinese herb medicine in augmented reality,"Augmented reality becomes popular in education gradually, which provides a
contextual and adaptive learning experience. Here, we develop a Chinese herb
medicine AR platform based the 3dsMax and the Unity that allows users to
visualize and interact with the herb model and learn the related information.
The users use their mobile camera to scan the 2D herb picture to trigger the
presentation of 3D AR model and corresponding text information on the screen in
real-time. The system shows good performance and has high accuracy for the
identification of herbal medicine after interference test and occlusion test.
Users can interact with the herb AR model by rotating, scaling, and viewing
transformation, which effectively enhances learners' interest in Chinese herb
medicine.","['Qianyun Zhu', 'Yifeng Xie', 'Fangyang Ye', 'Zhenyuan Gao', 'Binjie Che', 'Zhenglin Chen', 'Dongmei Yu']",2023-09-25T07:12:58Z,http://arxiv.org/abs/2309.13909v1
"Breamy: An augmented reality mHealth prototype for surgical
  decision-making in breast cancer","In 2020, according to WHO, breast cancer affected 2.3 million women
worldwide, resulting in 685,000 fatalities. By the end of the year,
approximately 7.8 million women worldwide had survived their breast cancer
making it the most widespread form of cancer globally. Surgical treatment
decisions, including choosing between oncoplastic options, often require quick
decision-making within an 8-week time frame. However, many women lack the
necessary knowledge and preparation for making such complex informed decisions.
Anxiety and unsatisfactory outcomes can result from inadequate decision-making
processes, leading to complications and the need for revision surgeries. Shared
decision-making and personalized decision aids have shown positive effects on
patient satisfaction and treatment outcomes. This paper introduces Breamy, a
prototype mobile health (mHealth) application that utilizes augmented reality
(AR) technology to assist breast cancer patients in making informed decisions.
The app provides 3D visualizations of different oncoplastic procedures, aiming
to improve confidence in surgical decision-making, reduce decisional regret,
and enhance patient well-being after surgery. To determine the perception of
the usefulness of Breamy, we collected data from 166 participants through an
online survey. The results suggest that Breamy has the potential to reduce
patient's anxiety levels and assist them during the decision-making process.","['Niki Najafi', 'Miranda Addie', 'Sarkis Meterissian', 'Marta Kersten-Oertel']",2023-09-27T17:56:01Z,http://arxiv.org/abs/2309.15893v1
"A Real-time Method for Inserting Virtual Objects into Neural Radiance
  Fields","We present the first real-time method for inserting a rigid virtual object
into a neural radiance field, which produces realistic lighting and shadowing
effects, as well as allows interactive manipulation of the object. By
exploiting the rich information about lighting and geometry in a NeRF, our
method overcomes several challenges of object insertion in augmented reality.
For lighting estimation, we produce accurate, robust and 3D spatially-varying
incident lighting that combines the near-field lighting from NeRF and an
environment lighting to account for sources not covered by the NeRF. For
occlusion, we blend the rendered virtual object with the background scene using
an opacity map integrated from the NeRF. For shadows, with a precomputed field
of spherical signed distance field, we query the visibility term for any point
around the virtual object, and cast soft, detailed shadows onto 3D surfaces.
Compared with state-of-the-art techniques, our approach can insert virtual
object into scenes with superior fidelity, and has a great potential to be
further applied to augmented reality systems.","['Keyang Ye', 'Hongzhi Wu', 'Xin Tong', 'Kun Zhou']",2023-10-09T16:26:34Z,http://arxiv.org/abs/2310.05837v1
MISAR: A Multimodal Instructional System with Augmented Reality,"Augmented reality (AR) requires the seamless integration of visual, auditory,
and linguistic channels for optimized human-computer interaction. While
auditory and visual inputs facilitate real-time and contextual user guidance,
the potential of large language models (LLMs) in this landscape remains largely
untapped. Our study introduces an innovative method harnessing LLMs to
assimilate information from visual, auditory, and contextual modalities.
Focusing on the unique challenge of task performance quantification in AR, we
utilize egocentric video, speech, and context analysis. The integration of LLMs
facilitates enhanced state estimation, marking a step towards more adaptive AR
systems. Code, dataset, and demo will be available at
https://github.com/nguyennm1024/misar.","['Jing Bi', 'Nguyen Manh Nguyen', 'Ali Vosoughi', 'Chenliang Xu']",2023-10-18T04:15:12Z,http://arxiv.org/abs/2310.11699v1
"3D-Mirrorcle: Bridging the Virtual and Real through Depth Alignment in
  AR Mirror Systems","Smart mirrors have emerged as a new form of augmented reality (AR) interface
for home environments. However, due to the parallax in human vision, one major
challenge hindering their development is the depth misalignment between the 3D
mirror reflection and the 2D screen display. This misalignment causes the
display content to appear as if it is floating above the mirror, thereby
disrupting the seamless integration of the two components and impacting the
overall quality and functionality of the mirror. In this study, we introduce
3D-Mirrorcle, an innovative augmented reality (AR) mirror system that
effectively addresses the issue of depth disparity through a hardware-software
co-design on a lenticular grating setup. With our implemented real-time
position adjustment and depth adaptation algorithms, the screen display can be
dynamically aligned to the user's depth perception for a highly realistic and
engaging experience. Our method has been validated through a prototype and
hands-on user experiments that engaged 36 participants, and the results show
significant improvements in terms of accuracy (24.72% $\uparrow$),
immersion(31.4% $\uparrow$), and user satisfaction (44.4% $\uparrow$) compared
to the existing works.","['Yujia Liu', 'Qi Xin', 'Chenzhuo Xiang', 'Yu Zhang', 'Lun Yiu Nie', 'Yingqing Xu']",2023-10-20T16:07:23Z,http://arxiv.org/abs/2310.13617v2
Use of Augmented Reality in Human Wayfinding: A Systematic Review,"Augmented reality technology has emerged as a promising solution to assist
with wayfinding difficulties, bridging the gap between obtaining navigational
assistance and maintaining an awareness of one's real-world surroundings. This
article presents a systematic review of research literature related to AR
navigation technologies. An in-depth analysis of 65 salient studies was
conducted, addressing four main research topics: 1) current state-of-the-art of
AR navigational assistance technologies, 2) user experiences with these
technologies, 3) the effect of AR on human wayfinding performance, and 4)
impacts of AR on human navigational cognition. Notably, studies demonstrate
that AR can decrease cognitive load and improve cognitive map development, in
contrast to traditional guidance modalities. However, findings regarding
wayfinding performance and user experience were mixed. Some studies suggest
little impact of AR on improving outdoor navigational performance, and certain
information modalities may be distracting and ineffective. This article
discusses these nuances in detail, supporting the conclusion that AR holds
great potential in enhancing wayfinding by providing enriched navigational
cues, interactive experiences, and improved situational awareness.","['Zhiwen Qiu', 'Armin Mostafavi', 'Saleh Kalantari']",2023-11-20T17:00:44Z,http://arxiv.org/abs/2311.11923v1
"NavMarkAR: A Landmark-based Augmented Reality (AR) Wayfinding System for
  Enhancing Spatial Learning of Older Adults","Wayfinding in complex indoor environments is often challenging for older
adults due to declines in navigational and spatial-cognition abilities. This
paper introduces NavMarkAR, an augmented reality navigation system designed for
smart-glasses to provide landmark-based guidance, aiming to enhance older
adults' spatial navigation skills. This work addresses a significant gap in
design research, with limited prior studies evaluating cognitive impacts of AR
navigation systems. An initial usability test involved 6 participants, leading
to prototype refinements, followed by a comprehensive study with 32
participants in a university setting. Results indicate improved wayfinding
efficiency and cognitive map accuracy when using NavMarkAR. Future research
will explore long-term cognitive skill retention with such navigational aids.","['Zhiwen Qiu', 'Mojtaba Ashour', 'Xiaohe Zhou', 'Saleh Kalantari']",2023-11-20T22:23:03Z,http://arxiv.org/abs/2311.12220v2
"Multi-3D-Models Registration-Based Augmented Reality (AR) Instructions
  for Assembly","This paper introduces a novel, markerless, step-by-step, in-situ 3D Augmented
Reality (AR) instruction method and its application - BRICKxAR (Multi 3D
Models/M3D) - for small parts assembly. BRICKxAR (M3D) realistically visualizes
rendered 3D assembly parts at the assembly location of the physical assembly
model (Figure 1). The user controls the assembly process through a user
interface. BRICKxAR (M3D) utilizes deep learning-trained 3D model-based
registration. Object recognition and tracking become challenging as the
assembly model updates at each step. Additionally, not every part in a 3D
assembly may be visible to the camera during the assembly. BRICKxAR (M3D)
combines multiple assembly phases with a step count to address these
challenges. Thus, using fewer phases simplifies the complex assembly process
while step count facilitates accurate object recognition and precise
visualization of each step. A testing and heuristic evaluation of the BRICKxAR
(M3D) prototype and qualitative analysis were conducted with users and experts
in visualization and human-computer interaction. Providing robust 3D AR
instructions and allowing the handling of the assembly model, BRICKxAR (M3D)
has the potential to be used at different scales ranging from manufacturing
assembly to construction.","['Seda Tuzun Canadinc', 'Wei Yan']",2023-11-27T21:53:17Z,http://arxiv.org/abs/2311.16337v2
"Optimization in Mobile Augmented Reality Systems for the Metaverse over
  Wireless Communications","As the essential technical support for Metaverse, Mobile Augmented Reality
(MAR) has attracted the attention of many researchers. MAR applications rely on
real-time processing of visual and audio data, and thus those heavy workloads
can quickly drain the battery of a mobile device. To address such problem,
edge-based solutions have appeared for handling some tasks that require more
computing power. However, such strategies introduce a new trade-off: reducing
the network latency and overall energy consumption requires limiting the size
of the data sent to the edge server, which, in turn, results in lower accuracy.
In this paper, we design an edge-based MAR system and propose a mathematical
model to describe it and analyze the trade-off between latency, accuracy,
server resources allocation and energy consumption. Furthermore, an algorithm
named LEAO is proposed to solve this problem. We evaluate the performance of
the LEAO and other related algorithms across various simulation scenarios. The
results demonstrate the superiority of the LEAO algorithm. Finally, our work
provides insight into optimization problem in edge-based MAR system for
Metaverse.","['Tianming Lan', 'Jun Zhao']",2023-11-29T13:44:15Z,http://arxiv.org/abs/2311.17630v1
"Shared Affordance-awareness via Augmented Reality for Proactive
  Assistance in Human-robot Collaboration","Enabling humans and robots to collaborate effectively requires purposeful
communication and an understanding of each other's affordances. Prior work in
human-robot collaboration has incorporated knowledge of human affordances,
i.e., their action possibilities in the current context, into autonomous robot
decision-making. This ""affordance awareness"" is especially promising for
service robots that need to know when and how to assist a person that cannot
independently complete a task. However, robots still fall short in performing
many common tasks autonomously. In this work-in-progress paper, we propose an
augmented reality (AR) framework that bridges the gap in an assistive robot's
capabilities by actively engaging with a human through a shared
affordance-awareness representation. Leveraging the different perspectives from
a human wearing an AR headset and a robot's equipped sensors, we can build a
perceptual representation of the shared environment and model regions of
respective agent affordances. The AR interface can also allow both agents to
communicate affordances with one another, as well as prompt for assistance when
attempting to perform an action outside their affordance region. This paper
presents the main components of the proposed framework and discusses its
potential through a domestic cleaning task experiment.","['Drake Moore', 'Mark Zolotas', 'Taskin Padir']",2023-12-20T20:28:26Z,http://arxiv.org/abs/2312.13410v1
"Detection and Pose Estimation of flat, Texture-less Industry Objects on
  HoloLens using synthetic Training","Current state-of-the-art 6d pose estimation is too compute intensive to be
deployed on edge devices, such as Microsoft HoloLens (2) or Apple iPad, both
used for an increasing number of augmented reality applications. The quality of
AR is greatly dependent on its capabilities to detect and overlay geometry
within the scene. We propose a synthetically trained client-server-based
augmented reality application, demonstrating state-of-the-art object pose
estimation of metallic and texture-less industry objects on edge devices.
Synthetic data enables training without real photographs, i.e. for
yet-to-be-manufactured objects. Our qualitative evaluation on an AR-assisted
sorting task, and quantitative evaluation on both renderings, as well as
real-world data recorded on HoloLens 2, sheds light on its real-world
applicability.","['Thomas Pöllabauer', 'Fabian Rücker', 'Andreas Franek', 'Felix Gorschlüter']",2024-02-07T15:57:28Z,http://arxiv.org/abs/2402.04979v1
"Exploring the Opportunity of Augmented Reality (AR) in Supporting Older
  Adults Explore and Learn Smartphone Applications","The global aging trend compels older adults to navigate the evolving digital
landscape, presenting a substantial challenge in mastering smartphone
applications. While Augmented Reality (AR) holds promise for enhancing learning
and user experience, its role in aiding older adults' smartphone app
exploration remains insufficiently explored. Therefore, we conducted a
two-phase study: (1) a workshop with 18 older adults to identify app
exploration challenges and potential AR interventions, and (2) tech-probe
participatory design sessions with 15 participants to co-create AR support
tools. Our research highlights AR's effectiveness in reducing physical and
cognitive strain among older adults during app exploration, especially during
multi-app usage and the trial-and-error learning process. We also examined
their interactional experiences with AR, yielding design considerations on
tailoring AR tools for smartphone app exploration. Ultimately, our study
unveils the prospective landscape of AR in supporting the older demographic,
both presently and in future scenarios.","['Xiaofu Jin', 'Wai Tong', 'Xiaoying Wei', 'Xian Wang', 'Emily Kuang', 'Xiaoyu Mo', 'Huamin Qu', 'Mingming Fan']",2024-02-07T16:09:35Z,http://arxiv.org/abs/2402.04991v1
"ARCollab: Towards Multi-User Interactive Cardiovascular Surgical
  Planning in Mobile Augmented Reality","Surgical planning for congenital heart diseases requires a collaborative
approach, traditionally involving the 3D-printing of physical heart models for
inspection by surgeons and cardiologists. Recent advancements in mobile
augmented reality (AR) technologies have offered a promising alternative, noted
for their ease-of-use and portability. Despite this progress, there remains a
gap in research exploring the use of multi-user mobile AR environments for
facilitating collaborative cardiovascular surgical planning. We are developing
ARCollab, an iOS AR application designed to allow multiple surgeons and
cardiologists to interact with patient-specific 3D heart models in a shared
environment. ARCollab allows surgeons and cardiologists to import heart models,
perform gestures to manipulate the heart, and collaborate with other users
without having to produce a physical heart model. We are excited by the
potential for ARCollab to make long-term real-world impact, thanks to the
ubiquity of iOS devices that will allow for ARCollab's easy distribution,
deployment and adoption.","['Pratham Mehta', 'Harsha Karanth', 'Haoyang Yang', 'Timothy Slesnick', 'Fawwaz Shaw', 'Duen Horng Chau']",2024-02-07T18:29:38Z,http://arxiv.org/abs/2402.05075v1
"Exploring users' sense of safety in public using an Augmented Reality
  application","Nowadays, Augmented Reality (AR) is available on almost all smartphones
creating some exciting interaction opportunities but also challenges. For
example, already after the famous AR app Pokemon GO was released in July 2016,
numerous accidents related to the use of the app were reported by users. At the
same time, the spread of AR can be noticed in the tourism industry, enabling
tourists to explore their surroundings in new ways but also exposing them to
safety issues. This preliminary study explores users' sense of safety when
manipulating the amount and UI elements visualization parameters of Point of
Interest (POI) markers in a developed AR application. The results show that the
amount of POI markers that are displayed is significant for participants' sense
of safety. The influence of manipulating UI elements in terms of transparency,
color, and size cannot be proven. Nevertheless, most tested people stated that
manipulating transparency and size somehow influences their sense of safety, so
a closer look at them should be taken in future studies.","['Maurizio Vergari', 'Tanja Kojić', 'Nicole Stefanie Bertges', 'Francesco Vona', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2024-02-21T10:45:09Z,http://arxiv.org/abs/2402.13688v1
3D Printed Waveguide for Augmented Reality,"Mass production of augmented reality (AR) waveguides has been challenging due
to the intricate nature of the fabrication technique and the high precision
required for its optical characteristics. In this paper, we have presented a
novel and low-cost approach for fabricating geometric optical waveguides
designed for AR applications utilizing 3D printing techniques. To strike a
balance between optical performance and fabrication feasibility, we have
optimized the conventional geometric waveguide design to facilitate easier
fabrication. It is worth noting that our proposed method does not require
molding, dicing, and post-surface polishing after printing. A prototype based
on this method has been successfully fabricated, showing the immersion between
the virtual image and the real-world scene. The proposed method has great
potential for adaptation to mass production in various AR applications.","['Dechuan Sun', 'Gregory Tanyi', 'Alan Lee', 'Chris French', 'Younger Liang', 'Christina Lim', 'Ranjith R Unnithan']",2024-03-06T12:22:56Z,http://arxiv.org/abs/2403.03652v1
"Comparison of Spatial Visualization Techniques for Radiation in
  Augmented Reality","Augmented Reality (AR) provides a safe and low-cost option for hazardous
safety training that allows for the visualization of aspects that may be
invisible, such as radiation. Effectively visually communicating such threats
in the environment around the user is not straightforward. This work describes
visually encoding radiation using the spatial awareness mesh of an AR Head
Mounted Display. We leverage the AR device's GPUs to develop a real time
solution that accumulates multiple dynamic sources and uses stencils to prevent
an environment being over saturated with a visualization, as well as supporting
the encoding of direction explicitly in the visualization. We perform a user
study (25 participants) of different visualizations and obtain user feedback.
Results show that there are complex interactions and while no visual
representation was statistically superior or inferior, user opinions vary
widely. We also discuss the evaluation approaches and provide recommendations.","['Fintan McGee', 'Roderick McCall', 'Joan Baixauli']",2024-03-08T15:59:26Z,http://arxiv.org/abs/2403.05403v1
"Influence of Personality and Communication Behavior of a Conversational
  Agent on User Experience and Social Presence in Augmented Reality","A virtual embodiment can benefit conversational agents, but it is unclear how
their personalities and non-verbal behavior influence the User Experience and
Social Presence in Augmented Reality (AR). We asked 30 users to converse with a
virtual assistant who gives recommendations about city activities. The
participants interacted with two different personalities: Sammy, a cheerful
blue mouse, and Olive, a serious green human-like agent. Each was presented
with two body languages - happy/friendly and annoyed/unfriendly. We conclude
how agent representation and humor affect User Experience aspects, and that
body language is significant in the evaluation and perception of the AR agent.","['Katerina Koleva', 'Maurizio Vergari', 'Tanja Kojić', 'Sebastian Möller', 'Jan-Niklas Voigt-Antons']",2024-03-14T21:32:04Z,http://arxiv.org/abs/2403.09883v1
"Advancing 6D Pose Estimation in Augmented Reality -- Overcoming
  Projection Ambiguity with Uncontrolled Imagery","This study addresses the challenge of accurate 6D pose estimation in
Augmented Reality (AR), a critical component for seamlessly integrating virtual
objects into real-world environments. Our research primarily addresses the
difficulty of estimating 6D poses from uncontrolled RGB images, a common
scenario in AR applications, which lacks metadata such as focal length. We
propose a novel approach that strategically decomposes the estimation of z-axis
translation and focal length, leveraging the neural-render and compare strategy
inherent in the FocalPose architecture. This methodology not only streamlines
the 6D pose estimation process but also significantly enhances the accuracy of
3D object overlaying in AR settings. Our experimental results demonstrate a
marked improvement in 6D pose estimation accuracy, with promising applications
in manufacturing and robotics. Here, the precise overlay of AR visualizations
and the advancement of robotic vision systems stand to benefit substantially
from our findings.","['Mayura Manawadu', 'Sieun Park', 'Soon-Yong Park']",2024-03-20T09:22:22Z,http://arxiv.org/abs/2403.13434v1
Augmented Reality Demonstrations for Scalable Robot Imitation Learning,"Robot Imitation Learning (IL) is a widely used method for training robots to
perform manipulation tasks that involve mimicking human demonstrations to
acquire skills. However, its practicality has been limited due to its
requirement that users be trained in operating real robot arms to provide
demonstrations. This paper presents an innovative solution: an Augmented
Reality (AR)-assisted framework for demonstration collection, empowering
non-roboticist users to produce demonstrations for robot IL using devices like
the HoloLens 2. Our framework facilitates scalable and diverse demonstration
collection for real-world tasks. We validate our approach with experiments on
three classical robotics tasks: reach, push, and pick-and-place. The real robot
performs each task successfully while replaying demonstrations collected via
AR.","['Yue Yang', 'Bryce Ikeda', 'Gedas Bertasius', 'Daniel Szafir']",2024-03-20T18:30:12Z,http://arxiv.org/abs/2403.13910v1
EVE: Enabling Anyone to Train Robots using Augmented Reality,"The increasing affordability of robot hardware is accelerating the
integration of robots into everyday activities. However, training robots to
automate tasks typically requires physical robots and expensive demonstration
data from trained human annotators. Consequently, only those with access to
physical robots produce demonstrations to train robots. To mitigate this issue,
we introduce EVE, an iOS app that enables everyday users to train robots using
intuitive augmented reality visualizations without needing a physical robot.
With EVE, users can collect demonstrations by specifying waypoints with their
hands, visually inspecting the environment for obstacles, modifying existing
waypoints, and verifying collected trajectories. In a user study ($N=14$,
$D=30$) consisting of three common tabletop tasks, EVE outperformed three
state-of-the-art interfaces in success rate and was comparable to kinesthetic
teaching-physically moving a real robot-in completion time, usability, motion
intent communication, enjoyment, and preference ($mean_{p}=0.30$). We conclude
by enumerating limitations and design considerations for future AR-based
demonstration collection systems for robotics.","['Jun Wang', 'Chun-Cheng Chang', 'Jiafei Duan', 'Dieter Fox', 'Ranjay Krishna']",2024-04-09T07:48:49Z,http://arxiv.org/abs/2404.06089v2
RASSAR: Room Accessibility and Safety Scanning in Augmented Reality,"The safety and accessibility of our homes is critical to quality of life and
evolves as we age, become ill, host guests, or experience life events such as
having children. Researchers and health professionals have created assessment
instruments such as checklists that enable homeowners and trained experts to
identify and mitigate safety and access issues. With advances in computer
vision, augmented reality (AR), and mobile sensors, new approaches are now
possible. We introduce RASSAR, a mobile AR application for semi-automatically
identifying, localizing, and visualizing indoor accessibility and safety issues
such as an inaccessible table height or unsafe loose rugs using LiDAR and
real-time computer vision. We present findings from three studies: a formative
study with 18 participants across five stakeholder groups to inform the design
of RASSAR, a technical performance evaluation across ten homes demonstrating
state-of-the-art performance, and a user study with six stakeholders. We close
with a discussion of future AI-based indoor accessibility assessment tools,
RASSAR's extensibility, and key application scenarios.","['Xia Su', 'Han Zhang', 'Kaiming Cheng', 'Jaewook Lee', 'Qiaochu Liu', 'Wyatt Olson', 'Jon Froehlich']",2024-04-11T05:12:13Z,http://arxiv.org/abs/2404.07479v1
"GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun
  Disambiguation in Wearable Augmented Reality","Voice assistants (VAs) like Siri and Alexa are transforming human-computer
interaction; however, they lack awareness of users' spatiotemporal context,
resulting in limited performance and unnatural dialogue. We introduce
GazePointAR, a fully-functional context-aware VA for wearable augmented reality
that leverages eye gaze, pointing gestures, and conversation history to
disambiguate speech queries. With GazePointAR, users can ask ""what's over
there?"" or ""how do I solve this math problem?"" simply by looking and/or
pointing. We evaluated GazePointAR in a three-part lab study (N=12): (1)
comparing GazePointAR to two commercial systems; (2) examining GazePointAR's
pronoun disambiguation across three tasks; (3) and an open-ended phase where
participants could suggest and try their own context-sensitive queries.
Participants appreciated the naturalness and human-like nature of
pronoun-driven queries, although sometimes pronoun use was counter-intuitive.
We then iterated on GazePointAR and conducted a first-person diary study
examining how GazePointAR performs in-the-wild. We conclude by enumerating
limitations and design considerations for future context-aware VAs.","['Jaewook Lee', 'Jun Wang', 'Elizabeth Brown', 'Liam Chu', 'Sebastian S. Rodriguez', 'Jon E. Froehlich']",2024-04-12T02:50:43Z,http://arxiv.org/abs/2404.08213v1
The application of Augmented Reality (AR) in Remote Work and Education,"With the rapid advancement of technology, Augmented Reality (AR) technology,
known for its ability to deeply integrate virtual information with the real
world, is gradually transforming traditional work modes and teaching methods.
Particularly in the realms of remote work and online education, AR technology
demonstrates a broad spectrum of application prospects. This paper delves into
the application potential and actual effects of AR technology in remote work
and education. Through a systematic literature review, this study outlines the
key features, advantages, and challenges of AR technology. Based on theoretical
analysis, it discusses the scientific basis and technical support that AR
technology provides for enhancing remote work efficiency and promoting
innovation in educational teaching models. Additionally, by designing an
empirical research plan and analyzing experimental data, this article reveals
the specific performance and influencing factors of AR technology in practical
applications. Finally, based on the results of the experiments, this research
summarizes the application value of AR technology in remote work and education,
looks forward to its future development trends, and proposes forward-looking
research directions and strategic suggestions, offering empirical foundation
and theoretical guidance for further promoting the in-depth application of AR
technology in related fields.","['Keqin Li', 'Peng Xirui', 'Jintong Song', 'Bo Hong', 'Jin Wang']",2024-04-16T14:04:46Z,http://arxiv.org/abs/2404.10579v1
"A Plausibility Study of Using Augmented Reality in the
  Ventriculoperitoneal Shunt Operations","The field of augmented reality (AR) has undergone substantial growth, finding
diverse applications in the medical industry. This paper delves into various
techniques employed in medical surgeries, scrutinizing factors such as cost,
implementation, and accessibility. The focus of this exploration is on AR-based
solutions, with a particular emphasis on addressing challenges and proposing an
innovative solution for ventriculoperitoneal shunt (VP) operations. The
proposed solution introduces a novel flow in the pre-surgery phase, aiming to
substantially reduce setup time and operation duration by creating 3D models of
the skull and ventricles. Experiments are conducted where the models are
visualized on a 3D- printed skull through an AR device, specifically the
Microsoft HoloLens 2. The paper then conducts an in-depth analysis of this
proposed solution, discussing its feasibility, advantages, limitations,and
future implications.","['Tandin Dorji', 'Pakinee Aimmanee', 'Vich Yindeedej']",2024-04-16T16:43:14Z,http://arxiv.org/abs/2404.10713v1
Teaching Linguistic Justice through Augmented Reality,"This position paper presents the AR Language Map, a speculative artifact
designed to enhance understanding of linguistic justice among middle and high
school students through augmented reality (AR) that allows students to map
their linguistic experiences. Through a social justice-oriented academic
outreach program aimed at linguistically, economically, and racially
minoritized students, academic concepts on language, culture, race, and power
are introduced to California middle school and high school students. The
curriculum has activities for each lesson plan drawn from students' culturally
relevant experiences. By enabling interactive exploration of linguistic
justice, this tool aims to foster empathy, challenge linguistic racism, and
valorize linguistic diversity. We discuss its conceptualization within the
broader context of AR in social justice education. The AR Language Map not only
deepens students' understanding of these critical issues but also enables them
to become co-creators of their learning experiences.","['Ashvini Varatharaj', 'Abigail Welch', 'Mary Bucholtz', 'Jin Sook Lee']",2024-04-19T00:55:19Z,http://arxiv.org/abs/2404.12563v1
"Display in the Air: Balancing Distraction and Workload in AR Glasses
  Interfaces for Driving Navigation","Augmented Reality (AR) navigation via Head-Mounted Displays (HMDs),
particularly AR glasses, is revolutionizing the driving experience by
integrating real-time routing information into the driver's field of view.
Despite the potential of AR glasses, the question of how to display navigation
information on the interface of these devices remains a valuable yet relatively
unexplored research area. This study employs a mixed-method approach involving
32 participants, combining qualitative feedback from semi-structured interviews
with quantitative data from usability questionnaires in both simulated and
real-world scenarios. Highlighting the necessity of real-world testing, the
research evaluates the impact of five icon placements on the efficiency and
effectiveness of information perception in both environments. The experiment
results indicate a preference for non-central icon placements, especially
bottom-center in real world, which mostly balances distraction and workload for
the driver. Moreover, these findings contribute to the formulation of four
specific design implications for augmented reality interfaces and systems. This
research advances the understanding of AR glasses in driving assistance and
sets the stage for further developments in this emerging technology field.","['Xiangyang He', 'Keyuan Zhou']",2024-04-29T01:37:11Z,http://arxiv.org/abs/2404.18357v1
"A pretest-posttest pilot study for augmented reality-based
  physical-cognitive training in community-dwelling older adults at risk of
  mild cognitive impairment","As cognitive interventions for older adults evolve, modern technologies are
increasingly integrated into their development. This study investigates the
efficacy of augmented reality (AR)-based physical-cognitive training using an
interactive game with Kinect motion sensor technology on older individuals at
risk of mild cognitive impairment. Utilizing a pretest-posttest experimental
design, twenty participants (mean age 66.8 SD. = 4.6 years, age range 60-78
years) underwent eighteen individual training sessions, lasting 45 to 60
minutes each, conducted three times a week over a span of 1.5 months. The
training modules from five activities, encompassing episodic and working
memory, attention and inhibition, cognitive flexibility, and speed processing,
were integrated with physical movement and culturally relevant Thai-context
activities. Results revealed significant improvements in inhibition, cognitive
flexibility, accuracy, and reaction time, with working memory demonstrating
enhancements in accuracy albeit not in reaction time. These findings underscore
the potential of AR interventions to bolster basic executive enhancement among
community-dwelling older adults at risk of cognitive decline.","['Sirinun Chaipunko', 'Watthanaree Ammawat', 'Keerathi Oanmun', 'Wanvipha Hongnaphadol', 'Supatida Sorasak', 'Pattrawadee Makmee']",2024-04-29T12:52:21Z,http://arxiv.org/abs/2404.18970v1
SonifyAR: Context-Aware Sound Generation in Augmented Reality,"Sound plays a crucial role in enhancing user experience and immersiveness in
Augmented Reality (AR). However, current platforms lack support for AR sound
authoring due to limited interaction types, challenges in collecting and
specifying context information, and difficulty in acquiring matching sound
assets. We present SonifyAR, an LLM-based AR sound authoring system that
generates context-aware sound effects for AR experiences. SonifyAR expands the
current design space of AR sound and implements a Programming by Demonstration
(PbD) pipeline to automatically collect contextual information of AR events,
including virtual content semantics and real world context. This context
information is then processed by a large language model to acquire sound
effects with Recommendation, Retrieval, Generation, and Transfer methods. To
evaluate the usability and performance of our system, we conducted a user study
with eight participants and created five example applications, including an
AR-based science experiment, an improving case for AR headset safety, and an
assisting example for low vision AR users.","['Xia Su', 'Jon E. Froehlich', 'Eunyee Koh', 'Chang Xiao']",2024-05-11T20:20:58Z,http://arxiv.org/abs/2405.07089v2
Markerless Augmented Advertising for Sports Videos,"Markerless augmented reality can be a challenging computer vision task,
especially in live broadcast settings and in the absence of information related
to the video capture such as the intrinsic camera parameters. This typically
requires the assistance of a skilled artist, along with the use of advanced
video editing tools in a post-production environment. We present an automated
video augmentation pipeline that identifies textures of interest and overlays
an advertisement onto these regions. We constrain the advertisement to be
placed in a way that is aesthetic and natural. The aim is to augment the scene
such that there is no longer a need for commercial breaks. In order to achieve
seamless integration of the advertisement with the original video we build a 3D
representation of the scene, place the advertisement in 3D, and then project it
back onto the image plane. After successful placement in a single frame, we use
homography-based, shape-preserving tracking such that the advertisement appears
perspective correct for the duration of a video clip. The tracker is designed
to handle smooth camera motion and shot boundaries.","['Hallee E. Wong', 'Osman Akar', 'Emmanuel Antonio Cuevas', 'Iuliana Tabian', 'Divyaa Ravichandran', 'Iris Fu', 'Cambron Carter']",2019-07-22T16:10:34Z,http://arxiv.org/abs/1907.09394v1
PISA: Point-cloud-based Instructed Scene Augmentation,"Indoor scene augmentation has become an emerging topic in the field of
computer vision with applications in augmented and virtual reality. However,
existing scene augmentation methods mostly require a pre-built object database
with a given position as the desired location. In this paper, we propose the
first end-to-end multi-modal deep neural network that can generate point cloud
objects consistent with their surroundings, conditioned on text instructions.
Our model generates a seemly object in the appropriate position based on the
inputs of a query and point clouds, thereby enabling the creation of new
scenarios involving previously unseen layouts of objects. Database of
pre-stored CAD models is no longer needed. We use Point-E as our generative
model and introduce methods including quantified position prediction and Top-K
estimation to mitigate the false negative problems caused by ambiguous language
description. Moreover, we evaluate the ability of our model by demonstrating
the diversity of generated objects, the effectiveness of instruction, and
quantitative metric results, which collectively indicate that our model is
capable of generating realistic in-door objects. For a more thorough
evaluation, we also incorporate visual grounding as a metric to assess the
quality of the scenes generated by our model.","['Yiyang Luo', 'Ke Lin']",2023-11-26T06:40:16Z,http://arxiv.org/abs/2311.16501v1
"Teach Me How to ImproVISe: Co-Designing an Augmented Piano Training
  System for Improvisation","Improvisation is a vital but often neglected aspect of traditional piano
teaching. Challenges such as difficulty in assessment and subjectivity have
hindered its effective instruction. Technological approaches, including
augmentation, aim to enhance piano instruction, but the specific application of
digital augmentation for piano improvisation is under-explored. This paper
outlines a co-design process developing an Augmented Reality (AR) Piano
Improvisation Training System, ImproVISe, involving improvisation teachers. The
prototype, featuring basic improvisation concepts, was created and refined
through expert interaction. Their insights guided the identification of
objectives, tools, interaction metaphors, and software features. The findings
offer design guidelines and recommendations to address challenges in assessing
piano improvisation in a learning context.","['Jordan Aiko Deja', 'Sandi Štor', 'Ilonka Pucihar', 'Klen Čopič Pucihar', 'Matjaž Kljun']",2024-02-05T13:37:55Z,http://arxiv.org/abs/2402.02999v1
"A feasibility study on SSVEP-based interaction with motivating and
  immersive virtual and augmented reality","Non-invasive steady-state visual evoked potential (SSVEP) based
brain-computer interface (BCI) systems offer high bandwidth compared to other
BCI types and require only minimal calibration and training. Virtual reality
(VR) has been already validated as effective, safe, affordable and motivating
feedback modality for BCI experiments. Augmented reality (AR) enhances the
physical world by superimposing informative, context sensitive, computer
generated content. In the context of BCI, AR can be used as a friendlier and
more intuitive real-world user interface, thereby facilitating a more seamless
and goal directed interaction. This can improve practicality and usability of
BCI systems and may help to compensate for their low bandwidth. In this
feasibility study, three healthy participants had to finish a complex
navigation task in immersive VR and AR conditions using an online SSVEP BCI.
Two out of three subjects were successful in all conditions. To our knowledge,
this is the first work to present an SSVEP BCI that operates using target
stimuli integrated in immersive VR and AR (head-mounted display and camera).
This research direction can benefit patients by introducing more intuitive and
effective real-world interaction (e.g. smart home control). It may also be
relevant for user groups that require or benefit from hands free operation
(e.g. due to temporary situational disability).","['Josef Faller', 'Brendan Z. Allison', 'Clemens Brunner', 'Reinhold Scherer', 'Dieter Schmalstieg', 'Gert Pfurtscheller', 'Christa Neuper']",2017-01-15T01:58:47Z,http://arxiv.org/abs/1701.03981v1
Optical Gaze Tracking with Spatially-Sparse Single-Pixel Detectors,"Gaze tracking is an essential component of next generation displays for
virtual reality and augmented reality applications. Traditional camera-based
gaze trackers used in next generation displays are known to be lacking in one
or multiple of the following metrics: power consumption, cost, computational
complexity, estimation accuracy, latency, and form-factor. We propose the use
of discrete photodiodes and light-emitting diodes (LEDs) as an alternative to
traditional camera-based gaze tracking approaches while taking all of these
metrics into consideration. We begin by developing a rendering-based simulation
framework for understanding the relationship between light sources and a
virtual model eyeball. Findings from this framework are used for the placement
of LEDs and photodiodes. Our first prototype uses a neural network to obtain an
average error rate of 2.67{\deg} at 400Hz while demanding only 16mW. By
simplifying the implementation to using only LEDs, duplexed as light
transceivers, and more minimal machine learning model, namely a light-weight
supervised Gaussian process regression algorithm, we show that our second
prototype is capable of an average error rate of 1.57{\deg} at 250 Hz using 800
mW.","['Richard Li', 'Eric Whitmire', 'Michael Stengel', 'Ben Boudaoud', 'Jan Kautz', 'David Luebke', 'Shwetak Patel', 'Kaan Akşit']",2020-09-15T05:50:13Z,http://arxiv.org/abs/2009.06875v2
"A Survey of Calibration Methods for Optical See-Through Head-Mounted
  Displays","Optical see-through head-mounted displays (OST HMDs) are a major output
medium for Augmented Reality, which have seen significant growth in popularity
and usage among the general public due to the growing release of
consumer-oriented models, such as the Microsoft Hololens. Unlike Virtual
Reality headsets, OST HMDs inherently support the addition of
computer-generated graphics directly into the light path between a user's eyes
and their view of the physical world. As with most Augmented and Virtual
Reality systems, the physical position of an OST HMD is typically determined by
an external or embedded 6-Degree-of-Freedom tracking system. However, in order
to properly render virtual objects, which are perceived as spatially aligned
with the physical environment, it is also necessary to accurately measure the
position of the user's eyes within the tracking system's coordinate frame. For
over 20 years, researchers have proposed various calibration methods to
determine this needed eye position. However, to date, there has not been a
comprehensive overview of these procedures and their requirements. Hence, this
paper surveys the field of calibration methods for OST HMDs. Specifically, it
provides insights into the fundamentals of calibration techniques, and presents
an overview of both manual and automatic approaches, as well as evaluation
methods and metrics. Finally, it also identifies opportunities for future
research. % relative to the tracking coordinate system, and, hence, its
position in 3D space.","['Jens Grubert', 'Yuta Itoh', 'Kenneth Moser', 'J. Edward Swan II']",2017-09-13T12:55:45Z,http://arxiv.org/abs/1709.04299v1
"Synthetic Video Generation for Robust Hand Gesture Recognition in
  Augmented Reality Applications","Hand gestures are a natural means of interaction in Augmented Reality and
Virtual Reality (AR/VR) applications. Recently, there has been an increased
focus on removing the dependence of accurate hand gesture recognition on
complex sensor setup found in expensive proprietary devices such as the
Microsoft HoloLens, Daqri and Meta Glasses. Most such solutions either rely on
multi-modal sensor data or deep neural networks that can benefit greatly from
abundance of labelled data. Datasets are an integral part of any deep learning
based research. They have been the principal reason for the substantial
progress in this field, both, in terms of providing enough data for the
training of these models, and, for benchmarking competing algorithms. However,
it is becoming increasingly difficult to generate enough labelled data for
complex tasks such as hand gesture recognition. The goal of this work is to
introduce a framework capable of generating photo-realistic videos that have
labelled hand bounding box and fingertip that can help in designing, training,
and benchmarking models for hand-gesture recognition in AR/VR applications. We
demonstrate the efficacy of our framework in generating videos with diverse
backgrounds.","['Varun Jain', 'Shivam Aggarwal', 'Suril Mehta', 'Ramya Hebbalaguppe']",2019-11-04T16:32:07Z,http://arxiv.org/abs/1911.01320v3
"One Point, One Object: Simultaneous 3D Object Segmentation and 6-DOF
  Pose Estimation","We propose a single-shot method for simultaneous 3D object segmentation and
6-DOF pose estimation in pure 3D point clouds scenes based on a consensus that
\emph{one point only belongs to one object}, i.e., each point has the potential
power to predict the 6-DOF pose of its corresponding object. Unlike the
recently proposed methods of the similar task, which rely on 2D detectors to
predict the projection of 3D corners of the 3D bounding boxes and the 6-DOF
pose must be estimated by a PnP like spatial transformation method, ours is
concise enough not to require additional spatial transformation between
different dimensions. Due to the lack of training data for many objects, the
recently proposed 2D detection methods try to generate training data by using
rendering engine and achieve good results. However, rendering in 3D space along
with 6-DOF is relatively difficult. Therefore, we propose an augmented reality
technology to generate the training data in semi-virtual reality 3D space. The
key component of our method is a multi-task CNN architecture that can
simultaneously predicts the 3D object segmentation and 6-DOF pose estimation in
pure 3D point clouds.
  For experimental evaluation, we generate expanded training data for two
state-of-the-arts 3D object datasets \cite{PLCHF}\cite{TLINEMOD} by using
Augmented Reality technology (AR). We evaluate our proposed method on the two
datasets. The results show that our method can be well generalized into
multiple scenarios and provide performance comparable to or better than the
state-of-the-arts.","['Hongsen Liu', 'Yang Cong', 'Yandong Tang']",2019-12-27T13:48:03Z,http://arxiv.org/abs/1912.12095v1
"Rate-Utility Optimized Streaming of Volumetric Media for Augmented
  Reality","Volumetric media, popularly known as holograms, need to be delivered to users
using both on-demand and live streaming, for new augmented reality (AR) and
virtual reality (VR) experiences. As in video streaming, hologram streaming
must support network adaptivity and fast startup, but must also moderate large
bandwidths, multiple simultaneously streaming objects, and frequent user
interaction, which requires low delay. In this paper, we introduce the first
system to our knowledge designed specifically for streaming volumetric media.
The system reduces bandwidth by introducing 3D tiles, and culling them or
reducing their level of detail depending on their relation to the user's view
frustum and distance to the user. Our system reduces latency by introducing a
window-based buffer, which in contrast to a queue-based buffer allows
insertions near the head of the buffer rather than only at the tail of the
buffer, to respond quickly to user interaction. To allocate bits between
different tiles across multiple objects, we introduce a simple greedy yet
provably optimal algorithm for rate-utility optimization. We introduce utility
measures based not only on the underlying quality of the representation, but on
the level of detail relative to the user's viewpoint and device resolution.
Simulation results show that the proposed algorithm provides superior quality
compared to existing video-streaming approaches adapted to hologram streaming,
in terms of utility and user experience over variable, throughput-constrained
networks.","['Jounsup Park', 'Philip A. Chou', 'Jenq-Neng Hwang']",2018-04-26T02:49:53Z,http://arxiv.org/abs/1804.09864v1
"Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for
  Autonomous Vehicle-to-Pedestrian Communication","Providing pedestrians and other vulnerable road users with a clear indication
about a fully autonomous vehicle status and intentions is crucial to make them
coexist. In the last few years, a variety of external interfaces have been
proposed, leveraging different paradigms and technologies including
vehicle-mounted devices (like LED panels), short-range on-road projections, and
road infrastructure interfaces (e.g., special asphalts with embedded displays).
These designs were experimented in different settings, using mockups, specially
prepared vehicles, or virtual environments, with heterogeneous evaluation
metrics. Promising interfaces based on Augmented Reality (AR) have been
proposed too, but their usability and effectiveness have not been tested yet.
This paper aims to complement such body of literature by presenting a
comparison of state-of-the-art interfaces and new designs under common
conditions. To this aim, an immersive Virtual Reality-based simulation was
developed, recreating a well-known scenario represented by pedestrians crossing
in urban environments under non-regulated conditions. A user study was then
performed to investigate the various dimensions of vehicle-to-pedestrian
interaction leveraging objective and subjective metrics. Even though no
interface clearly stood out over all the considered dimensions, one of the AR
designs achieved state-of-the-art results in terms of safety and trust, at the
cost of higher cognitive effort and lower intuitiveness compared to LED panels
showing anthropomorphic features. Together with rankings on the various
dimensions, indications about advantages and drawbacks of the various
alternatives that emerged from this study could provide important information
for next developments in the field.","['F. Gabriele Pratticò', 'Fabrizio Lamberti', 'Alberto Cannavò', 'Lia Morra', 'Paolo Montuschi']",2021-02-04T18:03:06Z,http://arxiv.org/abs/2102.02783v1
Experiences with User Studies in Augmented Reality,"The research field of augmented reality (AR) is of increasing popularity, as
seen, among others, in several recently published surveys. To produce further
advancements in AR, it is not only necessary to create new systems or
applications, but also to evaluate them. One important aspect in regards to the
evaluation is the general understanding of how users experience a given AR
application, which can also be seen by the increased number of papers focusing
on this topic that were published in the last years. With the steadily growing
understanding and development of AR in general, it is only a matter of time
until AR devices make the leap into the consumer market where such an in-depth
user understanding is even more essential. Thus, a better understanding of
factors that could influence the design and results of user experience studies
can help us to make them more robust and dependable in the future.
  In this position paper, we describe three challenges which researchers face
while designing and conducting AR users studies. We encountered these
challenges in our past and current research, including papers that focus on
perceptual studies of visualizations, interaction studies, and studies
exploring the use of AR applications and their design spaces.","['Marc Satkowski', 'Wolfgang Büschel', 'Raimund Dachselt']",2021-04-08T14:18:51Z,http://arxiv.org/abs/2104.03795v1
"A Tool for Organizing Key Characteristics of Virtual, Augmented, and
  Mixed Reality for Human-Robot Interaction Systems: Synthesizing VAM-HRI
  Trends and Takeaways","Frameworks have begun to emerge to categorize Virtual, Augmented, and Mixed
Reality (VAM) technologies that provide immersive, intuitive interfaces to
facilitate Human-Robot Interaction. These frameworks, however, fail to capture
key characteristics of the growing subfield of VAM-HRI and can be difficult to
consistently apply due to continuous scales. This work builds upon these prior
frameworks through the creation of a Tool for Organizing Key Characteristics of
VAM-HRI Systems (TOKCS). TOKCS discretizes the continuous scales used within
prior works for more consistent classification and adds additional
characteristics related to a robot's internal model, anchor locations,
manipulability, and the system's software and hardware. To showcase the tool's
capability, TOKCS is applied to the ten papers from the fourth VAM-HRI workshop
and examined for key trends and takeaways. These trends highlight the
expressive capability of TOKCS while also helping frame newer trends and future
work recommendations for VAM-HRI research.","['Thomas R. Groechel', 'Michael E. Walker', 'Christine T. Chang', 'Eric Rosen', 'Jessica Zosa Forde']",2021-08-07T16:01:42Z,http://arxiv.org/abs/2108.03477v3
"ARviz -- An Augmented Reality-enabled Visualization Platform for ROS
  Applications","Current robot interfaces such as teach pendants and 2D screen displays used
for task visualization and interaction often seem unintuitive and limited in
terms of information flow. This compromises task efficiency as interacting with
the interface can distract the user from the task at hand. Augmented Reality
(AR) technology offers the capability to create visually rich displays and
intuitive interaction elements in situ. In recent years, AR has shown promising
potential to enable effective human-robot interaction. We introduce ARviz - a
versatile, extendable AR visualization platform built for robot applications
developed with the widely used Robot Operating System (ROS) framework. ARviz
aims to provide both a universal visualization platform with the capability of
displaying any ROS message data type in AR, as well as a multimodal user
interface for interacting with robots over ROS. ARviz is built as a platform
incorporating a collection of plugins that provide visualization and/or
interaction components. Users can also extend the platform by implementing new
plugins to suit their needs. We present three use cases as well as two
potential use cases to showcase the capabilities and benefits of the ARviz
platform for human-robot interaction applications. The open access source code
for our ARviz platform is available at: https://github.com/hri-group/arviz.","['Khoa C. Hoang', 'Wesley P. Chan', 'Steven Lay', 'Akansel Cosgun', 'Elizabeth A. Croft']",2021-10-29T03:33:07Z,http://arxiv.org/abs/2110.15521v1
"Project IRL: Playful Co-Located Interactions with Mobile Augmented
  Reality","We present Project IRL (In Real Life), a suite of five mobile apps we created
to explore novel ways of supporting in-person social interactions with
augmented reality. In recent years, the tone of public discourse surrounding
digital technology has become increasingly critical, and technology's influence
on the way people relate to each other has been blamed for making people feel
""alone together,"" diverting their attention from truly engaging with one
another when they interact in person. Motivated by this challenge, we focus on
an under-explored design space: playful co-located interactions. We evaluated
the apps through a deployment study that involved interviews and participant
observations with 101 people. We synthesized the results into a series of
design guidelines that focus on four themes: (1) device arrangement (e.g., are
people sharing one phone, or does each person have their own?), (2) enablers
(e.g., should the activity focus on an object, body part, or pet?), (3)
affordances of modifying reality (i.e., features of the technology that enhance
its potential to encourage various aspects of social interaction), and (4)
co-located play (i.e., using technology to make in-person play engaging and
inviting). We conclude by presenting our design guidelines for future work on
embodied social AR.","['Ella Dagan', 'Ana Cárdenas Gasca', 'Ava Robinson', 'Anwar Noriega', 'Yu Jiang Tham', 'Rajan Vaish', 'Andrés Monroy-Hernández']",2022-01-07T17:31:43Z,http://arxiv.org/abs/2201.02558v2
"On the impact of VR assessment on the Quality of Experience of Highly
  Realistic Digital Humans","Fuelled by the increase in popularity of virtual and augmented reality
applications, point clouds have emerged as a popular 3D format for acquisition
and rendering of digital humans, thanks to their versatility and real-time
capabilities. Due to technological constraints and real-time rendering
limitations, however, the visual quality of dynamic point cloud contents is
seldom evaluated using virtual and augmented reality devices, instead relying
on prerecorded videos displayed on conventional 2D screens. In this study, we
evaluate how the visual quality of point clouds representing digital humans is
affected by compression distortions. In particular, we compare three different
viewing conditions based on the degrees of freedom that are granted to the
viewer: passive viewing (2DTV), head rotation (3DoF), and rotation and
translation (6DoF), to understand how interacting in the virtual space affects
the perception of quality. We provide both quantitative and qualitative results
of our evaluation involving 78 participants, and we make the data publicly
available. To the best of our knowledge, this is the first study evaluating the
quality of dynamic point clouds in virtual reality, and comparing it to
traditional viewing settings. Results highlight the dependency of visual
quality on the content under test, and limitations in the way current data sets
are used to evaluate compression solutions. Moreover, influencing factors in
quality evaluation in VR, and shortcomings in how point cloud encoding
solutions handle visually-lossless compression, are discussed.","['Irene Viola', 'Shishir Subramanyam', 'Jie Li', 'Pablo Cesar']",2022-01-19T16:37:08Z,http://arxiv.org/abs/2201.07701v1
"A Survey on Mobile Edge Computing for Video Streaming: Opportunities and
  Challenges","5G communication brings substantial improvements in the quality of service
provided to various applications by achieving higher throughput and lower
latency. However, interactive multimedia applications (e.g., ultra high
definition video conferencing, 3D and multiview video streaming, crowd-sourced
video streaming, cloud gaming, virtual and augmented reality) are becoming more
ambitious with high volume and low latency video streams putting strict demands
on the already congested networks. Mobile Edge Computing (MEC) is an emerging
paradigm that extends cloud computing capabilities to the edge of the network
i.e., at the base station level. To meet the latency requirements and avoid the
end-to-end communication with remote cloud data centers, MEC allows to store
and process video content (e.g., caching, transcoding, pre-processing) at the
base stations. Both video on demand and live video streaming can utilize MEC to
improve existing services and develop novel use cases, such as video analytics,
and targeted advertisements. MEC is expected to reshape the future of video
streaming by providing ultra-reliable and low latency streaming (e.g., in
augmented reality, virtual reality, and autonomous vehicles), pervasive
computing (e.g., in real-time video analytics), and blockchain-enabled
architecture for secure live streaming. This paper presents a comprehensive
survey of recent developments in MEC-enabled video streaming bringing
unprecedented improvement to enable novel use cases. A detailed review of the
state-of-the-art is presented covering novel caching schemes, optimal
computation offloading, cooperative caching and offloading and the use of
artificial intelligence (i.e., machine learning, deep learning, and
reinforcement learning) in MEC-assisted video streaming services.","['Muhammad Asif Khan', 'Emna Baccour', 'Zina Chkirbene', 'Aiman Erbad', 'Ridha Hamila', 'Mounir Hamdi', 'Moncef Gabbouj']",2022-09-13T06:53:32Z,http://arxiv.org/abs/2209.05761v1
"Augmented Reality for Maintenance Tasks with ChatGPT for Automated
  Text-to-Action","Advancements in sensor technology, artificial intelligence (AI), and
augmented reality (AR) have unlocked opportunities across various domains. AR
and large language models like GPT have witnessed substantial progress and are
increasingly being employed in diverse fields. One such promising application
is in operations and maintenance (O&M). O&M tasks often involve complex
procedures and sequences that can be challenging to memorize and execute
correctly, particularly for novices or under high-stress situations. By
marrying the advantages of superimposing virtual objects onto the physical
world, and generating human-like text using GPT, we can revolutionize O&M
operations. This study introduces a system that combines AR, Optical Character
Recognition (OCR), and the GPT language model to optimize user performance
while offering trustworthy interactions and alleviating workload in O&M tasks.
This system provides an interactive virtual environment controlled by the Unity
game engine, facilitating a seamless interaction between virtual and physical
realities. A case study (N=15) is conducted to illustrate the findings and
answer the research questions. The results indicate that users can complete
similarly challenging tasks in less time using our proposed AR and AI system.
Moreover, the collected data also suggests a reduction in cognitive load and an
increase in trust when executing the same operations using the AR and AI
system.","['Fang Xu', 'Tri Nguyen', 'Jing Du']",2023-07-07T02:18:17Z,http://arxiv.org/abs/2307.03351v1
"A Large-Scale Feasibility Study of Screen-based 3D Visualization and
  Augmented Reality Tools for Human Anatomy Education: Exploring Gender
  Perspectives in Learning Experience","Anatomy education is an indispensable part of medical training, but
traditional methods face challenges like limited resources for dissection in
large classes and difficulties understanding 2D anatomy in textbooks. Advanced
technologies, such as 3D visualization and augmented reality (AR), are
transforming anatomy learning. This paper presents two in-house solutions that
use handheld tablets or screen-based AR to visualize 3D anatomy models with
informative labels and in-situ visualizations of the muscle anatomy. To assess
these tools, a user study of muscle anatomy education involved 236 premedical
students in dyadic teams, with results showing that the tablet-based 3D
visualization and screen-based AR tools led to significantly higher learning
experience scores than traditional textbook. While knowledge retention didn't
differ significantly, ethnographic and gender analysis showed that male
students generally reported more positive learning experiences than female
students. This study discusses the implications for anatomy and medical
education, highlighting the potential of these innovative learning tools
considering gender and team dynamics in body painting anatomy learning
interventions.","['Roghayeh Leila Barmaki', 'Kangsoo Kim', 'Zhang Guo', 'Qile Wang', 'Kevin Yu', 'Rebecca Pearlman', 'Nassir Navab']",2023-07-26T01:28:58Z,http://arxiv.org/abs/2307.14383v2
MARVisT: Authoring Glyph-based Visualization in Mobile Augmented Reality,"Recent advances in mobile augmented reality (AR) techniques have shed new
light on personal visualization for their advantages of fitting visualization
within personal routines, situating visualization in a real-world context, and
arousing users' interests. However, enabling non-experts to create data
visualization in mobile AR environments is challenging given the lack of tools
that allow in-situ design while supporting the binding of data to AR content.
Most existing AR authoring tools require working on personal computers or
manually creating each virtual object and modifying its visual attributes. We
systematically study this issue by identifying the specificity of AR
glyph-based visualization authoring tool and distill four design
considerations. Following these design considerations, we design and implement
MARVisT, a mobile authoring tool that leverages information from reality to
assist non-experts in addressing relationships between data and virtual glyphs,
real objects and virtual glyphs, and real objects and data. With MARVisT, users
without visualization expertise can bind data to real-world objects to create
expressive AR glyph-based visualizations rapidly and effortlessly, reshaping
the representation of the real world with data. We use several examples to
demonstrate the expressiveness of MARVisT. A user study with non-experts is
also conducted to evaluate the authoring experience of MARVisT.","['Chen Zhu-Tian', 'Yijia Su', 'Yifang Wang', 'Qianwen Wang', 'Huamin Qu', 'Yingcai Wu']",2023-10-07T15:07:35Z,http://arxiv.org/abs/2310.04843v2
Collaboration in Immersive Environments: Challenges and Solutions,"Virtual Reality (VR) and Augmented Reality (AR) tools have been applied in
all engineering fields in order to avoid the use of physical prototypes, to
train in high-risk situations, and to interpret real or simulated results. In
order to complete a shared task or assign tasks to the agents in such immersive
environments, collaboration or Shared Cooperative Activities are a necessity.
Collaboration in immersive environments is an emerging field of research that
aims to study and enhance the ways in which people interact and work together
in Virtual and Augmented Reality settings. Collaboration in immersive
environments is a complex process that involves different factors such as
communication, coordination, and social presence. This paper provides an
overview of the current state of research on collaboration in immersive
environments. It discusses the different types of immersive environments,
including VR and AR, and the different forms of collaboration that can occur in
these environments. The paper also highlights the challenges and limitations of
collaboration in immersive environments, such as the lack of physical cues,
cost and usability and the need for further research in this area. Overall,
collaboration in immersive environments is a promising field with a wide range
of potential applications, from education to industry, and it can benefit both
individuals and groups by enhancing their ability to work together effectively.",['Shahin Doroudian'],2023-11-01T17:45:22Z,http://arxiv.org/abs/2311.00689v3
Free-form Shape Modeling in XR: A Systematic Review,"Shape modeling research in Computer Graphics has been an active area for
decades. The ability to create and edit complex 3D shapes has been of key
importance in Computer-Aided Design, Animation, Architecture, and
Entertainment. With the growing popularity of Virtual and Augmented Reality,
new applications and tools have been developed for artistic content creation;
real-time interactive shape modeling has become increasingly important for a
continuum of virtual and augmented reality environments (eXtended Reality
(XR)). Shape modeling in XR opens new possibilities for intuitive design and
shape modeling in an accessible way. Artificial Intelligence (AI) approaches
generating shape information from text prompts are set to change how artists
create and edit 3D models. There has been a substantial body of research on
interactive 3D shape modeling. However, there is no recent extensive review of
the existing techniques and what AI shape generation means for shape modeling
in interactive XR environments. In this state-of-the-art paper, we fill this
research gap in the literature by surveying free-form shape modeling work in
XR, with a focus on sculpting and 3D sketching, the most intuitive forms of
free-form shape modeling. We classify and discuss these works across five
dimensions: contribution of the articles, domain setting, interaction tool,
auto-completion, and collaborative designing. The paper concludes by discussing
the disconnect between interactive 3D sculpting and sketching and how this will
likely evolve with the prevalence of AI shape-generation tools in the future.",['Shounak Chatterjee'],2024-01-01T13:17:50Z,http://arxiv.org/abs/2401.00924v1
Security and Privacy Approaches in Mixed Reality: A Literature Survey,"Mixed reality (MR) technology development is now gaining momentum due to
advances in computer vision, sensor fusion, and realistic display technologies.
With most of the research and development focused on delivering the promise of
MR, there is only barely a few working on the privacy and security implications
of this technology. This survey paper aims to put in to light these risks, and
to look into the latest security and privacy work on MR. Specifically, we list
and review the different protection approaches that have been proposed to
ensure user and data security and privacy in MR. We extend the scope to include
work on related technologies such as augmented reality (AR), virtual reality
(VR), and human-computer interaction (HCI) as crucial components, if not the
origins, of MR, as well as numerous related work from the larger area of mobile
devices, wearables, and Internet-of-Things (IoT). We highlight the lack of
investigation, implementation, and evaluation of data protection approaches in
MR. Further challenges and directions on MR security and privacy are also
discussed.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-02-15T23:33:45Z,http://arxiv.org/abs/1802.05797v3
Mutual Scene Synthesis for Mixed Reality Telepresence,"Remote telepresence via next-generation mixed reality platforms can provide
higher levels of immersion for computer-mediated communications, allowing
participants to engage in a wide spectrum of activities, previously not
possible in 2D screen-based communication methods. However, as mixed reality
experiences are limited to the local physical surrounding of each user, finding
a common virtual ground where users can freely move and interact with each
other is challenging. In this paper, we propose a novel mutual scene synthesis
method that takes the participants' spaces as input, and generates a virtual
synthetic scene that corresponds to the functional features of all
participants' local spaces. Our method combines a mutual function optimization
module with a deep-learning conditional scene augmentation process to generate
a scene mutually and physically accessible to all participants of a mixed
reality telepresence scenario. The synthesized scene can hold mutual walkable,
sittable and workable functions, all corresponding to physical objects in the
users' real environments. We perform experiments using the MatterPort3D dataset
and conduct comparative user studies to evaluate the effectiveness of our
system. Our results show that our proposed approach can be a promising research
direction for facilitating contextualized telepresence systems for
next-generation spatial computing platforms.","['Mohammad Keshavarzi', 'Michael Zollhoefer', 'Allen Y. Yang', 'Patrick Peluse', 'Luisa Caldas']",2022-04-01T02:08:11Z,http://arxiv.org/abs/2204.00161v1
"A software toolkit and hardware platform for investigating and comparing
  robot autonomy algorithms in simulation and reality","We describe a software framework and a hardware platform used in tandem for
the design and analysis of robot autonomy algorithms in simulation and reality.
The software, which is open source, containerized, and operating system (OS)
independent, has three main components: a ROS 2 interface to a C++ vehicle
simulation framework (Chrono), which provides high-fidelity wheeled/tracked
vehicle and sensor simulation; a basic ROS 2-based autonomy stack for algorithm
design and testing; and, a development ecosystem which enables visualization,
and hardware-in-the-loop experimentation in perception, state estimation, path
planning, and controls. The accompanying hardware platform is a 1/6th scale
vehicle augmented with reconfigurable mountings for computing, sensing, and
tracking. Its purpose is to allow algorithms and sensor configurations to be
physically tested and improved. Since this vehicle platform has a digital twin
within the simulation environment, one can test and compare the same algorithms
and autonomy stack in simulation and reality. This platform has been built with
an eye towards characterizing and managing the simulation-to-reality gap.
Herein, we describe how this platform is set up, deployed, and used to improve
autonomy for mobility applications.","['Asher Elmquist', 'Aaron Young', 'Ishaan Mahajan', 'Kyle Fahey', 'Abhiraj Dashora', 'Sriram Ashokkumar', 'Stefan Caldararu', 'Victor Freire', 'Xiangru Xu', 'Radu Serban', 'Dan Negrut']",2022-06-14T01:03:58Z,http://arxiv.org/abs/2206.06537v1
"""Seeing the Faces Is So Important"" -- Experiences From Online Team
  Meetings on Commercial Virtual Reality Platforms","During the Covid-19 pandemic, online meetings became common for daily
teamwork in the home office. To understand the opportunities and challenges of
meeting in virtual reality (VR) compared to video conferences, we conducted the
weekly team meetings of our human-computer interaction research lab on five
off-the-shelf online meeting platforms over four months. After each of the 12
meetings, we asked the participants (N = 32) to share their experiences,
resulting in 200 completed online questionnaires. We evaluated the ratings of
the overall meeting experience and conducted an exploratory factor analysis of
the quantitative data to compare VR meetings and video calls in terms of
meeting involvement and co-presence. In addition, a thematic analysis of the
qualitative data revealed genuine insights covering five themes: spatial
aspects, meeting atmosphere, expression of emotions, meeting productivity, and
user needs. We reflect on our findings gained under authentic working
conditions, derive lessons learned for running successful team meetings in VR
supporting different kinds of meeting formats, and discuss the team's long-term
platform choice.","['Michael Bonfert', 'Anke V. Reinschluessel', 'Susanne Putze', 'Yenchin Lai', 'Dmitry Alexandrovsky', 'Rainer Malaka', 'Tanja Döring']",2022-10-12T13:19:26Z,http://arxiv.org/abs/2210.06190v2
"VRContour: Bringing Contour Delineations of Medical Structures Into
  Virtual Reality","Contouring is an indispensable step in Radiotherapy (RT) treatment planning.
However, today's contouring software is constrained to only work with a 2D
display, which is less intuitive and requires high task loads. Virtual Reality
(VR) has shown great potential in various specialties of healthcare and health
sciences education due to the unique advantages of intuitive and natural
interactions in immersive spaces. VR-based radiation oncology integration has
also been advocated as a target healthcare application, allowing providers to
directly interact with 3D medical structures. We present VRContour and
investigate how to effectively bring contouring for radiation oncology into VR.
Through an autobiographical iterative design, we defined three design spaces
focused on contouring in VR with the support of a tracked tablet and VR stylus,
and investigating dimensionality for information consumption and input (either
2D or 2D + 3D). Through a within-subject study (n = 8), we found that
visualizations of 3D medical structures significantly increase precision, and
reduce mental load, frustration, as well as overall contouring effort.
Participants also agreed with the benefits of using such metaphors for learning
purposes.","['Chen Chen', 'Matin Yarmand', 'Varun Singh', 'Michael V. Sherer', 'James D. Murphy', 'Yang Zhang', 'Nadir Weibel']",2022-10-21T23:22:21Z,http://arxiv.org/abs/2210.12298v2
"Secure and Trustworthy Artificial Intelligence-Extended Reality (AI-XR)
  for Metaverses","Metaverse is expected to emerge as a new paradigm for the next-generation
Internet, providing fully immersive and personalised experiences to socialize,
work, and play in self-sustaining and hyper-spatio-temporal virtual world(s).
The advancements in different technologies like augmented reality, virtual
reality, extended reality (XR), artificial intelligence (AI), and 5G/6G
communication will be the key enablers behind the realization of AI-XR
metaverse applications. While AI itself has many potential applications in the
aforementioned technologies (e.g., avatar generation, network optimization,
etc.), ensuring the security of AI in critical applications like AI-XR
metaverse applications is profoundly crucial to avoid undesirable actions that
could undermine users' privacy and safety, consequently putting their lives in
danger. To this end, we attempt to analyze the security, privacy, and
trustworthiness aspects associated with the use of various AI techniques in
AI-XR metaverse applications. Specifically, we discuss numerous such challenges
and present a taxonomy of potential solutions that could be leveraged to
develop secure, private, robust, and trustworthy AI-XR applications. To
highlight the real implications of AI-associated adversarial threats, we
designed a metaverse-specific case study and analyzed it through the
adversarial lens. Finally, we elaborate upon various open issues that require
further research interest from the community.","['Adnan Qayyum', 'Muhammad Atif Butt', 'Hassan Ali', 'Muhammad Usman', 'Osama Halabi', 'Ala Al-Fuqaha', 'Qammer H. Abbasi', 'Muhammad Ali Imran', 'Junaid Qadir']",2022-10-24T14:26:59Z,http://arxiv.org/abs/2210.13289v1
"MED1stMR: Mixed Reality to Enhance Training of Medical First
  Responder]{MED1stMR: Mixed Reality to Enhance the Training of Medical First
  Responders for Challenging Contexts","Mass-casualty incidents with a large number of injured persons caused by
human-made or by natural disasters are increasing globally. In such situations,
medical first responders (MFRs) need to perform diagnosis, basic life support,
or other first aid to help stabilize victims and keep them alive to wait for
the arrival of further support. Situational awareness and effective coping with
acute stressors is essential to enable first responders to take appropriate
action that saves lives.
  Virtual Reality (VR) has been demonstrated in several domains to be a serious
alternative, and in some areas also a significant improvement to conventional
learning and training. Especially for the challenges in the training of MFRs,
it can be highly useful for practicing and learning domains where the context
of the training is not easily available. VR training offers controlled,
easy-to-create environments that can be created and trained repeatedly under
the same conditions.
  As an advanced alternative to VR, Mixed Reality (MR) environments have the
potential to augment current VR training by providing a dynamic simulation of
an environment and hands-on practice on injured victims. Building on this
interpretation of MR, the main aim of MED1stMR is to develop a new generation
of MR training with haptic feedback for enhanced realism. in this workshop
paper, we will present the vision of the project and suggest questions for
discussion.","['Helmut Schrom-Feiertag', 'Georg Regal', 'Markus Murtinger']",2023-01-30T18:01:32Z,http://arxiv.org/abs/2301.13124v1
"Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended
  Reality","Real-time Stereo Matching is a cornerstone algorithm for many Extended
Reality (XR) applications, such as indoor 3D understanding, video pass-through,
and mixed-reality games. Despite significant advancements in deep stereo
methods, achieving real-time depth inference with high accuracy on a low-power
device remains a major challenge. One of the major difficulties is the lack of
high-quality indoor video stereo training datasets captured by head-mounted
VR/AR glasses. To address this issue, we introduce a novel video stereo
synthetic dataset that comprises photorealistic renderings of various indoor
scenes and realistic camera motion captured by a 6-DoF moving VR/AR
head-mounted display (HMD). This facilitates the evaluation of existing
approaches and promotes further research on indoor augmented reality scenarios.
Our newly proposed dataset enables us to develop a novel framework for
continuous video-rate stereo matching.
  As another contribution, our dataset enables us to proposed a new video-based
stereo matching approach tailored for XR applications, which achieves real-time
inference at an impressive 134fps on a standard desktop computer, or 30fps on a
battery-powered HMD. Our key insight is that disparity and contextual
information are highly correlated and redundant between consecutive stereo
frames. By unrolling an iterative cost aggregation in time (i.e. in the
temporal dimension), we are able to distribute and reuse the aggregated
features over time. This approach leads to a substantial reduction in
computation without sacrificing accuracy. We conducted extensive evaluations
and comparisons and demonstrated that our method achieves superior performance
compared to the current state-of-the-art, making it a strong contender for
real-time stereo matching in VR/AR applications.","['Ziang Cheng', 'Jiayu Yang', 'Hongdong Li']",2023-09-08T07:53:58Z,http://arxiv.org/abs/2309.04183v1
"A Novel Augmented Reality Ultrasound Framework Using an RGB-D Camera and
  a 3D-printed Marker","Purpose. Ability to locate and track ultrasound images in the 3D operating
space is of great benefit for multiple clinical applications. This is often
accomplished by tracking the probe using a precise but expensive optical or
electromagnetic tracking system. Our goal is to develop a simple and low cost
augmented reality echography framework using a standard RGB-D Camera.
  Methods. A prototype system consisting of an Occipital Structure Core RGB-D
camera, a specifically-designed 3D marker, and a fast point cloud registration
algorithm FaVoR was developed and evaluated on an Ultrasonix ultrasound system.
The probe was calibrated on a 3D-printed N-wire phantom using the software PLUS
toolkit. The proposed calibration method is simplified, requiring no additional
markers or sensors attached to the phantom. Also, a visualization software
based on OpenGL was developed for the augmented reality application.
  Results. The calibrated probe was used to augment a real-world video in a
simulated needle insertion scenario. The ultrasound images were rendered on the
video, and visually-coherent results were observed. We evaluated the end-to-end
accuracy of our AR US framework on localizing a cube of 5 cm size. From our two
experiments, the target pose localization error ranges from 5.6 to 5.9 mm and
from -3.9 to 4.2 degrees.
  Conclusion. We believe that with the potential democratization of RGB-D
cameras integrated in mobile devices and AR glasses in the future, our
prototype solution may facilitate the use of 3D freehand ultrasound in clinical
routine. Future work should include a more rigorous and thorough evaluation, by
comparing the calibration accuracy with those obtained by commercial tracking
solutions in both simulated and real medical scenarios.","['Yitian Zhou', 'Gaétan Lelu', 'Boris Labbé', 'Guillaume Pasquier', 'Pierre Le Gargasson', 'Albert Murienne', 'Laurent Launay']",2022-05-09T14:54:47Z,http://arxiv.org/abs/2205.04350v1
"Virtual Windshields: Merging Reality and Digital Content to Improve the
  Driving Experience","In recent years, the use of the automobile as the primary mode of
transportation has been increasing and driving has become an important part of
daily life. Driving is a multi-sensory experience as drivers rely on their
senses to provide them with important information. In a vehicular context human
senses are all too often limited and obstructed. Today, road accidents
constitute the eighth leading cause of death. The escalation of technology has
propelled new ways in which driver's senses may be augmented. The enclosed
aspect of a car, allied with the configuration of the controls and displays
directed towards the driver, offer significant advantages for augmented reality
(AR) systems when considering the amount of immersion it can provide to the
user. In addition, the inherent mobility and virtually unlimited power autonomy
transform cars into perfect mobile computing platforms. However, automobiles
currently present limited network connectivity and thus the created augmented
objects are merely providing information captured by in-vehicle sensors,
cameras and other databases. By combining the new paradigm of Vehicular Ad Hoc
Networking (VANET) with AR human machine interfaces, we show that it is
possible to design novel cooperative Advanced Driver Assistance Systems (ADAS),
that base the creation of AR content on the information collected from
neighbouring vehicles or roadside infrastructures. As such we implement
prototypes of both visual and acoustic AR systems, which can significantly
improve the driving experience. We believe our results contribute to the
formulation of a vision where the vehicle is perceived as an extension of the
body which permeates the human senses to the world outside the vessel, where
the car is used as a better, multi-sensory immersive version of a mobile phone
that integrates touch, vision and sound enhancements, leveraging unique
properties of VANET.",['Michelle Krüger Silvéria'],2014-05-05T14:28:13Z,http://arxiv.org/abs/1405.0910v1
Lightform: Procedural Effects for Projected AR,"Projected augmented reality, also called projection mapping or video mapping,
is a form of augmented reality that uses projected light to directly augment 3D
surfaces, as opposed to using pass-through screens or headsets. The value of
projected AR is its ability to add a layer of digital content directly onto
physical objects or environments in a way that can be instantaneously viewed by
multiple people, unencumbered by a screen or additional setup.
  Because projected AR typically involves projecting onto non-flat, textured
objects (especially those that are conventionally not used as projection
surfaces), the digital content needs to be mapped and aligned to precisely fit
the physical scene to ensure a compelling experience. Current projected AR
techniques require extensive calibration at the time of installation, which is
not conducive to iteration or change, whether intentional (the scene is
reconfigured) or not (the projector is bumped or settles). The workflows are
undefined and fragmented, thus making it confusing and difficult for many to
approach projected AR. For example, a digital artist may have the software
expertise to create AR content, but could not complete an installation without
experience in mounting, blending, and realigning projector(s); the converse is
true for many A/V installation teams/professionals. Projection mapping has
therefore been limited to high-end event productions, concerts, and films,
because it requires expensive, complex tools, and skilled teams ($100K+
budgets).
  Lightform provides a technology that makes projected AR approachable,
practical, intelligent, and robust through integrated hardware and
computer-vision software. Lightform brings together and unites a currently
fragmented workflow into a single cohesive process that provides users with an
approachable and robust method to create and control projected AR experiences.","['Brittany Factura', 'Laura LaPerche', 'Phil Reyneri', 'Brett Jones', 'Kevin Karsch']",2019-12-25T00:15:38Z,http://arxiv.org/abs/2001.00521v1
"Alternative Modes of Interaction in Proximal Human-in-the-Loop Operation
  of Robots","Ambiguity and noise in natural language instructions create a significant
barrier towards adopting autonomous systems into safety critical workflows
involving humans and machines. In this paper, we propose to build on recent
advances in electrophysiological monitoring methods and augmented reality
technologies, to develop alternative modes of communication between humans and
robots involved in large-scale proximal collaborative tasks. We will first
introduce augmented reality techniques for projecting a robot's intentions to
its human teammate, who can interact with these cues to engage in real-time
collaborative plan execution with the robot. We will then look at how
electroencephalographic (EEG) feedback can be used to monitor human response to
both discrete events, as well as longer term affective states while execution
of a plan. These signals can be used by a learning agent, a.k.a an affective
robot, to modify its policy. We will present an end-to-end system capable of
demonstrating these modalities of interaction. We hope that the proposed system
will inspire research in augmenting human-robot interactions with alternative
forms of communications in the interests of safety, productivity, and fluency
of teaming, particularly in engineered settings such as the factory floor or
the assembly line in the manufacturing industry where the use of such wearables
can be enforced.","['Tathagata Chakraborti', 'Sarath Sreedharan', 'Anagha Kulkarni', 'Subbarao Kambhampati']",2017-03-27T05:08:02Z,http://arxiv.org/abs/1703.08930v1
"Challenges and New Directions in Augmented Reality, Computer Security,
  and Neuroscience -- Part 1: Risks to Sensation and Perception","Rapidly advancing AR technologies are in a unique position to directly
mediate between the human brain and the physical world. Though this tight
coupling presents tremendous opportunities for human augmentation, it also
presents new risks due to potential adversaries, including AR applications or
devices themselves, as well as bugs or accidents. In this paper, we begin
exploring potential risks to the human brain from augmented reality. Our
initial focus is on sensory and perceptual risks (e.g., accidentally or
maliciously induced visual adaptations, motion-induced blindness, and
photosensitive epilepsy), but similar risks may span both lower- and
higher-level human brain functions, including cognition, memory, and
decision-making. Though they have not yet manifested in practice in
early-generation AR technologies, we believe that such risks are uniquely
dangerous in AR due to the richness and depth with which it interacts with a
user's experience of the physical world. We propose a framework, based in
computer security threat modeling, to conceptually and experimentally evaluate
such risks. The ultimate goal of our work is to aid AR technology developers,
researchers, and neuroscientists to consider these issues before AR
technologies are widely deployed and become targets for real adversaries. By
considering and addressing these issues now, we can help ensure that future AR
technologies can meet their full, positive potential.","['Stefano Baldassi', 'Tadayoshi Kohno', 'Franziska Roesner', 'Moqian Tian']",2018-06-27T16:30:57Z,http://arxiv.org/abs/1806.10557v1
A True AR Authoring Tool for Interactive Virtual Museums,"In this work, a new and innovative way of spatial computing that appeared
recently in the bibliography called True Augmented Reality (AR), is employed in
cultural heritage preservation. This innovation could be adapted by the Virtual
Museums of the future to enhance the quality of experience. It emphasises, the
fact that a visitor will not be able to tell, at a first glance, if the
artefact that he/she is looking at is real or not and it is expected to draw
the visitors' interest. True AR is not limited to artefacts but extends even to
buildings or life-sized character simulations of statues. It provides the best
visual quality possible so that the users will not be able to tell the real
objects from the augmented ones. Such applications can be beneficial for future
museums, as with True AR, 3D models of various exhibits, monuments, statues,
characters and buildings can be reconstructed and presented to the visitors in
a realistic and innovative way. We also propose our Virtual Reality Sample
application, a True AR playground featuring basic components and tools for
generating interactive Virtual Museum applications, alongside a 3D
reconstructed character (the priest of Asinou church) facilitating the
storyteller of the augmented experience.","['Efstratios Geronikolakis', 'Paul Zikas', 'Steve Kateros', 'Nick Lydatakis', 'Stelios Georgiou', 'Mike Kentros', 'George Papagiannakis']",2019-09-20T11:10:23Z,http://arxiv.org/abs/1909.09429v4
"Underwater Augmented Reality for improving the diving experience in
  submerged archaeological sites","The Mediterranean Sea has a vast maritime heritage which exploitation is made
difficult because of the many limitations imposed by the submerged environment.
Archaeological diving tours, in fact, suffer from the impossibility to provide
underwater an exhaustive explanation of the submerged remains. Furthermore, low
visibility conditions, due to water turbidity and biological colonization,
sometimes make very confusing for tourists to find their way around in the
underwater archaeological site. To this end, the paper investigates the
feasibility and potentials of the underwater Augmented Reality (UWAR)
technologies developed in the iMARECulture project for improving the experience
of the divers that visit the Underwater Archaeological Park of Baiae (Naples).
In particular, the paper presents two UWAR technologies that adopt hybrid
tracking techniques to perform an augmented visualization of the actual
conditions and of a hypothetical 3D reconstruction of the archaeological
remains as appeared in the past. The first one integrates a marker-based
tracking with inertial sensors, while the second one adopts a markerless
approach that integrates acoustic localization and visual-inertial odometry.
The experimentations show that the proposed UWAR technologies could contribute
to have a better comprehension of the underwater site and its archaeological
remains.","['Fabio Bruno', 'Loris Barbieri', 'Marino Mangeruga', 'Marco Cozza', 'Antonio Lagudi', 'Jan Čejka', 'Fotis Liarokapis', 'Dimitrios Skarlatos']",2020-10-14T14:08:00Z,http://arxiv.org/abs/2010.07113v1
"A Systematic Review of Extended Reality (XR) for Understanding and
  Augmenting Vision Loss","Over the past decade, extended reality (XR) has emerged as an assistive
technology not only to augment residual vision of people losing their sight but
also to study the rudimentary vision restored to blind people by a visual
neuroprosthesis. To make the best use of these emerging technologies, it is
valuable and timely to understand the state of this research and identify any
shortcomings that are present. Here we present a systematic literature review
of 227 publications from 106 different venues assessing the potential of XR
technology to further visual accessibility. In contrast to other reviews, we
sample studies from multiple scientific disciplines, focus on augmentation of a
person's residual vision, and require studies to feature a quantitative
evaluation with appropriate end users. We summarize prominent findings from
different XR research areas, show how the landscape has changed over the last
decade, and identify scientific gaps in the literature. Specifically, we
highlight the need for real-world validation, the broadening of end-user
participation, and a more nuanced understanding of the suitability and
usability of different XR-based accessibility aids. By broadening end-user
participation to early stages of the design process and shifting the focus from
behavioral performance to qualitative assessments of usability, future research
has the potential to develop XR technologies that may not only allow for
studying vision loss, but also enable novel visual accessibility aids with the
potential to impact the lives of millions of people living with vision loss.","['Justin Kasowski', 'Byron A. Johnson', 'Ryan Neydavood', 'Anvitha Akkaraju', 'Michael Beyeler']",2021-09-10T17:05:50Z,http://arxiv.org/abs/2109.04995v2
Saliency in Augmented Reality,"With the rapid development of multimedia technology, Augmented Reality (AR)
has become a promising next-generation mobile platform. The primary theory
underlying AR is human visual confusion, which allows users to perceive the
real-world scenes and augmented contents (virtual-world scenes) simultaneously
by superimposing them together. To achieve good Quality of Experience (QoE), it
is important to understand the interaction between two scenarios, and
harmoniously display AR contents. However, studies on how this superimposition
will influence the human visual attention are lacking. Therefore, in this
paper, we mainly analyze the interaction effect between background (BG) scenes
and AR contents, and study the saliency prediction problem in AR. Specifically,
we first construct a Saliency in AR Dataset (SARD), which contains 450 BG
images, 450 AR images, as well as 1350 superimposed images generated by
superimposing BG and AR images in pair with three mixing levels. A large-scale
eye-tracking experiment among 60 subjects is conducted to collect eye movement
data. To better predict the saliency in AR, we propose a vector quantized
saliency prediction method and generalize it for AR saliency prediction. For
comparison, three benchmark methods are proposed and evaluated together with
our proposed method on our SARD. Experimental results demonstrate the
superiority of our proposed method on both of the common saliency prediction
problem and the AR saliency prediction problem over benchmark methods. Our
dataset and code are available at: https://github.com/DuanHuiyu/ARSaliency.","['Huiyu Duan', 'Wei Shen', 'Xiongkuo Min', 'Danyang Tu', 'Jing Li', 'Guangtao Zhai']",2022-04-18T13:25:07Z,http://arxiv.org/abs/2204.08308v2
Safe Walking In VR using Augmented Virtuality,"New technologies allow ordinary people to access Virtual Reality at
affordable prices in their homes. One of the most important tasks when
interacting with immersive Virtual Reality is to navigate the virtual
environments (VEs). Arguably, the best methods to accomplish this use of direct
control interfaces. Among those, natural walking (NW) makes for enjoyable user
experience. However, common techniques to support direct control interfaces in
VEs feature constraints that make it difficult to use those methods in cramped
home environments. Indeed, NW requires unobstructed and open space. To approach
this problem, we propose a new virtual locomotion technique, Combined Walking
in Place (CWIP). CWIP allows people to take advantage of the available physical
space and empowers them to use NW to navigate in the virtual world. For longer
distances, we adopt Walking in Place (WIP) to enable them to move in the
virtual world beyond the confines of a cramped real room. However, roaming in
immersive alternate reality, while moving in the confines of a cluttered
environment can lead people to stumble and fall. To approach these problems, we
developed Augmented Virtual Reality (AVR), to inform users about real-world
hazards, such as chairs, drawers, walls via proxies and signs placed in the
virtual world. We propose thus CWIP-AVR as a way to safely explore VR in the
cramped confines of your own home. To our knowledge, this is the first approach
to combined different locomotion modalities in a safe manner. We evaluated it
in a user study with 20 participants to validate their ability to navigate a
virtual world while walking in a confined and cluttered real space. Our results
show that CWIP-AVR allows people to navigate VR safely, switching between
locomotion modes flexibly while maintaining a good immersion.","['Maurício Sousa', 'Daniel Mendes', 'Joaquim Jorge']",2019-11-29T10:09:19Z,http://arxiv.org/abs/1911.13032v1
"Forging the Industrial Metaverse -- Where Industry 5.0, Augmented and
  Mixed Reality, IIoT, Opportunistic Edge Computing and Digital Twins Meet","The Metaverse is a concept that proposes to immerse users into real-time
rendered 3D content virtual worlds delivered through Extended Reality (XR)
devices like Augmented and Mixed Reality (AR/MR) smart glasses and Virtual
Reality (VR) headsets. When the Metaverse concept is applied to industrial
environments, it is called Industrial Metaverse, a hybrid world where
industrial operators work by using some of the latest technologies. Currently,
such technologies are related to the ones fostered by Industry 4.0, which is
evolving towards Industry 5.0, a paradigm that enhances Industry 4.0 by
creating a sustainable and resilient world of industrial human-centric
applications. The Industrial Metaverse can benefit from Industry 5.0, since it
implies making use of dynamic and up-to-date content, as well as fast
human-to-machine interactions. To enable such enhancements, this article
proposes the concept of Meta-Operator: an Industry 5.0 worker that interacts
with Industrial Metaverse applications and with his/her surroundings through
advanced XR devices. This article provides a description of the technologies
that support Meta-Operators: the main components of the Industrial Metaverse,
the latest XR technologies and the use of Opportunistic Edge Computing
communications (to interact with surrounding IoT/IioT devices). Moreover, this
paper analyzes how to create the next generation of Industrial Metaverse
applications based on Industry 5.0, including the integration of AR/MR devices
with IoT/IIoT solutions, the development of advanced communications or the
creation of shared experiences. Finally, this article provides a list of
potential Industry 5.0 applications for the Industrial Metaverse and analyzes
the main challenges and research lines. Thus, this article provides useful
guidelines for the researchers that will create the next generation of
applications for the Industrial Metaverse.","['Tiago M. Fernández-Caramés', 'Paula Fraga-Lamas']",2024-03-17T19:14:28Z,http://arxiv.org/abs/2403.11312v1
"SceneGen: Generative Contextual Scene Augmentation using Scene Graph
  Priors","Spatial computing experiences are constrained by the real-world surroundings
of the user. In such experiences, augmenting virtual objects to existing scenes
require a contextual approach, where geometrical conflicts are avoided, and
functional and plausible relationships to other objects are maintained in the
target environment. Yet, due to the complexity and diversity of user
environments, automatically calculating ideal positions of virtual content that
is adaptive to the context of the scene is considered a challenging task.
Motivated by this problem, in this paper we introduce SceneGen, a generative
contextual augmentation framework that predicts virtual object positions and
orientations within existing scenes. SceneGen takes a semantically segmented
scene as input, and outputs positional and orientational probability maps for
placing virtual content. We formulate a novel spatial Scene Graph
representation, which encapsulates explicit topological properties between
objects, object groups, and the room. We believe providing explicit and
intuitive features plays an important role in informative content creation and
user interaction of spatial computing settings, a quality that is not captured
in implicit models. We use kernel density estimation (KDE) to build a
multivariate conditional knowledge model trained using prior spatial Scene
Graphs extracted from real-world 3D scanned data. To further capture
orientational properties, we develop a fast pose annotation tool to extend
current real-world datasets with orientational labels. Finally, to demonstrate
our system in action, we develop an Augmented Reality application, in which
objects can be contextually augmented in real-time.","['Mohammad Keshavarzi', 'Aakash Parikh', 'Xiyu Zhai', 'Melody Mao', 'Luisa Caldas', 'Allen Y. Yang']",2020-09-25T18:36:27Z,http://arxiv.org/abs/2009.12395v2
Multi-Scale Subgraph Contrastive Learning,"Graph-level contrastive learning, aiming to learn the representations for
each graph by contrasting two augmented graphs, has attracted considerable
attention. Previous studies usually simply assume that a graph and its
augmented graph as a positive pair, otherwise as a negative pair. However, it
is well known that graph structure is always complex and multi-scale, which
gives rise to a fundamental question: after graph augmentation, will the
previous assumption still hold in reality? By an experimental analysis, we
discover the semantic information of an augmented graph structure may be not
consistent as original graph structure, and whether two augmented graphs are
positive or negative pairs is highly related with the multi-scale structures.
Based on this finding, we propose a multi-scale subgraph contrastive learning
architecture which is able to characterize the fine-grained semantic
information. Specifically, we generate global and local views at different
scales based on subgraph sampling, and construct multiple contrastive
relationships according to their semantic associations to provide richer
self-supervised signals. Extensive experiments and parametric analyzes on eight
graph classification real-world datasets well demonstrate the effectiveness of
the proposed method.","['Yanbei Liu', 'Yu Zhao', 'Xiao Wang', 'Lei Geng', 'Zhitao Xiao']",2024-03-05T07:17:18Z,http://arxiv.org/abs/2403.02719v3
Optimal Control of Wireless Computing Networks,"Augmented information (AgI) services allow users to consume information that
results from the execution of a chain of service functions that process source
information to create real-time augmented value. Applications include real-time
analysis of remote sensing data, real-time computer vision, personalized video
streaming, and augmented reality, among others. We consider the problem of
optimal distribution of AgI services over a wireless computing network, in
which nodes are equipped with both communication and computing resources. We
characterize the wireless computing network capacity region and design a joint
flow scheduling and resource allocation algorithm that stabilizes the
underlying queuing system while achieving a network cost arbitrarily close to
the minimum, with a tradeoff in network delay. Our solution captures the unique
chaining and flow scaling aspects of AgI services, while exploiting the use of
the broadcast approach coding scheme over the wireless channel.","['Hao Feng', 'Jaime Llorca', 'Antonia M. Tulino', 'Andreas F. Molisch']",2017-10-27T23:58:31Z,http://arxiv.org/abs/1710.10356v1
"Tangible Holograms: Towards Mobile Physical Augmentation of Virtual
  Objects","The last two decades have seen the emergence and steady development of
tangible user interfaces. While most of these interfaces are applied for input
- with output still on traditional computer screens - the goal of programmable
matter and actuated shape-changing materials is to directly use the physical
objects for visual or tangible feedback. Advances in material sciences and
flexible display technologies are investigated to enable such reconfigurable
physical objects. While existing solutions aim for making physical objects more
controllable via the digital world, we propose an approach where holograms
(virtual objects) in a mixed reality environment are augmented with physical
variables such as shape, texture or temperature. As such, the support for
mobility forms an important contribution of the proposed solution since it
enables users to freely move within and across environments. Furthermore, our
augmented virtual objects can co-exist in a single environment with
programmable matter and other actuated shape-changing solutions. The future
potential of the proposed approach is illustrated in two usage scenarios and we
hope that the presentation of our work in progress on a novel way to realise
tangible holograms will foster some lively discussions in the CHI community.","['Beat Signer', 'Timothy J. Curtin']",2017-03-24T05:31:56Z,http://arxiv.org/abs/1703.08288v1
"Playing optical tweezers with deep reinforcement learning: in virtual,
  physical and augmented environments","Reinforcement learning was carried out in a simulated environment to learn
continuous velocity control over multiple motor axes. This was then applied to
a real-world optical tweezers experiment with the objective of moving a
laser-trapped microsphere to a target location whilst avoiding collisions with
other free-moving microspheres. The concept of training a neural network in a
virtual environment has significant potential in the application of machine
learning for experimental optimization and control, as the neural network can
discover optimal methods for problem solving without the risk of damage to
equipment, and at a speed not limited by movement in the physical environment.
As the neural network treats both virtual and physical environments
equivalently, we show that the network can also be applied to an augmented
environment, where a virtual environment is combined with the physical
environment. This technique may have the potential to unlock capabilities
associated with mixed and augmented reality, such as enforcing safety limits
for machine motion or as a method of inputting observations from additional
sensors.","['Matthew Praeger', 'Yunhui Xie', 'James A. Grant-Jacob', 'Robert W. Eason', 'Ben Mills']",2020-11-05T13:49:55Z,http://arxiv.org/abs/2011.04424v2
"Enabling Tangible Interaction through Detection and Augmentation of
  Everyday Objects","Digital interaction with everyday objects has become popular since the
proliferation of camera-based systems that detect and augment objects
""just-in-time"". Common systems use a vision-based approach to detect objects
and display their functionalities to the user. Sensors, such as color and depth
cameras, have become inexpensive and allow seamless environmental tracking in
mobile as well as stationary settings. However, object detection in different
contexts faces challenges as it highly depends on environmental parameters and
the conditions of the object itself. In this work, we present three tracking
algorithms which we have employed in past research projects to track and
recognize objects. We show, how mobile and stationary augmented reality can be
used to extend the functionalities of objects. We conclude, how common items
can provide user-defined tangible interaction beyond their regular
functionality.","['Thomas Kosch', 'Albrecht Schmidt']",2020-12-20T12:04:40Z,http://arxiv.org/abs/2012.10904v1
"Real-Aug: Realistic Scene Synthesis for LiDAR Augmentation in 3D Object
  Detection","Data and model are the undoubtable two supporting pillars for LiDAR object
detection. However, data-centric works have fallen far behind compared with the
ever-growing list of fancy new models. In this work, we systematically study
the synthesis-based LiDAR data augmentation approach (so-called GT-Aug) which
offers maxium controllability over generated data samples. We pinpoint the main
shortcoming of existing works is introducing unrealistic LiDAR scan patterns
during GT-Aug. In light of this finding, we propose Real-Aug, a synthesis-based
augmentation method which prioritizes on generating realistic LiDAR scans. Our
method consists a reality-conforming scene composition module which handles the
details of the composition and a real-synthesis mixing up training strategy
which gradually adapts the data distribution from synthetic data to the real
one. To verify the effectiveness of our methods, we conduct extensive ablation
studies and validate the proposed Real-Aug on a wide combination of detectors
and datasets. We achieve a state-of-the-art 0.744 NDS and 0.702 mAP on nuScenes
test set. The code shall be released soon.","['Jinglin Zhan', 'Tiejun Liu', 'Rengang Li', 'Jingwei Zhang', 'Zhaoxiang Zhang', 'Yuntao Chen']",2023-05-22T09:24:55Z,http://arxiv.org/abs/2305.12853v1
"RealityCanvas: Augmented Reality Sketching for Embedded and Responsive
  Scribble Animation Effects","We introduce RealityCanvas, a mobile AR sketching tool that can easily
augment real-world physical motion with responsive hand-drawn animation. Recent
research in AR sketching tools has enabled users to not only embed static
drawings into the real world but also dynamically animate them with physical
motion. However, existing tools often lack the flexibility and expressiveness
of possible animations, as they primarily support simple line-based geometry.
To address this limitation, we explore both expressive and improvisational AR
sketched animation by introducing a set of responsive scribble animation
techniques that can be directly embedded through sketching interactions: 1)
object binding, 2) flip-book animation, 3) action trigger, 4) particle effects,
5) motion trajectory, and 6) contour highlight. These six animation effects
were derived from the analysis of 172 existing video-edited scribble
animations. We showcase these techniques through various applications, such as
video creation, augmented education, storytelling, and AR prototyping. The
results of our user study and expert interviews confirm that our tool can lower
the barrier to creating AR-based sketched animation, while allowing creative,
expressive, and improvisational AR sketching experiences.","['Zhijie Xia', 'Kyzyl Monteiro', 'Kevin Van', 'Ryo Suzuki']",2023-07-30T03:31:48Z,http://arxiv.org/abs/2307.16116v1
"LFW-Beautified: A Dataset of Face Images with Beautification and
  Augmented Reality Filters","Selfie images enjoy huge popularity in social media. The same platforms
centered around sharing this type of images offer filters to beautify them or
incorporate augmented reality effects. Studies suggests that filtered images
attract more views and engagement. Selfie images are also in increasing use in
security applications due to mobiles becoming data hubs for many transactions.
Also, video conference applications, boomed during the pandemic, include such
filters.
  Such filters may destroy biometric features that would allow person
recognition or even detection of the face itself, even if such commodity
applications are not necessarily used to compromise facial systems. This could
also affect subsequent investigations like crimes in social media, where
automatic analysis is usually necessary given the amount of information posted
in social sites or stored in devices or cloud repositories.
  To help in counteracting such issues, we contribute with a database of facial
images that includes several manipulations. It includes image enhancement
filters (which mostly modify contrast and lightning) and augmented reality
filters that incorporate items like animal noses or glasses. Additionally,
images with sunglasses are processed with a reconstruction network trained to
learn to reverse such modifications. This is because obfuscating the eye region
has been observed in the literature to have the highest impact on the accuracy
of face detection or recognition.
  We start from the popular Labeled Faces in the Wild (LFW) database, to which
we apply different modifications, generating 8 datasets. Each dataset contains
4,324 images of size 64 x 64, with a total of 34,592 images. The use of a
public and widely employed face dataset allows for replication and comparison.
  The created database is available at
https://github.com/HalmstadUniversityBiometrics/LFW-Beautified","['Pontus Hedman', 'Vasilios Skepetzis', 'Kevin Hernandez-Diaz', 'Josef Bigun', 'Fernando Alonso-Fernandez']",2022-03-11T17:05:10Z,http://arxiv.org/abs/2203.06082v1
"Real-time Virtual-Try-On from a Single Example Image through Deep
  Inverse Graphics and Learned Differentiable Renderers","Augmented reality applications have rapidly spread across online platforms,
allowing consumers to virtually try-on a variety of products, such as makeup,
hair dying, or shoes. However, parametrizing a renderer to synthesize realistic
images of a given product remains a challenging task that requires expert
knowledge. While recent work has introduced neural rendering methods for
virtual try-on from example images, current approaches are based on large
generative models that cannot be used in real-time on mobile devices. This
calls for a hybrid method that combines the advantages of computer graphics and
neural rendering approaches. In this paper we propose a novel framework based
on deep learning to build a real-time inverse graphics encoder that learns to
map a single example image into the parameter space of a given augmented
reality rendering engine. Our method leverages self-supervised learning and
does not require labeled training data which makes it extendable to many
virtual try-on applications. Furthermore, most augmented reality renderers are
not differentiable in practice due to algorithmic choices or implementation
constraints to reach real-time on portable devices. To relax the need for a
graphics-based differentiable renderer in inverse graphics problems, we
introduce a trainable imitator module. Our imitator is a generative network
that learns to accurately reproduce the behavior of a given non-differentiable
renderer. We propose a novel rendering sensitivity loss to train the imitator,
which ensures that the network learns an accurate and continuous representation
for each rendering parameter. Our framework enables novel applications where
consumers can virtually try-on a novel unknown product from an inspirational
reference image on social media. It can also be used by graphics artists to
automatically create realistic rendering from a reference product image.","['Robin Kips', 'Ruowei Jiang', 'Sileye Ba', 'Brendan Duke', 'Matthieu Perrot', 'Pietro Gori', 'Isabelle Bloch']",2022-05-12T18:44:00Z,http://arxiv.org/abs/2205.06305v1
"DroneARchery: Human-Drone Interaction through Augmented Reality with
  Haptic Feedback and Multi-UAV Collision Avoidance Driven by Deep
  Reinforcement Learning","We propose a novel concept of augmented reality (AR) human-drone interaction
driven by RL-based swarm behavior to achieve intuitive and immersive control of
a swarm formation of unmanned aerial vehicles. The DroneARchery system
developed by us allows the user to quickly deploy a swarm of drones, generating
flight paths simulating archery. The haptic interface LinkGlide delivers a
tactile stimulus of the bowstring tension to the forearm to increase the
precision of aiming. The swarm of released drones dynamically avoids collisions
between each other, the drone following the user, and external obstacles with
behavior control based on deep reinforcement learning.
  The developed concept was tested in the scenario with a human, where the user
shoots from a virtual bow with a real drone to hit the target. The human
operator observes the ballistic trajectory of the drone in an AR and achieves a
realistic and highly recognizable experience of the bowstring tension through
the haptic display.
  The experimental results revealed that the system improves trajectory
prediction accuracy by 63.3% through applying AR technology and conveying
haptic feedback of pulling force. DroneARchery users highlighted the
naturalness (4.3 out of 5 point Likert scale) and increased confidence (4.7 out
of 5) when controlling the drone. We have designed the tactile patterns to
present four sliding distances (tension) and three applied force levels
(stiffness) of the haptic display. Users demonstrated the ability to
distinguish tactile patterns produced by the haptic display representing
varying bowstring tension(average recognition rate is of 72.8%) and stiffness
(average recognition rate is of 94.2%).
  The novelty of the research is the development of an AR-based approach for
drone control that does not require special skills and training from the
operator.","['Ekaterina Dorzhieva', 'Ahmed Baza', 'Ayush Gupta', 'Aleksey Fedoseev', 'Miguel Altamirano Cabrera', 'Ekaterina Karmanova', 'Dzmitry Tsetserukou']",2022-10-14T12:02:51Z,http://arxiv.org/abs/2210.07730v1
"Fast, Accurate, but Sometimes Too-Compelling Support: The Impact of
  Imperfectly Automated Cues in an Augmented Reality Head-Mounted Display on
  Visual Search Performance","While visual search for targets within a complex scene might benefit from
using augmented-reality (AR) head-mounted display (HMD) technologies helping to
efficiently direct human attention, imperfectly reliable automation support
could manifest in occasional errors. The current study examined the
effectiveness of different HMD cues that might support visual search
performance and their respective consequences following automation errors.
Fifty-six participants searched a 3D environment containing 48 objects in a
room, in order to locate a target object that was viewed prior to each trial.
They searched either unaided or assisted by one of three HMD types of cues: an
arrow pointing to the target, a plan-view minimap highlighting the target, and
a constantly visible icon depicting the appearance of the target object. The
cue was incorrect on 17% of the trials for one group of participants and 100%
correct for the second group. Through both analysis and modeling of both search
speed and accuracy, the results indicated that the arrow and minimap cues
depicting location information were more effective than the icon cue depicting
visual appearance, both overall, and when the cue was correct. However, there
was a tradeoff on the infrequent occasions when the cue erred. The most
effective AR-based cue led to a greater automation bias, in which the cue was
more often blindly followed without careful examination of the raw images. The
results speak to the benefits of augmented reality and the need to examine
potential costs when AR-conveyed information may be incorrect because of
imperfectly reliable systems.","['Amelia C. Warden', 'Christopher D. Wickens', 'Daniel Rehberg', 'Francisco R. Ortega', 'Benjamin A. Clegg']",2023-03-24T23:32:00Z,http://arxiv.org/abs/2303.14300v1
"A Consumer-tier based Visual-Brain Machine Interface for Augmented
  Reality Glasses Interactions","Objective.Visual-Brain Machine Interface(V-BMI) has provide a novel
interaction technique for Augmented Reality (AR) industries. Several
state-of-arts work has demonstates its high accuracy and real-time interaction
capbilities. However, most of the studies employ EEGs devices that are rigid
and difficult to apply in real-life AR glasseses application sceniraros. Here
we develop a consumer-tier Visual-Brain Machine Inteface(V-BMI) system
specialized for Augmented Reality(AR) glasses interactions. Approach. The
developed system consists of a wearable hardware which takes advantages of fast
set-up, reliable recording and comfortable wearable experience that
specificized for AR glasses applications. Complementing this hardware, we have
devised a software framework that facilitates real-time interactions within the
system while accommodating a modular configuration to enhance scalability. Main
results. The developed hardware is only 110g and 120x85x23 mm, which with 1
Tohm and peak to peak voltage is less than 1.5 uV, and a V-BMI based angry bird
game and an Internet of Thing (IoT) AR applications are deisgned, we
demonstrated such technology merits of intuitive experience and efficiency
interaction. The real-time interaction accuracy is between 85 and 96
percentages in a commercial AR glasses (DTI is 2.24s and ITR 65 bits-min ).
Significance. Our study indicates the developed system can provide an essential
hardware-software framework for consumer based V-BMI AR glasses. Also, we
derive several pivotal design factors for a consumer-grade V-BMI-based AR
system: 1) Dynamic adaptation of stimulation patterns-classification methods
via computer vision algorithms is necessary for AR glasses applications; and 2)
Algorithmic localization to foster system stability and latency reduction.","['Yuying Jiang', 'Fan Bai', 'Zicheng Zhang', 'Xiaochen Ye', 'Zheng Liu', 'Zhiping Shi', 'Jianwei Yao', 'Xiaojun Liu', 'Fangkun Zhu', 'Junling Li Qian Guo', 'Xiaoan Wang', 'Junwen Luo']",2023-08-29T06:33:13Z,http://arxiv.org/abs/2308.15056v1
"An objective comparison of methods for augmented reality in laparoscopic
  liver resection by preoperative-to-intraoperative image fusion","Augmented reality for laparoscopic liver resection is a visualisation mode
that allows a surgeon to localise tumours and vessels embedded within the liver
by projecting them on top of a laparoscopic image. Preoperative 3D models
extracted from CT or MRI data are registered to the intraoperative laparoscopic
images during this process. In terms of 3D-2D fusion, most of the algorithms
make use of anatomical landmarks to guide registration. These landmarks include
the liver's inferior ridge, the falciform ligament, and the occluding contours.
They are usually marked by hand in both the laparoscopic image and the 3D
model, which is time-consuming and may contain errors if done by a
non-experienced user. Therefore, there is a need to automate this process so
that augmented reality can be used effectively in the operating room. We
present the Preoperative-to-Intraoperative Laparoscopic Fusion Challenge
(P2ILF), held during the Medical Imaging and Computer Assisted Interventions
(MICCAI 2022) conference, which investigates the possibilities of detecting
these landmarks automatically and using them in registration. The challenge was
divided into two tasks: 1) A 2D and 3D landmark detection task and 2) a 3D-2D
registration task. The teams were provided with training data consisting of 167
laparoscopic images and 9 preoperative 3D models from 9 patients, with the
corresponding 2D and 3D landmark annotations. A total of 6 teams from 4
countries participated, whose proposed methods were evaluated on 16 images and
two preoperative 3D models from two patients. All the teams proposed deep
learning-based methods for the 2D and 3D landmark segmentation tasks and
differentiable rendering-based methods for the registration task. Based on the
experimental outcomes, we propose three key hypotheses that determine current
limitations and future directions for research in this domain.","['Sharib Ali', 'Yamid Espinel', 'Yueming Jin', 'Peng Liu', 'Bianca Güttner', 'Xukun Zhang', 'Lihua Zhang', 'Tom Dowrick', 'Matthew J. Clarkson', 'Shiting Xiao', 'Yifan Wu', 'Yijun Yang', 'Lei Zhu', 'Dai Sun', 'Lan Li', 'Micha Pfeiffer', 'Shahid Farid', 'Lena Maier-Hein', 'Emmanuel Buc', 'Adrien Bartoli']",2024-01-28T20:30:14Z,http://arxiv.org/abs/2401.15753v2
"A Practical Evaluation of Commercial Industrial Augmented Reality
  Systems in an Industry 4.0 Shipyard","The principles of the Industry 4.0 are guiding manufacturing companies
towards more automated and computerized factories. Such principles are also
applied in shipbuilding, which usually involves numerous complex processes
whose automation will improve its efficiency and performance. Navantia, a
company that has been building ships for 300 years, is modernizing its
shipyards according to the Industry 4.0 principles with the help of the latest
technologies. Augmented Reality (AR), which when utilized in an industrial
environment is called Industrial AR (IAR), is one of such technologies, since
it can be applied in numerous situations in order to provide useful and
attractive interfaces that allow shipyard operators to obtain information on
their tasks and to interact with certain elements that surround them. This
article first reviews the state of the art on IAR applications for shipbuilding
and smart manufacturing. Then, the most relevant IAR hardware and software
tools are detailed, as well as the main use cases for the application of IAR in
a shipyard. Next, it is described Navantia's IAR system, which is based on a
fog-computing architecture. Such a system is evaluated when making use of three
IAR devices (a smartphone, a tablet and a pair of smart glasses), two AR SDKs
(ARToolKit and Vuforia) and multiple IAR markers, with the objective of
determining their performance in a shipyard workshop and inside a ship under
construction. The results obtained show remarkable performance differences
among the different IAR tools and the impact of factors like lighting, pointing
out the best combinations of markers, hardware and software to be used
depending on the characteristics of the shipyard scenario.","['Oscar Blanco-Novoa', 'Tiago M Fernandez-Carames', 'Paula Fraga-Lamas', 'Miguel Vilar-Montesinos']",2024-02-01T18:13:56Z,http://arxiv.org/abs/2402.00925v1
Extended Reality for Mental Health Evaluation -A Scoping Review,"Mental health disorders are the leading cause of health-related problems
globally. It is projected that mental health disorders will be the leading
cause of morbidity among adults as the incidence rates of anxiety and
depression grows globally. Recently, extended reality (XR), a general term
covering virtual reality (VR), augmented reality (AR) and mixed reality (MR),
is paving a new way to deliver mental health care. In this paper, we conduct a
scoping review on the development and application of XR in the area of mental
disorders. We performed a scoping database search to identify the relevant
studies indexed in Google Scholar, PubMed, and the ACM Digital Library. A
search period between August 2016 and December 2023 was defined to select
articles related to the usage of VR, AR, and MR in a mental health context. We
identified a total of 85 studies from 27 countries across the globe. By
performing data analysis, we found that most of the studies focused on
developed countries such as the US (16.47%) and Germany (12.94%). None of the
studies were for African countries. The majority of the articles reported that
XR techniques led to a significant reduction in symptoms of anxiety or
depression. More studies were published in the year 2021, i.e., 31.76% (n =
31). This could indicate that mental disorder intervention received a higher
attention when COVID-19 emerged. Most studies (n = 65) focused on a population
between 18 and 65 years old, only a few studies focused on teenagers (n = 2).
Also, more studies were done experimentally (n = 67, 78.82%) rather than by
analytical and modeling approaches (n = 8, 9.41%). This shows that there is a
rapid development of XR technology for mental health care. Furthermore, these
studies showed that XR technology can effectively be used for evaluating mental
disorders in similar or better way as the conventional approaches.","['Omisore Olatunji', 'Ifeanyi Odenigbo', 'Joseph Orji', 'Amelia Beltran', 'Nilufar Baghaei', 'Meier Sandra', 'Rita Orji']",2022-04-04T09:46:30Z,http://arxiv.org/abs/2204.01348v2
"Augmented Reality Meets Computer Vision : Efficient Data Generation for
  Urban Driving Scenes","The success of deep learning in computer vision is based on availability of
large annotated datasets. To lower the need for hand labeled images, virtually
rendered 3D worlds have recently gained popularity. Creating realistic 3D
content is challenging on its own and requires significant human effort. In
this work, we propose an alternative paradigm which combines real and synthetic
data for learning semantic instance segmentation and object detection models.
Exploiting the fact that not all aspects of the scene are equally important for
this task, we propose to augment real-world imagery with virtual objects of the
target category. Capturing real-world images at large scale is easy and cheap,
and directly provides real background appearances without the need for creating
complex 3D models of the environment. We present an efficient procedure to
augment real images with virtual objects. This allows us to create realistic
composite images which exhibit both realistic background appearance and a large
number of complex object arrangements. In contrast to modeling complete 3D
environments, our augmentation approach requires only a few user interactions
in combination with 3D shapes of the target object. Through extensive
experimentation, we conclude the right set of parameters to produce augmented
data which can maximally enhance the performance of instance segmentation
models. Further, we demonstrate the utility of our approach on training
standard deep models for semantic instance segmentation and object detection of
cars in outdoor driving scenes. We test the models trained on our augmented
data on the KITTI 2015 dataset, which we have annotated with pixel-accurate
ground truth, and on Cityscapes dataset. Our experiments demonstrate that
models trained on augmented imagery generalize better than those trained on
synthetic data or models trained on limited amount of annotated real data.","['Hassan Abu Alhaija', 'Siva Karthik Mustikovela', 'Lars Mescheder', 'Andreas Geiger', 'Carsten Rother']",2017-08-04T16:03:52Z,http://arxiv.org/abs/1708.01566v1
"PhotoTwinVR: An Immersive System for Manipulation, Inspection and
  Dimension Measurements of the 3D Photogrammetric Models of Real-Life
  Structures in Virtual Reality","Photogrammetry is a science dealing with obtaining reliable information about
physical objects using their imagery description. Recent advancements in the
development of Virtual Reality (VR) can help to unlock the full potential
offered by the digital 3D-reality models generated using the state-of-art
photogrammetric technologies. These models are becoming a viable alternative
for providing high-quality content for such immersive environment.
Simultaneously, their analyses in VR could bring added-value to professionals
working in various engineering and non-engineering settings and help in
extracting useful information about physical objects. However, there is little
research published to date on feasible interaction methods in the VR-based
systems augmented with the 3D photogrammetric models, especially concerning
gestural input interfaces. Consequently, this paper presents the PhotoTwinVR --
an immersive, gesture-controlled system for manipulation and inspection of 3D
photogrammetric models of physical objects in VR. Our system allows the user to
perform basic engineering operations on the model subjected to the off-line
inspection process. An observational study with a group of three domain-expert
participants was completed to verify its feasibility. The system was populated
with a 3D photogrammetric model of an existing pipe-rack generated using a
commercial software package. The participants were asked to carry out a survey
measurement of the object using the measurement toolbox offered by PhotoTwinVR.
The study revealed a potential of such immersive tool to be applied in
practical real-words cases of off-line inspections of pipelines.","['Slawomir Konrad Tadeja', 'Wojciech Rydlewicz', 'Yupu Lu', 'Per Ola Kristensson', 'Tomasz Bubas', 'Maciej Rydlewicz']",2019-11-22T10:30:12Z,http://arxiv.org/abs/1911.09958v1
"Exploring Extended Reality with ILLIXR: A New Playground for
  Architecture Research","As we enter the era of domain-specific architectures, systems researchers
must understand the requirements of emerging application domains. Augmented and
virtual reality (AR/VR) or extended reality (XR) is one such important domain.
This paper presents ILLIXR, the first open source end-to-end XR system (1) with
state-of-the-art components, (2) integrated with a modular and extensible
multithreaded runtime, (3) providing an OpenXR compliant interface to XR
applications (e.g., game engines), and (4) with the ability to report (and
trade off) several quality of experience (QoE) metrics. We analyze performance,
power, and QoE metrics for the complete ILLIXR system and for its individual
components. Our analysis reveals several properties with implications for
architecture and systems research. These include demanding performance, power,
and QoE requirements, a large diversity of critical tasks, inter-dependent
execution pipelines with challenges in scheduling and resource management, and
a large tradeoff space between performance/power and human perception related
QoE metrics. ILLIXR and our analysis have the potential to propel new
directions in architecture and systems research in general, and impact XR in
particular. ILLIXR is open-source and available at https://illixr.github.io","['Muhammad Huzaifa', 'Rishi Desai', 'Samuel Grayson', 'Xutao Jiang', 'Ying Jing', 'Jae Lee', 'Fang Lu', 'Yihan Pang', 'Joseph Ravichandran', 'Finn Sinclair', 'Boyuan Tian', 'Hengzhi Yuan', 'Jeffrey Zhang', 'Sarita V. Adve']",2020-03-26T01:17:29Z,http://arxiv.org/abs/2004.04643v2
"Conservative Plane Releasing for Spatial Privacy Protection in Mixed
  Reality","Augmented reality (AR) or mixed reality (MR) platforms require spatial
understanding to detect objects or surfaces, often including their structural
(i.e. spatial geometry) and photometric (e.g. color, and texture) attributes,
to allow applications to place virtual or synthetic objects seemingly
""anchored"" on to real world objects; in some cases, even allowing interactions
between the physical and virtual objects. These functionalities require AR/MR
platforms to capture the 3D spatial information with high resolution and
frequency; however, these pose unprecedented risks to user privacy. Aside from
objects being detected, spatial information also reveals the location of the
user with high specificity, e.g. in which part of the house the user is. In
this work, we propose to leverage spatial generalizations coupled with
conservative releasing to provide spatial privacy while maintaining data
utility. We designed an adversary that builds up on existing place and shape
recognition methods over 3D data as attackers to which the proposed spatial
privacy approach can be evaluated against. Then, we simulate user movement
within spaces which reveals more of their space as they move around utilizing
3D point clouds collected from Microsoft HoloLens. Results show that revealing
no more than 11 generalized planes--accumulated from successively revealed
spaces with large enough radius, i.e. $r\leq1.0m$--can make an adversary fail
in identifying the spatial location of the user for at least half of the time.
Furthermore, if the accumulated spaces are of smaller radius, i.e. each
successively revealed space is $r\leq 0.5m$, we can release up to 29
generalized planes while enjoying both better data utility and privacy.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2020-04-17T01:57:58Z,http://arxiv.org/abs/2004.08029v1
Pen-based Interaction with Spreadsheets in Mobile Virtual Reality,"Virtual Reality (VR) can enhance the display and interaction of mobile
knowledge work and in particular, spreadsheet applications. While spreadsheets
are widely used yet are challenging to interact with, especially on mobile
devices, using them in VR has not been explored in depth. A special uniqueness
of the domain is the contrast between the immersive and large display space
afforded by VR, contrasted by the very limited interaction space that may be
afforded for the information worker on the go, such as an airplane seat or a
small work-space. To close this gap, we present a tool-set for enhancing
spreadsheet interaction on tablets using immersive VR headsets and pen-based
input. This combination opens up many possibilities for enhancing the
productivity for spreadsheet interaction. We propose to use the space around
and in front of the tablet for enhanced visualization of spreadsheet data and
meta-data. For example, extending sheet display beyond the bounds of the
physical screen, or easier debugging by uncovering hidden dependencies between
sheet's cells. Combining the precise on-screen input of a pen with spatial
sensing around the tablet, we propose tools for the efficient creation and
editing of spreadsheets functions such as off-the-screen layered menus,
visualization of sheets dependencies, and gaze-and-touch-based switching
between spreadsheet tabs. We study the feasibility of the proposed tool-set
using a video-based online survey and an expert-based assessment of indicative
human performance potential.","['Travis Gesslein', 'Verena Biener', 'Philipp Gagel', 'Daniel Schneider', 'Per Ola Kristensson', 'Eyal Ofek', 'Michel Pahud', 'Jens Grubert']",2020-08-11T06:39:35Z,http://arxiv.org/abs/2008.04543v1
"VR Sickness Prediction from Integrated HMD's Sensors using Multimodal
  Deep Fusion Network","Virtual Reality (VR) sickness commonly known as cybersickness is one of the
major problems for the comfortable use of VR systems. Researchers have proposed
different approaches for predicting cybersickness from bio-physiological data
(e.g., heart rate, breathing rate, electroencephalogram). However, collecting
bio-physiological data often requires external sensors, limiting locomotion and
3D-object manipulation during the virtual reality (VR) experience. Limited
research has been done to predict cybersickness from the data readily available
from the integrated sensors in head-mounted displays (HMDs) (e.g.,
head-tracking, eye-tracking, motion features), allowing free locomotion and
3D-object manipulation. This research proposes a novel deep fusion network to
predict cybersickness severity from heterogeneous data readily available from
the integrated HMD sensors. We extracted 1755 stereoscopic videos,
eye-tracking, and head-tracking data along with the corresponding self-reported
cybersickness severity collected from 30 participants during their VR gameplay.
We applied several deep fusion approaches with the heterogeneous data collected
from the participants. Our results suggest that cybersickness can be predicted
with an accuracy of 87.77\% and a root-mean-square error of 0.51 when using
only eye-tracking and head-tracking data. We concluded that eye-tracking and
head-tracking data are well suited for a standalone cybersickness prediction
framework.","['Rifatul Islam', 'Kevin Desai', 'John Quarles']",2021-08-14T01:28:15Z,http://arxiv.org/abs/2108.06437v1
"Extending 3-DoF Metrics to Model User Behaviour Similarity in 6-DoF
  Immersive Applications","Immersive reality technologies, such as Virtual and Augmented Reality, have
ushered a new era of user-centric systems, in which every aspect of the
coding--delivery--rendering chain is tailored to the interaction of the users.
Understanding the actual interactivity and behaviour of the users is still an
open challenge and a key step to enabling such a user-centric system. Our main
goal is to extend the applicability of existing behavioural methodologies for
studying user navigation in the case of 6 Degree-of-Freedom (DoF).
Specifically, we first compare the navigation in 6-DoF with its 3-DoF
counterpart highlighting the main differences and novelties. Then, we define
new metrics aimed at better modelling behavioural similarities between users in
a 6-DoF system. We validate and test our solutions on real navigation paths of
users interacting with dynamic volumetric media in 6-DoF Virtual Reality
conditions. Our results show that metrics that consider both user position and
viewing direction better perform in detecting user similarity while navigating
in a 6-DoF system. Having easy-to-use but robust metrics that underpin multiple
tools and answer the question ``how do we detect if two users look at the same
content?"" open the gate to new solutions for a user-centric system.","['Silvia Rossi', 'Irene Viola', 'Laura Toni', 'Pablo Cesar']",2021-12-17T09:29:06Z,http://arxiv.org/abs/2112.09402v2
"AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
  Sensing","Today's Mixed Reality head-mounted displays track the user's head pose in
world space as well as the user's hands for interaction in both Augmented
Reality and Virtual Reality scenarios. While this is adequate to support user
input, it unfortunately limits users' virtual representations to just their
upper bodies. Current systems thus resort to floating avatars, whose limitation
is particularly evident in collaborative settings. To estimate full-body poses
from the sparse input sources, prior work has incorporated additional trackers
and sensors at the pelvis or lower body, which increases setup complexity and
limits practical application in mobile settings. In this paper, we present
AvatarPoser, the first learning-based method that predicts full-body poses in
world coordinates using only motion input from the user's head and hands. Our
method builds on a Transformer encoder to extract deep features from the input
signals and decouples global motion from the learned local joint orientations
to guide pose estimation. To obtain accurate full-body motions that resemble
motion capture animations, we refine the arm joints' positions using an
optimization routine with inverse kinematics to match the original tracking
input. In our evaluation, AvatarPoser achieved new state-of-the-art results in
evaluations on large motion capture datasets (AMASS). At the same time, our
method's inference speed supports real-time operation, providing a practical
interface to support holistic avatar control and representation for Metaverse
applications.","['Jiaxi Jiang', 'Paul Streli', 'Huajian Qiu', 'Andreas Fender', 'Larissa Laich', 'Patrick Snape', 'Christian Holz']",2022-07-27T20:52:39Z,http://arxiv.org/abs/2207.13784v1
Deep Billboards towards Lossless Real2Sim in Virtual Reality,"An aspirational goal for virtual reality (VR) is to bring in a rich diversity
of real world objects losslessly. Existing VR applications often convert
objects into explicit 3D models with meshes or point clouds, which allow fast
interactive rendering but also severely limit its quality and the types of
supported objects, fundamentally upper-bounding the ""realism"" of VR. Inspired
by the classic ""billboards"" technique in gaming, we develop Deep Billboards
that model 3D objects implicitly using neural networks, where only 2D image is
rendered at a time based on the user's viewing direction. Our system,
connecting a commercial VR headset with a server running neural rendering,
allows real-time high-resolution simulation of detailed rigid objects, hairy
objects, actuated dynamic objects and more in an interactive VR world,
drastically narrowing the existing real-to-simulation (real2sim) gap.
Additionally, we augment Deep Billboards with physical interaction capability,
adapting classic billboards from screen-based games to immersive VR. At our
pavilion, the visitors can use our off-the-shelf setup for quickly capturing
their favorite objects, and within minutes, experience them in an immersive and
interactive VR world with minimal loss of reality. Our project page:
https://sites.google.com/view/deepbillboards/","['Naruya Kondo', 'So Kuroki', 'Ryosuke Hyakuta', 'Yutaka Matsuo', 'Shixiang Shane Gu', 'Yoichi Ochiai']",2022-08-08T16:16:29Z,http://arxiv.org/abs/2208.08861v1
"An Exploration of Hands-free Text Selection for Virtual Reality
  Head-Mounted Displays","Hand-based interaction, such as using a handheld controller or making hand
gestures, has been widely adopted as the primary method for interacting with
both virtual reality (VR) and augmented reality (AR) head-mounted displays
(HMDs). In contrast, hands-free interaction avoids the need for users' hands
and although it can afford additional benefits, there has been limited research
in exploring and evaluating hands-free techniques for these HMDs. As VR HMDs
become ubiquitous, people will need to do text editing, which requires
selecting text segments. Similar to hands-free interaction, text selection is
underexplored. This research focuses on both, text selection via hands-free
interaction. Our exploration involves a user study with 24 participants to
investigate the performance, user experience, and workload of three hands-free
selection mechanisms (Dwell, Blink, Voice) to complement head-based pointing.
Results indicate that Blink outperforms Dwell and Voice in completion time.
Users' subjective feedback also shows that Blink is the preferred technique for
text selection. This work is the first to explore hands-free interaction for
text selection in VR HMDs. Our results provide a solid platform for further
research in this important area.","['Xuanru Meng', 'Wenge Xu', 'Hai-Ning Liang']",2022-09-14T09:25:54Z,http://arxiv.org/abs/2209.06825v2
"Integrating Digital Twin and Advanced Intelligent Technologies to
  Realize the Metaverse","The advances in Artificial Intelligence (AI) have led to technological
advancements in a plethora of domains. Healthcare, education, and smart city
services are now enriched with AI capabilities. These technological
advancements would not have been realized without the assistance of fast,
secure, and fault-tolerant communication media. Traditional processing,
communication and storage technologies cannot maintain high levels of
scalability and user experience for immersive services. The metaverse is an
immersive three-dimensional (3D) virtual world that integrates fantasy and
reality into a virtual environment using advanced virtual reality (VR) and
augmented reality (AR) devices. Such an environment is still being developed
and requires extensive research in order for it to be realized to its highest
attainable levels. In this article, we discuss some of the key issues required
in order to attain realization of metaverse services. We propose a framework
that integrates digital twin (DT) with other advanced technologies such as the
sixth generation (6G) communication network, blockchain, and AI, to maintain
continuous end-to-end metaverse services. This article also outlines
requirements for an integrated, DT-enabled metaverse framework and provides a
look ahead into the evolving topic.","['Moayad Aloqaily', 'Ouns Bouachir', 'Fakhri Karray', 'Ismaeel Al Ridhawi', 'Abdulmotaleb El Saddik']",2022-10-03T17:02:58Z,http://arxiv.org/abs/2210.04606v1
Automated Reconstruction of 3D Open Surfaces from Sparse Point Clouds,"Real-world 3D data may contain intricate details defined by salient surface
gaps. Automated reconstruction of these open surfaces (e.g., non-watertight
meshes) is a challenging problem for environment synthesis in mixed reality
applications. Current learning-based implicit techniques can achieve high
fidelity on closed-surface reconstruction. However, their dependence on the
distinction between the inside and outside of a surface makes them incapable of
reconstructing open surfaces. Recently, a new class of implicit functions have
shown promise in reconstructing open surfaces by regressing an unsigned
distance field. Yet, these methods rely on a discretized representation of the
raw data, which loses important surface details and can lead to outliers in the
reconstruction. We propose IPVNet, a learning-based implicit model that
predicts the unsigned distance between a surface and a query point in 3D space
by leveraging both raw point cloud data and its discretized voxel counterpart.
Experiments on synthetic and real-world public datasets demonstrates that
IPVNet outperforms the state of the art while producing far fewer outliers in
the reconstruction.","['Mohammad Samiul Arshad', 'William J. Beksi']",2022-10-26T22:02:45Z,http://arxiv.org/abs/2210.15059v2
Big Data Meets Metaverse: A Survey,"We are living in the era of big data. The Metaverse is an emerging technology
in the future, and it has a combination of big data, AI (artificial
intelligence), VR (Virtual Reality), AR (Augmented Reality), MR (mixed
reality), and other technologies that will diminish the difference between
online and real-life interaction. It has the goal of becoming a platform where
we can work, go shopping, play around, and socialize. Each user who enters the
Metaverse interacts with the virtual world in a data way. With the development
and application of the Metaverse, the data will continue to grow, thus forming
a big data network, which will bring huge data processing pressure to the
digital world. Therefore, big data processing technology is one of the key
technologies to implement the Metaverse. In this survey, we provide a
comprehensive review of how Metaverse is changing big data. Moreover, we
discuss the key security and privacy of Metaverse big data in detail. Finally,
we summarize the open problems and opportunities of Metaverse, as well as the
future of Metaverse with big data. We hope that this survey will provide
researchers with the research direction and prospects of applying big data in
the Metaverse.","['Jiayi Sun', 'Wensheng Gan', 'Zefeng Chen', 'Junhui Li', 'Philip S. Yu']",2022-10-28T17:22:20Z,http://arxiv.org/abs/2210.16282v1
"SONIA: an immersive customizable virtual reality system for the
  education and exploration of brain networks","While mastery of neuroanatomy is important for the investigation of the
brain, there is an increasing interest in exploring the neural pathways to
better understand the roles of neural circuitry in brain functions. To tackle
the limitations of traditional 2D-display-based neuronavigation software in
intuitively visualizing complex 3D anatomies, several virtual reality (VR) and
augmented reality (AR) solutions have been proposed to facilitate
neuroanatomical education. However, with the increasing knowledge on brain
connectivity and the functioning of the sub-systems, there is still a lack of
similar software solutions for the education and exploration of these topics,
which demand more elaborate visualization and interaction strategies. To
address this gap, we designed the immerSive custOmizable Neuro learnIng plAform
(SONIA), a novel user-friendly VR software system with a multi-scale
interaction paradigm that allows flexible customization of learning materials.
With both quantitative and qualitative evaluations through user studies, the
proposed system is shown to have high usability, attractive visual design, and
good educational value. As the first immersive system that integrates
customizable design and detailed narratives of the brain sub-systems for the
education of neuroanatomy and brain connectivity, SONIA showcases new potential
directions and provides valuable insights regarding medical learning and
exploration in VR.","['Owen Hellum', 'Christopher Steele', 'Yiming Xiao']",2023-01-24T01:04:15Z,http://arxiv.org/abs/2301.09772v2
Enhance-NeRF: Multiple Performance Evaluation for Neural Radiance Fields,"The quality of three-dimensional reconstruction is a key factor affecting the
effectiveness of its application in areas such as virtual reality (VR) and
augmented reality (AR) technologies. Neural Radiance Fields (NeRF) can generate
realistic images from any viewpoint. It simultaneously reconstructs the shape,
lighting, and materials of objects, and without surface defects, which breaks
down the barrier between virtuality and reality. The potential spatial
correspondences displayed by NeRF between reconstructed scenes and real-world
scenes offer a wide range of practical applications possibilities. Despite
significant progress in 3D reconstruction since NeRF were introduced, there
remains considerable room for exploration and experimentation. NeRF-based
models are susceptible to interference issues caused by colored ""fog"" noise.
Additionally, they frequently encounter instabilities and failures while
attempting to reconstruct unbounded scenes. Moreover, the model takes a
significant amount of time to converge, making it even more challenging to use
in such scenarios. Our approach, coined Enhance-NeRF, which adopts joint color
to balance low and high reflectivity objects display, utilizes a decoding
architecture with prior knowledge to improve recognition, and employs
multi-layer performance evaluation mechanisms to enhance learning capacity. It
achieves reconstruction of outdoor scenes within one hour under single-card
condition. Based on experimental results, Enhance-NeRF partially enhances
fitness capability and provides some support to outdoor scene reconstruction.
The Enhance-NeRF method can be used as a plug-and-play component, making it
easy to integrate with other NeRF-based models. The code is available at:
https://github.com/TANQIanQ/Enhance-NeRF","['Qianqiu Tan', 'Tao Liu', 'Yinling Xie', 'Shuwan Yu', 'Baohua Zhang']",2023-06-08T15:49:30Z,http://arxiv.org/abs/2306.05303v1
"Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities","In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.","['Kai Li', 'Billy Pik Lik Lau', 'Xin Yuan', 'Wei Ni', 'Mohsen Guizani', 'Chau Yuen']",2023-07-13T11:14:46Z,http://arxiv.org/abs/2307.06687v2
"Feel the Breeze: Promoting Relaxation in Virtual Reality using Mid-Air
  Haptics","Mid-air haptic interfaces employ focused ultrasound waves to generate
touchless haptic sensations on the skin. Prior studies have demonstrated the
potential positive impact of mid-air haptic feedback on virtual experiences,
enhancing aspects such as enjoyment, immersion, and sense of agency. As a
highly immersive environment, Virtual Reality (VR) is being explored as a tool
for stress management and relaxation in current research. However, the impact
of incorporating mid-air haptic stimuli into relaxing experiences in VR has not
been studied thus far. In this paper, for the first time, we design a mid-air
haptic stimulation that is congruent with a relaxing scene in VR, and conduct a
user study investigating the effectiveness of this experience. Our user study
encompasses three different conditions: a control group with no relaxation
intervention, a VR-only relaxation experience, and a VR+Haptics relaxation
experience that includes the mid-air haptic feedback. While we did not find any
significant differences between the conditions, a trend suggesting that the
VR+Haptics condition might be associated with greater pleasure emerged,
requiring further validation with a larger sample size. These initial findings
set the foundation for future investigations into leveraging multimodal
interventions in VR, utilising mid-air haptics to potentially enhance
relaxation experiences.","['Naga Sai Surya Vamsy Malladi', 'Viktorija Paneva', 'Jörg Müller']",2023-08-18T09:45:42Z,http://arxiv.org/abs/2308.09424v1
"Mapping Eye Vergence Angle to the Depth of Real and Virtual Objects as
  an Objective Measure of Depth Perception","Recently, extended reality (XR) displays including augmented reality (AR) and
virtual reality (VR) have integrated eye tracking capabilities, which could
enable novel ways of interacting with XR content. The vergence angle of the
eyes constantly changes according to the distance of fixated objects. Here we
measured vergence angle for eye fixations on real and simulated target objects
in three different environments: real objects in the real-world (real), virtual
objects in the real-world (AR), and virtual objects in the virtual world (VR)
using gaze data from an eye-tracking device. In a repeated-measures design with
13 participants, Gaze-measured Vergence Angle (GVA) was measured while
participants fixated on targets at varying distances. As expected, results
showed a significant main effect of target depth such that increasing GVA was
associated with closer targets. However, there were consistent individual
differences in baseline GVA. When these individual differences were controlled
for, there was a small but statistically-significant main effect of environment
(real, AR, VR). Importantly, GVA was stable with respect to the starting depth
of previously fixated targets and invariant to directionality (convergence vs.
divergence). In addition, GVA proved to be a more veridical depth estimate than
subjective depth judgements.","['Mohammed Safayet Arefin', 'J. Edward Swan II', 'Russell Cohen Hoffing', 'Steven Thurman']",2023-11-12T06:50:32Z,http://arxiv.org/abs/2311.09242v2
"Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information","Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.","['Jie Li', 'Zhixin Li', 'Zhi Liu', 'Pengyuan Zhou', 'Richang Hong', 'Qiyue Li', 'Han Hu']",2023-11-28T03:45:29Z,http://arxiv.org/abs/2311.16462v1
"A Performance Analysis Modeling Framework for Extended Reality
  Applications in Edge-Assisted Wireless Networks","Extended reality (XR) is at the center of attraction in the research
community due to the emergence of augmented, mixed, and virtual reality
applications. The performance of such applications needs to be uptight to
maintain the requirements of latency, energy consumption, and freshness of
data. Therefore, a comprehensive performance analysis model is required to
assess the effectiveness of an XR application but is challenging to design due
to the dependence of the performance metrics on several difficult-to-model
parameters, such as computing resources and hardware utilization of XR and edge
devices, which are controlled by both their operating systems and the
application itself. Moreover, the heterogeneity in devices and wireless access
networks brings additional challenges in modeling. In this paper, we propose a
novel modeling framework for performance analysis of XR applications
considering edge-assisted wireless networks and validate the model with
experimental data collected from testbeds designed specifically for XR
applications. In addition, we present the challenges associated with
performance analysis modeling and present methods to overcome them in detail.
Finally, the performance evaluation shows that the proposed analytical model
can analyze XR applications' performance with high accuracy compared to the
state-of-the-art analytical models.","['Anik Mallik', 'Jiang Xie', 'Zhu Han']",2024-05-11T15:16:12Z,http://arxiv.org/abs/2405.07033v1
Evaluating Theory of Mind in Question Answering,"We propose a new dataset for evaluating question answering models with
respect to their capacity to reason about beliefs. Our tasks are inspired by
theory-of-mind experiments that examine whether children are able to reason
about the beliefs of others, in particular when those beliefs differ from
reality. We evaluate a number of recent neural models with memory augmentation.
We find that all fail on our tasks, which require keeping track of inconsistent
states of the world; moreover, the models' accuracy decreases notably when
random sentences are introduced to the tasks at test.","['Aida Nematzadeh', 'Kaylee Burns', 'Erin Grant', 'Alison Gopnik', 'Thomas L. Griffiths']",2018-08-28T15:16:17Z,http://arxiv.org/abs/1808.09352v1
Procams-Based Cybernetics,"Procams-based cybernetics is a unique, emerging research field, which aims at
enhancing and supporting our activities by naturally connecting human and
computers/machines as a cooperative integrated system via projector-camera
systems (procams). It rests on various research domains such as
virtual/augmented reality, computer vision, computer graphics, projection
display, human computer interface, human robot interaction and so on. This
laboratory presentation provides a brief history including recent achievements
of our procams-based cybernetics project.","['Kosuke Sato', 'Daisuke Iwai', 'Sei Ikeda', 'Noriko Takemura']",2015-10-09T15:47:00Z,http://arxiv.org/abs/1510.02710v1
"Cleverarm: A Novel Exoskeleton For Rehabilitation Of Upper Limb
  Impairments","CLEVERarm (Compact, Low-weight, Ergonomic, Virtual and Augmented Reality
Enhanced Rehabilitation arm) is a novel exoskeleton with eight degrees of
freedom supporting the motion of shoulder girdle, glenohumeral joint, elbow and
wrist. Of the eight degrees of freedom of the exoskeleton, six are active and
the two degrees of freedom supporting the motion of wrist are passive. This
paper briefly outlines the design of CLEVERarm and its control architectures.","['Rana Soltani-Zarrin', 'Amin Zeiaee', 'Andrew Eib', 'Reza Langari', 'Reza Tafreshi']",2017-12-06T18:44:41Z,http://arxiv.org/abs/1712.02322v2
FutureMapping: The Computational Structure of Spatial AI Systems,"We discuss and predict the evolution of Simultaneous Localisation and Mapping
(SLAM) into a general geometric and semantic `Spatial AI' perception capability
for intelligent embodied devices. A big gap remains between the visual
perception performance that devices such as augmented reality eyewear or
comsumer robots will require and what is possible within the constraints
imposed by real products. Co-design of algorithms, processors and sensors will
be needed. We explore the computational structure of current and future Spatial
AI algorithms and consider this within the landscape of ongoing hardware
developments.",['Andrew J. Davison'],2018-03-29T23:46:34Z,http://arxiv.org/abs/1803.11288v1
Visual-Inertial Navigation: A Concise Review,"As inertial and visual sensors are becoming ubiquitous, visual-inertial
navigation systems (VINS) have prevailed in a wide range of applications from
mobile augmented reality to aerial navigation to autonomous driving, in part
because of the complementary sensing capabilities and the decreasing costs and
size of the sensors. In this paper, we survey thoroughly the research efforts
taken in this field and strive to provide a concise but complete review of the
related work -- which is unfortunately missing in the literature while being
greatly demanded by researchers and engineers -- in the hope to accelerate the
VINS research and beyond in our society as a whole.",['Guoquan Huang'],2019-06-06T15:32:42Z,http://arxiv.org/abs/1906.02650v1
A Taxonomy for Virtual and Augmented Reality in Education,"In this paper, a taxonomy for VR/AR in education is presented that can help
differentiate and categorise education experiences and provide indication as to
why some applications of fail whereas others succeed. Examples will be
presented to illustrate the taxonomy, including its use in developing and
planning two current VR projects in our laboratory. The first project is a VR
application for the training of Chemical Engineering students (and potentially
industrial operators) on the use of a physical pilot plant facility. The second
project involves the use of VR cinematography for enacting ethics scenarios
(and thus ethical awareness and development) pertinent to engineering work
situations.","['Jiri Motejlek', 'Esat Alpay']",2019-06-28T05:56:57Z,http://arxiv.org/abs/1906.12051v1
User Guidance for Interactive Camera Calibration,"For building a Augmented Reality (AR) pipeline, the most crucial step is the
camera calibration as overall quality heavily depends on it. In turn camera
calibration itself is influenced most by the choice of camera-to-pattern poses
- yet currently there is only little research on guiding the user to a specific
pose. We build upon our novel camera calibration framework that is capable to
generate calibration poses in real-time and present a user study evaluating
different visualization methods to guide the user to a target pose. Using the
presented method even novel users are capable to perform a precise camera
calibration in about 2 minutes.",['Pavel Rojtberg'],2019-07-09T11:59:54Z,http://arxiv.org/abs/1907.04104v1
Real-time Facial Surface Geometry from Monocular Video on Mobile GPUs,"We present an end-to-end neural network-based model for inferring an
approximate 3D mesh representation of a human face from single camera input for
AR applications. The relatively dense mesh model of 468 vertices is well-suited
for face-based AR effects. The proposed model demonstrates super-realtime
inference speed on mobile GPUs (100-1000+ FPS, depending on the device and
model variant) and a high prediction quality that is comparable to the variance
in manual annotations of the same image.","['Yury Kartynnik', 'Artsiom Ablavatski', 'Ivan Grishchenko', 'Matthias Grundmann']",2019-07-15T20:08:17Z,http://arxiv.org/abs/1907.06724v1
Leveraging Sensing at the Infrastructure for mmWave Communication,"Vehicle-to-everything (V2X) communication in the mmWave band is one way to
achieve high data-rates for applications like infotainment, cooperative
perception, and augmented reality assisted driving etc. MmWave communication
relies on large antennas arrays, and configuring these arrays poses high
training overhead. In this article, we motivate the use of infrastructure
mounted sensors (which will be part of future smart cities) for mmWave
communication. We provide numerical and measurement results to demonstrate that
information from these infrastructure sensors reduces the mmWave array
configuration overhead. Finally, we outline future research directions to help
materialize the use of infrastructure sensors for mmWave communication.","['Anum Ali', 'Nuria González-Prelcic', 'Robert W. Heath Jr.', 'Amitava Ghosh']",2019-11-22T00:35:46Z,http://arxiv.org/abs/1911.09796v1
Multi-View Matching Network for 6D Pose Estimation,"Applications that interact with the real world such as augmented reality or
robot manipulation require a good understanding of the location and pose of the
surrounding objects. In this paper, we present a new approach to estimate the 6
Degree of Freedom (DoF) or 6D pose of objects from a single RGB image. Our
approach can be paired with an object detection and segmentation method to
estimate, refine and track the pose of the objects by matching the input image
with rendered images.","['Daniel Mas Montserrat', 'Jianhang Chen', 'Qian Lin', 'Jan P. Allebach', 'Edward J. Delp']",2019-11-27T18:16:45Z,http://arxiv.org/abs/1911.12330v1
Pokémon Go: Impact on Yelp Restaurant Reviews,"Pok\'emon Go, the popular Augmented Reality based mobile application,
launched in July of 2016. The game's meteoric rise in usage since that time has
had an impact on not just the mobile gaming industry, but also the physical
activity of players, where they travel, where they spend their money, and
possibly how they interact with other social media applications. In this paper,
we studied the impact of Pok\'emon Go on Yelp reviews. For restaurants near
Pok\'eStops, we found a slight drop in the number of online reviews.","['Pavan Ravikanth Kondamudi', 'Bradley Protono', 'Hamed Alhoori']",2017-06-07T13:58:51Z,http://arxiv.org/abs/1706.02192v1
"Enabling Interactive Mobile Simulations Through Distributed Reduced
  Models","Currently, various hardware and software companies are developing augmented
reality devices, most prominently Microsoft with its Hololens. Besides gaming,
such devices can be used for serious pervasive applications, like interactive
mobile simulations to support engineers in the field. Interactive simulations
have high demands on resources, which the mobile device alone is unable to
satisfy. Therefore, we propose a framework to support mobile simulations by
distributing the computation between the mobile device and a remote server
based on the reduced basis method. Evaluations show that we can speed-up the
numerical computation by over 131 times while using 73 times less energy.","['Christoph Dibak', 'Bernard Haasdonk', 'Andreas Schmidt', 'Frank Dürr', 'Kurt Rothermel']",2018-02-14T16:53:36Z,http://arxiv.org/abs/1802.05206v1
RGB-Depth SLAM Review,"Simultaneous Localization and Mapping (SLAM) have made the real-time dense
reconstruction possible increasing the prospects of navigation, tracking, and
augmented reality problems. Some breakthroughs have been achieved in this
regard during past few decades and more remarkable works are still going on.
This paper presents an overview of SLAM approaches that have been developed
till now. Kinect Fusion algorithm, its variants, and further developed
approaches are discussed in detailed. The algorithms and approaches are
compared for their effectiveness in tracking and mapping based on Root Mean
Square error over online available datasets.","['Redhwan Jamiruddin', 'Ali Osman Sari', 'Jahanzaib Shabbir', 'Tarique Anwer']",2018-05-20T03:20:38Z,http://arxiv.org/abs/1805.07696v1
Soccer on Your Tabletop,"We present a system that transforms a monocular video of a soccer game into a
moving 3D reconstruction, in which the players and field can be rendered
interactively with a 3D viewer or through an Augmented Reality device. At the
heart of our paper is an approach to estimate the depth map of each player,
using a CNN that is trained on 3D player data extracted from soccer video
games. We compare with state of the art body pose and depth estimation
techniques, and show results on both synthetic ground truth benchmarks, and
real YouTube soccer footage.","['Konstantinos Rematas', 'Ira Kemelmacher-Shlizerman', 'Brian Curless', 'Steve Seitz']",2018-06-03T22:51:35Z,http://arxiv.org/abs/1806.00890v1
NodeSLAM: Neural Object Descriptors for Multi-View Shape Reconstruction,"The choice of scene representation is crucial in both the shape inference
algorithms it requires and the smart applications it enables. We present
efficient and optimisable multi-class learned object descriptors together with
a novel probabilistic and differential rendering engine, for principled full
object shape inference from one or more RGB-D images. Our framework allows for
accurate and robust 3D object reconstruction which enables multiple
applications including robot grasping and placing, augmented reality, and the
first object-level SLAM system capable of optimising object poses and shapes
jointly with camera trajectory.","['Edgar Sucar', 'Kentaro Wada', 'Andrew Davison']",2020-04-09T11:09:56Z,http://arxiv.org/abs/2004.04485v2
Benchmarking the Gerchberg-Saxton Algorithm,"Due to the proliferation of spatial light modulators, digital holography is
finding wide-spread use in fields from augmented reality to medical imaging to
additive manufacturing to lithography to optical tweezing to
telecommunications. There are numerous types of SLM available with a multitude
of algorithms for generating holograms. Each algorithm has limitations in terms
of convergence speed, power efficiency, accuracy and data storage requirement.
  Here, we consider probably the most common algorithm for computer generated
holography - Gerchberg-Saxton - and examine the trade-off in convergent
quality, performance and efficiency. In particular, we focus on measuring and
understanding the factors that control runtime and convergence.","['Peter J. Christopher', 'George S. D. Gordon', 'Timothy D. Wilkinson']",2020-05-18T12:09:41Z,http://arxiv.org/abs/2005.08623v1
MediaPipe Hands: On-device Real-time Hand Tracking,"We present a real-time on-device hand tracking pipeline that predicts hand
skeleton from single RGB camera for AR/VR applications. The pipeline consists
of two models: 1) a palm detector, 2) a hand landmark model. It's implemented
via MediaPipe, a framework for building cross-platform ML solutions. The
proposed model and pipeline architecture demonstrates real-time inference speed
on mobile GPUs and high prediction quality. MediaPipe Hands is open sourced at
https://mediapipe.dev.","['Fan Zhang', 'Valentin Bazarevsky', 'Andrey Vakunov', 'Andrei Tkachenka', 'George Sung', 'Chuo-Ling Chang', 'Matthias Grundmann']",2020-06-18T00:19:13Z,http://arxiv.org/abs/2006.10214v1
Attention Mesh: High-fidelity Face Mesh Prediction in Real-time,"We present Attention Mesh, a lightweight architecture for 3D face mesh
prediction that uses attention to semantically meaningful regions. Our neural
network is designed for real-time on-device inference and runs at over 50 FPS
on a Pixel 2 phone. Our solution enables applications like AR makeup, eye
tracking and AR puppeteering that rely on highly accurate landmarks for eye and
lips regions. Our main contribution is a unified network architecture that
achieves the same accuracy on facial landmarks as a multi-stage cascaded
approach, while being 30 percent faster.","['Ivan Grishchenko', 'Artsiom Ablavatski', 'Yury Kartynnik', 'Karthik Raveendran', 'Matthias Grundmann']",2020-06-19T05:07:38Z,http://arxiv.org/abs/2006.10962v1
"Parallel Oculomotor Plant Mathematical Model for Large Scale Eye
  Movement Simulation","The usage of eye tracking sensors is expected to grow in virtual (VR) and
augmented reality (AR) platforms. Provided that users of these platforms
consent to employing captured eye movement signals for authentication and
health assessment, it becomes important to estimate oculomotor plant and brain
function characteristics in real time. This paper shows a path toward that goal
by presenting a parallel processing architecture capable of estimating
oculomotor plant characteristics and comparing its performance to a
single-threaded implementation. Results show that the parallel implementation
improves the speed, accuracy, and throughput of oculomotor plant characteristic
estimation versus the original serial version for both large-scale and
real-time simulation.","['Alex Karpov', 'Jacob Liberman', 'Dillon Lohr', 'Oleg Komogortsev']",2020-07-20T04:39:57Z,http://arxiv.org/abs/2007.09884v1
"PanoNet: Real-time Panoptic Segmentation through Position-Sensitive
  Feature Embedding","We propose a simple, fast, and flexible framework to generate simultaneously
semantic and instance masks for panoptic segmentation. Our method, called
PanoNet, incorporates a clean and natural structure design that tackles the
problem purely as a segmentation task without the time-consuming detection
process. We also introduce position-sensitive embedding for instance grouping
by accounting for both object's appearance and its spatial location. Overall,
PanoNet yields high panoptic quality results of high-resolution Cityscapes
images in real-time, significantly faster than all other methods with
comparable performance. Our approach well satisfies the practical speed and
memory requirement for many applications like autonomous driving and augmented
reality.","['Xia Chen', 'Jianren Wang', 'Martial Hebert']",2020-08-01T06:58:35Z,http://arxiv.org/abs/2008.00192v1
Adapting the Human: Leveraging Wearable Technology in HRI,"Adhering to current HRI paradigms, all of the sensors, visualisation and
legibility of actions and motions are borne by the robot or its working cell.
This necessarily makes robots more complex or confines them into specialised,
structured environments. We propose leveraging the state of the art of wearable
technologies, such as augmented reality head mounted displays, smart watches,
sensor tags and radio-frequency ranging, to ""adapt"" the human and reduce the
requirements and complexity of robots.","['David Puljiz', 'Björn Hein']",2020-12-10T18:01:46Z,http://arxiv.org/abs/2012.05854v1
"A New Efficient Numbering System : Application to Numbers Generation and
  Visual Markers Design","This short paper introduces a recently patented line based numbering system.
The last allows a best concordance with decimal digits values, and open up new
opportunities, which are not possible with the classical decimal numeration
system. Proposed OILU symbolic allows generating a new type of number series,
based on multi facets numbers splitting process. On the other hand, this new
symbolic is used in the development of new visual markers, highly required in
augmented reality and UAV's navigation applications.","['Messaoud Mostefai', 'Salah Khodja', 'Youssef Chahir']",2021-03-22T11:06:54Z,http://arxiv.org/abs/2103.11727v2
"A Fast Location Algorithm for Very Sparse Point Clouds Based on Object
  Detection","Limited by the performance factor, it is arduous to recognize target object
and locate it in Augmented Reality (AR) scenes on low-end mobile devices,
especially which using monocular cameras. In this paper, we proposed an
algorithm which can quickly locate the target object through image object
detection in the circumstances of having very sparse feature points. We
introduce YOLOv3-Tiny to our algorithm as the object detection module to filter
the possible points and using Principal Component Analysis (PCA) to determine
the location. We conduct the experiment in a manually designed scene by holding
a smartphone and the results represent high positioning speed and accuracy of
our method.",['Shiyu Fan'],2021-10-21T05:17:48Z,http://arxiv.org/abs/2110.10901v1
A Structure Feature Algorithm for Multi-modal Forearm Registration,"Augmented reality technology based on image registration is becoming
increasingly popular for the convenience of pre-surgery preparation and medical
education. This paper focuses on the registration of forearm images and digital
anatomical models. Due to the difference in texture features of forearm
multi-modal images, this paper proposes a forearm feature representation curve
(FFRC) based on structure compliant multi-modal image registration framework
(FAM) for the forearm.","['Jiaxin Li', 'Yan Ding', 'Weizhong Zhang', 'Yifan Zhao', 'Lingxi Guo', 'Zhe Yang']",2021-11-10T01:58:39Z,http://arxiv.org/abs/2111.05485v1
"Comparing Controller With the Hand Gestures Pinch and Grab for Picking
  Up and Placing Virtual Objects","Grabbing virtual objects is one of the essential tasks for Augmented,
Virtual, and Mixed Reality applications. Modern applications usually use a
simple pinch gesture for grabbing and moving objects. However, picking up
objects by pinching has disadvantages. It can be an unnatural gesture to pick
up objects and prevents the implementation of other gestures which would be
performed with thumb and index. Therefore it is not the optimal choice for many
applications. In this work, different implementations for grabbing and placing
virtual objects are proposed and compared. Performance and accuracy of the
proposed techniques are measured and compared.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2022-02-22T15:12:06Z,http://arxiv.org/abs/2202.10964v1
Exploring Human-robot Interaction by Simulating Robots,"As collaborative robots enter industrial shop floors, logistics, and
manufacturing, rapid and flexible evaluation of human-machine interaction has
become more important. The availability of consumer headsets for virtual and
augmented realities has lowered the barrier of entry for virtual environments.
In this paper, we explore the different aspects of using such environments for
simulating robots in user studies and present the first findings from our own
research work. Finally, we recommend directions for applying and using
simulation in human-robot interaction.","['Khaled Kassem', 'Florian Michahelles']",2022-09-20T15:20:09Z,http://arxiv.org/abs/2209.09787v1
Spatial Name System,"The development of emerging classes of hardware such as Internet of Thing
devices and Augmented Reality headsets has outpaced the development of Internet
infrastructure. We identify problems with latency, security and privacy in the
global hierarchical distributed Domain Name System. To remedy this, we propose
the Spatial Name System, an alternative network architecture that relies on the
innate physicality of this paradigm. Utilizing a device's pre-existing unique
identifier, its location, allows us to identify devices locally based on their
physical presence. A naming system tailored to the physical world for
ubiquitous computing can enable reliable, low latency, secure and private
communication.",['Ryan Thomas Gibb'],2022-10-10T22:37:37Z,http://arxiv.org/abs/2210.05036v2
"Review on 6D Object Pose Estimation with the focus on Indoor Scene
  Understanding","6D object pose estimation problem has been extensively studied in the field
of Computer Vision and Robotics. It has wide range of applications such as
robot manipulation, augmented reality, and 3D scene understanding. With the
advent of Deep Learning, many breakthroughs have been made; however, approaches
continue to struggle when they encounter unseen instances, new categories, or
real-world challenges such as cluttered backgrounds and occlusions. In this
study, we will explore the available methods based on input modality, problem
formulation, and whether it is a category-level or instance-level approach. As
a part of our discussion, we will focus on how 6D object pose estimation can be
used for understanding 3D scenes.","['Negar Nejatishahidin', 'Pooya Fayyazsanavi']",2022-12-04T20:45:46Z,http://arxiv.org/abs/2212.01920v1
Avatar-centred AR Collaborative Mobile Interaction,"Interaction with the physical environment and different users is essential to
foster a collaborative experience. For this, we propose an interaction based on
a central point represented by an Augmented Reality marker in which several
users can capture the attention and interact with a virtual avatar. The
interface provides different game modes, with various challenges, supporting a
collaborative mobile interaction. The system fosters various group interactions
with a virtual avatar and enables various tasks with playful and didactic
components.","['Bianca Marques', 'Rui Nóbrega', 'Carmen Morgado']",2023-01-06T14:38:35Z,http://arxiv.org/abs/2301.02527v1
"Teaching Color Science to EECS Students Using Interactive Tutorials:
  Tools and Lessons","Teaching color science to Electrical Engineering and Computer Science (EECS)
students is critical to preparing them for advanced topics such as graphics,
visualization, imaging, Augmented/Virtual Reality. Color historically receive
little attention in EECS curriculum; students find it difficult to grasp basic
concepts. This is because today's pedagogical approaches are nonintuitive and
lack rigor for teaching color science. We develop a set of interactive
tutorials that teach color science to EECS students. Each tutorial is backed up
by a mathematically rigorous narrative, but is presented in a form that invites
students to participate in developing each concept on their own through
visualization tools. This paper describes the tutorial series we developed and
discusses the design decisions we made.",['Yuhao Zhu'],2023-01-24T02:39:32Z,http://arxiv.org/abs/2301.09788v1
"Location-based AR for Social Justice: Case Studies, Lessons, and Open
  Challenges","Dear Visitor and Charleston Reconstructed were location-based augmented
reality (AR) experiences created between 2018 and 2020 dealing with two
controversial monument sites in the US. The projects were motivated by the
ability of AR to 1) link layers of context to physical sites in ways that are
otherwise difficult or impossible and 2) to visualize changes to physical
spaces, potentially inspiring changes to the spaces themselves. We discuss the
projects' motivations, designs, and deployments. We reflect on how physical
changes to the projects' respective sites radically altered their outcomes, and
we describe lessons for future work in location-based AR, particularly for
projects in contested spaces.","['Hope Schroeder', 'Rob Tokanel', 'Kyle Qian', 'Khoi Le']",2023-02-04T01:21:12Z,http://arxiv.org/abs/2302.02050v1
Freehand 2D Ultrasound Probe Calibration for Image Fusion with 3D MRI/CT,"The aim of this work is to implement a simple freehand ultrasound (US) probe
calibration technique. This will enable us to visualize US image data during
surgical procedures using augmented reality. The performance of the system was
evaluated with different experiments using two different pose estimation
techniques. A near-millimeter accuracy can be achieved with the proposed
approach. The developed system is cost-effective, simple and rapid with low
calibration error","['Yogesh Langhe', 'Katrin Skerl', 'Adrien Bartoli']",2023-03-14T08:55:24Z,http://arxiv.org/abs/2303.07714v1
"Prospects for the implementation of an affordable VR/AR-content
  management tool for Learning Management Systems","The article discusses the prospects for introducing into educational practice
the designer of electronic training courses based on virtual and augmented
reality technologies for LMS Moodle. The requirements for the functions,
interface, appearance of the module-designer being developed, the formation of
VR/AR-content in terms of its use by unprepared users, such as teachers and
develop-ers of training courses, are formulated.","['Anastasia Grigoreva', 'Stanislav Grigorev']",2023-03-29T14:19:48Z,http://arxiv.org/abs/2303.16723v1
Designing Situated Dashboards: Challenges and Opportunities,"Situated Visualization is an emerging field that unites several areas -
visualization, augmented reality, human-computer interaction, and
internet-of-things, to support human data activities within the ubiquitous
world. Likewise, dashboards are broadly used to simplify complex data through
multiple views. However, dashboards are only adapted for desktop settings, and
requires visual strategies to support situatedness. We propose the concept of
AR-based situated dashboards and present design considerations and challenges
developed over interviews with experts. These challenges aim to propose
directions and opportunities for facilitating the effective designing and
authoring of situated dashboards.","['Anika Sayara', 'Benjamin Lee', 'Carlos Quijano-Chavez', 'Michael Sedlmair']",2023-09-06T12:24:55Z,http://arxiv.org/abs/2309.02945v1
Targeted Adversarial Attacks on Generalizable Neural Radiance Fields,"Neural Radiance Fields (NeRFs) have recently emerged as a powerful tool for
3D scene representation and rendering. These data-driven models can learn to
synthesize high-quality images from sparse 2D observations, enabling realistic
and interactive scene reconstructions. However, the growing usage of NeRFs in
critical applications such as augmented reality, robotics, and virtual
environments could be threatened by adversarial attacks.
  In this paper we present how generalizable NeRFs can be attacked by both
low-intensity adversarial attacks and adversarial patches, where the later
could be robust enough to be used in real world applications. We also
demonstrate targeted attacks, where a specific, predefined output scene is
generated by these attack with success.","['Andras Horvath', 'Csaba M. Jozsa']",2023-10-05T14:59:18Z,http://arxiv.org/abs/2310.03578v1
Particles in a pocket,"Communicating science through mobile smartphone and tablet applications is
one of the most efficient ways to reach general public of diverse background
and age coverage. The Higgsy project was created in 2022 to celebrate the 10th
anniversary of the discovery of the Higgs boson at CERN. This project
introduces a mobile game to search for the Higgs boson production in a generic
particle detector. The MatterBricks is an augmented-reality project that was
created for a major national event in Belgium, held in 2023. The main features
of the two mobile applications and further prospects for reaching general
public through mobile application development process are discussed.",['Kirill Skovpen'],2023-10-10T11:35:36Z,http://arxiv.org/abs/2310.17656v1
"Retrospective Cost Attitude Filtering with Noisy Measurements and
  Unknown Gyro Bias","Attitude filtering is a critical technology with applications in diverse
domains such as aerospace engineering, robotics, computer vision, and augmented
reality. Although attitude filtering is a particular case of the state
estimation problem, attitude filtering is uniquely challenging due to the
special geometric structure of the attitude parameterization. This paper
presents a novel data-driven attitude filter, called the retrospective cost
attitude filter (RCAF), for the SO(3) attitude representation. Like the
multiplicative extended Kalman filter, RCAF uses a multiplicative correction
signal, but instead of computing correction gains using Jacobians, RCAF
computes the corrective signal using retrospective cost optimization and
measured data. The RCAF filter is validated numerically in a scenario with
noisy attitude measurements and noisy and biased rate-gyro measurements.","['Parham Oveissi', 'Ankit Goel']",2024-01-23T21:14:06Z,http://arxiv.org/abs/2401.13092v1
New Efficient Visual OILU Markers,"Basic patterns are the source of a wide range of more or less complex
geometric structures. We will exploit such patterns to develop new efficient
visual markers. Besides being projective invariants, the proposed markers allow
producing rich panel of unique identifiers, highly required for
resource-intensive navigation and augmented reality applications. The spiral
topology of our markers permits the validation of an accurate identification
scheme, which is based on level set methods. The robustness of the markers
against acquisition and geometric distortions is validated by extensive
experimental tests.","['Youssef Chahir', 'Messaoud Mostefai', 'Hamza Saida']",2024-04-12T13:55:05Z,http://arxiv.org/abs/2404.08477v1
"Dynamic Ego-Velocity estimation Using Moving mmWave Radar: A Phase-Based
  Approach","Precise ego-motion measurement is crucial for various applications, including
robotics, augmented reality, and autonomous navigation. In this poster, we
propose mmPhase, an odometry framework based on single-chip millimetre-wave
(mmWave) radar for robust ego-motion estimation in mobile platforms without
requiring additional modalities like the visual, wheel, or inertial odometry.
mmPhase leverages a phase-based velocity estimation approach to overcome the
limitations of conventional doppler resolution. For real-world evaluations of
mmPhase we have developed an ego-vehicle prototype. Compared to the
state-of-the-art baselines, mmPhase shows superior performance in ego-velocity
estimation.","['Argha Sen', 'Soham Chakraborty', 'Soham Tripathy', 'Sandip Chakraborty']",2024-04-15T11:46:06Z,http://arxiv.org/abs/2404.09691v1
"AR for Sexual Violence: Maintaining Ethical Balance While Enhancing
  Empathy","This study showcases an augmented reality (AR) experience designed to promote
gender justice and increase awareness of sexual violence in Taiwan. By
leveraging AR, this project overcomes the limitations of offline exhibitions on
social issues by motivating the public to participate and enhancing their
willingness to delve into the topic. The discussion explores how direct
exposure to sexual violence can induce negative emotions and secondary trauma
among users. It also suggests strategies for using AR to alleviate such issues,
particularly by avoiding simulations of actual incidents.",['Chunwei Lin'],2024-04-17T12:15:02Z,http://arxiv.org/abs/2404.11305v1
DinAR: Augmenting Reality for Sustainable Dining,"Sustainable food is among the many challenges associated with climate change.
The resources required to grow or gather the food and the distance it travels
to reach the consumer are two key factors of an ingredient's sustainability.
Food that is grown locally and is currently ""in-season"" will have a lower
carbon footprint, but when dining out these details unfortunately may not
affect one's ordering preferences. We introduce DinAR as an immersive
experience to make this information more accessible and to encourage better
dining choices through friendly competition with a leaderboard of
sustainability scores. Our study measures the effectiveness of immersive AR
experiences on impacting consumer preferences towards sustainability.","['MJ Johns', 'Eunsol Sol Choi', 'Derusha Baskaran']",2024-04-20T04:56:29Z,http://arxiv.org/abs/2404.13272v1
"Toward Improving Binary Program Comprehension via Embodied Immersion: A
  Survey","Binary program comprehension is critical for many use cases but is difficult,
suffering from compounded uncertainty and lack of full automation. We seek
methods to improve the effectiveness of the human-machine joint cognitive
system performing binary PC. We survey three research areas to perform an
indirect cognitive task analysis: cognitive models of the PC process, related
elements of cognitive theory, and applicable affordances of virtual reality.
Based on common elements in these areas, we identify three overarching themes:
enhancing abductive iteration, augmenting working memory, and supporting
information organization. These themes spotlight several affordances of VR to
exploit in future studies of immersive tools for binary PC.","['Dennis Brown', 'Emily Mulder', 'Samuel Mulder']",2024-04-25T21:19:20Z,http://arxiv.org/abs/2404.17051v1
"Gaze as a Supplementary Modality for Interacting with Ambient
  Intelligence Environments","We present our current research on the implementation of gaze as an efficient
and usable pointing modality supplementary to speech, for interacting with
augmented objects in our daily environment or large displays, especially
immersive virtual reality environments, such as reality centres and caves. We
are also addressing issues relating to the use of gaze as the main interaction
input modality. We have designed and developed two operational user interfaces:
one for providing motor-disabled users with easy gaze-based access to map
applications and graphical software; the other for iteratively testing and
improving the usability of gaze-contingent displays.","['Daniel Gepner', 'Jérôme Simonin', 'Noëlle Carbonell']",2007-08-26T18:53:41Z,http://arxiv.org/abs/0708.3505v1
"Towards Interconnected Virtual Reality: Opportunities, Challenges and
  Enablers","Just recently, the concept of augmented and virtual reality (AR/VR) over
wireless has taken the entire 5G ecosystem by storm spurring an unprecedented
interest from both academia, industry and others. Yet, the success of an
immersive VR experience hinges on solving a plethora of grand challenges
cutting across multiple disciplines. This article underscores the importance of
VR technology as a disruptive use case of 5G (and beyond) harnessing the latest
development of storage/memory, fog/edge computing, computer vision, artificial
intelligence and others. In particular, the main requirements of wireless
interconnected VR are described followed by a selection of key enablers, then,
research avenues and their underlying grand challenges are presented.
Furthermore, we examine three VR case studies and provide numerical results
under various storage, computing and network configurations. Finally, this
article exposes the limitations of current networks and makes the case for more
theory, and innovations to spearhead VR for the masses.","['Ejder Baştuğ', 'Mehdi Bennis', 'Muriel Médard', 'Mérouane Debbah']",2016-11-16T16:50:57Z,http://arxiv.org/abs/1611.05356v2
"Dissecting the End-to-end Latency of Interactive Mobile Video
  Applications","In this paper we measure the step-wise latency in the pipeline of three kinds
of interactive mobile video applications that are rapidly gaining popularity,
namely Remote Graphics Rendering (RGR) of which we focus on mobile cloud
gaming, Mobile Augmented Reality (MAR), and Mobile Virtual Reality (MVR). The
applications differ from each other by the way in which the user interacts with
the application, i.e., video I/O and user controls, but they all share in
common the fact that their user experience is highly sensitive to end-to-end
latency. Long latency between a user control event and display update renders
the application unusable. Hence, understanding the nature and origins of
latency of these applications is of paramount importance. We show through
extensive measurements that control input and display buffering have a
substantial effect on the overall delay. Our results shed light on the latency
bottlenecks and the maturity of technology for seamless user experience with
these applications.","['Teemu Kämäräinen', 'Matti Siekkinen', 'Antti Ylä-Jääski', 'Wenxiao Zhang', 'Pan Hui']",2016-11-25T17:11:57Z,http://arxiv.org/abs/1611.08520v1
Embedded Systems Architecture for SLAM Applications,"In recent years, we have observed a clear trend in the rapid rise of
autonomous vehicles, robotics, virtual reality, and augmented reality. The core
technology enabling these applications, Simultaneous Localization And Mapping
(SLAM), imposes two main challenges: first, these workloads are computationally
intensive and they often have real-time requirements; second, these workloads
run on battery-powered mobile devices with limited energy budget. In short, the
essence of these challenges is that performance should be improved while
simultaneously reducing energy consumption, two rather contradicting goals by
conventional wisdom. In this paper, we take a close look at state-of-the-art
Simultaneous Localization And Mapping (SLAM) workloads, especially how these
workloads behave on mobile devices. Based on the results, we propose a mobile
architecture to improve SLAM performance on mobile devices.","['Jie Tang', 'Shaoshan Liu', 'Jean-Luc Gaudiot']",2017-02-04T14:37:38Z,http://arxiv.org/abs/1702.01295v1
BodyDigitizer: An Open Source Photogrammetry-based 3D Body Scanner,"With the rising popularity of Augmented and Virtual Reality, there is a need
for representing humans as virtual avatars in various application domains
ranging from remote telepresence, games to medical applications. Besides
explicitly modelling 3D avatars, sensing approaches that create person-specific
avatars are becoming popular. However, affordable solutions typically suffer
from a low visual quality and professional solution are often too expensive to
be deployed in nonprofit projects.
  We present an open-source project, BodyDigitizer, which aims at providing
both build instructions and configuration software for a high-resolution
photogrammetry-based 3D body scanner. Our system encompasses up to 96 Rasperry
PI cameras, active LED lighting, a sturdy frame construction and open-source
configuration software. %We demonstrate the applicability of the body scanner
in a nonprofit Mixed Reality health project. The detailed build instruction and
software are available at http://www.bodydigitizer.org.","['Travis Gesslein', 'Daniel Scherer', 'Jens Grubert']",2017-10-03T20:10:10Z,http://arxiv.org/abs/1710.01370v2
"Air Mounted Eyepiece: Design Methods for Aerial Optical Functions of
  Near-Eye and See-Through Display using Transmissive Mirror Device","We propose a novel method to implement an optical see-through head mounted
display which renders real aerial images with a wide viewing angle, called an
Air Mounted Eyepiece (AME). To achieve the AMD design, we employ an
off-the-shelf head mounted display and Transmissive Mirror Device (TMD) which
is usually used in aerial real imaging systems. In the proposed method, we
replicate the function of the head mounted display (HMD) itself, which is used
in the air by using the TMD and presenting a real image of eyepiece in front of
the eye. Moreover, it can realize a wide viewing angle 3D display by placing a
virtual lens in front of the eye without wearing an HMD. In addition to
enhancing the experience of mixed reality and augmented reality, our proposed
method can be used as a 3D imaging method for use in other applications such as
in automobiles and desktop work. We aim to contribute to the field of
human-computer interaction and the research on eyepiece interfaces by
discussing the advantages and the limitations of this near-eye optical system.","['Yoichi Ochiai', 'Kazuki Otao', 'Hiroyuki Osone']",2017-10-11T03:08:02Z,http://arxiv.org/abs/1710.03889v1
"Visible Light Communication for Next Generation Untethered Virtual
  Reality Systems","Virtual and augmented reality (VR/AR) systems are emerging technologies
requiring data rates of multiple Gbps. Existing high quality VR headsets
require connections through HDMI cables to a computer rendering rich graphic
contents to meet the extremely high data transfer rate requirement. Such a
cable connection limits the VR user's mobility and interferes with the VR
experience. Current wireless technologies such as WiFi cannot support the
multi-Gbps graphics data transfer. Instead, we propose to use visible light
communication (VLC) for establishing high speed wireless links between a
rendering computer and a VR headset. But, VLC transceivers are highly
directional with narrow beams and require constant maintenance of line-of-sight
(LOS) alignment between the transmitter and the receiver. Thus, we present a
novel multi-detector hemispherical VR headset design to tackle the beam
misalignment problem caused by the VR user's random head orientation. We
provide detailed analysis on how the number of detectors on the headset can be
minimized while maintaining the required beam alignment and providing high
quality VR experience.","['Mahmudur Khan', 'Jacob Chakareski']",2019-04-07T20:24:55Z,http://arxiv.org/abs/1904.03735v1
Mid-Air Drawing of Curves on 3D Surfaces in Virtual Reality,"Complex 3D curves can be created by directly drawing mid-air in immersive
environments (Augmented and Virtual Realities). Drawing mid-air strokes
precisely on the surface of a 3D virtual object, however, is difficult;
necessitating a projection of the mid-air stroke onto the user ""intended""
surface curve. We present the first detailed investigation of the fundamental
problem of 3D stroke projection in VR. An assessment of the design requirements
of real-time drawing of curves on 3D objects in VR is followed by the
definition and classification of multiple techniques for 3D stroke projection.
We analyze the advantages and shortcomings of these approaches both
theoretically and via practical pilot testing. We then formally evaluate the
two most promising techniques spraycan and mimicry with 20 users in VR. The
study shows a strong qualitative and quantitative user preference for our novel
stroke mimicry projection algorithm. We further illustrate the effectiveness
and utility of stroke mimicry, to draw complex 3D curves on surfaces for
various artistic and functional design applications.","['Rahul Arora', 'Karan Singh']",2020-09-18T19:01:08Z,http://arxiv.org/abs/2009.09029v2
"Development of a wheelchair simulator for children with multiple
  disabilities","Virtual reality allows to create situations which can be experimented under
the control of the user, without risks, in a very flexible way. This allows to
develop skills and to have confidence to work in real conditions with real
equipment. VR is then widely used as a training and learning tool. More
recently, VR has also showed its potential in rehabilitation and therapy fields
because it provides users with the ability of repeat their actions several
times and to progress at their own pace. In this communication, we present our
work in the development of a wheelchair simulator designed to allow children
with multiple disabilities to familiarize themselves with the wheelchair.",['Nancy Rodriguez'],2016-01-18T09:23:46Z,http://arxiv.org/abs/1601.04436v1
Image Based Camera Localization: an Overview,"Recently, virtual reality, augmented reality, robotics, autonomous driving et
al attract much attention of both academic and industrial community, in which
image based camera localization is a key task. However, there has not been a
complete review on image-based camera localization. It is urgent to map this
topic to help people enter the field quickly. In this paper, an overview of
image based camera localization is presented. A new and complete kind of
classifications for image based camera localization is provided and the related
techniques are introduced. Trends for the future development are also
discussed. It will be useful to not only researchers but also engineers and
other people interested.","['Yihong Wu', 'Fulin Tang', 'Heping Li']",2016-10-12T10:19:36Z,http://arxiv.org/abs/1610.03660v4
"MIS-SLAM: Real-time Large Scale Dense Deformable SLAM System in Minimal
  Invasive Surgery Based on Heterogeneous Computing","Real-time simultaneously localization and dense mapping is very helpful for
providing Virtual Reality and Augmented Reality for surgeons or even surgical
robots. In this paper, we propose MIS-SLAM: a complete real-time large scale
dense deformable SLAM system with stereoscope in Minimal Invasive Surgery based
on heterogeneous computing by making full use of CPU and GPU. Idled CPU is used
to perform ORB- SLAM for providing robust global pose. Strategies are taken to
integrate modules from CPU and GPU. We solved the key problem raised in
previous work, that is, fast movement of scope and blurry images make the scope
tracking fail. Benefiting from improved localization, MIS-SLAM can achieve
large scale scope localizing and dense mapping in real-time. It transforms and
deforms current model and incrementally fuses new observation while keeping
vivid texture. In-vivo experiments conducted on publicly available datasets
presented in the form of videos demonstrate the feasibility and practicality of
MIS-SLAM for potential clinical purpose.","['Jingwei Song', 'Jun Wang', 'Liang Zhao', 'Shoudong Huang', 'Gamini Dissanayake']",2018-03-06T04:19:24Z,http://arxiv.org/abs/1803.02009v2
Peeking Behind Objects: Layered Depth Prediction from a Single Image,"While conventional depth estimation can infer the geometry of a scene from a
single RGB image, it fails to estimate scene regions that are occluded by
foreground objects. This limits the use of depth prediction in augmented and
virtual reality applications, that aim at scene exploration by synthesizing the
scene from a different vantage point, or at diminished reality. To address this
issue, we shift the focus from conventional depth map prediction to the
regression of a specific data representation called Layered Depth Image (LDI),
which contains information about the occluded regions in the reference frame
and can fill in occlusion gaps in case of small view changes. We propose a
novel approach based on Convolutional Neural Networks (CNNs) to jointly predict
depth maps and foreground separation masks used to condition Generative
Adversarial Networks (GANs) for hallucinating plausible color and depths in the
initially occluded areas. We demonstrate the effectiveness of our approach for
novel scene view synthesis from a single image.","['Helisa Dhamo', 'Keisuke Tateno', 'Iro Laina', 'Nassir Navab', 'Federico Tombari']",2018-07-23T18:23:53Z,http://arxiv.org/abs/1807.08776v1
"Quality 4.0: Let's Get Digital - The many ways the fourth industrial
  revolution is reshaping the way we think about quality","The technology landscape is richer and more promising than ever before. In
many ways, cloud computing, big data, virtual reality (VR), augmented reality
(AR), blockchain, additive manufacturing, artificial intelligence (AI), machine
learning (ML), Internet Protocol Version 6 (IPv6), cyber-physical systems and
the Internet of Things (IoT) all represent new frontiers. These technologies
can help improve product and service quality, and organizational performance.
In many regions, the internet is now as ubiquitous as electricity. Components
are relatively cheap. A robust ecosystem of open-source software libraries
means that engineers can solve problems 100 times faster than just two decades
ago. This digital transformation is leading us toward connected intelligent
automation: smart, hyperconnected agents deployed in environments where humans
and machines cooperate, and leverage data, to achieve shared goals. This is not
the worlds first industrial revolution. In fact, it is its fourth, and the
disruptive changes it will bring suggest we will need a fresh perspective on
quality to adapt to it.",['Nicole M. Radziwill'],2018-10-17T23:06:06Z,http://arxiv.org/abs/1810.07829v1
Very Long Term Field of View Prediction for 360-degree Video Streaming,"360-degree videos have gained increasing popularity in recent years with the
developments and advances in Virtual Reality (VR) and Augmented Reality (AR)
technologies. In such applications, a user only watches a video scene within a
field of view (FoV) centered in a certain direction. Predicting the future FoV
in a long time horizon (more than seconds ahead) can help save bandwidth
resources in on-demand video streaming while minimizing video freezing in
networks with significant bandwidth variations. In this work, we treat the FoV
prediction as a sequence learning problem, and propose to predict the target
user's future FoV not only based on the user's own past FoV center trajectory
but also other users' future FoV locations. We propose multiple prediction
models based on two different FoV representations: one using FoV center
trajectories and another using equirectangular heatmaps that represent the FoV
center distributions. Extensive evaluations with two public datasets
demonstrate that the proposed models can significantly outperform benchmark
models, and other users' FoVs are very helpful for improving long-term
predictions.","['Chenge Li', 'Weixi Zhang', 'Yong Liu', 'Yao Wang']",2019-02-04T19:43:40Z,http://arxiv.org/abs/1902.01439v1
"Hand-Gesture-Recognition Based Text Input Method for AR/VR Wearable
  Devices","Static and dynamic hand movements are basic way for human-machine
interactions. To recognize and classify these movements, first these movements
are captured by the cameras mounted on the augmented reality (AR) or virtual
reality (VR) wearable devices. The hand is segmented using segmentation method
and its gestures are passed to hand gesture recognition algorithm, which
depends on depth-wise separable convolutional neural network for training,
testing and finally running smoothly on mobile AR/VR devices, while maintaining
the accuracy and balancing the load. A number of gestures are processed for
identification of right gesture and to classify the gesture and ignore the all
intermittent gestures. With proposed method, a user can write letters and
numbers in air by just moving his/her hand in air. Gesture based operations are
performed, and trajectory of hand is recorded as handwritten text. Finally,
that handwritten text is processed for the text recognition.","['Nizamuddin Maitlo', 'Yanbo Wang', 'Chao Ping Chen', 'Lantian Mi', 'Wenbo Zhang']",2019-07-29T02:53:21Z,http://arxiv.org/abs/1907.12188v2
PlaneRCNN: 3D Plane Detection and Reconstruction from a Single Image,"This paper proposes a deep neural architecture, PlaneRCNN, that detects and
reconstructs piecewise planar surfaces from a single RGB image. PlaneRCNN
employs a variant of Mask R-CNN to detect planes with their plane parameters
and segmentation masks. PlaneRCNN then jointly refines all the segmentation
masks with a novel loss enforcing the consistency with a nearby view during
training. The paper also presents a new benchmark with more fine-grained plane
segmentations in the ground-truth, in which, PlaneRCNN outperforms existing
state-of-the-art methods with significant margins in the plane detection,
segmentation, and reconstruction metrics. PlaneRCNN makes an important step
towards robust plane extraction, which would have an immediate impact on a wide
range of applications including Robotics, Augmented Reality, and Virtual
Reality.","['Chen Liu', 'Kihwan Kim', 'Jinwei Gu', 'Yasutaka Furukawa', 'Jan Kautz']",2018-12-10T20:35:55Z,http://arxiv.org/abs/1812.04072v2
stdgpu: Efficient STL-like Data Structures on the GPU,"Tremendous advances in parallel computing and graphics hardware opened up
several novel real-time GPU applications in the fields of computer vision,
computer graphics as well as augmented reality (AR) and virtual reality (VR).
Although these applications built upon established open-source frameworks that
provide highly optimized algorithms, they often come with custom self-written
data structures to manage the underlying data. In this work, we present stdgpu,
an open-source library which defines several generic GPU data structures for
fast and reliable data management. Rather than abandoning previous established
frameworks, our library aims to extend them, therefore bridging the gap between
CPU and GPU computing. This way, it provides clean and familiar interfaces and
integrates seamlessly into new as well as existing projects. We hope to foster
further developments towards unified CPU and GPU computing and welcome
contributions from the community.",['Patrick Stotko'],2019-08-16T11:37:42Z,http://arxiv.org/abs/1908.05936v1
"Optimization and Manipulation of Contextual Mutual Spaces for Multi-User
  Virtual and Augmented Reality Interaction","Spatial computing experiences are physically constrained by the geometry and
semantics of the local user environment. This limitation is elevated in remote
multi-user interaction scenarios, where finding a common virtual ground
physically accessible for all participants becomes challenging. Locating a
common accessible virtual ground is difficult for the users themselves,
particularly if they are not aware of the spatial properties of other
participants. In this paper, we introduce a framework to generate an optimal
mutual virtual space for a multi-user interaction setting where remote users'
room spaces can have different layout and sizes. The framework further
recommends movement of surrounding furniture objects that expand the size of
the mutual space with minimal physical effort. Finally, we demonstrate the
performance of our solution on real-world datasets and also a real HoloLens
application. Results show the proposed algorithm can effectively discover
optimal shareable space for multi-user virtual interaction and hence facilitate
remote spatial computing communication in various collaborative workflows.","['Mohammad Keshavarzi', 'Allen Y. Yang', 'Woojin Ko', 'Luisa Caldas']",2019-10-14T09:10:54Z,http://arxiv.org/abs/1910.05998v2
"Occurence of A Cyber Security Eco-System: A Nature Oriented Project and
  Evaluation of An Indirect Social Experiment","Because of todays technological developments and the influence of digital
systems into every aspect of our lives, importance of cyber security improves
more and more day-by-day. Projects, educational processes and seminars realized
for this aim create and improve awareness among individuals and provide useful
tools for growing equipped generations. The aim of this study is to focus on a
cyber security eco-system, which was self-occurred within the interactive
educational environment designed under the scope of TUBITAK 4004 Nature
Education and Science Schools Projects (with the name of A Cyber Security
Adventure) with the use of important technologies such as virtual reality,
augmented reality, and artificial intelligence. The eco-system occurred within
the interactive educational process where high school students took place
caused both students and the project team to experience an indirect social
experiment environment. In this sense, it is thought that the findings and
comments presented in the study will give important ideas to everyone involved
in cyber security education, life-long learning processes, and the technology
use in software oriented educational tools.",['Utku Kose'],2019-09-29T22:04:19Z,http://arxiv.org/abs/1910.07083v1
"Computation Resource Allocation for Heterogeneous Time-Critical IoT
  Services in MEC","Mobile edge computing (MEC) is one of the promising solutions to process
computational-intensive tasks within short latency for emerging
Internet-of-Things (IoT) use cases, e.g., virtual reality (VR), augmented
reality (AR), autonomous vehicle. Due to the coexistence of heterogeneous
services in MEC system, the task arrival interval and required execution time
can vary depending on services. It is challenging to schedule computation
resource for the services with stochastic arrivals and runtime at an edge
server (ES). In this paper, we propose a flexible computation offloading
framework among users and ESs. Based on the framework, we propose a
Lyapunov-based algorithm to dynamically allocate computation resource for
heterogeneous time-critical services at the ES. The proposed algorithm
minimizes the average timeout probability without any prior knowledge on task
arrival process and required runtime. The numerical results show that, compared
with the standard queuing models used at ES, the proposed algorithm achieves at
least 35% reduction of the timeout probability, and approximated utilization
efficiency of computation resource to non-cause queuing model under various
scenarios.","['Jianhui Liu', 'Qi Zhang']",2020-02-12T09:00:55Z,http://arxiv.org/abs/2002.04851v1
C-D Ratio in multi-display environments,"Research in user interaction with mixed reality environments using multiple
displays has become increasingly relevant with the prevalence of mobile devices
in everyday life and increased commoditization of large display area
technologies using projectors or large displays. Previous work often combines
touch-based input with other approaches, such as gesture-based input, to expand
the possible interaction space or deal with limitations of other
two-dimensional input methods. In contrast to previous methods, we examine the
possibilities when the control-display (C-D) ratio is significantly smaller
than one and small input movements result in large output movements. To this
end one specific multi-display configuration is implemented in the form of a
spatial-augmented reality sandbox environment, and used to explore various
interaction techniques based on a variety of mobile device touch-based input
and optical marker tracking-based finger input. A small pilot study determines
the most promising input candidate, which is compared to traditional
touch-input based techniques in a user study that tests it for practical
relevance. Results and conclusions of the study are presented.","['Travis Gesslein', 'Jens Grubert']",2020-02-12T13:37:24Z,http://arxiv.org/abs/2002.04980v1
"Cloud Rendering-based Volumetric Video Streaming System for Mixed
  Reality Services","Volumetric video is an emerging technology for immersive representation of 3D
spaces that captures objects from all directions using multiple cameras and
creates a dynamic 3D model of the scene. However, processing volumetric content
requires high amounts of processing power and is still a very demanding task
for today's mobile devices. To mitigate this, we propose a volumetric video
streaming system that offloads the rendering to a powerful cloud/edge server
and only sends the rendered 2D view to the client instead of the full
volumetric content. We use 6DoF head movement prediction techniques, WebRTC
protocol and hardware video encoding to ensure low-latency in different parts
of the processing chain. We demonstrate our system using both a browser-based
client and a Microsoft HoloLens client. Our application contains generic
interfaces that allow for easy deployment of various augmented/mixed reality
clients using the same server implementation.","['Serhan Gül', 'Dimitri Podborski', 'Jangwoo Son', 'Gurdeep Singh Bhullar', 'Thomas Buchholz', 'Thomas Schierl', 'Cornelius Hellge']",2020-03-05T10:44:37Z,http://arxiv.org/abs/2003.02526v2
"MusicID: A Brainwave-based User Authentication System for Internet of
  Things","We propose MusicID, an authentication solution for smart devices that uses
music-induced brainwave patterns as a behavioral biometric modality. We
experimentally evaluate MusicID using data collected from real users whilst
they are listening to two forms of music; a popular English song and
individual's favorite song. We show that an accuracy over 98% for user
identification and an accuracy over 97% for user verification can be achieved
by using data collected from a 4-electrode commodity brainwave headset. We
further show that a single electrode is able to provide an accuracy of
approximately 85% and the use of two electrodes provides an accuracy of
approximately 95%. As already shown by commodity brain-sensing headsets for
meditation applications, we believe including dry EEG electrodes in
smart-headsets is feasible and MusicID has the potential of providing an entry
point and continuous authentication framework for upcoming surge of
smart-devices mainly driven by Augmented Reality (AR)/Virtual Reality (VR)
applications.","['Jinani Sooriyaarachchi', 'Suranga Seneviratne', 'Kanchana Thilakarathna', 'Albert Y. Zomaya']",2020-06-02T16:23:49Z,http://arxiv.org/abs/2006.01751v1
"Exploring Severe Occlusion: Multi-Person 3D Pose Estimation with Gated
  Convolution","3D human pose estimation (HPE) is crucial in many fields, such as human
behavior analysis, augmented reality/virtual reality (AR/VR) applications, and
self-driving industry. Videos that contain multiple potentially occluded people
captured from freely moving monocular cameras are very common in real-world
scenarios, while 3D HPE for such scenarios is quite challenging, partially
because there is a lack of such data with accurate 3D ground truth labels in
existing datasets. In this paper, we propose a temporal regression network with
a gated convolution module to transform 2D joints to 3D and recover the missing
occluded joints in the meantime. A simple yet effective localization approach
is further conducted to transform the normalized pose to the global trajectory.
To verify the effectiveness of our approach, we also collect a new moving
camera multi-human (MMHuman) dataset that includes multiple people with heavy
occlusion captured by moving cameras. The 3D ground truth joints are provided
by accurate motion capture (MoCap) system. From the experiments on
static-camera based Human3.6M data and our own collected moving-camera based
data, we show that our proposed method outperforms most state-of-the-art
2D-to-3D pose estimation methods, especially for the scenarios with heavy
occlusions.","['Renshu Gu', 'Gaoang Wang', 'Jenq-Neng Hwang']",2020-10-31T04:35:24Z,http://arxiv.org/abs/2011.00184v1
"Towards Neurohaptics: Brain-Computer Interfaces for Decoding Intuitive
  Sense of Touch","Noninvasive brain-computer interface (BCI) is widely used to recognize users'
intentions. Especially, BCI related to tactile and sensation decoding could
provide various effects on many industrial fields such as manufacturing
advanced touch displays, controlling robotic devices, and more immersive
virtual reality or augmented reality. In this paper, we introduce haptic and
sensory perception-based BCI systems called neurohaptics. It is a preliminary
study for a variety of scenarios using actual touch and touch imagery
paradigms. We designed a novel experimental environment and a device that could
acquire brain signals under touching designated materials to generate natural
touch and texture sensations. Through the experiment, we collected the
electroencephalogram (EEG) signals with respect to four different texture
objects. Seven subjects were recruited for the experiment and evaluated
classification performances using machine learning and deep learning
approaches. Hence, we could confirm the feasibility of decoding actual touch
and touch imagery on EEG signals to develop practical neurohaptics.","['Jeong-Hyun Cho', 'Ji-Hoon Jeong', 'Myoung-Ki Kim', 'Seong-Whan Lee']",2020-12-12T08:08:47Z,http://arxiv.org/abs/2012.06753v2
"TaNTIN: Terrestrial and Non-Terrestrial Integrated Networks-A
  collaborative technologies perspective for beyond 5G and 6G","The world is moving toward globalization rapidly. Everybody has easy access
to information with the spread of Internet technology. Businesses are growing
beyond national borders. Internationalization affects every aspect of life. In
this scenario, by dispersing functions and tasks across organizational borders,
time and space, global organizations have higher requirements for
collaboration. In order to allow decision-makers and knowledge workers,
situated at different times and spaces, to work more efficiently, collaborative
technologies are needed. In this paper, we give an overview of potential
collaborative technologies, their benefits, risks and challenges, types, and
elements. Based on the conceptualization of terrestrial and non-terrestrial
integrated networks (TaNTIN), we highlight artificial intelligence (AI),
blockchains, tactile Internet, mobile edge computing (MEC)/fog computing,
augmented reality and virtual reality, and so forth as the key features to
ensure quality-of-service (QoS) guarantee of futuristic collaborative services
such as telemedicine, e-education, online gaming, online businesses, the
entertainment industry. We also discuss how these technologies will impact
human life in the near future.","['Muhammad Waseem Akhtar', 'Syed Ali Hassan']",2021-01-20T17:17:05Z,http://arxiv.org/abs/2101.08221v1
"Evolution of Small Cell from 4G to 6G: Past, Present, and Future","To boost the capacity of the cellular system, the operators have started to
reuse the same licensed spectrum by deploying 4G LTE small cells (Femto Cells)
in the past. But in time, these small cell licensed spectrum is not sufficient
to satisfy future applications like augmented reality (AR)and virtual reality
(VR). Hence, cellular operators look for alternate unlicensed spectrum in Wi-Fi
5 GHz band, later 3GPP named as LTE Licensed Assisted Access (LAA). The recent
and current rollout of LAA deployments (in developed nations like the US)
provides an opportunity to understand coexistence profound ground truth. This
paper discusses a high-level overview of my past, present, and future research
works in the direction of small cell benefits. In the future, we shift the
focus onto the latest unlicensed band: 6 GHz, where the latest Wi-Fi version,
802.11ax, will coexist with the latest cellular technology, 5G New Radio(NR) in
unlicensed",['Vanlin Sathya'],2020-12-29T17:28:08Z,http://arxiv.org/abs/2101.10451v1
"OV$^{2}$SLAM : A Fully Online and Versatile Visual SLAM for Real-Time
  Applications","Many applications of Visual SLAM, such as augmented reality, virtual reality,
robotics or autonomous driving, require versatile, robust and precise
solutions, most often with real-time capability. In this work, we describe
OV$^{2}$SLAM, a fully online algorithm, handling both monocular and stereo
camera setups, various map scales and frame-rates ranging from a few Hertz up
to several hundreds. It combines numerous recent contributions in visual
localization within an efficient multi-threaded architecture. Extensive
comparisons with competing algorithms shows the state-of-the-art accuracy and
real-time performance of the resulting algorithm. For the benefit of the
community, we release the source code:
\url{https://github.com/ov2slam/ov2slam}.","['Maxime Ferrera', 'Alexandre Eudes', 'Julien Moras', 'Martial Sanfourche', 'Guy Le Besnerais']",2021-02-08T08:39:23Z,http://arxiv.org/abs/2102.04060v1
Retrieval and Localization with Observation Constraints,"Accurate visual re-localization is very critical to many artificial
intelligence applications, such as augmented reality, virtual reality, robotics
and autonomous driving. To accomplish this task, we propose an integrated
visual re-localization method called RLOCS by combining image retrieval,
semantic consistency and geometry verification to achieve accurate estimations.
The localization pipeline is designed as a coarse-to-fine paradigm. In the
retrieval part, we cascade the architecture of ResNet101-GeM-ArcFace and employ
DBSCAN followed by spatial verification to obtain a better initial coarse pose.
We design a module called observation constraints, which combines geometry
information and semantic consistency for filtering outliers. Comprehensive
experiments are conducted on open datasets, including retrieval on R-Oxford5k
and R-Paris6k, semantic segmentation on Cityscapes, localization on Aachen
Day-Night and InLoc. By creatively modifying separate modules in the total
pipeline, our method achieves many performance improvements on the challenging
localization benchmarks.","['Yuhao Zhou', 'Huanhuan Fan', 'Shuang Gao', 'Yuchen Yang', 'Xudong Zhang', 'Jijunnan Li', 'Yandong Guo']",2021-08-19T06:14:33Z,http://arxiv.org/abs/2108.08516v1
"The ""Kinesthetic HMD"": Enhancing Self-Motion Sensations in VR with
  Head-Based Force Feedback","The sensation of self-motion is essential in many virtual reality
applications, from entertainment to training, such as flying and driving
simulators. If the common approach used in amusement parks is to actuate the
seats with cumbersome systems, multisensory integration can also be leveraged
to get rich effects from lightweight solutions. In this short paper, we
introduce a novel approach called the ""Kinesthetic HMD"": actuating a
head-mounted display with force feedback in order to provide sensations of
self-motion. We discuss its design considerations and demonstrate an augmented
flight simulator use case with a proof-of-concept prototype. We conducted a
user study assessing our approach's ability to enhance self-motion sensations.
Taken together, our results show that our Kinesthetic HMD provides
significantly stronger and more egocentric sensations than a visual-only
self-motion experience. Thus, by providing congruent vestibular and
proprioceptive cues related to balance and self-motion, the Kinesthetic HMD
represents a promising approach for a variety of virtual reality applications
in which motion sensations are prominent.","['Antoine Costes', 'Anatole Lécuyer']",2021-08-23T14:26:34Z,http://arxiv.org/abs/2108.10196v1
"On Efficient Uncertainty Estimation for Resource-Constrained Mobile
  Applications","Deep neural networks have shown great success in prediction quality while
reliable and robust uncertainty estimation remains a challenge. Predictive
uncertainty supplements model predictions and enables improved functionality of
downstream tasks including embedded and mobile applications, such as virtual
reality, augmented reality, sensor fusion, and perception. These applications
often require a compromise in complexity to obtain uncertainty estimates due to
very limited memory and compute resources. We tackle this problem by building
upon Monte Carlo Dropout (MCDO) models using the Axolotl framework;
specifically, we diversify sampled subnetworks, leverage dropout patterns, and
use a branching technique to improve predictive performance while maintaining
fast computations. We conduct experiments on (1) a multi-class classification
task using the CIFAR10 dataset, and (2) a more complex human body segmentation
task. Our results show the effectiveness of our approach by reaching close to
Deep Ensemble prediction quality and uncertainty estimation, while still
achieving faster inference on resource-limited mobile platforms.","['Johanna Rock', 'Tiago Azevedo', 'René de Jong', 'Daniel Ruiz-Muñoz', 'Partha Maji']",2021-11-11T22:24:15Z,http://arxiv.org/abs/2111.09838v2
"The...Tinderverse?: Opportunities and Challenges for User Safety in
  Extended Reality (XR) Dating Apps","Dating apps such as Tinder have announced plans for a dating metaverse: the
incorporation of XR technologies into the online dating process to augment
interactions between potential sexual partners across virtual and physical
worlds. While the dating metaverse is still in conceptual stages we can
forecast significant harms that it may expose daters to given prior research
into the frequency and severity of sexual harms facilitated by dating apps as
well as harms within social VR environments. In this workshop paper we envision
how XR could enrich virtual-to-physical interaction between potential sexual
partners and outline harms that it will likely perpetuate as well. We then
introduce our ongoing research to preempt such harms: a participatory design
study with sexual violence experts and demographics at disproportionate risk of
sexual violence to produce mitigative solutions to sexual violence perpetuated
by XR-enabled dating apps.","['Sarath S. Shanker', 'Douglas Zytko']",2022-03-28T21:55:13Z,http://arxiv.org/abs/2203.15120v2
From PHY to QoE: A Parameterized Framework Design,"The rapid development of 5G communication technology has given birth to
various real-time broadband communication services, such as augmented reality
(AR), virtual reality (VR) and cloud games. Compared with traditional services,
consumers tend to focus more on their subjective experience when utilizing
these services. In the meantime, the problem of power consumption is
particularly prominent in 5G and beyond. The traditional design of physical
layer (PHY) receiver is based on maximizing spectrum efficiency or minimizing
error, but this will no longer be the best after considering energy efficiency
and these new-coming services. Therefore, this paper uses quality of experience
(QoE) as the optimization criterion of the PHY algorithm. In order to establish
the relationship between PHY and QoE, this paper models the end-to-end
transmission from UE perspective and proposes a five-layer framework based on
hierarchical analysis method, which includes system-level model, bitstream
model, packet model, service quality model and experience quality model. Real
data in 5G network is used to train the parameters of the involved models for
each type of services, respectively. The results show that the PHY algorithms
can be simplified in perspective of QoE.","['Hao Wang', 'Lei Ji', 'Zhenxing Gao']",2022-04-08T03:54:58Z,http://arxiv.org/abs/2204.03828v1
"i-GSI: A Fast and Reliable Grasp-type Switching Interface based on
  Augmented Reality and Eye-tracking","The control of multi-fingered dexterous prosthetics hand remains challenging
due to the lack of an intuitive and efficient Grasp-type Switching Interface
(GSI). We propose a new GSI (i-GSI) t hat integrates the manifold power of
eye-tracking and augmentced reality technologies to solve this problem. It runs
entirely in a HoloLens2 helmet, where users can glance at icons on the
holographic interface to switch between six daily grasp types quickly. Compared
to traditional GSIs (FSM-based, PR-based, and mobile APP-based), i-GSI achieved
the best results in the experiment with eight healthy subjects, achieving a
switching time of 0.84 s, a switching success rate of 99.0%, and learning
efficiency of 93.50%. By verifying on one patient with a congenital upper limb
deficiency, i-GSI achieved an equivalent great outcome as on healthy people,
with a switching time of 0.78 s and switching success rate of 100%. The new
i-GSI, as a standalone module, can be combined with traditional proportional
myoelectric control to form a hybrid-controlled prosthetic system that can help
patients accomplish dexterous operations in various daily-life activities.","['Chunyuan Shi', 'Dapeng Yang', 'Siyang Qiu', 'Jingdong Zhao']",2022-04-22T12:25:07Z,http://arxiv.org/abs/2204.10664v1
"Foveated Rendering: Motivation, Taxonomy, and Research Directions","With the recent interest in virtual reality and augmented reality, there is a
newfound demand for displays that can provide high resolution with a wide field
of view (FOV). However, such displays incur significantly higher costs for
rendering the larger number of pixels. This poses the challenge of rendering
realistic real-time images that have a wide FOV and high resolution using
limited computing resources. The human visual system does not need every pixel
to be rendered at a uniformly high quality. Foveated rendering methods provide
perceptually high-quality images while reducing computational workload and are
becoming a crucial component for large-scale rendering. In this paper, we
present key motivations, research directions, and challenges for leveraging the
limitations of the human visual system as they relate to foveated rendering. We
provide a taxonomy to compare and contrast various foveated techniques based on
key factors. We also review aliasing artifacts arising due to foveation methods
and discuss several approaches that attempt to mitigate such effects. Finally,
we present several open problems and possible future research directions that
can further reduce computational costs while generating perceptually
high-quality renderings.","['Susmija Jabbireddy', 'Xuetong Sun', 'Xiaoxu Meng', 'Amitabh Varshney']",2022-05-09T19:48:01Z,http://arxiv.org/abs/2205.04529v2
COFS: Controllable Furniture layout Synthesis,"Scalable generation of furniture layouts is essential for many applications
in virtual reality, augmented reality, game development and synthetic data
generation. Many existing methods tackle this problem as a sequence generation
problem which imposes a specific ordering on the elements of the layout making
such methods impractical for interactive editing or scene completion.
Additionally, most methods focus on generating layouts unconditionally and
offer minimal control over the generated layouts. We propose COFS, an
architecture based on standard transformer architecture blocks from language
modeling. The proposed model is invariant to object order by design, removing
the unnatural requirement of specifying an object generation order.
Furthermore, the model allows for user interaction at multiple levels enabling
fine grained control over the generation process. Our model consistently
outperforms other methods which we verify by performing quantitative
evaluations. Our method is also faster to train and sample from, compared to
existing methods.","['Wamiq Reyaz Para', 'Paul Guerrero', 'Niloy Mitra', 'Peter Wonka']",2022-05-29T13:31:18Z,http://arxiv.org/abs/2205.14657v1
"i-FlatCam: A 253 FPS, 91.49 $μ$J/Frame Ultra-Compact Intelligent
  Lensless Camera for Real-Time and Efficient Eye Tracking in VR/AR","We present a first-of-its-kind ultra-compact intelligent camera system,
dubbed i-FlatCam, including a lensless camera with a computational (Comp.)
chip. It highlights (1) a predict-then-focus eye tracking pipeline for boosted
efficiency without compromising the accuracy, (2) a unified compression scheme
for single-chip processing and improved frame rate per second (FPS), and (3)
dedicated intra-channel reuse design for depth-wise convolutional layers
(DW-CONV) to increase utilization. i-FlatCam demonstrates the first eye
tracking pipeline with a lensless camera and achieves 3.16 degrees of accuracy,
253 FPS, 91.49 $\mu$J/Frame, and 6.7mm x 8.9mm x 1.2mm camera form factor,
paving the way for next-generation Augmented Reality (AR) and Virtual Reality
(VR) devices.","['Yang Zhao', 'Ziyun Li', 'Yonggan Fu', 'Yongan Zhang', 'Chaojian Li', 'Cheng Wan', 'Haoran You', 'Shang Wu', 'Xu Ouyang', 'Vivek Boominathan', 'Ashok Veeraraghavan', 'Yingyan Lin']",2022-06-15T08:55:55Z,http://arxiv.org/abs/2206.08141v2
"GazBy: Gaze-Based BERT Model to Incorporate Human Attention in Neural
  Information Retrieval","This paper is interested in investigating whether human gaze signals can be
leveraged to improve state-of-the-art search engine performance and how to
incorporate this new input signal marked by human attention into existing
neural retrieval models. In this paper, we propose GazBy ({\bf Gaz}e-based {\bf
B}ert model for document relevanc{\bf y}), a light-weight joint model that
integrates human gaze fixation estimation into transformer models to predict
document relevance, incorporating more nuanced information about cognitive
processing into information retrieval (IR). We evaluate our model on the Text
Retrieval Conference (TREC) Deep Learning (DL) 2019 and 2020 Tracks. Our
experiments show encouraging results and illustrate the effective and
ineffective entry points for using human gaze to help with transformer-based
neural retrievers. With the rise of virtual reality (VR) and augmented reality
(AR), human gaze data will become more available. We hope this work serves as a
first step exploring using gaze signals in modern neural search engines.","['Sibo Dong', 'Justin Goldstein', 'Grace Hui Yang']",2022-07-04T18:50:48Z,http://arxiv.org/abs/2207.01674v1
"Path Tracing in 2D, 3D, and Physicalized Networks","It is common to advise against using 3D to visualize abstract data such as
networks, however Ware and Mitchell's 2008 study showed that path tracing in a
network is less error prone in 3D than in 2D. It is unclear, however, if 3D
retains its advantage when the 2D presentation of a network is improved using
edge-routing, and when simple interaction techniques for exploring the network
are available. We address this with two studies of path tracing under new
conditions. The first study was preregistered, involved 34 users, and compared
2D and 3D layouts that the user could rotate and move in virtual reality with a
handheld controller. Error rates were lower in 3D than in 2D, despite the use
of edge-routing in 2D and the use of mouse-driven interactive highlighting of
edges. The second study involved 12 users and investigated data
physicalization, comparing 3D layouts in virtual reality versus physical 3D
printouts of networks augmented with a Microsoft HoloLens headset. No
difference was found in error rate, but users performed a variety of actions
with their fingers in the physical condition which can inform new interaction
techniques.","['Michael J. McGuffin', 'Ryan Servera', 'Marie Forest']",2022-07-23T19:35:53Z,http://arxiv.org/abs/2207.11586v1
"A Pilot Study on The Impact of Stereoscopic Display Type on User
  Interactions Within A Immersive Analytics Environment","Immersive Analytics (IA) and consumer adoption of augmented reality (AR) and
virtual reality (VR) head-mounted displays (HMDs) are both rapidly growing.
When used in conjunction, stereoscopic IA environments can offer improved user
understanding and engagement; however, it is unclear how the choice of
stereoscopic display impacts user interactions within an IA environment. This
paper presents a pilot study that examines the impact of stereoscopic display
type on object manipulation and environmental navigation using
consumer-available AR and VR displays. This work finds that the display type
can impact how users manipulate virtual content, how they navigate the
environment, and how able they are to answer questions about the represented
data.","['Adam S. Williams', 'Xiaoyan Zhou', 'Michel Pahud', 'Francisco R. Ortega']",2022-07-25T22:23:53Z,http://arxiv.org/abs/2207.12558v1
LWA-HAND: Lightweight Attention Hand for Interacting Hand Reconstruction,"Recent years have witnessed great success for hand reconstruction in
real-time applications such as visual reality and augmented reality while
interacting with two-hand reconstruction through efficient transformers is left
unexplored. In this paper, we propose a method called lightweight attention
hand (LWA-HAND) to reconstruct hands in low flops from a single RGB image. To
solve the occlusion and interaction problem in efficient attention
architectures, we propose three mobile attention modules in this paper. The
first module is a lightweight feature attention module that extracts both local
occlusion representation and global image patch representation in a
coarse-to-fine manner. The second module is a cross image and graph bridge
module which fuses image context and hand vertex. The third module is a
lightweight cross-attention mechanism that uses element-wise operation for the
cross-attention of two hands in linear complexity. The resulting model achieves
comparable performance on the InterHand2.6M benchmark in comparison with the
state-of-the-art models. Simultaneously, it reduces the flops to $0.47GFlops$
while the state-of-the-art models have heavy computations between $10GFlops$
and $20GFlops$.","['Xinhan Di', 'Pengqian Yu']",2022-08-21T06:25:56Z,http://arxiv.org/abs/2208.09815v3
CU-Net: Real-Time High-Fidelity Color Upsampling for Point Clouds,"Point cloud upsampling is essential for high-quality augmented reality,
virtual reality, and telepresence applications, due to the capture, processing,
and communication limitations of existing technologies. Although geometry
upsampling to densify a point cloud's coordinates has been well studied, the
upsampling of the color attributes has been largely overlooked. In this paper,
we propose CU-Net, the first deep-learning point cloud color upsampling model
that enables low latency and high visual fidelity operation. CU-Net achieves
linear time and space complexity by leveraging a feature extractor based on
sparse convolution and a color prediction module based on neural implicit
function. Therefore, CU-Net is theoretically guaranteed to be more efficient
than most existing methods with quadratic complexity. Experimental results
demonstrate that CU-Net can colorize a photo-realistic point cloud with nearly
a million points in real time, while having notably better visual performance
than baselines. Besides, CU-Net can adapt to arbitrary upsampling ratios and
unseen objects without retraining. Our source code is available at
https://github.com/UMass-LIDS/cunet.","['Lingdong Wang', 'Mohammad Hajiesmaili', 'Jacob Chakareski', 'Ramesh K. Sitaraman']",2022-09-12T04:22:09Z,http://arxiv.org/abs/2209.06112v2
Self-supervised Multi-Modal Video Forgery Attack Detection,"Video forgery attack threatens the surveillance system by replacing the video
captures with unrealistic synthesis, which can be powered by the latest augment
reality and virtual reality technologies. From the machine perception aspect,
visual objects often have RF signatures that are naturally synchronized with
them during recording. In contrast to video captures, the RF signatures are
more difficult to attack given their concealed and ubiquitous nature. In this
work, we investigate multimodal video forgery attack detection methods using
both vision and wireless modalities. Since wireless signal-based human
perception is environmentally sensitive, we propose a self-supervised training
strategy to enable the system to work without external annotation and thus can
adapt to different environments. Our method achieves a perfect human detection
accuracy and a high forgery attack detection accuracy of 94.38% which is
comparable with supervised methods.","['Chenhui Zhao', 'Xiang Li', 'Rabih Younes']",2022-09-13T23:41:26Z,http://arxiv.org/abs/2209.06345v2
Mixed-Reality Robot Behavior Replay: A System Implementation,"As robots become increasingly complex, they must explain their behaviors to
gain trust and acceptance. However, it may be difficult through verbal
explanation alone to fully convey information about past behavior, especially
regarding objects no longer present due to robots' or humans' actions. Humans
often try to physically mimic past movements to accompany verbal explanations.
Inspired by this human-human interaction, we describe the technical
implementation of a system for past behavior replay for robots in this tool
paper. Specifically, we used Behavior Trees to encode and separate robot
behaviors, and schemaless MongoDB to structurally store and query the
underlying sensor data and joint control messages for future replay. Our
approach generalizes to different types of replays, including both manipulation
and navigation replay, and visual (i.e., augmented reality (AR)) and auditory
replay. Additionally, we briefly summarize a user study to further provide
empirical evidence of its effectiveness and efficiency. Sample code and
instructions are available on GitHub at
https://github.com/umhan35/robot-behavior-replay.","['Zhao Han', 'Tom Williams', 'Holly A. Yanco']",2022-09-30T20:19:07Z,http://arxiv.org/abs/2210.00075v1
"NeRF: Neural Radiance Field in 3D Vision, A Comprehensive Review","Neural Radiance Field (NeRF) has recently become a significant development in
the field of Computer Vision, allowing for implicit, neural network-based scene
representation and novel view synthesis. NeRF models have found diverse
applications in robotics, urban mapping, autonomous navigation, virtual
reality/augmented reality, and more. Due to the growing popularity of NeRF and
its expanding research area, we present a comprehensive survey of NeRF papers
from the past two years. Our survey is organized into architecture and
application-based taxonomies and provides an introduction to the theory of NeRF
and its training via differentiable volume rendering. We also present a
benchmark comparison of the performance and speed of key NeRF models. By
creating this survey, we hope to introduce new researchers to NeRF, provide a
helpful reference for influential works in this field, as well as motivate
future research directions with our discussion section.","['Kyle Gao', 'Yina Gao', 'Hongjie He', 'Dening Lu', 'Linlin Xu', 'Jonathan Li']",2022-10-01T21:35:11Z,http://arxiv.org/abs/2210.00379v5
"Review of Persuasive User Interface as Strategy for Technology Addiction
  in Virtual Environments","In the era of virtuality, the increasingly ubiquitous technology bears the
challenge of excessive user dependency, also known as user addiction. Augmented
reality (AR) and virtual reality (VR) have become increasingly integrated into
daily life. Although discussions about the drawbacks of these technologies are
abundant, their exploration for solutions is still rare. Thus, using the PRISMA
methodology, this paper reviewed the literature on technology addiction and
persuasive technology. After describing the key research trends, the paper
summed up nine persuasive elements of user interfaces (UIs) that AR and VR
developers could add to their apps to make them less addictive. Furthermore,
this review paper encourages more research into a persuasive strategy for
controlling user dependency in virtual-physical blended cyberspace.","['Fachrina Dewi Puspitasari', 'Lik-Hang Lee']",2022-10-18T06:54:06Z,http://arxiv.org/abs/2210.09628v1
Slippage-robust Gaze Tracking for Near-eye Display,"In recent years, head-mounted near-eye display devices have become the key
hardware foundation for virtual reality and augmented reality. Thus
head-mounted gaze tracking technology has received attention as an essential
part of human-computer interaction. However, unavoidable slippage of
head-mounted devices (HMD) often results higher gaze tracking errors and
hinders the practical usage of HMD. To tackle this problem, we propose a
slippage-robust gaze tracking for near-eye display method based on the aspheric
eyeball model and accurately compute the eyeball optical axis and rotation
center. We tested several methods on datasets with slippage and the
experimental results show that the proposed method significantly outperforms
the previous method (almost double the suboptimal method).","['Wei Zhang', 'Jiaxi Cao', 'Xiang Wang', 'Enqi Tian', 'Bin Li']",2022-10-20T23:47:56Z,http://arxiv.org/abs/2210.11637v2
Facial De-occlusion Network for Virtual Telepresence Systems,"To see what is not in the image is one of the broader missions of computer
vision. Technology to inpaint images has made significant progress with the
coming of deep learning. This paper proposes a method to tackle occlusion
specific to human faces. Virtual presence is a promising direction in
communication and recreation for the future. However, Virtual Reality (VR)
headsets occlude a significant portion of the face, hindering the
photo-realistic appearance of the face in the virtual world. State-of-the-art
image inpainting methods for de-occluding the eye region does not give usable
results. To this end, we propose a working solution that gives usable results
to tackle this problem enabling the use of the real-time photo-realistic
de-occluded face of the user in VR settings.","['Surabhi Gupta', 'Ashwath Shetty', 'Avinash Sharma']",2022-10-23T05:34:17Z,http://arxiv.org/abs/2210.12622v1
"Vox-Fusion: Dense Tracking and Mapping with Voxel-based Neural Implicit
  Representation","In this work, we present a dense tracking and mapping system named
Vox-Fusion, which seamlessly fuses neural implicit representations with
traditional volumetric fusion methods. Our approach is inspired by the recently
developed implicit mapping and positioning system and further extends the idea
so that it can be freely applied to practical scenarios. Specifically, we
leverage a voxel-based neural implicit surface representation to encode and
optimize the scene inside each voxel. Furthermore, we adopt an octree-based
structure to divide the scene and support dynamic expansion, enabling our
system to track and map arbitrary scenes without knowing the environment like
in previous works. Moreover, we proposed a high-performance multi-process
framework to speed up the method, thus supporting some applications that
require real-time performance. The evaluation results show that our methods can
achieve better accuracy and completeness than previous methods. We also show
that our Vox-Fusion can be used in augmented reality and virtual reality
applications. Our source code is publicly available at
https://github.com/zju3dv/Vox-Fusion.","['Xingrui Yang', 'Hai Li', 'Hongjia Zhai', 'Yuhang Ming', 'Yuqian Liu', 'Guofeng Zhang']",2022-10-28T02:56:47Z,http://arxiv.org/abs/2210.15858v3
"A systematic literature review on Virtual Reality and Augmented Reality
  in terms of privacy, authorization and data-leaks","In recent years, VR and AR has exploded into a multimillionaire market. As
this emerging technology has spread to a variety of businesses and is rapidly
increasing among users. It is critical to address potential privacy and
security concerns that these technologies might pose. In this study, we discuss
the current status of privacy and security in VR and AR. We analyse possible
problems and risks. Besides, we will look in detail at a few of the major
concerns issues and related security solutions for AR and VR. Additionally, as
VR and AR authentication is the most thoroughly studied aspect of the problem,
we concentrate on the research that has already been done in this area.","['Parth Dipakkumar Patel', 'Prem Trivedi']",2022-12-09T01:28:58Z,http://arxiv.org/abs/2212.04621v1
"What you see is (not) what you get: A VR Framework for Correcting Robot
  Errors","Many solutions tailored for intuitive visualization or teleoperation of
virtual, augmented and mixed (VAM) reality systems are not robust to robot
failures, such as the inability to detect and recognize objects in the
environment or planning unsafe trajectories. In this paper, we present a novel
virtual reality (VR) framework where users can (i) recognize when the robot has
failed to detect a real-world object, (ii) correct the error in VR, (iii)
modify proposed object trajectories and, (iv) implement behaviors on a
real-world robot. Finally, we propose a user study aimed at testing the
efficacy of our framework. Project materials can be found in the OSF
repository.","['Maciej K. Wozniak', 'Rebecca Stower', 'Patric Jensfelt', 'Andre Pereira']",2023-01-12T10:27:30Z,http://arxiv.org/abs/2301.04919v2
Implementing Continuous HRTF Measurement in Near-Field,"Head-related transfer function (HRTF) is an essential component to create an
immersive listening experience over headphones for virtual reality (VR) and
augmented reality (AR) applications. Metaverse combines VR and AR to create
immersive digital experiences, and users are very likely to interact with
virtual objects in the near-field (NF). The HRTFs of such objects are highly
individualized and dependent on directions and distances. Hence, a significant
number of HRTF measurements at different distances in the NF would be needed.
Using conventional static stop-and-go HRTF measurement methods to acquire these
measurements would be time-consuming and tedious for human listeners. In this
paper, we propose a continuous measurement system targeted for the NF, and
efficiently capturing HRTFs in the horizontal plane within 45 secs. Comparative
experiments are performed on head and torso simulator (HATS) and human
listeners to evaluate system consistency and robustness.","['Ee-Leng Tan', 'Santi Peksi', 'Woon-Seng Gan']",2023-03-15T05:41:35Z,http://arxiv.org/abs/2303.08379v4
"Learning to Read Braille: Bridging the Tactile Reality Gap with
  Diffusion Models","Simulating vision-based tactile sensors enables learning models for
contact-rich tasks when collecting real world data at scale can be prohibitive.
However, modeling the optical response of the gel deformation as well as
incorporating the dynamics of the contact makes sim2real challenging. Prior
works have explored data augmentation, fine-tuning, or learning generative
models to reduce the sim2real gap. In this work, we present the first method to
leverage probabilistic diffusion models for capturing complex illumination
changes from gel deformations. Our tactile diffusion model is able to generate
realistic tactile images from simulated contact depth bridging the reality gap
for vision-based tactile sensing. On real braille reading task with a DIGIT
sensor, a classifier trained with our diffusion model achieves 75.74% accuracy
outperforming classifiers trained with simulation and other approaches. Project
page: https://github.com/carolinahiguera/Tactile-Diffusion","['Carolina Higuera', 'Byron Boots', 'Mustafa Mukadam']",2023-04-03T17:51:07Z,http://arxiv.org/abs/2304.01182v1
"Neural Radiance Fields: Past, Present, and Future","The various aspects like modeling and interpreting 3D environments and
surroundings have enticed humans to progress their research in 3D Computer
Vision, Computer Graphics, and Machine Learning. An attempt made by Mildenhall
et al in their paper about NeRFs (Neural Radiance Fields) led to a boom in
Computer Graphics, Robotics, Computer Vision, and the possible scope of
High-Resolution Low Storage Augmented Reality and Virtual Reality-based 3D
models have gained traction from res with more than 1000 preprints related to
NeRFs published. This paper serves as a bridge for people starting to study
these fields by building on the basics of Mathematics, Geometry, Computer
Vision, and Computer Graphics to the difficulties encountered in Implicit
Representations at the intersection of all these disciplines. This survey
provides the history of rendering, Implicit Learning, and NeRFs, the
progression of research on NeRFs, and the potential applications and
implications of NeRFs in today's world. In doing so, this survey categorizes
all the NeRF-related research in terms of the datasets used, objective
functions, applications solved, and evaluation criteria for these applications.",['Ansh Mittal'],2023-04-20T02:17:08Z,http://arxiv.org/abs/2304.10050v2
"An XRI Mixed-Reality Internet-of-Things Architectural Framework Toward
  Immersive and Adaptive Smart Environments","The internet-of-things (IoT) refers to the growing number of embedded
interconnected devices within everyday ubiquitous objects and environments,
especially their networks, edge controllers, data gathering and management,
sharing, and contextual analysis capabilities. However, the IoT suffers from
inherent limitations in terms of human-computer interaction. In this landscape,
there is a need for interfaces that have the potential to translate the IoT
more solidly into the foreground of everyday smart environments, where its
users are multimodal, multifaceted, and where new forms of presentation,
adaptation, and immersion are essential. This work highlights the synergetic
opportunities for both IoT and XR to converge toward hybrid XR objects with
strong real-world connectivity, and IoT objects with rich XR interfaces. The
paper contributes i) an understanding of this multi-disciplinary domain XR-IoT
(XRI); ii) a theoretical perspective on how to design XRI agents based on the
literature; iii) a system design architectural framework for XRI smart
environment development; and iv) an early discussion of this process. It is
hoped that this research enables future researchers in both communities to
better understand and deploy hybrid smart XRI environments.","['Alexis Morris', 'Jie Guan', 'Amna Azhar']",2023-06-01T20:47:07Z,http://arxiv.org/abs/2306.01139v1
AI-Powered Interfaces for Extended Reality to support Remote Maintenance,"High-end components that conduct complicated tasks automatically are a part
of modern industrial systems. However, in order for these parts to function at
the desired level, they need to be maintained by qualified experts. Solutions
based on Augmented Reality (AR) have been established with the goal of raising
production rates and quality while lowering maintenance costs. With the
introduction of two unique interaction interfaces based on wearable targets and
human face orientation, we are proposing hands-free advanced interactive
solutions in this study with the goal of reducing the bias towards certain
users. Using traditional devices in real time, a comparison investigation using
alternative interaction interfaces is conducted. The suggested solutions are
supported by various AI powered methods such as novel gravity-map based motion
adjustment that is made possible by predictive deep models that reduce the bias
of traditional hand- or finger-based interaction interfaces","['Akos Nagy', 'George Amponis', 'Konstantinos Kyranou', 'Thomas Lagkas', 'Alexandros Apostolos Boulogeorgos', 'Panagiotis Sarigiannidis', 'Vasileios Argyriou']",2023-06-29T14:12:58Z,http://arxiv.org/abs/2306.16961v1
"Semantic Communications System with Model Division Multiple Access and
  Controllable Coding Rate for Point Cloud","Point cloud, as a 3D representation, is widely used in autonomous driving,
virtual reality (VR), and augmented reality (AR). However, traditional
communication systems think that the point cloud's semantic information is
irrelevant to communication, which hinders the efficient transmission of point
clouds in the era of artificial intelligence (AI). This paper proposes a point
cloud based semantic communication system (PCSC), which uses AI-based encoding
techniques to extract the semantic information of the point cloud and joint
source-channel coding (JSCC) technology to overcome the distortion caused by
noise channels and solve the ""cliff effect"" in traditional communication. In
addition, the system realizes the controllable coding rate without fine-tuning
the network. The method analyzes the coded semantic vector's importance and
discards semantically-unimportant information, thereby improving the
transmission efficiency. Besides, PCSC and the recently proposed non-orthogonal
model division multiple access (MDMA) technology are combined to design a point
cloud MDMA transmission system (M-PCSC) for multi-user transmission. Relevant
experimental results show that the proposed method outperforms the traditional
method 10dB in the same channel bandwidth ratio under the PSNR D1 and PSNR D2
metrics. In terms of transmission, the proposed method can effectively solve
the ""cliff effect"" in the traditional methods.","['Xiaoyi Liu', 'Haotai Liang', 'Zhicheng Bao', 'Chen Dong', 'Xiaodong Xu']",2023-07-12T09:16:33Z,http://arxiv.org/abs/2307.06027v1
Practical Commercial 5G Standalone (SA) Uplink Throughput Prediction,"While the 5G New Radio (NR) network promises a huge uplift of the uplink
throughput, the improvement can only be seen when the User Equipment (UE) is
connected to the high-frequency millimeter wave (mmWave) band. With the rise of
uplink-intensive smartphone applications such as the real-time transmission of
UHD 4K/8K videos, and Virtual Reality (VR)/Augmented Reality (AR) contents,
uplink throughput prediction plays a huge role in maximizing the users' quality
of experience (QoE). In this paper, we propose using a ConvLSTM-based neural
network to predict the future uplink throughput based on past uplink throughput
and RF parameters. The network is trained using the data from real-world drive
tests on commercial 5G SA networks while riding commuter trains, which
accounted for various frequency bands, handover, and blind spots. To make sure
our model can be practically implemented, we then limited our model to only use
the information available via Android API, then evaluate our model using the
data from both commuter trains and other methods of transportation. The results
show that our model reaches an average prediction accuracy of 98.9\% with an
average RMSE of 1.80 Mbps across all unseen evaluation scenarios.","['Kasidis Arunruangsirilert', 'Jiro Katto']",2023-07-23T20:01:18Z,http://arxiv.org/abs/2307.12417v1
"Exploring the Opportunities of AR for Enriching Storytelling with Family
  Photos between Grandparents and Grandchildren","Storytelling with family photos, as an important mode of reminiscence-based
activities, can be instrumental in promoting intergenerational communication
between grandparents and grandchildren by strengthening generation bonds and
shared family values. Motivated by challenges that existing technology
approaches encountered for improving intergenerational storytelling (e.g., the
need to hold the tablet, the potential view detachment from the physical world
in Virtual Reality (VR)), we sought to find new ways of using Augmented Reality
(AR) to support intergenerational storytelling, which offers new capabilities
(e.g., 3D models, new interactivity) to enhance the expression for the
storyteller. We conducted a two-part exploratory study, where pairs of
grandparents and grandchildren 1) participated in an in-person storytelling
activity with a semi-structured interview 2) and then a participatory design
session with AR technology probes that we designed to inspire their
exploration. Our findings revealed insights into the possible ways of
intergenerational storytelling, the feasibility and usages of AR in
facilitating it, and the key design implications for leveraging AR in
intergenerational storytelling.","['Zisu Li', 'Li Feng', 'Chen Liang', 'Yuru Huang', 'Mingming Fan']",2023-09-07T07:37:28Z,http://arxiv.org/abs/2309.03533v1
IEEE 802.11be Wi-Fi 7: Feature Summary and Performance Evaluation,"While the pace of commercial scale application of Wi-Fi 6 accelerates, the
IEEE 802.11 Working Group is about to complete the development of a new
amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known
as Wi-Fi 7, which can be used to meet the demand for the throughput of 4K/8K
videos up to tens of Gbps and low-latency video applications such as virtual
reality (VR) and augmented reality (AR). Wi-Fi 7 not only scales Wi-Fi 6 with
doubled bandwidth, but also supports real-time applications, which brings
revolutionary changes to Wi-Fi. In this article, we start by introducing the
main objectives and timeline of Wi-Fi 7 and then list the latest key techniques
which promote the performance improvement of Wi-Fi 7. Finally, we validate the
most critical objectives of Wi-Fi 7 -- the potential up to 30 Gbps throughput
and lower latency. System-level simulation results suggest that by combining
the new techniques, Wi-Fi 7 achieves 30 Gbps throughput and lower latency than
Wi-Fi 6.","['Xiaoqian Liu', 'Yuhan Dong', 'Yiqing Li', 'Yousi Lin', 'Xun Yang', 'Ming Gan']",2023-09-27T19:09:19Z,http://arxiv.org/abs/2309.15951v1
The Reality of the Situation: A Survey of Situated Analytics,"The advent of low cost, accessible, and high performance augmented reality
(AR) has shed light on a situated form of analytics where in-situ
visualizations embedded in the real world can facilitate sensemaking based on
the user's physical location. In this work, we identify prior literature in
this emerging field with a focus on situated analytics. After collecting 47
relevant situated analytics systems, we classify them using a taxonomy of three
dimensions: situating triggers, view situatedness, and data depiction. We then
identify four archetypical patterns in our classification using an ensemble
cluster analysis. We also assess the level which these systems support the
sensemaking process. Finally, we discuss insights and design guidelines that we
learned from our analysis.","['Sungbok Shin', 'Andrea Batch', 'Peter W. S. Butcher', 'Panagiotis D. Ritsos', 'Niklas Elmqvist']",2023-10-16T02:24:07Z,http://arxiv.org/abs/2310.10015v1
ARWalker: A Virtual Walking Companion Application,"Extended Reality (XR) technologies, including Augmented Reality (AR), have
attracted significant attention over the past few years and have been utilized
in several fields, including education, healthcare, and manufacturing. In this
paper, we aim to explore the use of AR in the field of biomechanics and human
movement through the development of ARWalker, which is an AR application that
features virtual walking companions (avatars). Research participants walk in
close synchrony with the virtual companions, whose gait exhibits properties
found in the gait of young and healthy adults. As a result, research
participants can train their gait to the gait of the avatar, thus regaining the
healthy properties of their gait and reducing the risk of falls. ARWalker can
especially help older adults and individuals with diseases, who exhibit
pathological gait thus being more prone to falls. We implement a prototype of
ARWalker and evaluate its systems performance while running on a Microsoft
Hololens 2 headset.","['Pubudu Wijesooriya', 'Aaron Likens', 'Nick Stergiou', 'Spyridon Mastorakis']",2023-11-13T17:44:24Z,http://arxiv.org/abs/2311.07502v1
DSR-Diff: Depth Map Super-Resolution with Diffusion Model,"Color-guided depth map super-resolution (CDSR) improve the spatial resolution
of a low-quality depth map with the corresponding high-quality color map,
benefiting various applications such as 3D reconstruction, virtual reality, and
augmented reality. While conventional CDSR methods typically rely on
convolutional neural networks or transformers, diffusion models (DMs) have
demonstrated notable effectiveness in high-level vision tasks. In this work, we
present a novel CDSR paradigm that utilizes a diffusion model within the latent
space to generate guidance for depth map super-resolution. The proposed method
comprises a guidance generation network (GGN), a depth map super-resolution
network (DSRN), and a guidance recovery network (GRN). The GGN is specifically
designed to generate the guidance while managing its compactness. Additionally,
we integrate a simple but effective feature fusion module and a
transformer-style feature extraction module into the DSRN, enabling it to
leverage guided priors in the extraction, fusion, and reconstruction of
multi-model images. Taking into account both accuracy and efficiency, our
proposed method has shown superior performance in extensive experiments when
compared to state-of-the-art methods. Our codes will be made available at
https://github.com/shiyuan7/DSR-Diff.","['Yuan Shi', 'Bin Xia', 'Rui Zhu', 'Qingmin Liao', 'Wenming Yang']",2023-11-16T14:18:10Z,http://arxiv.org/abs/2311.09919v1
Deep-learning-driven end-to-end metalens imaging,"Recent advances in metasurface lenses (metalenses) have shown great potential
for opening a new era in compact imaging, photography, light detection and
ranging (LiDAR), and virtual reality/augmented reality (VR/AR) applications.
However, the fundamental trade-off between broadband focusing efficiency and
operating bandwidth limits the performance of broadband metalenses, resulting
in chromatic aberration, angular aberration, and a relatively low efficiency.
In this study, a deep-learning-based image restoration framework is proposed to
overcome these limitations and realize end-to-end metalens imaging, thereby
achieving aberration-free full-color imaging for mass-produced metalenses with
10-mm diameter. Neural-network-assisted metalens imaging achieved a high
resolution comparable to that of the ground truth image.","['Joonhyuk Seo', 'Jaegang Jo', 'Joohoon Kim', 'Joonho Kang', 'Chanik Kang', 'Seongwon Moon', 'Eunji Lee', 'Jehyeong Hong', 'Junsuk Rho', 'Haejun Chung']",2023-12-05T11:22:09Z,http://arxiv.org/abs/2312.02669v3
"RACER: Rational Artificial Intelligence Car-following-model Enhanced by
  Reality","This paper introduces RACER, the Rational Artificial Intelligence
Car-following model Enhanced by Reality, a cutting-edge deep learning
car-following model, that satisfies partial derivative constraints, designed to
predict Adaptive Cruise Control (ACC) driving behavior while staying
theoretically feasible. Unlike conventional models, RACER effectively
integrates Rational Driving Constraints (RDCs), crucial tenets of actual
driving, resulting in strikingly accurate and realistic predictions. Against
established models like the Optimal Velocity Relative Velocity (OVRV), a
car-following Neural Network (NN), and a car-following Physics-Informed Neural
Network (PINN), RACER excels across key metrics, such as acceleration,
velocity, and spacing. Notably, it displays a perfect adherence to the RDCs,
registering zero violations, in stark contrast to other models. This study
highlights the immense value of incorporating physical constraints within AI
models, especially for augmenting safety measures in transportation. It also
paves the way for future research to test these models against human driving
data, with the potential to guide safer and more rational driving behavior. The
versatility of the proposed model, including its potential to incorporate
additional derivative constraints and broader architectural applications,
enhances its appeal and broadens its impact within the scientific community.","['Tianyi Li', 'Alexander Halatsis', 'Raphael Stern']",2023-12-12T06:21:30Z,http://arxiv.org/abs/2312.07003v1
"Emotion Based Prediction in the Context of Optimized Trajectory Planning
  for Immersive Learning","In the virtual elements of immersive learning, the use of Google Expedition
and touch-screen-based emotion are examined. The objective is to investigate
possible ways to combine these technologies to enhance virtual learning
environments and learners emotional engagement. Pedagogical application,
affordances, and cognitive load are the corresponding measures that are
involved. Students will gain insight into the reason behind their significantly
higher post-assessment Prediction Systems scores compared to preassessment
scores through this work that leverages technology. This suggests that it is
effective to include emotional elements in immersive learning scenarios. The
results of this study may help develop new strategies by leveraging the
features of immersive learning technology in educational technologies to
improve virtual reality and augmented reality experiences. Furthermore, the
effectiveness of immersive learning environments can be raised by utilizing
magnetic, optical, or hybrid trackers that considerably improve object
tracking.","['Akey Sungheetha', 'Rajesh Sharma R', 'Chinnaiyan R']",2023-12-18T09:24:35Z,http://arxiv.org/abs/2312.11576v2
"Tail-Learning: Adaptive Learning Method for Mitigating Tail Latency in
  Autonomous Edge Systems","In the realm of edge computing, the increasing demand for high Quality of
Service (QoS), particularly in dynamic multimedia streaming applications (e.g.,
Augmented Reality/Virtual Reality and online gaming), has prompted the need for
effective solutions. Nevertheless, adopting an edge paradigm grounded in
distributed computing has exacerbated the issue of tail latency. Given a
limited variety of multimedia services supported by edge servers and the
dynamic nature of user requests, employing traditional queuing methods to model
tail latency in distributed edge computing is challenging, substantially
exacerbating head-of-line (HoL) blocking. In response to this challenge, we
have developed a learning-based scheduling method to mitigate the overall tail
latency, which adaptively selects appropriate edge servers for execution as
incoming distributed tasks vary with unknown size. To optimize the utilization
of the edge computing paradigm, we leverage Laplace transform techniques to
theoretically derive an upper bound for the response time of edge servers.
Subsequently, we integrate this upper bound into reinforcement learning to
facilitate tail learning and enable informed decisions for autonomous
distributed scheduling. The experiment results demonstrate the efficiency in
reducing tail latency compared to existing methods.","['Cheng Zhang', 'Yinuo Deng', 'Hailiang Zhao', 'Tianlv Chen', 'Shuiguang Deng']",2023-12-28T08:16:54Z,http://arxiv.org/abs/2312.16883v1
Motion-enhanced Holography,"Holographic displays, which enable pixel-level depth control and aberration
correction, are considered the key technology for the next-generation virtual
reality (VR) and augmented reality (AR) applications. However, traditional
holographic systems suffer from limited spatial bandwidth product (SBP), which
makes them impossible to reproduce \textit{realistic} 3D displays.
Time-multiplexed holography creates different speckle patterns over time and
then averages them to achieve a speckle-free 3D display. However, this approach
requires spatial light modulators (SLMs) with ultra-fast refresh rates, and
current algorithms cannot update holograms at such speeds. To overcome the
aforementioned challenge, we proposed a novel architecture, motion-enhanced
holography, that achieves \textit{realistic} 3D holographic displays without
artifacts by continuously shifting a special hologram. We introduced an
iterative algorithm to synthesize motion-enhanced holograms and demonstrated
that our method achieved a 10 dB improvement in the peak signal-to-noise ratio
(PSNR) of 3D focal stacks in numerical simulations compared to traditional
holographic systems. Furthermore, we validated this idea in optical experiments
utilizing a high-speed and high-precision programmable three-axis displacement
stage to display full-color and high-quality 3D focal stacks.","['Zhenxing Dong', 'Yuye Ling', 'Yan Li', 'Yikai Su']",2024-01-23T07:43:11Z,http://arxiv.org/abs/2401.12537v1
3D Gaussian as a New Vision Era: A Survey,"3D Gaussian Splatting (3D-GS) has emerged as a significant advancement in the
field of Computer Graphics, offering explicit scene representation and novel
view synthesis without the reliance on neural networks, such as Neural Radiance
Fields (NeRF). This technique has found diverse applications in areas such as
robotics, urban mapping, autonomous navigation, and virtual reality/augmented
reality, just name a few. Given the growing popularity and expanding research
in 3D Gaussian Splatting, this paper presents a comprehensive survey of
relevant papers from the past year. We organize the survey into taxonomies
based on characteristics and applications, providing an introduction to the
theoretical underpinnings of 3D Gaussian Splatting. Our goal through this
survey is to acquaint new researchers with 3D Gaussian Splatting, serve as a
valuable reference for seminal works in the field, and inspire future research
directions, as discussed in our concluding section.","['Ben Fei', 'Jingyi Xu', 'Rui Zhang', 'Qingyuan Zhou', 'Weidong Yang', 'Ying He']",2024-02-11T12:33:08Z,http://arxiv.org/abs/2402.07181v1
"Mixed-Reality-Guided Teleoperation of a Collaborative Robot for Surgical
  Procedures","The development of advanced surgical systems embedding the Master-Slave
control strategy introduced the possibility of remote interaction between the
surgeon and the patient, also known as teleoperation. The present paper aims to
integrate innovative technologies into the teleoperation process to enhance
workflow during surgeries. The proposed system incorporates a collaborative
robot, Kuka IIWA LBR, and Hololens 2 (an augmented reality device), allowing
the user to control the robot in an expansive environment that integrates
actual (real data) with additional digital information imported via Hololens 2.
Experimental data demonstrate the user's ability to control the Kuka IIWA using
various gestures to position it with respect to real or digital objects. Thus,
this system offers a novel solution to manipulate robots used in surgeries in a
more intuitive manner, contributing to the reduction of the learning curve for
surgeons. Calibration and testing in multiple scenarios demonstrate the
efficiency of the system in providing seamless movements.","['Gabriela Rus', 'Nadim Al Hajjar', 'Paul Tucan', 'Andra Ciocan', 'Calin Vaida', 'Corina Radu', 'Damien Chablat', 'Doina Pisla']",2024-02-19T09:51:03Z,http://arxiv.org/abs/2402.12002v1
"VLPose: Bridging the Domain Gap in Pose Estimation with Language-Vision
  Tuning","Thanks to advances in deep learning techniques, Human Pose Estimation (HPE)
has achieved significant progress in natural scenarios. However, these models
perform poorly in artificial scenarios such as painting and sculpture due to
the domain gap, constraining the development of virtual reality and augmented
reality. With the growth of model size, retraining the whole model on both
natural and artificial data is computationally expensive and inefficient. Our
research aims to bridge the domain gap between natural and artificial scenarios
with efficient tuning strategies. Leveraging the potential of language models,
we enhance the adaptability of traditional pose estimation models across
diverse scenarios with a novel framework called VLPose. VLPose leverages the
synergy between language and vision to extend the generalization and robustness
of pose estimation models beyond the traditional domains. Our approach has
demonstrated improvements of 2.26% and 3.74% on HumanArt and MSCOCO,
respectively, compared to state-of-the-art tuning strategies.","['Jingyao Li', 'Pengguang Chen', 'Xuan Ju', 'Hong Xu', 'Jiaya Jia']",2024-02-22T11:21:54Z,http://arxiv.org/abs/2402.14456v1
Human Shape and Clothing Estimation,"Human shape and clothing estimation has gained significant prominence in
various domains, including online shopping, fashion retail, augmented reality
(AR), virtual reality (VR), and gaming. The visual representation of human
shape and clothing has become a focal point for computer vision researchers in
recent years. This paper presents a comprehensive survey of the major works in
the field, focusing on four key aspects: human shape estimation, fashion
generation, landmark detection, and attribute recognition. For each of these
tasks, the survey paper examines recent advancements, discusses their strengths
and limitations, and qualitative differences in approaches and outcomes. By
exploring the latest developments in human shape and clothing estimation, this
survey aims to provide a comprehensive understanding of the field and inspire
future research in this rapidly evolving domain.","['Aayush Gupta', 'Aditya Gulati', 'Himanshu', 'Lakshya LNU']",2024-02-28T04:00:57Z,http://arxiv.org/abs/2402.18032v1
Gaze-based Human-Robot Interaction System for Infrastructure Inspections,"Routine inspections for critical infrastructures such as bridges are required
in most jurisdictions worldwide. Such routine inspections are largely visual in
nature, which are qualitative, subjective, and not repeatable. Although robotic
infrastructure inspections address such limitations, they cannot replace the
superior ability of experts to make decisions in complex situations, thus
making human-robot interaction systems a promising technology. This study
presents a novel gaze-based human-robot interaction system, designed to augment
the visual inspection performance through mixed reality. Through holograms from
a mixed reality device, gaze can be utilized effectively to estimate the
properties of the defect in real-time. Additionally, inspectors can monitor the
inspection progress online, which enhances the speed of the entire inspection
process. Limited controlled experiments demonstrate its effectiveness across
various users and defect types. To our knowledge, this is the first
demonstration of the real-time application of eye gaze in civil infrastructure
inspections.","['Sunwoong Choi', 'Zaid Abbas Al-Sabbag', 'Sriram Narasimhan', 'Chul Min Yeum']",2024-03-12T20:26:51Z,http://arxiv.org/abs/2403.08061v1
"Enabling Waypoint Generation for Collaborative Robots using LLMs and
  Mixed Reality","Programming a robotic is a complex task, as it demands the user to have a
good command of specific programming languages and awareness of the robot's
physical constraints. We propose a framework that simplifies robot deployment
by allowing direct communication using natural language. It uses large language
models (LLM) for prompt processing, workspace understanding, and waypoint
generation. It also employs Augmented Reality (AR) to provide visual feedback
of the planned outcome. We showcase the effectiveness of our framework with a
simple pick-and-place task, which we implement on a real robot. Moreover, we
present an early concept of expressive robot behavior and skill generation that
can be used to communicate with the user and learn new skills (e.g., object
grasping).","['Cathy Mengying Fang', 'Krzysztof Zieliński', 'Pattie Maes', 'Joe Paradiso', 'Bruce Blumberg', 'Mikkel Baun Kjærgaard']",2024-03-14T11:59:07Z,http://arxiv.org/abs/2403.09308v1
RGBD GS-ICP SLAM,"Simultaneous Localization and Mapping (SLAM) with dense representation plays
a key role in robotics, Virtual Reality (VR), and Augmented Reality (AR)
applications. Recent advancements in dense representation SLAM have highlighted
the potential of leveraging neural scene representation and 3D Gaussian
representation for high-fidelity spatial representation. In this paper, we
propose a novel dense representation SLAM approach with a fusion of Generalized
Iterative Closest Point (G-ICP) and 3D Gaussian Splatting (3DGS). In contrast
to existing methods, we utilize a single Gaussian map for both tracking and
mapping, resulting in mutual benefits. Through the exchange of covariances
between tracking and mapping processes with scale alignment techniques, we
minimize redundant computations and achieve an efficient system. Additionally,
we enhance tracking accuracy and mapping quality through our keyframe selection
methods. Experimental results demonstrate the effectiveness of our approach,
showing an incredibly fast speed up to 107 FPS (for the entire system) and
superior quality of the reconstructed map.","['Seongbo Ha', 'Jiung Yeon', 'Hyeonwoo Yu']",2024-03-19T08:49:48Z,http://arxiv.org/abs/2403.12550v2
"AIGCOIQA2024: Perceptual Quality Assessment of AI Generated
  Omnidirectional Images","In recent years, the rapid advancement of Artificial Intelligence Generated
Content (AIGC) has attracted widespread attention. Among the AIGC, AI generated
omnidirectional images hold significant potential for Virtual Reality (VR) and
Augmented Reality (AR) applications, hence omnidirectional AIGC techniques have
also been widely studied. AI-generated omnidirectional images exhibit unique
distortions compared to natural omnidirectional images, however, there is no
dedicated Image Quality Assessment (IQA) criteria for assessing them. This
study addresses this gap by establishing a large-scale AI generated
omnidirectional image IQA database named AIGCOIQA2024 and constructing a
comprehensive benchmark. We first generate 300 omnidirectional images based on
5 AIGC models utilizing 25 text prompts. A subjective IQA experiment is
conducted subsequently to assess human visual preferences from three
perspectives including quality, comfortability, and correspondence. Finally, we
conduct a benchmark experiment to evaluate the performance of state-of-the-art
IQA models on our database. The database will be released to facilitate future
research.","['Liu Yang', 'Huiyu Duan', 'Long Teng', 'Yucheng Zhu', 'Xiaohong Liu', 'Menghan Hu', 'Xiongkuo Min', 'Guangtao Zhai', 'Patrick Le Callet']",2024-04-01T10:08:23Z,http://arxiv.org/abs/2404.01024v1
"Fusion of Mixture of Experts and Generative Artificial Intelligence in
  Mobile Edge Metaverse","In the digital transformation era, Metaverse offers a fusion of virtual
reality (VR), augmented reality (AR), and web technologies to create immersive
digital experiences. However, the evolution of the Metaverse is slowed down by
the challenges of content creation, scalability, and dynamic user interaction.
Our study investigates an integration of Mixture of Experts (MoE) models with
Generative Artificial Intelligence (GAI) for mobile edge computing to
revolutionize content creation and interaction in the Metaverse. Specifically,
we harness an MoE model's ability to efficiently manage complex data and
complex tasks by dynamically selecting the most relevant experts running
various sub-models to enhance the capabilities of GAI. We then present a novel
framework that improves video content generation quality and consistency, and
demonstrate its application through case studies. Our findings underscore the
efficacy of MoE and GAI integration to redefine virtual experiences by offering
a scalable, efficient pathway to harvest the Metaverse's full potential.","['Guangyuan Liu', 'Hongyang Du', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Abbas Jamalipour', 'Shiwen Mao', 'Dong In Kim']",2024-04-04T09:37:59Z,http://arxiv.org/abs/2404.03321v1
HFNeRF: Learning Human Biomechanic Features with Neural Radiance Fields,"In recent advancements in novel view synthesis, generalizable Neural Radiance
Fields (NeRF) based methods applied to human subjects have shown remarkable
results in generating novel views from few images. However, this generalization
ability cannot capture the underlying structural features of the skeleton
shared across all instances. Building upon this, we introduce HFNeRF: a novel
generalizable human feature NeRF aimed at generating human biomechanic features
using a pre-trained image encoder. While previous human NeRF methods have shown
promising results in the generation of photorealistic virtual avatars, such
methods lack underlying human structure or biomechanic features such as
skeleton or joint information that are crucial for downstream applications
including Augmented Reality (AR)/Virtual Reality (VR). HFNeRF leverages 2D
pre-trained foundation models toward learning human features in 3D using neural
rendering, and then volume rendering towards generating 2D feature maps. We
evaluate HFNeRF in the skeleton estimation task by predicting heatmaps as
features. The proposed method is fully differentiable, allowing to successfully
learn color, geometry, and human skeleton in a simultaneous manner. This paper
presents preliminary results of HFNeRF, illustrating its potential in
generating realistic virtual avatars with biomechanic features using NeRF.","['Arnab Dey', 'Di Yang', 'Antitza Dantcheva', 'Jean Martinet']",2024-04-09T09:23:04Z,http://arxiv.org/abs/2404.06152v1
"Beyond Gaze Points: Augmenting Eye Movement with Brainwave Data for
  Multimodal User Authentication in Extended Reality","The increasing adoption of Extended Reality (XR) in various applications
underscores the need for secure and user-friendly authentication methods.
However, existing methods can disrupt the immersive experience in XR settings,
or suffer from higher false acceptance rates. In this paper, we introduce a
multimodal biometric authentication system that combines eye movement and
brainwave patterns, as captured by consumer-grade low-fidelity sensors. Our
multimodal authentication exploits the non-invasive and hands-free properties
of eye movement and brainwaves to provide a seamless XR user experience and
enhanced security as well. Using synchronized eye and brainwave data collected
from 30 participants through consumer-grade devices, we investigated whether
twin neural networks can utilize these biometrics for identity verification.
Our multimodal authentication system yields an excellent Equal Error Rate (EER)
of 0.298\%, which means an 83.6\% reduction in EER compared to the single eye
movement modality or a 93.9\% reduction in EER compared to the single brainwave
modality.","['Matin Fallahi', 'Patricia Arias-Cabarcos', 'Thorsten Strufe']",2024-04-29T13:42:55Z,http://arxiv.org/abs/2404.18694v1
"Perception in Pixels: Understanding Avatar Representation in
  Video-Mediated Collaborative Interactions","Despite the abundance of research concerning virtual reality (VR) avatars,
the impact of screen-based or augmented reality (AR) avatars for real-world
applications remain relatively unexplored. Notably, there is a lack of research
examining video-mediated collaborative interaction experiences using AR avatars
for goal-directed group activities. This study bridges this gap with a
mixed-methods, quasi-experimental user study that investigates video-based
small-group interactions when employing AR avatars as opposed to traditional
video for user representation. We found that the use of avatars positively
influenced self-esteem and video-based collaboration satisfaction. In addition,
our group interview findings highlight experiences and perceptions regarding
the dynamic use of avatars in video-mediated collaborative interactions,
including benefits, challenges, and factors that would influence a decision to
use avatars. This study contributes an empirical understanding of avatar
representation in mediating video-based collaborative interactions,
implications and perceptions surrounding the adoption of AR avatars, and a
comprehensive comparison of key characteristics between user representations.","['Pitch Sinlapanuntakul', 'Mark Zachry']",2024-05-06T20:48:37Z,http://arxiv.org/abs/2405.03844v1
"PLASMA -- Platform for Service Management in Digital Remote Maintenance
  Applications","To support maintenance and servicing of industrial machines, service
processes are even today often performed manually and analogously, although
supportive technologies such as augmented reality, virtual reality and digital
platforms already exist. In many cases, neither technicians on-site nor remote
experts have all the essential information and options for suitable actions
available. Existing service products and platforms do not cover all the
required functions in practice in order to map end-to-end processes. PLASMA is
a concept for a Cloud-based remote maintenance platform designed to meet these
demands. But for a real-life implementation of PLASMA, security measures are
essential as we show in this paper.","['Natascha Stumpp', 'Doris Aschenbrenner', 'Manuel Stahl', 'Andreas Aßmuth']",2024-05-20T07:15:41Z,http://arxiv.org/abs/2405.11836v1
"Augmented Coaching Ecosystem for Non-obtrusive Adaptive Personalized
  Elderly Care on the Basis of Cloud-Fog-Dew Computing Paradigm","The concept of the augmented coaching ecosystem for non-obtrusive adaptive
personalized elderly care is proposed on the basis of the integration of new
and available ICT approaches. They include the multimodal user interface
(MMUI), augmented reality (AR), machine learning (ML), Internet of Things
(IoT), and machine-to-machine (M2M) interactions. The ecosystem is based on the
Cloud-Fog-Dew computing paradigm services, providing a full symbiosis by
integrating the whole range from low-level sensors up to high-level services
using integration efficiency inherent in synergistic use of applied
technologies. Inside of this ecosystem, all of them are encapsulated in the
following network layers: Dew, Fog, and Cloud computing layer. Instead of the
""spaghetti connections"", ""mosaic of buttons"", ""puzzles of output data"", etc.,
the proposed ecosystem provides the strict division in the following dataflow
channels: consumer interaction channel, machine interaction channel, and
caregiver interaction channel. This concept allows to decrease the physical,
cognitive, and mental load on elderly care stakeholders by decreasing the
secondary human-to-human (H2H), human-to-machine (H2M), and machine-to-human
(M2H) interactions in favor of M2M interactions and distributed Dew Computing
services environment. It allows to apply this non-obtrusive augmented reality
ecosystem for effective personalized elderly care to preserve their physical,
cognitive, mental and social well-being.","['Yu. Gordienko', 'S. Stirenko', 'O. Alienin', 'K. Skala', 'Z. Soyat', 'A. Rojbi', 'J. R. López Benito', 'E. Artetxe González', 'U. Lushchyk', 'L. Sajn', 'A. Llorente Coto', 'G. Jervan']",2017-04-13T11:55:44Z,http://arxiv.org/abs/1704.04988v1
"Spatiotemporal-Aware Augmented Reality: Redefining HCI in Image-Guided
  Therapy","Suboptimal interaction with patient data and challenges in mastering 3D
anatomy based on ill-posed 2D interventional images are essential concerns in
image-guided therapies. Augmented reality (AR) has been introduced in the
operating rooms in the last decade; however, in image-guided interventions, it
has often only been considered as a visualization device improving traditional
workflows. As a consequence, the technology is gaining minimum maturity that it
requires to redefine new procedures, user interfaces, and interactions. The
main contribution of this paper is to reveal how exemplary workflows are
redefined by taking full advantage of head-mounted displays when entirely
co-registered with the imaging system at all times. The proposed AR landscape
is enabled by co-localizing the users and the imaging devices via the operating
room environment and exploiting all involved frustums to move spatial
information between different bodies. The awareness of the system from the
geometric and physical characteristics of X-ray imaging allows the redefinition
of different human-machine interfaces. We demonstrate that this AR paradigm is
generic, and can benefit a wide variety of procedures. Our system achieved an
error of $4.76\pm2.91$ mm for placing K-wire in a fracture management
procedure, and yielded errors of $1.57\pm1.16^\circ$ and $1.46\pm1.00^\circ$ in
the abduction and anteversion angles, respectively, for total hip arthroplasty.
We hope that our holistic approach towards improving the interface of surgery
not only augments the surgeon's capabilities but also augments the surgical
team's experience in carrying out an effective intervention with reduced
complications and provide novel approaches of documenting procedures for
training purposes.","['Javad Fotouhi', 'Arian Mehrfard', 'Tianyu Song', 'Alex Johnson', 'Greg Osgood', 'Mathias Unberath', 'Mehran Armand', 'Nassir Navab']",2020-03-04T18:59:55Z,http://arxiv.org/abs/2003.02260v1
"Augment Yourself: Mixed Reality Self-Augmentation Using Optical
  See-through Head-mounted Displays and Physical Mirrors","Optical see-though head-mounted displays (OST HMDs) are one of the key
technologies for merging virtual objects and physical scenes to provide an
immersive mixed reality (MR) environment to its user. A fundamental limitation
of HMDs is, that the user itself cannot be augmented conveniently as, in casual
posture, only the distal upper extremities are within the field of view of the
HMD. Consequently, most MR applications that are centered around the user, such
as virtual dressing rooms or learning of body movements, cannot be realized
with HMDs. In this paper, we propose a novel concept and prototype system that
combines OST HMDs and physical mirrors to enable self-augmentation and provide
an immersive MR environment centered around the user. Our system, to the best
of our knowledge the first of its kind, estimates the user's pose in the
virtual image generated by the mirror using an RGBD camera attached to the HMD
and anchors virtual objects to the reflection rather than the user directly. We
evaluate our system quantitatively with respect to calibration accuracy and
infrared signal degradation effects due to the mirror, and show its potential
in applications where large mirrors are already an integral part of the
facility. Particularly, we demonstrate its use for virtual fitting rooms,
gaming applications, anatomy learning, and personal fitness. In contrast to
competing devices such as LCD-equipped smart mirrors, the proposed system
consists of only an HMD with RGBD camera and, thus, does not require a prepared
environment making it very flexible and generic. In future work, we will aim to
investigate how the system can be optimally used for physical rehabilitation
and personal training as a promising application.","['Mathias Unberath', 'Kevin Yu', 'Roghayeh Barmaki', 'Alex Johnson', 'Nassir Navab']",2020-07-06T16:53:47Z,http://arxiv.org/abs/2007.02884v1
Automated Training and Maintenance through Kinect,"In this paper, we have worked on reducing burden on mechanic involving
complex automobile maintenance activities that are performed in centralised
workshops. We have presented a system prototype that combines Augmented Reality
with Kinect. With the use of Kinect, very high quality sensors are available at
considerably low costs, thus reducing overall expenditure for system design.
The system can be operated either in Speech mode or in Gesture mode. The system
can be controlled by various audio commands if user opts for Speech mode. The
same controlling can also be done by using a set of Gestures in Gesture mode.
  Gesture recognition is the task performed by Kinect system. This system,
bundled with RGB and Depth camera, processes the skeletal data by keeping track
of 20 different body joints. Recognizing Gestures is done by verifying user
movements and checking them against predefined condition. Augmented Reality
module captures real-time image data streams from high resolution camera. This
module then generates 3D model that is superimposed on real time data.","['Saket Warade', 'Jagannath Aghav', 'Petitpierre Claude', 'Sandeep Udayagiri']",2012-07-11T11:17:28Z,http://arxiv.org/abs/1207.2597v1
"A Distributed Augmented Reality System for Medical Training and
  Simulation","Augmented Reality (AR) systems describe the class of systems that use
computers to overlay virtual information on the real world. AR environments
allow the development of promising tools in several application domains. In
medical training and simulation the learning potential of AR is significantly
amplified by the capability of the system to present 3D medical models in
real-time at remote locations. Furthermore the simulation applicability is
broadened by the use of real-time deformable medical models. This work presents
a distributed medical training prototype designed to train medical
practitioners' hand-eye coordination when performing endotracheal intubations.
The system we present accomplishes this task with the help of AR paradigms. An
extension of this prototype to medical simulations by employing deformable
medical models is possible. The shared state maintenance of the collaborative
AR environment is assured through a novel adaptive synchronization algorithm
(ASA) that increases the sense of presence among participants and facilitates
their interactivity in spite of infrastructure delays. The system will allow
paramedics, pre-hospital personnel, and students to practice their skills
without touching a real patient and will provide them with the visual feedback
they could not otherwise obtain. Such a distributed AR training tool has the
potential to: allow an instructor to simultaneously train local and remotely
located students and, allow students to actually ""see"" the internal anatomy and
therefore better understand their actions on a human patient simulator (HPS).","['Felix G. Hamza-Lup', 'Jannick P. Rolland', 'Charles Hughes']",2018-11-29T04:35:44Z,http://arxiv.org/abs/1811.12815v1
"Augmented Reality Chess Analyzer (ARChessAnalyzer): In-Device Inference
  of Physical Chess Game Positions through Board Segmentation and Piece
  Recognition using Convolutional Neural Network","Chess game position analysis is important in improving ones game. It requires
entry of moves into a chess engine which is, cumbersome and error prone. We
present ARChessAnalyzer, a complete pipeline from live image capture of a
physical chess game, to board and piece recognition, to move analysis and
finally to Augmented Reality (AR) overlay of the chess diagram position and
move on the physical board. ARChessAnalyzer is like a scene analyzer - it uses
an ensemble of traditional image and vision techniques to segment the scene (ie
the chess game) and uses Convolution Neural Networks (CNNs) to predict the
segmented pieces and combine it together to analyze the game. This paper
advances the state of the art in the first of its kind end to end integration
of robust detection and segmentation of the board, chess piece detection using
the fine-tuned AlexNet CNN and chess engine analyzer in a handheld device app.
The accuracy of the entire chess position prediction pipeline is 93.45\% and
takes 3-4.5sec from live capture to AR overlay. We also validated our
hypothesis that ARChessAnalyzer, is faster at analysis than manual entry for
all board positions for valid outcomes. Our hope is that the instantaneous
feedback this app provides will help chess learners worldwide at all levels
improve their game.",['Anav Mehta'],2020-08-18T20:05:06Z,http://arxiv.org/abs/2009.01649v1
"Understanding Gesture and Speech Multimodal Interactions for
  Manipulation Tasks in Augmented Reality Using Unconstrained Elicitation","This research establishes a better understanding of the syntax choices in
speech interactions and of how speech, gesture, and multimodal gesture and
speech interactions are produced by users in unconstrained object manipulation
environments using augmented reality. The work presents a multimodal
elicitation study conducted with 24 participants. The canonical referents for
translation, rotation, and scale were used along with some abstract referents
(create, destroy, and select). In this study time windows for gesture and
speech multimodal interactions are developed using the start and stop times of
gestures and speech as well as the stoke times for gestures. While gestures
commonly precede speech by 81 ms we find that the stroke of the gesture is
commonly within 10 ms of the start of speech. Indicating that the information
content of a gesture and its co-occurring speech are well aligned to each
other. Lastly, the trends across the most common proposals for each modality
are examined. Showing that the disagreement between proposals is often caused
by a variation of hand posture or syntax. Allowing us to present aliasing
recommendations to increase the percentage of users' natural interactions
captured by future multimodal interactive systems.","['Adam S. Williams', 'Francisco R. Ortega']",2020-09-14T17:21:24Z,http://arxiv.org/abs/2009.06591v3
"Virtual Borders: Accurate Definition of a Mobile Robot's Workspace Using
  Augmented Reality","We address the problem of interactively controlling the workspace of a mobile
robot to ensure a human-aware navigation. This is especially of relevance for
non-expert users living in human-robot shared spaces, e.g. home environments,
since they want to keep the control of their mobile robots, such as vacuum
cleaning or companion robots. Therefore, we introduce virtual borders that are
respected by a robot while performing its tasks. For this purpose, we employ a
RGB-D Google Tango tablet as human-robot interface in combination with an
augmented reality application to flexibly define virtual borders. We evaluated
our system with 15 non-expert users concerning accuracy, teaching time and
correctness and compared the results with other baseline methods based on
visual markers and a laser pointer. The experimental results show that our
method features an equally high accuracy while reducing the teaching time
significantly compared to the baseline methods. This holds for different border
lengths, shapes and variations in the teaching process. Finally, we
demonstrated the correctness of the approach, i.e. the mobile robot changes its
navigational behavior according to the user-defined virtual borders.","['Dennis Sprute', 'Klaus Tönnies', 'Matthias König']",2017-09-04T13:42:32Z,http://arxiv.org/abs/1709.00954v2
"6D Object Pose Estimation with Depth Images: A Seamless Approach for
  Robotic Interaction and Augmented Reality","To determine the 3D orientation and 3D location of objects in the
surroundings of a camera mounted on a robot or mobile device, we developed two
powerful algorithms in object detection and temporal tracking that are combined
seamlessly for robotic perception and interaction as well as Augmented Reality
(AR). A separate evaluation of, respectively, the object detection and the
temporal tracker demonstrates the important stride in research as well as the
impact on industrial robotic applications and AR. When evaluated on a standard
dataset, the detector produced the highest f1-score with a large margin while
the tracker generated the best accuracy at a very low latency of approximately
2 ms per frame with one CPU core: both algorithms outperforming the state of
the art. When combined, we achieve a powerful framework that is robust to
handle multiple instances of the same object under occlusion and clutter while
attaining real-time performance. Aiming at stepping beyond the simple scenarios
used by current systems, often constrained by having a single object in absence
of clutter, averting to touch the object to prevent close-range partial
occlusion, selecting brightly colored objects to easily segment them
individually or assuming that the object has simple geometric structure, we
demonstrate the capacity to handle challenging cases under clutter, partial
occlusion and varying lighting conditions with objects of different shapes and
sizes.","['David Joseph Tan', 'Nassir Navab', 'Federico Tombari']",2017-09-05T15:38:26Z,http://arxiv.org/abs/1709.01459v1
"The Effect of Focal Distance, Age, and Brightness on Near-Field
  Augmented Reality Depth Matching","Many augmented reality (AR) applications operate within near-field reaching
distances, and require matching the depth of a virtual object with a real
object. The accuracy of this matching was measured in three experiments, which
examined the effect of focal distance, age, and brightness, within distances of
33.3 to 50 cm, using a custom-built AR haploscope. Experiment I examined the
effect of focal demand, at the levels of collimated (infinite focal distance),
consistent with other depth cues, and at the midpoint of reaching distance.
Observers were too young to exhibit age-related reductions in accommodative
ability. The depth matches of collimated targets were increasingly
overestimated with increasing distance, consistent targets were slightly
underestimated, and midpoint targets were accurately estimated. Experiment II
replicated Experiment I, with older observers. Results were similar to
Experiment I. Experiment III replicated Experiment I with dimmer targets, using
young observers. Results were again consistent with Experiment I, except that
both consistent and midpoint targets were accurately estimated. In all cases,
collimated results were explained by a model, where the collimation biases the
eyes' vergence angle outwards by a constant amount. Focal demand and brightness
affect near-field AR depth matching, while age-related reductions in
accommodative ability have no effect.","['Gujot Singh', 'Stephen R. Ellis', 'J. Edward Swan II']",2017-11-30T21:24:22Z,http://arxiv.org/abs/1712.00088v1
"Closing the Calibration Loop: An Inside-out-tracking Paradigm for
  Augmented Reality in Orthopedic Surgery","In percutaneous orthopedic interventions the surgeon attempts to reduce and
fixate fractures in bony structures. The complexity of these interventions
arises when the surgeon performs the challenging task of navigating surgical
tools percutaneously only under the guidance of 2D interventional X-ray
imaging. Moreover, the intra-operatively acquired data is only visualized
indirectly on external displays. In this work, we propose a flexible Augmented
Reality (AR) paradigm using optical see-through head mounted displays. The key
technical contribution of this work includes the marker-less and dynamic
tracking concept which closes the calibration loop between patient, C-arm and
the surgeon. This calibration is enabled using Simultaneous Localization and
Mapping of the environment of the operating theater. In return, the proposed
solution provides in situ visualization of pre- and intra-operative 3D medical
data directly at the surgical site. We demonstrate pre-clinical evaluation of a
prototype system, and report errors for calibration and target registration.
Finally, we demonstrate the usefulness of the proposed inside-out tracking
system in achieving ""bull's eye"" view for C-arm-guided punctures. This AR
solution provides an intuitive visualization of the anatomy and can simplify
the hand-eye coordination for the orthopedic surgeon.","['Jonas Hajek', 'Mathias Unberath', 'Javad Fotouhi', 'Bastian Bier', 'Sing Chun Lee', 'Greg Osgood', 'Andreas Maier', 'Mehran Armand', 'Nassir Navab']",2018-03-22T23:15:59Z,http://arxiv.org/abs/1803.08610v1
"Augmented Reality Predictive Displays to Help Mitigate the Effects of
  Delayed Telesurgery","Surgical robots offer the exciting potential for remote telesurgery, but
advances are needed to make this technology efficient and accurate to ensure
patient safety. Achieving these goals is hindered by the deleterious effects of
latency between the remote operator and the bedside robot. Predictive displays
have found success in overcoming these effects by giving the operator immediate
visual feedback. However, previously developed predictive displays can not be
directly applied to telesurgery due to the unique challenges in tracking the 3D
geometry of the surgical environment. In this paper, we present the first
predictive display for teleoperated surgical robots. The predicted display is
stereoscopic, utilizes Augmented Reality (AR) to show the predicted motions
alongside the complex tissue found in-situ within surgical environments, and
overcomes the challenges in accurately tracking slave-tools in real-time. We
call this a Stereoscopic AR Predictive Display (SARPD). To test the SARPD's
performance, we conducted a user study with ten participants on the da
Vinci\textregistered{} Surgical System. The results showed with statistical
significance that using SARPD decreased time to complete task while having no
effect on error rates when operating under delay.","['Florian Richter', 'Yifei Zhang', 'Yuheng Zhi', 'Ryan K. Orosco', 'Michael C. Yip']",2018-09-23T16:36:23Z,http://arxiv.org/abs/1809.08627v3
"Ultrathin, polarization-independent, and focus-tunable liquid crystal
  diffractive lens for augmented reality","Despite the recent advances in augmented reality (AR), which has shown the
potential to significantly impact on our daily lives by offering a new way to
manipulate and interact with virtual information, minimizing visual discomfort
due to the vergence-accommodation conflict remains a challenge. Emerging AR
technologies often exploit focus-tunable optics to address this problem.
Although they demonstrated improved depth perception by enabling proper focus
cues, a bulky form factor of focus-tunable optics prevents their use in the
form of a pair of eyeglasses. Herein, we describe an ultrathin, focus-tunable
liquid crystal (LC) diffractive lens with a large aperture, a low weight, and a
low operating voltage. In addition, we show that the polarization dependence of
the lens, which is an inherent optical property of LC lenses, can be eliminated
using birefringent thin films as substrates and by aligning the optical axes of
the birefringent substrates and LC at a specific angle. The polarization
independence eliminates the need for a polarizer, thus further reducing the
form factor of the optical system. Next, we demonstrate a prototype of AR
glasses with addressable focal planes using the ultrathin lens. The prototype
AR glasses can adjust the accommodation distance of the virtual image,
mitigating the vergence-accommodation conflict without substantially
compromising the form factor or image quality. This research on ultrathin lens
technology shows promising potential for developing compact optical displays in
various applications.","['Mareddi Bharath Kumar', 'Daekyung Kang', 'Jihoon Jung', 'Hongsik Park', 'Joonku Hahn', 'Muhan Choi', 'Jin-Hyuk Bae', 'Hyunmin Kim', 'Jonghoo Park']",2019-02-28T04:38:22Z,http://arxiv.org/abs/1902.10889v1
"Non-user Inclusive Design for Maintaining Harmony of Real-Virtual Human
  Interaction in Augmented Reality","Augmented reality enables the illusion of contents such as objects and humans
in the virtual world co-existing with users in the real world. However,
non-users who are not aware of the presence of the virtual world and
dynamically move nearby might either cause a conflict by directly breaking into
space where a user is talking to a Virtual Human (VH), or be troubled when try
to avoid disturbing the user. To maintain harmony and keep both the user's and
non-users' comfort, we propose a method that controls the VH to adjust its own
position to avoid such potential conflict. The difficulty to address this
problem is that the agent must avoid potential conflict in a natural way to
keep the user away from feeling unnatural. Our idea is to endow the VH with
three capabilities: anticipating non-users walking around, understanding how to
establish and maintain proper formation to adapt to the environment, and
planning to avoid conflicts by shifting formation in advance. We develop a
non-user inclusive spatial formation model that realizes natural arrangement
shift corresponding to the environment based on theoretical sources from
literature. We implemented our proposed model into a VH behavior planning
system to achieve natural conflict avoidance. Evaluation experiments showed
that it successfully reduces potential conflicts caused by non-users.",['Chao Shi'],2019-06-28T08:32:22Z,http://arxiv.org/abs/1906.12088v1
"Plan in 2D, execute in 3D: An augmented reality solution for cup
  placement in total hip arthroplasty","Reproducibly achieving proper implant alignment is a critical step in total
hip arthroplasty (THA) procedures that has been shown to substantially affect
patient outcome. In current practice, correct alignment of the acetabular cup
is verified in C-arm X-ray images that are acquired in an anterior-posterior
(AP) view. Favorable surgical outcome is, therefore, heavily dependent on the
surgeon's experience in understanding the 3D orientation of a hemispheric
implant from 2D AP projection images. This work proposes an easy to use
intra-operative component planning system based on two C-arm X-ray images that
is combined with 3D augmented reality (AR) visualization that simplifies
impactor and cup placement according to the planning by providing a real-time
RGBD data overlay. We evaluate the feasibility of our system in a user study
comprising four orthopedic surgeons at the Johns Hopkins Hospital, and also
report errors in translation, anteversion, and abduction as low as 1.98 mm,
1.10 degrees, and 0.53 degrees, respectively. The promising performance of this
AR solution shows that deploying this system could eliminate the need for
excessive radiation, simplify the intervention, and enable reproducibly
accurate placement of acetabular implants.","['Javad Fotouhi', 'Clayton P. Alexander', 'Mathias Unberath', 'Giacomo Taylor', 'Sing Chun Lee', 'Bernhard Fuerst', 'Alex Johnson', 'Greg Osgood', 'Russell H. Taylor', 'Harpal Khanuja', 'Mehran Armand', 'Nassir Navab']",2018-01-04T22:00:20Z,http://arxiv.org/abs/1801.01557v1
"Towards Commodity, Web-Based Augmented Reality Applications for Research
  and Education in Chemistry and Structural Biology","This article reports prototype web apps that use commodity, open-source
technologies for augmented and virtual reality to provide immersive,
interactive human-computer interfaces for chemistry, structural biology and
related disciplines. The examples, which run in any standard web browser and
are accessible at
https://lucianoabriata.altervista.org/jsinscience/arjs/armodeling/ together
with demo videos, showcase applications that could go well beyond pedagogy,
i.e. advancing actual utility in research settings: molecular visualization at
atomistic and coarse-grained levels in interactive immersive 3D, coarse-grained
modeling of molecular physics and chemistry, and on-the-fly calculation of
experimental observables and overlay onto experimental data. From this
playground, I depict perspectives on how these emerging technologies might
couple in the future to neural network-based quantum mechanical calculations,
advanced forms of human-computer interaction such as speech-based
communication, and sockets for concurrent collaboration through the internet
-all technologies that are today maturing in web browsers- to deliver the next
generation of tools for truly interactive, immersive molecular modeling that
can streamline human thought and intent with the numerical processing power of
computers.",['Luciano A. Abriata'],2018-06-21T17:21:17Z,http://arxiv.org/abs/1806.08332v5
"Augmented Reality-based Feedback for Technician-in-the-loop C-arm
  Repositioning","Interventional C-arm imaging is crucial to percutaneous orthopedic procedures
as it enables the surgeon to monitor the progress of surgery on the anatomy
level. Minimally invasive interventions require repeated acquisition of X-ray
images from different anatomical views to verify tool placement. Achieving and
reproducing these views often comes at the cost of increased surgical time and
radiation dose to both patient and staff. This work proposes a marker-free
""technician-in-the-loop"" Augmented Reality (AR) solution for C-arm
repositioning. The X-ray technician operating the C-arm interventionally is
equipped with a head-mounted display capable of recording desired C-arm poses
in 3D via an integrated infrared sensor. For C-arm repositioning to a
particular target view, the recorded C-arm pose is restored as a virtual object
and visualized in an AR environment, serving as a perceptual reference for the
technician. We conduct experiments in a setting simulating orthopedic trauma
surgery. Our proof-of-principle findings indicate that the proposed system can
decrease the 2.76 X-ray images required per desired view down to zero,
suggesting substantial reductions of radiation dose during C-arm repositioning.
The proposed AR solution is a first step towards facilitating communication
between the surgeon and the surgical staff, improving the quality of surgical
image acquisition, and enabling context-aware guidance for surgery rooms of the
future. The concept of technician-in-the-loop design will become relevant to
various interventions considering the expected advancements of sensing and
wearable computing in the near future.","['Mathias Unberath', 'Javad Fotouhi', 'Jonas Hajek', 'Andreas Maier', 'Greg Osgood', 'Russell Taylor', 'Mehran Armand', 'Nassir Navab']",2018-06-22T18:34:48Z,http://arxiv.org/abs/1806.08814v1
Augmented Reality Prosthesis Training Setup for Motor Skill Enhancement,"Adjusting to amputation can often time be difficult for the body.
Post-surgery, amputees have to wait for up to several months before receiving a
properly fitted prosthesis. In recent years, there has been a trend toward
quantitative outcome measures. In this paper, we developed the augmented
reality (AR) version of one such measure, the Prosthetic Hand Assessment
Measure (PHAM). The AR version of the PHAM - HoloPHAM, offers amputees the
advantage to train with pattern recognition, at their own time and convenience,
pre- and post-prosthesis fitting. We provide a rigorous analysis of our system,
focusing on its ability to simulate reach, grasp, and touch in AR. Similarity
of motion joint dynamics for reach in physical and AR space were compared, with
experiments conducted to illustrate how depth in AR is perceived. To show the
effectiveness and validity of our system for prosthesis training, we conducted
a 10-day study with able-bodied subjects (N = 3) to see the effect that
training on the HoloPHAM had on other established functional outcome measures.
A washout phase of 5 days was incorporated to observe the effect without
training. Comparisons were made with standardized outcome metrics, along with
the progression of kinematic variability over time. Statistically significant
(p<0.05) improvements were observed between pre- and post-training stages. Our
results show that AR can be an effective tool for prosthesis training with
pattern recognition systems, fostering motor learning for reaching movement
tasks, and paving the possibility of replacing physical training.","['Avinash Sharma', 'Wally Niu', 'Christopher L. Hunt', 'George Levay', 'Rahul Kaliki', 'Nitish V. Thakor']",2019-03-05T18:25:39Z,http://arxiv.org/abs/1903.01968v1
Semi-Automatic Labeling for Deep Learning in Robotics,"In this paper, we propose Augmented Reality Semi-automatic labeling (ARS), a
semi-automatic method which leverages on moving a 2D camera by means of a
robot, proving precise camera tracking, and an augmented reality pen to define
initial object bounding box, to create large labeled datasets with minimal
human intervention. By removing the burden of generating annotated data from
humans, we make the Deep Learning technique applied to computer vision, that
typically requires very large datasets, truly automated and reliable. With the
ARS pipeline, we created effortlessly two novel datasets, one on
electromechanical components (industrial scenario) and one on fruits
(daily-living scenario), and trained robustly two state-of-the-art object
detectors, based on convolutional neural networks, such as YOLO and SSD. With
respect to the conventional manual annotation of 1000 frames that takes us
slightly more than 10 hours, the proposed approach based on ARS allows
annotating 9 sequences of about 35000 frames in less than one hour, with a gain
factor of about 450. Moreover, both the precision and recall of object
detection is increased by about 15\% with respect to manual labeling. All our
software is available as a ROS package in a public repository alongside the
novel annotated datasets.","['Daniele De Gregorio', 'Alessio Tonioni', 'Gianluca Palli', 'Luigi Di Stefano']",2019-08-05T21:10:12Z,http://arxiv.org/abs/1908.01862v1
"Image-based marker tracking and registration for intraoperative 3D
  image-guided interventions using augmented reality","Augmented reality has the potential to improve operating room workflow by
allowing physicians to ""see"" inside a patient through the projection of imaging
directly onto the surgical field. For this to be useful the acquired imaging
must be quickly and accurately registered with patient and the registration
must be maintained. Here we describe a method for projecting a CT scan with
Microsoft Hololens and then aligning that projection to a set of fiduciary
markers. Radio-opaque stickers with unique QR-codes are placed on an object
prior to acquiring a CT scan. The location of the markers in the CT scan are
extracted and the CT scan is converted into a 3D surface object. The 3D object
is then projected using the Hololens onto a table on which the same markers are
placed. We designed an algorithm that aligns the markers on the 3D object with
the markers on the table. To extract the markers and convert the CT into a 3D
object took less than 5 seconds. To align three markers, it took $0.9 \pm 0.2$
seconds to achieve an accuracy of $5 \pm 2$ mm. These findings show that it is
feasible to use a combined radio-opaque optical marker, placed on a patient
prior to a CT scan, to subsequently align the acquired CT scan with the
patient.","['Andong Cao', 'Ali Dhanaliwala', 'Jianbo Shi', 'Terence Gade', 'Brian Park']",2019-08-08T18:57:34Z,http://arxiv.org/abs/1908.03237v1
AiR -- An Augmented Reality Application for Visualizing Air Pollution,"Air quality is a term used to describe the concentration levels of various
pollutants in the air we breathe. The air quality, which is degrading rapidly
across the globe, has been a source of great concern. Across the globe,
governments are taking various measures to reduce air pollution. Bringing
awareness about environmental pollution among the public plays a major role in
controlling air pollution, as the programs proposed by governments require the
support of the public. Though information on air quality is present on multiple
portals such as the Central Pollution Control Board (CPCB), which provides Air
Quality Index that could be accessed by the public. However, such portals are
scarcely visited by the general public. Visualizing air quality in the location
where an individual resides could help in bringing awareness among the public.
This visualization could be rendered using Augmented Reality techniques.
Considering the widespread usage of Android based mobile devices in India, and
the importance of air quality visualization, we present AiR, as an Android
based mobile application. AiR considers the air quality measured by CPCB, in a
locality that is detected by the user's GPS or in a locality of user's choice,
and visualizes various air pollutants present in the locality $(PM_1{}_0,
PM_2{}_.{}_5, NO_2, SO_2, CO, O_3 \& NH_3)$ and displays them in the user's
surroundings. AiR also creates awareness in an interactive manner about the
different pollutants, sources, and their impacts on health.","['Noble Saji Mathews', 'Sridhar Chimalakonda', 'Suresh Jain']",2020-06-03T10:03:47Z,http://arxiv.org/abs/2006.02136v1
Evaluating Transport Protocols on 5G for Mobile Augmented Reality,"Mobile Augmented Reality (MAR) mixes physical environments with
user-interactive virtual annotations. Immersive MAR experiences are supported
by computation-intensive tasks which rely on offloading mechanisms to ease
device workloads. However, this introduces additional network traffic which in
turn influences the motion-to-photon latency (a determinant of user-perceived
quality of experience). Therefore, a proper transport protocol is crucial to
minimise transmission latency and ensure sufficient throughput to support MAR
performance. Relatedly, 5G, a potential MAR supporting technology, is widely
believed to be smarter, faster, and more efficient than its predecessors.
However, the suitability and performance of existing transport protocols in MAR
in the 5G context has not been explored. Therefore, we present an evaluation of
popular transport protocols, including UDP, TCP, MPEG-TS, RTP, and QUIC, with a
MAR system on a real-world 5G testbed. We also compare with their 5G
performance with LTE and WiFi. Our evaluation results indicate that TCP has the
lowest round-trip-time on 5G, with a median of $15.09\pm0.26$ ms, while QUIC
appears to perform better on LTE. Through an additional test with varying
signal quality (specifically, degrading secondary synchronisation signal
reference signal received quality), we discover that protocol performance
appears to be significantly impacted by signal quality.","['Jacky Cao', 'Xiang Su', 'Benjamin Finley', 'Pengyuan Zhou', 'Pan Hui']",2020-06-04T13:57:50Z,http://arxiv.org/abs/2006.02859v1
RGB-D-E: Event Camera Calibration for Fast 6-DOF Object Tracking,"Augmented reality devices require multiple sensors to perform various tasks
such as localization and tracking. Currently, popular cameras are mostly
frame-based (e.g. RGB and Depth) which impose a high data bandwidth and power
usage. With the necessity for low power and more responsive augmented reality
systems, using solely frame-based sensors imposes limits to the various
algorithms that needs high frequency data from the environement. As such,
event-based sensors have become increasingly popular due to their low power,
bandwidth and latency, as well as their very high frequency data acquisition
capabilities. In this paper, we propose, for the first time, to use an
event-based camera to increase the speed of 3D object tracking in 6 degrees of
freedom. This application requires handling very high object speed to convey
compelling AR experiences. To this end, we propose a new system which combines
a recent RGB-D sensor (Kinect Azure) with an event camera (DAVIS346). We
develop a deep learning approach, which combines an existing RGB-D network
along with a novel event-based network in a cascade fashion, and demonstrate
that our approach significantly improves the robustness of a state-of-the-art
frame-based 6-DOF object tracker using our RGB-D-E pipeline.","['Etienne Dubeau', 'Mathieu Garon', 'Benoit Debaque', 'Raoul de Charette', 'Jean-François Lalonde']",2020-06-09T01:55:48Z,http://arxiv.org/abs/2006.05011v2
"Joint Scene and Object Tracking for Cost-Effective Augmented Reality
  Assisted Patient Positioning in Radiation Therapy","BACKGROUND AND OBJECTIVE: The research done in the field of Augmented Reality
(AR) for patient positioning in radiation therapy is scarce. We propose an
efficient and cost-effective algorithm for tracking the scene and the patient
to interactively assist the patient's positioning process by providing visual
feedback to the operator. Up to our knowledge, this is the first framework that
can be employed for mobile interactive AR to guide patient positioning.
METHODS: We propose a point cloud processing method that combined with a
fiducial marker-mapper algorithm and the generalized ICP algorithm tracks the
patient and the camera precisely and efficiently only using the CPU unit. The
alignment between the 3D reference model and body marker map is calculated
employing an efficient body reconstruction algorithm. RESULTS: Our quantitative
evaluation shows that the proposed method achieves a translational and
rotational error of 4.17 mm/0.82 deg at 9 fps. Furthermore, the qualitative
results demonstrate the usefulness of our algorithm in patient positioning on
different human subjects. CONCLUSION: Since our algorithm achieves a relatively
high frame rate and accuracy employing a regular laptop (without the usage of a
dedicated GPU), it is a very cost-effective AR-based patient positioning
method. It also opens the way for other researchers by introducing a framework
that could be improved upon for better mobile interactive AR patient
positioning solutions in the future.","['Hamid Sarmadi', 'Rafael Muñoz-Salinas', 'M. Álvaro Berbís', 'Antonio Luna', 'R. Medina-Carnicer']",2020-10-05T10:20:46Z,http://arxiv.org/abs/2010.01895v2
"Evaluating Mixed and Augmented Reality: A Systematic Literature Review
  (2009-2019)","We present a systematic review of 458 papers that report on evaluations in
mixed and augmented reality (MR/AR) published in ISMAR, CHI, IEEE VR, and UIST
over a span of 11 years (2009-2019). Our goal is to provide guidance for future
evaluations of MR/AR approaches. To this end, we characterize publications by
paper type (e.g., technique, design study), research topic (e.g., tracking,
rendering), evaluation scenario (e.g., algorithm performance, user
performance), cognitive aspects (e.g., perception, emotion), and the context in
which evaluations were conducted (e.g., lab vs. in-the-wild). We found a strong
coupling of types, topics, and scenarios. We observe two groups: (a)
technology-centric performance evaluations of algorithms that focus on
improving tracking, displays, reconstruction, rendering, and calibration, and
(b) human-centric studies that analyze implications of applications and design,
human factors on perception, usability, decision making, emotion, and
attention. Amongst the 458 papers, we identified 248 user studies that involved
5,761 participants in total, of whom only 1,619 were identified as female. We
identified 43 data collection methods used to analyze 10 cognitive aspects. We
found nine objective methods, and eight methods that support qualitative
analysis. A majority (216/248) of user studies are conducted in a laboratory
setting. Often (138/248), such studies involve participants in a static way.
However, we also found a fair number (30/248) of in-the-wild studies that
involve participants in a mobile fashion. We consider this paper to be relevant
to academia and industry alike in presenting the state-of-the-art and guiding
the steps to designing, conducting, and analyzing results of evaluations in
MR/AR.","['Leonel Merino', 'Magdalena Schwarzl', 'Matthias Kraus', 'Michael Sedlmair', 'Dieter Schmalstieg', 'Daniel Weiskopf']",2020-10-12T19:30:46Z,http://arxiv.org/abs/2010.05988v1
On the Deployability of Augmented Reality Using Embedded Edge Devices,"Edge Computing exploits computational capabilities deployed at the very edge
of the network to support applications with low latency requirements. Such
capabilities can reside in small embedded devices that integrate dedicated
hardware -- e.g., a GPU -- in a low cost package. But these devices have
limited computing capabilities compared to standard server grade equipment.
When deploying an Edge Computing based application, understanding whether the
available hardware can meet target requirements is key in meeting the expected
performance. In this paper, we study the feasibility of deploying Augmented
Reality applications using Embedded Edge Devices (EEDs). We compare such
deployment approach to one exploiting a standard dedicated server grade
machine. Starting from an empirical evaluation of the capabilities of these
devices, we propose a simple theoretical model to compare the performance of
the two approaches. We then validate such model with NS-3 simulations and study
their feasibility. Our results show that there is no one-fits-all solution. If
we need to deploy high responsiveness applications, we need a centralized
server grade architecture and we can in any case only support very few users.
The centralized architecture fails to serve a larger number of users, even when
low to mid responsiveness is required. In this case, we need to resort instead
to a distributed deployment based on EEDs.","['Ayoub Ben-Ameur', 'Andrea Araldo', 'Francesco Bronzino']",2020-10-28T09:32:53Z,http://arxiv.org/abs/2010.14845v2
HDR Environment Map Estimation for Real-Time Augmented Reality,"We present a method to estimate an HDR environment map from a narrow
field-of-view LDR camera image in real-time. This enables perceptually
appealing reflections and shading on virtual objects of any material finish,
from mirror to diffuse, rendered into a real physical environment using
augmented reality. Our method is based on our efficient convolutional neural
network architecture, EnvMapNet, trained end-to-end with two novel losses,
ProjectionLoss for the generated image, and ClusterLoss for adversarial
training. Through qualitative and quantitative comparison to state-of-the-art
methods, we demonstrate that our algorithm reduces the directional error of
estimated light sources by more than 50%, and achieves 3.7 times lower Frechet
Inception Distance (FID). We further showcase a mobile application that is able
to run our neural network model in under 9 ms on an iPhone XS, and render in
real-time, visually coherent virtual objects in previously unseen real-world
environments.","['Gowri Somanath', 'Daniel Kurz']",2020-11-21T01:01:53Z,http://arxiv.org/abs/2011.10687v5
"Towards the Development of 3D Engine Assembly Simulation Learning Module
  for Senior High School","The focus of the study is to develop a 3D engine assembly simulation learning
module to address the lack of equipment in one senior high school in the
Philippines. The study used mixed-method to determine the considerations needed
in developing an application for educational use particularly among
laboratory/practical subjects like engine assembly. The study used ISO 25010
quality standards in evaluating the application(n=153 students and 3 ICT
experts).Results showed that the application is moderately acceptable(overall
mean = 3.52) under ISO 25010 quality standards. The study created an engine
simulation learning assembly in which teachers can use to augment their lesson.
The study also highlights the applicability of using 3D-related technologies
for practical and laboratory subjects particularly highly technical-related
subjects. Future studies may develop a similar application in the same context
using mobile and other emerging technology(i.e., Virtual Reality, Augmented
Reality) as well as making the content more customizable. Effectivity of the
system in an actual setting is also worth pursuing. The study highlighted the
potential use of 3D technology in a classroom setting.","['John Paul P. Miranda', 'Jaymark A. Yambao', 'Jhon Asley M. Marcelo', 'Christopher Robert N. Gonzales', 'Vee-jay T. Mungcal']",2020-11-19T10:45:29Z,http://arxiv.org/abs/2011.12767v1
"cMinMax: A Fast Algorithm to Find the Corners of an N-dimensional Convex
  Polytope","During the last years, the emerging field of Augmented & Virtual Reality
(AR-VR) has seen tremendousgrowth. At the same time there is a trend to develop
low cost high-quality AR systems where computing poweris in demand. Feature
points are extensively used in these real-time frame-rate and 3D applications,
thereforeefficient high-speed feature detectors are necessary. Corners are such
special features and often are used as thefirst step in the marker alignment in
Augmented Reality (AR). Corners are also used in image registration
andrecognition, tracking, SLAM, robot path finding and 2D or 3D object
detection and retrieval. Therefore thereis a large number of corner detection
algorithms but most of them are too computationally intensive for use
inreal-time applications of any complexity. Many times the border of the image
is a convex polygon. For thisspecial, but quite common case, we have developed
a specific algorithm, cMinMax. The proposed algorithmis faster, approximately
by a factor of 5 compared to the widely used Harris Corner Detection algorithm.
Inaddition is highly parallelizable. The algorithm is suitable for the fast
registration of markers in augmentedreality systems and in applications where a
computationally efficient real time feature detector is necessary.The algorithm
can also be extended to N-dimensional polyhedrons.","['Dimitrios Chamzas', 'Constantinos Chamzas', 'Konstantinos Moustakas']",2020-11-28T00:32:11Z,http://arxiv.org/abs/2011.14035v3
5G MEC Computation Handoff for Mobile Augmented Reality,"The combination of 5G and Multi-access Edge Computing (MEC) can significantly
reduce application delay by lowering transmission delay and bringing
computational capabilities closer to the end user. Therefore, 5G MEC could
enable excellent user experience in applications like Mobile Augmented Reality
(MAR), which are computation-intensive, and delay and jitter-sensitive.
However, existing 5G handoff algorithms often do not consider the computational
load of MEC servers, are too complex for real-time execution, or do not
integrate easily with the standard protocol stack. Thus they can impair the
performance of 5G MEC. To address this gap, we propose Comp-HO, a handoff
algorithm that finds a local solution to the joint problem of optimizing signal
strength and computational load. Additionally, Comp-HO can easily be integrated
into current LTE and 5G base stations thanks to its simplicity and
standard-friendly deployability. Specifically, we evaluate Comp-HO through a
custom NS-3 simulator which we calibrate via MAR prototype measurements from a
real-world 5G testbed. We simulate both Comp-HO and several classic handoff
algorithms. The results show that, even without a global optimum, the proposed
algorithm still significantly reduces the number of large delays, caused by
congestion at MECs, at the expense of a small increase in transmission delay.","['Pengyuan Zhou', 'Shuhao Fu', 'Benjamin Finley', 'Xuebing Li', 'Sasu Tarkoma', 'Jussi Kangasharju', 'Mostafa Ammar', 'Pan Hui']",2021-01-01T15:56:25Z,http://arxiv.org/abs/2101.00256v2
"Millimeter Wave MIMO based Depth Maps for Wireless Virtual and Augmented
  Reality","Augmented and virtual reality systems (AR/VR) are rapidly becoming key
components of the wireless landscape. For immersive AR/VR experience, these
devices should be able to construct accurate depth perception of the
surrounding environment. Current AR/VR devices rely heavily on using RGB-D
depth cameras to achieve this goal. The performance of these depth cameras,
however, has clear limitations in several scenarios, such as the cases with
shiny objects, dark surfaces, and abrupt color transition among other
limitations. In this paper, we propose a novel solution for AR/VR depth map
construction using mmWave MIMO communication transceivers. This is motivated by
the deployment of advanced mmWave communication systems in future AR/VR devices
for meeting the high data rate demands and by the interesting propagation
characteristics of mmWave signals. Accounting for the constraints on these
systems, we develop a comprehensive framework for constructing accurate and
high-resolution depth maps using mmWave systems. In this framework, we
developed new sensing beamforming codebook approaches that are specific for the
depth map construction objective. Using these codebooks, and leveraging tools
from successive interference cancellation, we develop a joint beam processing
approach that can construct high-resolution depth maps using practical mmWave
antenna arrays. Extensive simulation results highlight the potential of the
proposed solution in building accurate depth maps. Further, these simulations
show the promising gains of mmWave based depth perception compared to RGB-based
approaches in several important use cases.","['Abdelrahman Taha', 'Qi Qu', 'Sam Alex', 'Ping Wang', 'William L. Abbott', 'Ahmed Alkhateeb']",2021-02-11T18:57:58Z,http://arxiv.org/abs/2102.06198v2
Context-Responsive Labeling in Augmented Reality,"Route planning and navigation are common tasks that often require additional
information on points of interest. Augmented Reality (AR) enables mobile users
to utilize text labels, in order to provide a composite view associated with
additional information in a real-world environment. Nonetheless, displaying all
labels for points of interest on a mobile device will lead to unwanted overlaps
between information, and thus a context-responsive strategy to properly arrange
labels is expected. The technique should remove overlaps, show the right
level-of-detail, and maintain label coherence. This is necessary as the viewing
angle in an AR system may change rapidly due to users' behaviors. Coherence
plays an essential role in retaining user experience and knowledge, as well as
avoiding motion sickness. In this paper, we develop an approach that
systematically manages label visibility and levels-of-detail, as well as
eliminates unexpected incoherent movement. We introduce three label management
strategies, including (1) occlusion management, (2) level-of-detail management,
and (3) coherence management by balancing the usage of the mobile phone screen.
A greedy approach is developed for fast occlusion handling in AR. A
level-of-detail scheme is adopted to arrange various types of labels. A 3D
scene manipulation is then built to simultaneously suppress the incoherent
behaviors induced by viewing angle changes. Finally, we present the feasibility
and applicability of our approach through one synthetic and two real-world
scenarios, followed by a qualitative user study.","['Thomas Köppel', 'M. Eduard Gröller', 'Hsiang-Yun Wu']",2021-02-15T18:33:19Z,http://arxiv.org/abs/2102.07735v1
Towards augmented reality for corporate training,"Corporate training relates to employees acquiring essential skills to operate
equipment or effectively performing required tasks both competently and safely.
Unlike formal education, training can be incorporated into the task workflow
and performed during working hours. Increasingly, organizations adopt different
technologies to develop both individual skills and improve their organization.
Studies indicate that Augmented Reality (AR) is quickly becoming an effective
technology for training programs. This systematic literature review (SLR) aims
to screen works published on AR for corporate training. We describe AR training
applications, discuss current challenges, literature gaps, opportunities, and
tendencies of corporate AR solutions. We structured a protocol to define
keywords, the semantics of research, and databases used as sources of this SLR.
From a primary analysis, we considered 1952 articles in the review for
qualitative synthesis. We selected 60 among the selected articles for this
study. The survey shows a large number of 41.7% of applications focused on
automotive and medical training. Additionally, 20% of selected publications use
a camera-display with a tablet device, while 40% refer to
head-mounted-displays, and many surveyed approaches (45%) adopt marker-based
tracking. Results indicate that publications on AR for corporate training
increased significantly in recent years. AR has been used in many areas,
exhibiting high quality and provides viable approaches to On-The-Job training.
Finally, we discuss future research issues related to increasing relevance
regarding AR for corporate training.","['Bruno R. Martins', 'Joaquim A. Jorge', 'Ezequiel R. Zorzal']",2021-02-18T16:19:27Z,http://arxiv.org/abs/2102.09453v1
AR Mapping: Accurate and Efficient Mapping for Augmented Reality,"Augmented reality (AR) has gained increasingly attention from both research
and industry communities. By overlaying digital information and content onto
the physical world, AR enables users to experience the world in a more
informative and efficient manner. As a major building block for AR systems,
localization aims at determining the device's pose from a pre-built ""map""
consisting of visual and depth information in a known environment. While the
localization problem has been widely studied in the literature, the ""map"" for
AR systems is rarely discussed. In this paper, we introduce the AR Map for a
specific scene to be composed of 1) color images with 6-DOF poses; 2) dense
depth maps for each image and 3) a complete point cloud map. We then propose an
efficient end-to-end solution to generating and evaluating AR Maps. Firstly,
for efficient data capture, a backpack scanning device is presented with a
unified calibration pipeline. Secondly, we propose an AR mapping pipeline which
takes the input from the scanning device and produces accurate AR Maps.
Finally, we present an approach to evaluating the accuracy of AR Maps with the
help of the highly accurate reconstruction result from a high-end laser
scanner. To the best of our knowledge, it is the first time to present an
end-to-end solution to efficient and accurate mapping for AR applications.","['Rui Huang', 'Chuan Fang', 'Kejie Qiu', 'Le Cui', 'Zilong Dong', 'Siyu Zhu', 'Ping Tan']",2021-03-27T08:57:48Z,http://arxiv.org/abs/2103.14846v1
3DARVisualizer: Debugging 3D Models using Augmented Reality,"Often neglected in traditional education, spatial thinking has played a
critical role in science, technology, engineering, and mathematics (STEM)
education. Spatial thinking skills can be enhanced by training, life
experience, and practice. One approach to train these skills is through 3D
modeling (also known as Computer-Aided Design or CAD). Although 3D modeling
tools have shown promising results in training and enhancing spatial thinking
skills in undergraduate engineering students when it comes to novices,
especially middle and high-school students, they are not sufficient to provide
rich 3D experience since the 3D models created in CAD are isolated the actual
3D physical world. Resulting in novice students finding it difficult to create
error-free 3D models that would 3D print successfully. This leads to student
frustration where students are not motivated to create 3D models themselves;
instead, they prefer to download them from online repositories. To address this
problem, researchers are focusing on integrating 3D models and displays into
the physical world with the help of technologies like Augmented Reality (AR).
In this demo, we present an AR application, 3DARVisualizer, that helps us
explore the role of AR as a 3D model debugger, including enhancing 3D modeling
abilities and spatial thinking skills of middle- and high-school students.","['Srinjita Bhaduri', 'Peter Gyory', 'Tamara Sumner']",2021-05-22T18:15:39Z,http://arxiv.org/abs/2105.10783v2
"Mobile Augmented Reality: User Interfaces, Frameworks, and Intelligence","Mobile Augmented Reality (MAR) integrates computer-generated virtual objects
with physical environments for mobile devices. MAR systems enable users to
interact with MAR devices, such as smartphones and head-worn wearables, and
performs seamless transitions from the physical world to a mixed world with
digital entities. These MAR systems support user experiences by using MAR
devices to provide universal accessibility to digital contents. Over the past
20 years, a number of MAR systems have been developed, however, the studies and
design of MAR frameworks have not yet been systematically reviewed from the
perspective of user-centric design. This article presents the first effort of
surveying existing MAR frameworks (count: 37) and further discusses the latest
studies on MAR through a top-down approach: 1) MAR applications; 2) MAR
visualisation techniques adaptive to user mobility and contexts; 3) systematic
evaluation of MAR frameworks including supported platforms and corresponding
features such as tracking, feature extraction plus sensing capabilities; and 4)
underlying machine learning approaches supporting intelligent operations within
MAR systems. Finally, we summarise the development of emerging research fields,
current state-of-the-art, and discuss the important open challenges and
possible theoretical and technical directions. This survey aims to benefit both
researchers and MAR system developers alike.","['Jacky Cao', 'Kit-Yung Lam', 'Lik-Hang Lee', 'Xiaoli Liu', 'Pan Hui', 'Xiang Su']",2021-06-16T11:26:37Z,http://arxiv.org/abs/2106.08710v1
"Xihe: A 3D Vision-based Lighting Estimation Framework for Mobile
  Augmented Reality","Omnidirectional lighting provides the foundation for achieving
spatially-variant photorealistic 3D rendering, a desirable property for mobile
augmented reality applications. However, in practice, estimating
omnidirectional lighting can be challenging due to limitations such as partial
panoramas of the rendering positions, and the inherent environment lighting and
mobile user dynamics. A new opportunity arises recently with the advancements
in mobile 3D vision, including built-in high-accuracy depth sensors and deep
learning-powered algorithms, which provide the means to better sense and
understand the physical surroundings. Centering the key idea of 3D vision, in
this work, we design an edge-assisted framework called Xihe to provide mobile
AR applications the ability to obtain accurate omnidirectional lighting
estimation in real time. Specifically, we develop a novel sampling technique
that efficiently compresses the raw point cloud input generated at the mobile
device. This technique is derived based on our empirical analysis of a recent
3D indoor dataset and plays a key role in our 3D vision-based lighting
estimator pipeline design. To achieve the real-time goal, we develop a tailored
GPU pipeline for on-device point cloud processing and use an encoding technique
that reduces network transmitted bytes. Finally, we present an adaptive
triggering strategy that allows Xihe to skip unnecessary lighting estimations
and a practical way to provide temporal coherent rendering integration with the
mobile AR ecosystem. We evaluate both the lighting estimation accuracy and time
of Xihe using a reference mobile application developed with Xihe's APIs. Our
results show that Xihe takes as fast as 20.67ms per lighting estimation and
achieves 9.4% better estimation accuracy than a state-of-the-art neural
network.","['Yiqin Zhao', 'Tian Guo']",2021-05-30T13:48:29Z,http://arxiv.org/abs/2106.15280v1
"EasyCom: An Augmented Reality Dataset to Support Algorithms for Easy
  Communication in Noisy Environments","Augmented Reality (AR) as a platform has the potential to facilitate the
reduction of the cocktail party effect. Future AR headsets could potentially
leverage information from an array of sensors spanning many different
modalities. Training and testing signal processing and machine learning
algorithms on tasks such as beam-forming and speech enhancement require high
quality representative data. To the best of the author's knowledge, as of
publication there are no available datasets that contain synchronized
egocentric multi-channel audio and video with dynamic movement and
conversations in a noisy environment. In this work, we describe, evaluate and
release a dataset that contains over 5 hours of multi-modal data useful for
training and testing algorithms for the application of improving conversations
for an AR glasses wearer. We provide speech intelligibility, quality and
signal-to-noise ratio improvement results for a baseline method and show
improvements across all tested metrics. The dataset we are releasing contains
AR glasses egocentric multi-channel microphone array audio, wide field-of-view
RGB video, speech source pose, headset microphone audio, annotated voice
activity, speech transcriptions, head bounding boxes, target of speech and
source identification labels. We have created and are releasing this dataset to
facilitate research in multi-modal AR solutions to the cocktail party problem.","['Jacob Donley', 'Vladimir Tourbabin', 'Jung-Suk Lee', 'Mark Broyles', 'Hao Jiang', 'Jie Shen', 'Maja Pantic', 'Vamsi Krishna Ithapu', 'Ravish Mehra']",2021-07-09T02:00:47Z,http://arxiv.org/abs/2107.04174v2
"Integrating Deep Learning and Augmented Reality to Enhance Situational
  Awareness in Firefighting Environments","We present a new four-pronged approach to build firefighter's situational
awareness for the first time in the literature. We construct a series of deep
learning frameworks built on top of one another to enhance the safety,
efficiency, and successful completion of rescue missions conducted by
firefighters in emergency first response settings. First, we used a deep
Convolutional Neural Network (CNN) system to classify and identify objects of
interest from thermal imagery in real-time. Next, we extended this CNN
framework for object detection, tracking, segmentation with a Mask RCNN
framework, and scene description with a multimodal natural language
processing(NLP) framework. Third, we built a deep Q-learning-based agent,
immune to stress-induced disorientation and anxiety, capable of making clear
navigation decisions based on the observed and stored facts in live-fire
environments. Finally, we used a low computational unsupervised learning
technique called tensor decomposition to perform meaningful feature extraction
for anomaly detection in real-time. With these ad-hoc deep learning structures,
we built the artificial intelligence system's backbone for firefighters'
situational awareness. To bring the designed system into usage by firefighters,
we designed a physical structure where the processed results are used as inputs
in the creation of an augmented reality capable of advising firefighters of
their location and key features around them, which are vital to the rescue
operation at hand, as well as a path planning feature that acts as a virtual
guide to assist disoriented first responders in getting back to safety. When
combined, these four approaches present a novel approach to information
understanding, transfer, and synthesis that could dramatically improve
firefighter response and efficacy and reduce life loss.",['Manish Bhattarai'],2021-07-23T06:35:13Z,http://arxiv.org/abs/2107.11043v2
"Communicating Inferred Goals with Passive Augmented Reality and Active
  Haptic Feedback","Robots learn as they interact with humans. Consider a human teleoperating an
assistive robot arm: as the human guides and corrects the arm's motion, the
robot gathers information about the human's desired task. But how does the
human know what their robot has inferred? Today's approaches often focus on
conveying intent: for instance, upon legible motions or gestures to indicate
what the robot is planning. However, closing the loop on robot inference
requires more than just revealing the robot's current policy: the robot should
also display the alternatives it thinks are likely, and prompt the human
teacher when additional guidance is necessary. In this paper we propose a
multimodal approach for communicating robot inference that combines both
passive and active feedback. Specifically, we leverage information-rich
augmented reality to passively visualize what the robot has inferred, and
attention-grabbing haptic wristbands to actively prompt and direct the human's
teaching. We apply our system to shared autonomy tasks where the robot must
infer the human's goal in real-time. Within this context, we integrate passive
and active modalities into a single algorithmic framework that determines when
and which type of feedback to provide. Combining both passive and active
feedback experimentally outperforms single modality baselines; during an
in-person user study, we demonstrate that our integrated approach increases how
efficiently humans teach the robot while simultaneously decreasing the amount
of time humans spend interacting with the robot. Videos here:
https://youtu.be/swq_u4iIP-g","['James F. Mullen Jr', 'Josh Mosier', 'Sounak Chakrabarti', 'Anqi Chen', 'Tyler White', 'Dylan P. Losey']",2021-09-03T22:49:26Z,http://arxiv.org/abs/2109.01747v1
"Accuracy Evaluation of Touch Tasks in Commodity Virtual and Augmented
  Reality Head-Mounted Displays","An increasing number of consumer-oriented head-mounted displays (HMD) for
augmented and virtual reality (AR/VR) are capable of finger and hand tracking.
We report on the accuracy of off-the-shelf VR and AR HMDs when used for
touch-based tasks such as pointing or drawing. Specifically, we report on the
finger tracking accuracy of the VR head-mounted displays Oculus Quest, Vive Pro
and the Leap Motion controller, when attached to a VR HMD, as well as the
finger tracking accuracy of the AR head-mounted displays Microsoft HoloLens 2
and Magic Leap. We present the results of two experiments in which we compare
the accuracy for absolute and relative pointing tasks using both human
participants and a robot. The results suggest that HTC Vive has a lower spatial
accuracy than the Oculus Quest and Leap Motion and that the Microsoft HoloLens
2 provides higher spatial accuracy than Magic Leap One. These findings can
serve as decision support for researchers and practitioners in choosing which
systems to use in the future.","['Daniel Schneider', 'Verena Biener', 'Alexander Otte', 'Travis Gesslein', 'Philipp Gagel', 'Cuauhtli Campos', 'Klen Čopič Pucihar', 'Matjaž Kljun', 'Eyal Ofek', 'Michel Pahud', 'Per Ola Kristensson', 'Jens Grubert']",2021-09-22T09:21:29Z,http://arxiv.org/abs/2109.10607v1
"WebAssembly enables low latency interoperable augmented and virtual
  reality software","There is a clear difference in runtime performance between native
applications that use augmented/virtual reality (AR/VR) device-specific
hardware and comparable web-based implementations. Here we show that
WebAssembly (Wasm) offers a promising developer solution that can bring
near-native low latency performance to web-based applications, enabling
hardware-agnostic interoperability at scale through portable bytecode that runs
on any WiFi or cellular data network-enabled AR/VR device. Many software
application areas have begun to realize Wasm's potential as a key enabling
technology, but it has yet to establish a robust presence in the AR/VR domain.
When considering the limitations of current web-based AR/VR development
technologies such as WebXR, which provides an existing application programming
interface (API) that enables AR/VR capabilities for web-based programs, Wasm
can resolve critical issues faced with just-in-time (JIT) compilation, slow
run-times, large file sizes and big data, among other challenges. Existing
applications using Wasm-based WebXR are sparse but growing, and the potential
for porting native applications to use this emerging framework will benefit the
web-based AR/VR application space and bring it closer to its native
counterparts in terms of performance. Taken together, this kind of standardized
""write-once-deploy-everywhere"" software framework for AR/VR applications has
the potential to consolidate user experiences across different head-mounted
displays and other compatible hardware devices to ultimately create an
interoperable AR/VR ecosystem.",['Bohdan B. Khomtchouk'],2021-10-14T03:17:07Z,http://arxiv.org/abs/2110.07128v1
"Multi-User Augmented Reality with Infrastructure-free Collaborative
  Localization","Multi-user augmented reality (AR) could someday empower first responders with
the ability to see team members around corners and through walls. For this
vision of people tracking in dynamic environments to be practical, we need a
relative localization system that is nearly instantly available across
wide-areas without any existing infrastructure or manual setup. In this paper,
we present LocAR, an infrastructure-free 6-degrees-of-freedom (6DoF)
localization system for AR applications that uses motion estimates and range
measurements between users to establish an accurate relative coordinate system.
We show that not only is it possible to perform collaborative localization
without infrastructure or global coordinates, but that our approach provides
nearly the same level of accuracy as fixed infrastructure approaches for AR
teaming applications. LocAR uses visual-inertial odometry (VIO) in conjunction
with ultra-wideband (UWB) ranging radios to estimate the relative position of
each device in an ad-hoc manner. The system leverages a collaborative 6DoF
particle filtering formulation that operates on sporadic messages exchanged
between nearby users. Unlike map or landmark sharing approaches, this allows
for collaborative AR sessions even if users do not overlap the same spaces.
LocAR consists of an open-source UWB firmware and reference mobile phone
application that can display the location of team members in real-time using
mobile AR. We evaluate LocAR across multiple buildings under a wide-variety of
conditions including a contiguous 30,000 square foot region spanning multiple
floors and find that it achieves median geometric error in 3D of less than 1
meter between five users freely walking across 3 floors.","['John Miller', 'Elahe Soltanaghai', 'Raewyn Duvall', 'Jeff Chen', 'Vikram Bhat', 'Nuno Pereira', 'Anthony Rowe']",2021-10-30T04:48:53Z,http://arxiv.org/abs/2111.00174v1
"Utility of Optical See-Through Head Mounted Displays in Augmented
  Reality-Assisted Surgery: A systematic review","This article presents a systematic review of optical see-through head mounted
display (OST-HMD) usage in augmented reality (AR) surgery applications from
2013 to 2020. Articles were categorised by: OST-HMD device, surgical
speciality, surgical application context, visualisation content, experimental
design and evaluation, accuracy and human factors of human-computer
interaction. 91 articles fulfilled all inclusion criteria. Some clear trends
emerge. The Microsoft HoloLens increasingly dominates the field, with
orthopaedic surgery being the most popular application (28.6\%). By far the
most common surgical context is surgical guidance (n=58) and segmented
preoperative models dominate visualisation (n = 40). Experiments mainly involve
phantoms (n = 43) or system setup (n = 21), with patient case studies ranking
third (n = 19), reflecting the comparative infancy of the field. Experiments
cover issues from registration to perception with very different accuracy
results. Human factors emerge as significant to OST-HMD utility. Some factors
are addressed by the systems proposed, such as attention shift away from the
surgical site and mental mapping of 2D images to 3D patient anatomy. Other
persistent human factors remain or are caused by OST-HMD solutions, including
ease of use, comfort and spatial perception issues. The significant upward
trend in published articles is clear, but such devices are not yet established
in the operating room and clinical studies showing benefit are lacking. A
focused effort addressing technical registration and perceptual factors in the
lab coupled with design that incorporates human factors considerations to solve
clear clinical problems should ensure that the significant current research
efforts will succeed.","['Manuel Birlo', 'P. J. ""Eddie\'\' Edwards', 'Matthew Clarkson', 'Danail Stoyanov']",2022-02-08T21:02:12Z,http://arxiv.org/abs/2202.04141v1
"InfraredTags: Embedding Invisible AR Markers and Barcodes Using
  Low-Cost, Infrared-Based 3D Printing and Imaging Tools","Existing approaches for embedding unobtrusive tags inside 3D objects require
either complex fabrication or high-cost imaging equipment. We present
InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye
that can be 3D printed as part of objects, and detected rapidly by low-cost
near-infrared cameras. We achieve this by printing objects from an
infrared-transmitting filament, which infrared cameras can see through, and by
having air gaps inside for the tag's bits, which appear at a different
intensity in the infrared image.
  We built a user interface that facilitates the integration of common tags (QR
codes, ArUco markers) with the object geometry to make them 3D printable as
InfraredTags. We also developed a low-cost infrared imaging module that
augments existing mobile devices and decodes tags using our image processing
pipeline. Our evaluation shows that the tags can be detected with little
near-infrared illumination (0.2lux) and from distances as far as 250cm. We
demonstrate how our method enables various applications, such as object
tracking and embedding metadata for augmented reality and tangible
interactions.","['Mustafa Doga Dogan', 'Ahmad Taka', 'Michael Lu', 'Yunyi Zhu', 'Akshat Kumar', 'Aakar Gupta', 'Stefanie Mueller']",2022-02-12T23:45:18Z,http://arxiv.org/abs/2202.06165v1
"Design and Evaluation of an Augmented Reality Head-Mounted Display
  Interface for Human Robot Teams Collaborating in Physically Shared
  Manufacturing Tasks","We provide an experimental evaluation of a wearable augmented reality (AR)
system we have developed for human-robot teams working on tasks requiring
collaboration in shared physical workspace. Recent advances in AR technology
have facilitated the development of more intuitive user interfaces for many
human-robot interaction applications. While it has been anticipated that AR can
provided a more intuitive interface to robot assistants helping human workers
in various manufacturing scenarios, existing studies in robotics have been
largely limited to teleoperation and programming. Industry 5.0 envisions
cooperation between human and robot working in teams. Indeed, there exist many
industrial task that can benefit from human-robot collaboration. A prime
example is high-value composite manufacturing. Working with our industry
partner towards this example application, we evaluated our AR interface design
for shared physical workspace collaboration in human-robot teams. We conducted
a multi-dimensional analysis of our interface using establish metrics. Results
from our user study (n=26) show that subjectively, the AR interface feels more
novel and a standard joystick interface feels more dependable to users.
However, the AR interface was found to reduce physical demand and task
completion time, while increasing robot utilization. Furthermore, user's
freedom of choice to collaborate with the robot may also affect the perceived
usability of the system.","['Wesley P Chan', 'Geoffrey Hanks', 'Maram Sakr', 'Haomiao Zhang', 'Tiger Zuo', 'H F Machiel Van der Loos', 'Elizabeth Croft']",2022-03-16T01:30:02Z,http://arxiv.org/abs/2203.08343v1
Verifiable Access Control for Augmented Reality Localization and Mapping,"Localization and mapping is a key technology for bridging the virtual and
physical worlds in augmented reality (AR). Localization and mapping works by
creating and querying maps made of anchor points that enable the overlay of
these two worlds. As a result, information about the physical world is captured
in the map and naturally gives rise to concerns around who can map physical
spaces as well as who can access or modify the virtual ones. This paper
discusses how we can provide access controls over virtual maps as a basic
building block to enhance security and privacy of AR systems. In particular, we
propose VACMaps: an access control system for localization and mapping using
formal methods. VACMaps defines a domain-specific language that enables users
to specify access control policies for virtual spaces. Access requests to
virtual spaces are then evaluated against relevant policies in a way that
preserves confidentiality and integrity of virtual spaces owned by the users.
The precise semantics of the policies are defined by SMT formulas, which allow
VACMaps to reason about properties of access policies automatically. An
evaluation of VACMaps is provided using an AR testbed of a single-family home.
We show that VACMaps is scalable in that it can run at practical speeds and
that it can also reason about access control policies automatically to detect
potential policy misconfigurations.","['Shaowei Zhu', 'Hyo Jin Kim', 'Maurizio Monge', 'G. Edward Suh', 'Armin Alaghi', 'Brandon Reagen', 'Vincent Lee']",2022-03-24T19:19:38Z,http://arxiv.org/abs/2203.13308v1
"LINA -- A social augmented reality game around mental health, supporting
  real-world connection and sense of belonging for early adolescents","Early adolescence is a time of major social change; a strong sense of
belonging and peer connectedness is an essential protective factor in mental
health during that period. In this paper we introduce LINA, an augmented
reality (AR) smartphone-based serious game played in school by an entire class
(age 10+) together with their teacher, which aims to facilitate and improve
peer interaction, sense of belonging and class climate, while creating a safe
space to reflect on mental health and external stressors related to family
circumstance. LINA was developed through an interdisciplinary collaboration
involving a playwright, software developers, psychologists, and artists, via an
iterative co-development process with young people. A prototype has been
evaluated quantitatively for usability and qualitatively for efficacy in a
study with 91 early adolescents (agemean=11.41). Results from the Game User
Experience Satisfaction Scale (GUESS-18) and data from qualitative focus groups
showed high acceptability and preliminary efficacy of the game. Using AR, a
shared immersive narrative and collaborative gameplay in a shared physical
space offers an opportunity to harness adolescent affinity for digital
technology towards improving real-world social connection and sense of
belonging.","['Gloria Mittmann', 'Adam Barnard', 'Ina Krammer', 'Diogo Martins', 'João Dias']",2022-04-27T13:24:41Z,http://arxiv.org/abs/2204.12917v1
"LEAF + AIO: Edge-Assisted Energy-Aware Object Detection for Mobile
  Augmented Reality","Today very few deep learning-based mobile augmented reality (MAR)
applications are applied in mobile devices because they are significantly
energy-guzzling. In this paper, we design an edge-based energy-aware MAR system
that enables MAR devices to dynamically change their configurations, such as
CPU frequency, computation model size, and image offloading frequency based on
user preferences, camera sampling rates, and available radio resources. Our
proposed dynamic MAR configuration adaptations can minimize the per frame
energy consumption of multiple MAR clients without degrading their preferred
MAR performance metrics, such as latency and detection accuracy. To thoroughly
analyze the interactions among MAR configurations, user preferences, camera
sampling rate, and energy consumption, we propose, to the best of our
knowledge, the first comprehensive analytical energy model for MAR devices.
Based on the proposed analytical model, we design a LEAF optimization algorithm
to guide the MAR configuration adaptation and server radio resource allocation.
An image offloading frequency orchestrator, coordinating with the LEAF, is
developed to adaptively regulate the edge-based object detection invocations
and to further improve the energy efficiency of MAR devices. Extensive
evaluations are conducted to validate the performance of the proposed
analytical model and algorithms.","['Haoxin Wang', 'BaekGyu Kim', 'Jiang Xie', 'Zhu Han']",2022-05-27T06:11:50Z,http://arxiv.org/abs/2205.13770v1
"Smart operators in industry 4.0: A human-centered approach to enhance
  operators' capabilities and competencies within the new smart factory context","As the Industry 4.0 takes shape, human operators experience an increased
complexity of their daily tasks: they are required to be highly flexible and to
demonstrate adaptive capabilities in a very dynamic working environment. It
calls for tools and approaches that could be easily embedded into everyday
practices and able to combine complex methodologies with high usability
requirements. In this perspective, the proposed research work is focused on the
design and development of a practical solution, called Sophos-MS, able to
integrate augmented reality contents and intelligent tutoring systems with
cutting-edge fruition technologies for operators' support in complex
man-machine interactions. After establishing a reference methodological
framework for the smart operator concept within the Industry 4.0 paradigm, the
proposed solution is presented, along with its functional and non-function
requirements. Such requirements are fulfilled through a structured design
strategy whose main outcomes include a multi-layered modular solution,
Sophos-MS, that relies on Augmented Reality contents and on an intelligent
personal digital assistant with vocal interaction capabilities. The proposed
approach has been deployed and its training potentials have been investigated
with field experiments. The experimental campaign results have been firstly
checked to ensure their statistical relevance and then analytically assessed in
order to show that the proposed solution has a real impact on operators'
learning curves and can make the difference between who uses it and who does
not.","['Francesco Longo', 'Letizia Nicoletti', 'Antonio Padovano']",2022-05-30T15:32:05Z,http://arxiv.org/abs/2206.00104v1
"Effects of Augmented-Reality-Based Assisting Interfaces on Drivers'
  Object-wise Situational Awareness in Highly Autonomous Vehicles","Although partially autonomous driving (AD) systems are already available in
production vehicles, drivers are still required to maintain a sufficient level
of situational awareness (SA) during driving. Previous studies have shown that
providing information about the AD's capability using user interfaces can
improve the driver's SA. However, displaying too much information increases the
driver's workload and can distract or overwhelm the driver. Therefore, to
design an efficient user interface (UI), it is necessary to understand its
effect under different circumstances. In this paper, we focus on a UI based on
augmented reality (AR), which can highlight potential hazards on the road. To
understand the effect of highlighting on drivers' SA for objects with different
types and locations under various traffic densities, we conducted an in-person
experiment with 20 participants on a driving simulator. Our study results show
that the effects of highlighting on drivers' SA varied by traffic densities,
object locations and object types. We believe our study can provide guidance in
selecting which object to highlight for the AR-based driver-assistance
interface to optimize SA for drivers driving and monitoring partially
autonomous vehicles.","['Xiaofeng Gao', 'Xingwei Wu', 'Samson Ho', 'Teruhisa Misu', 'Kumar Akash']",2022-06-06T03:23:34Z,http://arxiv.org/abs/2206.02332v1
Volumetric Disentanglement for 3D Scene Manipulation,"Recently, advances in differential volumetric rendering enabled significant
breakthroughs in the photo-realistic and fine-detailed reconstruction of
complex 3D scenes, which is key for many virtual reality applications. However,
in the context of augmented reality, one may also wish to effect semantic
manipulations or augmentations of objects within a scene. To this end, we
propose a volumetric framework for (i) disentangling or separating, the
volumetric representation of a given foreground object from the background, and
(ii) semantically manipulating the foreground object, as well as the
background. Our framework takes as input a set of 2D masks specifying the
desired foreground object for training views, together with the associated 2D
views and poses, and produces a foreground-background disentanglement that
respects the surrounding illumination, reflections, and partial occlusions,
which can be applied to both training and novel views. Our method enables the
separate control of pixel color and depth as well as 3D similarity
transformations of both the foreground and background objects. We subsequently
demonstrate the applicability of our framework on a number of downstream
manipulation tasks including object camouflage, non-negative 3D object
inpainting, 3D object translation, 3D object inpainting, and 3D text-based
object manipulation. Full results are given in our project webpage at
https://sagiebenaim.github.io/volumetric-disentanglement/","['Sagie Benaim', 'Frederik Warburg', 'Peter Ebert Christensen', 'Serge Belongie']",2022-06-06T17:57:07Z,http://arxiv.org/abs/2206.02776v1
"Latents2Segments: Disentangling the Latent Space of Generative Models
  for Semantic Segmentation of Face Images","With the advent of an increasing number of Augmented and Virtual Reality
applications that aim to perform meaningful and controlled style edits on
images of human faces, the impetus for the task of parsing face images to
produce accurate and fine-grained semantic segmentation maps is more than ever
before. Few State of the Art (SOTA) methods which solve this problem, do so by
incorporating priors with respect to facial structure or other face attributes
such as expression and pose in their deep classifier architecture. Our
endeavour in this work is to do away with the priors and complex pre-processing
operations required by SOTA multi-class face segmentation models by reframing
this operation as a downstream task post infusion of disentanglement with
respect to facial semantic regions of interest (ROIs) in the latent space of a
Generative Autoencoder model. We present results for our model's performance on
the CelebAMask-HQ and HELEN datasets. The encoded latent space of our model
achieves significantly higher disentanglement with respect to semantic ROIs
than that of other SOTA works. Moreover, it achieves a 13% faster inference
rate and comparable accuracy with respect to the publicly available SOTA for
the downstream task of semantic segmentation of face images.","['Snehal Singh Tomar', 'A. N. Rajagopalan']",2022-07-05T08:09:15Z,http://arxiv.org/abs/2207.01871v2
Gaze-Vergence-Controlled See-Through Vision in Augmented Reality,"Augmented Reality (AR) see-through vision is an interesting research topic
since it enables users to see through a wall and see the occluded objects. Most
existing research focuses on the visual effects of see-through vision, while
the interaction method is less studied. However, we argue that using common
interaction modalities, e.g., midair click and speech, may not be the optimal
way to control see-through vision. This is because when we want to see through
something, it is physically related to our gaze depth/vergence and thus should
be naturally controlled by the eyes. Following this idea, this paper proposes a
novel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since
gaze depth is needed, we build a gaze tracking module with two infrared cameras
and the corresponding algorithm and assemble it into the Microsoft HoloLens 2
to achieve gaze depth estimation. We then propose two different GVC modes for
see-through vision to fit different scenarios. Extensive experimental results
demonstrate that our gaze depth estimation is efficient and accurate. By
comparing with conventional interaction modalities, our GVC techniques are also
shown to be superior in terms of efficiency and more preferred by users.
Finally, we present four example applications of gaze-vergence-controlled
see-through vision.","['Zhimin Wang', 'Yuxin Zhao', 'Feng Lu']",2022-07-06T13:11:34Z,http://arxiv.org/abs/2207.02645v1
Privacy-preserving Reflection Rendering for Augmented Reality,"Many augmented reality (AR) applications rely on omnidirectional environment
lighting to render photorealistic virtual objects. When the virtual objects
consist of reflective materials, such as a metallic sphere, the required
lighting information to render such objects can consist of privacy-sensitive
information that is outside the current camera view. In this paper, we show,
for the first time, that accuracy-driven multi-view environment lighting can
reveal out-of-camera scene information and compromise privacy. We present a
simple yet effective privacy attack that extracts sensitive scene information
such as human face and text information from the rendered objects, under a
number of application scenarios.
  To defend against such attacks, we develop a novel $IPC^{2}S$ defense and a
conditional $R^2$ defense. Our $IPC^{2}S$ defense, used in conjunction with a
generic lighting reconstruction method, preserves the scene geometry while
obfuscating the privacy-sensitive information. As a proof-of-concept, we
leverage existing OCR and face detection models to identify text and human
faces from past camera observations and blur the color pixels associated with
detected regions. We evaluate the visual quality impact of our defense by
comparing rendered virtual objects to ones rendered with a generic
multi-lighting reconstruction technique, ARKit, and $R^2$ defense. Our visual
and quantitative results demonstrate that our defense leads to structurally
similar reflections with up to 0.98 SSIM score across a variety of rendering
scenarios while preserving sensitive information by reducing the automatic
extraction success rate to at most 8.8%.","['Yiqin Zhao', 'Sheng Wei', 'Tian Guo']",2022-07-07T02:48:59Z,http://arxiv.org/abs/2207.03056v1
"Direction-Aware Adaptive Online Neural Speech Enhancement with an
  Augmented Reality Headset in Real Noisy Conversational Environments","This paper describes the practical response- and performance-aware
development of online speech enhancement for an augmented reality (AR) headset
that helps a user understand conversations made in real noisy echoic
environments (e.g., cocktail party). One may use a state-of-the-art blind
source separation method called fast multichannel nonnegative matrix
factorization (FastMNMF) that works well in various environments thanks to its
unsupervised nature. Its heavy computational cost, however, prevents its
application to real-time processing. In contrast, a supervised beamforming
method that uses a deep neural network (DNN) for estimating spatial information
of speech and noise readily fits real-time processing, but suffers from drastic
performance degradation in mismatched conditions. Given such complementary
characteristics, we propose a dual-process robust online speech enhancement
method based on DNN-based beamforming with FastMNMF-guided adaptation. FastMNMF
(back end) is performed in a mini-batch style and the noisy and enhanced speech
pairs are used together with the original parallel training data for updating
the direction-aware DNN (front end) with backpropagation at a
computationally-allowable interval. This method is used with a blind
dereverberation method called weighted prediction error (WPE) for transcribing
the noisy reverberant speech of a speaker, which can be detected from video or
selected by a user's hand gesture or eye gaze, in a streaming manner and
spatially showing the transcriptions with an AR technique. Our experiment
showed that the word error rate was improved by more than 10 points with the
run-time adaptation using only twelve minutes of observation.","['Kouhei Sekiguchi', 'Aditya Arie Nugraha', 'Yicheng Du', 'Yoshiaki Bando', 'Mathieu Fontaine', 'Kazuyoshi Yoshii']",2022-07-15T05:14:27Z,http://arxiv.org/abs/2207.07296v1
"ARShopping: In-Store Shopping Decision Support Through Augmented Reality
  and Immersive Visualization","Online shopping gives customers boundless options to choose from, backed by
extensive product details and customer reviews, all from the comfort of home;
yet, no amount of detailed, online information can outweigh the instant
gratification and hands-on understanding of a product that is provided by
physical stores. However, making purchasing decisions in physical stores can be
challenging due to a large number of similar alternatives and limited
accessibility of the relevant product information (e.g., features, ratings, and
reviews). In this work, we present ARShopping: a web-based prototype to
visually communicate detailed product information from an online setting on
portable smart devices (e.g., phones, tablets, glasses), within the physical
space at the point of purchase. This prototype uses augmented reality (AR) to
identify products and display detailed information to help consumers make
purchasing decisions that fulfill their needs while decreasing the
decision-making time. In particular, we use a data fusion algorithm to improve
the precision of the product detection; we then integrate AR visualizations
into the scene to facilitate comparisons across multiple products and features.
We designed our prototype based on interviews with 14 participants to better
understand the utility and ease of use of the prototype.","['Bingjie Xu', 'Shunan Guo', 'Eunyee Koh', 'Jane Hoffswell', 'Ryan Rossi', 'Fan Du']",2022-07-15T17:56:57Z,http://arxiv.org/abs/2207.07643v1
The HoloLens in Medicine: A systematic Review and Taxonomy,"The HoloLens (Microsoft Corp., Redmond, WA), a head-worn, optically
see-through augmented reality display, is the main player in the recent boost
in medical augmented reality research. In medical settings, the HoloLens
enables the physician to obtain immediate insight into patient information,
directly overlaid with their view of the clinical scenario, the medical student
to gain a better understanding of complex anatomies or procedures, and even the
patient to execute therapeutic tasks with improved, immersive guidance. In this
systematic review, we provide a comprehensive overview of the usage of the
first-generation HoloLens within the medical domain, from its release in March
2016, until the year of 2021, were attention is shifting towards it's
successor, the HoloLens 2. We identified 171 relevant publications through a
systematic search of the PubMed and Scopus databases. We analyze these
publications in regard to their intended use case, technical methodology for
registration and tracking, data sources, visualization as well as validation
and evaluation. We find that, although the feasibility of using the HoloLens in
various medical scenarios has been shown, increased efforts in the areas of
precision, reliability, usability, workflow and perception are necessary to
establish AR in clinical practice.","['Christina Gsaxner', 'Jianning Li', 'Antonio Pepe', 'Yuan Jin', 'Jens Kleesiek', 'Dieter Schmalstieg', 'Jan Egger']",2022-09-06T13:05:29Z,http://arxiv.org/abs/2209.03245v1
LaMAR: Benchmarking Localization and Mapping for Augmented Reality,"Localization and mapping is the foundational technology for augmented reality
(AR) that enables sharing and persistence of digital content in the real world.
While significant progress has been made, researchers are still mostly driven
by unrealistic benchmarks not representative of real-world AR scenarios. These
benchmarks are often based on small-scale datasets with low scene diversity,
captured from stationary cameras, and lack other sensor inputs like inertial,
radio, or depth data. Furthermore, their ground-truth (GT) accuracy is mostly
insufficient to satisfy AR requirements. To close this gap, we introduce LaMAR,
a new benchmark with a comprehensive capture and GT pipeline that co-registers
realistic trajectories and sensor streams captured by heterogeneous AR devices
in large, unconstrained scenes. To establish an accurate GT, our pipeline
robustly aligns the trajectories against laser scans in a fully automated
manner. As a result, we publish a benchmark dataset of diverse and large-scale
scenes recorded with head-mounted and hand-held AR devices. We extend several
state-of-the-art methods to take advantage of the AR-specific setup and
evaluate them on our benchmark. The results offer new insights on current
research and reveal promising avenues for future work in the field of
localization and mapping for AR.","['Paul-Edouard Sarlin', 'Mihai Dusmanu', 'Johannes L. Schönberger', 'Pablo Speciale', 'Lukas Gruber', 'Viktor Larsson', 'Ondrej Miksik', 'Marc Pollefeys']",2022-10-19T17:58:17Z,http://arxiv.org/abs/2210.10770v1
"Resource Allocation of Federated Learning for the Metaverse with Mobile
  Augmented Reality","The Metaverse has received much attention recently. Metaverse applications
via mobile augmented reality (MAR) require rapid and accurate object detection
to mix digital data with the real world. Federated learning (FL) is an
intriguing distributed machine learning approach due to its privacy-preserving
characteristics. Due to privacy concerns and the limited computation resources
on mobile devices, we incorporate FL into MAR systems of the Metaverse to train
a model cooperatively. Besides, to balance the trade-off between energy,
execution latency and model accuracy, thereby accommodating different demands
and application scenarios, we formulate an optimization problem to minimize a
weighted combination of total energy consumption, completion time and model
accuracy. Through decomposing the non-convex optimization problem into two
subproblems, we devise a resource allocation algorithm to determine the
bandwidth allocation, transmission power, CPU frequency and video frame
resolution for each participating device. We further present the convergence
analysis and computational complexity of the proposed algorithm. Numerical
results show that our proposed algorithm has better performance (in terms of
energy consumption, completion time and model accuracy) under different weight
parameters compared to existing benchmarks.","['Xinyu Zhou', 'Chang Liu', 'Jun Zhao']",2022-11-16T06:37:32Z,http://arxiv.org/abs/2211.08705v3
"Complementary Textures. A Novel Approach to Object Alignment in Mixed
  Reality","Alignment between real and virtual objects is a challenging task required for
the deployment of Mixed Reality (MR) into manufacturing, medical, and
construction applications. To face this challenge, a series of methods have
been proposed. While many approaches use dynamic augmentations such as
animations, arrows, or text to assist users, they require tracking the position
of real objects. In contrast, when tracking of the real objects is not
available or desired, alternative approaches use virtual replicas of real
objects to allow for interactive, perceptual virtual-to-real, and/or
real-to-virtual alignment. In these cases, the accuracy achieved strongly
depends on the quality of the perceptual information provided to the user. This
paper proposes a novel set of perceptual alignment concepts that go beyond the
use of traditional visualization of virtual replicas, introducing the concept
of COMPLEMENTARY TEXTURES to improve interactive alignment in MR applications.
To showcase the advantages of using COMPLEMENTARY TEXTURES, we describe three
different implementations that provide highly salient visual cues when
misalignment is observed; or present semantic augmentations that, when combined
with a real object, provide contextual information that can be used during the
alignment process. The authors aim to open new paths for the community to
explore rather than describing end-to-end solutions. The objective is to show
the multitude of opportunities such concepts could provide for further research
and development.","['Alejandro Martin-Gomez', 'Alexander Winkler', 'Rafael de la Tijera Obert', 'Javad Fotouhi', 'Daniel Roth', 'Ulrich Eck', 'Nassir Navab']",2022-11-16T16:50:21Z,http://arxiv.org/abs/2211.09037v1
"Resource Allocation for Augmented Reality Empowered Vehicular Edge
  Metaverse","Metaverse is considered to be the evolution of the next-generation networks,
providing users with experience sharing at the intersection between physical
and digital. Augmented reality (AR) is one of the primary supporting
technologies in the Metaverse, which can seamlessly integrate real-world
information with virtual world information to provide users with an immersive
interactive experience. Extraordinarily, AR has brought new opportunities for
assisting safe driving. Nevertheless, achieving efficient execution of AR tasks
and increasing system revenue are the main challenges faced by the Metaverse's
AR in-vehicle applications. In this paper, we are the first to propose an
efficient resource allocation framework for AR-empowered vehicular edge
Metaverse to improve system utility. We formulate an optimization problem
featuring multidimensional control to concurrently maximize data utility at the
Metaverse operator side and minimize energy consumption at the vehicles' side,
which jointly considers the computational resource allocation on the Metaverse
server, and AR vehicles' CPU frequency, transmit power, and computation model
size. Notwithstanding, the major impediment is how to design an efficient
algorithm to obtain the solutions of the optimization. Wherefore, we do this by
decoupling the optimization variables. We first derive the optimal computation
model size by the binary search, followed by obtaining the optimal power
allocation by the bisection method and finding a closed-form solution to the
optimal CPU frequency of AR vehicles, and finally, attain the optimal
allocation of computational resource on the server by the Lagrangian dual
method. To estimate the performance of our proposed scheme, we establish three
baseline schemes as a comparison, and simulation manifests that our proposed
scheme balances the operator's reward and the energy consumption of vehicles.","['Jie Feng', 'Jun Zhao']",2022-12-02T17:34:40Z,http://arxiv.org/abs/2212.01325v1
"Joint Optimization of Video-based AI Inference Tasks in MEC-assisted
  Augmented Reality Systems","The high computational complexity and energy consumption of artificial
intelligence (AI) algorithms hinder their application in augmented reality (AR)
systems. However, mobile edge computing (MEC) makes it possible to solve this
problem. This paper considers the scene of completing video-based AI inference
tasks in the MEC system. We formulate a mixed-integer nonlinear programming
problem (MINLP) to reduce inference delays, energy consumption and to improve
recognition accuracy. We give a simplified expression of the inference
complexity model and accuracy model through derivation and experimentation. The
problem is then solved iteratively by using alternating optimization.
Specifically, by assuming that the offloading decision is given, the problem is
decoupled into two sub-problems, i.e., the resource allocation problem for the
devices set that completes the inference tasks locally, and that for the
devices set that offloads tasks. For the problem of offloading decision
optimization, we propose a Channel-Aware heuristic algorithm. To further reduce
the complexity, we propose an alternating direction method of multipliers
(ADMM) based distributed algorithm. The ADMM-based algorithm has a low
computational complexity that grows linearly with the number of devices.
Numerical experiments show the effectiveness of proposed algorithms. The
trade-off relationship between delay, energy consumption, and accuracy is also
analyzed.","['Guangjin Pan', 'Heng Zhang', 'Shugong Xu', 'Shunqing Zhang', 'Xiaojing Chen']",2023-01-03T09:03:39Z,http://arxiv.org/abs/2301.01010v1
LitAR: Visually Coherent Lighting for Mobile Augmented Reality,"An accurate understanding of omnidirectional environment lighting is crucial
for high-quality virtual object rendering in mobile augmented reality (AR). In
particular, to support reflective rendering, existing methods have leveraged
deep learning models to estimate or have used physical light probes to capture
physical lighting, typically represented in the form of an environment map.
However, these methods often fail to provide visually coherent details or
require additional setups. For example, the commercial framework ARKit uses a
convolutional neural network that can generate realistic environment maps;
however the corresponding reflective rendering might not match the physical
environments. In this work, we present the design and implementation of a
lighting reconstruction framework called LitAR that enables realistic and
visually-coherent rendering. LitAR addresses several challenges of supporting
lighting information for mobile AR. First, to address the spatial variance
problem, LitAR uses two-field lighting reconstruction to divide the lighting
reconstruction task into the spatial variance-aware near-field reconstruction
and the directional-aware far-field reconstruction. The corresponding
environment map allows reflective rendering with correct color tones. Second,
LitAR uses two noise-tolerant data capturing policies to ensure data quality,
namely guided bootstrapped movement and motion-based automatic capturing.
Third, to handle the mismatch between the mobile computation capability and the
high computation requirement of lighting reconstruction, LitAR employs two
novel real-time environment map rendering techniques called multi-resolution
projection and anchor extrapolation. These two techniques effectively remove
the need of time-consuming mesh reconstruction while maintaining visual
quality.","['Yiqin Zhao', 'Chongyang Ma', 'Haibin Huang', 'Tian Guo']",2023-01-15T20:47:38Z,http://arxiv.org/abs/2301.06184v1
Optimal Mobility Aware Wireless Edge Cloud Support for the Metaverse,"Mobile augmented reality (MAR) applications extended in the metaverse could
provide mixed and immersive experiences by amalgamating the virtual and
physical world. However, the joint consideration between MAR and metaverse
seeks the reliable and high quality support for foreground interactions and
background contents from these applications, which intensifies their
consumption in energy, caching and computing resources. To tackle these
challenges, a more flexible request assignment and resource allocation with
more efficient processing are proposed in this paper through anchoring
decomposed metaverse AR services at different edge nodes and proactively
caching background metaverse region models embedded with target Augmented
Reality Objects (AROs).Advanced terminals are also considered to further reduce
service delay at an acceptable cost of energy consumption.We then propose and
solve a joint optimization problem that explicitly considers the balance
between service delay and energy consumption under the constraint of user
perception quality in a mobility event.By also explicitly taking into account
capabilities of user terminals, the proposed optimized scheme is compared to
its terminal oblivious version in this paper. According to a wide set of
numerical investigations, the proposed scheme owns advantages in service
latency and energy efficiency over other nominal baseline schemes which neglect
capacities of terminals, user physical mobility, service decomposition and the
inherent multi modality of the metaverse MAR service.","['Zhaohui Huang', 'Vasilis Friderikos']",2023-01-16T17:00:42Z,http://arxiv.org/abs/2301.06519v1
"Playing with Data: An Augmented Reality Approach to Interact with
  Visualizations of Industrial Process Tomography","Industrial process tomography (IPT) is a specialized imaging technique widely
used in industrial scenarios for process supervision and control. Today,
augmented/mixed reality (AR/MR) is increasingly being adopted in many
industrial occasions, even though there is still an obvious gap when it comes
to IPT. To bridge this gap, we propose the first systematic AR approach using
optical see-through (OST) head mounted displays (HMDs) with comparative
evaluation for domain users towards IPT visualization analysis. The
proof-of-concept was demonstrated by a within-subject user study (n=20) with
counterbalancing design. Both qualitative and quantitative measurements were
investigated. The results showed that our AR approach outperformed conventional
settings for IPT data visualization analysis in bringing higher
understandability, reduced task completion time, lower error rates for domain
tasks, increased usability with enhanced user experience, and a better
recommendation level. We summarize the findings and suggest future research
directions for benefiting IPT users with AR/MR.","['Yuchong Zhang', 'Yueming Xuan', 'Rahul Yadav', 'Adel Omrani', 'Morten Fjeld']",2023-02-03T12:19:01Z,http://arxiv.org/abs/2302.01686v5
"Multiperspective Teaching of Unknown Objects via Shared-gaze-based
  Multimodal Human-Robot Interaction","For successful deployment of robots in multifaceted situations, an
understanding of the robot for its environment is indispensable. With advancing
performance of state-of-the-art object detectors, the capability of robots to
detect objects within their interaction domain is also enhancing. However, it
binds the robot to a few trained classes and prevents it from adapting to
unfamiliar surroundings beyond predefined scenarios. In such scenarios, humans
could assist robots amidst the overwhelming number of interaction entities and
impart the requisite expertise by acting as teachers. We propose a novel
pipeline that effectively harnesses human gaze and augmented reality in a
human-robot collaboration context to teach a robot novel objects in its
surrounding environment. By intertwining gaze (to guide the robot's attention
to an object of interest) with augmented reality (to convey the respective
class information) we enable the robot to quickly acquire a significant amount
of automatically labeled training data on its own. Training in a transfer
learning fashion, we demonstrate the robot's capability to detect recently
learned objects and evaluate the influence of different machine learning models
and learning procedures as well as the amount of training data involved. Our
multimodal approach proves to be an efficient and natural way to teach the
robot novel objects based on a few instances and allows it to detect classes
for which no training dataset is available. In addition, we make our dataset
publicly available to the research community, which consists of RGB and depth
data, intrinsic and extrinsic camera parameters, along with regions of
interest.","['Daniel Weber', 'Wolfgang Fuhl', 'Enkelejda Kasneci', 'Andreas Zell']",2023-03-01T11:23:10Z,http://arxiv.org/abs/2303.00423v1
XAIR: A Framework of Explainable AI in Augmented Reality,"Explainable AI (XAI) has established itself as an important component of
AI-driven interactive systems. With Augmented Reality (AR) becoming more
integrated in daily lives, the role of XAI also becomes essential in AR because
end-users will frequently interact with intelligent services. However, it is
unclear how to design effective XAI experiences for AR. We propose XAIR, a
design framework that addresses ""when"", ""what"", and ""how"" to provide
explanations of AI output in AR. The framework was based on a
multi-disciplinary literature review of XAI and HCI research, a large-scale
survey probing 500+ end-users' preferences for AR-based explanations, and three
workshops with 12 experts collecting their insights about XAI design in AR.
XAIR's utility and effectiveness was verified via a study with 10 designers and
another study with 12 end-users. XAIR can provide guidelines for designers,
inspiring them to identify new design opportunities and achieve effective XAI
designs in AR.","['Xuhai Xu', 'Mengjie Yu', 'Tanya R. Jonker', 'Kashyap Todi', 'Feiyu Lu', 'Xun Qian', 'João Marcelo Evangelista Belo', 'Tianyi Wang', 'Michelle Li', 'Aran Mun', 'Te-Yen Wu', 'Junxiao Shen', 'Ting Zhang', 'Narine Kokhlikyan', 'Fulton Wang', 'Paul Sorenson', 'Sophie Kahyun Kim', 'Hrvoje Benko']",2023-03-28T20:14:29Z,http://arxiv.org/abs/2303.16292v1
Modular 3D Interface Design for Accessible VR Applications,"Designed with an accessible first design approach, the presented paper
describes how exploiting humans proprioception ability in 3D space can result
in a more natural interaction experience when using a 3D graphical user
interface in a virtual environment. The modularity of the designed interface
empowers the user to decide where they want to place interface elements in 3D
space allowing for a highly customizable experience, both in the context of the
player and the virtual space. Drawing inspiration from todays tangible
interfaces used, such as those in aircraft cockpits, a modular interface is
presented taking advantage of our natural understanding of interacting with 3D
objects and exploiting capabilities that otherwise have not been used in 2D
interaction. Additionally, the designed interface supports multimodal input
mechanisms which also demonstrates the opportunity for the design to cross over
to augmented reality applications. A focus group study was completed to better
understand the usability and constraints of the designed 3D GUI.","['Corrie Green', 'Dr Yang Jiang', 'Dr John Isaacs']",2023-04-08T17:07:46Z,http://arxiv.org/abs/2304.10541v2
"Supporting Construction and Architectural Visualization through BIM and
  AR/VR: A Systematic Literature Review","The Architecture, Engineering, Construction, and Facility Management (AEC/FM)
industry deals with the design, construction, and operation of complex
buildings. Today, Building Information Modeling (BIM) is used to represent
information about a building in a single, non-redundant representation. Here,
Augmented Reality (AR) and Virtual Reality (VR) can improve the visualization
and interaction with the resulting model by augmenting the real world with
information from the BIM model or allowing a user to immerse in a virtual world
generated from the BIM model. This can improve the design, construction, and
operation of buildings. While an increasing number of studies in HCI,
construction, or engineering have shown the potential of using AR and VR
technology together with BIM, often research remains focused on individual
explorations and key design strategies. In addition to that, a systematic
overview and discussion of recent works combining AR/VR with BIM are not yet
fully covered. Therefore, this paper systematically reviews recent approaches
combining AR/VR with BIM and categorizes the literature by the building's
lifecycle phase while systematically describing relevant use cases. In total,
32 out of 447 papers between 2017 and 2022 were categorized. The categorization
shows that most approaches focus on the construction phase and the use case of
review and quality assurance. In the design phase, most approaches use VR,
while in the construction and operation phases, AR is prevalent.","['Enes Yigitbas', 'Alexander Nowosad', 'Gregor Engels']",2023-06-21T13:52:37Z,http://arxiv.org/abs/2306.12274v1
PalmGazer: Unimanual Eye-hand Menus in Augmented Reality,"How can we design the user interfaces for augmented reality (AR) so that we
can interact as simple, flexible and expressive as we can with smartphones in
one hand? To explore this question, we propose PalmGazer as an interaction
concept integrating eye-hand interaction to establish a singlehandedly operable
menu system. In particular, PalmGazer is designed to support quick and
spontaneous digital commands -- such as to play a music track, check
notifications or browse visual media -- through our devised three-way
interaction model: hand opening to summon the menu UI, eye-hand input for
selection of items, and dragging gesture for navigation. A key aspect is that
it remains always-accessible and movable to the user, as the menu supports
meaningful hand and head based reference frames. We demonstrate the concept in
practice through a prototypical personal UI with application probes, and
describe technique designs specifically-tailored to the application UI. A
qualitative evaluation highlights the system's design benefits and drawbacks,
e.g., that common 2D scroll and selection tasks are simple to operate, but
higher degrees of freedom may be reserved for two hands. Our work contributes
interaction techniques and design insights to expand AR's uni-manual
capabilities.","['Ken Pfeuffer', 'Jan Obernolte', 'Felix Dietz', 'Ville Mäkelä', 'Ludwig Sidenmark', 'Pavel Manakhov', 'Minna Pakanen', 'Florian Alt']",2023-06-21T17:39:50Z,http://arxiv.org/abs/2306.12402v1
"Investigating the Usability of Collaborative Robot control through
  Hands-Free Operation using Eye gaze and Augmented Reality","This paper proposes a novel operation for controlling a mobile robot using a
head-mounted device. Conventionally, robots are operated using computers or a
joystick, which creates limitations in usability and flexibility because
control equipment has to be carried by hand. This lack of flexibility may
prevent workers from multitasking or carrying objects while operating the
robot. To address this limitation, we propose a hands-free method to operate
the mobile robot with a human gaze in an Augmented Reality (AR) environment.
The proposed work is demonstrated using the HoloLens 2 to control the mobile
robot, Robotnik Summit-XL, through the eye-gaze in AR. Stable speed control and
navigation of the mobile robot were achieved through admittance control which
was calculated using the gaze position. The experiment was conducted to compare
the usability between the joystick and the proposed operation, and the results
were validated through surveys (i.e., SUS, SEQ). The survey results from the
participants after the experiments showed that the wearer of the HoloLens
accurately operated the mobile robot in a collaborative manner. The results for
both the joystick and the HoloLens were marked as easy to use with
above-average usability. This suggests that the HoloLens can be used as a
replacement for the joystick to allow hands-free robot operation and has the
potential to increase the efficiency of human-robot collaboration in situations
when hands-free controls are needed.","['Joosun Lee', 'Taeyhang Lim', 'Wansoo Kim']",2023-06-22T17:46:15Z,http://arxiv.org/abs/2306.13072v1
"Unveiling the Invisible: Enhanced Detection and Analysis of Deteriorated
  Areas in Solar PV Modules Using Unsupervised Sensing Algorithms and 3D
  Augmented Reality","Solar Photovoltaic (PV) is increasingly being used to address the global
concern of energy security. However, hot spot and snail trails in PV modules
caused mostly by crakes reduce their efficiency and power capacity. This
article presents a groundbreaking methodology for automatically identifying and
analyzing anomalies like hot spots and snail trails in Solar Photovoltaic (PV)
modules, leveraging unsupervised sensing algorithms and 3D Augmented Reality
(AR) visualization. By transforming the traditional methods of diagnosis and
repair, our approach not only enhances efficiency but also substantially cuts
down the cost of PV system maintenance. Validated through computer simulations
and real-world image datasets, the proposed framework accurately identifies
dirty regions, emphasizing the critical role of regular maintenance in
optimizing the power capacity of solar PV modules. Our immediate objective is
to leverage drone technology for real-time, automatic solar panel detection,
significantly boosting the efficacy of PV maintenance. The proposed methodology
could revolutionize solar PV maintenance, enabling swift, precise anomaly
detection without human intervention. This could result in significant cost
savings, heightened energy production, and improved overall performance of
solar PV systems. Moreover, the novel combination of unsupervised sensing
algorithms with 3D AR visualization heralds new opportunities for further
research and development in solar PV maintenance.","['Adel Oulefki', 'Yassine Himeur', 'Thaweesak Trongtiraku', 'Kahina Amara', 'Sos Agaian', 'Samir Benbelkacem', 'Mohamed Amine Guerroudji', 'Mohamed Zemmouri', 'Sahla Ferhat', 'Nadia Zenati', 'Shadi Atalla', 'Wathiq Mansoor']",2023-07-11T09:27:00Z,http://arxiv.org/abs/2307.05136v2
"A Brain-Computer Interface Augmented Reality Framework with
  Auto-Adaptive SSVEP Recognition","Brain-Computer Interface (BCI) initially gained attention for developing
applications that aid physically impaired individuals. Recently, the idea of
integrating BCI with Augmented Reality (AR) emerged, which uses BCI not only to
enhance the quality of life for individuals with disabilities but also to
develop mainstream applications for healthy users. One commonly used BCI signal
pattern is the Steady-state Visually-evoked Potential (SSVEP), which captures
the brain's response to flickering visual stimuli. SSVEP-based BCI-AR
applications enable users to express their needs/wants by simply looking at
corresponding command options. However, individuals are different in brain
signals and thus require per-subject SSVEP recognition. Moreover, muscle
movements and eye blinks interfere with brain signals, and thus subjects are
required to remain still during BCI experiments, which limits AR engagement. In
this paper, we (1) propose a simple adaptive ensemble classification system
that handles the inter-subject variability, (2) present a simple BCI-AR
framework that supports the development of a wide range of SSVEP-based BCI-AR
applications, and (3) evaluate the performance of our ensemble algorithm in an
SSVEP-based BCI-AR application with head rotations which has demonstrated
robustness to the movement interference. Our testing on multiple subjects
achieved a mean accuracy of 80\% on a PC and 77\% using the HoloLens AR
headset, both of which surpass previous studies that incorporate individual
classifiers and head movements. In addition, our visual stimulation time is 5
seconds which is relatively short. The statistically significant results show
that our ensemble classification approach outperforms individual classifiers in
SSVEP-based BCIs.","['Yasmine Mustafa', 'Mohamed Elmahallawy', 'Tie Luo', 'Seif Eldawlatly']",2023-08-11T21:56:00Z,http://arxiv.org/abs/2308.06401v1
"SiTAR: Situated Trajectory Analysis for In-the-Wild Pose Error
  Estimation","Virtual content instability caused by device pose tracking error remains a
prevalent issue in markerless augmented reality (AR), especially on smartphones
and tablets. However, when examining environments which will host AR
experiences, it is challenging to determine where those instability artifacts
will occur; we rarely have access to ground truth pose to measure pose error,
and even if pose error is available, traditional visualizations do not connect
that data with the real environment, limiting their usefulness. To address
these issues we present SiTAR (Situated Trajectory Analysis for Augmented
Reality), the first situated trajectory analysis system for AR that
incorporates estimates of pose tracking error. We start by developing the first
uncertainty-based pose error estimation method for visual-inertial simultaneous
localization and mapping (VI-SLAM), which allows us to obtain pose error
estimates without ground truth; we achieve an average accuracy of up to 96.1%
and an average F1 score of up to 0.77 in our evaluations on four VI-SLAM
datasets. Next we present our SiTAR system, implemented for ARCore devices,
combining a backend that supplies uncertainty-based pose error estimates with a
frontend that generates situated trajectory visualizations. Finally, we
evaluate the efficacy of SiTAR in realistic conditions by testing three
visualization techniques in an in-the-wild study with 15 users and 13 diverse
environments; this study reveals the impact both environment scale and the
properties of surfaces present can have on user experience and task
performance.","['Tim Scargill', 'Ying Chen', 'Tianyi Hu', 'Maria Gorlatova']",2023-08-31T15:41:21Z,http://arxiv.org/abs/2308.16813v1
"Typing on Any Surface: A Deep Learning-based Method for Real-Time
  Keystroke Detection in Augmented Reality","Frustrating text entry interface has been a major obstacle in participating
in social activities in augmented reality (AR). Popular options, such as
mid-air keyboard interface, wireless keyboards or voice input, either suffer
from poor ergonomic design, limited accuracy, or are simply embarrassing to use
in public. This paper proposes and validates a deep-learning based approach,
that enables AR applications to accurately predict keystrokes from the user
perspective RGB video stream that can be captured by any AR headset. This
enables a user to perform typing activities on any flat surface and eliminates
the need of a physical or virtual keyboard. A two-stage model, combing an
off-the-shelf hand landmark extractor and a novel adaptive Convolutional
Recurrent Neural Network (C-RNN), was trained using our newly built dataset.
The final model was capable of adaptive processing user-perspective video
streams at ~32 FPS. This base model achieved an overall accuracy of $91.05\%$
when typing 40 Words per Minute (wpm), which is how fast an average person
types with two hands on a physical keyboard. The Normalised Levenshtein
Distance also further confirmed the real-world applicability of that our
approach. The promising results highlight the viability of our approach and the
potential for our method to be integrated into various applications. We also
discussed the limitations and future research required to bring such technique
into a production system.","['Xingyu Fu', 'Mingze Xi']",2023-08-31T23:58:25Z,http://arxiv.org/abs/2309.00174v2
RBF Weighted Hyper-Involution for RGB-D Object Detection,"A vast majority of conventional augmented reality devices are equipped with
depth sensors. Depth images produced by such sensors contain complementary
information for object detection when used with color images. Despite the
benefits, it remains a complex task to simultaneously extract photometric and
depth features in real time due to the immanent difference between depth and
color images. Moreover, standard convolution operations are not sufficient to
properly extract information directly from raw depth images leading to
intermediate representations of depth which is inefficient. To address these
issues, we propose a real-time and two stream RGBD object detection model. The
proposed model consists of two new components: a depth guided hyper-involution
that adapts dynamically based on the spatial interaction pattern in the raw
depth map and an up-sampling based trainable fusion layer that combines the
extracted depth and color image features without blocking the information
transfer between them. We show that the proposed model outperforms other RGB-D
based object detection models on NYU Depth v2 dataset and achieves comparable
(second best) results on SUN RGB-D. Additionally, we introduce a new outdoor
RGB-D object detection dataset where our proposed model outperforms other
models. The performance evaluation on diverse synthetic data generated from CAD
models and images shows the potential of the proposed model to be adapted to
augmented reality based applications.","['Mehfuz A Rahman', 'Jiju Peethambaran', 'Neil London']",2023-09-30T11:25:34Z,http://arxiv.org/abs/2310.00342v1
Can 5G NR Sidelink communications support wireless augmented reality?,"Smart glasses that support augmented reality (AR) have the potential to
become the consumer's primary medium of connecting to the future internet. For
the best quality of user experience, AR glasses must have a small form factor
and long battery life, while satisfying the data rate and latency requirements
of AR applications. To extend the AR glasses' battery life, the computation and
processing involved in AR may be offloaded to a companion device, such as a
smartphone, through a wireless connection. Sidelink (SL), i.e., the D2D
communication interface of 5G NR, is a potential candidate for this wireless
link. In this paper, we use system-level simulations to analyze the feasibility
of NR SL for supporting AR. Our simulator incorporates the PHY layer structure
and MAC layer resource scheduling of 3GPP SL, standard 3GPP channel models, and
MCS configurations. Our results suggest that the current SL standard
specifications are insufficient for high-end AR use cases with heavy
interaction but can support simpler previews and file transfers. We further
propose two enhancements to SL resource allocation, which have the potential to
offer significant performance improvements for AR applications.","['Ashutosh Srivastava', 'Qing Zhao', 'Yi Lu', 'Ping Wang', 'Qi Qu', 'Zhu Ji', 'Yee Sin Chan', 'Shivendra S. Panwar']",2023-10-03T19:48:47Z,http://arxiv.org/abs/2310.02399v1
"Augmented Reality and Human-Robot Collaboration Framework for
  Percutaneous Nephrolithotomy","During Percutaneous Nephrolithotomy (PCNL) operations, the surgeon is
required to define the incision point on the patient's back, align the needle
to a pre-planned path, and perform puncture operations afterward. The procedure
is currently performed manually using ultrasound or fluoroscopy imaging for
needle orientation, which, however, implies limited accuracy and low
reproducibility. This work incorporates Augmented Reality (AR) visualization
with an optical see-through head-mounted display (OST-HMD) and Human-Robot
Collaboration (HRC) framework to empower the surgeon's task completion
performance. In detail, Eye-to-Hand calibration, system registration, and
hologram model registration are performed to realize visual guidance. A
Cartesian impedance controller is used to guide the operator during the needle
puncture task execution. Experiments are conducted to verify the system
performance compared with conventional manual puncture procedures and a 2D
monitor-based visualisation interface. The results showed that the proposed
framework achieves the lowest median and standard deviation error across all
the experimental groups, respectively. Furthermore, the NASA-TLX user
evaluation results indicate that the proposed framework requires the lowest
workload score for task completion compared to other experimental setups. The
proposed framework exhibits significant potential for clinical application in
the PCNL task, as it enhances the surgeon's perception capability, facilitates
collision-free needle insertion path planning, and minimises errors in task
completion.","['Junling Fu', 'Matteo Pecorella', 'Elisa Iovene', 'Maria Chiara Palumbo', 'Alberto Rota', 'Alberto Redaelli', 'Giancarlo Ferrigno', 'Elena De Momi']",2024-01-09T11:12:29Z,http://arxiv.org/abs/2401.04492v1
"Theoretical efficiency limit of diffractive input couplers in augmented
  reality waveguides","Considerable efforts have been devoted into augmented reality (AR) displays
to enable the immersive user experience in the wearable glasses form factor.
Transparent waveguide combiners offer a compact solution to guide light from
the microdisplay to the front of eyes while maintaining the see-through optical
path to view the real world simultaneously. To deliver a realistic virtual
image with low power consumption, the waveguide combiners need to have high
efficiency and good image quality. One important limiting factor for the
efficiency of diffractive waveguide combiners is the out-coupling problem in
the input couplers, where the guided light interacts with the input gratings
again and get partially out-coupled. In this study, we introduce a theoretical
model to deterministically find the upper bound of the input efficiency of a
uniform input grating. Our model considers the polarization management at the
input coupler and can work for arbitrary input polarization state ensemble. Our
model also provides the corresponding characteristics of the input coupler,
such as the grating diffraction efficiencies and the Jones matrix of the
polarization management components, to achieve the optimal input efficiency.
Equipped with this theoretical model, we investigate how the upper bound of
input efficiency varies with geometric parameters including the waveguide
thickness, the projector pupil size, and the projector pupil relief distance.
Our study shines light on the fundamental efficiency limits on input couplers
in diffractive waveguide combiners and highlights the benefits of polarization
control in improving the input efficiency.","['Zhexin Zhao', 'Yun-Han Lee', 'Xiayu Feng', 'Michael J Escuti', 'Lu Lu', 'Barry Silverstein']",2024-01-12T21:36:18Z,http://arxiv.org/abs/2401.06900v1
"Monocular Microscope to CT Registration using Pose Estimation of the
  Incus for Augmented Reality Cochlear Implant Surgery","For those experiencing severe-to-profound sensorineural hearing loss, the
cochlear implant (CI) is the preferred treatment. Augmented reality (AR) aided
surgery can potentially improve CI procedures and hearing outcomes. Typically,
AR solutions for image-guided surgery rely on optical tracking systems to
register pre-operative planning information to the display so that hidden
anatomy or other important information can be overlayed and co-registered with
the view of the surgical scene. In this paper, our goal is to develop a method
that permits direct 2D-to-3D registration of the microscope video to the
pre-operative Computed Tomography (CT) scan without the need for external
tracking equipment. Our proposed solution involves using surface mapping of a
portion of the incus in surgical recordings and determining the pose of this
structure relative to the surgical microscope by performing pose estimation via
the perspective-n-point (PnP) algorithm. This registration can then be applied
to pre-operative segmentations of other anatomy-of-interest, as well as the
planned electrode insertion trajectory to co-register this information for the
AR display. Our results demonstrate the accuracy with an average rotation error
of less than 25 degrees and a translation error of less than 2 mm, 3 mm, and
0.55% for the x, y, and z axes, respectively. Our proposed method has the
potential to be applicable and generalized to other surgical procedures while
only needing a monocular microscope during intra-operation.","['Yike Zhang', 'Eduardo Davalos', 'Dingjie Su', 'Ange Lou', 'Jack H. Noble']",2024-03-12T00:26:08Z,http://arxiv.org/abs/2403.07219v1
"SLIMBRAIN: Augmented Reality Real-Time Acquisition and Processing System
  For Hyperspectral Classification Mapping with Depth Information for In-Vivo
  Surgical Procedures","Over the last two decades, augmented reality (AR) has led to the rapid
development of new interfaces in various fields of social and technological
application domains. One such domain is medicine, and to a higher extent
surgery, where these visualization techniques help to improve the effectiveness
of preoperative and intraoperative procedures. Following this trend, this paper
presents SLIMBRAIN, a real-time acquisition and processing AR system suitable
to classify and display brain tumor tissue from hyperspectral (HS) information.
This system captures and processes HS images at 14 frames per second (FPS)
during the course of a tumor resection operation to detect and delimit cancer
tissue at the same time the neurosurgeon operates. The result is represented in
an AR visualization where the classification results are overlapped with the
RGB point cloud captured by a LiDAR camera. This representation allows natural
navigation of the scene at the same time it is captured and processed,
improving the visualization and hence effectiveness of the HS technology to
delimit tumors. The whole system has been verified in real brain tumor
resection operations.","['Jaime Sancho', 'Manuel Villa', 'Miguel Chavarrías', 'Eduardo Juarez', 'Alfonso Lagares', 'César Sanz']",2024-03-25T11:10:49Z,http://arxiv.org/abs/2404.00048v1
"Investigating the impact of virtual element misalignment in
  collaborative Augmented Reality experiences","The collaboration in co-located shared environments has sparked an increased
interest in immersive technologies, including Augmented Reality (AR). Since
research in this field has primarily focused on individual user experiences in
AR, the collaborative aspects within shared AR spaces remain less explored, and
fewer studies can provide guidelines for designing this type of experience.
This article investigates how the user experience in a collaborative shared AR
space is affected by divergent perceptions of virtual objects and the effects
of positional synchrony and avatars. For this purpose, we developed an AR app
and used two distinct experimental conditions to study the influencing factors.
Forty-eight participants, organized into 24 pairs, participated in the
experiment and jointly interacted with shared virtual objects. Results indicate
that divergent perceptions of virtual objects did not directly influence
communication and collaboration dynamics. Conversely, positional synchrony
emerged as a critical factor, significantly enhancing the quality of the
collaborative experience. On the contrary, while not negligible, avatars played
a relatively less pronounced role in influencing these dynamics. The findings
can potentially offer valuable practical insights, guiding the development of
future collaborative AR/VR environments.","['Francesco Vona', 'Sina Hinzmann', 'Michael Stern', 'Tanja Kojić', 'Navid Ashrafi', 'David Grieshammer', 'Jan-Niklas Voigt-Antons']",2024-04-14T07:47:07Z,http://arxiv.org/abs/2404.09174v2
"Immersive Analysis: Enhancing Material Inspection of X-Ray Computed
  Tomography Datasets in Augmented Reality","This work introduces a novel Augmented Reality (AR) approach to visualize
material data alongside real objects in order to facilitate detailed material
analyses based on spatial non-destructive testing (NDT) data as generated in
X-ray computed tomography (XCT) imaging. For this purpose, we introduce a
framework that leverages the potential of AR devices, visualization and
interaction techniques to seamlessly explore complex primary and secondary XCT
data matched with real-world objects. The overall goal of the proposed analysis
scheme is to enable researchers and analysts to inspect material properties and
structures onsite and in-place. Coupling immersive visualization techniques
with real physical objects allows for highly intuitive workflows in material
analysis and inspection, which enables the identification of anomalies and
accelerates informed decision making. As a result, this framework generates an
immersive experience, which provides a more engaging and more natural analysis
of material data. A case study on fiber-reinforced polymer datasets was used to
validate the AR framework and its new workflow. Initial results revealed
positive feedback from experts, in particular regarding improved understanding
of spatial data and a more natural interaction with material samples, which may
have significant potential when combined with conventional analysis systems.","['Alexander Gall', 'Anja Heim', 'Patrick Weinberger', 'Bernhard Fröhler', 'Johann Kastner', 'Christoph Heinzl']",2024-04-19T09:57:06Z,http://arxiv.org/abs/2404.12751v1
"Attention and Sensory Processing in Augmented Reality: Empowering ADHD
  population","The brain's attention system is a complex and adaptive network of brain
regions that enables individuals to interact effectively with their
surroundings and perform complex tasks. This system involves the coordination
of various brain regions, including the prefrontal cortex and the parietal
lobes, to process and prioritize sensory information, manage tasks, and
maintain focus. In this study, we investigate the intricate mechanisms
underpinning the brain's attention system, followed by an exploration within
the context of augmented reality (AR) settings. AR emerges as a viable
technological intervention to address the multifaceted challenges faced by
individuals with Attention Deficit Hyperactivity Disorder (ADHD). Given that
the primary characteristics of ADHD include difficulties related to
inattention, hyperactivity, and impulsivity, AR offers tailor-made solutions
specifically designed to mitigate these challenges and enhance cognitive
functioning. On the other hand, if these ADHD-related issues are not adequately
addressed, it could lead to a worsening of their condition in AR. This
underscores the importance of employing effective interventions such as AR to
support individuals with ADHD in managing their symptoms. We examine the
attentional mechanisms within AR environments and the sensory processing
dynamics prevalent among the ADHD population. Our objective is to
comprehensively address the attentional needs of this population in AR settings
and offer a framework for designing cognitively accessible AR applications.","['Shiva Ghasemi', 'Majid Behravan', 'Sunday Uber', 'Denis Gracanin']",2024-05-02T11:58:43Z,http://arxiv.org/abs/2405.01218v1
"Semi-Automatic Infrared Calibration for Augmented Reality Systems in
  Surgery","Augmented reality (AR) has the potential to improve the immersion and
efficiency of computer-assisted orthopaedic surgery (CAOS) by allowing surgeons
to maintain focus on the operating site rather than external displays in the
operating theatre. Successful deployment of AR to CAOS requires a calibration
that can accurately calculate the spatial relationship between real and
holographic objects. Several studies attempt this calibration through manual
alignment or with additional fiducial markers in the surgical scene. We propose
a calibration system that offers a direct method for the calibration of AR
head-mounted displays (HMDs) with CAOS systems, by using infrared-reflective
marker-arrays widely used in CAOS. In our fast, user-agnostic setup, a HoloLens
2 detected the pose of marker arrays using infrared response and time-of-flight
depth obtained through sensors onboard the HMD. Registration with a
commercially available CAOS system was achieved when an IR marker-array was
visible to both devices. Study tests found relative-tracking mean errors of
2.03 mm and 1.12{\deg} when calculating the relative pose between two static
marker-arrays at short ranges. When using the calibration result to provide
in-situ holographic guidance for a simulated wire-insertion task, a
pre-clinical test reported mean errors of 2.07 mm and 1.54{\deg} when compared
to a pre-planned trajectory.","['Hisham Iqbal', 'Ferdinando Rodriguez y Baena']",2024-05-03T10:57:14Z,http://arxiv.org/abs/2405.01999v1
"With or Without Permission: Site-Specific Augmented Reality for Social
  Justice","Movements for social change are often tied to a particular locale. This makes
Augmented Reality (AR), which changes how people perceive their surroundings, a
promising technology for social justice. Site-specific AR empowers activists to
re-tell the story of a place, with or without permission of its owner. It has
been used, for example, to reveal hidden histories, re-imagine problematic
monuments, and celebrate minority cultures. However, challenges remain
concerning technological ownership and accessibility, scalability,
sustainability, and navigating collaborations with marginalized communities and
across disciplinary boundaries. This half-day workshop at CHI 2024 seeks to
bring together an interdisciplinary group of activists, computer scientists,
designers, media scholars, and more to identify opportunities and challenges
across domains. To anchor the discussion, participants will each share one
example of an artifact used in speculating, designing, and/or delivering
site-specific AR experiences. This collection of artifacts will inaugurate an
interactive database that can inspire a new wave of activists to leverage AR
for social justice.","['Rafael M. L. Silva', 'Ana María Cárdenas Gasca', 'Joshua A. Fisher', 'Erica Principe Cruz', 'Cinthya Jauregui', 'Amy Lueck', 'Fannie Liu', 'Andrés Monroy-Hernández', 'Kai Lukoff']",2024-05-06T23:05:58Z,http://arxiv.org/abs/2405.03898v1
"Intensity and Texture Correction of Omnidirectional Image Using Camera
  Images for Indirect Augmented Reality","Augmented reality (AR) using camera images in mobile devices is becoming
popular for tourism promotion. However, obstructions such as tourists appearing
in the camera images may cause the camera pose estimation error, resulting in
CG misalignment and reduced visibility of the contents. To avoid this problem,
Indirect AR (IAR), which does not use real-time camera images, has been
proposed. In this method, an omnidirectional image is captured and virtual
objects are synthesized on the image in advance. Users can experience AR by
viewing a scene extracted from the synthesized omnidirectional image according
to the device's sensor. This enables robustness and high visibility. However,
if the weather conditions and season in the pre-captured 360 images differs
from the current weather conditions and season when AR is experienced, the
realism of the AR experience is reduced. To overcome the problem, we propose a
method for correcting the intensity and texture of a past omnidirectional image
using camera images from mobile devices. We first perform semantic
segmentation. We then reproduce the current sky pattern by panoramic image
composition and inpainting. For the other areas, we correct the intensity by
histogram matching. In experiments, we show the effectiveness of the proposed
method using various scenes.","['Hakim Ikebayashi', 'Norihiko Kawai']",2024-05-25T02:14:07Z,http://arxiv.org/abs/2405.16008v1
"HoloDevice: Holographic Cross-Device Interactions for Remote
  Collaboration","This paper introduces holographic cross-device interaction, a new class of
remote cross-device interactions between local physical devices and
holographically rendered remote devices. Cross-device interactions have enabled
a rich set of interactions with device ecologies. Most existing research
focuses on co-located settings (meaning when users and devices are in the same
physical space) to achieve these rich interactions and affordances. In
contrast, holographic cross-device interaction allows remote interactions
between devices at distant locations by providing a rich visual affordance
through real-time holographic rendering of the device's motion, content, and
interactions on mixed reality head-mounted displays. This maintains the
advantages of having a physical device, such as precise input through touch and
pen interaction. Through holographic rendering, not only can remote devices
interact as if they are co-located, but they can also be virtually augmented to
further enrich interactions, going beyond what is possible with existing
cross-device systems. To demonstrate this concept, we developed HoloDevice, a
prototype system for holographic cross-device interaction using the Microsoft
Hololens 2 augmented reality headset. Our contribution is threefold. First, we
introduce the concept of holographic cross-device interaction. Second, we
present a design space containing three unique benefits, which include: (1)
spatial visualization of interaction and motion, (2) rich visual affordances
for intermediate transition, and (3) dynamic and fluid configuration. Last we
discuss a set of implementation demonstrations and use-case scenarios that
further explore the space.","['Neil Chulpongsatorn', 'Thien-Kim Nguyen', 'Nicolai Marquardt', 'Ryo Suzuki']",2024-05-28T22:49:01Z,http://arxiv.org/abs/2405.19377v1
Augmenting Visual SLAM with Wi-Fi Sensing For Indoor Applications,"Recent trends have accelerated the development of spatial applications on
mobile devices and robots. These include navigation, augmented reality,
human-robot interaction, and others. A key enabling technology for such
applications is the understanding of the device's location and the map of the
surrounding environment. This generic problem, referred to as Simultaneous
Localization and Mapping (SLAM), is an extensively researched topic in
robotics. However, visual SLAM algorithms face several challenges including
perceptual aliasing and high computational cost. These challenges affect the
accuracy, efficiency, and viability of visual SLAM algorithms, especially for
long-term SLAM, and their use in resource-constrained mobile devices.
  A parallel trend is the ubiquity of Wi-Fi routers for quick Internet access
in most urban environments. Most robots and mobile devices are equipped with a
Wi-Fi radio as well. We propose a method to utilize Wi-Fi received signal
strength to alleviate the challenges faced by visual SLAM algorithms. To
demonstrate the utility of this idea, this work makes the following
contributions: (i) We propose a generic way to integrate Wi-Fi sensing into
visual SLAM algorithms, (ii) We integrate such sensing into three well-known
SLAM algorithms, (iii) Using four distinct datasets, we demonstrate the
performance of such augmentation in comparison to the original visual
algorithms and (iv) We compare our work to Wi-Fi augmented FABMAP algorithm.
Overall, we show that our approach can improve the accuracy of visual SLAM
algorithms by 11% on average and reduce computation time on average by 15% to
25%.","['Zakieh S. Hashemifar', 'Charuvahan Adhivarahan', 'Anand Balakrishnan', 'Karthik Dantu']",2019-03-15T17:35:32Z,http://arxiv.org/abs/1903.06687v1
"AI-Augmented Behavior Analysis for Children with Developmental
  Disabilities: Building Towards Precision Treatment","Autism spectrum disorder is a developmental disorder characterized by
significant social, communication, and behavioral challenges. Individuals
diagnosed with autism, intellectual, and developmental disabilities (AUIDD)
typically require long-term care and targeted treatment and teaching. Effective
treatment of AUIDD relies on efficient and careful behavioral observations done
by trained applied behavioral analysts (ABAs). However, this process
overburdens ABAs by requiring the clinicians to collect and analyze data,
identify the problem behaviors, conduct pattern analysis to categorize and
predict categorical outcomes, hypothesize responsiveness to treatments, and
detect the effects of treatment plans. Successful integration of digital
technologies into clinical decision-making pipelines and the advancements in
automated decision-making using Artificial Intelligence (AI) algorithms
highlights the importance of augmenting teaching and treatments using novel
algorithms and high-fidelity sensors. In this article, we present an
AI-Augmented Learning and Applied Behavior Analytics (AI-ABA) platform to
provide personalized treatment and learning plans to AUIDD individuals. By
defining systematic experiments along with automated data collection and
analysis, AI-ABA can promote self-regulative behavior using reinforcement-based
augmented or virtual reality and other mobile platforms. Thus, AI-ABA could
assist clinicians to focus on making precise data-driven decisions and increase
the quality of individualized interventions for individuals with AUIDD.","['Shadi Ghafghazi', 'Amarie Carnett', 'Leslie Neely', 'Arun Das', 'Paul Rad']",2021-02-21T16:15:40Z,http://arxiv.org/abs/2102.10635v2
"Global Adaptation meets Local Generalization: Unsupervised Domain
  Adaptation for 3D Human Pose Estimation","When applying a pre-trained 2D-to-3D human pose lifting model to a target
unseen dataset, large performance degradation is commonly encountered due to
domain shift issues. We observe that the degradation is caused by two factors:
1) the large distribution gap over global positions of poses between the source
and target datasets due to variant camera parameters and settings, and 2) the
deficient diversity of local structures of poses in training. To this end, we
combine \textbf{global adaptation} and \textbf{local generalization} in
\textit{PoseDA}, a simple yet effective framework of unsupervised domain
adaptation for 3D human pose estimation. Specifically, global adaptation aims
to align global positions of poses from the source domain to the target domain
with a proposed global position alignment (GPA) module. And local
generalization is designed to enhance the diversity of 2D-3D pose mapping with
a local pose augmentation (LPA) module. These modules bring significant
performance improvement without introducing additional learnable parameters. In
addition, we propose local pose augmentation (LPA) to enhance the diversity of
3D poses following an adversarial training scheme consisting of 1) a
augmentation generator that generates the parameters of pre-defined pose
transformations and 2) an anchor discriminator to ensure the reality and
quality of the augmented data. Our approach can be applicable to almost all
2D-3D lifting models. \textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHP
under a cross-dataset evaluation setup, improving upon the previous
state-of-the-art method by 10.2\%.","['Wenhao Chai', 'Zhongyu Jiang', 'Jenq-Neng Hwang', 'Gaoang Wang']",2023-03-29T04:54:42Z,http://arxiv.org/abs/2303.16456v2
"Encode-Store-Retrieve: Enhancing Memory Augmentation through
  Language-Encoded Egocentric Perception","We depend on our own memory to encode, store, and retrieve our experiences.
However, memory lapses can occur. One promising avenue for achieving memory
augmentation is through the use of augmented reality head-mounted displays to
capture and preserve egocentric videos, a practice commonly referred to as life
logging. However, a significant challenge arises from the sheer volume of video
data generated through life logging, as the current technology lacks the
capability to encode and store such large amounts of data efficiently. Further,
retrieving specific information from extensive video archives requires
substantial computational power, further complicating the task of quickly
accessing desired content. To address these challenges, we propose a memory
augmentation system that involves leveraging natural language encoding for
video data and storing them in a vector database. This approach harnesses the
power of large vision language models to perform the language encoding process.
Additionally, we propose using large language models to facilitate natural
language querying. Our system underwent extensive evaluation using the QA-Ego4D
dataset and achieved state-of-the-art results with a BLEU score of 8.3,
outperforming conventional machine learning models that scored between 3.4 and
5.8. Additionally, in a user study, our system received a higher mean response
score of 4.13/5 compared to the human participants' score of 2.46/5 on
real-life episodic memory tasks.","['Junxiao Shen', 'John Dudley', 'Per Ola Kristensson']",2023-08-10T18:43:44Z,http://arxiv.org/abs/2308.05822v1
"Virtually Enriched NYU Depth V2 Dataset for Monocular Depth Estimation:
  Do We Need Artificial Augmentation?","We present ANYU, a new virtually augmented version of the NYU depth v2
dataset, designed for monocular depth estimation. In contrast to the well-known
approach where full 3D scenes of a virtual world are utilized to generate
artificial datasets, ANYU was created by incorporating RGB-D representations of
virtual reality objects into the original NYU depth v2 images. We specifically
did not match each generated virtual object with an appropriate texture and a
suitable location within the real-world image. Instead, an assignment of
texture, location, lighting, and other rendering parameters was randomized to
maximize a diversity of the training data, and to show that it is randomness
that can improve the generalizing ability of a dataset. By conducting extensive
experiments with our virtually modified dataset and validating on the original
NYU depth v2 and iBims-1 benchmarks, we show that ANYU improves the monocular
depth estimation performance and generalization of deep neural networks with
considerably different architectures, especially for the current
state-of-the-art VPD model. To the best of our knowledge, this is the first
work that augments a real-world dataset with randomly generated virtual 3D
objects for monocular depth estimation. We make our ANYU dataset publicly
available in two training configurations with 10% and 100% additional
synthetically enriched RGB-D pairs of training images, respectively, for
efficient training and empirical exploration of virtual augmentation at
https://github.com/ABrain-One/ANYU","['Dmitry Ignatov', 'Andrey Ignatov', 'Radu Timofte']",2024-04-15T05:44:03Z,http://arxiv.org/abs/2404.09469v1
"Augmented Conversation with Embedded Speech-Driven On-the-Fly
  Referencing in AR","This paper introduces the concept of augmented conversation, which aims to
support co-located in-person conversations via embedded speech-driven
on-the-fly referencing in augmented reality (AR). Today computing technologies
like smartphones allow quick access to a variety of references during the
conversation. However, these tools often create distractions, reducing eye
contact and forcing users to focus their attention on phone screens and
manually enter keywords to access relevant information. In contrast, AR-based
on-the-fly referencing provides relevant visual references in real-time, based
on keywords extracted automatically from the spoken conversation. By embedding
these visual references in AR around the conversation partner, augmented
conversation reduces distraction and friction, allowing users to maintain eye
contact and supporting more natural social interactions. To demonstrate this
concept, we developed \system, a Hololens-based interface that leverages
real-time speech recognition, natural language processing and gaze-based
interactions for on-the-fly embedded visual referencing. In this paper, we
explore the design space of visual referencing for conversations, and describe
our our implementation -- building on seven design guidelines identified
through a user-centered design process. An initial user study confirms that our
system decreases distraction and friction in conversations compared to
smartphone searches, while providing highly useful and relevant information.","['Shivesh Jadon', 'Mehrad Faridan', 'Edward Mah', 'Rajan Vaish', 'Wesley Willett', 'Ryo Suzuki']",2024-05-28T19:10:47Z,http://arxiv.org/abs/2405.18537v1
"CAVE-AR: A VR Authoring System to Interactively Design, Simulate, and
  Debug Multi-user AR Experiences","Despite advances in augmented reality (AR), the process of creating
meaningful experiences with this technology is still extremely challenging. Due
to different tracking implementations and hardware constraints, developing AR
applications either requires low-level programming skills, or is done through
specific authoring tools that largely sacrifice the possibility of customizing
the AR experience. Existing development workflows also do not support
previewing or simulating the AR experience, requiring a lengthy process of
trial and error by which content creators deploy and physically test
applications in each iteration. To mitigate these limitations, we propose
CAVE-AR, a novel virtual reality system for authoring, simulating and debugging
custom AR experiences. Available both as a standalone or a plug-in tool,
CAVE-AR is based on the concept of representing in the same global reference
system both in AR content and tracking information, mixing geographical
information, architectural features, and sensor data to simulate the context of
an AR experience. Thanks to its novel abstraction of existing tracking
technologies, CAVE-AR operates independently of users' devices, and integrates
with existing programming tools to provide maximum flexibility. Our VR
application provides designers with ways to create and modify an AR
application, even while others are in the midst of using it. CAVE-AR further
allows the designer to track how users are behaving, preview what they are
currently seeing, and interact with them through several different channels. To
illustrate our proposed development workflow and demonstrate the advantages of
our authoring system, we introduce two CAVEAR use cases in which an augmented
reality application is created and tested. We compare the CAVE-AR workflow to
traditional development methods and demonstrate the importance of simulation
and live application debugging.","['Marco Cavallo', 'Angus G. Forbes']",2018-09-14T16:52:06Z,http://arxiv.org/abs/1809.05500v2
"Visualization of Real-time Displacement Time History superimposed with
  Dynamic Experiments using Wireless Smart Sensors (WSS) and Augmented Reality
  (AR)","Wireless Smart Sensors (WSS) process field data and inform structural
engineers and owners about the infrastructure health and safety. In bridge
engineering, inspectors make decisions using objective data from each bridge.
They decide about repairs and replacements and prioritize the maintenance of
certain structure elements on the basis of changes in displacements under
loads. However, access to displacement information in the field and in
real-time remains a challenge. Displacement data provided by WSS in the field
undergoes additional processing and is seen at a different location by an
inspector and a sensor specialist. When the data is shared and streamed to the
field inspector, there is a inter-dependence between inspectors, sensor
specialists, and infrastructure owners, which limits the actionability of the
data related to the bridge condition. If inspectors were able to see structural
displacements in real-time at the locations of interest, they could conduct
additional observations, which would create a new, information-based,
decision-making reality in the field. This paper develops a new, human-centered
interface that provides inspectors with real-time access to actionable
structural data (real-time displacements under loads) during inspection and
monitoring enhanced by Augmented Reality (AR). It summarizes the development
and validation of the new human-infrastructure interface and evaluates its
efficiency through laboratory experiments. The experiments demonstrate that the
interface accurately estimates dynamic displacements in comparison with the
laser. Using this new AR interface tool, inspectors can observe and compare
displacement data, share it across space and time, and visualize displacements
in time history.","['M. Aguero', 'D. Doyle', 'D. Mascarenas', 'F. Moreu']",2021-10-17T01:40:28Z,http://arxiv.org/abs/2110.08700v1
"Augmented Reality Warnings in Roadway Work Zones: Evaluating the Effect
  of Modality on Worker Reaction Times","Given the aging highway infrastructure requiring extensive rebuilding and
enhancements, and the consequent rise in the number of work zones, there is an
urgent need to develop advanced safety systems to protect workers. While
Augmented Reality (AR) holds significant potential for delivering warnings to
workers, its integration into roadway work zones remains relatively unexplored.
The primary objective of this study is to improve safety measures within
roadway work zones by conducting an extensive analysis of how different
combinations of multimodal AR warnings influence the reaction times of workers.
This paper addresses this gap through a series of experiments that aim to
replicate the distinctive conditions of roadway work zones, both in real-world
and virtual reality environments. Our approach comprises three key components:
an advanced AR system prototype, a VR simulation of AR functionality within the
work zone environment, and the Wizard of Oz technique to synchronize user
experiences across experiments. To assess reaction times, we leverage both the
simple reaction time (SRT) technique and an innovative vision-based metric that
utilizes real-time pose estimation. By conducting five experiments in
controlled outdoor work zones and indoor VR settings, our study provides
valuable information on how various multimodal AR warnings impact workers
reaction times. Furthermore, our findings reveal the disparities in reaction
times between VR simulations and real-world scenarios, thereby gauging VR's
capability to mirror the dynamics of roadway work zones. Furthermore, our
results substantiate the potential and reliability of vision-based reaction
time measurements. These insights resonate well with those derived using the
SRT technique, underscoring the viability of this approach for tangible
real-world uses.","['Sepehr Sabeti', 'Fatemeh Banani Ardecani', 'Omidreza Shoghli']",2024-03-22T18:52:10Z,http://arxiv.org/abs/2403.15571v2
"Machine Learning Distinguishes Neurosurgical Skill Levels in a Virtual
  Reality Tumor Resection Task","Background: Virtual reality simulators and machine learning have the
potential to augment understanding, assessment and training of psychomotor
performance in neurosurgery residents. Objective: This study outlines the first
application of machine learning to distinguish ""skilled"" and ""novice""
psychomotor performance during a virtual reality neurosurgical task. Methods:
Twenty-three neurosurgeons and senior neurosurgery residents comprising the
""skilled"" group and 92 junior neurosurgery residents and medical students the
""novice"" group. The task involved removing a series of virtual brain tumors
without causing injury to surrounding tissue. Over 100 features were extracted
and 68 selected using t-test analysis. These features were provided to 4
classifiers: K-Nearest Neighbors, Parzen Window, Support Vector Machine, and
Fuzzy K-Nearest Neighbors. Equal Error Rate was used to assess classifier
performance. Results: Ratios of train set size to test set size from 10% to 90%
and 5 to 30 features, chosen by the forward feature selection algorithm, were
employed. A working point of 50% train to test set size ratio and 15 features
resulted in an equal error rates as low as 8.3% using the Fuzzy K-Nearest
Neighbors classifier. Conclusion: Machine learning may be one component helping
realign the traditional apprenticeship educational paradigm to a more objective
model based on proven performance standards.
  Keywords: Artificial intelligence, Classifiers, Machine learning,
Neurosurgery skill assessment, Surgical education, Tumor resection, Virtual
reality simulation","['Samaneh Siyar', 'Hamed Azarnoush', 'Saeid Rashidi', 'Alexandre Winkler-Schwartz', 'Vincent Bissonnette', 'Nirros Ponnudurai', 'Rolando F. Del Maestro']",2018-11-20T10:09:02Z,http://arxiv.org/abs/1811.08159v1
"XR: Enabling training mode in the human brain XR: Enabling training mode
  in the human brain","The face of simulation-based training has greatly evolved, with the most
recent tools giving the ability to create virtual environments that rival
realism. At first glance, it might appear that what the training sector needs
is the most realistic simulators possible, but traditional simulators are not
necessarily the most efficient or practical training tools. With all that these
new technologies have to offer; the challenge is to go back to the core of
training needs and identify the right vector of sensory cues that will most
effectively enable training mode in the human brain. Bigger and Pricier doesn't
necessarily mean better. Simulation with cross-reality content (XR), which by
definition encompasses virtual reality (VR), mixed reality (MR), and augmented
reality (AR), is the most practical solution for deploying any kind of
simulation-based training. The authors of this paper (a teacher and a
technology expert) share their experiences and expose XR-specific best
practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :
Starting his career in the modeling and simulation community more than 15 years
ago, S{\'e}bastien has focused on learning about the latest simulation
innovations and sharing information on how experts have solved their
challenges. He worked on the COTS integration at CAE and the Presagis focusing
on Simulation and Visualization products. More recently, Sebastien put together
simulation and training teams and strategies for emerging companies like CM
Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games,
focusing on helping companies develop real-time solutions for simulation-based
training. Philippe Lepinard: Former military helicopter pilot and simulation
officer, Philippe L{\'e}pinard is now an associate professor at the University
of Paris-Est Cr{\'e}teil (UPEC). His research is focusing on playful learning
and training through simulation. He is one of the founding members of the
French simulation association.","['Philippe Lépinard', 'Sébastien Lozé']",2019-04-26T07:55:39Z,http://arxiv.org/abs/1904.11704v1
Monocular LSD-SLAM Integration within AR System,"In this paper, we cover the process of integrating Large-Scale Direct
Simultaneous Localization and Mapping (LSD-SLAM) algorithm into our existing AR
stereo engine, developed for our modified ""Augmented Reality Oculus Rift"". With
that, we are able to track one of our realworld cameras which are mounted on
the rift, within a complete unknown environment. This makes it possible to
achieve a constant and full augmentation, synchronizing our 3D movement (x, y,
z) in both worlds, the real world and the virtual world. The development for
the basic AR setup using the Oculus Rift DK1 and two fisheye cameras is fully
documented in our previous paper. After an introduction to image-based
registration, we detail the LSD-SLAM algorithm and document our code
implementing our integration. The AR stereo engine with Oculus Rift support can
be accessed via the GIT repository https://github.com/MaXvanHeLL/ARift.git and
the modified LSD-SLAM project used for the integration is available here
https://github.com/MaXvanHeLL/LSD-SLAM.git.","['Markus Höll', 'Vincent Lepetit']",2017-02-08T16:52:19Z,http://arxiv.org/abs/1702.02514v1
To Know Where We Are: Vision-Based Positioning in Outdoor Environments,"Augmented reality (AR) displays become more and more popular recently,
because of its high intuitiveness for humans and high-quality head-mounted
display have rapidly developed. To achieve such displays with augmented
information, highly accurate image registration or ego-positioning are
required, but little attention have been paid for out-door environments. This
paper presents a method for ego-positioning in outdoor environments with low
cost monocular cameras. To reduce the computational and memory requirements as
well as the communication overheads, we formulate the model compression
algorithm as a weighted k-cover problem for better preserving model structures.
Specifically for real-world vision-based positioning applications, we consider
the issues with large scene change and propose a model update algorithm to
tackle these problems. A long- term positioning dataset with more than one
month, 106 sessions, and 14,275 images is constructed. Based on both local and
up-to-date models constructed in our approach, extensive experimental results
show that high positioning accuracy (mean ~ 30.9cm, stdev. ~ 15.4cm) can be
achieved, which outperforms existing vision-based algorithms.","['Kuan-Wen Chen', 'Chun-Hsin Wang', 'Xiao Wei', 'Qiao Liang', 'Ming-Hsuan Yang', 'Chu-Song Chen', 'Yi-Ping Hung']",2015-06-19T03:11:33Z,http://arxiv.org/abs/1506.05870v1
Real-time deep hair matting on mobile devices,"Augmented reality is an emerging technology in many application domains.
Among them is the beauty industry, where live virtual try-on of beauty products
is of great importance. In this paper, we address the problem of live hair
color augmentation. To achieve this goal, hair needs to be segmented quickly
and accurately. We show how a modified MobileNet CNN architecture can be used
to segment the hair in real-time. Instead of training this network using large
amounts of accurate segmentation data, which is difficult to obtain, we use
crowd sourced hair segmentation data. While such data is much simpler to
obtain, the segmentations there are noisy and coarse. Despite this, we show how
our system can produce accurate and fine-detailed hair mattes, while running at
over 30 fps on an iPad Pro tablet.","['Alex Levinshtein', 'Cheng Chang', 'Edmund Phung', 'Irina Kezele', 'Wenzhangzhi Guo', 'Parham Aarabi']",2017-12-19T19:36:08Z,http://arxiv.org/abs/1712.07168v2
"Prescription AR: A Fully-Customized Prescription-Embedded Augmented
  Reality Display","In this paper, we present a fully-customized AR display design that considers
the user's prescription, interpupillary distance, and taste of fashion. A
free-form image combiner embedded inside the prescription lens provides
augmented images onto the vision-corrected real world. We establish a
prescription-embedded AR display optical design method as well as the
customization method for individual users. Our design can cover myopia,
hyperopia, astigmatism, and presbyopia, and allows the eye-contact interaction
with privacy protection. A 169$g$ dynamic prototype showed a 40$^\circ$
$\times$ 20 $^\circ$ virtual image with a 23 cpd resolution at center field and
6 mm $\times$ 4 mm eye box, with the vision-correction and varifocal (0.5-3$m$)
capability.","['Jui-Yi Wu', 'Jonghyun Kim']",2019-07-09T18:21:50Z,http://arxiv.org/abs/1907.04353v1
"X-Vision: An augmented vision tool with real-time sensing ability in
  tagged environments","We present the concept of X-Vision, an enhanced Augmented Reality (AR)-based
visualization tool, with the real-time sensing capability in a tagged
environment. We envision that this type of a tool will enhance the
user-environment interaction and improve the productivity in factories,
smart-spaces, home & office environments, maintenance/facility rooms and
operation theatres, etc. In this paper, we describe the design of this
visualization system built upon combining the object's pose information
estimated by the depth camera and the object's ID & physical attributes
captured by the RFID tags. We built a physical prototype of the system
demonstrating the projection of 3D holograms of the objects encoded with sensed
information like water-level and temperature of common office/household
objects. The paper also discusses the quality metrics used to compare the pose
estimation algorithms for robust reconstruction of the object's 3D data.","['Yongbin Sun', 'Sai Nithin R. Kantareddy', 'Rahul Bhattacharyya', 'Sanjay E. Sarma']",2018-06-02T01:34:12Z,http://arxiv.org/abs/1806.00567v2
Deep ChArUco: Dark ChArUco Marker Pose Estimation,"ChArUco boards are used for camera calibration, monocular pose estimation,
and pose verification in both robotics and augmented reality. Such fiducials
are detectable via traditional computer vision methods (as found in OpenCV) in
well-lit environments, but classical methods fail when the lighting is poor or
when the image undergoes extreme motion blur. We present Deep ChArUco, a
real-time pose estimation system which combines two custom deep networks,
ChArUcoNet and RefineNet, with the Perspective-n-Point (PnP) algorithm to
estimate the marker's 6DoF pose. ChArUcoNet is a two-headed marker-specific
convolutional neural network (CNN) which jointly outputs ID-specific
classifiers and 2D point locations. The 2D point locations are further refined
into subpixel coordinates using RefineNet. Our networks are trained using a
combination of auto-labeled videos of the target marker, synthetic subpixel
corner data, and extreme data augmentation. We evaluate Deep ChArUco in
challenging low-light, high-motion, high-blur scenarios and demonstrate that
our approach is superior to a traditional OpenCV-based method for ChArUco
marker detection and pose estimation.","['Danying Hu', 'Daniel DeTone', 'Vikram Chauhan', 'Igor Spivak', 'Tomasz Malisiewicz']",2018-12-08T00:52:47Z,http://arxiv.org/abs/1812.03247v2
Expert-Augmented Machine Learning,"Machine Learning is proving invaluable across disciplines. However, its
success is often limited by the quality and quantity of available data, while
its adoption by the level of trust that models afford users. Human vs. machine
performance is commonly compared empirically to decide whether a certain task
should be performed by a computer or an expert. In reality, the optimal
learning strategy may involve combining the complementary strengths of man and
machine. Here we present Expert-Augmented Machine Learning (EAML), an automated
method that guides the extraction of expert knowledge and its integration into
machine-learned models. We use a large dataset of intensive care patient data
to predict mortality and show that we can extract expert knowledge using an
online platform, help reveal hidden confounders, improve generalizability on a
different population and learn using less data. EAML presents a novel framework
for high performance and dependable machine learning in critical applications.","['E. D. Gennatas', 'J. H. Friedman', 'L. H. Ungar', 'R. Pirracchio', 'E. Eaton', 'L. Reichman', 'Y. Interian', 'C. B. Simone', 'A. Auerbach', 'E. Delgado', 'M. J. Van der Laan', 'T. D. Solberg', 'G. Valdes']",2019-03-22T23:32:22Z,http://arxiv.org/abs/1903.09731v3
"Inverse Rendering for Complex Indoor Scenes: Shape, Spatially-Varying
  Lighting and SVBRDF from a Single Image","We propose a deep inverse rendering framework for indoor scenes. From a
single RGB image of an arbitrary indoor scene, we create a complete scene
reconstruction, estimating shape, spatially-varying lighting, and
spatially-varying, non-Lambertian surface reflectance. To train this network,
we augment the SUNCG indoor scene dataset with real-world materials and render
them with a fast, high-quality, physically-based GPU renderer to create a
large-scale, photorealistic indoor dataset. Our inverse rendering network
incorporates physical insights -- including a spatially-varying spherical
Gaussian lighting representation, a differentiable rendering layer to model
scene appearance, a cascade structure to iteratively refine the predictions and
a bilateral solver for refinement -- allowing us to jointly reason about shape,
lighting, and reflectance. Experiments show that our framework outperforms
previous methods for estimating individual scene components, which also enables
various novel applications for augmented reality, such as photorealistic object
insertion and material editing. Code and data will be made publicly available.","['Zhengqin Li', 'Mohammad Shafiei', 'Ravi Ramamoorthi', 'Kalyan Sunkavalli', 'Manmohan Chandraker']",2019-05-07T17:47:40Z,http://arxiv.org/abs/1905.02722v1
"Efficient 2.5D Hand Pose Estimation via Auxiliary Multi-Task Training
  for Embedded Devices","2D Key-point estimation is an important precursor to 3D pose estimation
problems for human body and hands. In this work, we discuss the data,
architecture, and training procedure necessary to deploy extremely efficient
2.5D hand pose estimation on embedded devices with highly constrained memory
and compute envelope, such as AR/VR wearables. Our 2.5D hand pose estimation
consists of 2D key-point estimation of joint positions on an egocentric image,
captured by a depth sensor, and lifted to 2.5D using the corresponding depth
values. Our contributions are two fold: (a) We discuss data labeling and
augmentation strategies, the modules in the network architecture that
collectively lead to $3\%$ the flop count and $2\%$ the number of parameters
when compared to the state of the art MobileNetV2 architecture. (b) We propose
an auxiliary multi-task training strategy needed to compensate for the small
capacity of the network while achieving comparable performance to MobileNetV2.
Our 32-bit trained model has a memory footprint of less than 300 Kilobytes,
operates at more than 50 Hz with less than 35 MFLOPs.","['Prajwal Chidananda', 'Ayan Sinha', 'Adithya Rao', 'Douglas Lee', 'Andrew Rabinovich']",2019-09-12T18:33:05Z,http://arxiv.org/abs/1909.05897v1
Voxel-Based Indoor Reconstruction From HoloLens Triangle Meshes,"Current mobile augmented reality devices are often equipped with range
sensors. The Microsoft HoloLens for instance is equipped with a Time-Of-Flight
(ToF) range camera providing coarse triangle meshes that can be used in custom
applications. We suggest to use the triangle meshes for the automatic
generation of indoor models that can serve as basis for augmenting their
physical counterpart with location-dependent information. In this paper, we
present a novel voxel-based approach for automated indoor reconstruction from
unstructured three-dimensional geometries like triangle meshes. After an
initial voxelization of the input data, rooms are detected in the resulting
voxel grid by segmenting connected voxel components of ceiling candidates and
extruding them downwards to find floor candidates. Semantic class labels like
'Wall', 'Wall Opening', 'Interior Object' and 'Empty Interior' are then
assigned to the room voxels in-between ceiling and floor by a rule-based voxel
sweep algorithm. Finally, the geometry of the detected walls and their openings
is refined in voxel representation. The proposed approach is not restricted to
Manhattan World scenarios and does not rely on room surfaces being planar.","['P. Hübner', 'M. Weinmann', 'S. Wursthorn']",2020-02-18T16:15:17Z,http://arxiv.org/abs/2002.07689v1
Democratizing the Edge: A Pervasive Edge Computing Framework,"The needs of emerging applications, such as augmented and virtual reality,
federated machine learning, and autonomous driving, have motivated edge
computing--the push of computation capabilities to the edge. Various edge
computing architectures have emerged, including multi-access edge computing and
edge-cloud, all with the premise of reducing communication latency and
augmenting privacy. However, these architectures rely on static and
pre-deployed infrastructure, falling short in harnessing the abundant resources
at the network's edge. In this paper, we discuss the design of Pervasive Edge
Computing (PEC)--a democratized edge computing framework, which enables
end-user devices (e.g., smartphones, IoT devices, and vehicles) to dynamically
participate in a large-scale computing ecosystem. Our vision of the
democratized edge involves the real-time composition of services using
available edge resources like data, software, and compute-hardware from
multiple stakeholders. We discuss how the novel Named-Data Networking
architecture can facilitate service deployment, discovery, invocation, and
migration. We also discuss the economic models critical to the adoption of PEC
and the outstanding challenges for its full realization.","['Reza Tourani', 'Srikathyayani Srikanteswara', 'Satyajayant Misra', 'Richard Chow', 'Lily Yang', 'Xiruo Liu', 'Yi Zhang']",2020-07-01T17:46:08Z,http://arxiv.org/abs/2007.00641v1
"IllumiNet: Transferring Illumination from Planar Surfaces to Virtual
  Objects in Augmented Reality","This paper presents an illumination estimation method for virtual objects in
real environment by learning. While previous works tackled this problem by
reconstructing high dynamic range (HDR) environment maps or the corresponding
spherical harmonics, we do not seek to recover the lighting environment of the
entire scene. Given a single RGB image, our method directly infers the relit
virtual object by transferring the illumination features extracted from planar
surfaces in the scene to the desired geometries. Compared to previous works,
our approach is more robust as it works in both indoor and outdoor environments
with spatially-varying illumination. Experiments and evaluation results show
that our approach outperforms the state-of-the-art quantitatively and
qualitatively, achieving realistic augmented experience.","['Di Xu', 'Zhen Li', 'Yanning Zhang', 'Qi Cao']",2020-07-12T13:11:14Z,http://arxiv.org/abs/2007.05981v1
MobileDepth: Efficient Monocular Depth Prediction on Mobile Devices,"Depth prediction is fundamental for many useful applications on computer
vision and robotic systems. On mobile phones, the performance of some useful
applications such as augmented reality, autofocus and so on could be enhanced
by accurate depth prediction. In this work, an efficient fully convolutional
network architecture for depth prediction has been proposed, which uses RegNetY
06 as the encoder and split-concatenate shuffle blocks as decoder. At the same
time, an appropriate combination of data augmentation, hyper-parameters and
loss functions to efficiently train the lightweight network has been provided.
Also, an Android application has been developed which can load CNN models to
predict depth map by the monocular images captured from the mobile camera and
evaluate the average latency and frame per second of the models. As a result,
the network achieves 82.7% {\delta}1 accuracy on NYU Depth v2 dataset and at
the same time, have only 62ms latency on ARM A76 CPUs so that it can predict
the depth map from the mobile camera in real-time.",['Yekai Wang'],2020-11-20T03:08:54Z,http://arxiv.org/abs/2011.10189v1
"Where is your place, Visual Place Recognition?","Visual Place Recognition (VPR) is often characterized as being able to
recognize the same place despite significant changes in appearance and
viewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling
robotic platforms and intelligent augmentation platforms such as augmented
reality devices to perceive and understand the physical world. In this paper,
we observe that there are three ""drivers"" that impose requirements on spatially
intelligent agents and thus VPR systems: 1) the particular agent including its
sensors and computational resources, 2) the operating environment of this
agent, and 3) the specific task that the artificial agent carries out. In this
paper, we characterize and survey key works in the VPR area considering those
drivers, including their place representation and place matching choices. We
also provide a new definition of VPR based on the visual overlap -- akin to
spatial view cells in the brain -- that enables us to find similarities and
differences to other research areas in the robotics and computer vision fields.
We identify numerous open challenges and suggest areas that require more
in-depth attention in future works.","['Sourav Garg', 'Tobias Fischer', 'Michael Milford']",2021-03-11T04:11:04Z,http://arxiv.org/abs/2103.06443v2
"Semantic Segmentation for Real Point Cloud Scenes via Bilateral
  Augmentation and Adaptive Fusion","Given the prominence of current 3D sensors, a fine-grained analysis on the
basic point cloud data is worthy of further investigation. Particularly, real
point cloud scenes can intuitively capture complex surroundings in the real
world, but due to 3D data's raw nature, it is very challenging for machine
perception. In this work, we concentrate on the essential visual task, semantic
segmentation, for large-scale point cloud data collected in reality. On the one
hand, to reduce the ambiguity in nearby points, we augment their local context
by fully utilizing both geometric and semantic features in a bilateral
structure. On the other hand, we comprehensively interpret the distinctness of
the points from multiple resolutions and represent the feature map following an
adaptive fusion method at point-level for accurate semantic segmentation.
Further, we provide specific ablation studies and intuitive visualizations to
validate our key modules. By comparing with state-of-the-art networks on three
different benchmarks, we demonstrate the effectiveness of our network.","['Shi Qiu', 'Saeed Anwar', 'Nick Barnes']",2021-03-12T04:13:20Z,http://arxiv.org/abs/2103.07074v2
"PlaneSegNet: Fast and Robust Plane Estimation Using a Single-stage
  Instance Segmentation CNN","Instance segmentation of planar regions in indoor scenes benefits visual SLAM
and other applications such as augmented reality (AR) where scene understanding
is required. Existing methods built upon two-stage frameworks show satisfactory
accuracy but are limited by low frame rates. In this work, we propose a
real-time deep neural architecture that estimates piece-wise planar regions
from a single RGB image. Our model employs a variant of a fast single-stage CNN
architecture to segment plane instances. Considering the particularity of the
target detected, we propose Fast Feature Non-maximum Suppression (FF-NMS) to
reduce the suppression errors resulted from overlapping bounding boxes of
planes. We also utilize a Residual Feature Augmentation module in the Feature
Pyramid Network (FPN). Our method achieves significantly higher frame-rates and
comparable segmentation accuracy against two-stage methods. We automatically
label over 70,000 images as ground truth from the Stanford 2D-3D-Semantics
dataset. Moreover, we incorporate our method with a state-of-the-art planar
SLAM and validate its benefits.","['Yaxu Xie', 'Jason Rambach', 'Fangwen Shu', 'Didier Stricker']",2021-03-29T08:53:05Z,http://arxiv.org/abs/2103.15428v1
"Exploratory Design of a Hands-free Video Game Controller for a
  Quadriplegic Individual","From colored pixels to hyper-realistic 3D landscapes of virtual reality,
video games have evolved immensely over the last few decades. However, video
game input still requires two-handed dexterous finger manipulations for
simultaneous joystick and trigger or mouse and keyboard presses. In this work,
we explore the design of a hands-free game control method using realtime facial
expression recognition for individuals with neurological and neuromuscular
diseases who are unable to use traditional game controllers. Similar to other
Assistive Technologies (AT), our facial input technique is also designed and
tested in collaboration with a graduate student who has Spinal Muscular
Atrophy. Our preliminary evaluation shows the potential of facial expression
recognition for augmenting the lives of quadriplegic individuals by enabling
them to accomplish things like walking, running, flying or other adventures
that may not be so attainable otherwise.","['Atieh Taheri', 'Ziv Weissman', 'Misha Sra']",2021-09-02T19:29:47Z,http://arxiv.org/abs/2109.01186v1
Distortion Reduction for Off-Center Perspective Projection of Panoramas,"A single Panorama can be drawn perspectively without distortions in arbitrary
viewing directions and field-of-views when the camera position is at the
origin. This is a key advantage in VR and virtual tour applications because it
enables the user to freely""look around"" in a virtual world with just a single
panorama, albeit at a fixed position. However, when the camera moves away from
the center, barrel distortions appear and realism breaks. We propose
modifications to the equirectangular-to-perspective(E2P) projection that
significantly reduce distortions when the camera position is away from the
origin. This enables users to not only ""look around"" but also ""walk around""
virtually in a single panorama with more convincing renderings. We compare with
other techniques that aim to augment panoramas with 3D information, including:
1) panoramas with depth information and 2) panoramas augmented with room
layouts, and show that our approach provides more visually convincing results","['Chi-Han Peng', 'Jiayao Zhang']",2021-11-23T17:29:26Z,http://arxiv.org/abs/2111.12018v1
"Boosting Generative Zero-Shot Learning by Synthesizing Diverse Features
  with Attribute Augmentation","The recent advance in deep generative models outlines a promising perspective
in the realm of Zero-Shot Learning (ZSL). Most generative ZSL methods use
category semantic attributes plus a Gaussian noise to generate visual features.
After generating unseen samples, this family of approaches effectively
transforms the ZSL problem into a supervised classification scheme. However,
the existing models use a single semantic attribute, which contains the
complete attribute information of the category. The generated data also carry
the complete attribute information, but in reality, visual samples usually have
limited attributes. Therefore, the generated data from attribute could have
incomplete semantics. Based on this fact, we propose a novel framework to boost
ZSL by synthesizing diverse features. This method uses augmented semantic
attributes to train the generative model, so as to simulate the real
distribution of visual features. We evaluate the proposed model on four
benchmark datasets, observing significant performance improvement against the
state-of-the-art.","['Xiaojie Zhao', 'Yuming Shen', 'Shidong Wang', 'Haofeng Zhang']",2021-12-23T14:32:51Z,http://arxiv.org/abs/2112.12573v1
"Distinguishing Non-natural from Natural Adversarial Samples for More
  Robust Pre-trained Language Model","Recently, the problem of robustness of pre-trained language models (PrLMs)
has received increasing research interest. Latest studies on adversarial
attacks achieve high attack success rates against PrLMs, claiming that PrLMs
are not robust. However, we find that the adversarial samples that PrLMs fail
are mostly non-natural and do not appear in reality. We question the validity
of current evaluation of robustness of PrLMs based on these non-natural
adversarial samples and propose an anomaly detector to evaluate the robustness
of PrLMs with more natural adversarial samples. We also investigate two
applications of the anomaly detector: (1) In data augmentation, we employ the
anomaly detector to force generating augmented data that are distinguished as
non-natural, which brings larger gains to the accuracy of PrLMs. (2) We apply
the anomaly detector to a defense framework to enhance the robustness of PrLMs.
It can be used to defend all types of attacks and achieves higher accuracy on
both adversarial samples and compliant samples than other defense frameworks.","['Jiayi Wang', 'Rongzhou Bao', 'Zhuosheng Zhang', 'Hai Zhao']",2022-03-19T14:06:46Z,http://arxiv.org/abs/2203.11199v1
Context-Aware Sequence Alignment using 4D Skeletal Augmentation,"Temporal alignment of fine-grained human actions in videos is important for
numerous applications in computer vision, robotics, and mixed reality.
State-of-the-art methods directly learn image-based embedding space by
leveraging powerful deep convolutional neural networks. While being
straightforward, their results are far from satisfactory, the aligned videos
exhibit severe temporal discontinuity without additional post-processing steps.
The recent advancements in human body and hand pose estimation in the wild
promise new ways of addressing the task of human action alignment in videos. In
this work, based on off-the-shelf human pose estimators, we propose a novel
context-aware self-supervised learning architecture to align sequences of
actions. We name it CASA. Specifically, CASA employs self-attention and
cross-attention mechanisms to incorporate the spatial and temporal context of
human actions, which can solve the temporal discontinuity problem. Moreover, we
introduce a self-supervised learning scheme that is empowered by novel 4D
augmentation techniques for 3D skeleton representations. We systematically
evaluate the key components of our method. Our experiments on three public
datasets demonstrate CASA significantly improves phase progress and Kendall's
Tau scores over the previous state-of-the-art methods.","['Taein Kwon', 'Bugra Tekin', 'Siyu Tang', 'Marc Pollefeys']",2022-04-26T10:59:29Z,http://arxiv.org/abs/2204.12223v1
KaliCalib: A Framework for Basketball Court Registration,"Tracking the players and the ball in team sports is key to analyse the
performance or to enhance the game watching experience with augmented reality.
When the only sources for this data are broadcast videos, sports-field
registration systems are required to estimate the homography and re-project the
ball or the players from the image space to the field space. This paper
describes a new basketball court registration framework in the context of the
MMSports 2022 camera calibration challenge. The method is based on the
estimation by an encoder-decoder network of the positions of keypoints sampled
with perspective-aware constraints. The regression of the basket positions and
heavy data augmentation techniques make the model robust to different arenas.
Ablation studies show the positive effects of our contributions on the
challenge test set. Our method divides the mean squared error by 4.7 compared
to the challenge baseline.","['Adrien Maglo', 'Astrid Orcesi', 'Quoc Cuong Pham']",2022-09-16T08:52:29Z,http://arxiv.org/abs/2209.07795v1
"Minimizing Human Assistance: Augmenting a Single Demonstration for Deep
  Reinforcement Learning","The use of human demonstrations in reinforcement learning has proven to
significantly improve agent performance. However, any requirement for a human
to manually 'teach' the model is somewhat antithetical to the goals of
reinforcement learning. This paper attempts to minimize human involvement in
the learning process while retaining the performance advantages by using a
single human example collected through a simple-to-use virtual reality
simulation to assist with RL training. Our method augments a single
demonstration to generate numerous human-like demonstrations that, when
combined with Deep Deterministic Policy Gradients and Hindsight Experience
Replay (DDPG + HER) significantly improve training time on simple tasks and
allows the agent to solve a complex task (block stacking) that DDPG + HER alone
cannot solve. The model achieves this significant training advantage using a
single human example, requiring less than a minute of human input. Moreover,
despite learning from a human example, the agent is not constrained to
human-level performance, often learning a policy that is significantly
different from the human demonstration.","['Abraham George', 'Alison Bartsch', 'Amir Barati Farimani']",2022-09-22T19:04:43Z,http://arxiv.org/abs/2209.11275v2
Gaussian-smoothed Imbalance Data Improves Speech Emotion Recognition,"In speech emotion recognition tasks, models learn emotional representations
from datasets. We find the data distribution in the IEMOCAP dataset is very
imbalanced, which may harm models to learn a better representation. To address
this issue, we propose a novel Pairwise-emotion Data Distribution Smoothing
(PDDS) method. PDDS considers that the distribution of emotional data should be
smooth in reality, then applies Gaussian smoothing to emotion-pairs for
constructing a new training set with a smoother distribution. The required new
data are complemented using the mixup augmentation. As PDDS is model and
modality agnostic, it is evaluated with three SOTA models on the IEMOCAP
dataset. The experimental results show that these models are improved by 0.2\%
- 4.8\% and 1.5\% - 5.9\% in terms of WA and UA. In addition, an ablation study
demonstrates that the key advantage of PDDS is the reasonable data distribution
rather than a simple data augmentation.","['Xuefeng Liang', 'Hexin Jiang', 'Wenxin Xu', 'Ying Zhou']",2023-02-17T01:50:46Z,http://arxiv.org/abs/2302.08650v1
"A Deep Learning Approach Towards Generating High-fidelity Diverse
  Synthetic Battery Datasets","Recent surge in the number of Electric Vehicles have created a need to
develop inexpensive energy-dense Battery Storage Systems. Many countries across
the planet have put in place concrete measures to reduce and subsequently limit
the number of vehicles powered by fossil fuels. Lithium-ion based batteries are
presently dominating the electric automotive sector. Energy research efforts
are also focussed on accurate computation of State-of-Charge of such batteries
to provide reliable vehicle range estimates. Although such estimation
algorithms provide precise estimates, all such techniques available in
literature presume availability of superior quality battery datasets. In
reality, gaining access to proprietary battery usage datasets is very tough for
battery scientists. Moreover, open access datasets lack the diverse battery
charge/discharge patterns needed to build generalized models. Curating battery
measurement data is time consuming and needs expensive equipment. To surmount
such limited data scenarios, we introduce few Deep Learning-based methods to
synthesize high-fidelity battery datasets, these augmented synthetic datasets
will help battery researchers build better estimation models in the presence of
limited data. We have released the code and dataset used in the present
approach to generate synthetic data. The battery data augmentation techniques
introduced here will alleviate limited battery dataset challenges.","['Janamejaya Channegowda', 'Vageesh Maiya', 'Chaitanya Lingaraj']",2023-04-09T05:41:21Z,http://arxiv.org/abs/2304.06043v1
Self-Supervised Learning based Depth Estimation from Monocular Images,"Depth Estimation has wide reaching applications in the field of Computer
vision such as target tracking, augmented reality, and self-driving cars. The
goal of Monocular Depth Estimation is to predict the depth map, given a 2D
monocular RGB image as input. The traditional depth estimation methods are
based on depth cues and used concepts like epipolar geometry. With the
evolution of Convolutional Neural Networks, depth estimation has undergone
tremendous strides. In this project, our aim is to explore possible extensions
to existing SoTA Deep Learning based Depth Estimation Models and to see whether
performance metrics could be further improved. In a broader sense, we are
looking at the possibility of implementing Pose Estimation, Efficient Sub-Pixel
Convolution Interpolation, Semantic Segmentation Estimation techniques to
further enhance our proposed architecture and to provide fine-grained and more
globally coherent depth map predictions. We also plan to do away with camera
intrinsic parameters during training and apply weather augmentations to further
generalize our model.","['Mayank Poddar', 'Akash Mishra', 'Mohit Kewlani', 'Haoyang Pei']",2023-04-14T07:14:08Z,http://arxiv.org/abs/2304.06966v1
"Angle Range and Identity Similarity Enhanced Gaze and Head Redirection
  based on Synthetic data","In this paper, we propose a method for improving the angular accuracy and
photo-reality of gaze and head redirection in full-face images. The problem
with current models is that they cannot handle redirection at large angles, and
this limitation mainly comes from the lack of training data. To resolve this
problem, we create data augmentation by monocular 3D face reconstruction to
extend the head pose and gaze range of the real data, which allows the model to
handle a wider redirection range. In addition to the main focus on data
augmentation, we also propose a framework with better image quality and
identity preservation of unseen subjects even training with synthetic data.
Experiments show that our method significantly improves redirection performance
in terms of redirection angular accuracy while maintaining high image quality,
especially when redirecting to large angles.","['Jiawei Qin', 'Xueting Wang']",2023-09-11T03:20:41Z,http://arxiv.org/abs/2309.05214v1
"A Cyberpunk 2077 perspective on the prediction and understanding of
  future technology","Science fiction and video games have long served as valuable tools for
envisioning and inspiring future technological advancements. This position
paper investigates the potential of Cyberpunk 2077, a popular science fiction
video game, to shed light on the future of technology, particularly in the
areas of artificial intelligence, edge computing, augmented humans, and
biotechnology. By analyzing the game's portrayal of these technologies and
their implications, we aim to understand the possibilities and challenges that
lie ahead. We discuss key themes such as neurolink and brain-computer
interfaces, multimodal recording systems, virtual and simulated reality,
digital representation of the physical world, augmented and AI-based home
appliances, smart clothing, and autonomous vehicles. The paper highlights the
importance of designing technologies that can coexist with existing preferences
and systems, considering the uneven adoption of new technologies. Through this
exploration, we emphasize the potential of science fiction and video games like
Cyberpunk 2077 as tools for guiding future technological advancements and
shaping public perception of emerging innovations.","['Miguel Bordallo López', 'Constantino Álvarez Casado']",2023-09-25T09:08:30Z,http://arxiv.org/abs/2309.13970v1
Time-Variant Overlap-Add in Partitions,"Virtual and augmented realities are increasingly popular tools in many
domains such as architecture, production, training and education,
(psycho)therapy, gaming, and others. For a convincing rendering of sound in
virtual and augmented environments, audio signals must be convolved in
real-time with impulse responses that change from one moment in time to
another. Key requirements for the implementation of such time-variant real-time
convolution algorithms are short latencies, moderate computational cost and
memory footprint, and no perceptible switching artifacts. In this engineering
report, we introduce a partitioned convolution algorithm that is able to
quickly switch between impulse responses without introducing perceptible
artifacts, while maintaining a constant computational load and low memory
usage. Implementations in several popular programming languages are freely
available via GitHub.","['Hagen Jaeger', 'Uwe Simmer', 'Jörg Bitzer', 'Matthias Blau']",2023-09-30T09:17:51Z,http://arxiv.org/abs/2310.00319v1
"Ball-AR: Fostering Playful Co-Located Interaction Through
  Environment-centric Physical Activity with AR","We present Ball-AR, an augmented reality (AR) game where two players in the
same physical space attempt to hit each other with virtual dodgeballs overlaid
on the physical world. Researchers have studied AR's potential for fostering
co-located interaction and physical activity; however, they have not
investigated the impacts of physical activity and physical environment on user
experiences and interaction. We created an AR dodgeball game centered around
encouraging physical activity and harnessing the physical environment. We then
evaluated the game with five dyads to analyze the impacts of these design
choices on the quality of gameplay and interaction between players. We found
that physical activity and the shared physical space created memorable
experiences and interactions among participants, although participants desired
a more augmented and immersive experience.","['Arnav Kumar', 'Andrés Monroy-Hernández']",2023-11-13T00:12:14Z,http://arxiv.org/abs/2311.06992v1
"BlissCam: Boosting Eye Tracking Efficiency with Learned In-Sensor Sparse
  Sampling","Eye tracking is becoming an increasingly important task domain in emerging
computing platforms such as Augmented/Virtual Reality (AR/VR). Today's eye
tracking system suffers from long end-to-end tracking latency and can easily
eat up half of the power budget of a mobile VR device. Most existing
optimization efforts exclusively focus on the computation pipeline by
optimizing the algorithm and/or designing dedicated accelerators while largely
ignoring the front-end of any eye tracking pipeline: the image sensor. This
paper makes a case for co-designing the imaging system with the computing
system. In particular, we propose the notion of ""in-sensor sparse sampling"",
whereby the pixels are drastically downsampled (by 20x) within the sensor. Such
in-sensor sampling enhances the overall tracking efficiency by significantly
reducing 1) the power consumption of the sensor readout chain and sensor-host
communication interfaces, two major power contributors, and 2) the work done on
the host, which receives and operates on far fewer pixels. With careful reuse
of existing pixel circuitry, our proposed BLISSCAM requires little hardware
augmentation to support the in-sensor operations. Our synthesis results show up
to 8.2x energy reduction and 1.4x latency reduction over existing eye tracking
pipelines.","['Yu Feng', 'Tianrui Ma', 'Yuhao Zhu', 'Xuan Zhang']",2024-04-24T08:41:35Z,http://arxiv.org/abs/2404.15733v1
"Exploring Non-Reversing Magic Mirrors for Screen-Based Augmented Reality
  Systems","Screen-based Augmented Reality (AR) systems can be built as a window into the
real world as often done in mobile AR applications or using the Magic Mirror
metaphor, where users can see themselves with augmented graphics on a large
display. Such Magic Mirror systems have been used in digital clothing
environments to create virtual dressing rooms, to teach human anatomy, and for
collaborative design tasks. The term Magic Mirror implies that the display
shows the users enantiomorph, i.e. the mirror image, such that the system
mimics a real-world physical mirror. However, the question arises whether one
should design a traditional mirror, or instead display the true mirror image by
means of a non-reversing mirror? This is an intriguing perceptual question, as
the image one observes in a mirror is not a real view, as it would be seen by
an external observer, but a reflection, i.e. a front-to-back reversed image. In
this paper, we discuss the perceptual differences between these two mirror
visualization concepts and present a first comparative study in the context of
Magic Mirror anatomy teaching. We investigate the ability of users to identify
the correct placement of virtual anatomical structures in our screen-based AR
system for two conditions: a regular mirror and a non-reversing mirror setup.
The results of our study indicate that the latter is more suitable for
applications where previously acquired domain-specific knowledge plays an
important role. The lessons learned open up new research directions in the
fields of user interfaces and interaction in non-reversing mirror environments
and could impact the implementation of general screen-based AR systems in other
domains.","['Felix Bork', 'Roghayeh Barmaki', 'Ulrich Eck', 'Pascal Fallavollita', 'Bernhard Fuerst', 'Nassir Navab']",2016-11-10T15:45:35Z,http://arxiv.org/abs/1611.03354v2
"User-driven Intelligent Interface on the Basis of Multimodal Augmented
  Reality and Brain-Computer Interaction for People with Functional
  Disabilities","The analysis of the current integration attempts of some modes and use cases
of user-machine interaction is presented. The new concept of the user-driven
intelligent interface is proposed on the basis of multimodal augmented reality
and brain-computer interaction for various applications: in disabilities
studies, education, home care, health care, etc. The several use cases of
multimodal augmentation are presented. The perspectives of the better human
comprehension by the immediate feedback through neurophysical channels by means
of brain-computer interaction are outlined. It is shown that brain-computer
interface (BCI) technology provides new strategies to overcome limits of the
currently available user interfaces, especially for people with functional
disabilities. The results of the previous studies of the low end consumer and
open-source BCI-devices allow us to conclude that combination of machine
learning (ML), multimodal interactions (visual, sound, tactile) with BCI will
profit from the immediate feedback from the actual neurophysical reactions
classified by ML methods. In general, BCI in combination with other modes of AR
interaction can deliver much more information than these types of interaction
themselves. Even in the current state the combined AR-BCI interfaces could
provide the highly adaptable and personal services, especially for people with
functional disabilities.","['S. Stirenko', 'Yu. Gordienko', 'T. Shemsedinov', 'O. Alienin', 'Yu. Kochura', 'N. Gordienko', 'A. Rojbi', 'J. R. López Benito', 'E. Artetxe González']",2017-04-12T21:03:52Z,http://arxiv.org/abs/1704.05915v2
"LookinGood: Enhancing Performance Capture with Real-time Neural
  Re-Rendering","Motivated by augmented and virtual reality applications such as telepresence,
there has been a recent focus in real-time performance capture of humans under
motion. However, given the real-time constraint, these systems often suffer
from artifacts in geometry and texture such as holes and noise in the final
rendering, poor lighting, and low-resolution textures. We take the novel
approach to augment such real-time performance capture systems with a deep
architecture that takes a rendering from an arbitrary viewpoint, and jointly
performs completion, super resolution, and denoising of the imagery in
real-time. We call this approach neural (re-)rendering, and our live system
""LookinGood"". Our deep architecture is trained to produce high resolution and
high quality images from a coarse rendering in real-time. First, we propose a
self-supervised training method that does not require manual ground-truth
annotation. We contribute a specialized reconstruction error that uses semantic
information to focus on relevant parts of the subject, e.g. the face. We also
introduce a salient reweighing scheme of the loss function that is able to
discard outliers. We specifically design the system for virtual and augmented
reality headsets where the consistency between the left and right eye plays a
crucial role in the final user experience. Finally, we generate temporally
stable results by explicitly minimizing the difference between two consecutive
frames. We tested the proposed system in two different scenarios: one involving
a single RGB-D sensor, and upper body reconstruction of an actor, the second
consisting of full body 360 degree capture. Through extensive experimentation,
we demonstrate how our system generalizes across unseen sequences and subjects.
The supplementary video is available at http://youtu.be/Md3tdAKoLGU.","['Ricardo Martin-Brualla', 'Rohit Pandey', 'Shuoran Yang', 'Pavel Pidlypenskyi', 'Jonathan Taylor', 'Julien Valentin', 'Sameh Khamis', 'Philip Davidson', 'Anastasia Tkach', 'Peter Lincoln', 'Adarsh Kowdle', 'Christoph Rhemann', 'Dan B Goldman', 'Cem Keskin', 'Steve Seitz', 'Shahram Izadi', 'Sean Fanello']",2018-11-12T22:51:19Z,http://arxiv.org/abs/1811.05029v1
"Towards Augmented Reality-based Suturing in Monocular Laparoscopic
  Training","Minimally Invasive Surgery (MIS) techniques have gained rapid popularity
among surgeons since they offer significant clinical benefits including reduced
recovery time and diminished post-operative adverse effects. However,
conventional endoscopic systems output monocular video which compromises depth
perception, spatial orientation and field of view. Suturing is one of the most
complex tasks performed under these circumstances. Key components of this tasks
are the interplay between needle holder and the surgical needle. Reliable 3D
localization of needle and instruments in real time could be used to augment
the scene with additional parameters that describe their quantitative geometric
relation, e.g. the relation between the estimated needle plane and its rotation
center and the instrument. This could contribute towards standardization and
training of basic skills and operative techniques, enhance overall surgical
performance, and reduce the risk of complications. The paper proposes an
Augmented Reality environment with quantitative and qualitative visual
representations to enhance laparoscopic training outcomes performed on a
silicone pad. This is enabled by a multi-task supervised deep neural network
which performs multi-class segmentation and depth map prediction. Scarcity of
labels has been conquered by creating a virtual environment which resembles the
surgical training scenario to generate dense depth maps and segmentation maps.
The proposed convolutional neural network was tested on real surgical training
scenarios and showed to be robust to occlusion of the needle. The network
achieves a dice score of 0.67 for surgical needle segmentation, 0.81 for needle
holder instrument segmentation and a mean absolute error of 6.5 mm for depth
estimation.","['Chandrakanth Jayachandran Preetha', 'Jonathan Kloss', 'Fabian Siegfried Wehrtmann', 'Lalith Sharan', 'Carolyn Fan', 'Beat Peter Müller-Stich', 'Felix Nickel', 'Sandy Engelhardt']",2020-01-19T19:59:58Z,http://arxiv.org/abs/2001.06894v1
"Personal Augmented Reality for Information Visualization on Large
  Interactive Displays","In this work we propose the combination of large interactive displays with
personal head-mounted Augmented Reality (AR) for information visualization to
facilitate data exploration and analysis. Even though large displays provide
more display space, they are challenging with regard to perception, effective
multi-user support, and managing data density and complexity. To address these
issues and illustrate our proposed setup, we contribute an extensive design
space comprising first, the spatial alignment of display, visualizations, and
objects in AR space. Next, we discuss which parts of a visualization can be
augmented. Finally, we analyze how AR can be used to display personal views in
order to show additional information and to minimize the mutual disturbance of
data analysts. Based on this conceptual foundation, we present a number of
exemplary techniques for extending visualizations with AR and discuss their
relation to our design space. We further describe how these techniques address
typical visualization problems that we have identified during our literature
research. To examine our concepts, we introduce a generic AR visualization
framework as well as a prototype implementing several example techniques. In
order to demonstrate their potential, we further present a use case walkthrough
in which we analyze a movie data set. From these experiences, we conclude that
the contributed techniques can be useful in exploring and understanding
multivariate data. We are convinced that the extension of large displays with
AR for information visualization has a great potential for data analysis and
sense-making.","['Patrick Reipschläger', 'Tamara Flemisch', 'Raimund Dachselt']",2020-09-07T17:08:15Z,http://arxiv.org/abs/2009.03237v3
"Augmented Reality needle ablation guidance tool for Irreversible
  Electroporation in the pancreas","Irreversible electroporation (IRE) is a soft tissue ablation technique
suitable for treatment of inoperable tumours in the pancreas. The process
involves applying a high voltage electric field to the tissue containing the
mass using needle electrodes, leaving cancerous cells irreversibly damaged and
vulnerable to apoptosis. Efficacy of the treatment depends heavily on the
accuracy of needle placement and requires a high degree of skill from the
operator. In this paper, we describe an Augmented Reality (AR) system designed
to overcome the challenges associated with planning and guiding the needle
insertion process. Our solution, based on the HoloLens (Microsoft, USA)
platform, tracks the position of the headset, needle electrodes and ultrasound
(US) probe in space. The proof of concept implementation of the system uses
this tracking data to render real-time holographic guides on the HoloLens,
giving the user insight into the current progress of needle insertion and an
indication of the target needle trajectory. The operator's field of view is
augmented using visual guides and real-time US feed rendered on a holographic
plane, eliminating the need to consult external monitors. Based on these early
prototypes, we are aiming to develop a system that will lower the skill level
required for IRE while increasing overall accuracy of needle insertion and,
hence, the likelihood of successful treatment.","['Timur Kuzhagaliyev', 'Neil T. Clancy', 'Mirek Janatka', 'Kevin Tchaka', 'Francisco Vasconcelos', 'Matthew J. Clarkson', 'Kurinchi Gurusamy', 'David J. Hawkes', 'Brian Davidson', 'Danail Stoyanov']",2018-02-09T14:25:56Z,http://arxiv.org/abs/1802.03274v1
Feedback and Control of Dynamics and Robotics using Augmented Reality,"Human-machine interaction (HMI) and human-robot interaction (HRI) can assist
structural monitoring and structural dynamics testing in the laboratory and
field. In vibratory experimentation, one mode of generating vibration is to use
electrodynamic exciters. Manual control is a common way of setting the input of
the exciter by the operator. To measure the structural responses to these
generated vibrations sensors are attached to the structure. These sensors can
be deployed by repeatable robots with high endurance, which require on-the-fly
control. If the interface between operators and the controls was augmented,
then operators can visualize the experiments, exciter levels, and define robot
input with a better awareness of the area of interest. Robots can provide
better aid to humans if intelligent on-the-fly control of the robot is: (1)
quantified and presented to the human; (2) conducted in real-time for human
feedback informed by data. Information provided by the new interface would be
used to change the control input based on their understanding of real-time
parameters. This research proposes using Augmented Reality (AR) applications to
provide humans with sensor feedback and control of actuators and robots. This
method improves cognition by allowing the operator to maintain awareness of
structures while adjusting conditions accordingly with the assistance of the
new real-time interface. One interface application is developed to plot sensor
data in addition to voltage, frequency, and duration controls for vibration
generation. Two more applications are developed under similar framework, one to
control the position of a mediating robot and one to control the frequency of
the robot movement. This paper presents the proposed model for the new control
loop and then compares the new approach with a traditional method by measuring
time delay in control input and user efficiency.","['Elijah Wyckoff', 'Ronan Reza', 'Fernando Moreu']",2023-03-23T03:43:36Z,http://arxiv.org/abs/2303.13016v1
"Impact of geolocation data on augmented reality usability: A comparative
  user test","Abstract. While the use of location-based augmented reality (AR) for
education has demonstrated benefits on participants' motivation, engagement,
and on their physical activity, geolocation data inaccuracy causes augmented
objects to jitter or drift, which is a factor in downgrading user experience.
We developed a free and open source web AR application and conducted a
comparative user test (n = 54) in order to assess the impact of geolocation
data on usability, exploration, and focus. A control group explored
biodiversity in nature using the system in combination with embedded GNSS data,
and an experimental group used an external module for RTK data. During the
test, eye tracking data, geolocated traces, and in-app user-triggered events
were recorded. Participants answered usability questionnaires (SUS, UEQ,
HARUS).We found that the geolocation data the RTK group was exposed to was less
accurate in average than that of the control group. The RTK group reported
lower usability scores on all scales, of which 5 out of 9 were significant,
indicating that inaccurate data negatively predicts usability. The GNSS group
walked more than the RTK group, indicating a partial effect on exploration. We
found no significant effect on interaction time with the screen, indicating no
specific relation between data accuracy and focus. While RTK data did not allow
us to better the usability of location-based AR interfaces, results allow us to
assess our system's overall usability as excellent, and to define optimal
operating conditions for future use with pupils.","['Julien Mercier', 'N. Chabloz', 'G. Dozot', 'C. Audrin', 'O. Ertz', 'E. Bocher', 'D. Rappo']",2023-08-21T11:25:06Z,http://arxiv.org/abs/2308.13544v1
Pivot calibration concept for sensor attached mobile c-arms,"Medical augmented reality has been actively studied for decades and many
methods have been proposed torevolutionize clinical procedures. One example is
the camera augmented mobile C-arm (CAMC), which providesa real-time video
augmentation onto medical images by rigidly mounting and calibrating a camera
to the imagingdevice. Since then, several CAMC variations have been suggested
by calibrating 2D/3D cameras, trackers, andmore recently a Microsoft HoloLens
to the C-arm. Different calibration methods have been applied to establishthe
correspondence between the rigidly attached sensor and the imaging device. A
crucial step for these methodsis the acquisition of X-Ray images or 3D
reconstruction volumes; therefore, requiring the emission of ionizingradiation.
In this work, we analyze the mechanical motion of the device and propose an
alternatative methodto calibrate sensors to the C-arm without emitting any
radiation. Given a sensor is rigidly attached to thedevice, we introduce an
extended pivot calibration concept to compute the fixed translation from the
sensor tothe C-arm rotation center. The fixed relationship between the sensor
and rotation center can be formulated as apivot calibration problem with the
pivot point moving on a locus. Our method exploits the rigid C-arm
motiondescribing a Torus surface to solve this calibration problem. We explain
the geometry of the C-arm motion andits relation to the attached sensor,
propose a calibration algorithm and show its robustness against noise, as
wellas trajectory and observed pose density by computer simulations. We discuss
this geometric-based formulationand its potential extensions to different C-arm
applications.","['Sing Chun Lee', 'Matthias Seibold', 'Philipp Fürnstahl', 'Mazda Farshad', 'Nassir Navab']",2020-01-09T15:57:14Z,http://arxiv.org/abs/2001.03075v1
"Augmenting Part-of-speech Tagging with Syntactic Information for
  Vietnamese and Chinese","Word segmentation and part-of-speech tagging are two critical preliminary
steps for downstream tasks in Vietnamese natural language processing. In
reality, people tend to consider also the phrase boundary when performing word
segmentation and part of speech tagging rather than solely process word by word
from left to right. In this paper, we implement this idea to improve word
segmentation and part of speech tagging the Vietnamese language by employing a
simplified constituency parser. Our neural model for joint word segmentation
and part-of-speech tagging has the architecture of the syllable-based CRF
constituency parser. To reduce the complexity of parsing, we replace all
constituent labels with a single label indicating for phrases. This model can
be augmented with predicted word boundary and part-of-speech tags by other
tools. Because Vietnamese and Chinese have some similar linguistic phenomena,
we evaluated the proposed model and its augmented versions on three Vietnamese
benchmark datasets and six Chinese benchmark datasets. Our experimental results
show that the proposed model achieves higher performances than previous works
for both languages.","['Duc-Vu Nguyen', 'Kiet Van Nguyen', 'Ngan Luu-Thuy Nguyen']",2021-02-24T08:57:02Z,http://arxiv.org/abs/2102.12136v2
"Dual Quaternion Ambisonics Array for Six-Degree-of-Freedom Acoustic
  Representation","Spatial audio methods are gaining a growing interest due to the spread of
immersive audio experiences and applications, such as virtual and augmented
reality. For these purposes, 3D audio signals are often acquired through arrays
of Ambisonics microphones, each comprising four capsules that decompose the
sound field in spherical harmonics. In this paper, we propose a dual quaternion
representation of the spatial sound field acquired through an array of two
First Order Ambisonics (FOA) microphones. The audio signals are encapsulated in
a dual quaternion that leverages quaternion algebra properties to exploit
correlations among them. This augmented representation with 6 degrees of
freedom (6DOF) involves a more accurate coverage of the sound field, resulting
in a more precise sound localization and a more immersive audio experience. We
evaluate our approach on a sound event localization and detection (SELD)
benchmark. We show that our dual quaternion SELD model with temporal
convolution blocks (DualQSELD-TCN) achieves better results with respect to real
and quaternion-valued baselines thanks to our augmented representation of the
sound field. Full code is available at:
https://github.com/ispamm/DualQSELD-TCN.","['Eleonora Grassucci', 'Gioia Mancini', 'Christian Brignone', 'Aurelio Uncini', 'Danilo Comminiello']",2022-04-04T21:11:00Z,http://arxiv.org/abs/2204.01851v2
"A Lightweight Domain Adaptive Absolute Pose Regressor Using Barlow Twins
  Objective","Identifying the camera pose for a given image is a challenging problem with
applications in robotics, autonomous vehicles, and augmented/virtual reality.
Lately, learning-based methods have shown to be effective for absolute camera
pose estimation. However, these methods are not accurate when generalizing to
different domains. In this paper, a domain adaptive training framework for
absolute pose regression is introduced. In the proposed framework, the scene
image is augmented for different domains by using generative methods to train
parallel branches using Barlow Twins objective. The parallel branches leverage
a lightweight CNN-based absolute pose regressor architecture. Further, the
efficacy of incorporating spatial and channel-wise attention in the regression
head for rotation prediction is investigated. Our method is evaluated with two
datasets, Cambridge landmarks and 7Scenes. The results demonstrate that, even
with using roughly 24 times fewer FLOPs, 12 times fewer activations, and 5
times fewer parameters than MS-Transformer, our approach outperforms all the
CNN-based architectures and achieves performance comparable to
transformer-based architectures. Our method ranks 2nd and 4th with the
Cambridge Landmarks and 7Scenes datasets, respectively. In addition, for
augmented domains not encountered during training, our approach significantly
outperforms the MS-transformer. Furthermore, it is shown that our domain
adaptive framework achieves better performance than the single branch model
trained with the identical CNN backbone with all instances of the unseen
distribution.","['Praveen Kumar Rajendran', 'Quoc-Vinh Lai-Dang', 'Luiz Felipe Vecchietti', 'Dongsoo Har']",2022-11-20T12:18:53Z,http://arxiv.org/abs/2211.10963v1
NaQ: Leveraging Narrations as Queries to Supervise Episodic Memory,"Searching long egocentric videos with natural language queries (NLQ) has
compelling applications in augmented reality and robotics, where a fluid index
into everything that a person (agent) has seen before could augment human
memory and surface relevant information on demand. However, the structured
nature of the learning problem (free-form text query inputs, localized video
temporal window outputs) and its needle-in-a-haystack nature makes it both
technically challenging and expensive to supervise. We introduce
Narrations-as-Queries (NaQ), a data augmentation strategy that transforms
standard video-text narrations into training data for a video query
localization model. Validating our idea on the Ego4D benchmark, we find it has
tremendous impact in practice. NaQ improves multiple top models by substantial
margins (even doubling their accuracy), and yields the very best results to
date on the Ego4D NLQ challenge, soundly outperforming all challenge winners in
the CVPR and ECCV 2022 competitions and topping the current public leaderboard.
Beyond achieving the state-of-the-art for NLQ, we also demonstrate unique
properties of our approach such as the ability to perform zero-shot and
few-shot NLQ, and improved performance on queries about long-tail object
categories. Code and models:
{\small\url{http://vision.cs.utexas.edu/projects/naq}}.","['Santhosh Kumar Ramakrishnan', 'Ziad Al-Halah', 'Kristen Grauman']",2023-01-02T16:40:15Z,http://arxiv.org/abs/2301.00746v2
"Learning to Assist and Communicate with Novice Drone Pilots for Expert
  Level Performance","Multi-task missions for unmanned aerial vehicles (UAVs) involving inspection
and landing tasks are challenging for novice pilots due to the difficulties
associated with depth perception and the control interface. We propose a shared
autonomy system, alongside supplementary information displays, to assist pilots
to successfully complete multi-task missions without any pilot training. Our
approach comprises of three modules: (1) a perception module that encodes
visual information onto a latent representation, (2) a policy module that
augments pilot's actions, and (3) an information augmentation module that
provides additional information to the pilot. The policy module is trained in
simulation with simulated users and transferred to the real world without
modification in a user study (n=29), alongside supplementary information
schemes including learnt red/green light feedback cues and an augmented reality
display. The pilot's intent is unknown to the policy module and is inferred
from the pilot's input and UAV's states. The assistant increased task success
rate for the landing and inspection tasks from [16.67% & 54.29%] respectively
to [95.59% & 96.22%]. With the assistant, inexperienced pilots achieved similar
performance to experienced pilots. Red/green light feedback cues reduced the
required time by 19.53% and trajectory length by 17.86% for the inspection
task, where participants rated it as their preferred condition due to the
intuitive interface and providing reassurance. This work demonstrates that
simple user models can train shared autonomy systems in simulation, and
transfer to physical tasks to estimate user intent and provide effective
assistance and information to the pilot.","['Kal Backman', 'Dana Kulić', 'Hoam Chung']",2023-06-16T02:59:20Z,http://arxiv.org/abs/2306.09600v1
"Connecting Everyday Objects with the Metaverse: A Unified Recognition
  Framework","The recent Facebook rebranding to Meta has drawn renewed attention to the
metaverse. Technology giants, amongst others, are increasingly embracing the
vision and opportunities of a hybrid social experience that mixes physical and
virtual interactions. As the metaverse gains in traction, it is expected that
everyday objects may soon connect more closely with virtual elements. However,
discovering this ""hidden"" virtual world will be a crucial first step to
interacting with it in this new augmented world. In this paper, we address the
problem of connecting physical objects with their virtual counterparts,
especially through connections built upon visual markers. We propose a unified
recognition framework that guides approaches to the metaverse access points. We
illustrate the use of the framework through experimental studies under
different conditions, in which an interactive and visually attractive
decoration pattern, an Artcode, is used as the approach to enable the
connection. This paper will be of interest to, amongst others, researchers
working in Interaction Design or Augmented Reality who are seeking techniques
or guidelines for augmenting physical objects in an unobtrusive, complementary
manner.","['Liming Xu', 'Dave Towey', 'Andrew P. French', 'Steve Benford']",2023-09-11T21:20:06Z,http://arxiv.org/abs/2309.06444v1
Improving Fairness using Vision-Language Driven Image Augmentation,"Fairness is crucial when training a deep-learning discriminative model,
especially in the facial domain. Models tend to correlate specific
characteristics (such as age and skin color) with unrelated attributes
(downstream tasks), resulting in biases which do not correspond to reality. It
is common knowledge that these correlations are present in the data and are
then transferred to the models during training. This paper proposes a method to
mitigate these correlations to improve fairness. To do so, we learn
interpretable and meaningful paths lying in the semantic space of a pre-trained
diffusion model (DiffAE) -- such paths being supervised by contrastive text
dipoles. That is, we learn to edit protected characteristics (age and skin
color). These paths are then applied to augment images to improve the fairness
of a given dataset. We test the proposed method on CelebA-HQ and UTKFace on
several downstream tasks with age and skin color as protected characteristics.
As a proxy for fairness, we compute the difference in accuracy with respect to
the protected characteristics. Quantitative results show how the augmented
images help the model improve the overall accuracy, the aforementioned metric,
and the disparity of equal opportunity. Code is available at:
https://github.com/Moreno98/Vision-Language-Bias-Control.","[""Moreno D'Incà"", 'Christos Tzelepis', 'Ioannis Patras', 'Nicu Sebe']",2023-11-02T19:51:10Z,http://arxiv.org/abs/2311.01573v1
"Augmented Reality based Simulated Data (ARSim) with multi-view
  consistency for AV perception networks","Detecting a diverse range of objects under various driving scenarios is
essential for the effectiveness of autonomous driving systems. However, the
real-world data collected often lacks the necessary diversity presenting a
long-tail distribution. Although synthetic data has been utilized to overcome
this issue by generating virtual scenes, it faces hurdles such as a significant
domain gap and the substantial efforts required from 3D artists to create
realistic environments. To overcome these challenges, we present ARSim, a fully
automated, comprehensive, modular framework designed to enhance real multi-view
image data with 3D synthetic objects of interest. The proposed method
integrates domain adaptation and randomization strategies to address covariate
shift between real and simulated data by inferring essential domain attributes
from real data and employing simulation-based randomization for other
attributes. We construct a simplified virtual scene using real data and
strategically place 3D synthetic assets within it. Illumination is achieved by
estimating light distribution from multiple images capturing the surroundings
of the vehicle. Camera parameters from real data are employed to render
synthetic assets in each frame. The resulting augmented multi-view consistent
dataset is used to train a multi-camera perception network for autonomous
vehicles. Experimental results on various AV perception tasks demonstrate the
superior performance of networks trained on the augmented dataset.","['Aqeel Anwar', 'Tae Eun Choe', 'Zian Wang', 'Sanja Fidler', 'Minwoo Park']",2024-03-22T17:49:11Z,http://arxiv.org/abs/2403.15370v1
Augmenting Textual Generation via Topology Aware Retrieval,"Despite the impressive advancements of Large Language Models (LLMs) in
generating text, they are often limited by the knowledge contained in the input
and prone to producing inaccurate or hallucinated content. To tackle these
issues, Retrieval-augmented Generation (RAG) is employed as an effective
strategy to enhance the available knowledge base and anchor the responses in
reality by pulling additional texts from external databases. In real-world
applications, texts are often linked through entities within a graph, such as
citations in academic papers or comments in social networks. This paper
exploits these topological relationships to guide the retrieval process in RAG.
Specifically, we explore two kinds of topological connections: proximity-based,
focusing on closely connected nodes, and role-based, which looks at nodes
sharing similar subgraph structures. Our empirical research confirms their
relevance to text relationships, leading us to develop a Topology-aware
Retrieval-augmented Generation framework. This framework includes a retrieval
module that selects texts based on their topological relationships and an
aggregation module that integrates these texts into prompts to stimulate LLMs
for text generation. We have curated established text-attributed networks and
conducted comprehensive experiments to validate the effectiveness of this
framework, demonstrating its potential to enhance RAG with topological
awareness.","['Yu Wang', 'Nedim Lipka', 'Ruiyi Zhang', 'Alexa Siu', 'Yuying Zhao', 'Bo Ni', 'Xin Wang', 'Ryan Rossi', 'Tyler Derr']",2024-05-27T19:02:18Z,http://arxiv.org/abs/2405.17602v1
"Leaving Reality to Imagination: Robust Classification via Generated
  Datasets","Recent research on robustness has revealed significant performance gaps
between neural image classifiers trained on datasets that are similar to the
test set, and those that are from a naturally shifted distribution, such as
sketches, paintings, and animations of the object categories observed during
training. Prior work focuses on reducing this gap by designing engineered
augmentations of training data or through unsupervised pretraining of a single
large model on massive in-the-wild training datasets scraped from the Internet.
However, the notion of a dataset is also undergoing a paradigm shift in recent
years. With drastic improvements in the quality, ease-of-use, and access to
modern generative models, generated data is pervading the web. In this light,
we study the question: How do these generated datasets influence the natural
robustness of image classifiers? We find that Imagenet classifiers trained on
real data augmented with generated data achieve higher accuracy and effective
robustness than standard training and popular augmentation strategies in the
presence of natural distribution shifts. We analyze various factors influencing
these results, including the choice of conditioning strategies and the amount
of generated data. Additionally, we find that the standard ImageNet classifiers
suffer a performance degradation of upto 20\% on the generated data, indicating
their fragility at accurately classifying the objects under novel variations.
Lastly, we demonstrate that the image classifiers, which have been trained on
real data augmented with generated data from the base generative model, exhibit
greater resilience to natural distribution shifts compared to the classifiers
trained on real data augmented with generated data from the finetuned
generative model on the real data. The code, models, and datasets are available
at https://github.com/Hritikbansal/generative-robustness.","['Hritik Bansal', 'Aditya Grover']",2023-02-05T22:49:33Z,http://arxiv.org/abs/2302.02503v2
Articulated Hand Pose Estimation Review,"With the increase number of companies focusing on commercializing Augmented
Reality (AR), Virtual Reality (VR) and wearable devices, the need for a hand
based input mechanism is becoming essential in order to make the experience
natural, seamless and immersive. Hand pose estimation has progressed
drastically in recent years due to the introduction of commodity depth cameras.
  Hand pose estimation based on vision is still a challenging problem due to
its complexity from self-occlusion (between fingers), close similarity between
fingers, dexterity of the hands, speed of the pose and the high dimension of
the hand kinematic parameters. Articulated hand pose estimation is still an
open problem and under intensive research from both academia and industry.
  The 2 approaches used for hand pose estimation are: discriminative and
generative. Generative approach is a model based that tries to fit a hand model
to the observed data. Discriminative approach is appearance based, usually
implemented with machine learning (ML) and require a large amount of training
data. Recent hand pose estimation uses hybrid approach by combining both
discriminative and generative methods into a single hand pipeline.
  In this paper, we focus on reviewing recent progress of hand pose estimation
from depth sensor. We will survey discriminative methods, generative methods
and hybrid methods. This paper is not a comprehensive review of all hand pose
estimation techniques, it is a subset of some of the recent state-of-the-art
techniques.",['Emad Barsoum'],2016-04-21T06:55:42Z,http://arxiv.org/abs/1604.06195v1
Real-time Hand Tracking under Occlusion from an Egocentric RGB-D Sensor,"We present an approach for real-time, robust and accurate hand pose
estimation from moving egocentric RGB-D cameras in cluttered real environments.
Existing methods typically fail for hand-object interactions in cluttered
scenes imaged from egocentric viewpoints, common for virtual or augmented
reality applications. Our approach uses two subsequently applied Convolutional
Neural Networks (CNNs) to localize the hand and regress 3D joint locations.
Hand localization is achieved by using a CNN to estimate the 2D position of the
hand center in the input, even in the presence of clutter and occlusions. The
localized hand position, together with the corresponding input depth value, is
used to generate a normalized cropped image that is fed into a second CNN to
regress relative 3D hand joint locations in real time. For added accuracy,
robustness and temporal stability, we refine the pose estimates using a
kinematic pose tracking energy. To train the CNNs, we introduce a new
photorealistic dataset that uses a merged reality approach to capture and
synthesize large amounts of annotated data of natural hand interaction in
cluttered scenes. Through quantitative and qualitative evaluation, we show that
our method is robust to self-occlusion and occlusions by objects, particularly
in moving egocentric perspectives.","['Franziska Mueller', 'Dushyant Mehta', 'Oleksandr Sotnychenko', 'Srinath Sridhar', 'Dan Casas', 'Christian Theobalt']",2017-04-07T12:23:03Z,http://arxiv.org/abs/1704.02201v2
A Lightweight Approach for On-the-Fly Reflectance Estimation,"Estimating surface reflectance (BRDF) is one key component for complete 3D
scene capture, with wide applications in virtual reality, augmented reality,
and human computer interaction. Prior work is either limited to controlled
environments (\eg gonioreflectometers, light stages, or multi-camera domes), or
requires the joint optimization of shape, illumination, and reflectance, which
is often computationally too expensive (\eg hours of running time) for
real-time applications. Moreover, most prior work requires HDR images as input
which further complicates the capture process. In this paper, we propose a
lightweight approach for surface reflectance estimation directly from $8$-bit
RGB images in real-time, which can be easily plugged into any 3D
scanning-and-fusion system with a commodity RGBD sensor. Our method is
learning-based, with an inference time of less than 90ms per scene and a model
size of less than 340K bytes. We propose two novel network architectures,
HemiCNN and Grouplet, to deal with the unstructured input data from multiple
viewpoints under unknown illumination. We further design a loss function to
resolve the color-constancy and scale ambiguity. In addition, we have created a
large synthetic dataset, SynBRDF, which comprises a total of $500$K RGBD images
rendered with a physically-based ray tracer under a variety of natural
illumination, covering $5000$ materials and $5000$ shapes. SynBRDF is the first
large-scale benchmark dataset for reflectance estimation. Experiments on both
synthetic data and real data show that the proposed method effectively recovers
surface reflectance, and outperforms prior work for reflectance estimation in
uncontrolled environments.","['Kihwan Kim', 'Jinwei Gu', 'Stephen Tyree', 'Pavlo Molchanov', 'Matthias Nießner', 'Jan Kautz']",2017-05-19T19:45:57Z,http://arxiv.org/abs/1705.07162v2
"Applying advanced machine learning models to classify
  electro-physiological activity of human brain for use in biometric
  identification","In this article we present the results of our research related to the study
of correlations between specific visual stimulation and the elicited brain's
electro-physiological response collected by EEG sensors from a group of
participants. We will look at how the various characteristics of visual
stimulation affect the measured electro-physiological response of the brain and
describe the optimal parameters found that elicit a steady-state visually
evoked potential (SSVEP) in certain parts of the cerebral cortex where it can
be reliably perceived by the electrode of the EEG device. After that, we
continue with a description of the advanced machine learning pipeline model
that can perform confident classification of the collected EEG data in order to
(a) reliably distinguish signal from noise (about 85% validation score) and (b)
reliably distinguish between EEG records collected from different human
participants (about 80% validation score). Finally, we demonstrate that the
proposed method works reliably even with an inexpensive (less than $100)
consumer-grade EEG sensing device and with participants who do not have
previous experience with EEG technology (EEG illiterate). All this in
combination opens up broad prospects for the development of new types of
consumer devices, [e.g.] based on virtual reality helmets or augmented reality
glasses where EEG sensor can be easily integrated. The proposed method can be
used to improve an online user experience by providing [e.g.] password-less
user identification for VR / AR applications. It can also find a more advanced
application in intensive care units where collected EEG data can be used to
classify the level of conscious awareness of patients during anesthesia or to
automatically detect hardware failures by classifying the input signal as
noise.",['Iaroslav Omelianenko'],2017-08-03T14:50:02Z,http://arxiv.org/abs/1708.01167v1
"Generating Classes of 3D Virtual Mandibles for AR-Based Medical
  Simulation","Simulation and modeling represent promising tools for several application
domains from engineering to forensic science and medicine. Advances in 3D
imaging technology convey paradigms such as augmented reality (AR) and mixed
reality inside promising simulation tools for the training industry. Motivated
by the requirement for superimposing anatomically correct 3D models on a Human
Patient Simulator (HPS) and visualizing them in an AR environment, the purpose
of this research effort is to derive method for scaling a source human mandible
to a target human mandible. Results show that, given a distance between two
same landmarks on two different mandibles, a relative scaling factor may be
computed. Using this scaling factor, results show that a 3D virtual mandible
model can be made morphometrically equivalent to a real target-specific
mandible within a 1.30 millimeter average error bound. The virtual mandible may
be further used as a reference target for registering other anatomical models,
such as the lungs, on the HPS. Such registration will be made possible by
physical constraints among the mandible and the spinal column in the horizontal
normal rest position.","['Neha R. Hippalgaonkar', 'Alexa D. Sider', 'Felix G. Hamza-Lup', 'Anand P. Santhanam', 'Bala Jaganathan', 'Celina Imielinska', 'Jannick P. Rolland']",2018-11-20T03:29:56Z,http://arxiv.org/abs/1811.08053v1
"Human Intention Estimation based on Hidden Markov Model Motion
  Validation for Safe Flexible Robotized Warehouses","With the substantial growth of logistics businesses the need for larger
warehouses and their automation arises, thus using robots as assistants to
human workers is becoming a priority. In order to operate efficiently and
safely, robot assistants or the supervising system should recognize human
intentions in real-time. Theory of mind (ToM) is an intuitive human conception
of other humans' mental state, i.e., beliefs and desires, and how they cause
behavior. In this paper we propose a ToM based human intention estimation
algorithm for flexible robotized warehouses. We observe human's, i.e., worker's
motion and validate it with respect to the goal locations using generalized
Voronoi diagram based path planning. These observations are then processed by
the proposed hidden Markov model framework which estimates worker intentions in
an online manner, capable of handling changing environments. To test the
proposed intention estimation we ran experiments in a real-world laboratory
warehouse with a worker wearing Microsoft Hololens augmented reality glasses.
Furthermore, in order to demonstrate the scalability of the approach to larger
warehouses, we propose to use virtual reality digital warehouse twins in order
to realistically simulate worker behavior. We conducted intention estimation
experiments in the larger warehouse digital twin with up to 24 running robots.
We demonstrate that the proposed framework estimates warehouse worker
intentions precisely and in the end we discuss the experimental results.","['Tomislav Petković', 'David Puljiz', 'Ivan Marković', 'Björn Hein']",2018-11-20T14:32:31Z,http://arxiv.org/abs/1811.08269v1
Multi-operator Network Sharing for Massive IoT,"Recent study predicts that by 2020 up to 50 billion IoT devices will be
connected to the Internet, straining the capacity of wireless network that has
already been overloaded with data-hungry mobile applications, such as
high-definition video streaming and virtual reality(VR)/augmented reality(AR).
How to accommodate the demand for both massive scale of IoT devices and
high-speed cellular services in the physically limited spectrum without
significantly increasing the operational and infrastructure costs is one of the
main challenges for operators. In this article, we introduce a new
multi-operator network sharing framework that supports the coexistence of IoT
and high-speed cellular services. Our framework is based on the radio access
network (RAN) sharing architecture recently introduced by 3GPP as a promising
solution for operators to improve their resource utilization and reduce the
system roll-out cost. We evaluate the performance of our proposed framework
using the real base station location data in the city of Dublin collected from
two major operators in Ireland. Numerical results show that our proposed
framework can almost double the total number of IoT devices that can be
supported and coexist with other cellular services compared with the case
without network sharing.","['Yong Xiao', 'Marwan Krunz', 'Tao Shu']",2020-01-25T08:16:28Z,http://arxiv.org/abs/2001.09276v1
A general approach to bridge the reality-gap,"Employing machine learning models in the real world requires collecting large
amounts of data, which is both time consuming and costly to collect. A common
approach to circumvent this is to leverage existing, similar data-sets with
large amounts of labelled data. However, models trained on these canonical
distributions do not readily transfer to real-world ones. Domain adaptation and
transfer learning are often used to breach this ""reality gap"", though both
require a substantial amount of real-world data. In this paper we discuss a
more general approach: we propose learning a general transformation to bring
arbitrary images towards a canonical distribution where we can naively apply
the trained machine learning models. This transformation is trained in an
unsupervised regime, leveraging data augmentation to generate off-canonical
examples of images and training a Deep Learning model to recover their original
counterpart. We quantify the performance of this transformation using
pre-trained ImageNet classifiers, demonstrating that this procedure can recover
half of the loss in performance on the distorted data-set. We then validate the
effectiveness of this approach on a series of pre-trained ImageNet models on a
real world data set collected by printing and photographing images in different
lighting conditions.","['Michael Lomnitz', 'Zigfried Hampel-Arias', 'Nina Lopatina', 'Felipe A. Mejia']",2020-09-03T18:19:28Z,http://arxiv.org/abs/2009.01865v1
"Improving the Usability of Virtual Reality Neuron Tracing with
  Topological Elements","Researchers in the field of connectomics are working to reconstruct a map of
neural connections in the brain in order to understand at a fundamental level
how the brain processes information. Constructing this wiring diagram is done
by tracing neurons through high-resolution image stacks acquired with
fluorescence microscopy imaging techniques. While a large number of automatic
tracing algorithms have been proposed, these frequently rely on local features
in the data and fail on noisy data or ambiguous cases, requiring time-consuming
manual correction. As a result, manual and semi-automatic tracing methods
remain the state-of-the-art for creating accurate neuron reconstructions. We
propose a new semi-automatic method that uses topological features to guide
users in tracing neurons and integrate this method within a virtual reality
(VR) framework previously used for manual tracing. Our approach augments both
visualization and interaction with topological elements, allowing rapid
understanding and tracing of complex morphologies. In our pilot study,
neuroscientists demonstrated a strong preference for using our tool over prior
approaches, reported less fatigue during tracing, and commended the ability to
better understand possible paths and alternatives. Quantitative evaluation of
the traces reveals that users' tracing speed increased, while retaining similar
accuracy compared to a fully manual approach.","['Torin McDonald', 'Will Usher', 'Nate Morrical', 'Attila Gyulassy', 'Steve Petruzza', 'Frederick Federer', 'Alessandra Angelucci', 'Valerio Pascucci']",2020-09-03T19:20:50Z,http://arxiv.org/abs/2009.01891v1
"Approaches, Challenges, and Applications for Deep Visual Odometry:
  Toward to Complicated and Emerging Areas","Visual odometry (VO) is a prevalent way to deal with the relative
localization problem, which is becoming increasingly mature and accurate, but
it tends to be fragile under challenging environments. Comparing with classical
geometry-based methods, deep learning-based methods can automatically learn
effective and robust representations, such as depth, optical flow, feature,
ego-motion, etc., from data without explicit computation. Nevertheless, there
still lacks a thorough review of the recent advances of deep learning-based VO
(Deep VO). Therefore, this paper aims to gain a deep insight on how deep
learning can profit and optimize the VO systems. We first screen out a number
of qualifications including accuracy, efficiency, scalability, dynamicity,
practicability, and extensibility, and employ them as the criteria. Then, using
the offered criteria as the uniform measurements, we detailedly evaluate and
discuss how deep learning improves the performance of VO from the aspects of
depth estimation, feature extraction and matching, pose estimation. We also
summarize the complicated and emerging areas of Deep VO, such as mobile robots,
medical robots, augmented reality and virtual reality, etc. Through the
literature decomposition, analysis, and comparison, we finally put forward a
number of open issues and raise some future research directions in this field.","['Ke Wang', 'Sai Ma', 'Junlan Chen', 'Fan Ren']",2020-09-06T08:25:23Z,http://arxiv.org/abs/2009.02672v1
Deep Analog-to-Digital Converter for Wireless Communication,"With the advent of the 5G wireless networks, achieving tens of gigabits per
second throughputs and low, milliseconds, latency has become a reality. This
level of performance will fuel numerous real-time applications, such as
autonomy and augmented reality, where the computationally heavy tasks can be
performed in the cloud. The increase in the bandwidth along with the use of
dense constellations places a significant burden on the speed and accuracy of
analog-to-digital converters (ADC). A popular approach to create wideband ADCs
is utilizing multiple channels each operating at a lower speed in the
time-interleaved fashion. However, an interleaved ADC comes with its own set of
challenges. The parallel architecture is very sensitive to the inter-channel
mismatch, timing jitter, clock skew between different ADC channels as well as
the nonlinearity within individual channels. Consequently, complex
post-calibration is required using digital signal processing (DSP) after the
ADC. The traditional DSP calibration consumes a significant amount of power and
its design requires knowledge of the source and type of errors which are
becoming increasingly difficult to predict in nanometer CMOS processes. In this
paper, instead of individually targeting each source of error, we utilize a
deep learning algorithm to learn the complete and complex ADC behavior and to
compensate for it in realtime. We demonstrate this ""Deep ADC"" technique on an
8G Sample/s 8-channel time-interleaved ADC with the QAM-OFDM modulated data.
Simulation results for different QAM symbol constellations and OFDM subcarriers
show dramatic improvements of approximately 5 bits in the dynamic range with a
concomitant drastic reduction in symbol error rate. We further discuss the
hardware implementation including latency, power consumption, memory
requirements, and chip area.","['Ashkan Samiee', 'Yiming Zhou', 'Tingyi Zhou', 'Bahram Jalali']",2020-09-11T17:36:13Z,http://arxiv.org/abs/2009.05553v1
"Leveraging Local and Global Descriptors in Parallel to Search
  Correspondences for Visual Localization","Visual localization to compute 6DoF camera pose from a given image has wide
applications such as in robotics, virtual reality, augmented reality, etc. Two
kinds of descriptors are important for the visual localization. One is global
descriptors that extract the whole feature from each image. The other is local
descriptors that extract the local feature from each image patch usually
enclosing a key point. More and more methods of the visual localization have
two stages: at first to perform image retrieval by global descriptors and then
from the retrieval feedback to make 2D-3D point correspondences by local
descriptors. The two stages are in serial for most of the methods. This simple
combination has not achieved superiority of fusing local and global
descriptors. The 3D points obtained from the retrieval feedback are as the
nearest neighbor candidates of the 2D image points only by global descriptors.
Each of the 2D image points is also called a query local feature when
performing the 2D-3D point correspondences. In this paper, we propose a novel
parallel search framework, which leverages advantages of both local and global
descriptors to get nearest neighbor candidates of a query local feature.
Specifically, besides using deep learning based global descriptors, we also
utilize local descriptors to construct random tree structures for obtaining
nearest neighbor candidates of the query local feature. We propose a new
probabilistic model and a new deep learning based local descriptor when
constructing the random trees. A weighted Hamming regularization term to keep
discriminativeness after binarization is given in the loss function for the
proposed local descriptor. The loss function co-trains both real and binary
descriptors of which the results are integrated into the random trees.","['Pengju Zhang', 'Yihong Wu', 'Bingxi Liu']",2020-09-23T01:49:03Z,http://arxiv.org/abs/2009.10891v1
"Controlling wheelchairs by body motions: A learning framework for the
  adaptive remapping of space","Learning to operate a vehicle is generally accomplished by forming a new
cognitive map between the body motions and extrapersonal space. Here, we
consider the challenge of remapping movement-to-space representations in
survivors of spinal cord injury, for the control of powered wheelchairs. Our
goal is to facilitate this remapping by developing interfaces between residual
body motions and navigational commands that exploit the degrees of freedom that
disabled individuals are most capable to coordinate. We present a new framework
for allowing spinal cord injured persons to control powered wheelchairs through
signals derived from their residual mobility. The main novelty of this approach
lies in substituting the more common joystick controllers of powered
wheelchairs with a sensor shirt. This allows the whole upper body of the user
to operate as an adaptive joystick. Considerations about learning and risks
have lead us to develop a safe testing environment in 3D Virtual Reality. A
Personal Augmented Reality Immersive System (PARIS) allows us to analyse
learning skills and provide users with an adequate training to control a
simulated wheelchair through the signals generated by body motions in a safe
environment. We provide a description of the basic theory, of the development
phases and of the operation of the complete system. We also present preliminary
results illustrating the processing of the data and supporting of the
feasibility of this approach.","['Tauseef Gulrez', 'Alessandro Tognetti', 'Alon Fishbach', 'Santiago Acosta', 'Christopher Scharver', 'Danilo De Rossi', 'Ferdinando A. Mussa-Ivaldi']",2011-07-27T05:30:40Z,http://arxiv.org/abs/1107.5387v1
Multi-level Chaotic Maps for 3D Textured Model Encryption,"With rapid progress of Virtual Reality and Augmented Reality technologies, 3D
contents are the next widespread media in many applications. Thus, the
protection of 3D models is primarily important. Encryption of 3D models is
essential to maintain confidentiality. Previous work on encryption of 3D
surface model often consider the point clouds, the meshes and the textures
individually. In this work, a multi-level chaotic maps models for 3D textured
encryption was presented by observing the different contributions for
recognizing cipher 3D models between vertices (point cloud), polygons and
textures. For vertices which make main contribution for recognizing, we use
high level 3D Lu chaotic map to encrypt them. For polygons and textures which
make relatively smaller contributions for recognizing, we use 2D Arnold's cat
map and 1D Logistic map to encrypt them, respectively. The experimental results
show that our method can get similar performance with the other method use the
same high level chaotic map for point cloud, polygons and textures, while we
use less time. Besides, our method can resist more method of attacks such as
statistic attack, brute-force attack, correlation attack.","['Xin Jin', 'Shuyun Zhu', 'Le Wu', 'Geng Zhao', 'Xiaodong Li', 'Quan Zhou', 'Huimin Lu']",2017-09-25T08:20:17Z,http://arxiv.org/abs/1709.08364v2
Holoscopic 3D Micro-Gesture Database for Wearable Device Interaction,"With the rapid development of augmented reality (AR) and virtual reality (VR)
technology, human-computer interaction (HCI) has been greatly improved for
gaming interaction of AR and VR control. The finger micro-gesture is one of the
important interactive methods for HCI applications such as in the Google Soli
and Microsoft Kinect projects. However, the progress in this research is slow
due to the lack of high quality public available database. In this paper,
holoscopic 3D camera is used to capture high quality micro-gesture images and a
new unique holoscopic 3D micro-gesture (HoMG) database is produced. The
principle of the holoscopic 3D camera is based on the fly viewing system to see
the objects. HoMG database recorded the image sequence of 3 conventional
gestures from 40 participants under different settings and conditions. For the
purpose of micro-gesture recognition, HoMG has a video subset with 960 videos
and a still image subset with 30635 images. Initial micro-gesture recognition
on both subsets has been conducted using traditional 2D image and video
features and popular classifiers and some encouraging performance has been
achieved. The database will be available for the research communities and speed
up the research in this area.","['Yi Liu', 'Hongying Meng', 'Mohammad Rafiq Swash', 'Yona Falinie A. Gaus', 'Rui Qin']",2017-12-15T07:49:04Z,http://arxiv.org/abs/1712.05570v2
"Energy-Efficient Mobile-Edge Computation Offloading for Applications
  with Shared Data","Mobile-edge computation offloading (MECO) has been recognized as a promising
solution to alleviate the burden of resource-limited Internet of Thing (IoT)
devices by offloading computation tasks to the edge of cellular networks (also
known as {\em cloudlet}). Specifically, latency-critical applications such as
virtual reality (VR) and augmented reality (AR) have inherent collaborative
properties since part of the input/output data are shared by different users in
proximity. In this paper, we consider a multi-user fog computing system, in
which multiple single-antenna mobile users running applications featuring
shared data can choose between (partially) offloading their individual tasks to
a nearby single-antenna cloudlet for remote execution and performing pure local
computation. The mobile users' energy minimization is formulated as a convex
problem, subject to the total computing latency constraint, the total energy
constraints for individual data downloading, and the computing frequency
constraints for local computing, for which classical Lagrangian duality can be
applied to find the optimal solution. Based upon the semi-closed form solution,
the shared data proves to be transmitted by only one of the mobile users
instead of multiple ones. Besides, compared to those baseline algorithms
without considering the shared data property or the mobile users' local
computing capabilities, the proposed joint computation offloading and
communications resource allocation provides significant energy saving.","['Xiangyu He', 'Hong Xing', 'Yue Chen', 'Arumugam Nallanathan']",2018-09-04T13:52:13Z,http://arxiv.org/abs/1809.00966v1
scenery: Flexible Virtual Reality Visualization on the Java VM,"Life science today involves computational analysis of a large amount and
variety of data, such as volumetric data acquired by state-of-the-art
microscopes, or mesh data from analysis of such data or simulations.
Visualization is often the first step in making sense of data, and a crucial
part of building and debugging analysis pipelines. It is therefore important
that visualizations can be quickly prototyped, as well as developed or embedded
into full applications. In order to better judge spatiotemporal relationships,
immersive hardware, such as Virtual or Augmented Reality (VR/AR) headsets and
associated controllers are becoming invaluable tools. In this work we introduce
scenery, a flexible VR/AR visualization framework for the Java VM that can
handle mesh and large volumetric data, containing multiple views, timepoints,
and color channels. scenery is free and open-source software, works on all
major platforms, and uses the Vulkan or OpenGL rendering APIs. We introduce
scenery's main features and example applications, such as its use in VR for
microscopy, in the biomedical image analysis software Fiji, or for visualizing
agent-based simulations.","['Ulrik Günther', 'Tobias Pietzsch', 'Aryaman Gupta', 'Kyle I. S. Harrington', 'Pavel Tomancak', 'Stefan Gumhold', 'Ivo F. Sbalzarini']",2019-06-16T17:01:20Z,http://arxiv.org/abs/1906.06726v3
FMHash: Deep Hashing of In-Air-Handwriting for User Identification,"Many mobile systems and wearable devices, such as Virtual Reality (VR) or
Augmented Reality (AR) headsets, lack a keyboard or touchscreen to type an ID
and password for signing into a virtual website. However, they are usually
equipped with gesture capture interfaces to allow the user to interact with the
system directly with hand gestures. Although gesture-based authentication has
been well-studied, less attention is paid to the gesture-based user
identification problem, which is essentially an input method of account ID and
an efficient searching and indexing method of a database of gesture signals. In
this paper, we propose FMHash (i.e., Finger Motion Hash), a user identification
framework that can generate a compact binary hash code from a piece of
in-air-handwriting of an ID string. This hash code enables indexing and fast
search of a large account database using the in-air-handwriting by a hash
table. To demonstrate the effectiveness of the framework, we implemented a
prototype and achieved >99.5% precision and >92.6% recall with exact hash code
match on a dataset of 200 accounts collected by us. The ability of hashing
in-air-handwriting pattern to binary code can be used to achieve convenient
sign-in and sign-up with in-air-handwriting gesture ID on future mobile and
wearable systems connected to the Internet.","['Duo Lu', 'Dijiang Huang', 'Anshul Rai']",2018-06-10T02:15:29Z,http://arxiv.org/abs/1806.03574v2
"Edge Cloud Offloading Algorithms: Issues, Methods, and Perspectives","Mobile devices supporting the ""Internet of Things"" (IoT), often have limited
capabilities in computation, battery energy, and storage space, especially to
support resource-intensive applications involving virtual reality (VR),
augmented reality (AR), multimedia delivery and artificial intelligence (AI),
which could require broad bandwidth, low response latency and large
computational power. Edge cloud or edge computing is an emerging topic and
technology that can tackle the deficiency of the currently centralized-only
cloud computing model and move the computation and storage resource closer to
the devices in support of the above-mentioned applications. To make this
happen, efficient coordination mechanisms and ""offloading"" algorithms are
needed to allow the mobile devices and the edge cloud to work together
smoothly. In this survey paper, we investigate the key issues, methods, and
various state-of-the-art efforts related to the offloading problem. We adopt a
new characterizing model to study the whole process of offloading from mobile
devices to the edge cloud. Through comprehensive discussions, we aim to draw an
overall ""big picture"" on the existing efforts and research directions. Our
study also indicates that the offloading algorithms in edge cloud have
demonstrated profound potentials for future technology and application
development.","['Jianyu Wang', 'Jianli Pan', 'Flavio Esposito', 'Prasad Calyam', 'Zhicheng Yang', 'Prasant Mohapatra']",2018-06-16T05:50:46Z,http://arxiv.org/abs/1806.06191v1
"FlexNGIA: A Flexible Internet Architecture for the Next-Generation
  Tactile Internet","From virtual reality and telepresence, to augmented reality, holoportation,
and remotely controlled robotics, these future network applications promise an
unprecedented development for society, economics and culture by revolutionizing
the way we live, learn, work and play. In order to deploy such futuristic
applications and to cater to their performance requirements, recent trends
stressed the need for the Tactile Internet, an Internet that, according to the
International Telecommunication Union, combines ultra low latency with
extremely high availability, reliability and security. Unfortunately, today's
Internet falls short when it comes to providing such stringent requirements due
to several fundamental limitations in the design of the current network
architecture and communication protocols. This brings the need to rethink the
network architecture and protocols, and efficiently harness recent
technological advances in terms of virtualization and network softwarization to
design the Tactile Internet of the future.
  In this paper, we start by analyzing the characteristics and requirements of
future networking applications. We then highlight the limitations of the
traditional network architecture and protocols and their inability to cater to
these requirements. Afterward, we put forward a novel network architecture
adapted to the Tactile Internet called FlexNGIA, a Flexible Next-Generation
Internet Architecture. We then describe some use-cases where we discuss the
potential mechanisms and control loops that could be offered by FlexNGIA in
order to ensure the required performance and reliability guarantees for future
applications. Finally, we identify the key research challenges to further
develop FlexNGIA towards a full-fledged architecture for the future Tactile
Internet.","['Mohamed Faten Zhani', 'Hesham ElBakoury']",2019-05-17T07:24:51Z,http://arxiv.org/abs/1905.07137v2
"Adaptive Generation of Phantom Limbs Using Visible Hierarchical
  Autoencoders","This paper proposed a hierarchical visible autoencoder in the adaptive
phantom limbs generation according to the kinetic behavior of functional
body-parts, which are measured by heterogeneous kinetic sensors. The proposed
visible hierarchical autoencoder consists of interpretable and multi-correlated
autoencoder pipelines, which is directly derived from the hierarchical network
described in forest data-structure. According to specified kinetic script
(e.g., dancing, running, etc.) and users' physical conditions, hierarchical
network is extracted from human musculoskeletal network, which is fabricated by
multiple body components (e.g., muscle, bone, and joints, etc.) that are
bio-mechanically, functionally, or nervously correlated with each other and
exhibit mostly non-divergent kinetic behaviors. Multi-layer perceptron (MLP)
regressor models, as well as several variations of autoencoder models, are
investigated for the sequential generation of missing or dysfunctional limbs.
The resulting kinematic behavior of phantom limbs will be constructed using
virtual reality and augmented reality (VR/AR), actuators, and potentially
controller for a prosthesis (an artificial device that replaces a missing body
part). The addressed work aims to develop practical innovative exercise methods
that (1) engage individuals at all ages, including those with a chronic health
condition(s) and/or disability, in regular physical activities, (2) accelerate
the rehabilitation of patients, and (3) release users' phantom limb pain. The
physiological and psychological impact of the addressed work will critically be
assessed in future work.","['Dakila Ledesma', 'Yu Liang', 'Dalei Wu']",2019-10-02T19:54:19Z,http://arxiv.org/abs/1910.01191v1
"Adaptive Task Partitioning at Local Device or Remote Edge Server for
  Offloading in MEC","Mobile edge computing (MEC) is one of the promising solutions to process
computational-intensive tasks for the emerging time-critical Internet-of-Things
(IoT) use cases, e.g., virtual reality (VR), augmented reality (AR), autonomous
vehicle. The latency can be reduced further, when a task is partitioned and
computed by multiple edge servers' (ESs) collaboration. However, the
state-of-the-art work studies the MEC-enabled offloading based on a static
framework, which partitions tasks at either the local user equipment (UE) or
the primary ES. The dynamic selection between the two offloading schemes has
not been well studied yet. In this paper, we investigate a dynamic offloading
framework in a multi-user scenario. Each UE can decide who partitions a task
according to the network status, e.g., channel quality and allocated
computation resource. Based on the framework, we model the latency to complete
a task, and formulate an optimization problem to minimize the average latency
among UEs. The problem is solved by jointly optimizing task partitioning and
the allocation of the communication and computation resources. The numerical
results show that, compared with the static offloading schemes, the proposed
algorithm achieves the lower latency in all tested scenarios. Moreover, both
mathematical derivation and simulation illustrate that the wireless channel
quality difference between a UE and different ESs can be used as an important
criterion to determine the right scheme.","['Jianhui Liu', 'Qi Zhang']",2020-02-12T09:13:07Z,http://arxiv.org/abs/2002.04858v1
6G Communication: Envisioning the Key Issues and Challenges,"In 2030, we are going to evidence the 6G mobile communication technology,
which will enable the Internet of Everything. Yet 5G has to be experienced by
people worldwide and B5G has to be developed; the researchers have already
started planning, visioning, and gathering requirements of the 6G. Moreover,
many countries have already initiated the research on 6G. 6G promises
connecting every smart device to the Internet from smartphone to intelligent
vehicles. 6G will provide sophisticated and high QoS such as holographic
communication, augmented reality/virtual reality and many more. Also, it will
focus on Quality of Experience (QoE) to provide rich experiences from 6G
technology. Notably, it is very important to vision the issues and challenges
of 6G technology, otherwise, promises may not be delivered on time. The
requirements of 6G poses new challenges to the research community. To achieve
desired parameters of 6G, researchers are exploring various alternatives.
Hence, there are diverse research challenges to envision, from devices to
softwarization. Therefore, in this article, we discuss the future issues and
challenges to be faced by the 6G technology. We have discussed issues and
challenges from every aspect from hardware to the enabling technologies which
will be utilized by 6G.","['Sabuzima Nayak', 'Ripon Patgiri']",2020-04-07T13:49:20Z,http://arxiv.org/abs/2004.04024v3
Imposing Regulation on Advanced Algorithms,"This book discusses the necessity and perhaps urgency for the regulation of
algorithms on which new technologies rely; technologies that have the potential
to re-shape human societies. From commerce and farming to medical care and
education, it is difficult to find any aspect of our lives that will not be
affected by these emerging technologies. At the same time, artificial
intelligence, deep learning, machine learning, cognitive computing, blockchain,
virtual reality and augmented reality, belong to the fields most likely to
affect law and, in particular, administrative law. The book examines
universally applicable patterns in administrative decisions and judicial
rulings. First, similarities and divergence in behavior among the different
cases are identified by analyzing parameters ranging from geographical location
and administrative decisions to judicial reasoning and legal basis. As it turns
out, in several of the cases presented, sources of general law, such as
competition or labor law, are invoked as a legal basis, due to the lack of
current specialized legislation. This book also investigates the role and
significance of national and indeed supranational regulatory bodies for
advanced algorithms and considers ENISA, an EU agency that focuses on network
and information security, as an interesting candidate for a European regulator
of advanced algorithms. Lastly, it discusses the involvement of representative
institutions in algorithmic regulation.",['Fotios Fitsilis'],2020-05-16T20:26:54Z,http://arxiv.org/abs/2005.08092v1
IEEE 802.11be-Wi-Fi 7: New Challenges and Opportunities,"With the emergence of 4k/8k video, the throughput requirement of video
delivery will keep grow to tens of Gbps. Other new high-throughput and
low-latency video applications including augmented reality (AR), virtual
reality (VR), and online gaming, are also proliferating. Due to the related
stringent requirements, supporting these applications over wireless local area
network (WLAN) is far beyond the capabilities of the new WLAN standard -- IEEE
802.11ax. To meet these emerging demands, the IEEE 802.11 will release a new
amendment standard IEEE 802.11be -- Extremely High Throughput (EHT), also known
as Wireless-Fidelity (Wi-Fi) 7. This article provides the comprehensive survey
on the key medium access control (MAC) layer techniques and physical layer
(PHY) techniques being discussed in the EHT task group, including the
channelization and tone plan, multiple resource units (multi-RU) support, 4096
quadrature amplitude modulation (4096-QAM), preamble designs, multiple link
operations (e.g., multi-link aggregation and channel access), multiple input
multiple output (MIMO) enhancement, multiple access point (multi-AP)
coordination (e.g., multi-AP joint transmission), enhanced link adaptation and
retransmission protocols (e.g., hybrid automatic repeat request (HARQ)). This
survey covers both the critical technologies being discussed in EHT standard
and the related latest progresses from worldwide research. Besides, the
potential developments beyond EHT are discussed to provide some possible future
research directions for WLAN.","['Cailian Deng', 'Xuming Fang', 'Xiao Han', 'Xianbin Wang', 'Li Yan', 'Rong He', 'Yan Long', 'Yuchen Guo']",2020-07-27T09:40:28Z,http://arxiv.org/abs/2007.13401v3
Attention-based 3D Object Reconstruction from a Single Image,"Recently, learning-based approaches for 3D reconstruction from 2D images have
gained popularity due to its modern applications, e.g., 3D printers, autonomous
robots, self-driving cars, virtual reality, and augmented reality. The computer
vision community has applied a great effort in developing functions to
reconstruct the full 3D geometry of objects and scenes. However, to extract
image features, they rely on convolutional neural networks, which are
ineffective in capturing long-range dependencies. In this paper, we propose to
substantially improve Occupancy Networks, a state-of-the-art method for 3D
object reconstruction. For such we apply the concept of self-attention within
the network's encoder in order to leverage complementary input features rather
than those based on local regions, helping the encoder to extract global
information. With our approach, we were capable of improving the original work
in 5.05% of mesh IoU, 0.83% of Normal Consistency, and more than 10X the
Chamfer-L1 distance. We also perform a qualitative study that shows that our
approach was able to generate much more consistent meshes, confirming its
increased generalization power over the current state-of-the-art.","['Andrey Salvi', 'Nathan Gavenski', 'Eduardo Pooch', 'Felipe Tasoniero', 'Rodrigo Barros']",2020-08-11T14:51:18Z,http://arxiv.org/abs/2008.04738v1
Interface Design for HCI Classroom: From Learners' Perspective,"Having a good Human-Computer Interaction (HCI) design is challenging.
Previous works have contributed significantly to fostering HCI, including
design principle with report study from the instructor view. The questions of
how and to what extent students perceive the design principles are still left
open. To answer this question, this paper conducts a study of HCI adoption in
the classroom. The studio-based learning method was adapted to teach 83
graduate and undergraduate students in 16 weeks long with four activities. A
standalone presentation tool for instant online peer feedback during the
presentation session was developed to help students justify and critique
other's work. Our tool provides a sandbox, which supports multiple application
types, including Web-applications, Object Detection, Web-based Virtual Reality
(VR), and Augmented Reality (AR). After presenting one assignment and two
projects, our results showed that students acquired a better understanding of
the Golden Rules principle over time, which was demonstrated by the development
of visual interface design. The Wordcloud reveals the primary focus was on the
user interface and shed some light on students' interest in user experience.
The inter-rater score indicates the agreement among students that they have the
same level of understanding of the principles. The results show a high level of
guideline compliance with HCI principles, in which we witnessed variations in
visual cognitive styles. Regardless of diversity in visual preference, the
students presented high consistency and a similar perspective on adopting HCI
design principles. The results also elicited suggestions into the development
of the HCI curriculum in the future.","['Huyen N. Nguyen', 'Vinh T. Nguyen', 'Tommy Dang']",2020-10-04T18:49:24Z,http://arxiv.org/abs/2010.01651v1
SHREC 2020 track: 6D Object Pose Estimation,"6D pose estimation is crucial for augmented reality, virtual reality, robotic
manipulation and visual navigation. However, the problem is challenging due to
the variety of objects in the real world. They have varying 3D shape and their
appearances in captured images are affected by sensor noise, changing lighting
conditions and occlusions between objects. Different pose estimation methods
have different strengths and weaknesses, depending on feature representations
and scene contents. At the same time, existing 3D datasets that are used for
data-driven methods to estimate 6D poses have limited view angles and low
resolution. To address these issues, we organize the Shape Retrieval Challenge
benchmark on 6D pose estimation and create a physically accurate simulator that
is able to generate photo-realistic color-and-depth image pairs with
corresponding ground truth 6D poses. From captured color and depth images, we
use this simulator to generate a 3D dataset which has 400 photo-realistic
synthesized color-and-depth image pairs with various view angles for training,
and another 100 captured and synthetic images for testing. Five research groups
register in this track and two of them submitted their results. Data-driven
methods are the current trend in 6D object pose estimation and our evaluation
results show that approaches which fully exploit the color and geometric
features are more robust for 6D pose estimation of reflective and texture-less
objects and occlusion. This benchmark and comparative evaluation results have
the potential to further enrich and boost the research of 6D object pose
estimation and its applications.","['Honglin Yuan', 'Remco C. Veltkamp', 'Georgios Albanis', 'Nikolaos Zioulis', 'Dimitrios Zarpalas', 'Petros Daras']",2020-10-19T09:45:42Z,http://arxiv.org/abs/2010.09355v1
Deep Learning-Based Human Pose Estimation: A Survey,"Human pose estimation aims to locate the human body parts and build human
body representation (e.g., body skeleton) from input data such as images and
videos. It has drawn increasing attention during the past decade and has been
utilized in a wide range of applications including human-computer interaction,
motion analysis, augmented reality, and virtual reality. Although the recently
developed deep learning-based solutions have achieved high performance in human
pose estimation, there still remain challenges due to insufficient training
data, depth ambiguities, and occlusion. The goal of this survey paper is to
provide a comprehensive review of recent deep learning-based solutions for both
2D and 3D pose estimation via a systematic analysis and comparison of these
solutions based on their input data and inference procedures. More than 250
research papers since 2014 are covered in this survey. Furthermore, 2D and 3D
human pose estimation datasets and evaluation metrics are included.
Quantitative performance comparisons of the reviewed methods on popular
datasets are summarized and discussed. Finally, the challenges involved,
applications, and future research directions are concluded. A regularly updated
project page is provided: \url{https://github.com/zczcwh/DL-HPE}","['Ce Zheng', 'Wenhan Wu', 'Chen Chen', 'Taojiannan Yang', 'Sijie Zhu', 'Ju Shen', 'Nasser Kehtarnavaz', 'Mubarak Shah']",2020-12-24T18:49:06Z,http://arxiv.org/abs/2012.13392v5
"TEyeD: Over 20 million real-world eye images with Pupil, Eyelid, and
  Iris 2D and 3D Segmentations, 2D and 3D Landmarks, 3D Eyeball, Gaze Vector,
  and Eye Movement Types","We present TEyeD, the world's largest unified public data set of eye images
taken with head-mounted devices. TEyeD was acquired with seven different
head-mounted eye trackers. Among them, two eye trackers were integrated into
virtual reality (VR) or augmented reality (AR) devices. The images in TEyeD
were obtained from various tasks, including car rides, simulator rides, outdoor
sports activities, and daily indoor activities. The data set includes 2D and 3D
landmarks, semantic segmentation, 3D eyeball annotation and the gaze vector and
eye movement types for all images. Landmarks and semantic segmentation are
provided for the pupil, iris and eyelids. Video lengths vary from a few minutes
to several hours. With more than 20 million carefully annotated images, TEyeD
provides a unique, coherent resource and a valuable foundation for advancing
research in the field of computer vision, eye tracking and gaze estimation in
modern VR and AR applications.
  Download: Just connect via FTP as user TEyeDUser and without password to
nephrit.cs.uni-tuebingen.de (ftp://nephrit.cs.uni-tuebingen.de).","['Wolfgang Fuhl', 'Gjergji Kasneci', 'Enkelejda Kasneci']",2021-02-03T15:48:22Z,http://arxiv.org/abs/2102.02115v3
"Defining Preferred and Natural Robot Motions in Immersive Telepresence
  from a First-Person Perspective","This paper presents some early work and future plans regarding how the
autonomous motions of a telepresence robot affect a person embodied in the
robot through a head-mounted display. We consider the preferences, comfort, and
the perceived naturalness of aspects of piecewise linear paths compared to the
same aspects on a smooth path. In a user study, thirty-six subjects (eighteen
females) watched panoramic videos of three different paths through a simulated
museum in virtual reality and responded to questionnaires regarding each path.
We found that comfort had a strong effect on path preference, and that the
subjective feeling of naturalness also had a strong effect on path preference,
even though people consider different things as natural. We describe a
categorization of the responses regarding the naturalness of the robot's motion
and provide a recommendation on how this can be applied more broadly. Although
immersive robotic telepresence is increasingly being used for remote education,
clinical care, and to assist people with disabilities or mobility
complications, the full potential of this technology is limited by issues
related to user experience. Our work addresses these shortcomings and will
enable the future personalization of telepresence experiences for the
improvement of overall remote communication and the enhancement of the feeling
of presence in a remote location.","['Katherine J. Mimnaugh', 'Markku Suomalainen', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-02-25T07:40:23Z,http://arxiv.org/abs/2102.12719v1
"Evaluating Metrics for Standardized Benchmarking of Remote Presence
  Systems","To reduce the need for business-related air travel and its associated energy
consumption and carbon footprint, the U.S. Department of Energy's ARPA-E is
supporting a research project called SCOTTIE - Systematic Communication
Objectives and Telecommunications Technology Investigations and Evaluations.
SCOTTIE tests virtual and augmented reality platforms in a functional
comparison with face-to-face (FtF) interactions to derive travel replacement
thresholds for common industrial training scenarios. The primary goal of Study
1 is to match the communication effectiveness and learning outcomes obtained
from a FtF control using virtual reality (VR) training scenarios in which a
local expert with physical equipment trains a remote apprentice without
physical equipment immediately present. This application scenario is
commonplace in industrial settings where access to expensive equipment and
materials is limited and a number of apprentices must travel to a central
location in order to undergo training. Supplying an empirically validated
virtual training alternative constitutes a readily adoptable use-case for
businesses looking to reduce time and monetary expenditures associated with
travel. The technology used for three different virtual presence technologies
was strategically selected for feasibility, relatively low cost, business
relevance, and potential for impact through transition. The authors suggest
that the results of this study might generalize to the challenge of virtual
conferences.","['Charles Peasley', 'Rachel Dianiska', 'Emily Oldham', 'Nicholas Wilson', 'Stephen Gilbert', 'Peggy Wu', 'Brett Israelsen', 'James Oliver']",2021-05-04T21:36:53Z,http://arxiv.org/abs/2105.01772v1
"Augmenting Teleportation in Virtual Reality With Discrete Rotation
  Angles","Locomotion is one of the most essential interaction tasks in virtual reality
(VR) with teleportation being widely accepted as the state-of-the-art
locomotion technique at the time of this writing. A major draw-back of
teleportation is the accompanying physical rotation that is necessary to adjust
the users' orientation either before or after teleportation. This is a limiting
factor for tethered head-mounted displays (HMDs) and static body postures and
can induce additional simulator sickness for HMDs with three degrees-of-freedom
(DOF) due to missing parallax cues. To avoid physical rotation, previous work
proposed discrete rotation at fixed intervals (InPlace) as a controller-based
technique with low simulator sickness, yet the impact of varying intervals on
spatial disorientation, user presence and performance remains to be explored.
An unevaluated technique found in commercial VR games is reorientation during
the teleportation process (TeleTurn), which prevents physical rotation but
potentially increases interaction time due to its continuous orientation
selection. In an exploratory user study, where participants were free to apply
both techniques, we evaluated the impact of rotation parameters of either
technique on user performance and preference. Our results indicate that
discrete InPlace rotation introduced no significant spatial disorientation,
while user presence scores were increased. Discrete TeleTurn and teleportation
without rotation was ranked higher and achieved a higher presence score than
continuous TeleTurn, which is the current state-of-the-art found in VR games.
Based on observations, that participants avoided TeleTurn rotation when
discrete InPlace rotation was available, we distilled guidelines for designing
teleportation without physical rotation.","['Dennis Wolf', 'Michael Rietzler', 'Laura Bottner', 'Enrico Rukzio']",2021-06-08T11:30:26Z,http://arxiv.org/abs/2106.04257v1
Object Wake-up: 3D Object Rigging from a Single Image,"Given a single image of a general object such as a chair, could we also
restore its articulated 3D shape similar to human modeling, so as to animate
its plausible articulations and diverse motions? This is an interesting new
question that may have numerous downstream augmented reality and virtual
reality applications. Comparing with previous efforts on object manipulation,
our work goes beyond 2D manipulation and rigid deformation, and involves
articulated manipulation. To achieve this goal, we propose an automated
approach to build such 3D generic objects from single images and embed
articulated skeletons in them. Specifically, our framework starts by
reconstructing the 3D object from an input image. Afterwards, to extract
skeletons for generic 3D objects, we develop a novel skeleton prediction method
with a multi-head structure for skeleton probability field estimation by
utilizing the deep implicit functions. A dataset of generic 3D objects with
ground-truth annotated skeletons is collected. Empirically our approach is
demonstrated with satisfactory performance on public datasets as well as our
in-house dataset; our results surpass those of the state-of-the-arts by a
noticeable margin on both 3D reconstruction and skeleton prediction.","['Ji Yang', 'Xinxin Zuo', 'Sen Wang', 'Zhenbo Yu', 'Xingyu Li', 'Bingbing Ni', 'Minglun Gong', 'Li Cheng']",2021-08-05T16:20:12Z,http://arxiv.org/abs/2108.02708v3
Automatic Gaze Analysis: A Survey of Deep Learning based Approaches,"Eye gaze analysis is an important research problem in the field of Computer
Vision and Human-Computer Interaction. Even with notable progress in the last
10 years, automatic gaze analysis still remains challenging due to the
uniqueness of eye appearance, eye-head interplay, occlusion, image quality, and
illumination conditions. There are several open questions, including what are
the important cues to interpret gaze direction in an unconstrained environment
without prior knowledge and how to encode them in real-time. We review the
progress across a range of gaze analysis tasks and applications to elucidate
these fundamental questions, identify effective methods in gaze analysis, and
provide possible future directions. We analyze recent gaze estimation and
segmentation methods, especially in the unsupervised and weakly supervised
domain, based on their advantages and reported evaluation metrics. Our analysis
shows that the development of a robust and generic gaze analysis method still
needs to address real-world challenges such as unconstrained setup and learning
with less supervision. We conclude by discussing future research directions for
designing a real-world gaze analysis system that can propagate to other domains
including Computer Vision, Augmented Reality (AR), Virtual Reality (VR), and
Human Computer Interaction (HCI). Project Page:
https://github.com/i-am-shreya/EyeGazeSurvey}{https://github.com/i-am-shreya/EyeGazeSurvey","['Shreya Ghosh', 'Abhinav Dhall', 'Munawar Hayat', 'Jarrod Knibbe', 'Qiang Ji']",2021-08-12T00:30:39Z,http://arxiv.org/abs/2108.05479v3
"Skeleton-Graph: Long-Term 3D Motion Prediction From 2D Observations
  Using Deep Spatio-Temporal Graph CNNs","Several applications such as autonomous driving, augmented reality and
virtual reality require a precise prediction of the 3D human pose. Recently, a
new problem was introduced in the field to predict the 3D human poses from
observed 2D poses. We propose Skeleton-Graph, a deep spatio-temporal graph CNN
model that predicts the future 3D skeleton poses in a single pass from the 2D
ones. Unlike prior works, Skeleton-Graph focuses on modeling the interaction
between the skeleton joints by exploiting their spatial configuration. This is
being achieved by formulating the problem as a graph structure while learning a
suitable graph adjacency kernel. By the design, Skeleton-Graph predicts the
future 3D poses without divergence in the long-term, unlike prior works. We
also introduce a new metric that measures the divergence of predictions in the
long term. Our results show an FDE improvement of at least 27% and an ADE of 4%
on both the GTA-IM and PROX datasets respectively in comparison with prior
works. Also, we are 88% and 93% less divergence on the long-term motion
prediction in comparison with prior works on both GTA-IM and PROX datasets.
Code is available at https://github.com/abduallahmohamed/Skeleton-Graph.git","['Abduallah Mohamed', 'Huancheng Chen', 'Zhangyang Wang', 'Christian Claudel']",2021-09-21T15:33:40Z,http://arxiv.org/abs/2109.10257v2
"Mixed Reality using Illumination-aware Gradient Mixing in Surgical
  Telepresence: Enhanced Multi-layer Visualization","Background and aim: Surgical telepresence using augmented perception has been
applied, but mixed reality is still being researched and is only theoretical.
The aim of this work is to propose a solution to improve the visualization in
the final merged video by producing globally consistent videos when the
intensity of illumination in the input source and target video varies.
Methodology: The proposed system uses an enhanced multi-layer visualization
with illumination-aware gradient mixing using Illumination Aware Video
Composition algorithm. Particle Swarm Optimization Algorithm is used to find
the best sample pair from foreground and background region and image pixel
correlation to estimate the alpha matte. Particle Swarm Optimization algorithm
helps to get the original colour and depth of the unknown pixel in the unknown
region. Result: Our results showed improved accuracy caused by reducing the
Mean squared Error for selecting the best sample pair for unknown region in 10
each sample for bowel, jaw and breast. The amount of this reduction is 16.48%
from the state of art system. As a result, the visibility accuracy is improved
from 89.4 to 97.7% which helped to clear the hand vision even in the difference
of light. Conclusion: Illumination effect and alpha pixel correlation improves
the visualization accuracy and produces a globally consistent composition
results and maintains the temporal coherency when compositing two videos with
high and inverse illumination effect. In addition, this paper provides a
solution for selecting the best sampling pair for the unknown region to obtain
the original colour and depth.","['Nirakar Puri', 'Abeer Alsadoon', 'P. W. C. Prasad', 'Nada Alsalami', 'Tarik A. Rashid']",2021-08-21T11:59:24Z,http://arxiv.org/abs/2110.09318v1
A QoE Model in Point Cloud Video Streaming,"Point cloud video has been widely used by augmented reality (AR) and virtual
reality (VR) applications as it allows users to have an immersive experience of
six degrees of freedom (6DoFs). Yet there is still a lack of research on
quality of experience (QoE) model of point cloud video streaming, which cannot
provide optimization metric for streaming systems. Besides, position and color
information contained in each pixel of point cloud video, and viewport distance
effect caused by 6DoFs viewing procedure make the traditional objective quality
evaluation metric cannot be directly used in point cloud video streaming
system. In this paper we first analyze the subjective and objective factors
related to QoE model. Then an experimental system to simulate point cloud video
streaming is setup and detailed subjective quality evaluation experiments are
carried out. Based on collected mean opinion score (MOS) data, we propose a QoE
model for point cloud video streaming. We also verify the model by actual
subjective scoring, and the results show that the proposed QoE model can
accurately reflect users' visual perception. We also make the experimental
database public to promote the QoE research of point cloud video streaming.","['Jie Li', 'Xiao Wang', 'Zhi Liu', 'Qiyue Li']",2021-11-04T16:29:43Z,http://arxiv.org/abs/2111.02985v4
"Assessing Human Interaction in Virtual Reality With Continually Learning
  Prediction Agents Based on Reinforcement Learning Algorithms: A Pilot Study","Artificial intelligence systems increasingly involve continual learning to
enable flexibility in general situations that are not encountered during system
training. Human interaction with autonomous systems is broadly studied, but
research has hitherto under-explored interactions that occur while the system
is actively learning, and can noticeably change its behaviour in minutes. In
this pilot study, we investigate how the interaction between a human and a
continually learning prediction agent develops as the agent develops
competency. Additionally, we compare two different agent architectures to
assess how representational choices in agent design affect the human-agent
interaction. We develop a virtual reality environment and a time-based
prediction task wherein learned predictions from a reinforcement learning (RL)
algorithm augment human predictions. We assess how a participant's performance
and behaviour in this task differs across agent types, using both quantitative
and qualitative analyses. Our findings suggest that human trust of the system
may be influenced by early interactions with the agent, and that trust in turn
affects strategic behaviour, but limitations of the pilot study rule out any
conclusive statement. We identify trust as a key feature of interaction to
focus on when considering RL-based technologies, and make several
recommendations for modification to this study in preparation for a
larger-scale investigation. A video summary of this paper can be found at
https://youtu.be/oVYJdnBqTwQ .","['Dylan J. A. Brenneis', 'Adam S. Parker', 'Michael Bradley Johanson', 'Andrew Butcher', 'Elnaz Davoodi', 'Leslie Acker', 'Matthew M. Botvinick', 'Joseph Modayil', 'Adam White', 'Patrick M. Pilarski']",2021-12-14T22:46:44Z,http://arxiv.org/abs/2112.07774v2
Metaverse Shape of Your Life for Future: A bibliometric snapshot,"The metaverse was first introduced in 1992. Many people saw Metaverse as a
new word but the concept of Metaverse is not a new term. However, Zuckerberg's
press release drew all the attention to the Metaverse. This study presents a
bibliometric evaluation of metaverse technology, which has been discussed in
the literature since the nineties. A field study is carried out especially for
the metaverse, which is a new and trendy subject. In this way, descriptive
information is presented on journals, institutions, prominent researchers, and
countries in the field, as well as extra evaluation on the prominent topics in
the field and researchers with heavy citations. In our study, which was carried
out by extracting the data of all documents between the years 1990-2021 from
the Web of Science database, it was seen that there were few studies in the
literature in the historical process for the metaverse, whose popularity has
reached its peak in recent months. In addition, it is seen that the subject is
handled intensively with virtual reality and augmented reality technologies,
and the education sector and digital marketing fields show interest in the
field. Metaverse will probably have entered many areas of our lives in the next
15-20 years, shape our lives by taking advantage of the opportunities of
developing technology.",['Muhammet Damar'],2021-12-08T13:46:16Z,http://arxiv.org/abs/2112.12068v1
Heterogenous Networks: From small cells to 5G NR-U,"With the exponential increase in mobile users, the mobile data demand has
grown tremendously. To meet these demands, cellular operators are constantly
innovating to enhance the capacity of cellular systems. Consequently, operators
have been reusing the licensed spectrum spatially, by deploying 4G/LTE small
cells (e.g., Femto Cells) in the past. However, despite the use of small cells,
licensed spectrum will be unable to meet the consistently rising data traffic
because of data-intensive applications such as augmented reality or virtual
reality (AR/VR) and on-the-go high-definition video streaming. Applications
such AR/VR and online gaming not only place extreme data demands on the
network, but are also latency-critical. To meet the QoS guarantees, cellular
operators have begun leveraging the unlicensed spectrum by coexisting with
Wi-Fi in the 5 GHz band. The standardizing body 3GPP, has prescribed cellular
standards for fair unlicensed coexistence with Wi-Fi, namely LTE Licensed
Assisted Access (LAA), New Radio in unlicensed (NR-U), and NR in Millimeter.
The rapid roll-out of LAA deployments in developed nations like the US, offers
an opportunity to study and analyze the performance of unlicensed coexistence
networks through real-world ground truth. Thus, this paper presents a
high-level overview of past, present, and future of the research in small cell
and unlicensed coexistence communication technologies. It outlines the vision
for future research work in the recently allocated unlicensed spectrum: The 6
GHz band, where the latest Wi-Fi standard, IEEE 802.11ax, will coexist with the
latest cellular technology, 5G New Radio (NR) in unlicensed.","['Vanlin Sathya', 'Srikant Manas Kala', 'Kalpana Naidu']",2021-12-28T18:01:34Z,http://arxiv.org/abs/2112.14240v1
"A Comprehensive Survey on Radio Frequency (RF) Fingerprinting:
  Traditional Approaches, Deep Learning, and Open Challenges","Fifth generation (5G) network and beyond envision massive Internet of Things
(IoT) rollout to support disruptive applications such as extended reality (XR),
augmented/virtual reality (AR/VR), industrial automation, autonomous driving,
and smart everything which brings together massive and diverse IoT devices
occupying the radio frequency (RF) spectrum. Along with the spectrum crunch and
throughput challenges, such a massive scale of wireless devices exposes
unprecedented threat surfaces. RF fingerprinting is heralded as a candidate
technology that can be combined with cryptographic and zero-trust security
measures to ensure data privacy, confidentiality, and integrity in wireless
networks. Motivated by the relevance of this subject in the future
communication networks, in this work, we present a comprehensive survey of RF
fingerprinting approaches ranging from a traditional view to the most recent
deep learning (DL)-based algorithms. Existing surveys have mostly focused on a
constrained presentation of the wireless fingerprinting approaches, however,
many aspects remain untold. In this work, however, we mitigate this by
addressing every aspect - background on signal intelligence (SIGINT),
applications, relevant DL algorithms, systematic literature review of RF
fingerprinting techniques spanning the past two decades, discussion on
datasets, and potential research avenues - necessary to elucidate this topic to
the reader in an encyclopedic manner.","['Anu Jagannath', 'Jithin Jagannath', 'Prem Sagar Pattanshetty Vasanth Kumar']",2022-01-03T14:42:53Z,http://arxiv.org/abs/2201.00680v3
Matching-based Service Offloading for Compute-less Driven IoT Networks,"With the advent of the Internet of Things (IoT) and 5G networks, edge
computing is offering new opportunities for business model and use cases
innovations. Service providers can now virtualize the cloud beyond the data
center to meet the latency, data sovereignty, reliability, and interoperability
requirements. Yet, many new applications (e.g., augmented reality, virtual
reality, artificial intelligence) are computation-intensive and
delay-sensitivity. These applications are invoked heavily with similar inputs
that could lead to the same output. Compute-less networks aim to implement a
network with a minimum amount of computation and communication. This can be
realized by offloading prevalent services to the edge and thus minimizing
communication in the core network and eliminating redundant computations using
the computation reuse concept. In this paper, we present matching-based
services offloading schemes for compute-less IoT networks. We adopt the
matching theory to match service offloading to the appropriate edge server(s).
Specifically, we design, WHISTLE, a vertical many-to-many offloading scheme
that aims to offload the most invoked and highly reusable services to the
appropriate edge servers. We further extend WHISTLE to provide horizontal
one-to-many computation reuse sharing among edge servers which leads to
bouncing less computation back to the cloud. We evaluate the efficiency and
effectiveness of WHISTLE with a real-world dataset. The obtained findings show
that WHISTLE is able to accelerate the tasks completion time by 20%, reduce the
computation up to 77%, and decrease the communication up to 71%. Theoretical
analyses also prove the stability of the designed schemes.","['Boubakr Nour', 'Soumaya Cherkaoui']",2022-01-11T20:55:19Z,http://arxiv.org/abs/2201.04195v1
"Spherical Convolution empowered FoV Prediction in 360-degree Video
  Multicast with Limited FoV Feedback","Field of view (FoV) prediction is critical in 360-degree video multicast,
which is a key component of the emerging Virtual Reality (VR) and Augmented
Reality (AR) applications. Most of the current prediction methods combining
saliency detection and FoV information neither take into account that the
distortion of projected 360-degree videos can invalidate the weight sharing of
traditional convolutional networks, nor do they adequately consider the
difficulty of obtaining complete multi-user FoV information, which degrades the
prediction performance. This paper proposes a spherical convolution-empowered
FoV prediction method, which is a multi-source prediction framework combining
salient features extracted from 360-degree video with limited FoV feedback
information. A spherical convolution neural network (CNN) is used instead of a
traditional two-dimensional CNN to eliminate the problem of weight sharing
failure caused by video projection distortion. Specifically, salient
spatial-temporal features are extracted through a spherical convolution-based
saliency detection model, after which the limited feedback FoV information is
represented as a time-series model based on a spherical convolution-empowered
gated recurrent unit network. Finally, the extracted salient video features are
combined to predict future user FoVs. The experimental results show that the
performance of the proposed method is better than other prediction methods.","['Jie Li', 'Ling Han', 'Cong Zhang', 'Qiyue Li', 'Zhi Liu']",2022-01-29T08:32:19Z,http://arxiv.org/abs/2201.12525v1
"Integrating Immersive Technologies for Algorithmic Design in
  Architecture","Architectural design practice has radically evolved over the course of its
history, due to technological improvements that gave rise to advanced automated
tools for many design tasks. Traditional paper drawings and scale models are
now accompanied by 2D and 3D Computer-Aided Architectural Design (CAAD)
software.
  While such tools improved in many ways, including performance and accuracy
improvements, the modalities of user interaction have mostly remained the same,
with 2D interfaces displayed on 2D screens. The maturation of Augmented Reality
(AR) and Virtual Reality (VR) technology has led to some level of integration
of these immersive technologies into architectural practice, but mostly limited
to visualisation purposes, e.g. to show a finished project to a potential
client.
  We posit that there is potential to employ such technologies earlier in the
architectural design process and therefore explore that possibility with a
focus on Algorithmic Design (AD), a CAAD paradigm that relies on (often visual)
algorithms to generate geometries. The main goal of this dissertation is to
demonstrate that AR and VR can be adopted for AD activities.
  To verify that claim, we follow an iterative prototype-based methodology to
develop research prototype software tools and evaluate them. The three
developed prototypes provide evidence that integrating immersive technologies
into the AD toolset provides opportunities for architects to improve their
workflow and to better present their creations to clients. Based on our
contributions and the feedback we gathered from architectural students and
other researchers that evaluated the developed prototypes, we additionally
provide insights as to future perspectives in the field.",['Adrien Coppens'],2022-02-25T14:18:04Z,http://arxiv.org/abs/2202.12722v1
"Distributed On-Sensor Compute System for AR/VR Devices: A
  Semi-Analytical Simulation Framework for Power Estimation","Augmented Reality/Virtual Reality (AR/VR) glasses are widely foreseen as the
next generation computing platform. AR/VR glasses are a complex ""system of
systems"" which must satisfy stringent form factor, computing-, power- and
thermal- requirements. In this paper, we will show that a novel distributed
on-sensor compute architecture, coupled with new semiconductor technologies
(such as dense 3D-IC interconnects and Spin-Transfer Torque Magneto Random
Access Memory, STT-MRAM) and, most importantly, a full hardware-software
co-optimization are the solutions to achieve attractive and socially acceptable
AR/VR glasses. To this end, we developed a semi-analytical simulation framework
to estimate the power consumption of novel AR/VR distributed on-sensor
computing architectures. The model allows the optimization of the main
technological features of the system modules, as well as the computer-vision
algorithm partition strategy across the distributed compute architecture. We
show that, in the case of the compute-intensive machine learning based Hand
Tracking algorithm, the distributed on-sensor compute architecture can reduce
the system power consumption compared to a centralized system, with the
additional benefits in terms of latency and privacy.","['Jorge Gomez', 'Saavan Patel', 'Syed Shakib Sarwar', 'Ziyun Li', 'Raffaele Capoccia', 'Zhao Wang', 'Reid Pinkham', 'Andrew Berkovich', 'Tsung-Hsun Tsai', 'Barbara De Salvo', 'Chiao Liu']",2022-03-14T20:18:24Z,http://arxiv.org/abs/2203.07474v1
RAZE: Region Guided Self-Supervised Gaze Representation Learning,"Automatic eye gaze estimation is an important problem in vision based
assistive technology with use cases in different emerging topics such as
augmented reality, virtual reality and human-computer interaction. Over the
past few years, there has been an increasing interest in unsupervised and
self-supervised learning paradigms as it overcomes the requirement of large
scale annotated data. In this paper, we propose RAZE, a Region guided
self-supervised gAZE representation learning framework which leverage from
non-annotated facial image data. RAZE learns gaze representation via auxiliary
supervision i.e. pseudo-gaze zone classification where the objective is to
classify visual field into different gaze zones (i.e. left, right and center)
by leveraging the relative position of pupil-centers. Thus, we automatically
annotate pseudo gaze zone labels of 154K web-crawled images and learn feature
representations via `Ize-Net' framework. `Ize-Net' is a capsule layer based CNN
architecture which can efficiently capture rich eye representation. The
discriminative behaviour of the feature representation is evaluated on four
benchmark datasets: CAVE, TabletGaze, MPII and RT-GENE. Additionally, we
evaluate the generalizability of the proposed network on two other downstream
task (i.e. driver gaze estimation and visual attention estimation) which
demonstrate the effectiveness of the learnt eye gaze representation.","['Neeru Dubey', 'Shreya Ghosh', 'Abhinav Dhall']",2022-08-04T06:23:49Z,http://arxiv.org/abs/2208.02485v2
"Metaverse for Healthcare: A Survey on Potential Applications, Challenges
  and Future Directions","The rapid progress in digitalization and automation have led to an
accelerated growth in healthcare, generating novel models that are creating new
channels for rendering treatment with reduced cost. The Metaverse is an
emerging technology in the digital space which has huge potential in
healthcare, enabling realistic experiences to the patients as well as the
medical practitioners. The Metaverse is a confluence of multiple enabling
technologies such as artificial intelligence, virtual reality, augmented
reality, internet of medical devices, robotics, quantum computing, etc. through
which new directions for providing quality healthcare treatment and services
can be explored. The amalgamation of these technologies ensures immersive,
intimate and personalized patient care. It also provides adaptive intelligent
solutions that eliminates the barriers between healthcare providers and
receivers. This article provides a comprehensive review of the Metaverse for
healthcare, emphasizing on the state of the art, the enabling technologies for
adopting the Metaverse for healthcare, the potential applications and the
related projects. The issues in the adaptation of the Metaverse for healthcare
applications are also identified and the plausible solutions are highlighted as
part of future research directions.","['Rajeswari Chengoden', 'Nancy Victor', 'Thien Huynh-The', 'Gokul Yenduri', 'Rutvij H. Jhaveri', 'Mamoun Alazab', 'Sweta Bhattacharya', 'Pawan Hegde', 'Praveen Kumar Reddy Maddikunta', 'Thippa Reddy Gadekallu']",2022-09-09T07:40:11Z,http://arxiv.org/abs/2209.04160v1
"MAGES 4.0: Accelerating the world's transition to VR training and
  democratizing the authoring of the medical metaverse","In this work, we propose MAGES 4.0, a novel Software Development Kit (SDK) to
accelerate the creation of collaborative medical training applications in
VR/AR. Our solution is essentially a low-code metaverse authoring platform for
developers to rapidly prototype high-fidelity and high-complexity medical
simulations. MAGES breaks the authoring boundaries across extended reality,
since networked participants can also collaborate using different
virtual/augmented reality as well as mobile and desktop devices, in the same
metaverse world. With MAGES we propose an upgrade to the outdated 150-year-old
master-apprentice medical training model. Our platform incorporates, in a
nutsell, the following novelties: a) 5G edge-cloud remote rendering and physics
dissection layer, b) realistic real-time simulation of organic tissues as
soft-bodies under 10ms, c) a highly realistic cutting and tearing algorithm, d)
neural network assessment for user profiling and, e) a VR recorder to record
and replay or debrief the training simulation from any perspective.","['Paul Zikas', 'Antonis Protopsaltis', 'Nick Lydatakis', 'Mike Kentros', 'Stratos Geronikolakis', 'Steve Kateros', 'Manos Kamarianakis', 'Giannis Evangelou', 'Achilleas Filippidis', 'Eleni Grigoriou', 'Dimitris Angelis', 'Michail Tamiolakis', 'Michael Dodis', 'George Kokiadis', 'John Petropoulos', 'Maria Pateraki', 'George Papagiannakis']",2022-09-19T08:10:35Z,http://arxiv.org/abs/2209.08819v2
Realistic Hair Synthesis with Generative Adversarial Networks,"Recent successes in generative modeling have accelerated studies on this
subject and attracted the attention of researchers. One of the most important
methods used to achieve this success is Generative Adversarial Networks (GANs).
It has many application areas such as; virtual reality (VR), augmented reality
(AR), super resolution, image enhancement. Despite the recent advances in hair
synthesis and style transfer using deep learning and generative modelling, due
to the complex nature of hair still contains unsolved challenges. The methods
proposed in the literature to solve this problem generally focus on making
high-quality hair edits on images. In this thesis, a generative adversarial
network method is proposed to solve the hair synthesis problem. While
developing this method, it is aimed to achieve real-time hair synthesis while
achieving visual outputs that compete with the best methods in the literature.
The proposed method was trained with the FFHQ dataset and then its results in
hair style transfer and hair reconstruction tasks were evaluated. The results
obtained in these tasks and the operating time of the method were compared with
MichiGAN, one of the best methods in the literature. The comparison was made at
a resolution of 128x128. As a result of the comparison, it has been shown that
the proposed method achieves competitive results with MichiGAN in terms of
realistic hair synthesis, and performs better in terms of operating time.","['Muhammed Pektas', 'Aybars Ugur']",2022-09-13T11:48:26Z,http://arxiv.org/abs/2209.12875v1
"A Novel Light Field Coding Scheme Based on Deep Belief Network &
  Weighted Binary Images for Additive Layered Displays","Light-field displays create an immersive experience by providing binocular
depth sensation and motion parallax. Stacking light attenuating layers is one
approach to implement a light field display with a broader depth of field, wide
viewing angles and high resolution. Due to the transparent holographic optical
element (HOE) layers, additive layered displays can be integrated into
augmented reality (AR) wearables to overlay virtual objects onto the real
world, creating a seamless mixed reality (XR) experience. This paper proposes a
novel framework for light field representation and coding that utilizes Deep
Belief Network (DBN) and weighted binary images suitable for additive layered
displays. The weighted binary representation of layers makes the framework more
flexible for adaptive bitrate encoding. The framework effectively captures
intrinsic redundancies in the light field data, and thus provides a scalable
solution for light field coding suitable for XR display applications. The
latent code is encoded by H.265 codec generating a rate-scalable bit-stream. We
achieve adaptive bitrate decoding by varying the number of weighted binary
images and the H.265 quantization parameter, while maintaining an optimal
reconstruction quality. The framework is tested on real and synthetic benchmark
datasets, and the results validate the rate-scalable property of the proposed
scheme.","['Sally Khaidem', 'Mansi Sharma']",2022-10-04T08:18:06Z,http://arxiv.org/abs/2210.01447v2
An Efficient FPGA Accelerator for Point Cloud,"Deep learning-based point cloud processing plays an important role in various
vision tasks, such as autonomous driving, virtual reality (VR), and augmented
reality (AR). The submanifold sparse convolutional network (SSCN) has been
widely used for the point cloud due to its unique advantages in terms of visual
results. However, existing convolutional neural network accelerators suffer
from non-trivial performance degradation when employed to accelerate SSCN
because of the extreme and unstructured sparsity, and the complex computational
dependency between the sparsity of the central activation and the neighborhood
ones. In this paper, we propose a high performance FPGA-based accelerator for
SSCN. Firstly, we develop a zero removing strategy to remove the coarse-grained
redundant regions, thus significantly improving computational efficiency.
Secondly, we propose a concise encoding scheme to obtain the matching
information for efficient point-wise multiplications. Thirdly, we develop a
sparse data matching unit and a computing core based on the proposed encoding
scheme, which can convert the irregular sparse operations into regular
multiply-accumulate operations. Finally, an efficient hardware architecture for
the submanifold sparse convolutional layer is developed and implemented on the
Xilinx ZCU102 field-programmable gate array board, where the 3D submanifold
sparse U-Net is taken as the benchmark. The experimental results demonstrate
that our design drastically improves computational efficiency, and can
dramatically improve the power efficiency by 51 times compared to GPU.","['Zilun Wang', 'Wendong Mao', 'Peixiang Yang', 'Zhongfeng Wang', 'Jun Lin']",2022-10-14T13:34:00Z,http://arxiv.org/abs/2210.07803v1
"Neural Distortion Fields for Spatial Calibration of Wide Field-of-View
  Near-Eye Displays","We propose a spatial calibration method for wide Field-of-View (FoV) Near-Eye
Displays (NEDs) with complex image distortions. Image distortions in NEDs can
destroy the reality of the virtual object and cause sickness. To achieve
distortion-free images in NEDs, it is necessary to establish a pixel-by-pixel
correspondence between the viewpoint and the displayed image. Designing compact
and wide-FoV NEDs requires complex optical designs. In such designs, the
displayed images are subject to gaze-contingent, non-linear geometric
distortions, which explicit geometric models can be difficult to represent or
computationally intensive to optimize.
  To solve these problems, we propose Neural Distortion Field (NDF), a
fully-connected deep neural network that implicitly represents display surfaces
complexly distorted in spaces. NDF takes spatial position and gaze direction as
input and outputs the display pixel coordinate and its intensity as perceived
in the input gaze direction. We synthesize the distortion map from a novel
viewpoint by querying points on the ray from the viewpoint and computing a
weighted sum to project output display coordinates into an image. Experiments
showed that NDF calibrates an augmented reality NED with 90$^{\circ}$ FoV with
about 3.23 pixel (5.8 arcmin) median error using only 8 training viewpoints.
Additionally, we confirmed that NDF calibrates more accurately than the
non-linear polynomial fitting, especially around the center of the FoV.","['Yuichi Hiroi', 'Kiyosato Someya', 'Yuta Itoh']",2022-10-22T08:48:31Z,http://arxiv.org/abs/2210.12389v1
"A DirectX-Based DICOM Viewer for Multi-User Surgical Planning in
  Augmented Reality","Preoperative medical imaging is an essential part of surgical planning. The
data from medical imaging devices, such as CT and MRI scanners, consist of
stacks of 2D images in DICOM format. Conversely, advances in 3D data
visualization provide further information by assembling cross-sections into 3D
volumetric datasets. As Microsoft unveiled the HoloLens 2 (HL2), which is
considered one of the best Mixed Reality (XR) headsets in the market, it
promised to enhance visualization in 3D by providing an immersive experience to
users. This paper introduces a prototype holographic XR DICOM Viewer for the 3D
visualization of DICOM image sets on HL2 for surgical planning. We first
developed a standalone graphical C++ engine using the native DirectX11 API and
HLSL shaders. Based on that, the prototype further applies the OpenXR API for
potential deployment on a wide range of devices from vendors across the XR
spectrum. With native access to the device, our prototype unravels the
limitation of hardware capabilities on HL2 for 3D volume rendering and
interaction. Moreover, smartphones can act as input devices to provide another
user interaction method by connecting to our server. In this paper, we present
a holographic DICOM viewer for the HoloLens 2 and contribute (i) a prototype
that renders the DICOM image stacks in real-time on HL2, (ii) three types of
user interactions in XR, and (iii) a preliminary qualitative evaluation of our
prototype.","['Menghe Zhang', 'Weichen Liu', 'Nadir Weibel', 'Jurgen Schulze']",2022-10-25T21:22:00Z,http://arxiv.org/abs/2210.14349v1
Learning Variational Motion Prior for Video-based Motion Capture,"Motion capture from a monocular video is fundamental and crucial for us
humans to naturally experience and interact with each other in Virtual Reality
(VR) and Augmented Reality (AR). However, existing methods still struggle with
challenging cases involving self-occlusion and complex poses due to the lack of
effective motion prior modeling. In this paper, we present a novel variational
motion prior (VMP) learning approach for video-based motion capture to resolve
the above issue. Instead of directly building the correspondence between the
video and motion domain, We propose to learn a generic latent space for
capturing the prior distribution of all natural motions, which serve as the
basis for subsequent video-based motion capture tasks. To improve the
generalization capacity of prior space, we propose a transformer-based
variational autoencoder pretrained over marker-based 3D mocap data, with a
novel style-mapping block to boost the generation quality. Afterward, a
separate video encoder is attached to the pretrained motion generator for
end-to-end fine-tuning over task-specific video datasets. Compared to existing
motion prior models, our VMP model serves as a motion rectifier that can
effectively reduce temporal jittering and failure modes in frame-wise pose
estimation, leading to temporally stable and visually realistic motion capture
results. Furthermore, our VMP-based framework models motion at sequence level
and can directly generate motion clips in the forward pass, achieving real-time
motion capture during inference. Extensive experiments over both public
datasets and in-the-wild videos have demonstrated the efficacy and
generalization capability of our framework.","['Xin Chen', 'Zhuo Su', 'Lingbo Yang', 'Pei Cheng', 'Lan Xu', 'Bin Fu', 'Gang Yu']",2022-10-27T02:45:48Z,http://arxiv.org/abs/2210.15134v2
"Rate-Distortion Modeling for Bit Rate Constrained Point Cloud
  Compression","As being one of the main representation formats of 3D real world and
well-suited for virtual reality and augmented reality applications, point
clouds have gained a lot of popularity. In order to reduce the huge amount of
data, a considerable amount of research on point cloud compression has been
done. However, given a target bit rate, how to properly choose the color and
geometry quantization parameters for compressing point clouds is still an open
issue. In this paper, we propose a rate-distortion model based quantization
parameter selection scheme for bit rate constrained point cloud compression.
Firstly, to overcome the measurement uncertainty in evaluating the distortion
of the point clouds, we propose a unified model to combine the geometry
distortion and color distortion. In this model, we take into account the
correlation between geometry and color variables of point clouds and derive a
dimensionless quantity to represent the overall quality degradation. Then, we
derive the relationships of overall distortion and bit rate with the
quantization parameters. Finally, we formulate the bit rate constrained point
cloud compression as a constrained minimization problem using the derived
polynomial models and deduce the solution via an iterative numerical method.
Experimental results show that the proposed algorithm can achieve optimal
decoded point cloud quality at various target bit rates, and substantially
outperform the video-rate-distortion model based point cloud compression
scheme.","['Pan Gao', 'Shengzhou Luo', 'Manoranjan Paul']",2022-11-19T10:21:06Z,http://arxiv.org/abs/2211.10646v1
"NeuralLift-360: Lifting An In-the-wild 2D Photo to A 3D Object with
  360° Views","Virtual reality and augmented reality (XR) bring increasing demand for 3D
content. However, creating high-quality 3D content requires tedious work that a
human expert must do. In this work, we study the challenging task of lifting a
single image to a 3D object and, for the first time, demonstrate the ability to
generate a plausible 3D object with 360{\deg} views that correspond well with
the given reference image. By conditioning on the reference image, our model
can fulfill the everlasting curiosity for synthesizing novel views of objects
from images. Our technique sheds light on a promising direction of easing the
workflows for 3D artists and XR designers. We propose a novel framework, dubbed
NeuralLift-360, that utilizes a depth-aware neural radiance representation
(NeRF) and learns to craft the scene guided by denoising diffusion models. By
introducing a ranking loss, our NeuralLift-360 can be guided with rough depth
estimation in the wild. We also adopt a CLIP-guided sampling strategy for the
diffusion prior to provide coherent guidance. Extensive experiments demonstrate
that our NeuralLift-360 significantly outperforms existing state-of-the-art
baselines. Project page: https://vita-group.github.io/NeuralLift-360/","['Dejia Xu', 'Yifan Jiang', 'Peihao Wang', 'Zhiwen Fan', 'Yi Wang', 'Zhangyang Wang']",2022-11-29T17:59:06Z,http://arxiv.org/abs/2211.16431v2
"Pensieve 5G: Implementation of RL-based ABR Algorithm for UHD 4K/8K
  Content Delivery on Commercial 5G SA/NR-DC Network","While the rollout of the fifth-generation mobile network (5G) is underway
across the globe with the intention to deliver 4K/8K UHD videos, Augmented
Reality (AR), and Virtual Reality (VR) content to the mass amounts of users,
the coverage and throughput are still one of the most significant issues,
especially in the rural areas, where only 5G in the low-frequency band are
being deployed. This called for a high-performance adaptive bitrate (ABR)
algorithm that can maximize the user quality of experience given 5G network
characteristics and data rate of UHD contents.
  Recently, many of the newly proposed ABR techniques were machine-learning
based. Among that, Pensieve is one of the state-of-the-art techniques, which
utilized reinforcement-learning to generate an ABR algorithm based on
observation of past decision performance. By incorporating the context of the
5G network and UHD content, Pensieve has been optimized into Pensieve 5G. New
QoE metrics that more accurately represent the QoE of UHD video streaming on
the different types of devices were proposed and used to evaluate Pensieve 5G
against other ABR techniques including the original Pensieve. The results from
the simulation based on the real 5G Standalone (SA) network throughput shows
that Pensieve 5G outperforms both conventional algorithms and Pensieve with the
average QoE improvement of 8.8% and 14.2%, respectively. Additionally, Pensieve
5G also performed well on the commercial 5G NR-NR Dual Connectivity (NR-DC)
Network, despite the training being done solely using the data from the 5G
Standalone (SA) network.","['Kasidis Arunruangsirilert', 'Bo Wei', 'Hang Song', 'Jiro Katto']",2022-12-29T23:03:47Z,http://arxiv.org/abs/2212.14479v1
"Passively Addressed Robotic Morphing Surface (PARMS) Based on Machine
  Learning","Reconfigurable morphing surfaces provide new opportunities for advanced
human-machine interfaces and bio-inspired robotics. Morphing into arbitrary
surfaces on demand requires a device with a sufficiently large number of
actuators and an inverse control strategy that can calculate the actuator
stimulation necessary to achieve a target surface. The programmability of a
morphing surface can be improved by increasing the number of independent
actuators, but this increases the complexity of the control system. Thus,
developing compact and efficient control interfaces and control algorithms is a
crucial knowledge gap for the adoption of morphing surfaces in broad
applications. In this work, we describe a passively addressed robotic morphing
surface (PARMS) composed of matrix-arranged ionic actuators. To reduce the
complexity of the physical control interface, we introduce passive matrix
addressing. Matrix addressing allows the control of independent actuators using
only 2N control inputs, which is significantly lower than control inputs
required for traditional direct addressing. Our control algorithm is based on
machine learning using finite element simulations as the training data. This
machine learning approach allows both forward and inverse control with high
precision in real time. Inverse control demonstrations show that the PARMS can
dynamically morph into arbitrary pre-defined surfaces on demand. These
innovations in actuator matrix control may enable future implementation of
PARMS in wearables, haptics, and augmented reality/virtual reality (AR/VR).","['Jue Wang', 'Michael Sotzing', 'Mina Lee', 'Alex Chortos']",2023-01-30T20:51:14Z,http://arxiv.org/abs/2301.13284v2
Augmented Reality Remote Operation of Dual Arm Manipulators in Hot Boxes,"In nuclear isotope and chemistry laboratories, hot cells and gloveboxes
provide scientists with a controlled and safe environment to perform
experiments. Working on experiments in these isolated containment cells
requires scientists to be physically present. For hot cell work today,
scientists manipulate equipment and radioactive material inside through a
bilateral mechanical control mechanism. Motions produced outside the cell with
the master control levers are mechanically transferred to the internal grippers
inside the shielded containment cell. There is a growing need to have the
capability to conduct experiments within these cells remotely. A simple method
to enable remote manipulations within hot cell and glovebox cells is to mount
two robotic arms inside a box to mimic the motions of human hands. An AR
application was built in this work to allow a user wearing a Microsoft HoloLens
2 headset to teleoperate dual arm manipulators by grasping robotic end-effector
digital replicas in AR from a remote location. In addition to the real-time
replica of the physical robotic arms in AR, the application enables users to
view a live video stream attached to the robotic arms and parse a 3D point
cloud of 3D objects in their remote AR environment for better situational
awareness. This work also provides users with virtual fixture to assist in
manipulation and other teleoperation tasks.","['Frank Regal', 'Young Soo Park', 'Jerry Nolan', 'Mitch Pryor']",2023-03-28T15:36:06Z,http://arxiv.org/abs/2303.16055v1
Visual Localization using Imperfect 3D Models from the Internet,"Visual localization is a core component in many applications, including
augmented reality (AR). Localization algorithms compute the camera pose of a
query image w.r.t. a scene representation, which is typically built from
images. This often requires capturing and storing large amounts of data,
followed by running Structure-from-Motion (SfM) algorithms. An interesting, and
underexplored, source of data for building scene representations are 3D models
that are readily available on the Internet, e.g., hand-drawn CAD models, 3D
models generated from building footprints, or from aerial images. These models
allow to perform visual localization right away without the time-consuming
scene capturing and model building steps. Yet, it also comes with challenges as
the available 3D models are often imperfect reflections of reality. E.g., the
models might only have generic or no textures at all, might only provide a
simple approximation of the scene geometry, or might be stretched. This paper
studies how the imperfections of these models affect localization accuracy. We
create a new benchmark for this task and provide a detailed experimental
evaluation based on multiple 3D models per scene. We show that 3D models from
the Internet show promise as an easy-to-obtain scene representation. At the
same time, there is significant room for improvement for visual localization
pipelines. To foster research on this interesting and challenging task, we
release our benchmark at v-pnk.github.io/cadloc.","['Vojtech Panek', 'Zuzana Kukelova', 'Torsten Sattler']",2023-04-12T16:15:05Z,http://arxiv.org/abs/2304.05947v1
"Revival of the Silk Road using the applications of AR/VR and its role on
  cultural tourism","This research project seeks to investigate the incorporation of augmented
reality (AR) and virtual reality (VR) technology with human-computer
interaction (HCI) in order to revitalize the Silk Road - specifically in
Kermanshah, Iran - and its effect on cultural tourism. Kermanshah has
underexplored the rich historical significance of the Silk Road, despite the
presence of 24 UNESCO World Heritage sites. From the 2nd century BCE to the
18th century CE, the Silk Road was a vital trade route connecting the West and
the East and had enormous cultural, economic, religious, and political effects.
The purpose of this study is to examine the application of AR/VR technologies
in HCI for the preservation, interpretation, and promotion of the Silk Road's
tangible and intangible cultural heritage in Kermanshah, as well as their
impact on cultural tourism development. The study also investigates how these
innovative technologies can enhance visitors' experiences through immersive and
interactive approaches, promote sustainable tourism practices, and contribute
to the region's broader socioeconomic benefits. The research will analyze the
challenges and opportunities of implementing AR/VR technology in HCI within the
context of cultural heritage and tourism in Kermanshah and the Silk Road region
more broadly. By combining HCI, AR/VR, and cultural tourism, this research
seeks to provide valuable insights into the development of user-centered,
immersive experiences that promote a deeper understanding and appreciation of
the Silk Road's distinctive cultural heritage.",['Sahar Zandi'],2023-04-13T11:31:53Z,http://arxiv.org/abs/2304.10545v1
"Privacy Computing Meets Metaverse: Necessity, Taxonomy and Challenges","Metaverse, the core of the next-generation Internet, is a computer-generated
holographic digital environment that simultaneously combines spatio-temporal,
immersive, real-time, sustainable, interoperable, and data-sensitive
characteristics. It cleverly blends the virtual and real worlds, allowing users
to create, communicate, and transact in virtual form. With the rapid
development of emerging technologies including augmented reality, virtual
reality and blockchain, the metaverse system is becoming more and more
sophisticated and widely used in various fields such as social, tourism,
industry and economy. However, the high level of interaction with the real
world also means a huge risk of privacy leakage both for individuals and
enterprises, which has hindered the wide deployment of metaverse. Then, it is
inevitable to apply privacy computing techniques in the framework of metaverse,
which is a current research hotspot. In this paper, we conduct comprehensive
research on the necessity, taxonomy and challenges when privacy computing meets
metaverse. Specifically, we first introduce the underlying technologies and
various applications of metaverse, on which we analyze the challenges of data
usage in metaverse, especially data privacy. Next, we review and summarize
state-of-the-art solutions based on federated learning, differential privacy,
homomorphic encryption, and zero-knowledge proofs for different privacy
problems in metaverse. Finally, we show the current security and privacy
challenges in the development of metaverse and provide open directions for
building a well-established privacy-preserving metaverse system. For easy
access and reference, we integrate the related publications and their codes
into a GitHub repository:
https://github.com/6lyc/Awesome-Privacy-Computing-in-Metaverse.git.","['Chuan Chen', 'Yuecheng Li', 'Zhenpeng Wu', 'Chengyuan Mai', 'Youming Liu', 'Yanming Hu', 'Zibin Zheng', 'Jiawen Kang']",2023-04-23T13:05:58Z,http://arxiv.org/abs/2304.11643v2
"HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection","An individualised head-related transfer function (HRTF) is very important for
creating realistic virtual reality (VR) and augmented reality (AR)
environments. However, acoustically measuring high-quality HRTFs requires
expensive equipment and an acoustic lab setting. To overcome these limitations
and to make this measurement more efficient HRTF upsampling has been exploited
in the past where a high-resolution HRTF is created from a low-resolution one.
This paper demonstrates how generative adversarial networks (GANs) can be
applied to HRTF upsampling. We propose a novel approach that transforms the
HRTF data for direct use with a convolutional super-resolution generative
adversarial network (SRGAN). This new approach is benchmarked against three
baselines: barycentric upsampling, spherical harmonic (SH) upsampling and an
HRTF selection approach. Experimental results show that the proposed method
outperforms all three baselines in terms of log-spectral distortion (LSD) and
localisation performance using perceptual models when the input HRTF is sparse
(less than 20 measured positions).","['Aidan O. T. Hogg', 'Mads Jenkins', 'He Liu', 'Isaac Squires', 'Samuel J. Cooper', 'Lorenzo Picinali']",2023-06-09T11:05:09Z,http://arxiv.org/abs/2306.05812v2
VibHead: An Authentication Scheme for Smart Headsets through Vibration,"Recent years have witnessed the fast penetration of Virtual Reality (VR) and
Augmented Reality (AR) systems into our daily life, the security and privacy
issues of the VR/AR applications have been attracting considerable attention.
Most VR/AR systems adopt head-mounted devices (i.e., smart headsets) to
interact with users and the devices usually store the users' private data.
Hence, authentication schemes are desired for the head-mounted devices.
Traditional knowledge-based authentication schemes for general personal devices
have been proved vulnerable to shoulder-surfing attacks, especially considering
the headsets may block the sight of the users. Although the robustness of the
knowledge-based authentication can be improved by designing complicated secret
codes in virtual space, this approach induces a compromise of usability.
Another choice is to leverage the users' biometrics; however, it either relies
on highly advanced equipments which may not always be available in commercial
headsets or introduce heavy cognitive load to users.
  In this paper, we propose a vibration-based authentication scheme, VibHead,
for smart headsets. Since the propagation of vibration signals through human
heads presents unique patterns for different individuals, VibHead employs a
CNN-based model to classify registered legitimate users based the features
extracted from the vibration signals. We also design a two-step authentication
scheme where the above user classifiers are utilized to distinguish the
legitimate user from illegitimate ones. We implement VibHead on a Microsoft
HoloLens equipped with a linear motor and an IMU sensor which are commonly used
in off-the-shelf personal smart devices. According to the results of our
extensive experiments, with short vibration signals ($\leq 1s$), VibHead has an
outstanding authentication accuracy; both FAR and FRR are around 5%.","['Feng Li', 'Jiayi Zhao', 'Huan Yang', 'Dongxiao Yu', 'Yuanfeng Zhou', 'Yiran Shen']",2023-06-29T15:00:32Z,http://arxiv.org/abs/2306.17002v1
"Robust Roadside Perception: an Automated Data Synthesis Pipeline
  Minimizing Human Annotation","Recently, advancements in vehicle-to-infrastructure communication
technologies have elevated the significance of infrastructure-based roadside
perception systems for cooperative driving. This paper delves into one of its
most pivotal challenges: data insufficiency. The lacking of high-quality
labeled roadside sensor data with high diversity leads to low robustness, and
low transfer-ability of current roadside perception systems. In this paper, a
novel solution is proposed to address this problem that creates synthesized
training data using Augmented Reality. A Generative Adversarial Network is then
applied to enhance the reality further, that produces a photo-realistic
synthesized dataset that is capable of training or fine-tuning a roadside
perception detector which is robust to different weather and lighting
conditions. Our approach was rigorously tested at two key intersections in
Michigan, USA: the Mcity intersection and the State St./Ellsworth Rd
roundabout. The Mcity intersection is located within the Mcity test field, a
controlled testing environment. In contrast, the State St./Ellsworth Rd
intersection is a bustling roundabout notorious for its high traffic flow and a
significant number of accidents annually. Experimental results demonstrate that
detectors trained solely on synthesized data exhibit commendable performance
across all conditions. Furthermore, when integrated with labeled data, the
synthesized data can notably bolster the performance of pre-existing detectors,
especially in adverse conditions.","['Rusheng Zhang', 'Depu Meng', 'Lance Bassett', 'Shengyin Shen', 'Zhengxia Zou', 'Henry X. Liu']",2023-06-29T21:00:57Z,http://arxiv.org/abs/2306.17302v2
"From Talent Shortage to Workforce Excellence in the CHIPS Act Era:
  Harnessing Industry 4.0 Paradigms for a Sustainable Future in Domestic Chip
  Production","The CHIPS Act is driving the U.S. towards a self-sustainable future in
domestic chip production. Decades of outsourced manufacturing, assembly,
testing, and packaging has diminished the workforce ecosystem, imposing major
limitations on semiconductor companies racing to build new fabrication sites as
part of the CHIPS Act. In response, a systemic alliance between academic
institutions, the industry, government, various consortiums, and organizations
has emerged to establish a pipeline to educate and onboard the next generation
of talent. Establishing a stable and continuous flow of talent requires
significant time investments and comes with no guarantees, particularly
factoring in the low workplace desirability in current fabrication houses for
U.S workforce. This paper will explore the feasibility of two paradigms of
Industry 4.0, automation and Augmented Reality(AR)/Virtual Reality(VR), to
complement ongoing workforce development efforts and optimize workplace
desirability by catalyzing core manufacturing processes and effectively
enhancing the education, onboarding, and professional realms-all with promising
capabilities amid the ongoing talent shortage and trajectory towards advanced
packaging.","['Aida Damanpak Rizi', 'Antika Roy', 'Rouhan Noor', 'Hyo Kang', 'Nitin Varshney', 'Katja Jacob', 'Sindia Rivera-Jimenez', 'Nathan Edwards', 'Volker J. Sorger', 'Hamed Dalir', 'Navid Asadizanjani']",2023-08-01T01:15:51Z,http://arxiv.org/abs/2308.00215v1
"Energy-Efficient Deadline-Aware Edge Computing: Bandit Learning with
  Partial Observations in Multi-Channel Systems","In this paper, we consider a task offloading problem in a multi-access edge
computing (MEC) network, in which edge users can either use their local
processing unit to compute their tasks or offload their tasks to a nearby edge
server through multiple communication channels each with different
characteristics. The main objective is to maximize the energy efficiency of the
edge users while meeting computing tasks deadlines. In the multi-user
multi-channel offloading scenario, users are distributed with partial
observations of the system states. We formulate this problem as a stochastic
optimization problem and leverage \emph{contextual neural multi-armed bandit}
models to develop an energy-efficient deadline-aware solution, dubbed E2DA. The
proposed E2DA framework only relies on partial state information (i.e.,
computation task features) to make offloading decisions. Through extensive
numerical analysis, we demonstrate that the E2DA algorithm can efficiently
learn an offloading policy and achieve close-to-optimal performance in
comparison with several baseline policies that optimize energy consumption
and/or response time. Furthermore, we provide a comprehensive set of results on
the MEC system performance for various applications such as augmented reality
(AR) and virtual reality (VR).","['Babak Badnava', 'Keenan Roach', 'Kenny Cheung', 'Morteza Hashemi', 'Ness B Shroff']",2023-08-12T21:48:04Z,http://arxiv.org/abs/2308.06647v1
"Metaverse: A Vision, Architectural Elements, and Future Directions for
  Scalable and Realtime Virtual Worlds","With the emergence of Cloud computing, Internet of Things-enabled
Human-Computer Interfaces, Generative Artificial Intelligence, and
high-accurate Machine and Deep-learning recognition and predictive models,
along with the Post Covid-19 proliferation of social networking, and remote
communications, the Metaverse gained a lot of popularity. Metaverse has the
prospective to extend the physical world using virtual and augmented reality so
the users can interact seamlessly with the real and virtual worlds using
avatars and holograms. It has the potential to impact people in the way they
interact on social media, collaborate in their work, perform marketing and
business, teach, learn, and even access personalized healthcare. Several works
in the literature examine Metaverse in terms of hardware wearable devices, and
virtual reality gaming applications. However, the requirements of realizing the
Metaverse in realtime and at a large-scale need yet to be examined for the
technology to be usable. To address this limitation, this paper presents the
temporal evolution of Metaverse definitions and captures its evolving
requirements. Consequently, we provide insights into Metaverse requirements. In
addition to enabling technologies, we lay out architectural elements for
scalable, reliable, and efficient Metaverse systems, and a classification of
existing Metaverse applications along with proposing required future research
directions.","['Leila Ismail', 'Rajkumar Buyya']",2023-08-21T08:23:10Z,http://arxiv.org/abs/2308.10559v2
"Immersive Technologies in Virtual Companions: A Systematic Literature
  Review","The emergence of virtual companions is transforming the evolution of
intelligent systems that effortlessly cater to the unique requirements of
users. These advanced systems not only take into account the user present
capabilities, preferences, and needs but also possess the capability to adapt
dynamically to changes in the environment, as well as fluctuations in the users
emotional state or behavior. A virtual companion is an intelligent software or
application that offers support, assistance, and companionship across various
aspects of users lives. Various enabling technologies are involved in building
virtual companion, among these, Augmented Reality (AR), and Virtual Reality
(VR) are emerging as transformative tools. While their potential for use in
virtual companions or digital assistants is promising, their applications in
these domains remain relatively unexplored. To address this gap, a systematic
review was conducted to investigate the applications of VR, AR, and MR
immersive technologies in the development of virtual companions. A
comprehensive search across PubMed, Scopus, and Google Scholar yielded 28
relevant articles out of a pool of 644. The review revealed that immersive
technologies, particularly VR and AR, play a significant role in creating
digital assistants, offering a wide range of applications that brings various
facilities in the individuals life in areas such as addressing social
isolation, enhancing cognitive abilities and dementia care, facilitating
education, and more. Additionally, AR and MR hold potential for enhancing
Quality of life (QoL) within the context of virtual companion technology. The
findings of this review provide a valuable foundation for further research in
this evolving field.","['Ziaullah Momand', 'Jonathan H. Chan', 'Pornchai Mongkolnam']",2023-09-03T16:39:22Z,http://arxiv.org/abs/2309.01214v1
"Hand Gesture Recognition with Two Stage Approach Using Transfer Learning
  and Deep Ensemble Learning","Human-Computer Interaction (HCI) has been the subject of research for many
years, and recent studies have focused on improving its performance through
various techniques. In the past decade, deep learning studies have shown high
performance in various research areas, leading researchers to explore their
application to HCI. Convolutional neural networks can be used to recognize hand
gestures from images using deep architectures. In this study, we evaluated
pre-trained high-performance deep architectures on the HG14 dataset, which
consists of 14 different hand gesture classes. Among 22 different models,
versions of the VGGNet and MobileNet models attained the highest accuracy
rates. Specifically, the VGG16 and VGG19 models achieved accuracy rates of
94.64% and 94.36%, respectively, while the MobileNet and MobileNetV2 models
achieved accuracy rates of 96.79% and 94.43%, respectively. We performed hand
gesture recognition on the dataset using an ensemble learning technique, which
combined the four most successful models. By utilizing these models as base
learners and applying the Dirichlet ensemble technique, we achieved an accuracy
rate of 98.88%. These results demonstrate the effectiveness of the deep
ensemble learning technique for HCI and its potential applications in areas
such as augmented reality, virtual reality, and game technologies.","['Serkan Savaş', 'Atilla Ergüzen']",2023-09-20T19:53:05Z,http://arxiv.org/abs/2309.11610v1
GaitGuard: Towards Private Gait in Mixed Reality,"Augmented/Mixed Reality (AR/MR) devices are unique from other mobile systems
because of their capability to offer an immersive multi-user collaborative
experience. While previous studies have explored privacy and security aspects
of multiple user interactions in AR/MR, a less-explored area is the
vulnerability of gait privacy. Gait is considered a private state because it is
a highly individualistic and a distinctive biometric trait. Thus, preserving
gait privacy in emerging AR/MR systems is crucial to safeguard individuals from
potential identity tracking and unauthorized profiling.
  This paper first introduces GaitExtract, a framework designed to
automatically detect gait information in humans, shedding light on the nuances
of gait privacy in AR/MR. In this paper, we designed GaitExtract, a framework
that can automatically detect the outside gait information of a human and
investigate the vulnerability of gait privacy in AR. In a user study with 20
participants, our findings reveal that participants were uniquely identifiable
with an accuracy of up to 78% using GaitExtract. Consequently, we propose
GaitGuard, a system that safeguards gait information of people appearing in the
camera view of the AR/MR device.
  Furthermore, we tested GaitGuard in an MR collaborative application,
achieving 22 fps while streaming mitigated frames to the collaborative server.
Our user-study survey indicated that users are more comfortable with releasing
videos of them walking when GaitGuard is applied to the frames. These results
underscore the efficacy and practicality of GaitGuard in mitigating gait
privacy concerns in MR contexts.","['Diana Romero', 'Ruchi Jagdish Patel', 'Athina Markopoulou', 'Salma Elmalaki']",2023-12-07T17:42:04Z,http://arxiv.org/abs/2312.04470v2
"Collaborative System Design of Mixed Reality Communication for Medical
  Training","We present the design of a mixed reality (MR) telehealth training system that
aims to close the gap between in-person and distance training and re-training
for medical procedures. Our system uses real-time volumetric capture as a means
for communicating and relating spatial information between the non-colocated
trainee and instructor. The system's design is based on a requirements
elicitation study performed in situ, at a medical school simulation training
center. The focus is on the lightweight real-time transmission of volumetric
data - meaning the use of consumer hardware, easy and quick deployment, and
low-demand computations. We evaluate the MR system design by analyzing the
workload for the users during medical training. We compare in-person, video,
and MR training workloads. The results indicate that the overall workload for
central line placement training with MR does not increase significantly
compared to video communication. Our work shows that, when designed
strategically together with domain experts, an MR communication system can be
used effectively for complex medical procedural training without increasing the
overall workload for users significantly. Moreover, MR systems offer new
opportunities for teaching due to spatial information, hand tracking, and
augmented communication.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka', 'Christian Guetl']",2023-12-14T22:36:56Z,http://arxiv.org/abs/2312.09382v1
Enabling Technologies for Web 3.0: A Comprehensive Survey,"Web 3.0 represents the next stage of Internet evolution, aiming to empower
users with increased autonomy, efficiency, quality, security, and privacy. This
evolution can potentially democratize content access by utilizing the latest
developments in enabling technologies. In this paper, we conduct an in-depth
survey of enabling technologies in the context of Web 3.0, such as blockchain,
semantic web, 3D interactive web, Metaverse, Virtual reality/Augmented reality,
Internet of Things technology, and their roles in shaping Web 3.0. We commence
by providing a comprehensive background of Web 3.0, including its concept,
basic architecture, potential applications, and industry adoption.
Subsequently, we examine recent breakthroughs in IoT, 5G, and blockchain
technologies that are pivotal to Web 3.0 development. Following that, other
enabling technologies, including AI, semantic web, and 3D interactive web, are
discussed. Utilizing these technologies can effectively address the critical
challenges in realizing Web 3.0, such as ensuring decentralized identity,
platform interoperability, data transparency, reducing latency, and enhancing
the system's scalability. Finally, we highlight significant challenges
associated with Web 3.0 implementation, emphasizing potential solutions and
providing insights into future research directions in this field.","['Md Arif Hassan', 'Mohammad Behdad Jamshidi', 'Bui Duc Manh', 'Nam H. Chu', 'Chi-Hieu Nguyen', 'Nguyen Quang Hieu', 'Cong T. Nguyen', 'Dinh Thai Hoang', 'Diep N. Nguyen', 'Nguyen Van Huynh', 'Mohammad Abu Alsheikh', 'Eryk Dutkiewicz']",2023-12-29T10:22:18Z,http://arxiv.org/abs/2401.10901v1
"Full-Body Motion Reconstruction with Sparse Sensing from Graph
  Perspective","Estimating 3D full-body pose from sparse sensor data is a pivotal technique
employed for the reconstruction of realistic human motions in Augmented Reality
and Virtual Reality. However, translating sparse sensor signals into
comprehensive human motion remains a challenge since the sparsely distributed
sensors in common VR systems fail to capture the motion of full human body. In
this paper, we use well-designed Body Pose Graph (BPG) to represent the human
body and translate the challenge into a prediction problem of graph missing
nodes. Then, we propose a novel full-body motion reconstruction framework based
on BPG. To establish BPG, nodes are initially endowed with features extracted
from sparse sensor signals. Features from identifiable joint nodes across
diverse sensors are amalgamated and processed from both temporal and spatial
perspectives. Temporal dynamics are captured using the Temporal Pyramid
Structure, while spatial relations in joint movements inform the spatial
attributes. The resultant features serve as the foundational elements of the
BPG nodes. To further refine the BPG, node features are updated through a graph
neural network that incorporates edge reflecting varying joint relations. Our
method's effectiveness is evidenced by the attained state-of-the-art
performance, particularly in lower body motion, outperforming other baseline
methods. Additionally, an ablation study validates the efficacy of each module
in our proposed framework.","['Feiyu Yao', 'Zongkai Wu', 'Li Yi']",2024-01-22T09:29:42Z,http://arxiv.org/abs/2401.11783v1
Anything in Any Scene: Photorealistic Video Object Insertion,"Realistic video simulation has shown significant potential across diverse
applications, from virtual reality to film production. This is particularly
true for scenarios where capturing videos in real-world settings is either
impractical or expensive. Existing approaches in video simulation often fail to
accurately model the lighting environment, represent the object geometry, or
achieve high levels of photorealism. In this paper, we propose Anything in Any
Scene, a novel and generic framework for realistic video simulation that
seamlessly inserts any object into an existing dynamic video with a strong
emphasis on physical realism. Our proposed general framework encompasses three
key processes: 1) integrating a realistic object into a given scene video with
proper placement to ensure geometric realism; 2) estimating the sky and
environmental lighting distribution and simulating realistic shadows to enhance
the light realism; 3) employing a style transfer network that refines the final
video output to maximize photorealism. We experimentally demonstrate that
Anything in Any Scene framework produces simulated videos of great geometric
realism, lighting realism, and photorealism. By significantly mitigating the
challenges associated with video data generation, our framework offers an
efficient and cost-effective solution for acquiring high-quality videos.
Furthermore, its applications extend well beyond video data augmentation,
showing promising potential in virtual reality, video editing, and various
other video-centric applications. Please check our project website
https://anythinginanyscene.github.io for access to our project code and more
high-resolution video results.","['Chen Bai', 'Zeman Shao', 'Guoxiang Zhang', 'Di Liang', 'Jie Yang', 'Zhuorui Zhang', 'Yujian Guo', 'Chengzhang Zhong', 'Yiqiao Qiu', 'Zhendong Wang', 'Yichen Guan', 'Xiaoyin Zheng', 'Tao Wang', 'Cheng Lu']",2024-01-30T23:54:43Z,http://arxiv.org/abs/2401.17509v1
A Disruptive Research Playbook for Studying Disruptive Innovations,"As researchers, we are now witnessing a fundamental change in our
technologically-enabled world due to the advent and diffusion of highly
disruptive technologies such as generative AI, Augmented Reality (AR) and
Virtual Reality (VR). In particular, software engineering has been profoundly
affected by the transformative power of disruptive innovations for decades,
with a significant impact of technical advancements on social dynamics due to
its the socio-technical nature. In this paper, we reflect on the importance of
formulating and addressing research in software engineering through a
socio-technical lens, thus ensuring a holistic understanding of the complex
phenomena in this field. We propose a research playbook with the goal of
providing a guide to formulate compelling and socially relevant research
questions and to identify the appropriate research strategies for empirical
investigations, with an eye on the long-term implications of technologies or
their use. We showcase how to apply the research playbook. Firstly, we show how
it can be used retrospectively to reflect on a prior disruptive technology,
Stack Overflow, and its impact on software development. Secondly, we show it
can be used to question the impact of two current disruptive technologies: AI
and AR/VR. Finally, we introduce a specialized GPT model to support the
researcher in framing future investigations. We conclude by discussing the
broader implications of adopting the playbook for both researchers and
practitioners in software engineering and beyond.","['Margaret-Anne Storey', 'Daniel Russo', 'Nicole Novielli', 'Takashi Kobayashi', 'Dong Wang']",2024-02-20T19:13:36Z,http://arxiv.org/abs/2402.13329v1
"Polarization-Encoded Lenticular Nano-Printing with Single-Layer
  Metasurfaces","Metasurface-based nano-printing has enabled ultrahigh-resolution grayscale or
color image display. However, the maximum number of independent nano-printing
images allowed by one single-layer metasurface is still limited despite many
multiplexing methods that have been proposed to increase the design degree of
freedom. In this work, we substantially push the multiplexing limit of
nano-printing by transforming images at different observation angles into
mapping the corresponding images to different positions in the Fourier space,
and simultaneously controlling the complex electric field across multiple
polarization channels. Our proposed Polarization-Encoded Lenticular
Nano-Printing (Pollen), aided by a modified evolutionary algorithm, allows the
display of several images based on the viewing angle, similar to traditional
lenticular printing but without requiring a lenticular layer. In addition, it
extends the display capability to encompass multiple polarization states.
Empowered by the ability to control the complex amplitude of three polarization
channels, we numerically and experimentally demonstrate the generation of 13
distinguished gray-scale Chinese ink wash painting images, 49 binary patterns,
and three sets of 3D nano-printing images, totaling 25 unique visuals. These
results present the largest number of recorded images with ultra-high
resolution to date. Our innovative Pollen technique is expected to benefit the
development of modern optical applications, including but not limited to
optical encryption, optical data storage, lightweight display, and augmented
reality and virtual reality.","['Lin Deng', 'Ziqiang Cai', 'Yongmin Liu']",2024-03-05T03:21:21Z,http://arxiv.org/abs/2403.02620v1
"Optimizing Service Placement in Edge-to-Cloud AR/VR Systems using a
  Multi-Objective Genetic Algorithm","Augmented Reality (AR) and Virtual Reality (VR) systems involve
computationally intensive image processing algorithms that can burden
end-devices with limited resources, leading to poor performance in providing
low latency services. Edge-to-cloud computing overcomes the limitations of
end-devices by offloading their computations to nearby edge devices or remote
cloud servers. Although this proves to be sufficient for many applications,
optimal placement of latency sensitive AR/VR services in edge-to-cloud
infrastructures (to provide desirable service response times and reliability)
remain a formidable challenging. To address this challenge, this paper develops
a Multi-Objective Genetic Algorithm (MOGA) to optimize the placement of
AR/VR-based services in multi-tier edge-to-cloud environments. The primary
objective of the proposed MOGA is to minimize the response time of all running
services, while maximizing the reliability of the underlying system from both
software and hardware perspectives. To evaluate its performance, we
mathematically modeled all components and developed a tailor-made simulator to
assess its effectiveness on various scales. MOGA was compared with several
heuristics to prove that intuitive solutions, which are usually assumed
sufficient, are not efficient enough for the stated problem. The experimental
results indicated that MOGA can significantly reduce the response time of
deployed services by an average of 67\% on different scales, compared to other
heuristic methods. MOGA also ensures reliability of the 97\% infrastructure
(hardware) and 95\% services (software).","['Mohammadsadeq Garshasbi Herabad', 'Javid Taheri', 'Bestoun S. Ahmed', 'Calin Curescu']",2024-03-19T15:54:56Z,http://arxiv.org/abs/2403.12849v1
"AAM-VDT: Vehicle Digital Twin for Tele-Operations in Advanced Air
  Mobility","This study advanced tele-operations in Advanced Air Mobility (AAM) through
the creation of a Vehicle Digital Twin (VDT) system for eVTOL aircraft,
tailored to enhance remote control safety and efficiency, especially for Beyond
Visual Line of Sight (BVLOS) operations. By synergizing digital twin technology
with immersive Virtual Reality (VR) interfaces, we notably elevate situational
awareness and control precision for remote operators. Our VDT framework
integrates immersive tele-operation with a high-fidelity aerodynamic database,
essential for authentically simulating flight dynamics and control tactics. At
the heart of our methodology lies an eVTOL's high-fidelity digital replica,
placed within a simulated reality that accurately reflects physical laws,
enabling operators to manage the aircraft via a master-slave dynamic,
substantially outperforming traditional 2D interfaces. The architecture of the
designed system ensures seamless interaction between the operator, the digital
twin, and the actual aircraft, facilitating exact, instantaneous feedback.
Experimental assessments, involving propulsion data gathering, simulation
database fidelity verification, and tele-operation testing, verify the system's
capability in precise control command transmission and maintaining the
digital-physical eVTOL synchronization. Our findings underscore the VDT
system's potential in augmenting AAM efficiency and safety, paving the way for
broader digital twin application in autonomous aerial vehicles.","['Tuan Anh Nguyen', 'Taeho Kwag', 'Vinh Pham', 'Viet Nghia Nguyen', 'Jeongseok Hyun', 'Minseok Jang', 'Jae-Woo Lee']",2024-04-15T09:49:17Z,http://arxiv.org/abs/2404.09621v1
"Divide-Conquer-and-Merge: Memory- and Time-Efficient Holographic
  Displays","Recently, deep learning-based computer-generated holography (CGH) has
demonstrated tremendous potential in three-dimensional (3D) displays and
yielded impressive display quality. However, most existing deep learning-based
CGH techniques can only generate holograms of 1080p resolution, which is far
from the ultra-high resolution (16K+) required for practical virtual reality
(VR) and augmented reality (AR) applications to support a wide field of view
and large eye box. One of the major obstacles in current CGH frameworks lies in
the limited memory available on consumer-grade GPUs which could not facilitate
the generation of higher-definition holograms. To overcome the aforementioned
challenge, we proposed a divide-conquer-and-merge strategy to address the
memory and computational capacity scarcity in ultra-high-definition CGH
generation. This algorithm empowers existing CGH frameworks to synthesize
higher-definition holograms at a faster speed while maintaining high-fidelity
image display quality. Both simulations and experiments were conducted to
demonstrate the capabilities of the proposed framework. By integrating our
strategy into HoloNet and CCNNs, we achieved significant reductions in GPU
memory usage during the training period by 64.3\% and 12.9\%, respectively.
Furthermore, we observed substantial speed improvements in hologram generation,
with an acceleration of up to 3$\times$ and 2 $\times$, respectively.
Particularly, we successfully trained and inferred 8K definition holograms on
an NVIDIA GeForce RTX 3090 GPU for the first time in simulations. Furthermore,
we conducted full-color optical experiments to verify the effectiveness of our
method. We believe our strategy can provide a novel approach for memory- and
time-efficient holographic displays.","['Zhenxing Dong', 'Jidong Jia', 'Yan Li', 'Yuye Ling']",2024-02-25T13:58:03Z,http://arxiv.org/abs/2404.10777v1
Motor Focus: Ego-Motion Prediction with All-Pixel Matching,"Motion analysis plays a critical role in various applications, from virtual
reality and augmented reality to assistive visual navigation. Traditional
self-driving technologies, while advanced, typically do not translate directly
to pedestrian applications due to their reliance on extensive sensor arrays and
non-feasible computational frameworks. This highlights a significant gap in
applying these solutions to human users since human navigation introduces
unique challenges, including the unpredictable nature of human movement,
limited processing capabilities of portable devices, and the need for
directional responsiveness due to the limited perception range of humans. In
this project, we introduce an image-only method that applies motion analysis
using optical flow with ego-motion compensation to predict Motor Focus-where
and how humans or machines focus their movement intentions. Meanwhile, this
paper addresses the camera shaking issue in handheld and body-mounted devices
which can severely degrade performance and accuracy, by applying a Gaussian
aggregation to stabilize the predicted motor focus area and enhance the
prediction accuracy of movement direction. This also provides a robust,
real-time solution that adapts to the user's immediate environment.
Furthermore, in the experiments part, we show the qualitative analysis of motor
focus estimation between the conventional dense optical flow-based method and
the proposed method. In quantitative tests, we show the performance of the
proposed method on a collected small dataset that is specialized for motor
focus estimation tasks.","['Hao Wang', 'Jiayou Qin', 'Xiwen Chen', 'Ashish Bastola', 'John Suchanek', 'Zihao Gong', 'Abolfazl Razi']",2024-04-25T20:45:39Z,http://arxiv.org/abs/2404.17031v1
"Sports Analysis and VR Viewing System Based on Player Tracking and Pose
  Estimation with Multimodal and Multiview Sensors","Sports analysis and viewing play a pivotal role in the current sports domain,
offering significant value not only to coaches and athletes but also to fans
and the media. In recent years, the rapid development of virtual reality (VR)
and augmented reality (AR) technologies have introduced a new platform for
watching games. Visualization of sports competitions in VR/AR represents a
revolutionary technology, providing audiences with a novel immersive viewing
experience. However, there is still a lack of related research in this area. In
this work, we present for the first time a comprehensive system for sports
competition analysis and real-time visualization on VR/AR platforms. First, we
utilize multiview LiDARs and cameras to collect multimodal game data.
Subsequently, we propose a framework for multi-player tracking and pose
estimation based on a limited amount of supervised data, which extracts precise
player positions and movements from point clouds and images. Moreover, we
perform avatar modeling of players to obtain their 3D models. Ultimately, using
these 3D player data, we conduct competition analysis and real-time
visualization on VR/AR. Extensive quantitative experiments demonstrate the
accuracy and robustness of our multi-player tracking and pose estimation
framework. The visualization results showcase the immense potential of our
sports visualization system on the domain of watching games on VR/AR devices.
The multimodal competition dataset we collected and all related code will be
released soon.","['Wenxuan Guo', 'Zhiyu Pan', 'Ziheng Xi', 'Alapati Tuerxun', 'Jianjiang Feng', 'Jie Zhou']",2024-05-02T09:19:43Z,http://arxiv.org/abs/2405.01112v1
3D Hand Mesh Recovery from Monocular RGB in Camera Space,"With the rapid advancement of technologies such as virtual reality, augmented
reality, and gesture control, users expect interactions with computer
interfaces to be more natural and intuitive. Existing visual algorithms often
struggle to accomplish advanced human-computer interaction tasks, necessitating
accurate and reliable absolute spatial prediction methods. Moreover, dealing
with complex scenes and occlusions in monocular images poses entirely new
challenges. This study proposes a network model that performs parallel
processing of root-relative grids and root recovery tasks. The model enables
the recovery of 3D hand meshes in camera space from monocular RGB images. To
facilitate end-to-end training, we utilize an implicit learning approach for 2D
heatmaps, enhancing the compatibility of 2D cues across different subtasks.
Incorporate the Inception concept into spectral graph convolutional network to
explore relative mesh of root, and integrate it with the locally detailed and
globally attentive method designed for root recovery exploration. This approach
improves the model's predictive performance in complex environments and
self-occluded scenes. Through evaluation on the large-scale hand dataset
FreiHAND, we have demonstrated that our proposed model is comparable with
state-of-the-art models. This study contributes to the advancement of
techniques for accurate and reliable absolute spatial prediction in various
human-computer interaction applications.","['Haonan Li', 'Patrick P. K. Chen', 'Yitong Zhou']",2024-05-12T05:36:37Z,http://arxiv.org/abs/2405.07167v1
"Improving the Real-Data Driven Network Evaluation Model for Digital Twin
  Networks","With the emergence and proliferation of new forms of large-scale services
such as smart homes, virtual reality/augmented reality, the increasingly
complex networks are raising concerns about significant operational costs. As a
result, the need for network management automation is emphasized, and Digital
Twin Networks (DTN) technology is expected to become the foundation technology
for autonomous networks. DTN has the advantage of being able to operate and
system networks based on real-time collected data in a closed-loop system, and
currently it is mainly designed for optimization scenarios. To improve network
performance in optimization scenarios, it is necessary to select appropriate
configurations and perform accurate performance evaluation based on real data.
However, most network evaluation models currently use simulation data.
Meanwhile, according to DTN standards documents, artificial intelligence (AI)
models can ensure scalability, real-time performance, and accuracy in
large-scale networks. Various AI research and standardization work is ongoing
to optimize the use of DTN. When designing AI models, it is crucial to consider
the characteristics of the data. This paper presents an autoencoder-based skip
connected message passing neural network (AE-SMPN) as a network evaluation
model using real network data. The model is created by utilizing graph neural
network (GNN) with recurrent neural network (RNN) models to capture the
spatiotemporal features of network data. Additionally, an AutoEncoder (AE) is
employed to extract initial features. The neural network was trained using the
real DTN dataset provided by the Barcelona Neural Networking Center (BNN-UPC),
and the paper presents the analysis of the model structure along with
experimental results.","['Hyeju Shin', 'Ibrahim Aliyu', 'Abubakar Isah', 'Jinsul Kim']",2024-05-14T09:55:03Z,http://arxiv.org/abs/2405.08473v1
"Assessing 3D scan quality in Virtual Reality through paired-comparisons
  psychophysics test","Consumer 3D scanners and depth cameras are increasingly being used to
generate content and avatars for Virtual Reality (VR) environments and avoid
the inconveniences of hand modeling; however, it is sometimes difficult to
evaluate quantitatively the mesh quality at which 3D scans should be exported,
and whether the object perception might be affected by its shading. We propose
using a paired-comparisons test based on psychophysics of perception to do that
evaluation. As psychophysics is not subject to opinion, skill level, mental
state, or economic situation it can be considered a quantitative way to measure
how people perceive the mesh quality. In particular, we propose using the
psychophysical measure for the comparison of four different levels of mesh
quality (1K, 5K, 10K and 20K triangles). We present two studies within
subjects: in one we investigate the quality perception variations of seeing an
object in a regular screen monitor against an stereoscopic Head Mounted Display
(HMD); while in the second experiment we aim at detecting the effects of
shading into quality perception. At each iteration of the pair-test comparisons
participants pick the mesh that they think had higher quality; by the end of
the experiment we compile a preference matrix. The matrix evidences the
correlation between real quality and assessed quality. Regarding the shading
mode, we find an interaction with quality and shading when the model has high
definition. Furthermore, we assess the subjective realism of the most/least
preferred scans using an Immersive Augmented Reality (IAR) video-see-through
setup. Results show higher levels of realism were perceived through the HMD
than when using a monitor, although the quality was similarly perceived in both
systems.","['Jacob Thorn', 'Rodrigo Pizarro', 'Bernhard Spanlang', 'Pablo Bermell-Garcia', 'Mar Gonzalez-Franco']",2016-01-31T12:43:34Z,http://arxiv.org/abs/1602.00238v2
"The G332 molecular cloud ring: I. Morphology and physical
  characteristics","We present a morphological and physical analysis of a Giant Molecular Cloud
(GMC) using the carbon monoxide isotopologues ($^{12}$CO, $^{13}$CO, C$^{18}$O
$^{3}P_{2}\rightarrow$ $^{3}P_{1}$) survey of the Galactic Plane (Mopra CO
Southern Galactic Plane Survey), supplemented with neutral carbon maps from the
HEAT telescope in Antarctica. The giant molecular cloud structure (hereinafter
the ring) covers the sky region $332^\circ$ < $\ell$ < $333^\circ$ and
$\mathit{b}$ = $\pm 0.5^\circ$ (hereinafter the G332 region). The mass of the
ring and its distance are determined to be respectively
~2$\times10^{5}\mathrm{M_{\odot}}$ and ~3.7 kpc from Sun. The dark molecular
gas fraction, estimated from the $^{13}$CO and [CI] lines, is $\sim17\%$ for a
CO T$_{\mathrm{ex}}$ between [10,20 K]. Comparing the [CI] integrated intensity
and N(H$_{2}$) traced by $^{13}$CO and $^{12}$CO, we define an
X$\mathrm{_{CI}^{809}}$ factor, analogous to the usual X$_{\mathrm{co}}$,
through the [CI] line. X$\mathrm{_{CI}^{809}}$ ranges between
[1.8,2.0]$\times10^{21}\mathrm{cm}^{-2}\mathrm{K}^{-1}\mathrm{km}^{-1}\mathrm{s}$.
We examined local variation in X$_{\mathrm{co}}$ and T$_{\mathrm{ex}}$ across
the cloud, and find in regions where the star formation activity is not in an
advanced state, an increase in the mean and dispersion of the X$_{\mathrm{co}}$
factor as the excitation temperature decreases. We present a catalogue of
C$^{18}$O clumps within the cloud. The star formation (SF) activity ongoing in
the cloud shows a correlation with T$_{\mathrm{ex}}$, [CI] and CO emissions,
and anti-correlation with X$_{\mathrm{co}}$, suggesting a North-South spatial
gradient in the SF activity. We propose a method to disentangle dust emission
across the Galaxy, using HI and $^{13}$CO data. We describe Virtual Reality
(VR) and Augmented Reality (AR) data visualisation techniques for the analysis
of radio astronomy data.","['Domenico Romano', 'Michael G. Burton', 'Michael C. B. Ashley', 'Sergio Molinari', 'David Rebolledo', 'Catherine Braiding', 'Eugenio Schisano']",2019-01-17T16:03:42Z,http://arxiv.org/abs/1901.05961v1
"An In-Depth Exploration of the Effect of 2D/3D Views and Controller
  Types on First Person Shooter Games in Virtual Reality","The amount of interest in Virtual Reality (VR) research has significantly
increased over the past few years, both in academia and industry. The release
of commercial VR Head-Mounted Displays (HMDs) has been a major contributing
factor. However, there is still much to be learned, especially how views and
input techniques, as well as their interaction, affect the VR experience. There
is little work done on First-Person Shooter (FPS) games in VR, and those few
studies have focused on a single aspect of VR FPS. They either focused on the
view, e.g., comparing VR to a typical 2D display or on the controller types. To
the best of our knowledge, there are no studies investigating variations of
2D/3D views in HMDs, controller types, and their interactions. As such, it is
challenging to distinguish findings related to the controller type from those
related to the view. If a study does not control for the input method and finds
that 2D displays lead to higher performance than VR, we cannot generalize the
results because of the confounding variables. To understand their interaction,
we propose to analyze in more depth, whether it is the view (2D vs. 3D) or the
way it is controlled that gives the platforms their respective advantages. To
study the effects of the 2D/3D views, we created a 2D visual technique,
PlaneFrame, that was applied inside the VR headset. Our results show that the
controller type can have a significant positive impact on performance,
immersion, and simulator sickness when associated with a 2D view. They further
our understanding of the interactions that controllers and views have and
demonstrate that comparisons are highly dependent on how both factors go
together. Further, through a series of three experiments, we developed a
technique that can lead to a substantial performance, a good level of
immersion, and can minimize the level of simulator sickness.","['Diego Monteiro', 'Hai-Ning Liang', 'Jialin Wang', 'Hao Chen', 'Nilufar Baghaei']",2020-10-07T08:17:07Z,http://arxiv.org/abs/2010.03256v1
"All One Needs to Know about Metaverse: A Complete Survey on
  Technological Singularity, Virtual Ecosystem, and Research Agenda","Since the popularisation of the Internet in the 1990s, the cyberspace has
kept evolving. We have created various computer-mediated virtual environments
including social networks, video conferencing, virtual 3D worlds (e.g., VR
Chat), augmented reality applications (e.g., Pokemon Go), and Non-Fungible
Token Games (e.g., Upland). Such virtual environments, albeit non-perpetual and
unconnected, have bought us various degrees of digital transformation. The term
`metaverse' has been coined to further facilitate the digital transformation in
every aspect of our physical lives. At the core of the metaverse stands the
vision of an immersive Internet as a gigantic, unified, persistent, and shared
realm. While the metaverse may seem futuristic, catalysed by emerging
technologies such as Extended Reality, 5G, and Artificial Intelligence, the
digital `big bang' of our cyberspace is not far away. This survey paper
presents the first effort to offer a comprehensive framework that examines the
latest metaverse development under the dimensions of state-of-the-art
technologies and metaverse ecosystems, and illustrates the possibility of the
digital `big bang'. First, technologies are the enablers that drive the
transition from the current Internet to the metaverse. We thus examine eight
enabling technologies rigorously - Extended Reality, User Interactivity
(Human-Computer Interaction), Artificial Intelligence, Blockchain, Computer
Vision, IoT and Robotics, Edge and Cloud computing, and Future Mobile Networks.
In terms of applications, the metaverse ecosystem allows human users to live
and play within a self-sustaining, persistent, and shared realm. Therefore, we
discuss six user-centric factors -- Avatar, Content Creation, Virtual Economy,
Social Acceptability, Security and Privacy, and Trust and Accountability.
Finally, we propose a concrete research agenda for the development of the
metaverse.","['Lik-Hang Lee', 'Tristan Braud', 'Pengyuan Zhou', 'Lin Wang', 'Dianlei Xu', 'Zijun Lin', 'Abhishek Kumar', 'Carlos Bermejo', 'Pan Hui']",2021-10-06T07:44:52Z,http://arxiv.org/abs/2110.05352v3
"Action-Specific Perception & Performance on a Fitts's Law Task in
  Virtual Reality: The Role of Haptic Feedback","While user's perception & performance are predominantly examined
independently in virtual reality, the Action-Specific Perception (ASP) theory
postulates that the performance of an individual on a task modulates this
individual's spatial & time perception pertinent to the task's components &
procedures. This paper examines the association between performance &
perception & the potential effects that tactile feedback modalities could
generate. This paper reports a user study (N=24), in which participants
performed a Fitts's law target acquisition task by using three feedback
modalities: visual, visuo-electrotactile, & visuo-vibrotactile. The users
completed 3 Target Sizes X 2 Distances X 3 feedback modalities = 18 trials. The
size perception, distance perception, & (movement) time perception were
assessed at the end of each trial. Performance-wise, the results showed that
electrotactile feedback facilitates a significantly better accuracy compared to
vibrotactile & visual feedback, while vibrotactile provided the worst accuracy.
Electrotactile & visual feedback enabled a comparable reaction time, while the
vibrotactile offered a substantially slower reaction time than visual feedback.
Although amongst feedback types the pattern of differences in perceptual
aspects were comparable to performance differences, none of them was
statistically significant. However, performance indeed modulated perception.
Significant action-specific effects on spatial & time perception were detected.
Changes in accuracy modulate both size perception & time perception, while
changes in movement speed modulate distance perception. Also, the index of
difficulty was found to modulate perception. These outcomes highlighted the
importance of haptic feedback on performance, & importantly the significance of
action-specific effects on spatial & time perception in VR, which should be
considered in future VR studies.","['Panagiotis Kourtesis', 'Sebastian Vizcay', 'Maud Marchal', 'Claudio Pacchierotti', 'Ferran Argelaguet']",2022-07-15T11:07:15Z,http://arxiv.org/abs/2207.07400v2
"AI and 6G into the Metaverse: Fundamentals, Challenges and Future
  Research Trends","Since Facebook was renamed Meta, a lot of attention, debate, and exploration
have intensified about what the Metaverse is, how it works, and the possible
ways to exploit it. It is anticipated that Metaverse will be a continuum of
rapidly emerging technologies, usecases, capabilities, and experiences that
will make it up for the next evolution of the Internet. Several researchers
have already surveyed the literature on artificial intelligence (AI) and
wireless communications in realizing the Metaverse. However, due to the rapid
emergence and continuous evolution of technologies, there is a need for a
comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both
in realizing the immersive experiences of Metaverse. Therefore, in this survey,
we first introduce the background and ongoing progress in augmented reality
(AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed
by the technical aspects of AI and 6G. Then, we survey the role of AI in the
Metaverse by reviewing the state-of-the-art in deep learning, computer vision,
and Edge AI to extract the requirements of 6G in Metaverse. Next, we
investigate the promising services of B5G/6G towards Metaverse, followed by
identifying the role of AI in 6G networks and 6G networks for AI in support of
Metaverse applications, and the need for sustainability in Metaverse. Finally,
we enlist the existing and potential applications, usecases, and projects to
highlight the importance of progress in the Metaverse. Moreover, in order to
provide potential research directions to researchers, we underline the
challenges, research gaps, and lessons learned identified from the literature
review of the aforementioned technologies.","['Muhammad Zawish', 'Fayaz Ali Dharejo', 'Sunder Ali Khowaja', 'Kapal Dev', 'Steven Davy', 'Nawab Muhammad Faseeh Qureshi', 'Paolo Bellavista']",2022-08-23T12:48:53Z,http://arxiv.org/abs/2208.10921v2
"Apple Vision Pro for Healthcare: ""The Ultimate Display""? -- Entering the
  Wonderland of Precision Medicine","At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user's eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers' eyes to ""outsiders"" or a button on
the top, called ""Digital Crown"", that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the ""Ultimate
Display"", which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.","['Jan Egger', 'Christina Gsaxner', 'Xiaojun Chen', 'Jiang Bian', 'Jens Kleesiek', 'Behrus Puladi']",2023-08-08T15:01:51Z,http://arxiv.org/abs/2308.04313v4
"Well-being in isolation: Exploring artistic immersive virtual
  environments in a simulated lunar habitat to alleviate asthenia symptoms","Revived interest in lunar and planetary exploration is heralding a new era
for human spaceflight, characterized by frequent strain on astronaut's mental
well-being, which stems from increased exposure to isolated, confined, and
extreme (ICE) conditions. Whilst Immersive Virtual Reality (IVR) has been
employed to facilitate self-help interventions to mitigate challenges caused by
isolated environments in several domains, its applicability in support of
future space expeditions remains largely unexplored. To address this
limitation, we administered the use of distinct IVR environments to crew
members (n=5) partaking in a simulated lunar habitat study. Utilizing a
Bayesian approach to scrutinize small group data, we discovered a significant
relationship between IVR usage and a reduction in perceived stress-related
symptoms, particularly those associated with asthenia (syndrome often linked to
chronic fatigue and weakness; a condition characterized by feelings of energy
depletion or exhaustion that can be amplified in ICE conditions). The
reductions were most prominent with the use of interactive virtual
environments. The 'Aesthetic Realities' - virtual environments conceived as art
exhibits - received exceptional praise from our participants. These
environments mark a fascinating convergence of art and science, holding promise
to mitigate effects related to isolation in spaceflight training and beyond.","['Grzegorz Pochwatko', 'Wieslaw Kopec', 'Justyna Swidrak', 'Anna Jaskulska', 'Kinga H. Skorupska', 'Barbara Karpowicz', 'Rafał Masłyk', 'Maciej Grzeszczuk', 'Steven Barnes', 'Paulina Borkiewicz', 'Paweł Kobyliński', 'Michał Pabiś-Orzeszyna', 'Robert Balas', 'Jagoda Lazarek', 'Florian Dufresne', 'Leonie Bensch', 'Tommy Nilsson']",2023-11-15T20:04:00Z,http://arxiv.org/abs/2311.09343v1
"Navigating the Landscape for Real-time Localisation and Mapping for
  Robotics and Virtual and Augmented Reality","Visual understanding of 3D environments in real-time, at low power, is a huge
computational challenge. Often referred to as SLAM (Simultaneous Localisation
and Mapping), it is central to applications spanning domestic and industrial
robotics, autonomous vehicles, virtual and augmented reality. This paper
describes the results of a major research effort to assemble the algorithms,
architectures, tools, and systems software needed to enable delivery of SLAM,
by supporting applications specialists in selecting and configuring the
appropriate algorithm and the appropriate hardware, and compilation pathway, to
meet their performance, accuracy, and energy consumption goals. The major
contributions we present are (1) tools and methodology for systematic
quantitative evaluation of SLAM algorithms, (2) automated,
machine-learning-guided exploration of the algorithmic and implementation
design space with respect to multiple objectives, (3) end-to-end simulation
tools to enable optimisation of heterogeneous, accelerated architectures for
the specific algorithmic requirements of the various SLAM algorithmic
approaches, and (4) tools for delivering, where appropriate, accelerated,
adaptive SLAM solutions in a managed, JIT-compiled, adaptive runtime context.","['Sajad Saeedi', 'Bruno Bodin', 'Harry Wagstaff', 'Andy Nisbet', 'Luigi Nardi', 'John Mawer', 'Nicolas Melot', 'Oscar Palomar', 'Emanuele Vespa', 'Tom Spink', 'Cosmin Gorgovan', 'Andrew Webb', 'James Clarkson', 'Erik Tomusk', 'Thomas Debrunner', 'Kuba Kaszyk', 'Pablo Gonzalez-de-Aledo', 'Andrey Rodchenko', 'Graham Riley', 'Christos Kotselidis', 'Björn Franke', ""Michael F. P. O'Boyle"", 'Andrew J. Davison', 'Paul H. J. Kelly', 'Mikel Luján', 'Steve Furber']",2018-08-20T09:06:21Z,http://arxiv.org/abs/1808.06352v1
"LE-HGR: A Lightweight and Efficient RGB-based Online Gesture Recognition
  Network for Embedded AR Devices","Online hand gesture recognition (HGR) techniques are essential in augmented
reality (AR) applications for enabling natural human-to-computer interaction
and communication. In recent years, the consumer market for low-cost AR devices
has been rapidly growing, while the technology maturity in this domain is still
limited. Those devices are typical of low prices, limited memory, and
resource-constrained computational units, which makes online HGR a challenging
problem. To tackle this problem, we propose a lightweight and computationally
efficient HGR framework, namely LE-HGR, to enable real-time gesture recognition
on embedded devices with low computing power. We also show that the proposed
method is of high accuracy and robustness, which is able to reach high-end
performance in a variety of complicated interaction environments. To achieve
our goal, we first propose a cascaded multi-task convolutional neural network
(CNN) to simultaneously predict probabilities of hand detection and regress
hand keypoint locations online. We show that, with the proposed cascaded
architecture design, false-positive estimates can be largely eliminated.
Additionally, an associated mapping approach is introduced to track the hand
trace via the predicted locations, which addresses the interference of
multi-handedness. Subsequently, we propose a trace sequence neural network
(TraceSeqNN) to recognize the hand gesture by exploiting the motion features of
the tracked trace. Finally, we provide a variety of experimental results to
show that the proposed framework is able to achieve state-of-the-art accuracy
with significantly reduced computational cost, which are the key properties for
enabling real-time applications in low-cost commercial devices such as mobile
devices and AR/VR headsets.","['Hongwei Xie', 'Jiafang Wang', 'Baitao Shao', 'Jian Gu', 'Mingyang Li']",2020-01-16T05:23:24Z,http://arxiv.org/abs/2001.05654v1
"Microscope 2.0: An Augmented Reality Microscope with Real-time
  Artificial Intelligence Integration","The brightfield microscope is instrumental in the visual examination of both
biological and physical samples at sub-millimeter scales. One key clinical
application has been in cancer histopathology, where the microscopic assessment
of the tissue samples is used for the diagnosis and staging of cancer and thus
guides clinical therapy. However, the interpretation of these samples is
inherently subjective, resulting in significant diagnostic variability.
Moreover, in many regions of the world, access to pathologists is severely
limited due to lack of trained personnel. In this regard, Artificial
Intelligence (AI) based tools promise to improve the access and quality of
healthcare. However, despite significant advances in AI research, integration
of these tools into real-world cancer diagnosis workflows remains challenging
because of the costs of image digitization and difficulties in deploying AI
solutions. Here we propose a cost-effective solution to the integration of AI:
the Augmented Reality Microscope (ARM). The ARM overlays AI-based information
onto the current view of the sample through the optical pathway in real-time,
enabling seamless integration of AI into the regular microscopy workflow. We
demonstrate the utility of ARM in the detection of lymph node metastases in
breast cancer and the identification of prostate cancer with a latency that
supports real-time workflows. We anticipate that ARM will remove barriers
towards the use of AI in microscopic analysis and thus improve the accuracy and
efficiency of cancer diagnosis. This approach is applicable to other microscopy
tasks and AI algorithms in the life sciences and beyond.","['Po-Hsuan Cameron Chen', 'Krishna Gadepalli', 'Robert MacDonald', 'Yun Liu', 'Kunal Nagpal', 'Timo Kohlberger', 'Jeffrey Dean', 'Greg S. Corrado', 'Jason D. Hipp', 'Martin C. Stumpe']",2018-11-21T21:02:50Z,http://arxiv.org/abs/1812.00825v2
"Realizing a Low-Power Head-Mounted Phase-Only Holographic Display by
  Light-Weight Compression","Head-mounted holographic displays (HMHD) are projected to be the first
commercial realization of holographic video display systems. HMHDs use liquid
crystal on silicon (LCoS) spatial light modulators (SLM), which are best suited
to display phase-only holograms (POH). The performance/watt requirement of a
monochrome, 60 fps Full HD, 2-eye, POH HMHD system is about 10 TFLOPS/W, which
is orders of magnitude higher than that is achievable by commercially available
mobile processors. To mitigate this compute power constraint, display-ready
POHs shall be generated on a nearby server and sent to the HMHD in compressed
form over a wireless link. This paper discusses design of a feasible HMHD-based
augmented reality system, focusing on compression requirements and per-pixel
rate-distortion trade-off for transmission of display-ready POH from the server
to HMHD. Since the decoder in the HMHD needs to operate on low power, only
coding methods that have low-power decoder implementation are considered.
Effects of 2D phase unwrapping and flat quantization on compression performance
are also reported. We next propose a versatile PCM-POH codec with progressive
quantization that can adapt to SLM-dynamic-range and available bitrate, and
features per-pixel rate-distortion control to achieve acceptable POH quality at
target rates of 60-200 Mbit/s that can be reliably achieved by current wireless
technologies. Our results demonstrate feasibility of realizing a low-power,
quality-ensured, multi-user, interactive HMHD augmented reality system with
commercially available components using the proposed adaptive compression of
display-ready POH with light-weight decoding.","['Burak Soner', 'Erdem Ulusoy', 'A. Murat Tekalp', 'Hakan Urey']",2020-02-14T09:03:01Z,http://arxiv.org/abs/2002.05922v1
"3D Augmented Reality-Assisted CT-Guided Interventions: System Design and
  Preclinical Trial on an Abdominal Phantom using HoloLens 2","Background: Out-of-plane lesions pose challenges for CT-guided interventions.
Augmented reality (AR) headset devices have evolved and are readily capable to
provide virtual 3D guidance to improve CT-guided targeting.
  Purpose: To describe the design of a three-dimensional (3D) AR-assisted
navigation system using HoloLens 2 and evaluate its performance through
CT-guided simulations.
  Materials and Methods: A prospective trial was performed assessing CT-guided
needle targeting on an abdominal phantom with and without AR guidance. A total
of 8 operators with varying clinical experience were enrolled and performed a
total of 86 needle passes. Procedure efficiency, radiation dose, and
complication rates were compared with and without AR guidance. Vector analysis
of the first needle pass was also performed.
  Results: Average total number of needle passes to reach the target reduced
from 7.4 passes without AR to 3.4 passes with AR (54.2% decrease, p=0.011).
Average dose-length product (DLP) decreased from 538 mGy-cm without AR to 318
mGy-cm with AR (41.0% decrease, p=0.009). Complication rate of hitting a
non-targeted lesion decreased from 11.9% without AR (7/59 needle passes) to 0%
with AR (0/27 needle passes). First needle passes were more nearly aligned with
the ideal target trajectory with AR versus without AR (4.6{\deg} vs 8.0{\deg}
offset, respectively, p=0.018). Medical students, residents, and attendings all
performed at the same level with AR guidance.
  Conclusions: 3D AR guidance can provide significant improvements in
procedural efficiency and radiation dose savings for targeting challenging,
out-of-plane lesions. AR guidance elevated the performance of all operators to
the same level irrespective of prior clinical experience.","['Brian J. Park', 'Stephen J. Hunt', 'Gregory J. Nadolski', 'Terence P. Gade']",2020-05-19T00:22:24Z,http://arxiv.org/abs/2005.09146v1
"On the Ruin of Age of Information in Augmented Reality over Wireless
  Terahertz (THz) Networks","Guaranteeing fresh and reliable information for augmented reality (AR)
services is a key challenge to enable a real-time experience and sustain a high
quality of physical experience (QoPE) for the users. In this paper, a terahertz
(THz) cellular network is used to exchange rate-hungry AR content. For this
network, guaranteeing an instantaneous low peak age of information (PAoI) is
necessary to overcome the uncertainty stemming from the THz channel. In
particular, a novel economic concept, namely, the risk of ruin is proposed to
examine the probability of occurrence of rare, but extremely high PAoI that can
jeopardize the operation of the AR service. To assess the severity of these
hazards, the cumulative distribution function (CDF) of the PAoI is derived for
two different scheduling policies. This CDF is then used to find the
probability of maximum severity of ruin PAoI. Furthermore, to provide long term
insights about the AR content's age, the average PAoI of the overall system is
also derived. Simulation results show that an increase in the number of users
will positively impact the PAoI in both the expected and worst-case scenarios.
Meanwhile, an increase in the bandwidth reduces the average PAoI but leads to a
decline in the severity of ruin performance. The results also show that a
system with preemptive last come first served (LCFS) queues of limited size
buffers have a better ruin performance (12% increase in the probability of
guaranteeing a less severe PAoI while increasing the number of users), whereas
first come first served (FCFS) queues of limited buffers lead to a better
average PAoI performance (45% lower PAoI as we increase the bandwidth).","['Christina Chaccour', 'Walid Saad']",2020-08-23T05:08:05Z,http://arxiv.org/abs/2008.09959v1
Optimal Assistance for Object-Rearrangement Tasks in Augmented Reality,"Augmented-reality (AR) glasses that will have access to onboard sensors and
an ability to display relevant information to the user present an opportunity
to provide user assistance in quotidian tasks. Many such tasks can be
characterized as object-rearrangement tasks. We introduce a novel framework for
computing and displaying AR assistance that consists of (1) associating an
optimal action sequence with the policy of an embodied agent and (2) presenting
this sequence to the user as suggestions in the AR system's heads-up display.
The embodied agent comprises a ""hybrid"" between the AR system and the user,
with the AR system's observation space (i.e., sensors) and the user's action
space (i.e., task-execution actions); its policy is learned by minimizing the
task-completion time. In this initial study, we assume that the AR system's
observations include the environment's map and localization of the objects and
the user. These choices allow us to formalize the problem of computing AR
assistance for any object-rearrangement task as a planning problem,
specifically as a capacitated vehicle-routing problem. Further, we introduce a
novel AR simulator that can enable web-based evaluation of AR-like assistance
and associated at-scale data collection via the Habitat simulator for embodied
artificial intelligence. Finally, we perform a study that evaluates user
response to the proposed form of AR assistance on a specific quotidian
object-rearrangement task, house cleaning, using our proposed AR simulator on
mechanical turk. In particular, we study the effect of the proposed AR
assistance on users' task performance and sense of agency over a range of task
difficulties. Our results indicate that providing users with such assistance
improves their overall performance and while users report a negative impact to
their agency, they may still prefer the proposed assistance to having no
assistance at all.","['Benjamin Newman', 'Kevin Carlberg', 'Ruta Desai']",2020-10-14T18:46:07Z,http://arxiv.org/abs/2010.07358v1
"Confusing Image Quality Assessment: Towards Better Augmented Reality
  Experience","With the development of multimedia technology, Augmented Reality (AR) has
become a promising next-generation mobile platform. The primary value of AR is
to promote the fusion of digital contents and real-world environments, however,
studies on how this fusion will influence the Quality of Experience (QoE) of
these two components are lacking. To achieve better QoE of AR, whose two layers
are influenced by each other, it is important to evaluate its perceptual
quality first. In this paper, we consider AR technology as the superimposition
of virtual scenes and real scenes, and introduce visual confusion as its basic
theory. A more general problem is first proposed, which is evaluating the
perceptual quality of superimposed images, i.e., confusing image quality
assessment. A ConFusing Image Quality Assessment (CFIQA) database is
established, which includes 600 reference images and 300 distorted images
generated by mixing reference images in pairs. Then a subjective quality
perception study and an objective model evaluation experiment are conducted
towards attaining a better understanding of how humans perceive the confusing
images. An objective metric termed CFIQA is also proposed to better evaluate
the confusing image quality. Moreover, an extended ARIQA study is further
conducted based on the CFIQA study. We establish an ARIQA database to better
simulate the real AR application scenarios, which contains 20 AR reference
images, 20 background (BG) reference images, and 560 distorted images generated
from AR and BG references, as well as the correspondingly collected subjective
quality ratings. We also design three types of full-reference (FR) IQA metrics
to study whether we should consider the visual confusion when designing
corresponding IQA algorithms. An ARIQA metric is finally proposed for better
evaluating the perceptual quality of AR images.","['Huiyu Duan', 'Xiongkuo Min', 'Yucheng Zhu', 'Guangtao Zhai', 'Xiaokang Yang', 'Patrick Le Callet']",2022-04-11T07:03:06Z,http://arxiv.org/abs/2204.04900v2
"Deep Learning-based Framework for Automatic Cranial Defect
  Reconstruction and Implant Modeling","The goal of this work is to propose a robust, fast, and fully automatic
method for personalized cranial defect reconstruction and implant modeling.
  We propose a two-step deep learning-based method using a modified U-Net
architecture to perform the defect reconstruction, and a dedicated iterative
procedure to improve the implant geometry, followed by automatic generation of
models ready for 3-D printing. We propose a cross-case augmentation based on
imperfect image registration combining cases from different datasets. We
perform ablation studies regarding different augmentation strategies and
compare them to other state-of-the-art methods.
  We evaluate the method on three datasets introduced during the AutoImplant
2021 challenge, organized jointly with the MICCAI conference. We perform the
quantitative evaluation using the Dice and boundary Dice coefficients, and the
Hausdorff distance. The average Dice coefficient, boundary Dice coefficient,
and the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm
respectively. We perform an additional qualitative evaluation by 3-D printing
and visualization in mixed reality to confirm the implant's usefulness.
  We propose a complete pipeline that enables one to create the cranial implant
model ready for 3-D printing. The described method is a greatly extended
version of the method that scored 1st place in all AutoImplant 2021 challenge
tasks. We freely release the source code, that together with the open datasets,
makes the results fully reproducible. The automatic reconstruction of cranial
defects may enable manufacturing personalized implants in a significantly
shorter time, possibly allowing one to perform the 3-D printing process
directly during a given intervention. Moreover, we show the usability of the
defect reconstruction in mixed reality that may further reduce the surgery
time.","['Marek Wodzinski', 'Mateusz Daniol', 'Miroslaw Socha', 'Daria Hemmerling', 'Maciej Stanuch', 'Andrzej Skalski']",2022-04-13T11:33:26Z,http://arxiv.org/abs/2204.06310v1
"Practical Saccade Prediction for Head-Mounted Displays: Towards a
  Comprehensive Model","Eye-tracking technology is an integral component of new display devices such
as virtual and augmented reality headsets. Applications of gaze information
range from new interaction techniques exploiting eye patterns to
gaze-contingent digital content creation. However, system latency is still a
significant issue in many of these applications because it breaks the
synchronization between the current and measured gaze positions. Consequently,
it may lead to unwanted visual artifacts and degradation of user experience. In
this work, we focus on foveated rendering applications where the quality of an
image is reduced towards the periphery for computational savings. In foveated
rendering, the presence of latency leads to delayed updates to the rendered
frame, making the quality degradation visible to the user. To address this
issue and to combat system latency, recent work proposes to use saccade landing
position prediction to extrapolate the gaze information from delayed
eye-tracking samples. While the benefits of such a strategy have already been
demonstrated, the solutions range from simple and efficient ones, which make
several assumptions about the saccadic eye movements, to more complex and
costly ones, which use machine learning techniques. Yet, it is unclear to what
extent the prediction can benefit from accounting for additional factors. This
paper presents a series of experiments investigating the importance of
different factors for saccades prediction in common virtual and augmented
reality applications. In particular, we investigate the effects of saccade
orientation in 3D space and smooth pursuit eye-motion (SPEM) and how their
influence compares to the variability across users. We also present a simple
yet efficient correction method that adapts the existing saccade prediction
methods to handle these factors without performing extensive data collection.","['Elena Arabadzhiyska', 'Cara Tursun', 'Hans-Peter Seidel', 'Piotr Didyk']",2022-05-03T17:02:30Z,http://arxiv.org/abs/2205.01624v2
"i-MYO: A Hybrid Prosthetic Hand Control System based on Eye-tracking,
  Augmented Reality and Myoelectric signal","Dexterous prosthetic hands have better grasp performance than traditional
ones. However, patients still find it difficult to use these hands without a
suitable control system. A new hybrid myoelectric control system, termed i-MYO,
is presented and evaluated to solve this problem. The core component of the
i-MYO is a novel grasp-type switching interface based on eye-tracking and
augmented reality (AR), termed i-GSI. With the i-GSI, the user can easily
switch a grasp type (six total) for a prosthetic hand by gazing at a
GazeButton. The i-GSI is implemented in an AR helmet and is integrated, as an
individual module, into the i-MYO system. In the i-MYO system, the myoelectric
signal was used to control hand opening /closing proportionally. The operation
of the i-MYO was tested on nine healthy subjects who wore HIT-V hand on the
forearm and manipulated objects in a reach-and-grasp task. It was also tested
on one patient who had an inferior myoelectric signal and was required to
control the HIT-V hand to grasp objects. Results showed that in 91.6% of the
trials, inexperienced healthy subjects accomplished the task within 5.9 s, and
most failed trials were caused by a lack of experience in fine grasping. In
addition, in about 1.5% of trials, the subjects also successfully transferred
the objects but with a non-optimal grasp type. In 97.0% of the trials, the
subjects spent ~1.3 s switching the optimal grasp types. A higher success rate
in grasp type (99.1%) for the untrained patient has been observed thanks to
more trials conducted. In 98.7 % of trials, the patient only needed another 2 s
to control the hand to grasp the object after switching to the optimal grasp
type. The tests demonstrate the control capability of the new system in
multi-DOF prosthetics, and all inexperienced subjects were able to master the
operation of the i-MYO quickly within a few pieces of training and apply it
easily.","['Chunyuan Shi', 'Dapeng Yang', 'Jingdong Zhao', 'Li Jiang']",2022-05-18T14:24:26Z,http://arxiv.org/abs/2205.08948v1
"STTAR: Surgical Tool Tracking using off-the-shelf Augmented Reality
  Head-Mounted Displays","The use of Augmented Reality (AR) for navigation purposes has shown
beneficial in assisting physicians during the performance of surgical
procedures. These applications commonly require knowing the pose of surgical
tools and patients to provide visual information that surgeons can use during
the task performance. Existing medical-grade tracking systems use infrared
cameras placed inside the Operating Room (OR) to identify retro-reflective
markers attached to objects of interest and compute their pose. Some
commercially available AR Head-Mounted Displays (HMDs) use similar cameras for
self-localization, hand tracking, and estimating the objects' depth. This work
presents a framework that uses the built-in cameras of AR HMDs to enable
accurate tracking of retro-reflective markers, such as those used in surgical
procedures, without the need to integrate any additional components. This
framework is also capable of simultaneously tracking multiple tools. Our
results show that the tracking and detection of the markers can be achieved
with an accuracy of 0.09 +- 0.06 mm on lateral translation, 0.42 +- 0.32 mm on
longitudinal translation, and 0.80 +- 0.39 deg for rotations around the
vertical axis. Furthermore, to showcase the relevance of the proposed
framework, we evaluate the system's performance in the context of surgical
procedures. This use case was designed to replicate the scenarios of k-wire
insertions in orthopedic procedures. For evaluation, two surgeons and one
biomedical researcher were provided with visual navigation, each performing 21
injections. Results from this use case provide comparable accuracy to those
reported in the literature for AR-based navigation procedures.","['Alejandro Martin-Gomez', 'Haowei Li', 'Tianyu Song', 'Sheng Yang', 'Guangzhi Wang', 'Hui Ding', 'Nassir Navab', 'Zhe Zhao', 'Mehran Armand']",2022-08-17T00:30:15Z,http://arxiv.org/abs/2208.08880v1
"Categorisation of future applications for Augmented Reality in human
  lunar exploration","The European Space Agency (ESA) has a clear mission to go forward to the Moon
in preparation of human presence on Mars. One of the technologies looked at to
increase safety and efficiency of astronauts in this context is Augmented
Reality (AR). This technology allows digital visual information to be overlaid
onto the user's environment through some type of display or projector. In
recent years separate studies have been conducted to test the potential value
of AR for astronauts by implementing a few functionalities on an AR display
followed by testing in terrestrial analogue environments. One of the groups
contributing to these investigations is Spaceship EAC (SSEAC). SSEAC is a group
of interns and trainees at the European Astronaut Centre (EAC) focusing on
emerging technologies for human space exploration. This paper presents an
outcome of SSEAC's activities related to AR for lunar extravehicular activities
(EVAs), in which an approach similar to design thinking was used to explore,
identify, and structure the opportunities offered by this technology. The
resulting categorization of AR use cases can be used to identify new
functionalities to test through prototyping and usability tests and can also be
used to relate individual studies to each other to gain insight into the
overall potential value AR has to offer to human lunar exploration. The
approach adopted in this paper is based on the Fuzzy Front End (FFE) model from
the innovation management domain. Utilising a user-driven instead of
technology-driven method resulted in findings that are relevant irrespective of
the hardware and software implementation. Instead, the outcome is an overview
of use cases in which some type of AR system could provide value by
contributing to increased astronaut safety, efficiency and/or efficacy.","['Paul Topf Aguiar de Medeiros', 'Paul Njayou', 'Flavie Rometsch', 'Tommy Nilsson', 'Leonie Becker', 'Aidan Cowley']",2022-11-19T16:33:27Z,http://arxiv.org/abs/2301.00838v1
"EVD Surgical Guidance with Retro-Reflective Tool Tracking and Spatial
  Reconstruction using Head-Mounted Augmented Reality Device","Augmented Reality (AR) has been used to facilitate surgical guidance during
External Ventricular Drain (EVD) surgery, reducing the risks of misplacement in
manual operations. During this procedure, the key challenge is accurately
estimating the spatial relationship between pre-operative images and actual
patient anatomy in AR environment. This research proposes a novel framework
utilizing Time of Flight (ToF) depth sensors integrated in commercially
available AR Head Mounted Devices (HMD) for precise EVD surgical guidance. As
previous studies have proven depth errors for ToF sensors, we first assessed
their properties on AR-HMDs. Subsequently, a depth error model and
patient-specific parameter identification method are introduced for accurate
surface information. A tracking pipeline combining retro-reflective markers and
point clouds is then proposed for accurate head tracking. The head surface is
reconstructed using depth data for spatial registration, avoiding fixing
tracking targets rigidly on the patient's skull. Firstly, $7.580\pm 1.488 mm$
depth value error was revealed on human skin, indicating the significance of
depth correction. Our results showed that the error was reduced by over $85\%$
using proposed depth correction method on head phantoms in different materials.
Meanwhile, the head surface reconstructed with corrected depth data achieved
sub-millimetre accuracy. An experiment on sheep head revealed $0.79 mm$
reconstruction error. Furthermore, a user study was conducted for the
performance in simulated EVD surgery, where five surgeons performed nine k-wire
injections on a head phantom with virtual guidance. Results of this study
revealed $2.09 \pm 0.16 mm$ translational accuracy and $2.97\pm 0.91$ degree
orientational accuracy.","['Haowei Li', 'Wenqing Yan', 'Du Liu', 'Long Qian', 'Yuxing Yang', 'Yihao Liu', 'Zhe Zhao', 'Hui Ding', 'Guangzhi Wang']",2023-06-27T14:11:48Z,http://arxiv.org/abs/2306.15490v2
"Automatic registration with continuous pose updates for marker-less
  surgical navigation in spine surgery","Established surgical navigation systems for pedicle screw placement have been
proven to be accurate, but still reveal limitations in registration or surgical
guidance. Registration of preoperative data to the intraoperative anatomy
remains a time-consuming, error-prone task that includes exposure to harmful
radiation. Surgical guidance through conventional displays has well-known
drawbacks, as information cannot be presented in-situ and from the surgeon's
perspective. Consequently, radiation-free and more automatic registration
methods with subsequent surgeon-centric navigation feedback are desirable. In
this work, we present an approach that automatically solves the registration
problem for lumbar spinal fusion surgery in a radiation-free manner. A deep
neural network was trained to segment the lumbar spine and simultaneously
predict its orientation, yielding an initial pose for preoperative models,
which then is refined for each vertebra individually and updated in real-time
with GPU acceleration while handling surgeon occlusions. An intuitive surgical
guidance is provided thanks to the integration into an augmented reality based
navigation system. The registration method was verified on a public dataset
with a mean of 96\% successful registrations, a target registration error of
2.73 mm, a screw trajectory error of 1.79{\deg} and a screw entry point error
of 2.43 mm. Additionally, the whole pipeline was validated in an ex-vivo
surgery, yielding a 100\% screw accuracy and a registration accuracy of 1.20
mm. Our results meet clinical demands and emphasize the potential of RGB-D data
for fully automatic registration approaches in combination with augmented
reality guidance.","['Florentin Liebmann', 'Marco von Atzigen', 'Dominik Stütz', 'Julian Wolf', 'Lukas Zingg', 'Daniel Suter', 'Laura Leoty', 'Hooman Esfandiari', 'Jess G. Snedeker', 'Martin R. Oswald', 'Marc Pollefeys', 'Mazda Farshad', 'Philipp Fürnstahl']",2023-08-05T16:26:41Z,http://arxiv.org/abs/2308.02917v1
"Phase-Specific Augmented Reality Guidance for Microscopic Cataract
  Surgery Using Long-Short Spatiotemporal Aggregation Transformer","Phacoemulsification cataract surgery (PCS) is a routine procedure conducted
using a surgical microscope, heavily reliant on the skill of the
ophthalmologist. While existing PCS guidance systems extract valuable
information from surgical microscopic videos to enhance intraoperative
proficiency, they suffer from non-phasespecific guidance, leading to redundant
visual information. In this study, our major contribution is the development of
a novel phase-specific augmented reality (AR) guidance system, which offers
tailored AR information corresponding to the recognized surgical phase.
Leveraging the inherent quasi-standardized nature of PCS procedures, we propose
a two-stage surgical microscopic video recognition network. In the first stage,
we implement a multi-task learning structure to segment the surgical limbus
region and extract limbus region-focused spatial feature for each frame. In the
second stage, we propose the long-short spatiotemporal aggregation transformer
(LS-SAT) network to model local fine-grained and global temporal relationships,
and combine the extracted spatial features to recognize the current surgical
phase. Additionally, we collaborate closely with ophthalmologists to design AR
visual cues by utilizing techniques such as limbus ellipse fitting and regional
restricted normal cross-correlation rotation computation. We evaluated the
network on publicly available and in-house datasets, with comparison results
demonstrating its superior performance compared to related works. Ablation
results further validated the effectiveness of the limbus region-focused
spatial feature extractor and the combination of temporal features.
Furthermore, the developed system was evaluated in a clinical setup, with
results indicating remarkable accuracy and real-time performance. underscoring
its potential for clinical applications.","['Puxun Tu', 'Hongfei Ye', 'Haochen Shi', 'Jeff Young', 'Meng Xie', 'Peiquan Zhao', 'Ce Zheng', 'Xiaoyi Jiang', 'Xiaojun Chen']",2023-09-11T02:56:56Z,http://arxiv.org/abs/2309.05209v2
"Impact of Augmented reality system on elementary school ESL learners in
  country side of china: Motivations, achievements, behaviors and cognitive
  attainment","The English proficiency of students in rural areas of China tends to be lower
than that of their urban counterparts, owing to outdated teaching methods, a
lack of advanced technology resources, and low motivation for English learning.
This study examines the impact of an Augmented Reality English Words Learning
(AREWL) system on the learning motivation, achievement, behavioral patterns,
and cognitive attainment of elementary school students in rural China. The
study explores whether student motivation varies with their level of
achievement and vice versa, and provides an analysis of behavioral patterns and
cognitive attainment. The AREWL system employs 3D virtual objects, animations,
and assessments to teach English pronunciation and spelling. Instructions are
provided in both English and Chinese for ease of use.
  The sample group consisted of 20 students from grades 1 and 2, selected based
on low pretest scores, along with five non-native teachers. Data were collected
through pretests and posttests, questionnaires, surveys, video recordings, and
in-app evaluations. Quantitative methods were used to analyze test scores and
teacher opinions, while qualitative methods were employed to study student
behavior and its relationship with cognitive attainment.
  Results indicate that both teachers and students responded favorably to the
AREWL system. Students exhibited both intrinsic and extrinsic motivation, which
correlated significantly with their learning achievements. While behavioral
analysis showed interactive engagement with the AREWL system, cognitive
attainment was found to be relatively low. The study concludes that AR-based
learning applications can play an important role in motivating English learning
among young learners in China. The findings contribute to the field of
educational technology by introducing a new AR-based English words learning
application.",['Ijaz Ul Haq'],2023-09-18T15:58:34Z,http://arxiv.org/abs/2309.09894v1
"Teaching Unknown Objects by Leveraging Human Gaze and Augmented Reality
  in Human-Robot Interaction","Robots are becoming increasingly popular in a wide range of environments due
to their exceptional work capacity, precision, efficiency, and scalability.
This development has been further encouraged by advances in Artificial
Intelligence, particularly Machine Learning. By employing sophisticated neural
networks, robots are given the ability to detect and interact with objects in
their vicinity. However, a significant drawback arises from the underlying
dependency on extensive datasets and the availability of substantial amounts of
training data for these object detection models. This issue becomes
particularly problematic when the specific deployment location of the robot and
the surroundings, are not known in advance. The vast and ever-expanding array
of objects makes it virtually impossible to comprehensively cover the entire
spectrum of existing objects using preexisting datasets alone. The goal of this
dissertation was to teach a robot unknown objects in the context of Human-Robot
Interaction (HRI) in order to liberate it from its data dependency, unleashing
it from predefined scenarios. In this context, the combination of eye tracking
and Augmented Reality created a powerful synergy that empowered the human
teacher to communicate with the robot and effortlessly point out objects by
means of human gaze. This holistic approach led to the development of a
multimodal HRI system that enabled the robot to identify and visually segment
the Objects of Interest in 3D space. Through the class information provided by
the human, the robot was able to learn the objects and redetect them at a later
stage. Due to the knowledge gained from this HRI based teaching, the robot's
object detection capabilities exhibited comparable performance to
state-of-the-art object detectors trained on extensive datasets, without being
restricted to predefined classes, showcasing its versatility and adaptability.",['Daniel Weber'],2023-12-12T11:34:43Z,http://arxiv.org/abs/2312.07638v1
"Navigate Biopsy with Ultrasound under Augmented Reality Device: Towards
  Higher System Performance","Purpose: Biopsies play a crucial role in determining the classification and
staging of tumors. Ultrasound is frequently used in this procedure to provide
real-time anatomical information. Using augmented reality (AR), surgeons can
visualize ultrasound data and spatial navigation information seamlessly
integrated with real tissues. This innovation facilitates faster and more
precise biopsy operations. Methods: We developed an AR biopsy navigation system
with low display latency and high accuracy. Ultrasound data is initially read
by an image capture card and streamed to Unity via net communication. In Unity,
navigation information is rendered and transmitted to the HoloLens 2 device
using holographic remoting. Retro-reflective tool tracking is implemented on
the HoloLens 2, enabling simultaneous tracking of the ultrasound probe and
biopsy needle. Distinct navigation information is provided during in-plane and
out-of-plane punctuation. To evaluate the effectiveness of our system, we
conducted a study involving ten participants, for puncture accuracy and biopsy
time, comparing to traditional methods. Results: Our proposed framework enables
ultrasound visualization in AR with only $16.22\pm11.45ms$ additional latency.
Navigation accuracy reached $1.23\pm 0.68mm$ in the image plane and $0.95\pm
0.70mm$ outside the image plane. Remarkably, the utilization of our system led
to $98\%$ and $95\%$ success rate in out-of-plane and in-plane biopsy.
Conclusion: To sum up, this paper introduces an AR-based ultrasound biopsy
navigation system characterized by high navigation accuracy and minimal
latency. The system provides distinct visualization contents during in-plane
and out-of-plane operations according to their different characteristics. Use
case study in this paper proved that our system can help young surgeons perform
biopsy faster and more accurately.","['Haowei Li', 'Wenqing Yan', 'Jiasheng Zhao', 'Yuqi Ji', 'Long Qian', 'Hui Ding', 'Zhe Zhao', 'Guangzhi Wang']",2024-02-04T09:18:43Z,http://arxiv.org/abs/2402.02414v1
"An Intelligent Assistive System Based on Augmented Reality and Internet
  of Things for Patients with Alzheimer's Disease","Independent life of the individuals suffering from Alzheimer's disease (AD)
is compromised due to their memory loss. As a result, they depend on others to
help them lead their daily life. In this situation, either the family members
or the caregivers offer their help; they attach notes on every single object or
take out the contents of a drawer to make those visible when they leave the
patient alone. The aim of this thesis is to provide multi-level support and
some helping means for AD patients and their family members through the
integration of existing science and methods. This study reports results on an
intelligent assistive (IA) system, achieved through the integration of Internet
of Things (IoT), augmented reality (AR), and adaptive fuzzy decision-making
methods. The proposed system has four main components; (1) a location and
heading data stored in the local fog layer, (2) an AR device to make
interactions with the AD patient, (3) a supervisory decision-maker to handle
the direct and environmental interactions with the patient, (4) and a user
interface for family or caregivers to monitor the patient's real-time situation
and send reminders once required. The system operates in different modes,
including automated and semi-automated. The first one helps the user complete
the activities in their daily life by showing AR messages or making automatic
changes. The second one allows manual changes after the real-time assessment of
the user's cognitive state based on the AR game score. We provide further
evidence that the accuracy, reliability and response time of the IA system are
appropriate to be implemented in AD patients' homes. Moreover, the system
response in the semi-automated mode causes less data loss than the automated
mode, as the number of active devices decreases.",['Fatemeh Ghorbani'],2024-02-21T15:29:20Z,http://arxiv.org/abs/2403.05569v1
"Leveraging Artificial Intelligence to Promote Awareness in Augmented
  Reality Systems","Recent developments in artificial intelligence (AI) have permeated through an
array of different immersive environments, including virtual, augmented, and
mixed realities. AI brings a wealth of potential that centers on its ability to
critically analyze environments, identify relevant artifacts to a goal or
action, and then autonomously execute decision-making strategies to optimize
the reward-to-risk ratio. However, the inherent benefits of AI are not without
disadvantages as the autonomy and communication methodology can interfere with
the human's awareness of their environment. More specifically in the case of
autonomy, the relevant human-computer interaction literature cites that high
autonomy results in an ""out-of-the-loop"" experience for the human such that
they are not aware of critical artifacts or situational changes that require
their attention. At the same time, low autonomy of an AI system can limit the
human's own autonomy with repeated requests to approve its decisions. In these
circumstances, humans enter into supervisor roles, which tend to increase their
workload and, therefore, decrease their awareness in a multitude of ways. In
this position statement, we call for the development of human-centered AI in
immersive environments to sustain and promote awareness. It is our position
then that we believe with the inherent risk presented in both AI and AR/VR
systems, we need to examine the interaction between them when we integrate the
two to create a new system for any unforeseen risks, and that it is crucial to
do so because of its practical application in many high-risk environments.","['Wangfan Li', 'Rohit Mallick', 'Carlos Toxtli-Hernandez', 'Christopher Flathmann', 'Nathan J. McNeese']",2024-04-23T17:47:51Z,http://arxiv.org/abs/2405.05916v1
"ConnectiCity, augmented perception of the city","As we move through cities in our daily lives, we are in a constant state of
transformation of the spaces around us. The form and essence of urban space
directly affects people's behavior, describing in their perception what is
possible or impossible, allowed or prohibited, suggested or advised against. We
are now able to fill and stratify space/time with digital information layers,
completely wrapping cities in a membrane of information and of opportunities
for interaction and communication. Mobile devices, smartphones, wearables,
digital tags, near field communication devices, location based services and
mixed/augmented reality have gone much further in this direction, turning the
world into an essentially read/write, ubiquitous publishing surface. The usage
of mobile devices and ubiquitous technologies alters the understanding of
place. In this process, the definition of (urban) landscape powerfully shifts
from a definition which is purely administrative (e.g.: the borders of the
flower bed in the middle of a roundabout) to one that is multiplied according
to all individuals which experience that location; as a lossless sum of their
perceptions; as a stratification of interpretations and activities which forms
our cognition of space and time. In our research we investigated the
possibilities to use the scenario which sees urban spaces progressively filling
with multiple layers of real-time, ubiquitous, digital information to
conceptualize, design and implement a series of usage scenarios. It is possible
to create multiple layers of narratives which traverse the city and which allow
us to read them in different ways, according to the different strategies and
methodologies enabling us to highlight how cities express points of view on the
environment, culture, economy, transports, energy and politics.","['Salvatore Iaconesi', 'Oriana Persico']",2012-07-18T08:02:33Z,http://arxiv.org/abs/1207.4291v1
"Adapting a Formal Model Theory to Applications in Augmented Personalized
  Medicine","The goal of this paper is to advance an extensible theory of living systems
using an approach to biomathematics and biocomputation that suitably addresses
self-organized, self-referential and anticipatory systems with multi-temporal
multi-agents. Our first step is to provide foundations for modelling of
emergent and evolving dynamic multi-level organic complexes and their
sustentative processes in artificial and natural life systems. Main
applications are in life sciences, medicine, ecology and astrobiology, as well
as robotics, industrial automation and man-machine interface. Since 2011 over
100 scientists from a number of disciplines have been exploring a substantial
set of theoretical frameworks for a comprehensive theory of life known as
Integral Biomathics. That effort identified the need for a robust core model of
organisms as dynamic wholes, using advanced and adequately computable
mathematics. The work described here for that core combines the advantages of a
situation and context aware multivalent computational logic for active
self-organizing networks, Wandering Logic Intelligence (WLI), and a multi-scale
dynamic category theory, Memory Evolutive Systems (MES), hence WLIMES. This is
presented to the modeller via a formal augmented reality language as a first
step towards practical modelling and simulation of multi-level living systems.
Initial work focuses on the design and implementation of this visual language
and calculus (VLC) and its graphical user interface. The results will be
integrated within the current methodology and practices of theoretical biology
and (personalized) medicine to deepen and to enhance the holistic understanding
of life.","['Plamen L. Simeonov', 'Andrée C. Ehresmann']",2017-10-01T20:10:11Z,http://arxiv.org/abs/1710.03571v4
SPP-Net: Deep Absolute Pose Regression with Synthetic Views,"Image based localization is one of the important problems in computer vision
due to its wide applicability in robotics, augmented reality, and autonomous
systems. There is a rich set of methods described in the literature how to
geometrically register a 2D image w.r.t.\ a 3D model. Recently, methods based
on deep (and convolutional) feedforward networks (CNNs) became popular for pose
regression. However, these CNN-based methods are still less accurate than
geometry based methods despite being fast and memory efficient. In this work we
design a deep neural network architecture based on sparse feature descriptors
to estimate the absolute pose of an image. Our choice of using sparse feature
descriptors has two major advantages: first, our network is significantly
smaller than the CNNs proposed in the literature for this task---thereby making
our approach more efficient and scalable. Second---and more importantly---,
usage of sparse features allows to augment the training data with synthetic
viewpoints, which leads to substantial improvements in the generalization
performance to unseen poses. Thus, our proposed method aims to combine the best
of the two worlds---feature-based localization and CNN-based pose
regression--to achieve state-of-the-art performance in the absolute pose
estimation. A detailed analysis of the proposed architecture and a rigorous
evaluation on the existing datasets are provided to support our method.","['Pulak Purkait', 'Cheng Zhao', 'Christopher Zach']",2017-12-09T23:45:03Z,http://arxiv.org/abs/1712.03452v1
"Reflective-AR Display: An Interaction Methodology for Virtual-Real
  Alignment in Medical Robotics","Robot-assisted minimally invasive surgery has shown to improve patient
outcomes, as well as reduce complications and recovery time for several
clinical applications. While increasingly configurable robotic arms can
maximize reach and avoid collisions in cluttered environments, positioning them
appropriately during surgery is complicated because safety regulations prevent
automatic driving. We propose a head-mounted display (HMD) based augmented
reality (AR) system designed to guide optimal surgical arm set up. The staff
equipped with HMD aligns the robot with its planned virtual counterpart. In
this user-centric setting, the main challenge is the perspective ambiguities
hindering such collaborative robotic solution. To overcome this challenge, we
introduce a novel registration concept for intuitive alignment of AR content to
its physical counterpart by providing a multi-view AR experience via
reflective-AR displays that simultaneously show the augmentations from multiple
viewpoints. Using this system, users can visualize different perspectives while
actively adjusting the pose to determine the registration transformation that
most closely superimposes the virtual onto the real. The experimental results
demonstrate improvement in the interactive alignment of a virtual and real
robot when using a reflective-AR display. We also present measurements from
configuring a robotic manipulator in a simulated trocar placement surgery using
the AR guidance methodology.","['Javad Fotouhi', 'Tianyu Song', 'Arian Mehrfard', 'Giacomo Taylor', 'Qiaochu Wang', 'Fengfang Xian', 'Alejandro Martin-Gomez', 'Bernhard Fuerst', 'Mehran Armand', 'Mathias Unberath', 'Nassir Navab']",2019-07-23T21:27:44Z,http://arxiv.org/abs/1907.10138v2
"Smart Context-aware Rejuvenation of Engagement on Urban Ambient
  Augmented Things","The concern over global urbanization trend imposes smart-city as enabling
information and communication technology (ICT) to improve urban governance.
However, the light trance on better living space is stimulated by socioeconomic
impact of escalated senior generation. Hence, ambient assisted living (AAL)
emerges for the autonomous provisioning of pervasive things or objects from
relevant perturbation for advanced scientific instrumentation. Meanwhile,
citizens are observed in being transfixed by lively stimuli of monotonous urban
events with the advent of virtual reality or augmented things. Thus, due to the
involvement of situation-awareness or contextualization,
engagement/participation information as a utility promises to improve urban
experience. However, it is complex to grapple meaningful concepts due to
personalization obstacles, such as citizen psychology, information gap,
service-visualization. Moreover, recommended practices deficit adaptation to
monochromatic choice, disparate impairments, mobility and annotation-richness
in urban space. Hence, rejuvenation of engagement relates to monitoring and
quantification of 'service consumption and graceful degradation' of experience.
However, paramount challenges are imposed on this stipulation, such as,
unobservability, independence and composite relationship of contexts.
Therefore, a parametric Bayesian based model is envisioned to address
observability and scalability of contexts and its conjugal relationship with
engagement. Last but not the least, systematic framework is demonstrated, which
pinpoints key goals of context-aware engagement from participants' opinions,
usages and feed-backs.","['Rossi Kamal', 'Choong Seon Hong']",2016-03-09T13:59:40Z,http://arxiv.org/abs/1603.03732v1
"FloorLevel-Net: Recognizing Floor-Level Lines with
  Height-Attention-Guided Multi-task Learning","The ability to recognize the position and order of the floor-level lines that
divide adjacent building floors can benefit many applications, for example,
urban augmented reality (AR). This work tackles the problem of locating
floor-level lines in street-view images, using a supervised deep learning
approach. Unfortunately, very little data is available for training such a
network $-$ current street-view datasets contain either semantic annotations
that lack geometric attributes, or rectified facades without perspective
priors. To address this issue, we first compile a new dataset and develop a new
data augmentation scheme to synthesize training samples by harassing (i) the
rich semantics of existing rectified facades and (ii) perspective priors of
buildings in diverse street views. Next, we design FloorLevel-Net, a multi-task
learning network that associates explicit features of building facades and
implicit floor-level lines, along with a height-attention mechanism to help
enforce a vertical ordering of floor-level lines. The generated segmentations
are then passed to a second-stage geometry post-processing to exploit
self-constrained geometric priors for plausible and consistent reconstruction
of floor-level lines. Quantitative and qualitative evaluations conducted on
assorted facades in existing datasets and street views from Google demonstrate
the effectiveness of our approach. Also, we present context-aware image overlay
results and show the potentials of our approach in enriching AR-related
applications.","['Mengyang Wu', 'Wei Zeng', 'Chi-Wing Fu']",2021-07-06T08:17:59Z,http://arxiv.org/abs/2107.02462v1
Off-policy Imitation Learning from Visual Inputs,"Recently, various successful applications utilizing expert states in
imitation learning (IL) have been witnessed. However, another IL setting -- IL
from visual inputs (ILfVI), which has a greater promise to be applied in
reality by utilizing online visual resources, suffers from low data-efficiency
and poor performance resulted from an on-policy learning manner and
high-dimensional visual inputs. We propose OPIfVI (Off-Policy Imitation from
Visual Inputs), which is composed of an off-policy learning manner, data
augmentation, and encoder techniques, to tackle the mentioned challenges,
respectively. More specifically, to improve data-efficiency, OPIfVI conducts IL
in an off-policy manner, with which sampled data can be used multiple times. In
addition, we enhance the stability of OPIfVI with spectral normalization to
mitigate the side-effect of off-policy training. The core factor, contributing
to the poor performance of ILfVI, that we think is the agent could not extract
meaningful features from visual inputs. Hence, OPIfVI employs data augmentation
from computer vision to help train encoders that can better extract features
from visual inputs. In addition, a specific structure of gradient
backpropagation for the encoder is designed to stabilize the encoder training.
At last, we demonstrate that OPIfVI is able to achieve expert-level performance
and outperform existing baselines no matter visual demonstrations or visual
observations are provided via extensive experiments using DeepMind Control
Suite.","['Zhihao Cheng', 'Li Shen', 'Dacheng Tao']",2021-11-08T09:06:12Z,http://arxiv.org/abs/2111.04345v1
Temporally Consistent Online Depth Estimation in Dynamic Scenes,"Temporally consistent depth estimation is crucial for online applications
such as augmented reality. While stereo depth estimation has received
substantial attention as a promising way to generate 3D information, there is
relatively little work focused on maintaining temporal stability. Indeed, based
on our analysis, current techniques still suffer from poor temporal
consistency. Stabilizing depth temporally in dynamic scenes is challenging due
to concurrent object and camera motion. In an online setting, this process is
further aggravated because only past frames are available. We present a
framework named Consistent Online Dynamic Depth (CODD) to produce temporally
consistent depth estimates in dynamic scenes in an online setting. CODD
augments per-frame stereo networks with novel motion and fusion networks. The
motion network accounts for dynamics by predicting a per-pixel SE3
transformation and aligning the observations. The fusion network improves
temporal depth consistency by aggregating the current and past estimates. We
conduct extensive experiments and demonstrate quantitatively and qualitatively
that CODD outperforms competing methods in terms of temporal consistency and
performs on par in terms of per-frame accuracy.","['Zhaoshuo Li', 'Wei Ye', 'Dilin Wang', 'Francis X. Creighton', 'Russell H. Taylor', 'Ganesh Venkatesh', 'Mathias Unberath']",2021-11-17T19:00:51Z,http://arxiv.org/abs/2111.09337v3
GAN-Supervised Dense Visual Alignment,"We propose GAN-Supervised Learning, a framework for learning discriminative
models and their GAN-generated training data jointly end-to-end. We apply our
framework to the dense visual alignment problem. Inspired by the classic
Congealing method, our GANgealing algorithm trains a Spatial Transformer to map
random samples from a GAN trained on unaligned data to a common,
jointly-learned target mode. We show results on eight datasets, all of which
demonstrate our method successfully aligns complex data and discovers dense
correspondences. GANgealing significantly outperforms past self-supervised
correspondence algorithms and performs on-par with (and sometimes exceeds)
state-of-the-art supervised correspondence algorithms on several datasets --
without making use of any correspondence supervision or data augmentation and
despite being trained exclusively on GAN-generated data. For precise
correspondence, we improve upon state-of-the-art supervised methods by as much
as $3\times$. We show applications of our method for augmented reality, image
editing and automated pre-processing of image datasets for downstream GAN
training.","['William Peebles', 'Jun-Yan Zhu', 'Richard Zhang', 'Antonio Torralba', 'Alexei A. Efros', 'Eli Shechtman']",2021-12-09T18:59:58Z,http://arxiv.org/abs/2112.05143v2
Deep Graph Learning for Spatially-Varying Indoor Lighting Prediction,"Lighting prediction from a single image is becoming increasingly important in
many vision and augmented reality (AR) applications in which shading and shadow
consistency between virtual and real objects should be guaranteed. However,
this is a notoriously ill-posed problem, especially for indoor scenarios,
because of the complexity of indoor luminaires and the limited information
involved in 2D images. In this paper, we propose a graph learning-based
framework for indoor lighting estimation. At its core is a new lighting model
(dubbed DSGLight) based on depth-augmented Spherical Gaussians (SG) and a Graph
Convolutional Network (GCN) that infers the new lighting representation from a
single LDR image of limited field-of-view. Our lighting model builds 128 evenly
distributed SGs over the indoor panorama, where each SG encoding the lighting
and the depth around that node. The proposed GCN then learns the mapping from
the input image to DSGLight. Compared with existing lighting models, our
DSGLight encodes both direct lighting and indirect environmental lighting more
faithfully and compactly. It also makes network training and inference more
stable. The estimated depth distribution enables temporally stable shading and
shadows under spatially-varying lighting. Through thorough experiments, we show
that our method obviously outperforms existing methods both qualitatively and
quantitatively.","['Jiayang Bai', 'Jie Guo', 'Chenchen Wan', 'Zhenyu Chen', 'Zhen He', 'Shan Yang', 'Piaopiao Yu', 'Yan Zhang', 'Yanwen Guo']",2022-02-13T12:49:37Z,http://arxiv.org/abs/2202.06300v1
"RelMobNet: End-to-end relative camera pose estimation using a robust
  two-stage training","Relative camera pose estimation, i.e. estimating the translation and rotation
vectors using a pair of images taken in different locations, is an important
part of systems in augmented reality and robotics. In this paper, we present an
end-to-end relative camera pose estimation network using a siamese architecture
that is independent of camera parameters. The network is trained using the
Cambridge Landmarks data with four individual scene datasets and a dataset
combining the four scenes. To improve generalization, we propose a novel
two-stage training that alleviates the need of a hyperparameter to balance the
translation and rotation loss scale. The proposed method is compared with
one-stage training CNN-based methods such as RPNet and RCPNet and demonstrate
that the proposed model improves translation vector estimation by 16.11%,
28.88%, and 52.27% on the Kings College, Old Hospital, and St Marys Church
scenes, respectively. For proving texture invariance, we investigate the
generalization of the proposed method augmenting the datasets to different
scene styles, as ablation studies, using generative adversarial networks. Also,
we present a qualitative assessment of epipolar lines of our network
predictions and ground truth poses.","['Praveen Kumar Rajendran', 'Sumit Mishra', 'Luiz Felipe Vecchietti', 'Dongsoo Har']",2022-02-25T17:27:26Z,http://arxiv.org/abs/2202.12838v2
"ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial
  Viewpoints","Recent studies have demonstrated that visual recognition models lack
robustness to distribution shift. However, current work mainly considers model
robustness to 2D image transformations, leaving viewpoint changes in the 3D
world less explored. In general, viewpoint changes are prevalent in various
real-world applications (e.g., autonomous driving), making it imperative to
evaluate viewpoint robustness. In this paper, we propose a novel method called
ViewFool to find adversarial viewpoints that mislead visual recognition models.
By encoding real-world objects as neural radiance fields (NeRF), ViewFool
characterizes a distribution of diverse adversarial viewpoints under an
entropic regularizer, which helps to handle the fluctuations of the real camera
pose and mitigate the reality gap between the real objects and their neural
representations. Experiments validate that the common image classifiers are
extremely vulnerable to the generated adversarial viewpoints, which also
exhibit high cross-model transferability. Based on ViewFool, we introduce
ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint
robustness of image classifiers. Evaluation results on 40 classifiers with
diverse architectures, objective functions, and data augmentations reveal a
significant drop in model performance when tested on ImageNet-V, which provides
a possibility to leverage ViewFool as an effective data augmentation strategy
to improve viewpoint robustness.","['Yinpeng Dong', 'Shouwei Ruan', 'Hang Su', 'Caixin Kang', 'Xingxing Wei', 'Jun Zhu']",2022-10-08T03:06:49Z,http://arxiv.org/abs/2210.03895v1
"LACV-Net: Semantic Segmentation of Large-Scale Point Cloud Scene via
  Local Adaptive and Comprehensive VLAD","Large-scale point cloud semantic segmentation is an important task in 3D
computer vision, which is widely applied in autonomous driving, robotics, and
virtual reality. Current large-scale point cloud semantic segmentation methods
usually use down-sampling operations to improve computation efficiency and
acquire point clouds with multi-resolution. However, this may cause the problem
of missing local information. Meanwhile, it is difficult for networks to
capture global information in large-scale distributed contexts. To capture
local and global information effectively, we propose an end-to-end deep neural
network called LACV-Net for large-scale point cloud semantic segmentation. The
proposed network contains three main components: 1) a local adaptive feature
augmentation module (LAFA) to adaptively learn the similarity of centroids and
neighboring points to augment the local context; 2) a comprehensive VLAD module
(C-VLAD) that fuses local features with multi-layer, multi-scale, and
multi-resolution to represent a comprehensive global description vector; and 3)
an aggregation loss function to effectively optimize the segmentation
boundaries by constraining the adaptive weight from the LAFA module. Compared
to state-of-the-art networks on several large-scale benchmark datasets,
including S3DIS, Toronto3D, and SensatUrban, we demonstrated the effectiveness
of the proposed network.","['Ziyin Zeng', 'Yongyang Xu', 'Zhong Xie', 'Wei Tang', 'Jie Wan', 'Weichao Wu']",2022-10-12T02:11:00Z,http://arxiv.org/abs/2210.05870v1
"ART/ATK: A research platform for assessing and mitigating the
  sim-to-real gap in robotics and autonomous vehicle engineering","We discuss a platform that has both software and hardware components, and
whose purpose is to support research into characterizing and mitigating the
sim-to-real gap in robotics and vehicle autonomy engineering. The software is
operating-system independent and has three main components: a simulation engine
called Chrono, which supports high-fidelity vehicle and sensor simulation; an
autonomy stack for algorithm design and testing; and a development environment
that supports visualization and hardware-in-the-loop experimentation. The
accompanying hardware platform is a 1/6th scale vehicle augmented with
reconfigurable mountings for computing, sensing, and tracking. Since this
vehicle platform has a digital twin within the simulation environment, one can
test the same autonomy perception, state estimation, or controls algorithms, as
well as the processors they run on, in both simulation and reality. A
demonstration is provided to show the utilization of this platform for autonomy
research. Future work will concentrate on augmenting ART/ATK with support for a
full-sized Chevy Bolt EUV, which will be made available to this group in the
immediate future.","['Asher Elmquist', 'Aaron Young', 'Thomas Hansen', 'Sriram Ashokkumar', 'Stefan Caldararu', 'Abhiraj Dashora', 'Ishaan Mahajan', 'Harry Zhang', 'Luning Fang', 'He Shen', 'Xiangru Xu', 'Radu Serban', 'Dan Negrut']",2022-11-09T13:47:41Z,http://arxiv.org/abs/2211.04886v1
"Digital Twin Tracking Dataset (DTTD): A New RGB+Depth 3D Dataset for
  Longer-Range Object Tracking Applications","Digital twin is a problem of augmenting real objects with their digital
counterparts. It can underpin a wide range of applications in augmented reality
(AR), autonomy, and UI/UX. A critical component in a good digital-twin system
is real-time, accurate 3D object tracking. Most existing works solve 3D object
tracking through the lens of robotic grasping, employ older generations of
depth sensors, and measure performance metrics that may not apply to other
digital-twin applications such as in AR. In this work, we create a novel RGB-D
dataset, called Digital Twin Tracking Dataset (DTTD), to enable further
research of the problem and extend potential solutions towards longer ranges
and mm localization accuracy. To reduce point cloud noise from the input
source, we select the latest Microsoft Azure Kinect as the state-of-the-art
time-of-flight (ToF) camera. In total, 103 scenes of 10 common off-the-shelf
objects with rich textures are recorded, with each frame annotated with a
per-pixel semantic segmentation and ground-truth object poses provided by a
commercial motion capturing system. Through extensive experiments with
model-level and dataset-level analysis, we demonstrate that DTTD can help
researchers develop future object tracking methods and analyze new challenges.
The dataset, data generation, annotation, and model evaluation pipeline are
made publicly available as open source code at:
https://github.com/augcog/DTTDv1.","['Weiyu Feng', 'Seth Z. Zhao', 'Chuanyu Pan', 'Adam Chang', 'Yichen Chen', 'Zekun Wang', 'Allen Y. Yang']",2023-02-12T20:06:07Z,http://arxiv.org/abs/2302.05991v2
"Time-aware Graph Structure Learning via Sequence Prediction on Temporal
  Graphs","Temporal Graph Learning, which aims to model the time-evolving nature of
graphs, has gained increasing attention and achieved remarkable performance
recently. However, in reality, graph structures are often incomplete and noisy,
which hinders temporal graph networks (TGNs) from learning informative
representations. Graph contrastive learning uses data augmentation to generate
plausible variations of existing data and learn robust representations.
However, rule-based augmentation approaches may be suboptimal as they lack
learnability and fail to leverage rich information from downstream tasks. To
address these issues, we propose a Time-aware Graph Structure Learning (TGSL)
approach via sequence prediction on temporal graphs, which learns better graph
structures for downstream tasks through adding potential temporal edges. In
particular, it predicts time-aware context embedding based on previously
observed interactions and uses the Gumble-Top-K to select the closest candidate
edges to this context embedding. Additionally, several candidate sampling
strategies are proposed to ensure both efficiency and diversity. Furthermore,
we jointly learn the graph structure and TGNs in an end-to-end manner and
perform inference on the refined graph. Extensive experiments on temporal link
prediction benchmarks demonstrate that TGSL yields significant gains for the
popular TGNs such as TGAT and GraphMixer, and it outperforms other contrastive
learning methods on temporal graphs. We release the code at
https://github.com/ViktorAxelsen/TGSL.","['Haozhen Zhang', 'Xueting Han', 'Xi Xiao', 'Jing Bai']",2023-06-13T11:34:36Z,http://arxiv.org/abs/2306.07699v2
"Beyond Semantics: Learning a Behavior Augmented Relevance Model with
  Self-supervised Learning","Relevance modeling aims to locate desirable items for corresponding queries,
which is crucial for search engines to ensure user experience. Although most
conventional approaches address this problem by assessing the semantic
similarity between the query and item, pure semantic matching is not
everything. In reality, auxiliary query-item interactions extracted from user
historical behavior data of the search log could provide hints to reveal users'
search intents further. Drawing inspiration from this, we devise a novel
Behavior Augmented Relevance Learning model for Alipay Search (BARL-ASe) that
leverages neighbor queries of target item and neighbor items of target query to
complement target query-item semantic matching. Specifically, our model builds
multi-level co-attention for distilling coarse-grained and fine-grained
semantic representations from both neighbor and target views. The model
subsequently employs neighbor-target self-supervised learning to improve the
accuracy and robustness of BARL-ASe by strengthening representation and logit
learning. Furthermore, we discuss how to deal with the long-tail query-item
matching of the mini apps search scenario of Alipay practically. Experiments on
real-world industry data and online A/B testing demonstrate our proposal
achieves promising performance with low latency.","['Zeyuan Chen', 'Wei Chen', 'Jia Xu', 'Zhongyi Liu', 'Wei Zhang']",2023-08-10T06:52:53Z,http://arxiv.org/abs/2308.05379v4
Comparative Knowledge Distillation,"In the era of large scale pretrained models, Knowledge Distillation (KD)
serves an important role in transferring the wisdom of computationally heavy
teacher models to lightweight, efficient student models while preserving
performance. Traditional KD paradigms, however, assume readily available access
to teacher models for frequent inference -- a notion increasingly at odds with
the realities of costly, often proprietary, large scale models. Addressing this
gap, our paper considers how to minimize the dependency on teacher model
inferences in KD in a setting we term Few Teacher Inference Knowledge
Distillation (FTI KD). We observe that prevalent KD techniques and state of the
art data augmentation strategies fall short in this constrained setting.
Drawing inspiration from educational principles that emphasize learning through
comparison, we propose Comparative Knowledge Distillation (CKD), which
encourages student models to understand the nuanced differences in a teacher
model's interpretations of samples. Critically, CKD provides additional
learning signals to the student without making additional teacher calls. We
also extend the principle of CKD to groups of samples, enabling even more
efficient learning from limited teacher calls. Empirical evaluation across
varied experimental settings indicates that CKD consistently outperforms state
of the art data augmentation and KD techniques.","['Alex Wilf', 'Alex Tianyi Xu', 'Paul Pu Liang', 'Alexander Obolenskiy', 'Daniel Fried', 'Louis-Philippe Morency']",2023-11-03T21:55:33Z,http://arxiv.org/abs/2311.02253v1
"Topology of Surface Electromyogram Signals: Hand Gesture Decoding on
  Riemannian Manifolds","Decoding gestures from the upper limb using noninvasive surface
electromyogram (sEMG) signals is of keen interest for the rehabilitation of
amputees, artificial supernumerary limb augmentation, gestural control of
computers, and virtual/augmented realities. We show that sEMG signals recorded
across an array of sensor electrodes in multiple spatial locations around the
forearm evince a rich geometric pattern of global motor unit (MU) activity that
can be leveraged to distinguish different hand gestures. We demonstrate a
simple technique to analyze spatial patterns of muscle MU activity within a
temporal window and show that distinct gestures can be classified in both
supervised and unsupervised manners. Specifically, we construct symmetric
positive definite (SPD) covariance matrices to represent the spatial
distribution of MU activity in a time window of interest, calculated as
pairwise covariance of electrical signals measured across different electrodes.
This allows us to understand and manipulate multivariate sEMG timeseries on a
more natural subspace -the Riemannian manifold. Furthermore, it directly
addresses signal variability across individuals and sessions, which remains a
major challenge in the field. sEMG signals measured at a single electrode lack
contextual information such as how various anatomical and physiological factors
influence the signals and how their combined effect alters the evident
interaction among neighboring muscles. As we show here, analyzing spatial
patterns using covariance matrices on Riemannian manifolds allows us to
robustly model complex interactions across spatially distributed MUs and
provides a flexible and transparent framework to quantify differences in sEMG
signals across individuals. The proposed method is novel in the study of sEMG
signals and its performance exceeds the current benchmarks while maintaining
exceptional computational efficiency.","['Harshavardhana T. Gowda', 'Lee M. Miller']",2023-11-14T21:20:54Z,http://arxiv.org/abs/2311.08548v1
Learn to Augment Network Simulators Towards Digital Network Twins,"Digital network twin (DNT) is a promising paradigm to replicate real-world
cellular networks toward continual assessment, proactive management, and
what-if analysis. Existing discussions have been focusing on using only deep
learning techniques to build DNTs, which raises widespread concerns regarding
their generalization, explainability, and transparency. In this paper, we
explore an alternative approach to augment network simulators with
context-aware neural agents. The main challenge lies in the non-trivial
simulation-to-reality (sim-to-real) discrepancy between offline simulators and
real-world networks. To solve the challenge, we propose a new learn-to-bridge
algorithm to cost-efficiently bridge the sim-to-real discrepancy in two
alternative stages. In the first stage, we select states to query performances
in real-world networks by using newly-designed cost-aware Bayesian
optimization. In the second stage, we train the neural agent to learn the state
context and bridge the probabilistic discrepancy based on Bayesian neural
networks (BNN). In addition, we build a small-scale end-to-end network testbed
based on OpenAirInterface RAN and Core with USRP B210 and a smartphone, and
replicate the network in NS-3. The evaluation results show that, our proposed
solution substantially outperforms existing methods, with more than 92\%
reduction in the sim-to-real discrepancy.","['Yuru Zhang', 'Ming Zhao', 'Qiang Liu']",2023-11-21T17:32:42Z,http://arxiv.org/abs/2311.12745v1
The Ultrasound Visualization Pipeline - A Survey,"Ultrasound is one of the most frequently used imaging modality in medicine.
The high spatial resolution, its interactive nature and non-invasiveness makes
it the first choice in many examinations. Image interpretation is one of
ultrasound's main challenges. Much training is required to obtain a confident
skill level in ultrasound-based diagnostics. State-of-the-art graphics
techniques is needed to provide meaningful visualizations of ultrasound in
real-time. In this paper we present the process-pipeline for ultrasound
visualization, including an overview of the tasks performed in the specific
steps. To provide an insight into the trends of ultrasound visualization
research, we have selected a set of significant publications and divided them
into a technique-based taxonomy covering the topics pre-processing,
segmentation, registration, rendering and augmented reality. For the different
technique types we discuss the difference between ultrasound-based techniques
and techniques for other modalities.","['Åsmund Birkeland', 'Veronika Solteszova', 'Dieter Hönigmann', 'Odd Helge Gilja', 'Svein Brekke', 'Timo Ropinski', 'Ivan Viola']",2012-06-18T16:05:47Z,http://arxiv.org/abs/1206.3975v1
State-of-the Art Motion Estimation in the Context of 3D TV,"Progress in image sensors and computation power has fueled studies to improve
acquisition, processing, and analysis of 3D streams along with 3D
scenes/objects reconstruction. The role of motion compensation/motion
estimation (MCME) in 3D TV from end-to-end user is investigated in this
chapter. Motion vectors (MVs) are closely related to the concept of
disparities, and they can help improving dynamic scene acquisition, content
creation, 2D to 3D conversion, compression coding, decompression/decoding,
scene rendering, error concealment, virtual/augmented reality handling,
intelligent content retrieval, and displaying. Although there are different 3D
shape extraction methods, this chapter focuses mostly on shape-from-motion
(SfM) techniques due to their relevance to 3D TV. SfM extraction can restore 3D
shape information from a single camera data.","['Vania V. Estrela', 'Alessandra M. Coelho']",2013-12-23T09:43:09Z,http://arxiv.org/abs/1312.6497v1
Trends and Perspectives for Signal Processing in Consumer Audio,"The trend in media consumption towards streaming and portability offers new
challenges and opportunities for signal processing in audio and acoustics. The
most significant embodiment of this trend is that most music consumption now
happens on-the-go which has recently led to an explosion in headphone sales and
small portable speakers. In particular, premium headphones offer a gateway for
a younger generation to experience high quality sound. Additionally, through
technologies incorporating head-related transfer functions headphones can also
offer unique new experiences in gaming, augmented reality, and surround sound
listening. Home audio has also seen a transition to smaller sound systems in
the form of sound bars. This speaker configuration offers many exciting
challenges for surround sound reproduction which has traditionally used five
speakers surrounding the listener. Furthermore, modern home entertainment
systems offer more than just content delivery; users now expect wireless and
connected smart devices with video conferencing, gaming, and other interactive
capabilities. With this comes challenges for voice interaction at a distance
and in demanding conditions, e.g., during content playback, and opportunities
for new smart interactive experiences based on awareness of environment and
user biometrics.","['Joshua Atkins', 'Daniele Giacobello']",2014-05-19T19:10:15Z,http://arxiv.org/abs/1405.4843v1
A human centered perspective of E-maintenance,"E-maintenance is a technology aiming to organize and structure the ICT during
the whole life cycle of the product, to develop a maintenance support system
that is effective and efficient. A current challenge of E-maintenance is the
development of generic visualization solutions for users responsible for the
maintenance. AR can be a potential technology for E-maintenance visualization,
since it can bring knowledge to the real physical world, to assist the
technician perform his/her work without the need to interrupt to consult
manuals for information. This paper proposes a methodology for the development
of advanced interfaces for human aware E-maintenance so that complex
maintenance processes can be made safer, better quality, faster, anytime and
anywhere.","['Allan Oliveira', 'Regina Araujo']",2014-07-10T14:36:10Z,http://arxiv.org/abs/1407.2807v1
"Kadupul: Livin' on the Edge with Virtual Currencies and Time-Locked
  Puzzles","Devices connected to the Internet today have a wide range of local
communication channels available, such as wireless Wifi, Bluetooth or NFC, as
well as wired backhaul. In densely populated areas it is possible to create
heterogeneous, multihop communication paths using a combination of these
technologies, and often transmit data with lower latency than via a wired
Internet connection. However, the potential for sharing meshed wireless radios
in this way has never been realised due to the lack of economic incentives to
do so on the part of individual nodes.
  In this paper, we explore how virtual currencies such as Bitcoin might be
used to provide an end-to-end incentive scheme to convince forwarding nodes
that it is profitable to send packets on via the lowest latency mechanism
available. Clients inject a small amount of money to transmit a datagram, and
forwarding engines compete to solve a time-locked puzzle that can be claimed by
the node that delivers the result in the lowest latency. This approach
naturally extends congestion control techniques to a surge pricing model when
available bandwidth is low. We conclude by discussing several latency-sensitive
applications that would benefit for this, such as video streaming and local
augmented reality systems.","['Magnus Skjegstad', 'Anil Madhavapeddy', 'Jon Crowcroft']",2014-12-15T15:31:05Z,http://arxiv.org/abs/1412.4638v1
Mountain Peak Detection in Online Social Media,"We present a system for the classification of mountain panoramas from
user-generated photographs followed by identification and extraction of
mountain peaks from those panoramas. We have developed an automatic technique
that, given as input a geo-tagged photograph, estimates its FOV (Field Of View)
and the direction of the camera using a matching algorithm on the photograph
edge maps and a rendered view of the mountain silhouettes that should be seen
from the observer's point of view. The extraction algorithm then identifies the
mountain peaks present in the photograph and their profiles. We discuss
possible applications in social fields such as photograph peak tagging on
social portals, augmented reality on mobile devices when viewing a mountain
panorama, and generation of collective intelligence systems (such as
environmental models) from massive social media collections (e.g. snow water
availability maps based on mountain peak states extracted from photograph
hosting services).",['Roman Fedorov'],2015-08-12T15:43:16Z,http://arxiv.org/abs/1508.02959v1
"Mobile Edge Computing, Fog et al.: A Survey and Analysis of Security
  Threats and Challenges","For various reasons, the cloud computing paradigm is unable to meet certain
requirements (e.g. low latency and jitter, context awareness, mobility support)
that are crucial for several applications (e.g. vehicular networks, augmented
reality). To fulfil these requirements, various paradigms, such as fog
computing, mobile edge computing, and mobile cloud computing, have emerged in
recent years. While these edge paradigms share several features, most of the
existing research is compartmentalised; no synergies have been explored. This
is especially true in the field of security, where most analyses focus only on
one edge paradigm, while ignoring the others. The main goal of this study is to
holistically analyse the security threats, challenges, and mechanisms inherent
in all edge paradigms, while highlighting potential synergies and venues of
collaboration. In our results, we will show that all edge paradigms should
consider the advances in other paradigms.","['Rodrigo Roman', 'Javier Lopez', 'Masahiro Mambo']",2016-02-01T11:46:41Z,http://arxiv.org/abs/1602.00484v2
Detecting Engagement in Egocentric Video,"In a wearable camera video, we see what the camera wearer sees. While this
makes it easy to know roughly what he chose to look at, it does not immediately
reveal when he was engaged with the environment. Specifically, at what moments
did his focus linger, as he paused to gather more information about something
he saw? Knowing this answer would benefit various applications in video
summarization and augmented reality, yet prior work focuses solely on the
""what"" question (estimating saliency, gaze) without considering the ""when""
(engagement). We propose a learning-based approach that uses long-term
egomotion cues to detect engagement, specifically in browsing scenarios where
one frequently takes in new visual information (e.g., shopping, touring). We
introduce a large, richly annotated dataset for ego-engagement that is the
first of its kind. Our approach outperforms a wide array of existing methods.
We show engagement can be detected well independent of both scene appearance
and the camera wearer's identity.","['Yu-Chuan Su', 'Kristen Grauman']",2016-04-04T15:21:16Z,http://arxiv.org/abs/1604.00906v1
A Practical Approach to Spatiotemporal Data Compression,"Datasets representing the world around us are becoming ever more unwieldy as
data volumes grow. This is largely due to increased measurement and modelling
resolution, but the problem is often exacerbated when data are stored at
spuriously high precisions. In an effort to facilitate analysis of these
datasets, computationally intensive calculations are increasingly being
performed on specialised remote servers before the reduced data are transferred
to the consumer. Due to bandwidth limitations, this often means data are
displayed as simple 2D data visualisations, such as scatter plots or images. We
present here a novel way to efficiently encode and transmit 4D data fields
on-demand so that they can be locally visualised and interrogated. This nascent
""4D video"" format allows us to more flexibly move the boundary between data
server and consumer client. However, it has applications beyond purely
scientific visualisation, in the transmission of data to virtual and augmented
reality.","['Niall H. Robinson', 'Rachel Prudden', 'Alberto Arribas']",2016-04-13T08:33:36Z,http://arxiv.org/abs/1604.03688v2
"Keyframe-based monocular SLAM: design, survey, and future directions","Extensive research in the field of monocular SLAM for the past fifteen years
has yielded workable systems that found their way into various applications in
robotics and augmented reality. Although filter-based monocular SLAM systems
were common at some time, the more efficient keyframe-based solutions are
becoming the de facto methodology for building a monocular SLAM system. The
objective of this paper is threefold: first, the paper serves as a guideline
for people seeking to design their own monocular SLAM according to specific
environmental constraints. Second, it presents a survey that covers the various
keyframe-based monocular SLAM systems in the literature, detailing the
components of their implementation, and critically assessing the specific
strategies made in each proposed solution. Third, the paper provides insight
into the direction of future research in this field, to address the major
limitations still facing monocular SLAM; namely, in the issues of illumination
changes, initialization, highly dynamic motion, poorly textured scenes,
repetitive textures, map maintenance, and failure recovery.","['Georges Younes', 'Daniel Asmar', 'Elie Shammas', 'John Zelek']",2016-07-02T06:01:16Z,http://arxiv.org/abs/1607.00470v2
"Big IoT and social networking data for smart cities: Algorithmic
  improvements on Big Data Analysis in the context of RADICAL city applications","In this paper we present a SOA (Service Oriented Architecture)-based
platform, enabling the retrieval and analysis of big datasets stemming from
social networking (SN) sites and Internet of Things (IoT) devices, collected by
smart city applications and socially-aware data aggregation services. A large
set of city applications in the areas of Participating Urbanism, Augmented
Reality and Sound-Mapping throughout participating cities is being applied,
resulting into produced sets of millions of user-generated events and online SN
reports fed into the RADICAL platform. Moreover, we study the application of
data analytics such as sentiment analysis to the combined IoT and SN data saved
into an SQL database, further investigating algorithmic and configurations to
minimize delays in dataset processing and results retrieval.","['Evangelos Psomakelis', 'Fotis Aisopos', 'Antonios Litke', 'Konstantinos Tserpes', 'Magdalini Kardara', 'Pablo Martínez Campo']",2016-07-02T13:35:02Z,http://arxiv.org/abs/1607.00509v1
Automated Application Offloading through Ant-inspired Decision-Making,"-The explosive trend of smartphone usage as the most effective and convenient
communication tools of human life in recent years make developers build ever
more complex smartphone applications. Gaming, navigation, video editing,
augmented reality, and speech recognition applications require considerable
computational power and energy. Although smart- phones have a wide range of
capabilities - GPS, WiFi, cameras - their inherent limitations - frequent
disconnections, mobility - and significant constraints - size, lower weights,
longer battery life - make difficult to exploiting their full potential to run
complex applications. Several research works have proposed solutions in
application offloading domain, but few ones concerning the highly changing
properties of the environment. To address these issues, we realize an automated
application offloading middleware, ACOMMA, with dynamic and re-adaptable
decision-making engine. The decision engine of ACOMMA is based on an ant-
inspired algorithm.","['Roya Golchay', 'Frédéric Le Mouël', 'Julien Ponge', 'Nicolas Stouls']",2016-11-07T10:37:38Z,http://arxiv.org/abs/1611.02275v1
Deep Cuboid Detection: Beyond 2D Bounding Boxes,"We present a Deep Cuboid Detector which takes a consumer-quality RGB image of
a cluttered scene and localizes all 3D cuboids (box-like objects). Contrary to
classical approaches which fit a 3D model from low-level cues like corners,
edges, and vanishing points, we propose an end-to-end deep learning system to
detect cuboids across many semantic categories (e.g., ovens, shipping boxes,
and furniture). We localize cuboids with a 2D bounding box, and simultaneously
localize the cuboid's corners, effectively producing a 3D interpretation of
box-like objects. We refine keypoints by pooling convolutional features
iteratively, improving the baseline method significantly. Our deep learning
cuboid detector is trained in an end-to-end fashion and is suitable for
real-time applications in augmented reality (AR) and robotics.","['Debidatta Dwibedi', 'Tomasz Malisiewicz', 'Vijay Badrinarayanan', 'Andrew Rabinovich']",2016-11-30T06:00:47Z,http://arxiv.org/abs/1611.10010v1
"EgoCap: Egocentric Marker-less Motion Capture with Two Fisheye Cameras
  (Extended Abstract)","Marker-based and marker-less optical skeletal motion-capture methods use an
outside-in arrangement of cameras placed around a scene, with viewpoints
converging on the center. They often create discomfort by possibly needed
marker suits, and their recording volume is severely restricted and often
constrained to indoor scenes with controlled backgrounds. We therefore propose
a new method for real-time, marker-less and egocentric motion capture which
estimates the full-body skeleton pose from a lightweight stereo pair of fisheye
cameras that are attached to a helmet or virtual-reality headset. It combines
the strength of a new generative pose estimation framework for fisheye views
with a ConvNet-based body-part detector trained on a new automatically
annotated and augmented dataset. Our inside-in method captures full-body motion
in general indoor and outdoor scenes, and also crowded scenes.","['Helge Rhodin', 'Christian Richardt', 'Dan Casas', 'Eldar Insafutdinov', 'Mohammad Shafiei', 'Hans-Peter Seidel', 'Bernt Schiele', 'Christian Theobalt']",2016-12-31T16:49:39Z,http://arxiv.org/abs/1701.00142v1
"On-the-Fly Adaptation of Regression Forests for Online Camera
  Relocalisation","Camera relocalisation is an important problem in computer vision, with
applications in simultaneous localisation and mapping, virtual/augmented
reality and navigation. Common techniques either match the current image
against keyframes with known poses coming from a tracker, or establish 2D-to-3D
correspondences between keypoints in the current image and points in the scene
in order to estimate the camera pose. Recently, regression forests have become
a popular alternative to establish such correspondences. They achieve accurate
results, but must be trained offline on the target scene, preventing
relocalisation in new environments. In this paper, we show how to circumvent
this limitation by adapting a pre-trained forest to a new scene on the fly. Our
adapted forests achieve relocalisation performance that is on par with that of
offline forests, and our approach runs in under 150ms, making it desirable for
real-time systems that require online relocalisation.","['Tommaso Cavallari', 'Stuart Golodetz', 'Nicholas A. Lord', 'Julien Valentin', 'Luigi Di Stefano', 'Philip H. S. Torr']",2017-02-09T10:39:05Z,http://arxiv.org/abs/1702.02779v2
A Review on Deep Learning Techniques Applied to Semantic Segmentation,"Image semantic segmentation is more and more being of interest for computer
vision and machine learning researchers. Many applications on the rise need
accurate and efficient segmentation mechanisms: autonomous driving, indoor
navigation, and even virtual or augmented reality systems to name a few. This
demand coincides with the rise of deep learning approaches in almost every
field or application target related to computer vision, including semantic
segmentation or scene understanding. This paper provides a review on deep
learning methods for semantic segmentation applied to various application
areas. Firstly, we describe the terminology of this field as well as mandatory
background concepts. Next, the main datasets and challenges are exposed to help
researchers decide which are the ones that best suit their needs and their
targets. Then, existing methods are reviewed, highlighting their contributions
and their significance in the field. Finally, quantitative results are given
for the described methods and the datasets in which they were evaluated,
following up with a discussion of the results. At last, we point out a set of
promising future works and draw our own conclusions about the state of the art
of semantic segmentation using deep learning techniques.","['Alberto Garcia-Garcia', 'Sergio Orts-Escolano', 'Sergiu Oprea', 'Victor Villena-Martinez', 'Jose Garcia-Rodriguez']",2017-04-22T23:37:43Z,http://arxiv.org/abs/1704.06857v1
"Intelligent Infrastructure for Smart Agriculture: An Integrated Food,
  Energy and Water System","Agriculture provides economic opportunity through innovation; helps rural
America to thrive; promotes agricultural production that better nourishes
Americans; and aims to preserve natural resources through healthy private
working lands, conservation, improved watersheds, and restored forests. From
agricultural production to food supply, agriculture supports rural and urban
economies across the U.S. It accounts for 10% of U.S. jobs and is currently
creating new jobs in the growing field of data-driven farming. However, U.S.
global competitiveness associated with food and nutrition security is at risk
because of accelerated investments by many other countries in agriculture,
food, energy, and resource management. To ensure U.S. global competitiveness
and long-term food security, it is imperative that we build sustainable
physical and cyber infrastructures to enable self-managing and sustainable
farming. Such infrastructures should enable next generation precision-farms by
harnessing modern and emerging technologies such as small satellites, broadband
Internet, tele-operation, augmented reality, advanced data analytics, sensors,
and robotics.","['Shashi Shekhar', 'Joe Colletti', 'Francisco Muñoz-Arriola', 'Lakshmish Ramaswamy', 'Chandra Krintz', 'Lav Varshney', 'Debra Richardson']",2017-05-04T19:55:23Z,http://arxiv.org/abs/1705.01993v1
On human motion prediction using recurrent neural networks,"Human motion modelling is a classical problem at the intersection of graphics
and computer vision, with applications spanning human-computer interaction,
motion synthesis, and motion prediction for virtual and augmented reality.
Following the success of deep learning methods in several computer vision
tasks, recent work has focused on using deep recurrent neural networks (RNNs)
to model human motion, with the goal of learning time-dependent representations
that perform tasks such as short-term motion prediction and long-term human
motion synthesis. We examine recent work, with a focus on the evaluation
methodologies commonly used in the literature, and show that, surprisingly,
state-of-the-art performance can be achieved by a simple baseline that does not
attempt to model motion at all. We investigate this result, and analyze recent
RNN methods by looking at the architectures, loss functions, and training
procedures used in state-of-the-art approaches. We propose three changes to the
standard RNN models typically used for human motion, which result in a simple
and scalable RNN architecture that obtains state-of-the-art performance on
human motion prediction.","['Julieta Martinez', 'Michael J. Black', 'Javier Romero']",2017-05-06T05:08:05Z,http://arxiv.org/abs/1705.02445v1
"SLAM based Quasi Dense Reconstruction For Minimally Invasive Surgery
  Scenes","Recovering surgical scene structure in laparoscope surgery is crucial step
for surgical guidance and augmented reality applications. In this paper, a
quasi dense reconstruction algorithm of surgical scene is proposed. This is
based on a state-of-the-art SLAM system, and is exploiting the initial
exploration phase that is typically performed by the surgeon at the beginning
of the surgery. We show how to convert the sparse SLAM map to a quasi dense
scene reconstruction, using pairs of keyframe images and correlation-based
featureless patch matching. We have validated the approach with a live porcine
experiment using Computed Tomography as ground truth, yielding a Root Mean
Squared Error of 4.9mm.","['Nader Mahmoud', 'Alexandre Hostettler', 'Toby Collins', 'Luc Soler', 'Christophe Doignon', 'J. M. M. Montiel']",2017-05-25T09:44:34Z,http://arxiv.org/abs/1705.09107v1
Interacting with Acoustic Simulation and Fabrication,"Incorporating accurate physics-based simulation into interactive design tools
is challenging. However, adding the physics accurately becomes crucial to
several emerging technologies. For example, in virtual/augmented reality
(VR/AR) videos, the faithful reproduction of surrounding audios is required to
bring the immersion to the next level. Similarly, as personal fabrication is
made possible with accessible 3D printers, more intuitive tools that respect
the physical constraints can help artists to prototype designs. One main hurdle
is the sheer amount of computation complexity to accurately reproduce the
real-world phenomena through physics-based simulation. In my thesis research, I
develop interactive tools that implement efficient physics-based simulation
algorithms for automatic optimization and intuitive user interaction.",['Dingzeyu Li'],2017-08-09T16:20:12Z,http://arxiv.org/abs/1708.02895v2
"DeformNet: Free-Form Deformation Network for 3D Shape Reconstruction
  from a Single Image","3D reconstruction from a single image is a key problem in multiple
applications ranging from robotic manipulation to augmented reality. Prior
methods have tackled this problem through generative models which predict 3D
reconstructions as voxels or point clouds. However, these methods can be
computationally expensive and miss fine details. We introduce a new
differentiable layer for 3D data deformation and use it in DeformNet to learn a
model for 3D reconstruction-through-deformation. DeformNet takes an image
input, searches the nearest shape template from a database, and deforms the
template to match the query image. We evaluate our approach on the ShapeNet
dataset and show that - (a) the Free-Form Deformation layer is a powerful new
building block for Deep Learning models that manipulate 3D data (b) DeformNet
uses this FFD layer combined with shape retrieval for smooth and
detail-preserving 3D reconstruction of qualitatively plausible point clouds
with respect to a single query image (c) compared to other state-of-the-art 3D
reconstruction methods, DeformNet quantitatively matches or outperforms their
benchmarks by significant margins. For more information, visit:
https://deformnet-site.github.io/DeformNet-website/ .","['Andrey Kurenkov', 'Jingwei Ji', 'Animesh Garg', 'Viraj Mehta', 'JunYoung Gwak', 'Christopher Choy', 'Silvio Savarese']",2017-08-11T00:43:19Z,http://arxiv.org/abs/1708.04672v1
Seminar Innovation Management - Winter Term 2017,"This document contains the results obtained by the Innovation Management
Seminar in winter term 2017. In total 11 ideas have been developed by the team.
In the document all 11 ideas show improvements for future applications in
ophthalmology. The 11 ideas are AR/VR Glasses with Medical Applications,
Augmented Reality Eye Surgery, Game Diagnosis, Intelligent Adapting Glasses, MD
Facebook, Medical Crowd Segmentation, Personalized 3D Model of the Human Eye,
Photoacoustic Contact Lens, Power Supply Smart Contact Lens, VR-Cornea and Head
Mount for Fundus Imaging","['Gerd Häusler', 'Aleksandra Milczarek', 'Markus Schreiter', 'Thomas Kästner', 'Florian Willomitzer', 'Andreas Maier', 'Florian Schiffers', 'Stefan Steidl', 'Temitope Paul Onanuga', 'Mathias Unberath', 'Florian Dötzer', 'Maike Stöve', 'Jonas Hajek', 'Christian Heidorn', 'Felix Häußler', 'Tobias Geimer', 'Johannes Wendel']",2017-08-22T06:27:18Z,http://arxiv.org/abs/1708.09706v1
VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection,"Accurate detection of objects in 3D point clouds is a central problem in many
applications, such as autonomous navigation, housekeeping robots, and
augmented/virtual reality. To interface a highly sparse LiDAR point cloud with
a region proposal network (RPN), most existing efforts have focused on
hand-crafted feature representations, for example, a bird's eye view
projection. In this work, we remove the need of manual feature engineering for
3D point clouds and propose VoxelNet, a generic 3D detection network that
unifies feature extraction and bounding box prediction into a single stage,
end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud
into equally spaced 3D voxels and transforms a group of points within each
voxel into a unified feature representation through the newly introduced voxel
feature encoding (VFE) layer. In this way, the point cloud is encoded as a
descriptive volumetric representation, which is then connected to a RPN to
generate detections. Experiments on the KITTI car detection benchmark show that
VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a
large margin. Furthermore, our network learns an effective discriminative
representation of objects with various geometries, leading to encouraging
results in 3D detection of pedestrians and cyclists, based on only LiDAR.","['Yin Zhou', 'Oncel Tuzel']",2017-11-17T04:25:24Z,http://arxiv.org/abs/1711.06396v1
Dynamic High Resolution Deformable Articulated Tracking,"The last several years have seen significant progress in using depth cameras
for tracking articulated objects such as human bodies, hands, and robotic
manipulators. Most approaches focus on tracking skeletal parameters of a fixed
shape model, which makes them insufficient for applications that require
accurate estimates of deformable object surfaces. To overcome this limitation,
we present a 3D model-based tracking system for articulated deformable objects.
Our system is able to track human body pose and high resolution surface
contours in real time using a commodity depth sensor and GPU hardware. We
implement this as a joint optimization over a skeleton to account for changes
in pose, and over the vertices of a high resolution mesh to track the subject's
shape. Through experimental results we show that we are able to capture dynamic
sub-centimeter surface detail such as folds and wrinkles in clothing. We also
show that this shape estimation aids kinematic pose estimation by providing a
more accurate target to match against the point cloud. The end result is highly
accurate spatiotemporal and semantic information which is well suited for
physical human robot interaction as well as virtual and augmented reality
systems.","['Aaron Walsman', 'Weilin Wan', 'Tanner Schmidt', 'Dieter Fox']",2017-11-21T19:07:30Z,http://arxiv.org/abs/1711.07999v1
"A surgical system for automatic registration, stiffness mapping and
  dynamic image overlay","In this paper we develop a surgical system using the da Vinci research kit
(dVRK) that is capable of autonomously searching for tumors and dynamically
displaying the tumor location using augmented reality. Such a system has the
potential to quickly reveal the location and shape of tumors and visually
overlay that information to reduce the cognitive overload of the surgeon. We
believe that our approach is one of the first to incorporate state-of-the-art
methods in registration, force sensing and tumor localization into a unified
surgical system. First, the preoperative model is registered to the
intra-operative scene using a Bingham distribution-based filtering approach. An
active level set estimation is then used to find the location and the shape of
the tumors. We use a recently developed miniature force sensor to perform the
palpation. The estimated stiffness map is then dynamically overlaid onto the
registered preoperative model of the organ. We demonstrate the efficacy of our
system by performing experiments on phantom prostate models with embedded stiff
inclusions.","['Nicolas Zevallos', 'Rangaprasad Arun Srivatsan', 'Hadi Salman', 'Lu Li', 'Jianing Qian', 'Saumya Saxena', 'Mengyun Xu', 'Kartik Patath', 'Howie Choset']",2017-11-23T21:15:52Z,http://arxiv.org/abs/1711.08828v1
Learning Less is More - 6D Camera Localization via 3D Surface Regression,"Popular research areas like autonomous driving and augmented reality have
renewed the interest in image-based camera localization. In this work, we
address the task of predicting the 6D camera pose from a single RGB image in a
given 3D environment. With the advent of neural networks, previous works have
either learned the entire camera localization process, or multiple components
of a camera localization pipeline. Our key contribution is to demonstrate and
explain that learning a single component of this pipeline is sufficient. This
component is a fully convolutional neural network for densely regressing
so-called scene coordinates, defining the correspondence between the input
image and the 3D scene space. The neural network is prepended to a new
end-to-end trainable pipeline. Our system is efficient, highly accurate, robust
in training, and exhibits outstanding generalization capabilities. It exceeds
state-of-the-art consistently on indoor and outdoor datasets. Interestingly,
our approach surpasses existing techniques even without utilizing a 3D model of
the scene during training, since the network is able to discover 3D scene
geometry automatically, solely from single-view constraints.","['Eric Brachmann', 'Carsten Rother']",2017-11-28T11:11:03Z,http://arxiv.org/abs/1711.10228v2
Online Temporal Calibration for Monocular Visual-Inertial Systems,"Accurate state estimation is a fundamental module for various intelligent
applications, such as robot navigation, autonomous driving, virtual and
augmented reality. Visual and inertial fusion is a popular technology for 6-DOF
state estimation in recent years. Time instants at which different sensors'
measurements are recorded are of crucial importance to the system's robustness
and accuracy. In practice, timestamps of each sensor typically suffer from
triggering and transmission delays, leading to temporal misalignment (time
offsets) among different sensors. Such temporal offset dramatically influences
the performance of sensor fusion. To this end, we propose an online approach
for calibrating temporal offset between visual and inertial measurements. Our
approach achieves temporal offset calibration by jointly optimizing time
offset, camera and IMU states, as well as feature locations in a SLAM system.
Furthermore, the approach is a general model, which can be easily employed in
several feature-based optimization frameworks. Simulation and experimental
results demonstrate the high accuracy of our calibration approach even compared
with other state-of-art offline tools. The VIO comparison against other methods
proves that the online temporal calibration significantly benefits
visual-inertial systems. The source code of temporal calibration is integrated
into our public project, VINS-Mono.","['Tong Qin', 'Shaojie Shen']",2018-08-02T07:16:25Z,http://arxiv.org/abs/1808.00692v1
"Estimating Metric Poses of Dynamic Objects Using Monocular
  Visual-Inertial Fusion","A monocular 3D object tracking system generally has only up-to-scale pose
estimation results without any prior knowledge of the tracked object. In this
paper, we propose a novel idea to recover the metric scale of an arbitrary
dynamic object by optimizing the trajectory of the objects in the world frame,
without motion assumptions. By introducing an additional constraint in the time
domain, our monocular visual-inertial tracking system can obtain continuous six
degree of freedom (6-DoF) pose estimation without scale ambiguity. Our method
requires neither fixed multi-camera nor depth sensor settings for scale
observability, instead, the IMU inside the monocular sensing suite provides
scale information for both camera itself and the tracked object. We build the
proposed system on top of our monocular visual-inertial system (VINS) to obtain
accurate state estimation of the monocular camera in the world frame. The whole
system consists of a 2D object tracker, an object region-based visual bundle
adjustment (BA), VINS and a correlation analysis-based metric scale estimator.
Experimental comparisons with ground truth demonstrate the tracking accuracy of
our 3D tracking performance while a mobile augmented reality (AR) demo shows
the feasibility of potential applications.","['Kejie Qiu', 'Tong Qin', 'Hongwen Xie', 'Shaojie Shen']",2018-08-21T04:04:20Z,http://arxiv.org/abs/1808.06753v1
Evolvement Constrained Adversarial Learning for Video Style Transfer,"Video style transfer is a useful component for applications such as augmented
reality, non-photorealistic rendering, and interactive games. Many existing
methods use optical flow to preserve the temporal smoothness of the synthesized
video. However, the estimation of optical flow is sensitive to occlusions and
rapid motions. Thus, in this work, we introduce a novel evolve-sync loss
computed by evolvements to replace optical flow. Using this evolve-sync loss,
we build an adversarial learning framework, termed as Video Style Transfer
Generative Adversarial Network (VST-GAN), which improves upon the MGAN method
for image style transfer for more efficient video style transfer. We perform
extensive experimental evaluations of our method and show quantitative and
qualitative improvements over the state-of-the-art methods.","['Wenbo Li', 'Longyin Wen', 'Xiao Bian', 'Siwei Lyu']",2018-11-06T16:31:19Z,http://arxiv.org/abs/1811.02476v1
"Micro-Operator driven Local 5G Network Architecture for Industrial
  Internet","In addition to the high degree of flexibility and customization required by
different vertical sectors, 5G calls for a network architecture that ensures
ultra-responsive and ultra-reliable communication links. The novel concept
called micro-operator (uO) enables a versatile set of stakeholders to operate
local 5G networks within their premises with a guaranteed quality and
reliability to complement mobile network operators' (MNOs) offerings. In this
paper, we propose a descriptive architecture for emerging 5G uOs which provides
user specific and location specific services in a spatially confined
environment. The architecture is discussed in terms of network functions and
the operational units which entail the core and radio access networks in a
smart factory environment which supports industry 4.0 standards. Moreover, in
order to realize the conceptual design, we provide simulation results for the
latency measurements of the proposed uO architecture with respect to an
augmented reality use case in industrial internet. Thereby we discuss the
benefits of having uO driven local 5G networks for specialized user
requirements, rather than continuing with the conventional approach where only
MNOs can deploy cellular networks.","['Yushan Siriwardhana', 'Pawani Porambage', 'Madhusanka Liyanage', 'Jaspreet Singh Walia', 'Marja Matinmikko-Blue', 'Mika Ylianttila']",2018-11-10T20:05:34Z,http://arxiv.org/abs/1811.04299v1
"You Only Live Multiple Times: A Blackbox Solution for Reusing Crash-Stop
  Algorithms In Realistic Crash-Recovery Settings","Distributed agreement-based algorithms are often specified in a crash-stop
asynchronous model augmented by Chandra and Toueg's unreliable failure
detectors. In such models, correct nodes stay up forever, incorrect nodes
eventually crash and remain down forever, and failure detectors behave
correctly forever eventually, However, in reality, nodes as well as
communication links both crash and recover without deterministic guarantees to
remain in some state forever.
  In this paper, we capture this realistic temporary and probabilitic behaviour
in a simple new system model. Moreover, we identify a large algorithm class for
which we devis a property-preserving transformation. Using this transformation,
many algorithms written for the asynchronous crash-stop model run correctly and
unchanged in real systems.","['David Kozhaya', 'Ognjen Maric', 'Yvonne-Anne Pignolet']",2018-11-12T21:20:04Z,http://arxiv.org/abs/1811.05007v1
Edge Cloud System Evaluation,"Real-time applications in the next generation networks often rely upon
offloading the computational task to a \textit{nearby} server to achieve
ultra-low latency. Augmented reality applications for instance have strict
latency requirements which can be fulfilled by an interplay between cloud and
edge servers. In this work, we study the impact of load on a hybrid edge cloud
system. The resource distribution between central cloud and edge affects the
capacity of the network. Optimizing delay and capacity constraints of this
hybrid network is similar to maximum cardinal bin packing problem which is
NP-hard. We design a simulation framework using a city-scale access point
dataset to propose an enhanced capacity edge cloud network while answering
following questions: (a) how much load an edge cloud network can support
without affecting the performance of an application, (b) how is application
delay-constraint limit affects the capacity of the network, (c) what is the
impact of load and resource distribution on goodput, (d) under what
circumstances, cloud can perform better than edge network and (e) what is the
impact of inter-edge networking bandwidth on the system capacity. An evaluation
system and model is developed to analyze the tradeoffs of different edge cloud
deployments and results are shown to support the claims.","['Sumit Maheshwari', 'Dipankar Raychaudhuri']",2018-09-16T20:29:11Z,http://arxiv.org/abs/1811.11244v1
"Probabilistic Regression of Rotations using Quaternion Averaging and a
  Deep Multi-Headed Network","Accurate estimates of rotation are crucial to vision-based motion estimation
in augmented reality and robotics. In this work, we present a method to extract
probabilistic estimates of rotation from deep regression models. First, we
build on prior work and argue that a multi-headed network structure we name
HydraNet provides better calibrated uncertainty estimates than methods that
rely on stochastic forward passes. Second, we extend HydraNet to targets that
belong to the rotation group, SO(3), by regressing unit quaternions and using
the tools of rotation averaging and uncertainty injection onto the manifold to
produce three-dimensional covariances. Finally, we present results and analysis
on a synthetic dataset, learn consistent orientation estimates on the 7-Scenes
dataset, and show how we can use our learned covariances to fuse deep estimates
of relative orientation with classical stereo visual odometry to improve
localization on the KITTI dataset.","['Valentin Peretroukhin', 'Brandon Wagstaff', 'Matthew Giamou', 'Jonathan Kelly']",2019-04-01T19:39:09Z,http://arxiv.org/abs/1904.03182v2
"PerfVis: Pervasive Visualization in Immersive AugmentedReality for
  Performance Awareness","Developers are usually unaware of the impact of code changes to the
performance of software systems. Although developers can analyze the
performance of a system by executing, for instance, a performance test to
compare the performance of two consecutive versions of the system, changing
from a programming task to a testing task would disrupt the development flow.
In this paper, we propose the use of a city visualization that dynamically
provides developers with a pervasive view of the continuous performance of a
system. We use an immersive augmented reality device (Microsoft HoloLens) to
display our visualization and extend the integrated development environment on
a computer screen to use the physical space. We report on technical details of
the design and implementation of our visualization tool, and discuss early
feedback that we collected of its usability. Our investigation explores a new
visual metaphor to support the exploration and analysis of possibly very large
and multidimensional performance data. Our initial result indicates that the
city metaphor can be adequate to analyze dynamic performance data on a large
and non-trivial software system.","['Leonel Merino', 'Mario Hess', 'Alexandre Bergel', 'Oscar Nierstrasz', 'Daniel Weiskopf']",2019-04-05T14:59:01Z,http://arxiv.org/abs/1904.06399v1
"Complexer-YOLO: Real-Time 3D Object Detection and Tracking on Semantic
  Point Clouds","Accurate detection of 3D objects is a fundamental problem in computer vision
and has an enormous impact on autonomous cars, augmented/virtual reality and
many applications in robotics. In this work we present a novel fusion of neural
network based state-of-the-art 3D detector and visual semantic segmentation in
the context of autonomous driving. Additionally, we introduce
Scale-Rotation-Translation score (SRTs), a fast and highly parameterizable
evaluation metric for comparison of object detections, which speeds up our
inference time up to 20\% and halves training time. On top, we apply
state-of-the-art online multi target feature tracking on the object
measurements to further increase accuracy and robustness utilizing temporal
information. Our experiments on KITTI show that we achieve same results as
state-of-the-art in all related categories, while maintaining the performance
and accuracy trade-off and still run in real-time. Furthermore, our model is
the first one that fuses visual semantic with 3D object detection.","['Martin Simon', 'Karl Amende', 'Andrea Kraus', 'Jens Honer', 'Timo Sämann', 'Hauke Kaulbersch', 'Stefan Milz', 'Horst Michael Gross']",2019-04-16T08:49:06Z,http://arxiv.org/abs/1904.07537v1
User interface design for military AR applications,"Designing a user interface for military situation awareness presents
challenges for managing information in a useful and usable manner. We present
an integrated set of functions for the presentation of and interaction with
information for a mobile augmented reality application for military
applications. Our research has concentrated on four areas. We filter
information based on relevance to the user (in turn based on location),
evaluate methods for presenting information that represents entities occluded
from the user's view, enable interaction through a top-down map view metaphor
akin to current techniques used in the military, and facilitate collaboration
with other mobile users and/or a command center. In addition, we refined the
user interface architecture to conform to requirements from subject matter
experts. We discuss the lessons learned in our work and directions for future
research.","['Mark A. Livingston', 'Zhuming Ai', 'Kevin Karsch', 'Gregory O. Gibson']",2019-04-21T02:10:15Z,http://arxiv.org/abs/1904.09529v1
"You2Me: Inferring Body Pose in Egocentric Video via First and Second
  Person Interactions","The body pose of a person wearing a camera is of great interest for
applications in augmented reality, healthcare, and robotics, yet much of the
person's body is out of view for a typical wearable camera. We propose a
learning-based approach to estimate the camera wearer's 3D body pose from
egocentric video sequences. Our key insight is to leverage interactions with
another person---whose body pose we can directly observe---as a signal
inherently linked to the body pose of the first-person subject. We show that
since interactions between individuals often induce a well-ordered series of
back-and-forth responses, it is possible to learn a temporal model of the
interlinked poses even though one party is largely out of view. We demonstrate
our idea on a variety of domains with dyadic interaction and show the
substantial impact on egocentric body pose estimation, which improves the state
of the art. Video results are available at
http://vision.cs.utexas.edu/projects/you2me/","['Evonne Ng', 'Donglai Xiang', 'Hanbyul Joo', 'Kristen Grauman']",2019-04-22T13:58:49Z,http://arxiv.org/abs/1904.09882v2
DeepPerimeter: Indoor Boundary Estimation from Posed Monocular Sequences,"We present DeepPerimeter, a deep learning based pipeline for inferring a full
indoor perimeter (i.e. exterior boundary map) from a sequence of posed RGB
images. Our method relies on robust deep methods for depth estimation and wall
segmentation to generate an exterior boundary point cloud, and then uses deep
unsupervised clustering to fit wall planes to obtain a final boundary map of
the room. We demonstrate that DeepPerimeter results in excellent visual and
quantitative performance on the popular ScanNet and FloorNet datasets and works
for room shapes of various complexities as well as in multiroom scenarios. We
also establish important baselines for future work on indoor perimeter
estimation, topics which will become increasingly prevalent as application
areas like augmented reality and robotics become more significant.","['Ameya Phalak', 'Zhao Chen', 'Darvin Yi', 'Khushi Gupta', 'Vijay Badrinarayanan', 'Andrew Rabinovich']",2019-04-25T21:20:41Z,http://arxiv.org/abs/1904.11595v2
EM-Fusion: Dynamic Object-Level SLAM with Probabilistic Data Association,"The majority of approaches for acquiring dense 3D environment maps with RGB-D
cameras assumes static environments or rejects moving objects as outliers. The
representation and tracking of moving objects, however, has significant
potential for applications in robotics or augmented reality. In this paper, we
propose a novel approach to dynamic SLAM with dense object-level
representations. We represent rigid objects in local volumetric signed distance
function (SDF) maps, and formulate multi-object tracking as direct alignment of
RGB-D images with the SDF representations. Our main novelty is a probabilistic
formulation which naturally leads to strategies for data association and
occlusion handling. We analyze our approach in experiments and demonstrate that
our approach compares favorably with the state-of-the-art methods in terms of
robustness and accuracy.","['Michael Strecke', 'Jörg Stückler']",2019-04-26T11:54:50Z,http://arxiv.org/abs/1904.11781v2
The Mobile AR Sensor Logger for Android and iOS Devices,"In recent years, commodity mobile devices equipped with cameras and inertial
measurement units (IMUs) have attracted much research and design effort for
augmented reality (AR) and robotics applications. Based on such sensors, many
commercial AR toolkits and public benchmark datasets have been made available
to accelerate hatching and validating new ideas. To lower the difficulty and
enhance the flexibility in accessing the rich raw data of typical AR sensors on
mobile devices, this paper present the mobile AR sensor (MARS) logger for two
of the most popular mobile operating systems, Android and iOS. The logger
highlights the best possible synchronization between the camera and the IMU
allowed by a mobile device, and efficient saving of images at about 30Hz, and
recording the metadata relevant to AR applications. This logger has been tested
on a relatively large spectrum of mobile devices, and the collected data has
been used for analyzing the sensor characteristics. We see that this
application will facilitate research and development related to AR and
robotics, so it has been open sourced at
https://github.com/OSUPCVLab/mobile-ar-sensor-logger.","['Jianzhu Huai', 'Yujia Zhang', 'Alper Yilmaz']",2019-12-21T13:41:02Z,http://arxiv.org/abs/2001.00470v1
Foveated Haptic Gaze,"As digital worlds become ubiquitous via video games, simulations, virtual and
augmented reality, people with disabilities who cannot access those worlds are
becoming increasingly disenfranchised. More often than not the design of these
environments focuses on vision, making them inaccessible in whole or in part to
people with visual impairments. Accessible games and visual aids have been
developed but their lack of prevalence or unintuitive interfaces make them
impractical for daily use. To address this gap, we present Foveated Haptic
Gaze, a method for conveying visual information via haptics that is intuitive
and designed for interacting with real-time 3-dimensional environments. To
validate our approach we developed a prototype of the system along with a
simplified first-person shooter game. Lastly we present encouraging user study
results of both sighted and blind participants using our system to play the
game with no visual feedback.","['Bijan Fakhri', 'Troy McDaniel', 'Heni Ben Amor', 'Hemanth Venkateswara', 'Abhik Chowdhury', 'Sethuraman Panchanathan']",2020-01-07T00:35:23Z,http://arxiv.org/abs/2001.01824v3
"Communications-Caching-Computing Tradeoff Analysis for Bidirectional
  Data Computation in Mobile Edge Networks","With the advent of the modern mobile traffic, e.g., online gaming, augmented
reality delivery and etc., a novel bidirectional computation task model where
the input data of each task consists of two parts, one generated at the mobile
device in real-time and the other originated from the Internet proactively, is
emerging as an important use case of 5G. In this paper, for ease of analytical
analysis, we consider the homogeneous bidirectional computation task model in a
mobile edge network which consists of one mobile edge computing (MEC) server
and one mobile device, both enabled with computing and caching capabilities.
Each task can be served via three mechanisms, i.e., local computing with local
caching, local computing without local caching and computing at the MEC server.
To minimize the average bandwidth, we formulate the joint caching and computing
optimization problem under the latency, cache size and average power
constraints. We derive the closed-form expressions for the optimal policy and
the minimum bandwidth. The tradeoff among communications, computing and caching
is illustrated both analytically and numerically, which provides insightful
guideline for the network designers.","['Yaping Sun', 'Lyutianyang Zhang', 'Zhiyong Chen', 'Sumit Roy']",2020-01-12T15:23:28Z,http://arxiv.org/abs/2001.03946v1
DeepFactors: Real-Time Probabilistic Dense Monocular SLAM,"The ability to estimate rich geometry and camera motion from monocular
imagery is fundamental to future interactive robotics and augmented reality
applications. Different approaches have been proposed that vary in scene
geometry representation (sparse landmarks, dense maps), the consistency metric
used for optimising the multi-view problem, and the use of learned priors. We
present a SLAM system that unifies these methods in a probabilistic framework
while still maintaining real-time performance. This is achieved through the use
of a learned compact depth map representation and reformulating three different
types of errors: photometric, reprojection and geometric, which we make use of
within standard factor graph software. We evaluate our system on trajectory
estimation and depth reconstruction on real-world sequences and present various
examples of estimated dense geometry.","['Jan Czarnowski', 'Tristan Laidlow', 'Ronald Clark', 'Andrew J. Davison']",2020-01-14T21:08:51Z,http://arxiv.org/abs/2001.05049v1
Image Segmentation Using Deep Learning: A Survey,"Image segmentation is a key topic in image processing and computer vision
with applications such as scene understanding, medical image analysis, robotic
perception, video surveillance, augmented reality, and image compression, among
many others. Various algorithms for image segmentation have been developed in
the literature. Recently, due to the success of deep learning models in a wide
range of vision applications, there has been a substantial amount of works
aimed at developing image segmentation approaches using deep learning models.
In this survey, we provide a comprehensive review of the literature at the time
of this writing, covering a broad spectrum of pioneering works for semantic and
instance-level segmentation, including fully convolutional pixel-labeling
networks, encoder-decoder architectures, multi-scale and pyramid based
approaches, recurrent networks, visual attention models, and generative models
in adversarial settings. We investigate the similarity, strengths and
challenges of these deep learning models, examine the most widely used
datasets, report performances, and discuss promising future research directions
in this area.","['Shervin Minaee', 'Yuri Boykov', 'Fatih Porikli', 'Antonio Plaza', 'Nasser Kehtarnavaz', 'Demetri Terzopoulos']",2020-01-15T21:37:47Z,http://arxiv.org/abs/2001.05566v5
Geometric Proxies for Live RGB-D Stream Enhancement and Consolidation,"We propose a geometric superstructure for unified real-time processing of
RGB-D data. Modern RGB-D sensors are widely used for indoor 3D capture, with
applications ranging from modeling to robotics, through augmented reality.
Nevertheless, their use is limited by their low resolution, with frames often
corrupted with noise, missing data and temporal inconsistencies. Our approach
consists in generating and updating through time a single set of compact local
statistics parameterized over detected geometric proxies, which are fed from
raw RGB-D data. Our proxies provide several processing primitives, which
improve the quality of the RGB-D stream on the fly or lighten further
operations. Experimental results confirm that our lightweight analysis
framework copes well with embedded execution as well as moderate memory and
computational capabilities compared to state-of-the-art methods. Processing
RGB-D data with our proxies allows noise and temporal flickering removal, hole
filling and resampling. As a substitute of the observed scene, our proxies can
additionally be applied to compression and scene reconstruction. We present
experiments performed with our framework in indoor scenes of different natures
within a recent open RGB-D dataset.","['Adrien Kaiser', 'José Alonso Ybanez Zepeda', 'Tamy Boubekeur']",2020-01-21T14:42:35Z,http://arxiv.org/abs/2001.07577v1
"AprilTags 3D: Dynamic Fiducial Markers for Robust Pose Estimation in
  Highly Reflective Environments and Indirect Communication in Swarm Robotics","Although fiducial markers give an accurate pose estimation in laboratory
conditions, where the noisy factors are controlled, using them in field robotic
applications remains a challenge. This is constrained to the fiducial maker
systems, since they only work within the RGB image space. As a result, noises
in the image produce large pose estimation errors. In robotic applications,
fiducial markers have been mainly used in its original and simple form, as a
plane in a printed paper sheet. This setup is sufficient for basic visual
servoing and augmented reality applications, but not for complex swarm robotic
applications in which the setup consists of multiple dynamic markers (tags
displayed on LCD screen). This paper describes a novel methodology, called
AprilTags3D, that improves pose estimation accuracy of AprilTags in field
robotics with only RGB sensor by adding a third dimension to the marker
detector. Also, presents experimental results from applying the proposed
methodology to swarm autonomous robotic boats for latching between them and for
creating robotic formations.",['Luis A. Mateos'],2020-01-11T23:07:19Z,http://arxiv.org/abs/2001.08622v1
"Patient Specific Biomechanics Are Clinically Significant In Accurate
  Computer Aided Surgical Image Guidance","Augmented Reality is used in Image Guided surgery (AR IG) to fuse surgical
landmarks from preoperative images into a video overlay. Physical simulation is
essential to maintaining accurate position of the landmarks as surgery
progresses and ensuring patient safety by avoiding accidental damage to vessels
etc. In liver procedures, AR IG simulation accuracy is hampered by an inability
to model stiffness variations unique to the patients disease. We introduce a
novel method to account for patient specific stiffness variation based on
Magnetic Resonance Elastography (MRE) data. To the best of our knowledge we are
the first to demonstrate the use of in-vivo biomechanical data for AR IG
landmark placement. In this early work, a comparative evaluation of our MRE
data driven simulation and the traditional method shows clinically significant
differences in accuracy during landmark placement and motivates further animal
model trials.","['Michael Barrow', 'Alice Chao', 'Qizhi He', 'Sonia Ramamoorthy', 'Claude Sirlin', 'Ryan Kastner']",2020-01-29T08:11:07Z,http://arxiv.org/abs/2001.10717v1
View-invariant action recognition,"Human action recognition is an important problem in computer vision. It has a
wide range of applications in surveillance, human-computer interaction,
augmented reality, video indexing, and retrieval. The varying pattern of
spatio-temporal appearance generated by human action is key for identifying the
performed action. We have seen a lot of research exploring this dynamics of
spatio-temporal appearance for learning a visual representation of human
actions. However, most of the research in action recognition is focused on some
common viewpoints, and these approaches do not perform well when there is a
change in viewpoint. Human actions are performed in a 3-dimensional environment
and are projected to a 2-dimensional space when captured as a video from a
given viewpoint. Therefore, an action will have a different spatio-temporal
appearance from different viewpoints. The research in view-invariant action
recognition addresses this problem and focuses on recognizing human actions
from unseen viewpoints.","['Yogesh S Rawat', 'Shruti Vyas']",2020-09-01T18:08:46Z,http://arxiv.org/abs/2009.00638v1
Deep Learning Interference Cancellation in Wireless Networks,"With the crowding of the electromagnetic spectrum and the shrinking cell size
in wireless networks, crosstalk between base stations and users is a major
problem. Although hand-crafted functional blocks and coding schemes are proven
effective to guarantee reliable data transfer, currently deep learning-based
approaches have drawn increasing attention in the communication system
modeling. In this paper, we propose a Neural Network (NN) based signal
processing technique that works with traditional DSP algorithms to overcome the
interference problem in realtime. This technique doesn't require any feedback
protocol between the receiver and transmitter which makes it very suitable for
low-latency and high data-rate applications such as autonomy and augmented
reality. While there has been recent work on the use of Reinforcement Learning
(RL) in the control layer to manage and control the interference, our approach
is novel in the sense that it introduces a neural network for signal processing
at baseband data rate and in the physical layer. We demonstrate this ""Deep
Interference Cancellation"" technique using a convolutional LSTM autoencoder.
When applied to QAM-OFDM modulated data, the network produces significant
improvement in the symbol error rate (SER). We further discuss the hardware
implementation including latency, power consumption, memory requirements, and
chip area.","['Yiming Zhou', 'Ashkan Samiee', 'Tingyi Zhou', 'Bahram Jalali']",2020-09-11T17:09:35Z,http://arxiv.org/abs/2009.05533v1
WDRN : A Wavelet Decomposed RelightNet for Image Relighting,"The task of recalibrating the illumination settings in an image to a target
configuration is known as relighting. Relighting techniques have potential
applications in digital photography, gaming industry and in augmented reality.
In this paper, we address the one-to-one relighting problem where an image at a
target illumination settings is predicted given an input image with specific
illumination conditions. To this end, we propose a wavelet decomposed
RelightNet called WDRN which is a novel encoder-decoder network employing
wavelet based decomposition followed by convolution layers under a
muti-resolution framework. We also propose a novel loss function called gray
loss that ensures efficient learning of gradient in illumination along
different directions of the ground truth image giving rise to visually superior
relit images. The proposed solution won the first position in the relighting
challenge event in advances in image manipulation (AIM) 2020 workshop which
proves its effectiveness measured in terms of a Mean Perceptual Score which in
turn is measured using SSIM and a Learned Perceptual Image Patch Similarity
score.","['Densen Puthussery', 'Hrishikesh P. S.', 'Melvin Kuriakose', 'Jiji C. V']",2020-09-14T18:23:10Z,http://arxiv.org/abs/2009.06678v1
"Multifunctional Resonant Wavefront-Shaping Meta-Optics Based on
  Multilayer and Multi-Perturbation Nonlocal Metasurfaces","Photonic devices rarely provide both elaborate spatial control and sharp
spectral control over an incoming wavefront. In optical metasurfaces, for
example, the localized modes of individual meta-units govern the wavefront
shape over a broad bandwidth, while nonlocal lattice modes extended over many
unit cells support high quality-factor resonances. Here, we experimentally
demonstrate nonlocal dielectric metasurfaces in the near-infrared that offer
both spatial and spectral control of light, realizing metalenses focusing light
exclusively over a narrowband resonance while leaving off-resonant frequencies
unaffected. Our devices attain this functionality by supporting a quasi-bound
state in the continuum encoded with a spatially varying geometric phase. We
leverage this capability to experimentally realize a versatile platform for
multispectral wavefront shaping where a stack of metasurfaces, each supporting
multiple independently controlled quasi-bound states in the continuum, molds
the optical wavefront distinctively at multiple wavelengths and yet stay
transparent over the rest of the spectrum. Such a platform is scalable to the
visible for applications in augmented reality and transparent displays.","['Stephanie C. Malek', 'Adam C. Overvig', 'Andrea Alù', 'Nanfang Yu']",2020-09-15T13:07:14Z,http://arxiv.org/abs/2009.07054v2
Edge Learning with Timeliness Constraints: Challenges and Solutions,"Future machine learning (ML) powered applications, such as autonomous driving
and augmented reality, involve training and inference tasks with timeliness
requirements and are communication and computation intensive, which demands for
the edge learning framework. The real-time requirements drive us to go beyond
accuracy for ML. In this article, we introduce the concept of timely edge
learning, aiming to achieve accurate training and inference while minimizing
the communication and computation delay. We discuss key challenges and propose
corresponding solutions from data, model and resource management perspectives
to meet the timeliness requirements. Particularly, for edge training, we argue
that the total training delay rather than rounds should be considered, and
propose data or model compression, and joint device scheduling and resource
management schemes for both centralized training and federated learning
systems. For edge inference, we explore the dependency between accuracy and
delay for communication and computation, and propose dynamic data compression
and flexible pruning schemes. Two case studies show that the timeliness
performances, including the training accuracy under a given delay budget and
the completion ratio of inference tasks within deadline, are highly improved
with the proposed solutions.","['Yuxuan Sun', 'Wenqi Shi', 'Xiufeng Huang', 'Sheng Zhou', 'Zhisheng Niu']",2020-09-23T11:38:47Z,http://arxiv.org/abs/2009.11065v1
"A Comprehensive Survey of the Tactile Internet: State of the art and
  Research Directions","The Internet has made several giant leaps over the years, from a fixed to a
mobile Internet, then to the Internet of Things, and now to a Tactile Internet.
The Tactile Internet goes far beyond data, audio and video delivery over fixed
and mobile networks, and even beyond allowing communication and collaboration
among things. It is expected to enable haptic communication and allow skill set
delivery over networks. Some examples of potential applications are
tele-surgery, vehicle fleets, augmented reality and industrial process
automation. Several papers already cover many of the Tactile Internet-related
concepts and technologies, such as haptic codecs, applications, and supporting
technologies. However, none of them offers a comprehensive survey of the
Tactile Internet, including its architectures and algorithms. Furthermore, none
of them provides a systematic and critical review of the existing solutions. To
address these lacunae, we provide a comprehensive survey of the architectures
and algorithms proposed to date for the Tactile Internet. In addition, we
critically review them using a well-defined set of requirements and discuss
some of the lessons learned as well as the most promising research directions.","['N. Promwongsa', 'A. Ebrahimzadeh', 'D. Naboulsi', 'S. Kianpisheh', 'F. Belqasmi', 'R. Glitho', 'N. Crespi', 'O. Alfandi']",2020-09-22T14:45:37Z,http://arxiv.org/abs/2009.12164v1
SwiftFace: Real-Time Face Detection,"Computer vision is a field of artificial intelligence that trains computers
to interpret the visual world in a way similar to that of humans. Due to the
rapid advancements in technology and the increasing availability of
sufficiently large training datasets, the topics within computer vision have
experienced a steep growth in the last decade. Among them, one of the most
promising fields is face detection. Being used daily in a wide variety of
fields; from mobile apps and augmented reality for entertainment purposes, to
social studies and security cameras; designing high-performance models for face
detection is crucial. On top of that, with the aforementioned growth in face
detection technologies, precision and accuracy are no longer the only relevant
factors: for real-time face detection, speed of detection is essential.
SwiftFace is a novel deep learning model created solely to be a fast face
detection model. By focusing only on detecting faces, SwiftFace performs 30%
faster than current state-of-the-art face detection models. Code available at
https://github.com/leo7r/swiftface","['Leonardo Ramos', 'Bernardo Morales']",2020-09-29T03:09:29Z,http://arxiv.org/abs/2009.13743v1
Non-relativistic twistor theory and Newton--Cartan geometry,"We develop a non-relativistic twistor theory, in which Newton--Cartan
structures of Newtonian gravity correspond to complex three-manifolds with a
four-parameter family of rational curves with normal bundle ${\mathcal
O}\oplus{\mathcal O}(2)$. We show that the Newton--Cartan space-times are
unstable under the general Kodaira deformation of the twistor complex
structure. The Newton--Cartan connections can nevertheless be reconstructed
from Merkulov's generalisation of the Kodaira map augmented by a choice of a
holomorphic line bundle over the twistor space trivial on twistor lines. The
Coriolis force may be incorporated by holomorphic vector bundles, which in
general are non--trivial on twistor lines. The resulting geometries agree with
non--relativistic limits of anti-self-dual gravitational instantons.","['Maciej Dunajski', 'James Gundry']",2015-02-10T18:47:38Z,http://arxiv.org/abs/1502.03034v2
"Efficient Hand Articulations Tracking using Adaptive Hand Model and
  Depth map","Real-time hand articulations tracking is important for many applications such
as interacting with virtual / augmented reality devices or tablets. However,
most of existing algorithms highly rely on expensive and high power-consuming
GPUs to achieve real-time processing. Consequently, these systems are
inappropriate for mobile and wearable devices. In this paper, we propose an
efficient hand tracking system which does not require high performance GPUs. In
our system, we track hand articulations by minimizing discrepancy between depth
map from sensor and computer-generated hand model. We also initialize hand pose
at each frame using finger detection and classification. Our contributions are:
(a) propose adaptive hand model to consider different hand shapes of users
without generating personalized hand model; (b) improve the highly efficient
frame initialization for robust tracking and automatic initialization; (c)
propose hierarchical random sampling of pixels from each depth map to improve
tracking accuracy while limiting required computations. To the best of our
knowledge, it is the first system that achieves both automatic hand model
adjustment and real-time tracking without using GPUs.","['Byeongkeun Kang', 'Yeejin Lee', 'Truong Q. Nguyen']",2015-10-04T20:34:15Z,http://arxiv.org/abs/1510.00981v3
"Egocentric Field-of-View Localization Using First-Person Point-of-View
  Devices","We present a technique that uses images, videos and sensor data taken from
first-person point-of-view devices to perform egocentric field-of-view (FOV)
localization. We define egocentric FOV localization as capturing the visual
information from a person's field-of-view in a given environment and
transferring this information onto a reference corpus of images and videos of
the same space, hence determining what a person is attending to. Our method
matches images and video taken from the first-person perspective with the
reference corpus and refines the results using the first-person's head
orientation information obtained using the device sensors. We demonstrate
single and multi-user egocentric FOV localization in different indoor and
outdoor environments with applications in augmented reality, event
understanding and studying social interactions.","['Vinay Bettadapura', 'Irfan Essa', 'Caroline Pantofaru']",2015-10-07T19:39:26Z,http://arxiv.org/abs/1510.02073v1
Cardea: Context-Aware Visual Privacy Protection from Pervasive Cameras,"The growing popularity of mobile and wearable devices with built-in cameras,
the bright prospect of camera related applications such as augmented reality
and life-logging system, the increased ease of taking and sharing photos, and
advances in computer vision techniques have greatly facilitated people's lives
in many aspects, but have also inevitably raised people's concerns about visual
privacy at the same time. Motivated by recent user studies that people's
privacy concerns are dependent on the context, in this paper, we propose
Cardea, a context-aware and interactive visual privacy protection framework
that enforces privacy protection according to people's privacy preferences. The
framework provides people with fine-grained visual privacy protection using: i)
personal privacy profiles, with which people can define their context-dependent
privacy preferences; and ii) visual indicators: face features, for devices to
automatically locate individuals who request privacy protection; and iii) hand
gestures, for people to flexibly interact with cameras to temporarily change
their privacy preferences. We design and implement the framework consisting of
the client app on Android devices and the cloud server. Our evaluation results
confirm this framework is practical and effective with 86% overall accuracy,
showing promising future for context-aware visual privacy protection from
pervasive cameras.","['Jiayu Shu', 'Rui Zheng', 'Pan Hui']",2016-10-04T08:01:27Z,http://arxiv.org/abs/1610.00889v1
"Real-time Joint Tracking of a Hand Manipulating an Object from RGB-D
  Input","Real-time simultaneous tracking of hands manipulating and interacting with
external objects has many potential applications in augmented reality, tangible
computing, and wearable computing. However, due to difficult occlusions, fast
motions, and uniform hand appearance, jointly tracking hand and object pose is
more challenging than tracking either of the two separately. Many previous
approaches resort to complex multi-camera setups to remedy the occlusion
problem and often employ expensive segmentation and optimization steps which
makes real-time tracking impossible. In this paper, we propose a real-time
solution that uses a single commodity RGB-D camera. The core of our approach is
a 3D articulated Gaussian mixture alignment strategy tailored to hand-object
tracking that allows fast pose optimization. The alignment energy uses novel
regularizers to address occlusions and hand-object contacts. For added
robustness, we guide the optimization with discriminative part classification
of the hand and segmentation of the object. We conducted extensive experiments
on several existing datasets and introduce a new annotated hand-object dataset.
Quantitative and qualitative results show the key advantages of our method:
speed, accuracy, and robustness.","['Srinath Sridhar', 'Franziska Mueller', 'Michael Zollhöfer', 'Dan Casas', 'Antti Oulasvirta', 'Christian Theobalt']",2016-10-16T17:11:58Z,http://arxiv.org/abs/1610.04889v1
MultiCol-SLAM - A Modular Real-Time Multi-Camera SLAM System,"The basis for most vision based applications like robotics, self-driving cars
and potentially augmented and virtual reality is a robust, continuous
estimation of the position and orientation of a camera system w.r.t the
observed environment (scene). In recent years many vision based systems that
perform simultaneous localization and mapping (SLAM) have been presented and
released as open source. In this paper, we extend and improve upon a
state-of-the-art SLAM to make it applicable to arbitrary, rigidly coupled
multi-camera systems (MCS) using the MultiCol model. In addition, we include a
performance evaluation on accurate ground truth and compare the robustness of
the proposed method to a single camera version of the SLAM system. An open
source implementation of the proposed multi-fisheye camera SLAM system can be
found on-line https://github.com/urbste/MultiCol-SLAM.","['Steffen Urban', 'Stefan Hinz']",2016-10-24T09:27:47Z,http://arxiv.org/abs/1610.07336v1
Learnable Visual Markers,"We propose a new approach to designing visual markers (analogous to QR-codes,
markers for augmented reality, and robotic fiducial tags) based on the advances
in deep generative networks. In our approach, the markers are obtained as color
images synthesized by a deep network from input bit strings, whereas another
deep network is trained to recover the bit strings back from the photos of
these markers. The two networks are trained simultaneously in a joint
backpropagation process that takes characteristic photometric and geometric
distortions associated with marker fabrication and marker scanning into
account. Additionally, a stylization loss based on statistics of activations in
a pretrained classification network can be inserted into the learning in order
to shift the marker appearance towards some texture prototype. In the
experiments, we demonstrate that the markers obtained using our approach are
capable of retaining bit strings that are long enough to be practical. The
ability to automatically adapt markers according to the usage scenario and the
desired capacity as well as the ability to combine information encoding with
artistic stylization are the unique properties of our approach. As a byproduct,
our approach provides an insight on the structure of patterns that are most
suitable for recognition by ConvNets and on their ability to distinguish
composite patterns.","['Oleg Grinchuk', 'Vadim Lebedev', 'Victor Lempitsky']",2016-10-28T14:31:02Z,http://arxiv.org/abs/1610.09237v1
Virtual Blood Vessels in Complex Background using Stereo X-ray Images,"We propose a fully automatic system to reconstruct and visualize 3D blood
vessels in Augmented Reality (AR) system from stereo X-ray images with bones
and body fat. Currently, typical 3D imaging technologies are expensive and
carrying the risk of irradiation exposure. To reduce the potential harm, we
only need to take two X-ray images before visualizing the vessels. Our system
can effectively reconstruct and visualize vessels in following steps. We first
conduct initial segmentation using Markov Random Field and then refine
segmentation in an entropy based post-process. We parse the segmented vessels
by extracting their centerlines and generating trees. We propose a
coarse-to-fine scheme for stereo matching, including initial matching using
affine transform and dense matching using Hungarian algorithm guided by
Gaussian regression. Finally, we render and visualize the reconstructed model
in a HoloLens based AR system, which can essentially change the way of
visualizing medical data. We have evaluated its performance by using synthetic
and real stereo X-ray images, and achieved satisfactory quantitative and
qualitative results.","['Qiuyu Chen', 'Ryoma Bise', 'Lin Gu', 'Yinqiang Zheng', 'Imari Sato', 'Jenq-Neng Hwang', 'Nobuaki Imanishi', 'Sadakazu Aiso']",2017-09-22T00:43:55Z,http://arxiv.org/abs/1709.07551v1
Smart Guiding Glasses for Visually Impaired People in Indoor Environment,"To overcome the travelling difficulty for the visually impaired group, this
paper presents a novel ETA (Electronic Travel Aids)-smart guiding device in the
shape of a pair of eyeglasses for giving these people guidance efficiently and
safely. Different from existing works, a novel multi sensor fusion based
obstacle avoiding algorithm is proposed, which utilizes both the depth sensor
and ultrasonic sensor to solve the problems of detecting small obstacles, and
transparent obstacles, e.g. the French door. For totally blind people, three
kinds of auditory cues were developed to inform the direction where they can go
ahead. Whereas for weak sighted people, visual enhancement which leverages the
AR (Augment Reality) technique and integrates the traversable direction is
adopted. The prototype consisting of a pair of display glasses and several low
cost sensors is developed, and its efficiency and accuracy were tested by a
number of users. The experimental results show that the smart guiding glasses
can effectively improve the user's travelling experience in complicated indoor
environment. Thus it serves as a consumer device for helping the visually
impaired people to travel safely.","['Jinqiang Bai', 'Shiguo Lian', 'Zhaoxiang Liu', 'Kai Wang', 'Dijun Liu']",2017-09-27T06:58:20Z,http://arxiv.org/abs/1709.09359v1
Semantic Visual Localization,"Robust visual localization under a wide range of viewing conditions is a
fundamental problem in computer vision. Handling the difficult cases of this
problem is not only very challenging but also of high practical relevance,
e.g., in the context of life-long localization for augmented reality or
autonomous robots. In this paper, we propose a novel approach based on a joint
3D geometric and semantic understanding of the world, enabling it to succeed
under conditions where previous approaches failed. Our method leverages a novel
generative model for descriptor learning, trained on semantic scene completion
as an auxiliary task. The resulting 3D descriptors are robust to missing
observations by encoding high-level 3D geometric and semantic information.
Experiments on several challenging large-scale localization datasets
demonstrate reliable localization under extreme viewpoint, illumination, and
geometry changes.","['Johannes L. Schönberger', 'Marc Pollefeys', 'Andreas Geiger', 'Torsten Sattler']",2017-12-15T18:02:47Z,http://arxiv.org/abs/1712.05773v2
"A Neural Network Approach to Missing Marker Reconstruction in Human
  Motion Capture","Optical motion capture systems have become a widely used technology in
various fields, such as augmented reality, robotics, movie production, etc.
Such systems use a large number of cameras to triangulate the position of
optical markers.The marker positions are estimated with high accuracy. However,
especially when tracking articulated bodies, a fraction of the markers in each
timestep is missing from the reconstruction. In this paper, we propose to use a
neural network approach to learn how human motion is temporally and spatially
correlated, and reconstruct missing markers positions through this model. We
experiment with two different models, one LSTM-based and one time-window-based.
Both methods produce state-of-the-art results, while working online, as opposed
to most of the alternative methods, which require the complete sequence to be
known. The implementation is publicly available at
https://github.com/Svito-zar/NN-for-Missing-Marker-Reconstruction .","['Taras Kucherenko', 'Jonas Beskow', 'Hedvig Kjellström']",2018-03-07T14:16:59Z,http://arxiv.org/abs/1803.02665v4
5G Control Channel Design for Ultra-Reliable Low-Latency Communications,"The fifth generation (5G) of wireless systems holds the promise of supporting
a wide range of services with different communication requirements.
Ultra-reliable low-latency communications (URLLC) is a generic service that
enables mission-critical applications, such as industrial automation, augmented
reality, and vehicular communications. URLLC has stringent requirements for
reliability and latency of delivering both data and control information. In
order to meet these requirements, the Third Generation Partnership Project
(3GPP) has been introducing new features to the upcoming releases of the
cellular system standards, namely releases 15 and beyond. This article reviews
some of these features and introduces new enhancements for designing the
control channels to efficiently support the URLLC. In particular, a flexible
slot structure is presented as a solution to detect a failure in delivering the
control information at an early stage, thereby allowing timely retransmission
of the control information. Finally, some remaining challenges and envisioned
research directions are discussed for shaping the 5G new radio (NR) as a
unified wireless access technology for supporting different services.","['Hamidreza Shariatmadari', 'Sassan Iraji', 'Riku Jantti', 'Petar Popovski', 'Zexian Li', 'Mikko A. Uusitalo']",2018-03-12T07:28:13Z,http://arxiv.org/abs/1803.04139v1
Generalized Scene Reconstruction,"A new passive approach called Generalized Scene Reconstruction (GSR) enables
""generalized scenes"" to be effectively reconstructed. Generalized scenes are
defined to be ""boundless"" spaces that include non-Lambertian, partially
transmissive, textureless and finely-structured matter. A new data structure
called a plenoptic octree is introduced to enable efficient (database-like)
light and matter field reconstruction in devices such as mobile phones,
augmented reality (AR) glasses and drones. To satisfy threshold requirements
for GSR accuracy, scenes are represented as systems of partially polarized
light, radiometrically interacting with matter. To demonstrate GSR, a prototype
imaging polarimeter is used to reconstruct (in generalized light fields) highly
reflective, hail-damaged automobile body panels. Follow-on GSR experiments are
described.","['John K. Leffingwell', 'Donald J. Meagher', 'Khan W. Mahmud', 'Scott Ackerson']",2018-03-22T17:59:19Z,http://arxiv.org/abs/1803.08496v3
"Weighted Multi-projection: 3D Point Cloud Denoising with Estimated
  Tangent Planes","As a collection of 3D points sampled from surfaces of objects, a 3D point
cloud is widely used in robotics, autonomous driving and augmented reality. Due
to the physical limitations of 3D sensing devices, 3D point clouds are usually
noisy, which influences subsequent computations, such as surface
reconstruction, recognition and many others. To denoise a 3D point cloud, we
present a novel algorithm, called weighted multi-projection. Compared to many
previous works on denoising, instead of directly smoothing the coordinates of
3D points, we use a two-fold smoothing: We first estimate a local tangent plane
at each 3D point and then reconstruct each 3D point by weighted averaging of
its projections on multiple tangent planes. We also provide the theoretical
analysis for the surface normal estimation and achieve a tighter bound than in
a previous work. We validate the empirical performance on the dataset of
ShapeNetCore and show that weighted multi-projection outperforms its
competitors in all nine classes.","['Chaojing Duan', 'Siheng Chen', 'Jelena Kovačević']",2018-07-01T01:42:04Z,http://arxiv.org/abs/1807.00253v1
"Modernization of Professional Training of Electromechanics Bachelors:
  ICT-based Competence Approach","Analysis of the standards for the preparation of electromechanics in Ukraine
showed that the electromechanic engineer is able to solve complex specialized
problems and practical problems in a certain area of professional activity or
in the process of study. These problems are characterized by complexity and
uncertainty of conditions. The main competencies include social-personal,
general-scientific, instrumental, general-professional and
specialized-professional. A review of scientific publications devoted to the
training of electromechanics has shown that four branches of engineering are
involved in the training of electromechanical engineers: mechanical and
electrical engineering (with a common core of electromechanics), electronic
engineering and automation. The common use of the theory, methods and means of
these industries leads to the emergence of a combined field of engineering -
mechatronics. Summarizing the experience of electrical engineers professional
training in Ukraine and abroad makes it possible to determine the main
directions of their professional training modernization.","['Yevhenii O. Modlo', 'Serhiy O. Semerikov', 'Ekaterina O. Shmeltzer']",2018-07-02T17:55:08Z,http://arxiv.org/abs/1807.00803v2
"Defining the Structure of Environmental Competence of Future Mining
  Engineers: ICT Approach","The object is to the reasonable selection of the ICT tools for formation of
ecological competence. Pressing task is constructive and research approach to
preparation of future engineers to performance of professional duties in order
to make them capable to develop engineering projects independently and exercise
control competently. Subject of research: the theoretical justification of
competence system of future mining engineers. Methods: source analysis on the
problem of ecological competence formation. Results: defining the structure of
environmental competence of future mining engineers. Conclusion: the relevance
of the material covered in the article, due to the need to ensure the
effectiveness of the educational process in the preparation of the future
mining engineers.","['Vladimir S. Morkun', 'Serhiy O. Semerikov', 'Nataliya V. Morkun', 'Svitlana M. Hryshchenko', 'Arnold E. Kiv']",2018-07-02T17:59:44Z,http://arxiv.org/abs/1807.00805v2
"Robust and Scalable Differentiable Neural Computer for Question
  Answering","Deep learning models are often not easily adaptable to new tasks and require
task-specific adjustments. The differentiable neural computer (DNC), a
memory-augmented neural network, is designed as a general problem solver which
can be used in a wide range of tasks. But in reality, it is hard to apply this
model to new tasks. We analyze the DNC and identify possible improvements
within the application of question answering. This motivates a more robust and
scalable DNC (rsDNC). The objective precondition is to keep the general
character of this model intact while making its application more reliable and
speeding up its required training time. The rsDNC is distinguished by a more
robust training, a slim memory unit and a bidirectional architecture. We not
only achieve new state-of-the-art performance on the bAbI task, but also
minimize the performance variance between different initializations.
Furthermore, we demonstrate the simplified applicability of the rsDNC to new
tasks with passable results on the CNN RC task without adaptions.","['Jörg Franke', 'Jan Niehues', 'Alex Waibel']",2018-07-07T12:44:32Z,http://arxiv.org/abs/1807.02658v1
The Double Sphere Camera Model,"Vision-based motion estimation and 3D reconstruction, which have numerous
applications (e.g., autonomous driving, navigation systems for airborne devices
and augmented reality) are receiving significant research attention. To
increase the accuracy and robustness, several researchers have recently
demonstrated the benefit of using large field-of-view cameras for such
applications. In this paper, we provide an extensive review of existing models
for large field-of-view cameras. For each model we provide projection and
unprojection functions and the subspace of points that result in valid
projection. Then, we propose the Double Sphere camera model that well fits with
large field-of-view lenses, is computationally inexpensive and has a
closed-form inverse. We evaluate the model using a calibration dataset with
several different lenses and compare the models using the metrics that are
relevant for Visual Odometry, i.e., reprojection error, as well as computation
time for projection and unprojection functions and their Jacobians. We also
provide qualitative results and discuss the performance of all models.","['Vladyslav Usenko', 'Nikolaus Demmel', 'Daniel Cremers']",2018-07-24T08:42:00Z,http://arxiv.org/abs/1807.08957v2
Radiative Transport Based Flame Volume Reconstruction from Videos,"We introduce a novel approach for flame volume reconstruction from videos
using inexpensive charge-coupled device (CCD) consumer cameras. The approach
includes an economical data capture technique using inexpensive CCD cameras.
Leveraging the smear feature of the CCD chip, we present a technique for
synchronizing CCD cameras while capturing flame videos from different views.
Our reconstruction is based on the radiative transport equation which enables
complex phenomena such as emission, extinction, and scattering to be used in
the rendering process. Both the color intensity and temperature reconstructions
are implemented using the CUDA parallel computing framework, which provides
real-time performance and allows visualization of reconstruction results after
every iteration. We present the results of our approach using real captured
data and physically-based simulated data. Finally, we also compare our approach
against the other state-of-the-art flame volume reconstruction methods and
demonstrate the efficacy and efficiency of our approach in four different
applications: (1) rendering of reconstructed flames in virtual environments,
(2) rendering of reconstructed flames in augmented reality, (3) flame
stylization, and (4) reconstruction of other semitransparent phenomena.","['Liang Shen', 'Dengming Zhu', 'Saad Nadeem', 'Zhaoqi Wang', 'Arie Kaufman']",2018-09-17T19:57:06Z,http://arxiv.org/abs/1809.06417v1
Empty Cities: Image Inpainting for a Dynamic-Object-Invariant Space,"In this paper we present an end-to-end deep learning framework to turn images
that show dynamic content, such as vehicles or pedestrians, into realistic
static frames. This objective encounters two main challenges: detecting all the
dynamic objects, and inpainting the static occluded background with plausible
imagery. The second problem is approached with a conditional generative
adversarial model that, taking as input the original dynamic image and its
dynamic/static binary mask, is capable of generating the final static image.
The former challenge is addressed by the use of a convolutional network that
learns a multi-class semantic segmentation of the image.
  These generated images can be used for applications such as augmented reality
or vision-based robot localization purposes. To validate our approach, we show
both qualitative and quantitative comparisons against other state-of-the-art
inpainting methods by removing the dynamic objects and hallucinating the static
structure behind them. Furthermore, to demonstrate the potential of our
results, we carry out pilot experiments that show the benefits of our proposal
for visual place recognition.","['Berta Bescos', 'José Neira', 'Roland Siegwart', 'Cesar Cadena']",2018-09-20T08:13:52Z,http://arxiv.org/abs/1809.10239v2
"Assessing Performance of Aerobic Routines using Background Subtraction
  and Intersected Image Region","It is recommended for a novice to engage a trained and experience person,
i.e., a coach before starting an unfamiliar aerobic or weight routine. The
coach's task is to provide real-time feedbacks to ensure that the routine is
performed in a correct manner. This greatly reduces the risk of injury and
maximise physical gains. We present a simple image similarity measure based on
intersected image region to assess a subject's performance of an aerobic
routine. The method is implemented inside an Augmented Reality (AR) desktop app
that employs a single RGB camera to capture still images of the subject as he
or she progresses through the routine. The background-subtracted body pose
image is compared against the exemplar body pose image (i.e., AR template) at
specific intervals. Based on a limited dataset, our pose matching function is
reported to have an accuracy of 93.67%.","['Faustine John', 'Irwandi Hipiny', 'Hamimah Ujir', 'Mohd Shahrizal Sunar']",2018-10-03T02:04:15Z,http://arxiv.org/abs/1810.01564v1
Low Power Depth Estimation of Rigid Objects for Time-of-Flight Imaging,"Depth sensing is useful in a variety of applications that range from
augmented reality to robotics. Time-of-flight (TOF) cameras are appealing
because they obtain dense depth measurements with minimal latency. However, for
many battery-powered devices, the illumination source of a TOF camera is power
hungry and can limit the battery life of the device. To address this issue, we
present an algorithm that lowers the power for depth sensing by reducing the
usage of the TOF camera and estimating depth maps using concurrently collected
images. Our technique also adaptively controls the TOF camera and enables it
when an accurate depth map cannot be estimated. To ensure that the overall
system power for depth sensing is reduced, we design our algorithm to run on a
low power embedded platform, where it outputs 640x480 depth maps at 30 frames
per second. We evaluate our approach on several RGB-D datasets, where it
produces depth maps with an overall mean relative error of 0.96% and reduces
the usage of the TOF camera by 85%. When used with commercial TOF cameras, we
estimate that our algorithm can lower the total power for depth sensing by up
to 73%.","['James Noraky', 'Vivienne Sze']",2018-10-03T19:43:12Z,http://arxiv.org/abs/1810.01930v2
"A broadband achromatic polarization-insensitive metalens consisting of
  anisotropic nanostructures","Metasurfaces have attracted widespread attention due to an increasing demand
of compact and wearable optical devices. For many applications,
polarization-insensitive metasurfaces are highly desirable and appear to limit
the choice of their constituent elements to isotropic nanostructures. This
greatly restricts the degrees of geometric parameters available in designing
each nanostructure. Here, we demonstrate a polarization-insensitive metalens
using otherwise anisotropic nanofins which offer additional control over the
dispersion and phase of the output light. As a result, we can render a metalens
achromatic and polarization-insensitive across nearly the entire visible
spectrum from wavelength 460 nm to 700 nm, while maintaining
diffraction-limited performance. The metalens is comprised of just a single
layer of TiO2 nanofins and has a numerical aperture of 0.2 with a diameter of
26.4 um. The generality of our polarization-insensitive design allows it to be
implemented in a plethora of other metasurface devices with applications
ranging from imaging to virtual/augmented reality.","['Wei Ting Chen', 'Alexander Y. Zhu', 'Jared Sisler', 'Zameer Bharwani', 'Federico Capasso']",2018-10-11T14:40:52Z,http://arxiv.org/abs/1810.05050v1
"Path Loss Characterization for Intra-Vehicle Wearable Deployments at 60
  GHz","In this work, we present the results of a wideband measurement campaign at 60
GHz conducted inside a Linkker electric city bus. Targeting prospective
millimeter-wave (mmWave) public transportation wearable scenarios, we mimic a
typical deployment of mobile high-end consumer devices in a dense environment.
Specifically, our intra-vehicle deployment includes one receiver and multiple
transmitters corresponding to a mmWave access point and passengers' wearable
and hand-held devices. While the receiver is located in the front part of the
bus, the transmitters repeat realistic locations of personal devices (i) at the
seat level (e.g., a hand-held device) and (ii) at a height 70 cm above the seat
(e.g., a wearable device: augmented reality glasses or a head-mounted display).
Based on the measured received power, we construct a logarithmic model for the
distance-dependent path loss. The parametrized models developed in the course
of this study have the potential to become an attractive ground for the link
budget estimation and interference footprint studies in crowded public
transportation scenarios.","['Vasilii Semkin', 'Aleksei Ponomarenko-Timofeev', 'Aki Karttunen', 'Olga Galinina', 'Sergey Andreev', 'Yevgeni Koucheryavy']",2019-01-31T15:55:50Z,http://arxiv.org/abs/1902.01949v1
"wavEMS: Improving Signal Variation Freedom of Electrical Muscle
  Stimulation","There has been a long history in electrical muscle stimulation (EMS), which
has been used for medical and interaction purposes. Human-computer interaction
(HCI) researchers are now working on various applications, including virtual
reality (VR), notification, and learning. For the electric signals applied to
the human body, various types of waveforms have been considered and tested. In
typical applications, pulses with short duration are applied, however, many
perspectives are required to be considered. In addition to the duration and
polarity of the pulse/waves, the wave shapes can also be an essential factor to
consider. A problem of conventional EMS toolkits and systems are that they have
a limitation to the variety of signals that it can produce. For example, some
may be limited to monophonic pulses. Furthermore, they are usually limited to
rectangular pulses and a limited range of frequencies, and other waveforms
cannot be produced. These kinds of limitations make us challenging to consider
variations of EMS signals in HCI research and applications. The purpose of
""{\it wavEMS}"" is to encourage testing of a variety of waveforms for EMS, which
can be manipulated through audio output. We believe that this can help improve
HCI applications, and to open up new application areas.","['Michinari Kono', 'Jun Rekimoto']",2019-02-08T16:46:43Z,http://arxiv.org/abs/1902.03184v1
Learning Physics-guided Face Relighting under Directional Light,"Relighting is an essential step in realistically transferring objects from a
captured image into another environment. For example, authentic telepresence in
Augmented Reality requires faces to be displayed and relit consistent with the
observer's scene lighting. We investigate end-to-end deep learning
architectures that both de-light and relight an image of a human face. Our
model decomposes the input image into intrinsic components according to a
diffuse physics-based image formation model. We enable non-diffuse effects
including cast shadows and specular highlights by predicting a residual
correction to the diffuse render. To train and evaluate our model, we collected
a portrait database of 21 subjects with various expressions and poses. Each
sample is captured in a controlled light stage setup with 32 individual light
sources. Our method creates precise and believable relighting results and
generalizes to complex illumination conditions and challenging poses, including
when the subject is not looking straight at the camera.","['Thomas Nestmeyer', 'Jean-François Lalonde', 'Iain Matthews', 'Andreas M. Lehrmann']",2019-06-07T23:16:34Z,http://arxiv.org/abs/1906.03355v2
Fast Spatially-Varying Indoor Lighting Estimation,"We propose a real-time method to estimate spatiallyvarying indoor lighting
from a single RGB image. Given an image and a 2D location in that image, our
CNN estimates a 5th order spherical harmonic representation of the lighting at
the given location in less than 20ms on a laptop mobile graphics card. While
existing approaches estimate a single, global lighting representation or
require depth as input, our method reasons about local lighting without
requiring any geometry information. We demonstrate, through quantitative
experiments including a user study, that our results achieve lower lighting
estimation errors and are preferred by users over the state-of-the-art. Our
approach can be used directly for augmented reality applications, where a
virtual object is relit realistically at any position in the scene in
real-time.","['Mathieu Garon', 'Kalyan Sunkavalli', 'Sunil Hadap', 'Nathan Carr', 'Jean-François Lalonde']",2019-06-10T05:25:58Z,http://arxiv.org/abs/1906.03799v1
"Egocentric affordance detection with the one-shot geometry-driven
  Interaction Tensor","In this abstract we describe recent [4,7] and latest work on the
determination of affordances in visually perceived 3D scenes. Our method builds
on the hypothesis that geometry on its own provides enough information to
enable the detection of significant interaction possibilities in the
environment. The motivation behind this is that geometric information is
intimately related to the physical interactions afforded by objects in the
world. The approach uses a generic representation for the interaction between
everyday objects such as a mug or an umbrella with the environment, and also
for more complex affordances such as humans Sitting or Riding a motorcycle.
Experiments with synthetic and real RGB-D scenes show that the representation
enables the prediction of affordance candidate locations in novel environments
at fast rates and from a single (one-shot) training example. The determination
of affordances is a crucial step towards systems that need to perceive and
interact with their surroundings. We here illustrate output on two cases for a
simulated robot and for an Augmented Reality setting, both perceiving in an
egocentric manner.","['Eduardo Ruiz', 'Walterio Mayol-Cuevas']",2019-06-13T16:28:54Z,http://arxiv.org/abs/1906.05794v1
"CoAug-MR: An MR-based Interactive Office Workstation Design System via
  Augmented Multi-Person Collaboration","Digital prototyping and evaluation using 3D modeling and digital human models
are becoming more practical for customizing products to the preference of a
user. However, the 3D modeling is less accessible to casual users, and digital
human models suffer from insufficient body data and less intuitive illustration
on how people use the product or how it accommodates to their body. Recently,
VR-supported 'Do It Yourself' design has achieved real-time ergonomic
evaluation with users themselves by capturing their poses, however, it lacks
reliability and quality of design. In this paper, we explore a multi-person
interactive design approach that enables designers, users, and even ergonomists
to collaborate to achieve effective and reliable design and prototyping tasks.
Mixed Reality that utilizes Hololens and motion tracking devices had been
developed to provide instant design feedback and evaluation and to experience
prototyping in physical space. We evaluate the system based on the usability
study, where casual users and designers are engaged in the interactive process
of designing items with respect to the body information, the preference, and
the environment.","['Lin Wang', 'Kuk-Jin Yoon']",2019-07-06T10:19:04Z,http://arxiv.org/abs/1907.03107v3
"calibDB: enabling web based computer vision through on-the-fly camera
  calibration","For many computer vision applications, the availability of camera calibration
data is crucial as overall quality heavily depends on it. While calibration
data is available on some devices through Augmented Reality (AR) frameworks
like ARCore and ARKit, for most cameras this information is not available.
Therefore, we propose a web based calibration service that not only aggregates
calibration data, but also allows calibrating new cameras on-the-fly. We build
upon a novel camera calibration framework that enables even novice users to
perform a precise camera calibration in about 2 minutes. This allows general
deployment of computer vision algorithms on the web, which was previously not
possible due to lack of calibration data.","['Pavel Rojtberg', 'Felix Gorschlüter']",2019-07-09T11:51:44Z,http://arxiv.org/abs/1907.04100v2
Real-time Hair Segmentation and Recoloring on Mobile GPUs,"We present a novel approach for neural network-based hair segmentation from a
single camera input specifically designed for real-time, mobile application.
Our relatively small neural network produces a high-quality hair segmentation
mask that is well suited for AR effects, e.g. virtual hair recoloring. The
proposed model achieves real-time inference speed on mobile GPUs (30-100+ FPS,
depending on the device) with high accuracy. We also propose a very realistic
hair recoloring scheme. Our method has been deployed in major AR application
and is used by millions of users.","['Andrei Tkachenka', 'Gregory Karpiak', 'Andrey Vakunov', 'Yury Kartynnik', 'Artsiom Ablavatski', 'Valentin Bazarevsky', 'Siargey Pisarchyk']",2019-07-15T20:39:15Z,http://arxiv.org/abs/1907.06740v1
"EnforceNet: Monocular Camera Localization in Large Scale Indoor Sparse
  LiDAR Point Cloud","Pose estimation is a fundamental building block for robotic applications such
as autonomous vehicles, UAV, and large scale augmented reality. It is also a
prohibitive factor for those applications to be in mass production, since the
state-of-the-art, centimeter-level pose estimation often requires long mapping
procedures and expensive localization sensors, e.g. LiDAR and high precision
GPS/IMU, etc. To overcome the cost barrier, we propose a neural network based
solution to localize a consumer degree RGB camera within a prior sparse LiDAR
map with comparable centimeter-level precision. We achieved it by introducing a
novel network module, which we call resistor module, to enforce the network
generalize better, predicts more accurately, and converge faster. Such results
are benchmarked by several datasets we collected in the large scale indoor
parking garage scenes. We plan to open both the data and the code for the
community to join the effort to advance this field.","['Yu Chen', 'Guan Wang']",2019-07-16T17:35:53Z,http://arxiv.org/abs/1907.07160v1
Fog Computing Applications: Taxonomy and Requirements,"Fog computing was designed to support the specific needs of latency-critical
applications such as augmented reality, and IoT applications which produce
massive volumes of data that are impractical to send to faraway cloud data
centers for analysis. However this also created new opportunities for a wider
range of applications which in turn impose their own requirements on future fog
computing platforms. This article presents a study of a representative set of
30 fog computing applications and the requirements that a general-purpose fog
computing platform should support.","['Arif Ahmed', 'HamidReza Arkian', 'Davaadorj Battulga', 'Ali J. Fahs', 'Mozhdeh Farhadi', 'Dimitrios Giouroukis', 'Adrien Gougeon', 'Felipe Oliveira Gutierrez', 'Guillaume Pierre', 'Paulo R. Souza Jr', 'Mulugeta Ayalew Tamiru', 'Li Wu']",2019-07-26T15:20:57Z,http://arxiv.org/abs/1907.11621v1
"LiDAR ICPS-net: Indoor Camera Positioning based-on Generative
  Adversarial Network for RGB to Point-Cloud Translation","Indoor positioning aims at navigation inside areas with no GPS-data
availability and could be employed in many applications such as augmented
reality, autonomous driving specially inside closed areas and tunnels. In this
paper, a deep neural network-based architecture has been proposed to address
this problem. In this regard, a tandem set of convolutional neural networks, as
well as a Pix2Pix GAN network have been leveraged to perform as the scene
classifier, scene RGB image to point cloud converter, and position regressor,
respectively. The proposed architecture outperforms the previous works,
including our recent work, in the sense that it makes data generation task
easier and more robust against scene small variations, whilst the accuracy of
the positioning is remarkably well, for both Cartesian position and quaternion
information of the camera.","['Ali Ghofrani', 'Rahil Mahdian Toroghi', 'Seyed Mojtaba Tabatabaie', 'Seyed Maziar Tabasi']",2019-11-14T00:46:17Z,http://arxiv.org/abs/1911.05871v1
MaskedFusion: Mask-based 6D Object Pose Estimation,"MaskedFusion is a framework to estimate the 6D pose of objects using RGB-D
data, with an architecture that leverages multiple sub-tasks in a pipeline to
achieve accurate 6D poses. 6D pose estimation is an open challenge due to
complex world objects and many possible problems when capturing data from the
real world, e.g., occlusions, truncations, and noise in the data. Achieving
accurate 6D poses will improve results in other open problems like robot
grasping or positioning objects in augmented reality. MaskedFusion improves the
state-of-the-art by using object masks to eliminate non-relevant data. With the
inclusion of the masks on the neural network that estimates the 6D pose of an
object we also have features that represent the object shape. MaskedFusion is a
modular pipeline where each sub-task can have different methods that achieve
the objective. MaskedFusion achieved 97.3% on average using the ADD metric on
the LineMOD dataset and 93.3% using the ADD-S AUC metric on YCB-Video Dataset,
which is an improvement, compared to the state-of-the-art methods. The code is
available on GitHub (https://github.com/kroglice/MaskedFusion).","['Nuno Pereira', 'Luís A. Alexandre']",2019-11-18T17:09:19Z,http://arxiv.org/abs/1911.07771v2
"Robust Sub-Meter Level Indoor Localization With a Single WiFi Access
  Point-Regression Versus Classification","Precise indoor localization is an increasingly demanding requirement for
various emerging applications, like Virtual/Augmented reality and personalized
advertising. Current indoor environments are equipped with pluralities of WiFi
access points (APs), whose deployment is expected to be massive in the future
enabling highly precise localization approaches. Though the conventional
model-based localization schemes have achieved sub-meter level accuracy by
fusing multiple channel state information (CSI) observations, the corresponding
computational overhead is usually significant, especially in the current
multiple-input multiple-output orthogonal frequency division multiplexing
(MIMO-OFDM) systems. In order to address this issue, model-free localization
techniques using deep learning frameworks have been lately proposed, where
mainly classification methods were applied. In this paper, instead of
classification based mechanism, we propose a logistic regression based scheme
with the deep learning framework, combined with Cram\'er-Rao lower bound (CRLB)
assisted robust training, which achieves more robust sub-meter level accuracy
(0.97m median distance error) in the standard laboratory environment and
maintains reasonable online prediction overhead under the single WiFi AP
settings.","['Chenlu Xiang', 'Shunqing Zhang', 'Shugong Xu', 'Xiaojing Chen', 'George C. Alexandropoulos', 'Vincent K. N. Lau']",2019-11-17T12:50:05Z,http://arxiv.org/abs/1911.08563v1
Digital Twin: Acquiring High-Fidelity 3D Avatar from a Single Image,"We present an approach to generate high fidelity 3D face avatar with a
high-resolution UV texture map from a single image. To estimate the face
geometry, we use a deep neural network to directly predict vertex coordinates
of the 3D face model from the given image. The 3D face geometry is further
refined by a non-rigid deformation process to more accurately capture facial
landmarks before texture projection. A key novelty of our approach is to train
the shape regression network on facial images synthetically generated using a
high-quality rendering engine. Moreover, our shape estimator fully leverages
the discriminative power of deep facial identity features learned from millions
of facial images. We have conducted extensive experiments to demonstrate the
superiority of our optimized 2D-to-3D rendering approach, especially its
excellent generalization property on real-world selfie images. Our proposed
system of rendering 3D avatars from 2D images has a wide range of applications
from virtual/augmented reality (VR/AR) and telepsychiatry to human-computer
interaction and social networks.","['Ruizhe Wang', 'Chih-Fan Chen', 'Hao Peng', 'Xudong Liu', 'Oliver Liu', 'Xin Li']",2019-12-07T07:36:10Z,http://arxiv.org/abs/1912.03455v1
TextSLAM: Visual SLAM with Planar Text Features,"We propose to integrate text objects in man-made scenes tightly into the
visual SLAM pipeline. The key idea of our novel text-based visual SLAM is to
treat each detected text as a planar feature which is rich of textures and
semantic meanings. The text feature is compactly represented by three
parameters and integrated into visual SLAM by adopting the
illumination-invariant photometric error. We also describe important details
involved in implementing a full pipeline of text-based visual SLAM. To our best
knowledge, this is the first visual SLAM method tightly coupled with the text
features. We tested our method in both indoor and outdoor environments. The
results show that with text features, the visual SLAM system becomes more
robust and produces much more accurate 3D text maps that could be useful for
navigation and scene understanding in robotic or augmented reality
applications.","['Boying Li', 'Danping Zou', 'Daniele Sartori', 'Ling Pei', 'Wenxian Yu']",2019-11-26T07:10:25Z,http://arxiv.org/abs/1912.05002v2
A Method for Arbitrary Instance Style Transfer,"The ability to synthesize style and content of different images to form a
visually coherent image holds great promise in various applications such as
stylistic painting, design prototyping, image editing, and augmented reality.
However, the majority of works in image style transfer have focused on
transferring the style of an image to the entirety of another image, and only a
very small number of works have experimented on methods to transfer style to an
instance of another image. Researchers have proposed methods to circumvent the
difficulty of transferring style to an instance in an arbitrary shape. In this
paper, we propose a topologically inspired algorithm called Forward Stretching
to tackle this problem by transforming an instance into a tensor
representation, which allows us to transfer style to this instance itself
directly. Forward Stretching maps pixels to specific positions and interpolate
values between pixels to transform an instance to a tensor. This algorithm
allows us to introduce a method to transfer arbitrary style to an instance in
an arbitrary shape. We showcase the results of our method in this paper.","['Zhifeng Yu', 'Yusheng Wu', 'Tianyou Wang']",2019-12-13T07:42:35Z,http://arxiv.org/abs/1912.06347v1
GrabAR: Occlusion-aware Grabbing Virtual Objects in AR,"Existing augmented reality (AR) applications often ignore occlusion between
real hands and virtual objects when incorporating virtual objects in our views.
The challenges come from the lack of accurate depth and mismatch between real
and virtual depth. This paper presents GrabAR, a new approach that directly
predicts the real-and-virtual occlusion, and bypasses the depth acquisition and
inference. Our goal is to enhance AR applications with interactions between
hand (real) and grabbable objects (virtual). With paired images of hand and
object as inputs, we formulate a neural network that learns to generate the
occlusion mask. To train the network, we compile a synthetic dataset to
pre-train it and a real dataset to fine-tune it, thus reducing the burden of
manual labels and addressing the domain difference. Then, we embed the trained
network in a prototyping AR system that supports hand grabbing of various
virtual objects, demonstrate the system performance, both quantitatively and
qualitatively, and showcase interaction scenarios, in which we can use bare
hand to grab virtual objects and directly manipulate them.","['Xiao Tang', 'Xiaowei Hu', 'Chi-Wing Fu', 'Daniel Cohen-Or']",2019-12-23T05:47:21Z,http://arxiv.org/abs/1912.10637v3
Hand Segmentation for Hand-Object Interaction from Depth map,"Hand segmentation for hand-object interaction is a necessary preprocessing
step in many applications such as augmented reality, medical application, and
human-robot interaction. However, typical methods are based on color
information which is not robust to objects with skin color, skin pigment
difference, and light condition variations. Thus, we propose hand segmentation
method for hand-object interaction using only a depth map. It is challenging
because of the small depth difference between a hand and objects during an
interaction. To overcome this challenge, we propose the two-stage random
decision forest (RDF) method consisting of detecting hands and segmenting
hands. To validate the proposed method, we demonstrate results on the publicly
available dataset of hand segmentation for hand-object interaction. The
proposed method achieves high accuracy in short processing time comparing to
the other state-of-the-art methods.","['Byeongkeun Kang', 'Kar-Han Tan', 'Nan Jiang', 'Hung-Shuo Tai', 'Daniel Tretter', 'Truong Q. Nguyen']",2016-03-08T00:22:59Z,http://arxiv.org/abs/1603.02345v3
A Tangible Volume for Portable 3D Interaction,"We present a new approach to achieve tangible object manipulation with a
single, fully portable and self-contained device. Our solution is based on the
concept of a ""tangible volume"". We turn a tangible object into a handheld
fish-tank display. The tangible volume represents a volume of space that can be
freely manipulated within a virtual scene. This volume can be positioned onto
virtual objects to directly grasp them, and to manipulate them in 3D space. We
investigate this concept through two user studies. The first study evaluates
the intuitiveness of using a tangible volume for grasping and manipulating
virtual objects. The second study evaluates the effects of the limited field of
view on spatial awareness. Finally, we present a generalization of this concept
to other forms of interaction through the surface of the volume.","['Paul Issartel', 'Lonni Besançon', 'Tobias Isenberg', 'Mehdi Ammi']",2016-03-08T19:53:07Z,http://arxiv.org/abs/1603.02642v1
ORBSLAM-based Endoscope Tracking and 3D Reconstruction,"We aim to track the endoscope location inside the surgical scene and provide
3D reconstruction, in real-time, from the sole input of the image sequence
captured by the monocular endoscope. This information offers new possibilities
for developing surgical navigation and augmented reality applications. The main
benefit of this approach is the lack of extra tracking elements which can
disturb the surgeon performance in the clinical routine. It is our first
contribution to exploit ORBSLAM, one of the best performing monocular SLAM
algorithms, to estimate both of the endoscope location, and 3D structure of the
surgical scene. However, the reconstructed 3D map poorly describe textureless
soft organ surfaces such as liver. It is our second contribution to extend
ORBSLAM to be able to reconstruct a semi-dense map of soft organs. Experimental
results on in-vivo pigs, shows a robust endoscope tracking even with organs
deformations and partial instrument occlusions. It also shows the
reconstruction density, and accuracy against ground truth surface obtained from
CT.","['Nader Mahmoud', 'Iñigo Cirauqui', 'Alexandre Hostettler', 'Christophe Doignon', 'Luc Soler', 'Jacques Marescaux', 'J. M. M. Montiel']",2016-08-29T17:10:26Z,http://arxiv.org/abs/1608.08149v1
Spatial Interference Detection for Mobile Visible Light Communication,"Taking advantage of the rolling shutter effect of CMOS cameras in smartphones
is a common practice to increase the transfered data rate with visible light
communication (VLC) without employing external equipment such as photodiodes.
VLC can then be used as replacement of other marker based techniques for object
identification for Augmented Reality and Ubiquitous computing applications.
However, the rolling shutter effect only allows to transmit data over a single
dimension, which considerably limits the available bandwidth. In this article
we propose a new method exploiting spacial interference detection to enable
parallel transmission and design a protocol that enables easy identification of
interferences between two signals. By introducing a second dimension, we are
not only able to significantly increase the available bandwidth, but also
identify and isolate light sources in close proximity.","['Ali Ugur Guler', 'Tristan Braud', 'Pan Hui']",2017-07-13T05:49:14Z,http://arxiv.org/abs/1707.03984v1
"MixedPeds: Pedestrian Detection in Unannotated Videos using
  Synthetically Generated Human-agents for Training","We present a new method for training pedestrian detectors on an unannotated
set of images. We produce a mixed reality dataset that is composed of
real-world background images and synthetically generated static human-agents.
Our approach is general, robust, and makes no other assumptions about the
unannotated dataset regarding the number or location of pedestrians. We
automatically extract from the dataset: i) the vanishing point to calibrate the
virtual camera, and ii) the pedestrians' scales to generate a Spawn Probability
Map, which is a novel concept that guides our algorithm to place the
pedestrians at appropriate locations. After putting synthetic human-agents in
the unannotated images, we use these augmented images to train a Pedestrian
Detector, with the annotations generated along with the synthetic agents. We
conducted our experiments using Faster R-CNN by comparing the detection results
on the unannotated dataset performed by the detector trained using our approach
and detectors trained with other manually labeled datasets. We showed that our
approach improves the average precision by 5-13% over these detectors.","['Ernest C. Cheung', 'Tsan Kwong Wong', 'Aniket Bera', 'Dinesh Manocha']",2017-07-28T04:05:33Z,http://arxiv.org/abs/1707.09100v2
Error-Robust Multi-View Clustering,"In the era of big data, data may come from multiple sources, known as
multi-view data. Multi-view clustering aims at generating better clusters by
exploiting complementary and consistent information from multiple views rather
than relying on the individual view. Due to inevitable system errors caused by
data-captured sensors or others, the data in each view may be erroneous.
Various types of errors behave differently and inconsistently in each view.
More precisely, error could exhibit as noise and corruptions in reality.
Unfortunately, none of the existing multi-view clustering approaches handle all
of these error types. Consequently, their clustering performance is
dramatically degraded. In this paper, we propose a novel Markov chain method
for Error-Robust Multi-View Clustering (EMVC). By decomposing each view into a
shared transition probability matrix and error matrix and imposing structured
sparsity-inducing norms on error matrices, we characterize and handle typical
types of errors explicitly. To solve the challenging optimization problem, we
propose a new efficient algorithm based on Augmented Lagrangian Multipliers and
prove its convergence rigorously. Experimental results on various synthetic and
real-world datasets show the superiority of the proposed EMVC method over the
baseline methods and its robustness against different types of errors.","['Mehrnaz Najafi', 'Lifang He', 'Philip S. Yu']",2018-01-01T02:42:04Z,http://arxiv.org/abs/1801.00384v1
"PrivacEye: Privacy-Preserving Head-Mounted Eye Tracking Using Egocentric
  Scene Image and Eye Movement Features","Eyewear devices, such as augmented reality displays, increasingly integrate
eye tracking but the first-person camera required to map a user's gaze to the
visual scene can pose a significant threat to user and bystander privacy. We
present PrivacEye, a method to detect privacy-sensitive everyday situations and
automatically enable and disable the eye tracker's first-person camera using a
mechanical shutter. To close the shutter in privacy-sensitive situations, the
method uses a deep representation of the first-person video combined with rich
features that encode users' eye movements. To open the shutter without visual
input, PrivacEye detects changes in users' eye movements alone to gauge changes
in the ""privacy level"" of the current situation. We evaluate our method on a
first-person video dataset recorded in daily life situations of 17
participants, annotated by themselves for privacy sensitivity, and show that
our method is effective in preserving privacy in this challenging setting.","['Julian Steil', 'Marion Koelle', 'Wilko Heuten', 'Susanne Boll', 'Andreas Bulling']",2018-01-13T15:46:22Z,http://arxiv.org/abs/1801.04457v4
"The edge cloud: A holistic view of communication, computation and
  caching","The evolution of communication networks shows a clear shift of focus from
just improving the communications aspects to enabling new important services,
from Industry 4.0 to automated driving, virtual/augmented reality, Internet of
Things (IoT), and so on. This trend is evident in the roadmap planned for the
deployment of the fifth generation (5G) communication networks. This ambitious
goal requires a paradigm shift towards a vision that looks at communication,
computation and caching (3C) resources as three components of a single holistic
system. The further step is to bring these 3C resources closer to the mobile
user, at the edge of the network, to enable very low latency and high
reliability services. The scope of this chapter is to show that signal
processing techniques can play a key role in this new vision. In particular, we
motivate the joint optimization of 3C resources. Then we show how graph-based
representations can play a key role in building effective learning methods and
devising innovative resource allocation techniques.","['Sergio Barbarossa', 'Stefania Sardellitti', 'Elena Ceci', 'Mattia Merluzzi']",2018-02-02T14:34:12Z,http://arxiv.org/abs/1802.00700v1
"UAV-Enabled Mobile Edge Computing: Offloading Optimization and
  Trajectory Design","With the emergence of diverse mobile applications (such as augmented
reality), the quality of experience of mobile users is greatly limited by their
computation capacity and finite battery lifetime. Mobile edge computing (MEC)
and wireless power transfer are promising to address this issue. However, these
two techniques are susceptible to propagation delay and loss. Motivated by the
chance of short-distance line-of-sight achieved by leveraging unmanned aerial
vehicle (UAV) communications, an UAV-enabled wireless powered MEC system is
studied. A power minimization problem is formulated subject to the constraints
on the number of the computation bits and energy harvesting causality. The
problem is non-convex and challenging to tackle. An alternative optimization
algorithm is proposed based on sequential convex optimization. Simulation
results show that our proposed design is superior to other benchmark schemes
and the proposed algorithm is efficient in terms of the convergence.","['Fuhui Zhou', 'Yongpeng Wu', 'Haijian Sun', 'Zheng Chu']",2018-02-12T06:35:41Z,http://arxiv.org/abs/1802.03906v1
Egocentric 6-DoF Tracking of Small Handheld Objects,"Virtual and augmented reality technologies have seen significant growth in
the past few years. A key component of such systems is the ability to track the
pose of head mounted displays and controllers in 3D space. We tackle the
problem of efficient 6-DoF tracking of a handheld controller from egocentric
camera perspectives. We collected the HMD Controller dataset which consist of
over 540,000 stereo image pairs labelled with the full 6-DoF pose of the
handheld controller. Our proposed SSD-AF-Stereo3D model achieves a mean average
error of 33.5 millimeters in 3D keypoint prediction and is used in conjunction
with an IMU sensor on the controller to enable 6-DoF tracking. We also present
results on approaches for model based full 6-DoF tracking. All our models
operate under the strict constraints of real time mobile CPU inference.","['Rohit Pandey', 'Pavel Pidlypenskyi', 'Shuoran Yang', 'Christine Kaeser-Chen']",2018-04-16T18:08:51Z,http://arxiv.org/abs/1804.05870v1
"Reliable Low Latency Wireless Communication Enabling Industrial Mobile
  Control and Safety Applications","Advanced industrial applications for human-machine interaction such as
augmented reality support for maintenance works or mobile control panels for
operating production facility set high demands on underlying wireless
connectivity so- lution. Based on 802.11 standard, this paper proposes a
concept of a new system, which is capable of those requirements. For increasing
reliability, an agile triple-band (2.4 GHz, 5 GHz and 60 GHz) communication
system can be used. In order to deal with latency and deterministic channel
access, PHY and MAC techniques such as new waveforms or hybrid MAC schemes are
investigated. Integration of precise localization introduces new possibilities
for safety-critical applications.","['Sergiy Melnyk', 'Abraham Gebru Tesfay', 'Khurshid Alam', 'Hans D. Schotten', 'Vladica Sark', 'Nebojsa Maletic', 'Mohammed Ramadan', 'Marcus Ehrig', 'Thomas Augustin', 'Norman Franch', 'Gerhard Fettweis']",2018-04-20T11:15:38Z,http://arxiv.org/abs/1804.07553v1
4D Temporally Coherent Light-field Video,"Light-field video has recently been used in virtual and augmented reality
applications to increase realism and immersion. However, existing light-field
methods are generally limited to static scenes due to the requirement to
acquire a dense scene representation. The large amount of data and the absence
of methods to infer temporal coherence pose major challenges in storage,
compression and editing compared to conventional video. In this paper, we
propose the first method to extract a spatio-temporally coherent light-field
video representation. A novel method to obtain Epipolar Plane Images (EPIs)
from a spare light-field camera array is proposed. EPIs are used to constrain
scene flow estimation to obtain 4D temporally coherent representations of
dynamic light-fields. Temporal coherence is achieved on a variety of
light-field datasets. Evaluation of the proposed light-field scene flow against
existing multi-view dense correspondence approaches demonstrates a significant
improvement in accuracy of temporal coherence.","['Armin Mustafa', 'Marco Volino', 'Jean-yves Guillemaut', 'Adrian Hilton']",2018-04-30T15:33:13Z,http://arxiv.org/abs/1804.11276v1
eDisco: Discovering Edge Nodes Along the Path,"Edge computing is seen as an enabler for upcoming applications requiring low
latency offloading, such as augmented reality, and as a key building block for
Internet of Things. Edge computing extends the centralized cloud computing
model by distributing servers also close to the users, at the edge of the
network. A key challenge for the clients remains on how to discover the nearby
edge servers and how to access them. In this paper, we present eDisco,
DNS-based edge discovery, that leverages existing protocols and requires no
modifications to deployed infrastructure. eDisco enables service providers and
clients to discover edge servers, and determine the optimal edge deployment
configuration.","['Aleksandr Zavodovski', 'Nitinder Mohan', 'Jussi Kangasharju']",2018-05-04T11:52:35Z,http://arxiv.org/abs/1805.01725v1
"A Mixed Classification-Regression Framework for 3D Pose Estimation from
  2D Images","3D pose estimation from a single 2D image is an important and challenging
task in computer vision with applications in autonomous driving, robot
manipulation and augmented reality. Since 3D pose is a continuous quantity, a
natural formulation for this task is to solve a pose regression problem.
However, since pose regression methods return a single estimate of the pose,
they have difficulties handling multimodal pose distributions (e.g. in the case
of symmetric objects). An alternative formulation, which can capture multimodal
pose distributions, is to discretize the pose space into bins and solve a pose
classification problem. However, pose classification methods can give large
pose estimation errors depending on the coarseness of the discretization. In
this paper, we propose a mixed classification-regression framework that uses a
classification network to produce a discrete multimodal pose estimate and a
regression network to produce a continuous refinement of the discrete estimate.
The proposed framework can accommodate different architectures and loss
functions, leading to multiple classification-regression models, some of which
achieve state-of-the-art performance on the challenging Pascal3D+ dataset.","['Siddharth Mahendran', 'Haider Ali', 'Rene Vidal']",2018-05-08T18:32:04Z,http://arxiv.org/abs/1805.03225v1
Stereo Magnification: Learning View Synthesis using Multiplane Images,"The view synthesis problem--generating novel views of a scene from known
imagery--has garnered recent attention due in part to compelling applications
in virtual and augmented reality. In this paper, we explore an intriguing
scenario for view synthesis: extrapolating views from imagery captured by
narrow-baseline stereo cameras, including VR cameras and now-widespread
dual-lens camera phones. We call this problem stereo magnification, and propose
a learning framework that leverages a new layered representation that we call
multiplane images (MPIs). Our method also uses a massive new data source for
learning view extrapolation: online videos on YouTube. Using data mined from
such videos, we train a deep network that predicts an MPI from an input stereo
image pair. This inferred MPI can then be used to synthesize a range of novel
views of the scene, including views that extrapolate significantly beyond the
input baseline. We show that our method compares favorably with several recent
view synthesis methods, and demonstrate applications in magnifying
narrow-baseline stereo images.","['Tinghui Zhou', 'Richard Tucker', 'John Flynn', 'Graham Fyffe', 'Noah Snavely']",2018-05-24T17:58:02Z,http://arxiv.org/abs/1805.09817v1
"Implementation of Augmented Reality in Autonomous Warehouses: Challenges
  and Opportunities","Autonomous warehouses with mobile, rack-carrying robots are starting to
become commonplace, with systems such as Amazon's Kiva and Swisslog's CarryPick
already implemented in functional warehouses. Such warehouses however still
require human intervention for object picking and maintenance. In the European
project SafeLog we are developing a safety-vest, used for safety-critical
ranging and stopping of mobile robots, an improved planner that can handle
large fleets of heterogeneous agents as well as an AR interaction system to
navigate and support human workers in such automated environments. Here we
present the AR interaction modalities, namely navigation, pick-by-AR and
general system interactions that were developed at the moment of writing, as
well as the overall system concept and planned future work.","['David Puljiz', 'Gleb Gorbachev', 'Björn Hein']",2018-06-01T12:58:54Z,http://arxiv.org/abs/1806.00324v1
"ECC: Platform-Independent Energy-Constrained Deep Neural Network
  Compression via a Bilinear Regression Model","Many DNN-enabled vision applications constantly operate under severe energy
constraints such as unmanned aerial vehicles, Augmented Reality headsets, and
smartphones. Designing DNNs that can meet a stringent energy budget is becoming
increasingly important. This paper proposes ECC, a framework that compresses
DNNs to meet a given energy constraint while minimizing accuracy loss. The key
idea of ECC is to model the DNN energy consumption via a novel bilinear
regression function. The energy estimate model allows us to formulate DNN
compression as a constrained optimization that minimizes the DNN loss function
over the energy constraint. The optimization problem, however, has nontrivial
constraints. Therefore, existing deep learning solvers do not apply directly.
We propose an optimization algorithm that combines the essence of the
Alternating Direction Method of Multipliers (ADMM) framework with
gradient-based learning algorithms. The algorithm decomposes the original
constrained optimization into several subproblems that are solved iteratively
and efficiently. ECC is also portable across different hardware platforms
without requiring hardware knowledge. Experiments show that ECC achieves higher
accuracy under the same or lower energy budget compared to state-of-the-art
resource-constrained DNN compression techniques.","['Haichuan Yang', 'Yuhao Zhu', 'Ji Liu']",2018-12-05T03:31:02Z,http://arxiv.org/abs/1812.01803v3
Photo Wake-Up: 3D Character Animation from a Single Photo,"We present a method and application for animating a human subject from a
single photo. E.g., the character can walk out, run, sit, or jump in 3D. The
key contributions of this paper are: 1) an application of viewing and animating
humans in single photos in 3D, 2) a novel 2D warping method to deform a posable
template body model to fit the person's complex silhouette to create an
animatable mesh, and 3) a method for handling partial self occlusions. We
compare to state-of-the-art related methods and evaluate results with human
studies. Further, we present an interactive interface that allows re-posing the
person in 3D, and an augmented reality setup where the animated 3D person can
emerge from the photo into the real world. We demonstrate the method on photos,
posters, and art.","['Chung-Yi Weng', 'Brian Curless', 'Ira Kemelmacher-Shlizerman']",2018-12-05T22:09:52Z,http://arxiv.org/abs/1812.02246v1
From Coarse to Fine: Robust Hierarchical Localization at Large Scale,"Robust and accurate visual localization is a fundamental capability for
numerous applications, such as autonomous driving, mobile robotics, or
augmented reality. It remains, however, a challenging task, particularly for
large-scale environments and in presence of significant appearance changes.
State-of-the-art methods not only struggle with such scenarios, but are often
too resource intensive for certain real-time applications. In this paper we
propose HF-Net, a hierarchical localization approach based on a monolithic CNN
that simultaneously predicts local features and global descriptors for accurate
6-DoF localization. We exploit the coarse-to-fine localization paradigm: we
first perform a global retrieval to obtain location hypotheses and only later
match local features within those candidate places. This hierarchical approach
incurs significant runtime savings and makes our system suitable for real-time
operation. By leveraging learned descriptors, our method achieves remarkable
localization robustness across large variations of appearance and sets a new
state-of-the-art on two challenging benchmarks for large-scale localization.","['Paul-Edouard Sarlin', 'Cesar Cadena', 'Roland Siegwart', 'Marcin Dymczyk']",2018-12-09T15:56:36Z,http://arxiv.org/abs/1812.03506v2
Privacy-Aware Eye Tracking Using Differential Privacy,"With eye tracking being increasingly integrated into virtual and augmented
reality (VR/AR) head-mounted displays, preserving users' privacy is an ever
more important, yet under-explored, topic in the eye tracking community. We
report a large-scale online survey (N=124) on privacy aspects of eye tracking
that provides the first comprehensive account of with whom, for which services,
and to what extent users are willing to share their gaze data. Using these
insights, we design a privacy-aware VR interface that uses differential
privacy, which we evaluate on a new 20-participant dataset for two privacy
sensitive tasks: We show that our method can prevent user re-identification and
protect gender information while maintaining high performance for gaze-based
document type classification. Our results highlight the privacy challenges
particular to gaze data and demonstrate that differential privacy is a
potential means to address them. Thus, this paper lays important foundations
for future research on privacy-aware gaze interfaces.","['Julian Steil', 'Inken Hagestedt', 'Michael Xuelin Huang', 'Andreas Bulling']",2018-12-19T15:10:05Z,http://arxiv.org/abs/1812.08000v3
Gamified Automation in Immersive Media for Education and Research,"The potential of using video games as well as gaming engines for educational
and research purposes is promising, especially with the current progress of
Industry 4.0 technologies such as augmented and virtual reality devices.
However, it is important to be aware of the barriers of these technologies.
Integrating additional software libraries into current developer environments
would not only increase accessibility for the general public, but would gamify
the development process through multiplayer functionality. In this paper we
will briefly discuss a couple case studies demonstrating the usefulness of
using Unreal Engine 4 for collaborative research purposes. In addition, we
present several ideas on how to extend such development environments to further
assist researchers and students to easily create prototypes for the purpose of
gamified scientific creativity and inquiry.","['Janelle Resch', 'Ireneusz', 'Ocelewski', 'Judy Ehrentraut', 'Michael Barnett-Cowan']",2018-12-11T22:26:21Z,http://arxiv.org/abs/1901.00729v1
"Agent-Based Modelling Approach for Distributed Decision Support in an
  IoT Network","An increasing number of emerging applications, e.g., internet of things,
vehicular communications, augmented reality, and the growing complexity due to
the interoperability requirements of these systems, lead to the need to change
the tools used for the modeling and analysis of those networks. Agent-Based
Modeling (ABM) as a bottom-up modeling approach considers a network of
autonomous agents interacting with each other, and therefore represents an
ideal framework to comprehend the interactions of heterogeneous nodes in a
complex environment. Here, we investigate the suitability of ABM to model the
communication aspects of a road traffic management system, as an example of an
Internet of Things (IoT) network. We model, analyze and compare various Medium
Access Control (MAC) layer protocols for two different scenarios, namely
uncoordinated and coordinated. Besides, we model the scheduling mechanisms for
the coordinated scenario as a high level MAC protocol by using three different
approaches: Centralized Decision Maker, DESYNC and decentralized learning MAC
(L-MAC). The results clearly show the importance of coordination between
multiple decision makers in order to improve the accuracy of information and
spectrum utilization of the system.","['Merim Dzaferagic', 'M. Majid Butt', 'Maria Murphy', 'Nicholas Kaminski', 'Nicola Marchetti']",2019-01-09T13:19:14Z,http://arxiv.org/abs/1901.04585v1
Mixed-Granularity Human-Swarm Interaction,"We present an augmented reality human-swarm interface that combines two
modalities of interaction: environment-oriented and robot-oriented. The
environment-oriented modality allows the user to modify the environment (either
virtual or physical) to indicate a goal to attain for the robot swarm. The
robot-oriented modality makes it possible to select individual robots to
reassign them to other tasks to increase performance or remedy failures.
Previous research has concluded that environment-oriented interaction might
prove more difficult to grasp for untrained users. In this paper, we report a
user study which indicates that, at least in collective transport,
environment-oriented interaction is more effective than purely robot-oriented
interaction, and that the two combined achieve remarkable efficacy.","['Jayam Patel', 'Yicong Xu', 'Carlo Pinciroli']",2019-01-24T17:35:39Z,http://arxiv.org/abs/1901.08522v1
F1/10: An Open-Source Autonomous Cyber-Physical Platform,"In 2005 DARPA labeled the realization of viable autonomous vehicles (AVs) a
grand challenge; a short time later the idea became a moonshot that could
change the automotive industry. Today, the question of safety stands between
reality and solved. Given the right platform the CPS community is poised to
offer unique insights. However, testing the limits of safety and performance on
real vehicles is costly and hazardous. The use of such vehicles is also outside
the reach of most researchers and students. In this paper, we present F1/10: an
open-source, affordable, and high-performance 1/10 scale autonomous vehicle
testbed. The F1/10 testbed carries a full suite of sensors, perception,
planning, control, and networking software stacks that are similar to full
scale solutions. We demonstrate key examples of the research enabled by the
F1/10 testbed, and how the platform can be used to augment research and
education in autonomous systems, making autonomy more accessible.","[""Matthew O'Kelly"", 'Varundev Sukhil', 'Houssam Abbas', 'Jack Harkins', 'Chris Kao', 'Yash Vardhan Pant', 'Rahul Mangharam', 'Dipshil Agarwal', 'Madhur Behl', 'Paolo Burgio', 'Marko Bertogna']",2019-01-24T18:34:50Z,http://arxiv.org/abs/1901.08567v1
"Cascade LSTM Based Visual-Inertial Navigation for Magnetic Levitation
  Haptic Interaction","Haptic feedback is essential to acquire immersive experience when interacting
in virtual or augmented reality. Although the existing promising magnetic
levitation (maglev) haptic system has advantages of none mechanical friction,
its performance is limited by its navigation method, which mainly results from
the challenge that it is difficult to obtain high precision, high frame rate
and good stability with lightweight design at the same. In this study, we
propose to perform the visual-inertial fusion navigation based on
sequence-to-sequence learning for the maglev haptic interaction. Cascade LSTM
based-increment learning method is first presented to progressively learn the
increments of the target variables. Then, two cascade LSTM networks are
separately trained for accomplishing the visual-inertial fusion navigation in a
loosely-coupled mode. Additionally, we set up a maglev haptic platform as the
system testbed. Experimental results show that the proposed cascade LSTM
based-increment learning method can achieve high-precision prediction, and our
cascade LSTM based visual-inertial fusion navigation method can reach 200Hz
while maintaining high-precision (the mean absolute error of the position and
orientation is respectively less than 1mm and 0.02{\deg})navigation for the
maglev haptic interaction application.","['Qianqian Tong', 'Xiaosa Li', 'Kai Lin', 'Zhiyong Yuan']",2019-01-26T14:56:16Z,http://arxiv.org/abs/1901.09224v1
X-Section: Cross-Section Prediction for Enhanced RGBD Fusion,"Detailed 3D reconstruction is an important challenge with application to
robotics, augmented and virtual reality, which has seen impressive progress
throughout the past years. Advancements were driven by the availability of
depth cameras (RGB-D), as well as increased compute power, e.g.\ in the form of
GPUs -- but also thanks to inclusion of machine learning in the process. Here,
we propose X-Section, an RGB-D 3D reconstruction approach that leverages deep
learning to make object-level predictions about thicknesses that can be readily
integrated into a volumetric multi-view fusion process, where we propose an
extension to the popular KinectFusion approach. In essence, our method allows
to complete shape in general indoor scenes behind what is sensed by the RGB-D
camera, which may be crucial e.g.\ for robotic manipulation tasks or efficient
scene exploration. Predicting object thicknesses rather than volumes allows us
to work with comparably high spatial resolution without exploding memory and
training data requirements on the employed Convolutional Neural Networks. In a
series of qualitative and quantitative evaluations, we demonstrate how we
accurately predict object thickness and reconstruct general 3D scenes
containing multiple objects.","['Andrea Nicastro', 'Ronald Clark', 'Stefan Leutenegger']",2019-03-03T20:58:04Z,http://arxiv.org/abs/1903.00987v3
3DN: 3D Deformation Network,"Applications in virtual and augmented reality create a demand for rapid
creation and easy access to large sets of 3D models. An effective way to
address this demand is to edit or deform existing 3D models based on a
reference, e.g., a 2D image which is very easy to acquire. Given such a source
3D model and a target which can be a 2D image, 3D model, or a point cloud
acquired as a depth scan, we introduce 3DN, an end-to-end network that deforms
the source model to resemble the target. Our method infers per-vertex offset
displacements while keeping the mesh connectivity of the source model fixed. We
present a training strategy which uses a novel differentiable operation, mesh
sampling operator, to generalize our method across source and target models
with varying mesh densities. Mesh sampling operator can be seamlessly
integrated into the network to handle meshes with different topologies.
Qualitative and quantitative results show that our method generates higher
quality results compared to the state-of-the art learning-based methods for 3D
shape generation. Code is available at github.com/laughtervv/3DN.","['Weiyue Wang', 'Duygu Ceylan', 'Radomir Mech', 'Ulrich Neumann']",2019-03-08T08:35:48Z,http://arxiv.org/abs/1903.03322v1
"LumiPath -- Towards Real-time Physically-based Rendering on Embedded
  Devices","With the increasing computational power of today's workstations, real-time
physically-based rendering is within reach, rapidly gaining attention across a
variety of domains. These have expeditiously applied to medicine, where it is a
powerful tool for intuitive 3D data visualization. Embedded devices such as
optical see-through head-mounted displays (OST HMDs) have been a trend for
medical augmented reality. However, leveraging the obvious benefits of
physically-based rendering remains challenging on these devices because of
limited computational power, memory usage, and power consumption. We navigate
the compromise between device limitations and image quality to achieve
reasonable rendering results by introducing a novel light field that can be
sampled in real-time on embedded devices. We demonstrate its applications in
medicine and discuss limitations of the proposed method. An open-source version
of this project is available at https://github.com/lorafib/LumiPath which
provides full insight on implementation and exemplary demonstrational material.","['Laura Fink', 'Sing Chun Lee', 'Jie Ying Wu', 'Xingtong Liu', 'Tianyu Song', 'Yordanka Stoyanova', 'Marc Stamminger', 'Nassir Navab', 'Mathias Unberath']",2019-03-09T17:49:44Z,http://arxiv.org/abs/1903.03837v2
Instance- and Category-level 6D Object Pose Estimation,"6D object pose estimation is an important task that determines the 3D
position and 3D rotation of an object in camera-centred coordinates. By
utilizing such a task, one can propose promising solutions for various problems
related to scene understanding, augmented reality, control and navigation of
robotics. Recent developments on visual depth sensors and low-cost availability
of depth data significantly facilitate object pose estimation. Using depth
information from RGB-D sensors, substantial progress has been made in the last
decade by the methods addressing the challenges such as viewpoint variability,
occlusion and clutter, and similar looking distractors. Particularly, with the
recent advent of convolutional neural networks, RGB-only based solutions have
been presented. However, improved results have only been reported for
recovering the pose of known instances, i.e., for the instance-level object
pose estimation tasks. More recently, state-of-the-art approaches target to
solve object pose estimation problem at the level of categories, recovering the
6D pose of unknown instances. To this end, they address the challenges of the
category-level tasks such as distribution shift among source and target
domains, high intra-class variations, and shape discrepancies between objects.","['Caner Sahin', 'Guillermo Garcia-Hernando', 'Juil Sock', 'Tae-Kyun Kim']",2019-03-11T11:45:46Z,http://arxiv.org/abs/1903.04229v1
Privacy Preserving Image-Based Localization,"Image-based localization is a core component of many augmented/mixed reality
(AR/MR) and autonomous robotic systems. Current localization systems rely on
the persistent storage of 3D point clouds of the scene to enable camera pose
estimation, but such data reveals potentially sensitive scene information. This
gives rise to significant privacy risks, especially as for many applications 3D
mapping is a background process that the user might not be fully aware of. We
pose the following question: How can we avoid disclosing confidential
information about the captured 3D scene, and yet allow reliable camera pose
estimation? This paper proposes the first solution to what we call privacy
preserving image-based localization. The key idea of our approach is to lift
the map representation from a 3D point cloud to a 3D line cloud. This novel
representation obfuscates the underlying scene geometry while providing
sufficient geometric constraints to enable robust and accurate 6-DOF camera
pose estimation. Extensive experiments on several datasets and localization
scenarios underline the high practical relevance of our proposed approach.","['Pablo Speciale', 'Johannes L. Schönberger', 'Sing Bing Kang', 'Sudipta N. Sinha', 'Marc Pollefeys']",2019-03-13T16:12:04Z,http://arxiv.org/abs/1903.05572v1
High-index dielectric metasurfaces performing mathematical operations,"Image processing and edge detection are at the core of several newly emerging
technologies, such as augmented reality, autonomous driving and more generally
object recognition. Image processing is typically performed digitally using
integrated electronic circuits and algorithms, implying fundamental size and
speed limitations, as well as significant power needs. On the other hand, it
can also be performed in a low-power analog fashion using Fourier optics,
requiring however bulky optical components. Here, we introduce dielectric
metasurfaces that perform optical image edge detection in the analog domain
using a subwavelength geometry that can be readily integrated with detectors.
The metasurface is composed of a suitably engineered array of nanobeams
designed to perform either 1st- or 2nd-order spatial differentiation. We
experimentally demonstrate the 2nd-derivative operation on an input image,
showing the potential of all-optical edge detection using a silicon metasurface
geometry working at a numerical aperture as large as 0.35.","['Andrea Cordaro', 'Hoyeong Kwon', 'Dimitrios Sounas', 'A. Femius Koenderink', 'Andrea Alù', 'Albert Polman']",2019-03-20T09:32:32Z,http://arxiv.org/abs/1903.08402v1
Haptics-Augmented Physics Simulation: Coriolis Effect,"The teaching of abstract physics concepts can be enhanced by incorporating
visual and haptic sensory modalities in the classroom, using the correct
perspectives. We have developed virtual reality simulations to assist students
in learning the Coriolis effect, an apparent deflection on an object in motion
when observed from within a rotating frame of reference. Twenty four
undergraduate physics students participated in this study. Students were able
to feel the forces through feedback on a Novint Falcon device. The assessment
results show an improvement in the learning experience and better content
retention as compared with traditional instruction methods. We prove that large
scale deployment of visuo-haptic reconfigurable applications is now possible
and feasible in a science laboratory setup.","['Felix G. Hamza-Lup', 'Benjamin Page']",2019-03-07T02:04:56Z,http://arxiv.org/abs/1903.11567v1
"FrameNet: Learning Local Canonical Frames of 3D Surfaces from a Single
  RGB Image","In this work, we introduce the novel problem of identifying dense canonical
3D coordinate frames from a single RGB image. We observe that each pixel in an
image corresponds to a surface in the underlying 3D geometry, where a canonical
frame can be identified as represented by three orthogonal axes, one along its
normal direction and two in its tangent plane. We propose an algorithm to
predict these axes from RGB. Our first insight is that canonical frames
computed automatically with recently introduced direction field synthesis
methods can provide training data for the task. Our second insight is that
networks designed for surface normal prediction provide better results when
trained jointly to predict canonical frames, and even better when trained to
also predict 2D projections of canonical frames. We conjecture this is because
projections of canonical tangent directions often align with local gradients in
images, and because those directions are tightly linked to 3D canonical frames
through projective geometry and orthogonality constraints. In our experiments,
we find that our method predicts 3D canonical frames that can be used in
applications ranging from surface normal estimation, feature matching, and
augmented reality.","['Jingwei Huang', 'Yichao Zhou', 'Thomas Funkhouser', 'Leonidas Guibas']",2019-03-29T00:42:52Z,http://arxiv.org/abs/1903.12305v1
"Panoramic Annular Localizer: Tackling the Variation Challenges of
  Outdoor Localization Using Panoramic Annular Images and Active Deep
  Descriptors","Visual localization is an attractive problem that estimates the camera
localization from database images based on the query image. It is a crucial
task for various applications, such as autonomous vehicles, assistive
navigation and augmented reality. The challenging issues of the task lie in
various appearance variations between query and database images, including
illumination variations, dynamic object variations and viewpoint variations. In
order to tackle those challenges, Panoramic Annular Localizer into which
panoramic annular lens and robust deep image descriptors are incorporated is
proposed in this paper. The panoramic annular images captured by the single
camera are processed and fed into the NetVLAD network to form the active deep
descriptor, and sequential matching is utilized to generate the localization
result. The experiments carried on the public datasets and in the field
illustrate the validation of the proposed system.","['Ruiqi Cheng', 'Kaiwei Wang', 'Shufei Lin', 'Weijian Hu', 'Kailun Yang', 'Xiao Huang', 'Huabing Li', 'Dongming Sun', 'Jian Bai']",2019-05-14T07:26:48Z,http://arxiv.org/abs/1905.05425v2
"Flat2Layout: Flat Representation for Estimating Layout of General Room
  Types","This paper proposes a new approach, Flat2Layout, for estimating general
indoor room layout from a single-view RGB image whereas existing methods can
only produce layout topologies captured from the box-shaped room. The proposed
flat representation encodes the layout information into row vectors which are
treated as the training target of the deep model. A dynamic programming based
postprocessing is employed to decode the estimated flat output from the deep
model into the final room layout. Flat2Layout achieves state-of-the-art
performance on existing room layout benchmark. This paper also constructs a
benchmark for validating the performance on general layout topologies, where
Flat2Layout achieves good performance on general room types. Flat2Layout is
applicable on more scenario for layout estimation and would have an impact on
applications of Scene Modeling, Robotics, and Augmented Reality.","['Chi-Wei Hsiao', 'Cheng Sun', 'Min Sun', 'Hwann-Tzong Chen']",2019-05-29T16:26:43Z,http://arxiv.org/abs/1905.12571v1
Blocks: Collaborative and Persistent Augmented Reality Experiences,"We introduce Blocks, a mobile application that enables people to co-create AR
structures that persist in the physical environment. Using Blocks, end users
can collaborate synchronously or asynchronously, whether they are colocated or
remote. Additionally, the AR structures can be tied to a physical location or
can be accessed from anywhere. We evaluated how people used Blocks through a
series of lab and field deployment studies with over 160 participants, and
explored the interplay between two collaborative dimensions: space and time. We
found that participants preferred creating structures synchronously with
colocated collaborators. Additionally, they were most active when they created
structures that were not restricted by time or place. Unlike most of today's AR
experiences, which focus on content consumption, this work outlines new design
opportunities for persistent and collaborative AR experiences that empower
anyone to collaborate and create AR content.","['Anhong Guo', 'Ilter Canberk', 'Hannah Murphy', 'Andrés Monroy-Hernández', 'Rajan Vaish']",2019-08-07T00:49:35Z,http://arxiv.org/abs/1908.02409v2
"Is This The Right Place? Geometric-Semantic Pose Verification for Indoor
  Visual Localization","Visual localization in large and complex indoor scenes, dominated by weakly
textured rooms and repeating geometric patterns, is a challenging problem with
high practical relevance for applications such as Augmented Reality and
robotics. To handle the ambiguities arising in this scenario, a common strategy
is, first, to generate multiple estimates for the camera pose from which a
given query image was taken. The pose with the largest geometric consistency
with the query image, e.g., in the form of an inlier count, is then selected in
a second stage. While a significant amount of research has concentrated on the
first stage, there is considerably less work on the second stage. In this
paper, we thus focus on pose verification. We show that combining different
modalities, namely appearance, geometry, and semantics, considerably boosts
pose verification and consequently pose accuracy. We develop multiple
hand-crafted as well as a trainable approach to join into the
geometric-semantic verification and show significant improvements over
state-of-the-art on a very challenging indoor dataset.","['Hajime Taira', 'Ignacio Rocco', 'Jiri Sedlar', 'Masatoshi Okutomi', 'Josef Sivic', 'Tomas Pajdla', 'Torsten Sattler', 'Akihiko Torii']",2019-08-13T12:12:05Z,http://arxiv.org/abs/1908.04598v2
General Hand Guidance Framework using Microsoft HoloLens,"Hand guidance emerged from the safety requirements for collaborative robots,
namely possessing joint-torque sensors. Since then it has proven to be a
powerful tool for easy trajectory programming, allowing lay-users to reprogram
robots intuitively. Going beyond, a robot can learn tasks by user
demonstrations through kinesthetic teaching, enabling robots to generalise
tasks and further reducing the need for reprogramming. However, hand guidance
is still mostly relegated to collaborative robots. Here we propose a method
that doesn't require any sensors on the robot or in the robot cell, by using a
Microsoft HoloLens augmented reality head mounted display. We reference the
robot using a registration algorithm to match the robot model to the spatial
mesh. The in-built hand tracking and localisation capabilities are then used to
calculate the position of the hands relative to the robot. By decomposing the
hand movements into orthogonal rotations and propagating it down through the
kinematic chain, we achieve a generalised hand guidance without the need to
build a dynamic model of the robot itself. We tested our approach on a commonly
used industrial manipulator, the KUKA KR-5.","['David Puljiz', 'Erik Stöhr', 'Katharina S. Riesterer', 'Björn Hein', 'Torsten Kröger']",2019-08-13T15:05:42Z,http://arxiv.org/abs/1908.04692v1
"Meeting QoS of Users in a Edge to Cloud Platform via Optimally Placing
  Services and Scheduling Tasks","This paper considers the problem of service placement and task scheduling on
a three-tiered edge-to-cloud platform when user requests must be met by a
certain deadline. Time-sensitive applications (e.g., augmented reality, gaming,
real-time video analysis) have tight constraints that must be met. With
multiple possible computation centers, the ""where"" and ""when"" of solving these
requests becomes paramount when meeting their deadlines. We formulate the
problem of meeting users' deadlines while minimizing the total cost of the
edge-to-cloud service provider as an Integer Linear Programming (ILP) problem.
We show the NP-hardness of this problem, and propose two heuristics based on
making decisions on a local vs global scale. We vary the number of users, the
QoS constraint, and the cost difference between remote cloud and cloudlets(edge
clouds), and run multiple Monte-Carlo runs for each case. Our simulation
results show that the proposed heuristics are performing close to optimal while
reducing the complexity.","['Matthew Turner', 'Hana Khamfroush']",2019-08-13T18:55:29Z,http://arxiv.org/abs/1908.04824v1
Hyperparameter-Free Losses for Model-Based Monocular Reconstruction,"This work proposes novel hyperparameter-free losses for single view 3D
reconstruction with morphable models (3DMM). We dispense with the
hyperparameters used in other works by exploiting geometry, so that the shape
of the object and the camera pose are jointly optimized in a sole term
expression. This simplification reduces the optimization time and its
complexity. Moreover, we propose a novel implicit regularization technique
based on random virtual projections that does not require additional 2D or 3D
annotations. Our experiments suggest that minimizing a shape reprojection error
together with the proposed implicit regularization is especially suitable for
applications that require precise alignment between geometry and image spaces,
such as augmented reality. We evaluate our losses on a large scale dataset with
3D ground truth and publish our implementations to facilitate reproducibility
and public benchmarking in this field.","['Eduard Ramon', 'Guillermo Ruiz', 'Thomas Batard', 'Xavier Giró-i-Nieto']",2019-08-16T14:32:19Z,http://arxiv.org/abs/1908.09001v1
AR-based interaction for safe human-robot collaborative manufacturing,"Industrial standards define safety requirements for Human-Robot Collaboration
(HRC) in industrial manufacturing. The standards particularly require real-time
monitoring and securing of the minimum protective distance between a robot and
an operator. In this work, we propose a depth-sensor based model for workspace
monitoring and an interactive Augmented Reality (AR) User Interface (UI) for
safe HRC. The AR UI is implemented on two different hardware: a
projector-mirror setup anda wearable AR gear (HoloLens). We experiment the
workspace model and UIs for a realistic diesel motor assembly task. The
AR-based interactive UIs provide 21-24% and 57-64% reduction in the task
completion and robot idle time, respectively, as compared to a baseline without
interaction and workspace sharing. However, subjective evaluations reveal that
HoloLens based AR is not yet suitable for industrial manufacturing while the
projector-mirror setup shows clear improvements in safety and work ergonomics.","['Antti Hietanen', 'Jyrki Latokartano', 'Roel Pieters', 'Minna Lanz', 'Joni-Kristian Kämäräinen']",2019-09-06T14:34:21Z,http://arxiv.org/abs/1909.02933v1
"Advanced Materials and Device Architectures for Magnetooptical Spatial
  Light Modulators","Faraday and Kerr rotations are magnetooptical (MO) effects used for rotating
the polarization of light in transmission and reflection from a magnetized
medium, respectively. MO effects combined with intrinsically fast magnetization
reversal, which can go down to a few tens of femtoseconds or less, can be
applied in magnetooptical spatial light modulators (MOSLMs) promising for
nonvolatile, ultrafast, and high-resolution spatial modulation of light. With
the recent progress in low-power switching of magnetic and MO materials, MOSLMs
may lead to major breakthroughs and benefit beyond state-of-the-art holography,
data storage, optical communications, heads-up displays, virtual and augmented
reality devices, and solid-state light detection and ranging (LIDAR). In this
study, the recent developments in the growth, processing, and engineering of
advanced materials with high MO figures of merit for practical MOSLM devices
are reviewed. The challenges with MOSLM functionalities including the intrinsic
weakness of MO effect and large power requirement for switching are assessed.
The suggested solutions are evaluated, different driving systems are
investigated, and resulting device architectures are benchmarked. Finally, the
research opportunities on MOSLMs for achieving integrated, high-contrast, and
low-power devices are presented.","['Soheila Kharratian', 'Hakan Urey', 'Mehmet C. Onbaşlı']",2019-09-16T21:42:57Z,http://arxiv.org/abs/1909.07494v2
Learning an Action-Conditional Model for Haptic Texture Generation,"Rich haptic sensory feedback in response to user interactions is desirable
for an effective, immersive virtual reality or teleoperation system. However,
this feedback depends on material properties and user interactions in a
complex, non-linear manner. Therefore, it is challenging to model the mapping
from material and user interactions to haptic feedback in a way that
generalizes over many variations of the user's input. Current methodologies are
typically conditioned on user interactions, but require a separate model for
each material. In this paper, we present a learned action-conditional model
that uses data from a vision-based tactile sensor (GelSight) and user's action
as input. This model predicts an induced acceleration that could be used to
provide haptic vibration feedback to a user. We trained our proposed model on a
publicly available dataset (Penn Haptic Texture Toolkit) that we augmented with
GelSight measurements of the different materials. We show that a unified model
over all materials outperforms previous methods and generalizes to new actions
and new instances of the material categories in the dataset.","['Negin Heravi', 'Wenzhen Yuan', 'Allison M. Okamura', 'Jeannette Bohg']",2019-09-28T04:45:42Z,http://arxiv.org/abs/1909.13025v2
"Motion Planning through Demonstration to Deal with Complex Motions in
  Assembly Process","Complex and skillful motions in actual assembly process are challenging for
the robot to generate with existing motion planning approaches, because some
key poses during the human assembly can be too skillful for the robot to
realize automatically. In order to deal with this problem, this paper develops
a motion planning method using skillful motions from demonstration, which can
be applied to complete robotic assembly process including complex and skillful
motions. In order to demonstrate conveniently without redundant third-party
devices, we attach augmented reality (AR) markers to the manipulated object to
track and capture poses of the object during the human assembly process, which
are employed as key poses to execute motion planning by the planner. Derivative
of every key pose serves as criterion to determine the priority of use of key
poses in order to accelerate the motion planning. The effectiveness of the
presented method is verified through some numerical examples and actual robot
experiments.","['Yan Wang', 'Kensuke Harada', 'Weiwei Wan']",2019-10-04T07:25:46Z,http://arxiv.org/abs/1910.01821v1
Identifying Candidate Spaces for Advert Implantation,"Virtual advertising is an important and promising feature in the area of
online advertising. It involves integrating adverts onto live or recorded
videos for product placements and targeted advertisements. Such integration of
adverts is primarily done by video editors in the post-production stage, which
is cumbersome and time-consuming. Therefore, it is important to automatically
identify candidate spaces in a video frame, wherein new adverts can be
implanted. The candidate space should match the scene perspective, and also
have a high quality of experience according to human subjective judgment. In
this paper, we propose the use of a bespoke neural net that can assist the
video editors in identifying candidate spaces. We benchmark our approach
against several deep-learning architectures on a large-scale image dataset of
candidate spaces of outdoor scenes. Our work is the first of its kind in this
area of multimedia and augmented reality applications, and achieves the best
results.","['Soumyabrata Dev', 'Hossein Javidnia', 'Murhaf Hossari', 'Matthew Nicholson', 'Killian McCabe', 'Atul Nautiyal', 'Clare Conran', 'Jian Tang', 'Wei Xu', 'François Pitié']",2019-10-08T06:12:53Z,http://arxiv.org/abs/1910.03227v1
"Eyenet: Attention based Convolutional Encoder-Decoder Network for Eye
  Region Segmentation","With the immersive development in the field of augmented and virtual reality,
accurate and speedy eye-tracking is required. Facebook Research has organized a
challenge, named OpenEDS Semantic Segmentation challenge for per-pixel
segmentation of the key eye regions: the sclera, the iris, the pupil, and
everything else (background). There are two constraints set for the
participants viz MIOU and the computational complexity of the model. More
recently, researchers have achieved quite a good result using the convolutional
neural networks (CNN) in segmenting eyeregions. However, the environmental
challenges involved in this task such as low resolution, blur, unusual glint
and, illumination, off-angles, off-axis, use of glasses and different color of
iris region hinder the accuracy of segmentation. To address the challenges in
eye segmentation, the present work proposes a robust and computationally
efficient attention-based convolutional encoder-decoder network for segmenting
all the eye regions. Our model, named EyeNet, includes modified residual units
as the backbone, two types of attention blocks and multi-scale supervision for
segmenting the aforesaid four eye regions. Our proposed model achieved a total
score of 0.974(EDS Evaluation metric) on test data, which demonstrates superior
results compared to the baseline methods.","['Priya Kansal', 'Sabari Nathan']",2019-10-08T08:43:27Z,http://arxiv.org/abs/1910.03274v1
"ICPS-net: An End-to-End RGB-based Indoor Camera Positioning System using
  deep convolutional neural networks","Indoor positioning and navigation inside an area with no GPS-data
availability is a challenging problem. There are applications such as augmented
reality, autonomous driving, navigation of drones inside tunnels, in which
indoor positioning gets crucial. In this paper, a tandem architecture of deep
network-based systems, for the first time to our knowledge, is developed to
address this problem. This structure is trained on the scene images being
obtained through scanning of the desired area segments using photogrammetry. A
CNN structure based on EfficientNet is trained as a classifier of the scenes,
followed by a MobileNet CNN structure which is trained to perform as a
regressor. The proposed system achieves amazingly fine precisions for both
Cartesian position and Quaternion information of the camera.","['Ali Ghofrani', 'Rahil Mahdian Toroghi', 'Sayed Mojtaba Tabatabaie']",2019-10-14T15:43:56Z,http://arxiv.org/abs/1910.06219v1
"Spatial Data Science: Closing the human-spatial computing-environment
  loop","Over the last decade, the term spatial computing has grown to have two
different, though not entirely unrelated, definitions. The first definition of
spatial computing stems from industry, where it refers primarily to new kinds
of augmented, virtual, mixed-reality, and natural user interface technologies.
A second definition coming out of academia takes a broader perspective that
includes active research in geographic information science as well as the
aforementioned novel UI technologies. Both senses reflect an ongoing shift
toward increased interaction with computing interfaces and sensors embedded in
the environment and how the use of these technologies influence how we behave
and make sense of and even change the world we live in. Regardless of the
definition, research in spatial computing is humming along nicely without the
need to identify new research agendas or new labels for communities of
researchers. However, as a field of research, it could be helpful to view
spatial data science as the glue that coheres spatial computing with
problem-solving and learning in the real world into a more holistic discipline.",['Benjamin Adams'],2019-10-15T02:19:19Z,http://arxiv.org/abs/1910.06484v1
"Active 6D Multi-Object Pose Estimation in Cluttered Scenarios with Deep
  Reinforcement Learning","In this work, we explore how a strategic selection of camera movements can
facilitate the task of 6D multi-object pose estimation in cluttered scenarios
while respecting real-world constraints important in robotics and augmented
reality applications, such as time and distance traveled. In the proposed
framework, a set of multiple object hypotheses is given to an agent, which is
inferred by an object pose estimator and subsequently spatio-temporally
selected by a fusion function that makes use of a verification score that
circumvents the need of ground-truth annotations. The agent reasons about these
hypotheses, directing its attention to the object which it is most uncertain
about, moving the camera towards such an object. Unlike previous works that
propose short-sighted policies, our agent is trained in simulated scenarios
using reinforcement learning, attempting to learn the camera moves that produce
the most accurate object poses hypotheses for a given temporal and spatial
budget, without the need of viewpoints rendering during inference. Our
experiments show that the proposed approach successfully estimates the 6D
object pose of a stack of objects in both challenging cluttered synthetic and
real scenarios, showing superior performance compared to strong baselines.","['Juil Sock', 'Guillermo Garcia-Hernando', 'Tae-Kyun Kim']",2019-10-19T17:56:43Z,http://arxiv.org/abs/1910.08811v1
"Mobile Recognition of Wikipedia Featured Sites using Deep Learning and
  Crowd-sourced Imagery","Rendering Wikipedia content through mobile and augmented reality mediums can
enable new forms of interaction in urban-focused user communities facilitating
learning, communication and knowledge exchange. With this objective in mind, in
this work we develop a mobile application that allows for the recognition of
notable sites featured on Wikipedia. The application is powered by a deep
neural network that has been trained on crowd-sourced imagery describing sites
of interest, such as buildings, statues, museums or other physical entities
that are present and visually accessible in an urban environment. We describe
an end-to-end pipeline that describes data collection, model training and
evaluation of our application considering online and real world scenarios. We
identify a number of challenges in the site recognition task which arise due to
visual similarities amongst the classified sites as well as due to noise
introduce by the surrounding built environment. We demonstrate how using mobile
contextual information, such as user location, orientation and attention
patterns can significantly alleviate such challenges. Moreover, we present an
unsupervised learning technique to de-noise crowd-sourced imagery which
improves classification performance further.","['Jimin Tan', 'Anastasios Noulas', 'Diego Sáez', 'Rossano Schifanella']",2019-10-22T00:17:55Z,http://arxiv.org/abs/1910.09705v2
"Broadband Optical Fully Differential Operation Based on the Spin-orbit
  Interaction of Light","Optical technology may provide important architectures for future computing,
such as analog optical computing and image processing. Compared with
traditional electric operation, optical operation has shown some unique
advantages including faster operating speeds and lower power consumption. Here,
we propose an optical full differentiator based on the spin-orbit interaction
of light at a simple optical interface. The broadband optical operation is
independent on the wavelength due to the nature of purely geometric. As an
important application of the fully differential operation, the broadband image
processing of edge detection is demonstrated. By adjusting the polarization of
the incident beam, the one-dimension edge imaging at any desirable direction
can be obtained. The broadband image processing of edge detection provides
possible applications in autonomous driving, target recognition, microscopic
imaging, and augmented reality.","['Shanshan He', 'Junxiao Zhou', 'Shizhen Chen', 'Weixing Shu', 'Hailu Luo', 'Shuangchun Wen']",2019-10-22T06:51:17Z,http://arxiv.org/abs/1910.09789v1
Single-shot three-dimensional imaging with a metasurface depth camera,"Depth imaging is vital for many emerging technologies with applications in
augmented reality, robotics, gesture detection, and facial recognition. These
applications, however, demand compact and low-power systems beyond the
capabilities of state-of-the-art depth cameras. Here, we leverage ultrathin
dielectric metasurfaces to demonstrate a solution that, with a single surface,
replicates the functionality of a high-performance depth camera typically
comprising a spatial light modulator, polarizer, and three lenses. Using
cylindrical nano-scatterers that can arbitrarily modify the phase of an
incident wavefront, our metasurface passively encodes two complementary optical
responses to depth information in a scene with a single camera snapshot. By
decoding the captured data in software, our system produces a fully
reconstructed image and transverse depth map of three-dimensional scenes with a
fractional ranging error of 1.7%. We demonstrate the first visible wavelength
and polarization-insensitive metasurface depth camera, representing a
significant form factor reduction for such systems.","['Shane Colburn', 'Arka Majumdar']",2019-10-26T18:01:06Z,http://arxiv.org/abs/1910.12111v1
A Survey on Human Machine Interaction in Industry 4.0,"Industry 4.0 or Industrial IoT both describe new paradigms for seamless
interaction between humans and machines. Both concepts rely on intelligent,
inter-connected cyber-physical production systems that are able to control the
process flow of industrial production. As those machines take many decisions
autonomously and further interact with production and manufacturing planning
systems, the integration of human users requires new paradigms. In this paper,
we provide an analysis of the current state-of-the-art in human-machine
interaction in the Industry 4.0 domain.We focus on new paradigms that integrate
the application of augmented and virtual reality technology. Based on our
analysis, we further provide a discussion of research challenges.","['Christian Krupitzer', 'Sebastian Müller', 'Veronika Lesch', 'Marwin Züfle', 'Janick Edinger', 'Alexander Lemken', 'Dominik Schäfer', 'Samuel Kounev', 'Christian Becker']",2020-02-03T21:53:48Z,http://arxiv.org/abs/2002.01025v1
"ViWi Vision-Aided mmWave Beam Tracking: Dataset, Task, and Baseline
  Solutions","Vision-aided wireless communication is motivated by the recent advances in
deep learning and computer vision as well as the increasing dependence on
line-of-sight links in millimeter wave (mmWave) and terahertz systems. By
leveraging vision, this new research direction enables an interesting set of
new capabilities such as vision-aided mmWave beam and blockage prediction,
proactive hand-off, and resource allocation among others. These capabilities
have the potential of reliably supporting highly-mobile applications such as
vehicular/drone communications and wireless virtual/augmented reality in mmWave
and terahertz systems. Investigating these interesting applications, however,
requires the development of special dataset and machine learning tasks. Based
on the Vision-Wireless (ViWi) dataset generation framework [1], this paper
develops an advanced and realistic scenario/dataset that features multiple base
stations, mobile users, and rich dynamics. Enabled by this dataset, the paper
defines the vision-wireless mmWave beam tracking task (ViWi-BT) and proposes a
baseline solution that can provide an initial benchmark for the future ViWi-BT
algorithms.","['Muhammad Alrabeiah', 'Jayden Booth', 'Andrew Hredzak', 'Ahmed Alkhateeb']",2020-02-06T18:53:30Z,http://arxiv.org/abs/2002.02445v3
"An overview of physical layer design for Ultra-Reliable Low-Latency
  Communications in 3GPP Release 15 and Release 16","For the fifth generation (5G) wireless network technology, the most important
design goals were improved metrics of reliability and latency, in addition to
network resilience and flexibility so as to best adapt the network operation
for new applications such as augmented virtual reality, industrial automation
and autonomous vehicles. That led to design efforts conducted under the subject
of ultra-reliable low-latency communication (URLLC). The technologies
integrated in Release 15 of the 3rd Generation Partnership Project (3GPP)
finalized in December 2017 provided the foundation of physical layer design for
URLLC. New features such as numerologies, new transmission schemes with
sub-slot interval, configured grant resources, etc., were standardized to
improve mainly the latency aspects.
  5G evolution in Release 16, PHY version finalized in December 2019, allows
achieving improved metrics for latency and reliability by standardizing a
number of new features. This article provides a detailed overview of the URLLC
features from 5G Release 15 and Release 16, describing how these features
permit URLLC operation in 5G networks.","['Trung-Kien Le', 'Umer Salim', 'Florian Kaltenberger']",2020-02-10T13:18:19Z,http://arxiv.org/abs/2002.03713v1
Real-time binaural speech separation with preserved spatial cues,"Deep learning speech separation algorithms have achieved great success in
improving the quality and intelligibility of separated speech from mixed audio.
Most previous methods focused on generating a single-channel output for each of
the target speakers, hence discarding the spatial cues needed for the
localization of sound sources in space. However, preserving the spatial
information is important in many applications that aim to accurately render the
acoustic scene such as in hearing aids and augmented reality (AR). Here, we
propose a speech separation algorithm that preserves the interaural cues of
separated sound sources and can be implemented with low latency and high
fidelity, therefore enabling a real-time modification of the acoustic scene.
Based on the time-domain audio separation network (TasNet), a single-channel
time-domain speech separation system that can be implemented in real-time, we
propose a multi-input-multi-output (MIMO) end-to-end extension of TasNet that
takes binaural mixed audio as input and simultaneously separates target
speakers in both channels. Experimental results show that the proposed
end-to-end MIMO system is able to significantly improve the separation
performance and keep the perceived location of the modified sources intact in
various acoustic scenes.","['Cong Han', 'Yi Luo', 'Nima Mesgarani']",2020-02-16T18:18:19Z,http://arxiv.org/abs/2002.06637v1
OccuSeg: Occupancy-aware 3D Instance Segmentation,"3D instance segmentation, with a variety of applications in robotics and
augmented reality, is in large demands these days. Unlike 2D images that are
projective observations of the environment, 3D models provide metric
reconstruction of the scenes without occlusion or scale ambiguity. In this
paper, we define ""3D occupancy size"", as the number of voxels occupied by each
instance. It owns advantages of robustness in prediction, on which basis,
OccuSeg, an occupancy-aware 3D instance segmentation scheme is proposed. Our
multi-task learning produces both occupancy signal and embedding
representations, where the training of spatial and feature embeddings varies
with their difference in scale-aware. Our clustering scheme benefits from the
reliable comparison between the predicted occupancy size and the clustered
occupancy size, which encourages hard samples being correctly clustered and
avoids over segmentation. The proposed approach achieves state-of-the-art
performance on 3 real-world datasets, i.e. ScanNetV2, S3DIS and SceneNN, while
maintaining high efficiency.","['Lei Han', 'Tian Zheng', 'Lan Xu', 'Lu Fang']",2020-03-14T02:48:55Z,http://arxiv.org/abs/2003.06537v3
DeepCap: Monocular Human Performance Capture Using Weak Supervision,"Human performance capture is a highly important computer vision problem with
many applications in movie production and virtual/augmented reality. Many
previous performance capture approaches either required expensive multi-view
setups or did not recover dense space-time coherent geometry with
frame-to-frame correspondences. We propose a novel deep learning approach for
monocular dense human performance capture. Our method is trained in a weakly
supervised manner based on multi-view supervision completely removing the need
for training data with 3D ground truth annotations. The network architecture is
based on two separate networks that disentangle the task into a pose estimation
and a non-rigid surface deformation step. Extensive qualitative and
quantitative evaluations show that our approach outperforms the state of the
art in terms of quality and robustness.","['Marc Habermann', 'Weipeng Xu', 'Michael Zollhoefer', 'Gerard Pons-Moll', 'Christian Theobalt']",2020-03-18T16:39:56Z,http://arxiv.org/abs/2003.08325v1
"Refined Plane Segmentation for Cuboid-Shaped Objects by Leveraging Edge
  Detection","Recent advances in the area of plane segmentation from single RGB images show
strong accuracy improvements and now allow a reliable segmentation of indoor
scenes into planes. Nonetheless, fine-grained details of these segmentation
masks are still lacking accuracy, thus restricting the usability of such
techniques on a larger scale in numerous applications, such as inpainting for
Augmented Reality use cases. We propose a post-processing algorithm to align
the segmented plane masks with edges detected in the image. This allows us to
increase the accuracy of state-of-the-art approaches, while limiting ourselves
to cuboid-shaped objects. Our approach is motivated by logistics, where this
assumption is valid and refined planes can be used to perform robust object
detection without the need for supervised learning. Results for two baselines
and our approach are reported on our own dataset, which we made publicly
available. The results show a consistent improvement over the state-of-the-art.
The influence of the prior segmentation and the edge detection is investigated
and finally, areas for future research are proposed.","['Alexander Naumann', 'Laura Dörr', 'Niels Ole Salscheider', 'Kai Furmans']",2020-03-28T18:51:43Z,http://arxiv.org/abs/2003.12870v1
"gDLS*: Generalized Pose-and-Scale Estimation Given Scale and Gravity
  Priors","Many real-world applications in augmented reality (AR), 3D mapping, and
robotics require both fast and accurate estimation of camera poses and scales
from multiple images captured by multiple cameras or a single moving camera.
Achieving high speed and maintaining high accuracy in a pose-and-scale
estimator are often conflicting goals. To simultaneously achieve both, we
exploit a priori knowledge about the solution space. We present gDLS*, a
generalized-camera-model pose-and-scale estimator that utilizes rotation and
scale priors. gDLS* allows an application to flexibly weigh the contribution of
each prior, which is important since priors often come from noisy sensors.
Compared to state-of-the-art generalized-pose-and-scale estimators (e.g.,
gDLS), our experiments on both synthetic and real data consistently demonstrate
that gDLS* accelerates the estimation process and improves scale and pose
accuracy.","['Victor Fragoso', 'Joseph DeGol', 'Gang Hua']",2020-04-05T00:01:19Z,http://arxiv.org/abs/2004.02052v1
