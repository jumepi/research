title,summary,authors,published,link,category
Experimental evidence for mixed reality states,"Recently researchers at the University of Illinois coupled a real pendulum to
its virtual counterpart. They observed that the two pendulums suddenly start to
move in synchrony if their lengths are sufficiently close. In this synchronized
state, the boundary between the real system and the virtual system is blurred,
that is, the pendulums are in a mixed reality state. An instantaneous,
bidirectional coupling is a prerequisite for mixed reality states. In this
article we explore the implications of mixed reality states in the context of
controlling real-world systems.","['Alfred Hubler', 'Vadas Gintautas']",2011-03-24T18:50:15Z,http://arxiv.org/abs/1103.4838v1,['physics.class-ph']
"Addressing the Privacy Implications of Mixed Reality: A Regulatory
  Approach","Mixed reality (MR) technologies are emerging into the mainstream with
affordable devices like the Oculus Quest. These devices blend the physical and
virtual in novel ways that blur the lines that exist in legal precedent, like
those between speech and conduct. In this paper, we discuss the challenges of
regulating immersive technologies, focusing on the potential for extensive data
collection, and examine the trade-offs of three potential approaches to
protecting data privacy in the context of mixed reality environments.","['Nicole Shadowen', 'Diane Hosfelt']",2020-07-20T16:35:17Z,http://arxiv.org/abs/2007.10246v1,"['cs.CR', 'cs.CY', 'cs.HC']"
Mixed Reality Interface for Digital Twin of Plant Factory,"An easier and intuitive interface architecture is necessary for digital twin
of plant factory. I suggest an immersive and interactive mixed reality
interface for digital twin models of smart farming, for remote work rather than
simulation of components. The environment is constructed with UI display and a
streaming background scene, which is a real time scene taken from camera device
located in the plant factory, processed with deformable neural radiance fields.
User can monitor and control the remote plant factory facilities with HMD or 2D
display based mixed reality environment. This paper also introduces detailed
concept and describes the system architecture to implement suggested mixed
reality interface.",['Byunghyun Ban'],2022-10-29T01:40:04Z,http://arxiv.org/abs/2211.00597v1,"['cs.HC', 'cs.LG', 'cs.MM', 'H.5.2']"
Multi-layer Visualization for Medical Mixed Reality,"Medical Mixed Reality helps surgeons to contextualize intraoperative data
with video of the surgical scene. Nonetheless, the surgical scene and
anatomical target are often occluded by surgical instruments and surgeon hands.
In this paper and to our knowledge, we propose a multi-layer visualization in
Medical Mixed Reality solution which subtly improves a surgeon's visualization
by making transparent the occluding objects. As an example scenario, we use an
augmented reality C-arm fluoroscope device. A video image is created using a
volumetric-based image synthesization technique and stereo-RGBD cameras mounted
on the C-arm. From this synthesized view, the background which is occluded by
the surgical instruments and surgeon hands is recovered by modifying the
volumetric-based image synthesization technique. The occluding objects can,
therefore, become transparent over the surgical scene. Experimentation with
different augmented reality scenarios yield results demonstrating that the
background of the surgical scenes can be recovered with accuracy between
45%-99%. In conclusion, we presented a solution that a Mixed Reality solution
for medicine, providing transparency to objects occluding the surgical scene.
This work is also the first application of volumetric field for Diminished
Reality/ Mixed Reality.","['Séverine Habert', 'Ma Meng', 'Pascal Fallavollita', 'Nassir Navab']",2017-09-26T12:13:01Z,http://arxiv.org/abs/1709.08962v1,['cs.CV']
"Wish You Were Here: Mental and Physiological Effects of Remote Music
  Collaboration in Mixed Reality","With face-to-face music collaboration being severely limited during the
recent pandemic, mixed reality technologies and their potential to provide
musicians a feeling of ""being there"" with their musical partner can offer
tremendous opportunities. In order to assess this potential, we conducted a
laboratory study in which musicians made music together in real-time while
simultaneously seeing their jamming partner's mixed reality point cloud via a
head-mounted display and compared mental effects such as flow, affect, and
co-presence to an audio-only baseline. In addition, we tracked the musicians'
physiological signals and evaluated their features during times of
self-reported flow. For users jamming in mixed reality, we observed a
significant increase in co-presence. Regardless of the condition (mixed reality
or audio-only), we observed an increase in positive affect after jamming
remotely. Furthermore, we identified heart rate and HF/LF as promising features
for classifying the flow state musicians experienced while making music
together.","['Ruben Schlagowski', 'Dariia Nazarenko', 'Yekta Can', 'Kunal Gupta', 'Silvan Mertes', 'Mark Billinghurst', 'Elisabeth André']",2023-01-23T12:56:03Z,http://arxiv.org/abs/2301.09402v1,['cs.HC']
Toward Mixed Reality Hybrid Objects with IoT Avatar Agents,"The internet-of-things (IoT) refers to the growing field of interconnected
pervasive computing devices and the networking that supports smart, embedded
applications. The IoT has multiple human-computer interaction challenges due to
its many formats and interlinked components, and central to these is the need
to provide sensory information and situational context pertaining to users in a
more human-friendly, easily understandable format. This work addresses this by
applying mixed reality toward expressing the underlying behaviors and states
internal to IoT devices and IoT-enabled objects. It extends the authors'
previous research on IoT Avatars (mixed reality character representations of
physical IoT devices), presenting a new head-mounted display framework and
interconnection architecture. This contributes i) an exploration of mixed
reality for smart spaces, ii) an approach toward expressive avatar behaviors
using fuzzy inference, and iii) an early functional prototype of a hybrid
physical and mixed reality IoT-enabled object. This approach is a step toward
new information presentation, interaction, and engagement capabilities for
smart devices and environments.","['Alexis Morris', 'Jie Guan', 'Nadine Lessio', 'Yiyi Shao']",2023-05-19T19:10:09Z,http://arxiv.org/abs/2305.11960v1,['cs.HC']
A 3D Mixed Reality Interface for Human-Robot Teaming,"This paper presents a mixed-reality human-robot teaming system. It allows
human operators to see in real-time where robots are located, even if they are
not in line of sight. The operator can also visualize the map that the robots
create of their environment and can easily send robots to new goal positions.
The system mainly consists of a mapping and a control module. The mapping
module is a real-time multi-agent visual SLAM system that co-localizes all
robots and mixed-reality devices to a common reference frame. Visualizations in
the mixed-reality device then allow operators to see a virtual life-sized
representation of the cumulative 3D map overlaid onto the real environment. As
such, the operator can effectively ""see through"" walls into other rooms. To
control robots and send them to new locations, we propose a drag-and-drop
interface. An operator can grab any robot hologram in a 3D mini map and drag it
to a new desired goal pose. We validate the proposed system through a user
study and real-world deployments. We make the mixed-reality application
publicly available at https://github.com/cvg/HoloLens_ros.","['Jiaqi Chen', 'Boyang Sun', 'Marc Pollefeys', 'Hermann Blum']",2023-10-03T19:25:43Z,http://arxiv.org/abs/2310.02392v1,['cs.RO']
CPR Emergency Assistance Through Mixed Reality Communication,"We design and evaluate a mixed reality real-time communication system for
remote assistance during CPR emergencies. Our system allows an expert to guide
a first responder, remotely, on how to give first aid. RGBD cameras capture a
volumetric view of the local scene including the patient, the first responder,
and the environment. The volumetric capture is augmented onto the remote
expert's view to spatially guide the first responder using visual and verbal
instructions. We evaluate the mixed reality communication system in a research
study in which participants face a simulated emergency. The first responder
moves the patient to the recovery position and performs chest compressions as
well as mouth-to-mask ventilation. Our study compares mixed reality against
videoconferencing-based assistance using CPR performance measures, cognitive
workload surveys, and semi-structured interviews. We find that more visual
communication including gestures and objects is used by the remote expert when
assisting in mixed reality compared to videoconferencing. Moreover, the
performance and the workload of the first responder during simulation do not
differ significantly between the two technologies.","['Manuel Rebol', 'Alexander Steinmaurer', 'Florian Gamillscheg', 'Krzysztof Pietroszek', 'Christian Gütl', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka']",2023-12-14T17:21:15Z,http://arxiv.org/abs/2312.09150v1,['cs.HC']
Semi-autonomous Robotic Disassembly Enhanced by Mixed Reality,"In this study, we introduce ""SARDiM,"" a modular semi-autonomous platform
enhanced with mixed reality for industrial disassembly tasks. Through a case
study focused on EV battery disassembly, SARDiM integrates Mixed Reality,
object segmentation, teleoperation, force feedback, and variable autonomy.
Utilising the ROS, Unity, and MATLAB platforms, alongside a joint impedance
controller, SARDiM facilitates teleoperated disassembly. The approach combines
FastSAM for real-time object segmentation, generating data which is
subsequently processed through a cluster analysis algorithm to determine the
centroid and orientation of the components, categorizing them by size and
disassembly priority. This data guides the MoveIt platform in trajectory
planning for the Franka Robot arm. SARDiM provides the capability to switch
between two teleoperation modes: manual and semi-autonomous with variable
autonomy. Each was evaluated using four different Interface Methods (IM):
direct view, monitor feed, mixed reality with monitor feed, and point cloud
mixed reality. Evaluations across the eight IMs demonstrated a 40.61% decrease
in joint limit violations using Mode 2. Moreover, Mode 2-IM4 outperformed Mode
1-IM1 by achieving a 2.33%-time reduction while considerably increasing safety,
making it optimal for operating in hazardous environments at a safe distance,
with the same ease of use as teleoperation with a direct view of the
environment.","['Alireza Rastegarpanah', 'Cesar Alan Contreras', 'Rustam Stolkin']",2024-05-06T14:47:40Z,http://arxiv.org/abs/2405.03530v1,['cs.RO']
Privacy Implications of Eye Tracking in Mixed Reality,"Mixed Reality (MR) devices require a world with always-on sensors and
real-time processing applied to their outputs. We have grappled with some of
the ethical concerns presented by this scenario, such as bystander privacy
issues with smartphones and cameras. However, MR technologies demand that we
define and defend privacy in this new paradigm. This paper focuses on the
challenges presented by eye tracking and gaze tracking, techniques that have
commonly been deployed in the HCI community for years but are now being
integrated into MR devices by default.","['Diane Hosfelt', 'Nicole Shadowen']",2020-07-20T16:25:35Z,http://arxiv.org/abs/2007.10235v1,"['cs.HC', 'cs.CY']"
"Questionnaires and Qualitative Feedback Methods to Measure User
  Experience in Mixed Reality","Evaluating the user experience of a software system is an essential final
step of every research. Several concepts such as flow, affective state,
presences, or immersion exist to measure user experience. Typical measurement
techniques analyze physiological data, gameplay data, and questionnaires.
Qualitative feedback methods are another approach to collect detailed user
insights. In this position paper, we will discuss how we used questionnaires
and qualitative feedback methods in previous mixed reality work to measure user
experience. We will present several measurement examples, discuss their current
limitations, and provide guideline propositions to support comparable mixed
reality user experience research in the future.","['Tobias Drey', 'Michael Rietzler', 'Enrico Rukzio']",2021-04-13T14:15:16Z,http://arxiv.org/abs/2104.06221v1,['cs.HC']
Mixed Reality: The Interface of the Future,"The world is slowly moving towards everything being simulated digitally and
virtually. Mixed Reality (MR) is the amalgam of the real world with virtual
stimuli. It has great prospects in the future in terms of various applications
additionally with some challenges. This paper focuses on how Mixed Reality
could be used in the future along with the challenges that could arise. Several
application areas along with the potential benefits are studied in this
research. Three research questions are proposed, analyzed, and concluded
through the experiments. While the availability of MR devices could introduce a
lot of potential, specific challenges need to be scrutinized by the developers
and manufacturers. Overall, MR technology has a chance to enhance personalized,
supportive, and interactive experiences for human lives.",['Dipesh Gyawali'],2023-09-02T04:12:30Z,http://arxiv.org/abs/2309.00819v1,['cs.HC']
Transreality puzzle as new genres of entertainment technology,"The author considers a class of mechatronic puzzles falling in the
mixed-reality category, present examples of such devices, and propose a way to
categorize them. Close relationships of such devices with the Tangible User
Interface are described. The device designed by the author as an illustration
of a mixed reality puzzle is presented.",['Ilya V Osipov'],2017-05-10T23:22:43Z,http://arxiv.org/abs/1705.03973v1,['cs.HC']
Experiencing avatar direction in low cost theatrical mixed reality setup,"We introduce the setup and programming framework of AvatarStaging theatrical
mixed reality experiment. We focus on a configuration addressing movement
issues between physical and 3D digital spaces from performers and directors'
points of view. We propose 3 practical exercises.","['Georges Gagneré', 'Cédric Plessiet']",2023-03-13T10:34:05Z,http://arxiv.org/abs/2303.06984v1,['cs.GR']
Mixed Reality Serious Games: The Therapist Perspective,"The objective of this paper is to present a Mixed Reality System (MRS) for
rehabilitation of the upper limb after stroke. The system answers the following
challenges: (i) increase motivation of patients by making the training a
personalized experience; (ii) take into account patients' impairments by
offering intuitive and easy to use interaction modalities; (iii) make it
possible to therapists to track patient's activities and to evaluate/track
their progress; (iv) open opportunities for telemedicine and tele
rehabilitation; (v) and provide an economically acceptable system by reducing
both equipment and management costs. In order to test this system a pilot study
has been conducted in conjunction with a French hospital in order to understand
the potential and benefits of mixed reality. The pilot involved 3 therapists
who 'played the role' of patients. Three sessions, one using conventional
rehabilitation, another using an ad hoc developed game on a PC, and another
using a mixed reality version of the same game were held. Results have shown
the MRS and the PC game to be accepted more than physical rehabilitation.","['Ines Di Loreto Ines', 'Abdelkader Gouaïch']",2010-11-06T12:58:05Z,http://arxiv.org/abs/1011.1560v1,['cs.HC']
"Using Socially Expressive Mixed Reality Arms for Enhancing
  Low-Expressivity Robots","Expressivity--the use of multiple modalities to convey internal state and
intent of a robot--is critical for interaction. Yet, due to cost, safety, and
other constraints, many robots lack high degrees of physical expressivity. This
paper explores using mixed reality to enhance a robot with limited expressivity
by adding virtual arms that extend the robot's expressiveness. The arms,
capable of a range of non-physically-constrained gestures, were evaluated in a
between-subject study ($n=34$) where participants engaged in a mixed reality
mathematics task with a socially assistive robot. The study results indicate
that the virtual arms added a higher degree of perceived emotion, helpfulness,
and physical presence to the robot. Users who reported a higher perceived
physical presence also found the robot to have a higher degree of social
presence, ease of use, usefulness, and had a positive attitude toward using the
robot with mixed reality. The results also demonstrate the users' ability to
distinguish the virtual gestures' valence and intent.","['Thomas R. Groechel', 'Zhonghao Shi', 'Roxanna Pakkar', 'Maja J. Matarić']",2019-11-21T19:23:04Z,http://arxiv.org/abs/1911.09713v2,"['cs.RO', 'cs.HC']"
"Spatial Computing and Intuitive Interaction: Bringing Mixed Reality and
  Robotics Together","Spatial computing -- the ability of devices to be aware of their surroundings
and to represent this digitally -- offers novel capabilities in human-robot
interaction. In particular, the combination of spatial computing and egocentric
sensing on mixed reality devices enables them to capture and understand human
actions and translate these to actions with spatial meaning, which offers
exciting new possibilities for collaboration between humans and robots. This
paper presents several human-robot systems that utilize these capabilities to
enable novel robot use cases: mission planning for inspection, gesture-based
control, and immersive teleoperation. These works demonstrate the power of
mixed reality as a tool for human-robot interaction, and the potential of
spatial computing and mixed reality to drive the future of human-robot
interaction.","['Jeffrey Delmerico', 'Roi Poranne', 'Federica Bogo', 'Helen Oleynikova', 'Eric Vollenweider', 'Stelian Coros', 'Juan Nieto', 'Marc Pollefeys']",2022-02-03T10:04:26Z,http://arxiv.org/abs/2202.01493v1,"['cs.RO', 'cs.CV', 'cs.HC']"
"The AR/VR Technology Stack: A Central Repository of Software Development
  Libraries, Platforms, and Tools","A comprehensive repository of software development libraries, platforms, and
tools specifically to the domains of augmented, virtual, and mixed reality.",['Jasmine Roberts'],2023-05-13T05:50:26Z,http://arxiv.org/abs/2305.07842v1,['cs.SE']
Literature Review of Mixed Reality Research,"In the global context, while mixed reality has been an emerging concept for
years, recent technological and scientific advancements have now made it poised
to revolutionize industries and daily life by offering enhanced functionalities
and improved services. Besides reviewing the highly cited papers in the last 20
years among over a thousand research papers on mixed reality, this systematic
review provides the state-of-the-art applications and utilities of the mixed
reality by primarily scrutinizing the associated papers in 2022 and 2023.
Focusing on the potentials that this technology have in providing digitally
supported simulations and other utilities in the era of large language models,
highlighting the potential and limitations of the innovative solutions and also
bringing focus to emerging research directions, such as telemedicine, remote
control and optimization of direct volume rendering. The paper's associated
repository is publicly accessible at https://aizierjiang.github.io/mr.",['Aizierjiang Aiersilan'],2023-11-29T13:53:13Z,http://arxiv.org/abs/2312.02995v2,"['cs.HC', 'cs.GR']"
"Towards Mixed Reality as the Everyday Computing Paradigm: Challenges &
  Design Recommendations","This research presents a proof-of-concept prototype of an all-in-one mixed
reality application platform, developed to investigate the needs and
expectations of users from mixed reality systems. The study involved an
extensive user study with 1,052 participants, including the collection of
diaries from 6 users and conducting interviews with 15 participants to gain
deeper insights into their experiences. The findings from the interviews
revealed that directly porting current user flows into 3D environments was not
well-received by the target users. Instead, users expressed a clear preference
for alternative 3D interactions along with the continued use of 2D interfaces.
This study provides insights for understanding user preferences and
interactions in mixed reality systems, and design recommendations to facilitate
the mass adoption of MR systems.","['Amir Reza Asadi', 'Reza Hemadi']",2024-02-25T03:37:32Z,http://arxiv.org/abs/2402.15974v2,['cs.HC']
"Assessing User Apprehensions About Mixed Reality Artifacts and
  Applications: The Mixed Reality Concerns (MRC) Questionnaire","Current research in Mixed Reality (MR) presents a wide range of novel use
cases for blending virtual elements with the real world. This
yet-to-be-ubiquitous technology challenges how users currently work and
interact with digital content. While offering many potential advantages, MR
technologies introduce new security, safety, and privacy challenges. Thus, it
is relevant to understand users' apprehensions towards MR technologies, ranging
from security concerns to social acceptance. To address this challenge, we
present the Mixed Reality Concerns (MRC) Questionnaire, designed to assess
users' concerns towards MR artifacts and applications systematically. The
development followed a structured process considering previous work, expert
interviews, iterative refinements, and confirmatory tests to analytically
validate the questionnaire. The MRC Questionnaire offers a new method of
assessing users' critical opinions to compare and assess novel MR artifacts and
applications regarding security, privacy, social implications, and trust.","['Christopher Katins', 'Paweł W. Woźniak', 'Aodi Chen', 'Ihsan Tumay', 'Luu Viet Trinh Le', 'John Uschold', 'Thomas Kosch']",2024-03-09T09:54:22Z,http://arxiv.org/abs/2403.05855v2,['cs.HC']
"Catalyzing Social Interactions in Mixed Reality using ML Recommendation
  Systems","We create an innovative mixed reality-first social recommendation model,
utilizing features uniquely collected through mixed reality (MR) systems to
promote social interaction, such as gaze recognition, proximity, noise level,
congestion level, and conversational intensity. We further extend these models
to include right-time features to deliver timely notifications. We measure
performance metrics across various models by creating a new intersection of
user features, MR features, and right-time features. We create four model types
trained on different combinations of the feature classes, where we compare the
baseline model trained on the class of user features against the models trained
on MR features, right-time features, and a combination of all of the feature
classes. Due to limitations in data collection and cost, we observe performance
degradation in the right-time, mixed reality, and combination models. Despite
these challenges, we introduce optimizations to improve accuracy across all
models by over 14 percentage points, where the best performing model achieved
24% greater accuracy.","['Sparsh Srivastava', 'Rohan Arora']",2024-04-29T20:19:35Z,http://arxiv.org/abs/2404.19095v1,"['cs.HC', 'cs.IR', 'cs.LG', 'cs.SI']"
"SIGMA: An Open-Source Interactive System for Mixed-Reality Task
  Assistance Research","We introduce an open-source system called SIGMA (short for ""Situated
Interactive Guidance, Monitoring, and Assistance"") as a platform for conducting
research on task-assistive agents in mixed-reality scenarios. The system
leverages the sensing and rendering affordances of a head-mounted mixed-reality
device in conjunction with large language and vision models to guide users step
by step through procedural tasks. We present the system's core capabilities,
discuss its overall design and implementation, and outline directions for
future research enabled by the system. SIGMA is easily extensible and provides
a useful basis for future research at the intersection of mixed reality and AI.
By open-sourcing an end-to-end implementation, we aim to lower the barrier to
entry, accelerate research in this space, and chart a path towards
community-driven end-to-end evaluation of large language, vision, and
multimodal models in the context of real-world interactive applications.","['Dan Bohus', 'Sean Andrist', 'Nick Saw', 'Ann Paradiso', 'Ishani Chakraborty', 'Mahdi Rad']",2024-05-16T21:21:09Z,http://arxiv.org/abs/2405.13035v1,"['cs.HC', 'cs.AI']"
"Haptic-enabled Mixed Reality System for Mixed-initiative Remote Robot
  Control","Robots assist in many areas that are considered unsafe for humans to operate.
For instance, in handling pandemic diseases such as the recent Covid-19
outbreak and other outbreaks like Ebola, robots can assist in reaching areas
dangerous for humans and do simple tasks such as pick up the correct medicine
(among a set of bottles prescribed) and deliver to patients. In such cases, it
might not be good to rely on the fully autonomous operation of robots. Since
many mobile robots are fully functional with low-level tasks such as grabbing
and moving, we consider the mixed-initiative control where the user can guide
the robot remotely to finish such tasks. For this mixed-initiative control, the
user controlling the robot needs to visualize a 3D scene as seen by the robot
and guide it. Mixed reality can virtualize reality and immerse users in the 3D
scene that is reconstructed from the real-world environment. This technique
provides the user more freedom such as choosing viewpoints at view time. In
recent years, benefiting from the high-quality data from Light Detection and
Ranging (LIDAR) and RGBD cameras, mixed reality is widely used to build
networked platforms to improve the performance of robot teleoperations and
robot-human collaboration, and enhanced feedback for mixed-initiative control.
In this paper, we proposed a novel haptic-enabled mixed reality system, that
provides haptic interfaces to interact with the virtualized environments and
give remote guidance for mobile robots towards high-level tasks. The
experimental results show the effectiveness and flexibility of the proposed
haptic enabled mixed reality system.","['Yuan Tian', 'Lianjun Li', 'Andrea Fumagalli', 'Yonas Tadesse', 'Balakrishnan Prabhakaran']",2021-02-06T06:15:15Z,http://arxiv.org/abs/2102.03521v2,['cs.RO']
Solving Poisson's Equation on the Microsoft HoloLens,"We present a mixed reality application (HoloFEM) for the Microsoft HoloLens.
The application lets a user define and solve a physical problem governed by
Poisson's equation with the surrounding real world geometry as input data.
Holograms are used to visualise both the problem and the solution. The finite
element method is used to solve Poisson's equation. Solving and visualising
partial differential equations in mixed reality could have potential usage in
areas such as building planning and safety engineering.","['Anders Logg', 'Carl Lundholm', 'Magne Nordaas']",2017-11-17T13:44:33Z,http://arxiv.org/abs/1711.07790v1,"['cs.GR', 'cs.MS']"
"Opportunities and Limitations of Mixed Reality Holograms in Industrial
  Robotics","This paper introduces two case studies combining the field of industrial
robotics with Mixed Reality (MR). The goal of those case studies is to get a
better understanding of how MR can be useful and what are the limitations. The
first case study describes an approach to visualize the digital twin of a robot
arm. The second case study aims at facilitating the commissioning of industrial
robots. Furthermore, this paper reports the experiences gained by implementing
those two scenarios and discusses the limitations.","['Michael Filipenko', 'Andreas Angerer', 'Alwin Hoffmann', 'Wolfgang Reif']",2020-01-22T17:30:45Z,http://arxiv.org/abs/2001.08166v1,"['cs.RO', 'cs.HC']"
"Exploring Temporal Dependencies in Multimodal Referring Expressions with
  Mixed Reality","In collaborative tasks, people rely both on verbal and non-verbal cues
simultaneously to communicate with each other. For human-robot interaction to
run smoothly and naturally, a robot should be equipped with the ability to
robustly disambiguate referring expressions. In this work, we propose a model
that can disambiguate multimodal fetching requests using modalities such as
head movements, hand gestures, and speech. We analysed the acquired data from
mixed reality experiments and formulated a hypothesis that modelling temporal
dependencies of events in these three modalities increases the model's
predictive power. We evaluated our model on a Bayesian framework to interpret
referring expressions with and without exploiting a temporal prior.","['Elena Sibirtseva', 'Ali Ghadirzadeh', 'Iolanda Leite', 'Mårten Björkman', 'Danica Kragic']",2019-02-04T10:44:50Z,http://arxiv.org/abs/1902.01117v1,"['cs.HC', 'cs.RO']"
"Excuse me! Perception of Abrupt Direction Changes Using Body Cues and
  Paths on Mixed Reality Avatars","We evaluate two methods of signalling abrupt direction changes of a robotic
platform using a Mixed Reality avatar. The ""Body"" method uses gaze, gesture and
torso direction to point to upcoming waypoints. The ""Path"" method visualises
the change in direction using an angled path on the ground. We compare these
two methods using a controlled user study and show that each method has its
strengths depending on the situation. Overall the ""Path"" method was slightly
more accurate in communicating the direction change of the robot but
participants overall preferred the ""Body"" method.","['Nicholas Katzakis', 'Frank Steinicke']",2018-01-16T01:10:54Z,http://arxiv.org/abs/1801.05085v1,"['cs.HC', 'cs.RO']"
"I-nteract: A cyber-physical system for real-time interaction with
  physical and virtual objects using mixed reality technologies for additive
  manufacturing","This paper presents I-nteract, a cyber-physical system that enables real-time
interaction with real and virtual objects in a mixed augmented reality
environment to design 3D models for additive manufacturing. The system has been
developed using mixed reality technologies such as HoloLens, for augmenting
visual feedback, and haptic gloves, for augmenting haptic force feedback. The
efficacy of the system has been demonstrated by generating 3D model using a
novel scanning method to 3D print a customized orthopedic cast for human arm,
by estimating spring rates of compression springs, and by simulating
interaction with a virtual spring using hand.","['Ammar Malik', 'Hugo Lhachemi', 'Robert Shorten']",2020-02-14T22:57:35Z,http://arxiv.org/abs/2002.06280v1,['cs.HC']
Mixed Reality Interaction Techniques,"This chapter gives an overview of interaction techniques for mixed reality
including augmented and virtual reality (AR/VR). Various modalities for input
and output are discussed. Specifically, techniques for tangible and
surface-based interaction, gesture-based, pen-based, gaze-based, keyboard and
mouse-based, as well as haptic interaction are discussed. Furthermore, the
combination of multiple modalities in multisensory and multimodal interaction,
as well as interaction using multiple physical or virtual displays, are
presented. Finally, interaction with intelligent virtual agents is considered.",['Jens Grubert'],2021-03-10T10:47:10Z,http://arxiv.org/abs/2103.05984v1,['cs.HC']
What We Measure in Mixed Reality Experiments,"There are many potential measures that one might use when evaluating
mixed-reality experiences. In this position paper I will argue that there are
various stances to take for evaluation, depending on the framing of the
experience within a larger body of work. I will draw upon various types of work
that my team has been involved with in order to illustrate these different
stances. I will then sketch out some directions for developing more robust
measures that can help the field move forward.",['Anthony Steed'],2021-04-12T11:18:35Z,http://arxiv.org/abs/2104.05356v1,['cs.HC']
Dynamic X-Ray Vision in Mixed Reality,"X-ray vision, a technique that allows users to see through walls and other
obstacles, is a popular technique for Augmented Reality (AR) and Mixed Reality
(MR). In this paper, we demonstrate a dynamic X-ray vision window that is
rendered in real-time based on the user's current position and changes with
movement in the physical environment. Moreover, the location and transparency
of the window are also dynamically rendered based on the user's eye gaze. We
build this X-ray vision window for a current state-of-the-art MR Head-Mounted
Device (HMD) -- HoloLens 2 by integrating several different features: scene
understanding, eye tracking, and clipping primitive.","['Hung-Jui Guo', 'Jonathan Z. Bakdash', 'Laura R. Marusich', 'Balakrishnan Prabhakaran']",2022-09-15T03:32:10Z,http://arxiv.org/abs/2209.07025v1,['cs.HC']
Challenges of movement quality using motion capture in theatre,"We describe1 two case studies of AvatarStaging theatrical mixed reality
framework combining avatars and performers acting in an artistic context. We
outline a qualitative approach toward the condition for stage presence for the
avatars. We describe the motion control solutions we experimented with from the
perspective of building a protocol of avatar direction in a mixed reality
appropriate to live performance.","['Georges Gagneré', 'Andy Lavender', 'Cédric Plessiet', 'Tim White']",2023-03-13T10:37:20Z,http://arxiv.org/abs/2303.06987v1,['cs.GR']
"How Can Mixed Reality Benefit From Physiologically-Adaptive Systems?
  Challenges and Opportunities for Human Factors Applications","Mixed Reality (MR) allows users to interact with digital objects in a
physical environment, but several limitations have hampered widespread
adoption. Physiologically adaptive systems detecting user's states can drive
interaction and address these limitations. Here, we highlight potential
usability and interaction limitations in MR and how physiologically adaptive
systems can benefit MR experiences and applications. We specifically address
potential applications for human factors and operational settings such as
healthcare, education, and entertainment. We further discuss benefits and
applications in light of ethical and privacy concerns. The use of
physiologically adaptive systems in MR has the potential to revolutionize
human-computer interactions and provide users with a more personalized and
engaging experience.","['Francesco Chiossi', 'Sven Mayer']",2023-03-31T11:25:10Z,http://arxiv.org/abs/2303.17978v1,['cs.HC']
"PWR-Align: Leveraging Part-Whole Relationships for Part-wise Rigid Point
  Cloud Registration in Mixed Reality Applications","We present an efficient and robust point cloud registration (PCR) workflow
for part-wise rigid point cloud alignment using the Microsoft HoloLens 2. Point
Cloud Registration (PCR) is an important problem in Augmented and Mixed Reality
use cases, and we present a study for a special class of non-rigid
transformations. Many commonly encountered objects are composed of rigid parts
that move relative to one another about joints resulting in non-rigid
deformation of the whole object such as robots with manipulators, and machines
with hinges. The workflow presented allows us to register the point cloud with
various configurations of the point cloud.","['Manorama Jha', 'Bhaskar Banerjee']",2023-06-11T16:36:31Z,http://arxiv.org/abs/2306.06717v1,"['cs.CV', 'cs.GR']"
"Securing Bystander Privacy in Mixed Reality While Protecting the User
  Experience","The modern Mixed Reality devices that make the Metaverse viable require vast
information about the physical world and can also violate the privacy of
unsuspecting or unwilling bystanders in their vicinity. In this article, we
provide an introduction to the problem, existing solutions, and avenues for
future research.","['Matthew Corbett', 'Brendan David-John', 'Jiacheng Shang', 'Y. Charlie Hu', 'Bo Ji']",2023-07-24T14:44:27Z,http://arxiv.org/abs/2307.12847v2,"['cs.CY', 'cs.CR']"
"RealityDrop: A Multimodal Mixed Reality Framework to Manipulate Virtual
  Content between Cross-system Displays","In this poster, we present RealityDrop, a novel multimodal framework that
uses Mixed Reality (MR) technology to manipulate, display, and transfer virtual
content across different display systems. Employing MR as the centre of
control, RealityDrop affords concise information dissemination among diverse
collaborators, through varied representations that best fit each display
system's unique features using `superhuman' gaze and gesture interactions.
Three multimodal interaction techniques, a customised content interpreter, and
two cross-system interfaces are incorporated for fluent content manipulation
and presentation.","['Jeremy McDade', 'Allison Jing', 'Andrew Cunningham']",2023-10-11T13:03:23Z,http://arxiv.org/abs/2310.07458v1,['cs.HC']
Named Service Networking as a primer for the Metaverse,"Ubiquitous extended reality environments such as the Metaverse will have a
significant impact on the Internet, which will evolve to interconnect a large
number of mixed reality spaces. Currently, Metaverse development is related to
the creation of mixed reality environments, not tackling the required
networking functionalities. This article analyzes suitable networking design
choices to support the Metaverse, proposing a new service-centric networking
approach capable of incorporating low-latency data fetching, distributed
computing, and fusion of heterogeneous data types over the Cloud-to-Thing
continuum.",['Paulo Mendes'],2024-01-17T12:11:42Z,http://arxiv.org/abs/2401.09162v1,['cs.NI']
"Never Tell the Trick: Covert Interactive Mixed Reality System for
  Immersive Storytelling","This study explores the integration of Ultra-Wideband (UWB) technology into
Mixed Reality (MR) Systems for immersive storytelling. Addressing the
limitations of existing technologies like Microsoft Kinect and HTC Vive, the
research focuses on overcoming challenges in robustness to occlusion, tracking
volume, and cost efficiency in props tracking. Utilizing UWB technology, the
interactive MR system enhances the scope of performance art by enabling larger
tracking areas, more reliable and cheaper multi-prop tracking, and reducing
occlusion issues. Preliminary user tests suggest meaningful improvements in
immersive experience, promising a new possibility in Extended Reality (XR)
theater, performance art and immersive game.","['Chanwoo Lee', 'Kyubeom Shim', 'Sanggyo Seo', 'Gwonu Ryu', 'Yongsoon Choi']",2024-03-03T19:17:14Z,http://arxiv.org/abs/2403.01594v1,['cs.HC']
"Joint Point Cloud and Image Based Localization For Efficient Inspection
  in Mixed Reality","This paper introduces a method of structure inspection using mixed-reality
headsets to reduce the human effort in reporting accurate inspection
information such as fault locations in 3D coordinates. Prior to every
inspection, the headset needs to be localized. While external pose estimation
and fiducial marker based localization would require setup, maintenance, and
manual calibration; marker-free self-localization can be achieved using the
onboard depth sensor and camera. However, due to limited depth sensor range of
portable mixed-reality headsets like Microsoft HoloLens, localization based on
simple point cloud registration (sPCR) would require extensive mapping of the
environment. Also, localization based on camera image would face the same
issues as stereo ambiguities and hence depends on viewpoint. We thus introduce
a novel approach to Joint Point Cloud and Image-based Localization (JPIL) for
mixed-reality headsets that use visual cues and headset orientation to register
small, partially overlapped point clouds and save significant manual labor and
time in environment mapping. Our empirical results compared to sPCR show
average 10 fold reduction of required overlap surface area that could
potentially save on average 20 minutes per inspection. JPIL is not only
restricted to inspection tasks but also can be essential in enabling intuitive
human-robot interaction for spatial mapping and scene understanding in
conjunction with other agents like autonomous robotic systems that are
increasingly being deployed in outdoor environments for applications like
structural inspection.","['Manash Pratim Das', 'Zhen Dong', 'Sebastian Scherer']",2018-11-05T23:04:07Z,http://arxiv.org/abs/1811.02563v1,['cs.RO']
Yes and...? Using Improv to Design for Narrative in Lights Out,"Mixed reality experiences often require detailed narrative that can be used
to craft physical and virtual design components. This work elaborates on a
mentoring experience at the Carnegie Mellon's ETC to consider how improv games
may be used ideate and iterate on storytelling experiences.",['Alina Striner'],2018-04-23T19:42:55Z,http://arxiv.org/abs/1804.10685v1,['cs.HC']
"Spatially and color consistent environment lighting estimation using
  deep neural networks for mixed reality","The representation of consistent mixed reality (XR) environments requires
adequate real and virtual illumination composition in real-time. Estimating the
lighting of a real scenario is still a challenge. Due to the ill-posed nature
of the problem, classical inverse-rendering techniques tackle the problem for
simple lighting setups. However, those assumptions do not satisfy the current
state-of-art in computer graphics and XR applications. While many recent works
solve the problem using machine learning techniques to estimate the environment
light and scene's materials, most of them are limited to geometry or previous
knowledge. This paper presents a CNN-based model to estimate complex lighting
for mixed reality environments with no previous information about the scene. We
model the environment illumination using a set of spherical harmonics (SH)
environment lighting, capable of efficiently represent area lighting. We
propose a new CNN architecture that inputs an RGB image and recognizes, in
real-time, the environment lighting. Unlike previous CNN-based lighting
estimation methods, we propose using a highly optimized deep neural network
architecture, with a reduced number of parameters, that can learn high complex
lighting scenarios from real-world high-dynamic-range (HDR) environment images.
We show in the experiments that the CNN architecture can predict the
environment lighting with an average mean squared error (MSE) of \num{7.85e-04}
when comparing SH lighting coefficients. We validate our model in a variety of
mixed reality scenarios. Furthermore, we present qualitative results comparing
relights of real-world scenes.","['Bruno Augusto Dorta Marques', 'Esteban Walter Gonzalez Clua', 'Anselmo Antunes Montenegro', 'Cristina Nader Vasconcelos']",2021-08-17T23:03:55Z,http://arxiv.org/abs/2108.07903v1,"['cs.CV', 'cs.GR']"
Mutual Scene Synthesis for Mixed Reality Telepresence,"Remote telepresence via next-generation mixed reality platforms can provide
higher levels of immersion for computer-mediated communications, allowing
participants to engage in a wide spectrum of activities, previously not
possible in 2D screen-based communication methods. However, as mixed reality
experiences are limited to the local physical surrounding of each user, finding
a common virtual ground where users can freely move and interact with each
other is challenging. In this paper, we propose a novel mutual scene synthesis
method that takes the participants' spaces as input, and generates a virtual
synthetic scene that corresponds to the functional features of all
participants' local spaces. Our method combines a mutual function optimization
module with a deep-learning conditional scene augmentation process to generate
a scene mutually and physically accessible to all participants of a mixed
reality telepresence scenario. The synthesized scene can hold mutual walkable,
sittable and workable functions, all corresponding to physical objects in the
users' real environments. We perform experiments using the MatterPort3D dataset
and conduct comparative user studies to evaluate the effectiveness of our
system. Our results show that our proposed approach can be a promising research
direction for facilitating contextualized telepresence systems for
next-generation spatial computing platforms.","['Mohammad Keshavarzi', 'Michael Zollhoefer', 'Allen Y. Yang', 'Patrick Peluse', 'Luisa Caldas']",2022-04-01T02:08:11Z,http://arxiv.org/abs/2204.00161v1,"['cs.HC', 'cs.AI', 'cs.CV']"
"Effects of a mixed reality headset on the delay of visually evoked
  potentials","Virtual and mixed reality (VR, MR) technologies offer a powerful solution for
on-the-ground flight training curricula. While these technologies offer safer
and cheaper instructional programs, it is still unclear how they impact
neuronal brain dynamics. Indeed, MR simulations engage students in a strange
mix of incongruous visual, somatosensory and vestibular sensory input.
Characterizing brain dynamics during MR simulation is important for
understanding cognitive processes during virtual flight training. To this end,
we studies the delays introduced in the neuronal stream from the retina to the
visual cortex when presented with visual stimuli using a Varjo-XR3 headset. We
recorded cortical visual evoked potentials (VEPs) from 6 subjects under two
conditions. First, we recorded normal VEPs triggered by short flashes. Second,
we recorded VEPs triggered by an internal image of the flashes produced by the
Varjo-XR3 headset. All subjects had used the headset before and were familiar
with immersive experiences. Our results show mixed-reality stimulation imposes
a small, but consistent, 4 [ms] processing delay in the N2-VEP component during
MR stimulation as compared to direct stimulation. Also we found that VEP
amplitudes during MR stimulation were also decreased. These results suggest
that visual cognition during mixed-reality training is delayed, not only by the
unavoidalbe hardware/software processing delays of the headset and the attached
computer, but also by an extra biological delay induced by the headset's
limited visual display in terms of the image intensity and contrast. As flight
training is a demanding task, this study measures visual signal latency to
better understand how MR affects the sensation of immersion.","['Víctor Manuel Hidalgo', 'Carlos Andrés Bazaes', 'Juan-Carlos Letelier']",2023-12-04T19:39:32Z,http://arxiv.org/abs/2312.02305v1,['q-bio.NC']
"Eye-tracking in Mixed Reality for Diagnosis of Neurodegenerative
  Diseases","Parkinson's disease ranks as the second most prevalent neurodegenerative
disorder globally. This research aims to develop a system leveraging Mixed
Reality capabilities for tracking and assessing eye movements. In this paper,
we present a medical scenario and outline the development of an application
designed to capture eye-tracking signals through Mixed Reality technology for
the evaluation of neurodegenerative diseases. Additionally, we introduce a
pipeline for extracting clinically relevant features from eye-gaze analysis,
describing the capabilities of the proposed system from a medical perspective.
The study involved a cohort of healthy control individuals and patients
suffering from Parkinson's disease, showcasing the feasibility and potential of
the proposed technology for non-intrusive monitoring of eye movement patterns
for the diagnosis of neurodegenerative diseases.
  Clinical relevance - Developing a non-invasive biomarker for Parkinson's
disease is urgently needed to accurately detect the disease's onset. This would
allow for the timely introduction of neuroprotective treatment at the earliest
stage and enable the continuous monitoring of intervention outcomes. The
ability to detect subtle changes in eye movements allows for early diagnosis,
offering a critical window for intervention before more pronounced symptoms
emerge. Eye tracking provides objective and quantifiable biomarkers, ensuring
reliable assessments of disease progression and cognitive function. The eye
gaze analysis using Mixed Reality glasses is wireless, facilitating convenient
assessments in both home and hospital settings. The approach offers the
advantage of utilizing hardware that requires no additional specialized
attachments, enabling examinations through personal eyewear.","['Mateusz Daniol', 'Daria Hemmerling', 'Jakub Sikora', 'Pawel Jemiolo', 'Marek Wodzinski', 'Magdalena Wojcik-Pedziwiatr']",2024-04-19T16:34:15Z,http://arxiv.org/abs/2404.12984v2,"['cs.HC', 'cs.AI']"
Mixed Reality States in a Bidirectionally Coupled Interreality System,"We present experimental data on the limiting behavior of an interreality
system comprising a virtual horizontally driven pendulum coupled to its
real-world counterpart, where the interaction time scale is much shorter than
the time scale of the dynamical system. We present experimental evidence that
if the physical parameters of the simplified virtual system match those of the
real system within a certain tolerance, there is a transition from an
uncorrelated dual reality state to a mixed reality state of the system in which
the motion of the two pendula is highly correlated. The region in parameter
space for stable solutions has an Arnold tongue structure for both the
experimental data and for a numerical simulation. As virtual systems better
approximate real ones, even weak coupling in other interreality systems may
produce sudden changes to mixed reality states.","['Vadas Gintautas', 'Alfred Hubler']",2006-11-29T22:33:12Z,http://arxiv.org/abs/physics/0611293v2,"['physics.class-ph', 'physics.gen-ph']"
"IRVO: an Interaction Model for designing Collaborative Mixed Reality
  systems","This paper presents an interaction model adapted to mixed reality
environments known as IRVO (Interacting with Real and Virtual Objects). IRVO
aims at modeling the interaction between one or more users and the Mixed
Reality system by representing explicitly the objects and tools involved and
their relationship. IRVO covers the design phase of the life cycle and models
the intended use of the system. In a first part, we present a brief review of
related HCI models. The second part is devoted to the IRVO model, its notation
and some examples. In the third part, we present how IRVO is used for designing
applications and in particular we show how this model can be integrated in a
Model-Based Approach (CoCSys) which is currently designed at our lab.","['René Chalon', 'Bertrand T. David']",2007-07-10T16:01:30Z,http://arxiv.org/abs/0707.1480v1,['cs.HC']
"Perceiving Mass in Mixed Reality through Pseudo-Haptic Rendering of
  Newton's Third Law","In mixed reality, real objects can be used to interact with virtual objects.
However, unlike in the real world, real objects do not encounter any opposite
reaction force when pushing against virtual objects. The lack of reaction force
during manipulation prevents users from perceiving the mass of virtual objects.
Although this could be addressed by equipping real objects with force-feedback
devices, such a solution remains complex and impractical.In this work, we
present a technique to produce an illusion of mass without any active
force-feedback mechanism. This is achieved by simulating the effects of this
reaction force in a purely visual way. A first study demonstrates that our
technique indeed allows users to differentiate light virtual objects from heavy
virtual objects. In addition, it shows that the illusion is immediately
effective, with no prior training. In a second study, we measure the lowest
mass difference (JND) that can be perceived with this technique. The
effectiveness and ease of implementation of our solution provides an
opportunity to enhance mixed reality interaction at no additional cost.","['Paul Issartel', 'Florimond Guéniat', 'Sabine Coquillart', 'Mehdi Ammi']",2016-02-02T08:53:44Z,http://arxiv.org/abs/1602.00831v1,['cs.HC']
"Communicating Robot Arm Motion Intent Through Mixed Reality Head-mounted
  Displays","Efficient motion intent communication is necessary for safe and collaborative
work environments with collocated humans and robots. Humans efficiently
communicate their motion intent to other humans through gestures, gaze, and
social cues. However, robots often have difficulty efficiently communicating
their motion intent to humans via these methods. Many existing methods for
robot motion intent communication rely on 2D displays, which require the human
to continually pause their work and check a visualization. We propose a mixed
reality head-mounted display visualization of the proposed robot motion over
the wearer's real-world view of the robot and its environment. To evaluate the
effectiveness of this system against a 2D display visualization and against no
visualization, we asked 32 participants to labeled different robot arm motions
as either colliding or non-colliding with blocks on a table. We found a 16%
increase in accuracy with a 62% decrease in the time it took to complete the
task compared to the next best system. This demonstrates that a mixed-reality
HMD allows a human to more quickly and accurately tell where the robot is going
to move than the compared baselines.","['Eric Rosen', 'David Whitney', 'Elizabeth Phillips', 'Gary Chien', 'James Tompkin', 'George Konidaris', 'Stefanie Tellex']",2017-08-11T18:28:02Z,http://arxiv.org/abs/1708.03655v1,"['cs.RO', 'cs.HC']"
Mid-Air Haptic Bio-Holograms in Mixed Reality,"We present a prototype demonstrator that integrates three technologies, mixed
reality head-mounted displays, wearable bio-sensors, and mid-air haptic
projectors to deliver an interactive tactile experience with a bio-hologram.
Users of this prototype are able to see, touch and feel a hologram of a heart
that is beating at the same rhythm as their own. The demo uses an Ultrahaptics
device, a Magic Leap One Mixed Reality headset, and an Apple Watch that
measures the wearer's heart rate, all synchronized and networked together such
that updates from the wristband dynamically change the haptic feedback and the
animation speed of the beating heart thus creating a more personalised
experience.","['Ted Romanus', 'Sam Frish', 'Mykola Maksymenko', 'William Frier', 'Loïc Corenthy', 'Orestis Georgiou']",2020-01-06T09:04:51Z,http://arxiv.org/abs/2001.01441v1,['cs.HC']
"Occlusion Handling using Semantic Segmentation and Visibility-Based
  Rendering for Mixed Reality","Real-time occlusion handling is a major problem in outdoor mixed reality
system because it requires great computational cost mainly due to the
complexity of the scene. Using only segmentation, it is difficult to accurately
render a virtual object occluded by complex objects such as trees, bushes etc.
In this paper, we propose a novel occlusion handling method for real-time,
outdoor, and omni-directional mixed reality system using only the information
from a monocular image sequence. We first present a semantic segmentation
scheme for predicting the amount of visibility for different type of objects in
the scene. We also simultaneously calculate a foreground probability map using
depth estimation derived from optical flow. Finally, we combine the
segmentation result and the probability map to render the computer generated
object and the real scene using a visibility-based rendering method. Our
results show great improvement in handling occlusions compared to existing
blending based methods.","['Menandro Roxas', 'Tomoki Hori', 'Taiki Fukiage', 'Yasuhide Okamoto', 'Takeshi Oishi']",2017-07-30T10:01:57Z,http://arxiv.org/abs/1707.09603v1,['cs.CV']
"A Comparison of Visualisation Methods for Disambiguating Verbal Requests
  in Human-Robot Interaction","Picking up objects requested by a human user is a common task in human-robot
interaction. When multiple objects match the user's verbal description, the
robot needs to clarify which object the user is referring to before executing
the action. Previous research has focused on perceiving user's multimodal
behaviour to complement verbal commands or minimising the number of follow up
questions to reduce task time. In this paper, we propose a system for reference
disambiguation based on visualisation and compare three methods to disambiguate
natural language instructions. In a controlled experiment with a YuMi robot, we
investigated real-time augmentations of the workspace in three conditions --
mixed reality, augmented reality, and a monitor as the baseline -- using
objective measures such as time and accuracy, and subjective measures like
engagement, immersion, and display interference. Significant differences were
found in accuracy and engagement between the conditions, but no differences
were found in task time. Despite the higher error rates in the mixed reality
condition, participants found that modality more engaging than the other two,
but overall showed preference for the augmented reality condition over the
monitor and mixed reality conditions.","['Elena Sibirtseva', 'Dimosthenis Kontogiorgos', 'Olov Nykvist', 'Hakan Karaoguz', 'Iolanda Leite', 'Joakim Gustafson', 'Danica Kragic']",2018-01-26T11:24:47Z,http://arxiv.org/abs/1801.08760v1,"['cs.RO', 'cs.HC']"
"Real-time Collaboration Between Mixed Reality Users in Geo-referenced
  Virtual Environment","Collaboration using mixed reality technology is an active area of research,
where significant research is done to virtually bridge physical distances.
There exist a diverse set of platforms and devices that can be used for a
mixed-reality collaboration, and is largely focused for indoor scenarios,
where, a stable tracking can be assumed. We focus on supporting collaboration
between VR and AR users, where AR user is mobile outdoors, and VR user is
immersed in true-sized digital twin. This cross-platform solution requires new
user experiences for interaction, accurate modelling of the real-world, and
working with noisy outdoor tracking sensor such as GPS. In this paper, we
present our results and observations of real-time collaboration between
cross-platform users, in the context of a geo-referenced virtual environment.
We propose a solution for using GPS measurement in VSLAM to localize the AR
user in an outdoor environment. The client applications enable VR and AR user
to collaborate across the heterogeneous platforms seamlessly. The user can
place or load dynamic contents tagged to a geolocation and share their
experience with remote users in real-time.","['Shubham Singh', 'Zengou Ma', 'Daniele Giunchi', 'Anthony Steed']",2020-10-02T14:23:39Z,http://arxiv.org/abs/2010.01023v1,['cs.HC']
"I-nteract 2.0: A Cyber-Physical System to Design 3D Models using Mixed
  Reality Technologies and Deep Learning for Additive Manufacturing","I-nteract is a cyber-physical system that enables real-time interaction with
both virtual and real artifacts to design 3D models for additive manufacturing
by leveraging on mixed reality technologies. This paper presents novel advances
in the development of the interaction platform I-nteract to generate 3D models
using both constructive solid geometry and artificial intelligence. The system
also enables the user to adjust the dimensions of the 3D models with respect to
their physical workspace. The effectiveness of the system is demonstrated by
generating 3D models of furniture (e.g., chairs and tables) and fitting them
into the physical space in a mixed reality environment.","['Ammar Malik', 'Hugo Lhachemi', 'Robert Shorten']",2020-10-21T14:13:21Z,http://arxiv.org/abs/2010.11025v1,"['cs.HC', 'cs.AI', 'cs.LG', 'cs.MA']"
"Considerations and Challenges of Measuring Operator Performance in
  Telepresence and Teleoperation Entailing Mixed Reality Technologies","Assessing human performance in robotic scenarios such as those seen in
telepresence and teleoperation has always been a challenging task. With the
recent spike in mixed reality technologies and the subsequent focus by
researchers, new pathways have opened in elucidating human perception and
maximising overall immersion. Yet with the multitude of different assessment
methods in evaluating operator performance in virtual environments within the
field of HCI and HRI, inter-study comparability and transferability are
limited. In this short paper, we present a brief overview of existing methods
in assessing operator performance including subjective and objective approaches
while also attempting to capture future technical challenges and frontiers. The
ultimate goal is to assist and pinpoint readers towards potentially important
directions with the future hope of providing a unified immersion framework for
teleoperation and telepresence by standardizing a set of guidelines and
evaluation methods.","['Eleftherios Triantafyllidis', 'Zhibin Li']",2021-03-23T17:24:09Z,http://arxiv.org/abs/2103.12702v2,"['cs.HC', 'cs.RO']"
"Virtual, Augmented, and Mixed Reality for Human-Robot Interaction: A
  Survey and Virtual Design Element Taxonomy","Virtual, Augmented, and Mixed Reality for Human-Robot Interaction (VAM-HRI)
has been gaining considerable attention in research in recent years. However,
the HRI community lacks a set of shared terminology and framework for
characterizing aspects of mixed reality interfaces, presenting serious problems
for future research. Therefore, it is important to have a common set of terms
and concepts that can be used to precisely describe and organize the diverse
array of work being done within the field. In this paper, we present a novel
taxonomic framework for different types of VAM-HRI interfaces, composed of four
main categories of virtual design elements (VDEs). We present and justify our
taxonomy and explain how its elements have been developed over the last 30
years as well as the current directions VAM-HRI is headed in the coming decade.","['Michael Walker', 'Thao Phung', 'Tathagata Chakraborti', 'Tom Williams', 'Daniel Szafir']",2022-02-23T00:39:44Z,http://arxiv.org/abs/2202.11249v1,"['cs.RO', 'cs.AI', 'cs.GR', 'cs.HC']"
Remote Assistance with Mixed Reality for Procedural Tasks,"We present a volumetric communication system that is designed for remote
assistance of procedural tasks. The system allows a remote expert to visually
guide a local operator. The two parties share a view that is spatially
identical, but for the local operator it is of the object on which they
operate, while for the remote expert, the object is presented as a mixed
reality ""hologram"". Guidance is provided by voice, gestures, and annotations
performed directly on the object of interest or its hologram. At each end of
the communication, spatial is visualized using mixed-reality glasses.","['Manuel Rebol', 'Colton Hood', 'Claudia Ranniger', 'Adam Rutenberg', 'Neal Sikka', 'Erin Maria Horan', 'Christian Gütl', 'Krzysztof Pietroszek']",2022-08-05T16:29:43Z,http://arxiv.org/abs/2208.03261v1,['cs.HC']
"Thermodynamics-informed neural networks for physically realistic mixed
  reality","The imminent impact of immersive technologies in society urges for active
research in real-time and interactive physics simulation for virtual worlds to
be realistic. In this context, realistic means to be compliant to the laws of
physics. In this paper we present a method for computing the dynamic response
of (possibly non-linear and dissipative) deformable objects induced by
real-time user interactions in mixed reality using deep learning. The
graph-based architecture of the method ensures the thermodynamic consistency of
the predictions, whereas the visualization pipeline allows a natural and
realistic user experience. Two examples of virtual solids interacting with
virtual or physical solids in mixed reality scenarios are provided to prove the
performance of the method.","['Quercus Hernández', 'Alberto Badías', 'Francisco Chinesta', 'Elías Cueto']",2022-10-24T17:30:08Z,http://arxiv.org/abs/2210.13414v2,"['cs.GR', 'cs.AI', 'cs.LG', 'math.DS']"
"A mixed-reality dataset for category-level 6D pose and size estimation
  of hand-occluded containers","Estimating the 6D pose and size of household containers is challenging due to
large intra-class variations in the object properties, such as shape, size,
appearance, and transparency. The task is made more difficult when these
objects are held and manipulated by a person due to varying degrees of hand
occlusions caused by the type of grasps and by the viewpoint of the camera
observing the person holding the object. In this paper, we present a
mixed-reality dataset of hand-occluded containers for category-level 6D object
pose and size estimation. The dataset consists of 138,240 images of rendered
hands and forearms holding 48 synthetic objects, split into 3 grasp categories
over 30 real backgrounds. We re-train and test an existing model for 6D object
pose estimation on our mixed-reality dataset. We discuss the impact of the use
of this dataset in improving the task of 6D pose and size estimation.","['Xavier Weber', 'Alessio Xompero', 'Andrea Cavallaro']",2022-11-18T19:14:52Z,http://arxiv.org/abs/2211.10470v1,['cs.CV']
"Point Cloud Registration of non-rigid objects in sparse 3D Scans with
  applications in Mixed Reality","Point Cloud Registration is the problem of aligning the corresponding points
of two 3D point clouds referring to the same object. The challenges include
dealing with noise and partial match of real-world 3D scans. For non-rigid
objects, there is an additional challenge of accounting for deformations in the
object shape that happen to the object in between the two 3D scans. In this
project, we study the problem of non-rigid point cloud registration for use
cases in the Augmented/Mixed Reality domain. We focus our attention on a
special class of non-rigid deformations that happen in rigid objects with parts
that move relative to one another about joints, for example, robots with hands
and machines with hinges. We propose an efficient and robust point-cloud
registration workflow for such objects and evaluate it on real-world data
collected using Microsoft Hololens 2, a leading Mixed Reality device.",['Manorama Jha'],2022-12-07T18:54:32Z,http://arxiv.org/abs/2212.03856v2,"['cs.CV', 'cs.GR', 'cs.HC']"
"Extending the Metaverse: Hyper-Connected Smart Environments with Mixed
  Reality and the Internet of Things","The metaverse, i.e., the collection of technologies that provide a virtual
twin of the real world via mixed reality, internet of things, and others, is
gaining prominence. However, the metaverse faces challenges as it grows toward
mainstream adoption. Among these is the lack of strong connections between
metaverse objects and traditional physical objects and environments, which
leads to inconsistencies for users within metaverse environments. To address
this issue, this work explores the design and development of a framework for
bridging the physical environment and the metaverse through the use of
internet-of-things objects and mixed reality designs. The contributions of this
include: i) an architectural framework for extending the metaverse, ii) design
prototypes using the framework. Together, this exploration charts the course
toward a more cohesive and hyper-connected metaverse smart environment.","['Jie Guan', 'Alexis Morris', 'Jay Irizawa']",2023-06-01T20:42:49Z,http://arxiv.org/abs/2306.01137v1,['cs.HC']
"HoloBots: Augmenting Holographic Telepresence with Mobile Robots for
  Tangible Remote Collaboration in Mixed Reality","This paper introduces HoloBots, a mixed reality remote collaboration system
that augments holographic telepresence with synchronized mobile robots. Beyond
existing mixed reality telepresence, HoloBots lets remote users not only be
visually and spatially present, but also physically engage with local users and
their environment. HoloBots allows the users to touch, grasp, manipulate, and
interact with the remote physical environment as if they were co-located in the
same shared space. We achieve this by synchronizing holographic user motion
(Hololens 2 and Azure Kinect) with tabletop mobile robots (Sony Toio). Beyond
the existing physical telepresence, HoloBots contributes to an exploration of
broader design space, such as object actuation, virtual hand physicalization,
world-in-miniature exploration, shared tangible interfaces, embodied guidance,
and haptic communication. We evaluate our system with twelve participants by
comparing it with hologram-only and robot-only conditions. Both quantitative
and qualitative results confirm that our system significantly enhances the
level of co-presence and shared experience, compared to the other conditions.","['Keiichi Ihara', 'Mehrad Faridan', 'Ayumi Ichikawa', 'Ikkaku Kawaguchi', 'Ryo Suzuki']",2023-07-30T03:20:12Z,http://arxiv.org/abs/2307.16114v1,"['cs.HC', 'cs.RO']"
"RICO-MR: An Open-Source Architecture for Robot Intent Communication
  through Mixed Reality","This article presents an open-source architecture for conveying robots'
intentions to human teammates using Mixed Reality and Head-Mounted Displays.
The architecture has been developed focusing on its modularity and re-usability
aspects. Both binaries and source code are available, enabling researchers and
companies to adopt the proposed architecture as a standalone solution or to
integrate it in more comprehensive implementations. Due to its scalability, the
proposed architecture can be easily employed to develop shared Mixed Reality
experiences involving multiple robots and human teammates in complex
collaborative scenarios.","['Simone Macciò', 'Mohamad Shaaban', 'Alessandro Carfì', 'Renato Zaccaria', 'Fulvio Mastrogiovanni']",2023-09-09T11:35:09Z,http://arxiv.org/abs/2309.04765v1,['cs.RO']
"Using Terrestrial Laser Scanning, Unmanned Aerial Vehicles and Mixed
  Reality Methodologies for Digital Survey, 3D Modelling and Historical
  Recreation of Religious Heritage Monuments","Preserving and safeguarding the Cultural Heritage (CH) of our world from
unforeseen hazards should be viewed as a collective responsibility for
humanity. Consequently, there is a growing imperative for targeted measures
aimed at conserving, rejuvenating, and safeguarding historical assets that
carry cultural significance. In recent times, Terrestrial Laser Scanning (TLS),
Unmanned Aerial Vehicle (UAV) Photogrammetry, and applications in Mixed Reality
(MR) have assumed a pivotal role in the mapping, recording, preservation, and
promotion of Cultural Heritage. This ar-ticle endeavors to present a
comprehensive approach spanning from 3D surveying to the 3D representation and
promotion of Religious Cultural Heritage, offering an overview of the applied
methodologies. Through the integration of TLS and UAV photogrammetry
techniques, a comprehensive digital record of Panagia Ekatontapyliani, the
adjoining Church of Agios Nikolaos, and the Baptistery, along with their wall
paintings (hagiographies) and natural surroundings, has been obtained. This
record serves as the foundation for historical documentation and recreation
using the HBIM concept, paving the way for the development of diverse Mixed
Reality applications. These applications aim to enhance the visibility,
accessibility, and visitability of the Monument.","['Aristeidis Zachos', 'Christos-Nikolaos Anagnostopoulos']",2023-12-31T16:24:22Z,http://arxiv.org/abs/2401.01380v1,['cs.OH']
"Motion Control of Interactive Robotic Arms Based on Mixed Reality
  Development","Mixed Reality (MR) is constantly evolving to inspire new patterns of robot
manipulation for more advanced Human- Robot Interaction under the 4th
Industrial Revolution Paradigm. Consider that Mixed Reality aims to connect
physical and digital worlds to provide special immersive experiences, it is
necessary to establish the information exchange platform and robot control
systems within the developed MR scenarios. In this work, we mainly present
multiple effective motion control methods applied on different interactive
robotic arms (e.g., UR5, UR5e, myCobot) for the Unity-based development of MR
applications, including GUI control panel, text input control panel,
end-effector object dynamic tracking and ROS-Unity digital-twin connection.",['Hanxiao Chen'],2024-01-03T09:38:08Z,http://arxiv.org/abs/2401.01644v1,"['cs.RO', 'cs.HC']"
IllusionX: An LLM-powered mixed reality personal companion,"Mixed Reality (MR) and Artificial Intelligence (AI) are increasingly becoming
integral parts of our daily lives. Their applications range in fields from
healthcare to education to entertainment. MR has opened a new frontier for such
fields as well as new methods of enhancing user engagement. In this paper, We
propose a new system one that combines the power of Large Language Models
(LLMs) and mixed reality (MR) to provide a personalized companion for
educational purposes. We present an overview of its structure and components as
well tests to measure its performance. We found that our system is better in
generating coherent information, however it's rather limited by the documents
provided to it. This interdisciplinary approach aims to provide a better user
experience and enhance user engagement. The user can interact with the system
through a custom-design smart watch, smart glasses and a mobile app.","['Ramez Yousri', 'Zeyad Essam', 'Yehia Kareem', 'Youstina Sherief', 'Sherry Gamil', 'Soha Safwat']",2024-02-04T15:52:41Z,http://arxiv.org/abs/2402.07924v1,"['cs.HC', 'cs.MM']"
"MRNaB: Mixed Reality-based Robot Navigation Interface using
  Optical-see-through MR-beacon","Recent advancements in robotics have led to the development of numerous
interfaces to enhance the intuitiveness of robot navigation. However, the
reliance on traditional 2D displays imposes limitations on the simultaneous
visualization of information. Mixed Reality (MR) technology addresses this
issue by enhancing the dimensionality of information visualization, allowing
users to perceive multiple pieces of information concurrently. This paper
proposes Mixed reality-based robot navigation interface using an
optical-see-through MR-beacon (MRNaB), a novel approach that incorporates an
MR-beacon, situated atop the real-world environment, to function as a signal
transmitter for robot navigation. This MR-beacon is designed to be persistent,
eliminating the need for repeated navigation inputs for the same location. Our
system is mainly constructed into four primary functions: ""Add"", ""Move"",
""Delete"", and ""Select"". These allow for the addition of a MR-beacon, location
movement, its deletion, and the selection of MR-beacon for navigation purposes,
respectively. The effectiveness of the proposed method was then validated
through experiments by comparing it with the traditional 2D system. As the
result, MRNaB was proven to increase the performance of the user when doing
navigation to a certain place subjectively and objectively. For additional
material, please check: https://mertcookimg.github.io/mrnab","['Eduardo Iglesius', 'Masato Kobayashi', 'Yuki Uranishi', 'Haruo Takemura']",2024-03-28T10:56:38Z,http://arxiv.org/abs/2403.19310v1,['cs.RO']
"Mixed Reality Heritage Performance As a Decolonising Tool for Heritage
  Sites","In this paper we introduce two world-first Mixed Reality (MR) experiences
that fuse smart AR glasses and live theatre and take place in a heritage site
with the purpose to reveal the site's hidden and difficult histories about
slavery. We term these unique general audience experiences Mixed Reality
Heritage Performances (MRHP). Along with the development of our initial two
performances we designed and developed a tool and guidelines that can help
heritage organisations with their decolonising process by critically engaging
the public with under-represented voices and viewpoints of troubled European
and colonial narratives. The evaluations showed the embodied and affective
potential of MRHP to attract and educate heritage audiences visitors. Insights
of the design process are being formulated into an extensive design toolkit
that aims to support experience design, theatre and heritage professionals to
collaboratively carry out similar projects.","['Mariza Dima', 'Damon Daylamani-Zad', 'Vangelis Lympouridis']",2024-04-10T21:08:06Z,http://arxiv.org/abs/2404.07348v1,['cs.HC']
"Enhancing Sign Language Teaching: A Mixed Reality Approach for Immersive
  Learning and Multi-Dimensional Feedback","Traditional sign language teaching methods face challenges such as limited
feedback and diverse learning scenarios. Although 2D resources lack real-time
feedback, classroom teaching is constrained by a scarcity of teacher. Methods
based on VR and AR have relatively primitive interaction feedback mechanisms.
This study proposes an innovative teaching model that uses real-time monocular
vision and mixed reality technology. First, we introduce an improved
hand-posture reconstruction method to achieve sign language semantic retention
and real-time feedback. Second, a ternary system evaluation algorithm is
proposed for a comprehensive assessment, maintaining good consistency with
experts in sign language. Furthermore, we use mixed reality technology to
construct a scenario-based 3D sign language classroom and explore the user
experience of scenario teaching. Overall, this paper presents a novel teaching
method that provides an immersive learning experience, advanced posture
reconstruction, and precise feedback, achieving positive feedback on user
experience and learning effectiveness.","['Hongli Wen', 'Yang Xu', 'Lin Li', 'Xudong Ru', 'Xingce Wang', 'Zhongke Wu']",2024-04-16T11:57:03Z,http://arxiv.org/abs/2404.10490v2,['cs.CV']
"Discussing Risks and Benefits in the Future of Hybrid Rehabilitation and
  Fitness in Mixed Reality","In a world where in-person context transitions more into remote and hybrid
concepts, we should consider new concepts of interaction in health and
rehabilitation and what advantages and disadvantages they bring. One of the
rising topics is mixed reality, where we can use the advantages of immersive
3D, 360-degree environments. Meanwhile, physical activity is further decreasing
and with it negative effects increase through sedentary behaviour or wrong and
untrained movements. In this position paper, we discuss these new risks and
potential benefits of mixed reality technology when used for rehabilitation and
fitness. We conclude with suggesting better feedback and guidance for physical
movement and tasks at home. Improving feedback and guidance for participants
could be achieved through using new technologies like virtual reality and
motion tracking.","['Jana Franceska Funke', 'Enrico Rukzio']",2024-05-16T12:49:52Z,http://arxiv.org/abs/2405.10059v1,['cs.HC']
"Video2MR: Automatically Generating Mixed Reality 3D Instructions by
  Augmenting Extracted Motion from 2D Videos","This paper introduces Video2MR, a mixed reality system that automatically
generates 3D sports and exercise instructions from 2D videos. Mixed reality
instructions have great potential for physical training, but existing works
require substantial time and cost to create these 3D experiences. Video2MR
overcomes this limitation by transforming arbitrary instructional videos
available online into MR 3D avatars with AI-enabled motion capture
(DeepMotion). Then, it automatically enhances the avatar motion through the
following augmentation techniques: 1) contrasting and highlighting differences
between the user and avatar postures, 2) visualizing key trajectories and
movements of specific body parts, 3) manipulation of time and speed using body
motion, and 4) spatially repositioning avatars for different perspectives.
Developed on Hololens 2 and Azure Kinect, we showcase various use cases,
including yoga, dancing, soccer, tennis, and other physical exercises. The
study results confirm that Video2MR provides more engaging and playful learning
experiences, compared to existing 2D video instructions.","['Keiichi Ihara', 'Kyzyl Monteiro', 'Mehrad Faridan', 'Rubaiat Habib Kazi', 'Ryo Suzuki']",2024-05-28T20:19:38Z,http://arxiv.org/abs/2405.18565v1,['cs.HC']
"Technical Note: Towards Virtual Monitors for Image Guided Interventions
  - Real-time Streaming to Optical See-Through Head-Mounted Displays","Purpose: Image guidance is crucial for the success of many interventions.
Images are displayed on designated monitors that cannot be positioned optimally
due to sterility and spatial constraints. This indirect visualization causes
potential occlusion, hinders hand-eye coordination, leads to increased
procedure duration and surgeon load. Methods: We propose a virtual monitor
system that displays medical images in a mixed reality visualization using
optical see-through head-mounted displays. The system streams high-resolution
medical images from any modality to the head-mounted display in real-time that
are blended with the surgical site. It allows for mixed reality visualization
of images in head-, world-, or body-anchored mode and can thus be adapted to
specific procedural needs. Results: For typical image sizes, the proposed
system exhibits an average end-to-end delay and refresh rate of 214 +- 30 ms
and 41:4 +- 32:0 Hz, respectively. Conclusions: The proposed virtual monitor
system is capable of real-time mixed reality visualization of medical images.
In future, we seek to conduct first pre-clinical studies to quantitatively
assess the impact of the system on standard image guided procedures.","['Long Qian', 'Mathias Unberath', 'Kevin Yu', 'Bernhard Fuerst', 'Alex Johnson', 'Nassir Navab', 'Greg Osgood']",2017-10-02T17:39:07Z,http://arxiv.org/abs/1710.00808v1,"['cs.OH', 'cs.HC']"
"Multi-Vehicle Mixed-Reality Reinforcement Learning for Autonomous
  Multi-Lane Driving","Autonomous driving promises to transform road transport. Multi-vehicle and
multi-lane scenarios, however, present unique challenges due to constrained
navigation and unpredictable vehicle interactions. Learning-based
methods---such as deep reinforcement learning---are emerging as a promising
approach to automatically design intelligent driving policies that can cope
with these challenges. Yet, the process of safely learning multi-vehicle
driving behaviours is hard: while collisions---and their near-avoidance---are
essential to the learning process, directly executing immature policies on
autonomous vehicles raises considerable safety concerns. In this article, we
present a safe and efficient framework that enables the learning of driving
policies for autonomous vehicles operating in a shared workspace, where the
absence of collisions cannot be guaranteed. Key to our learning procedure is a
sim2real approach that uses real-world online policy adaptation in a
mixed-reality setup, where other vehicles and static obstacles exist in the
virtual domain. This allows us to perform safe learning by simulating (and
learning from) collisions between the learning agent(s) and other objects in
virtual reality. Our results demonstrate that, after only a few runs in
mixed-reality, collisions are significantly reduced.","['Rupert Mitchell', 'Jenny Fletcher', 'Jacopo Panerati', 'Amanda Prorok']",2019-11-26T17:08:40Z,http://arxiv.org/abs/1911.11699v2,"['cs.RO', 'cs.AI', 'cs.LG', 'cs.MA', 'I.2.6; I.2.9']"
"Artificial Intelligence Assisted Infrastructure Assessment Using Mixed
  Reality Systems","Conventional methods for visual assessment of civil infrastructures have
certain limitations, such as subjectivity of the collected data, long
inspection time, and high cost of labor. Although some new technologies i.e.
robotic techniques that are currently in practice can collect objective,
quantified data, the inspectors own expertise is still critical in many
instances since these technologies are not designed to work interactively with
human inspector. This study aims to create a smart, human centered method that
offers significant contributions to infrastructure inspection, maintenance,
management practice, and safety for the bridge owners. By developing a smart
Mixed Reality framework, which can be integrated into a wearable holographic
headset device, a bridge inspector, for example, can automatically analyze a
certain defect such as a crack that he or she sees on an element, display its
dimension information in real-time along with the condition state. Such systems
can potentially decrease the time and cost of infrastructure inspections by
accelerating essential tasks of the inspector such as defect measurement,
condition assessment and data processing to management systems. The human
centered artificial intelligence will help the inspector collect more
quantified and objective data while incorporating inspectors professional
judgement. This study explains in detail the described system and related
methodologies of implementing attention guided semi supervised deep learning
into mixed reality technology, which interacts with the human inspector during
assessment. Thereby, the inspector and the AI will collaborate or communicate
for improved visual inspection.","['Enes Karaaslan', 'Ulas Bagci', 'F. Necati Catbas']",2018-12-09T19:46:00Z,http://arxiv.org/abs/1812.05659v1,"['cs.CV', 'cs.AI', 'cs.HC', 'cs.LG']"
"Mix&Match: Towards Omitting Modelling Through In-Situ Alteration and
  Remixing of Model Repository Artifacts in Mixed Reality","The accessibility of tools to model artifacts is one of the core driving
factors for the adoption of Personal Fabrication. Subsequently, model
repositories like Thingiverse became important tools in (novice) makers'
processes. They allow them to shorten or even omit the design process,
offloading a majority of the effort to other parties. However, steps like
measurement of surrounding constraints (e.g., clearance) which exist only
inside the users' environment, can not be similarly outsourced. We propose
Mix&Match a mixed-reality-based system which allows users to browse model
repositories, preview the models in-situ, and adapt them to their environment
in a simple and immediate fashion. Mix&Match aims to provide users with CSG
operations which can be based on both virtual and real geometry. We present
interaction patterns and scenarios for Mix&Match, arguing for the combination
of mixed reality and model repositories. This enables almost modelling-free
personal fabrication for both novices and expert makers.","['Evgeny Stemasov', 'Tobias Wagner', 'Jan Gugenheimer', 'Enrico Rukzio']",2020-03-20T10:03:51Z,http://arxiv.org/abs/2003.09169v1,"['cs.HC', 'H.5.2']"
"MED1stMR: Mixed Reality to Enhance Training of Medical First
  Responder]{MED1stMR: Mixed Reality to Enhance the Training of Medical First
  Responders for Challenging Contexts","Mass-casualty incidents with a large number of injured persons caused by
human-made or by natural disasters are increasing globally. In such situations,
medical first responders (MFRs) need to perform diagnosis, basic life support,
or other first aid to help stabilize victims and keep them alive to wait for
the arrival of further support. Situational awareness and effective coping with
acute stressors is essential to enable first responders to take appropriate
action that saves lives.
  Virtual Reality (VR) has been demonstrated in several domains to be a serious
alternative, and in some areas also a significant improvement to conventional
learning and training. Especially for the challenges in the training of MFRs,
it can be highly useful for practicing and learning domains where the context
of the training is not easily available. VR training offers controlled,
easy-to-create environments that can be created and trained repeatedly under
the same conditions.
  As an advanced alternative to VR, Mixed Reality (MR) environments have the
potential to augment current VR training by providing a dynamic simulation of
an environment and hands-on practice on injured victims. Building on this
interpretation of MR, the main aim of MED1stMR is to develop a new generation
of MR training with haptic feedback for enhanced realism. in this workshop
paper, we will present the vision of the project and suggest questions for
discussion.","['Helmut Schrom-Feiertag', 'Georg Regal', 'Markus Murtinger']",2023-01-30T18:01:32Z,http://arxiv.org/abs/2301.13124v1,"['cs.CY', 'cs.HC']"
"A Mixed Reality System for Interaction with Heterogeneous Robotic
  Systems","The growing spread of robots for service and industrial purposes calls for
versatile, intuitive and portable interaction approaches. In particular, in
industrial environments, operators should be able to interact with robots in a
fast, effective, and possibly effortless manner. To this end, reality
enhancement techniques have been used to achieve efficient management and
simplify interactions, in particular in manufacturing and logistics processes.
Building upon this, in this paper we propose a system based on mixed reality
that allows a ubiquitous interface for heterogeneous robotic systems in dynamic
scenarios, where users are involved in different tasks and need to interact
with different robots. By means of mixed reality, users can interact with a
robot through manipulation of its virtual replica, which is always colocated
with the user and is extracted when interaction is needed. The system has been
tested in a simulated intralogistics setting, where different robots are
present and require sporadic intervention by human operators, who are involved
in other tasks. In our setting we consider the presence of drones and AGVs with
different levels of autonomy, calling for different user interventions. The
proposed approach has been validated in virtual reality, considering
quantitative and qualitative assessment of performance and user's feedback.","['Valeria Villani', 'Beatrice Capelli', 'Lorenzo Sabattini']",2023-07-11T14:18:25Z,http://arxiv.org/abs/2307.05280v2,['cs.RO']
"Experiences with CAMRE: Single-Device Collaborative Adaptive Mixed
  Reality Environment","During collaboration in XR (eXtended Reality), users typically share and
interact with virtual objects in a common, shared virtual environment.
Specifically, collaboration among users in Mixed Reality (MR) requires knowing
their position, movement, and understanding of the visual scene surrounding
their physical environments. Otherwise, one user could move an important
virtual object to a position blocked by the physical environment for others.
However, even for a single physical environment, 3D reconstruction takes a long
time and the produced 3D data is typically very large in size. Also, these
large amounts of 3D data take a long time to be streamed to receivers making
real-time updates on the rendered scene challenging. Furthermore, many
collaboration systems in MR require multiple devices, which take up space and
make setup difficult. To address these challenges, in this paper, we describe a
single-device system called Collaborative Adaptive Mixed Reality Environment
(CAMRE). We build CAMRE using the scene understanding capabilities of HoloLens
2 devices to create shared MR virtual environments for each connected user and
demonstrate using a Leader-Follower(s) paradigm: faster reconstruction and
scene update times due to smaller data. Consequently, multiple users can
receive shared, synchronized, and close-to-real-time latency virtual scenes
from a chosen Leader, based on their physical position and movement. We also
illustrate other expanded features of CAMRE MR virtual environment such as
navigation using a real-time virtual mini-map and X-ray vision for handling
adaptive wall opacity. We share several experimental results that evaluate the
performance of CAMRE in terms of the network latency in sharing virtual objects
and other capabilities.","['Hung-Jui Guo', 'Omeed Eshaghi Ashtiani', 'Balakrishnan Prabhakaran']",2023-10-08T03:48:04Z,http://arxiv.org/abs/2310.04996v1,['cs.HC']
"Mixed Reality Communication for Medical Procedures: Teaching the
  Placement of a Central Venous Catheter","Medical procedures are an essential part of healthcare delivery, and the
acquisition of procedural skills is a critical component of medical education.
Unfortunately, procedural skill is not evenly distributed among medical
providers. Skills may vary within departments or institutions, and across
geographic regions, depending on the provider's training and ongoing
experience. We present a mixed reality real-time communication system to
increase access to procedural skill training and to improve remote emergency
assistance. Our system allows a remote expert to guide a local operator through
a medical procedure. RGBD cameras capture a volumetric view of the local scene
including the patient, the operator, and the medical equipment. The volumetric
capture is augmented onto the remote expert's view to allow the expert to
spatially guide the local operator using visual and verbal instructions. We
evaluated our mixed reality communication system in a study in which experts
teach the ultrasound-guided placement of a central venous catheter (CVC) to
students in a simulation setting. The study compares state-of-the-art video
communication against our system. The results indicate that our system enhances
and offers new possibilities for visual communication compared to video
teleconference-based training.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka', 'David Li', 'Christian Gütl']",2023-12-14T03:11:20Z,http://arxiv.org/abs/2312.08624v1,"['cs.CV', 'cs.HC']"
"Implementation of communication media around a mixed reality experience
  with HoloLens headset, as part of a digitalization of a nutrition workshop","The release of Microsoft's HoloLens headset addresses new types of issues
that would have been difficult to design without such a hardware. This
semi-transparent visor headset allows the user who wears it to view the
projection of 3D virtual objects placed in its real environment. The user can
also interact with these 3D objects, which can interact with each other. The
framework of this new technology is called mixed reality. We had the
opportunity to numerically transform a conventional human nutrition workshop
for patients waiting for bariatric surgery by developing a software called
HOLO_NUTRI using the HoloLens headset. Unlike our experience of user and
conventional programmer specialized in the development of interactive 3D
graphics applications, we realized that such a mixed reality experience
required specific programming concepts quite different from those of
conventional software or those of virtual reality applications, but above all
required a thorough reflection about communication for users. In this article,
we propose to explain our design of communication (graphic supports, tutorials
of use of material, explanatory videos), a step which was crucial for the good
progress of our project. The software was used by thirty patients from Le
Puy-en-Velay Hospital during 10 sessions of one hour and a half during which
patients had to take in hand the headset and software HOLO_NUTRI. We also
proposed a series of questions to patients to have an assessment of both the
adequacy and the importance of this communication approach for such experience.
As the mixed reality technology is very recent but the number of applications
based on it significantly increases, the reflection on the implementation of
the elements of communication described in this article (videos, exercise of
learning for the use of the headset, communication leaflet, etc.) can help
developers of such applications.","['Owen Kevin Appadoo', 'Hugo Rositi', 'Sylvie Valarier', 'Marie-Claire Ombret', 'Émilie Gadéa', 'Christine Barret-Grimault', 'Christophe Lohou']",2023-03-23T07:15:28Z,http://arxiv.org/abs/2303.13079v1,"['cs.HC', 'cs.MM', 'H.5.1; J.3.2']"
Recent Developments and Future Challenges in Medical Mixed Reality,"Mixed Reality (MR) is of increasing interest within technology-driven modern
medicine but is not yet used in everyday practice. This situation is changing
rapidly, however, and this paper explores the emergence of MR technology and
the importance of its utility within medical applications. A classification of
medical MR has been obtained by applying an unbiased text mining method to a
database of 1,403 relevant research papers published over the last two decades.
The classification results reveal a taxonomy for the development of medical MR
research during this period as well as suggesting future trends. We then use
the classification to analyse the technology and applications developed in the
last five years. Our objective is to aid researchers to focus on the areas
where technology advancements in medical MR are most needed, as well as
providing medical practitioners with a useful source of reference.","['Long Chen', 'Thomas Day', 'Wen Tang', 'Nigel W. John']",2017-08-03T17:15:18Z,http://arxiv.org/abs/1708.01225v1,['cs.CV']
Sensors in Distributed Mixed Reality Environments,"With the advances in sensors and computer networks an increased number of
Mixed Reality (MR) applications require large amounts of information from the
real world. Such information is collected through sensors (e.g. position and
orientation tracking sensors). These sensors collect data from the physical
environment in real-time at different locations and a distributed system
connecting them must assure data distribution among collaborative sites at
interactive speeds. We propose a new architecture for sensor based interactive
distributed environments that falls in-between the atomistic peer-to-peer model
and the traditional client-server model. Each node in the system is autonomous
and fully manages its resources and connectivity. The dynamic behavior of the
nodes is triggered by the human participants that manipulate the sensors
attached to the nodes.","['Felix G. Hamza-Lup', 'Charles Hughes', 'Jannick P. Rolland']",2018-11-29T04:17:43Z,http://arxiv.org/abs/1811.11955v1,['cs.NI']
DeepLight: Learning Illumination for Unconstrained Mobile Mixed Reality,"We present a learning-based method to infer plausible high dynamic range
(HDR), omnidirectional illumination given an unconstrained, low dynamic range
(LDR) image from a mobile phone camera with a limited field of view (FOV). For
training data, we collect videos of various reflective spheres placed within
the camera's FOV, leaving most of the background unoccluded, leveraging that
materials with diverse reflectance functions reveal different lighting cues in
a single exposure. We train a deep neural network to regress from the LDR
background image to HDR lighting by matching the LDR ground truth sphere images
to those rendered with the predicted illumination using image-based relighting,
which is differentiable. Our inference runs at interactive frame rates on a
mobile device, enabling realistic rendering of virtual objects into real scenes
for mobile mixed reality. Training on automatically exposed and white-balanced
videos, we improve the realism of rendered objects compared to the state-of-the
art methods for both indoor and outdoor scenes.","['Chloe LeGendre', 'Wan-Chun Ma', 'Graham Fyffe', 'John Flynn', 'Laurent Charbonnel', 'Jay Busch', 'Paul Debevec']",2019-04-02T02:15:09Z,http://arxiv.org/abs/1904.01175v1,"['cs.CV', 'cs.GR']"
Registration made easy -- standalone orthopedic navigation with HoloLens,"In surgical navigation, finding correspondence between preoperative plan and
intraoperative anatomy, the so-called registration task, is imperative. One
promising approach is to intraoperatively digitize anatomy and register it with
the preoperative plan. State-of-the-art commercial navigation systems implement
such approaches for pedicle screw placement in spinal fusion surgery. Although
these systems improve surgical accuracy, they are not gold standard in clinical
practice. Besides economical reasons, this may be due to their difficult
integration into clinical workflows and unintuitive navigation feedback.
Augmented Reality has the potential to overcome these limitations.
Consequently, we propose a surgical navigation approach comprising
intraoperative surface digitization for registration and intuitive holographic
navigation for pedicle screw placement that runs entirely on the Microsoft
HoloLens. Preliminary results from phantom experiments suggest that the method
may meet clinical accuracy requirements.","['Florentin Liebmann', 'Simon Roner', 'Marco von Atzigen', 'Florian Wanivenhaus', 'Caroline Neuhaus', 'José Spirig', 'Davide Scaramuzza', 'Reto Sutter', 'Jess Snedeker', 'Mazda Farshad', 'Philipp Fürnstahl']",2020-01-17T09:22:21Z,http://arxiv.org/abs/2001.06209v1,"['cs.CV', 'I.4.1']"
"Collective motion patterns of swarms with delay coupling: theory and
  experiment","The formation of coherent patterns in swarms of interacting self-propelled
autonomous agents is a subject of great interest in a wide range of application
areas, ranging from engineering and physics to biology. In this paper, we model
and experimentally realize a mixed-reality large-scale swarm of delay-coupled
agents. The coupling term is modeled as a delayed communication relay of
position. Our analyses, assuming agents communicating over an Erdos-Renyi
network, demonstrate the existence of stable coherent patterns that can only be
achieved with delay coupling and that are robust to decreasing network
connectivity and heterogeneity in agent dynamics. We also show how the
bifurcation structure for emergence of different patterns changes with
heterogeneity in agent acceleration capabilities and limited connectivity in
the network as a function of coupling strength and delay. Our results are
verified through simulation as well as preliminary experimental results of
delay-induced pattern formation in a mixed-reality swarm.","['Klementyna Szwaykowska', 'Ira B. Schwartz', 'Luis Mier-y-Teran Romero', 'Christoffer R. Heckman', 'Dan Mox', 'M. Ani Hsieh']",2016-01-29T14:44:45Z,http://arxiv.org/abs/1601.08134v1,"['nlin.AO', 'nlin.PS']"
"Die Zukunft sehen: Die Chancen und Herausforderungen der Erweiterten und
  Virtuellen Realität für industrielle Anwendungen","Digitalization offers chances as well as risks for industrial companies. This
article describes how the area of Mixed Reality, with its manifestations
Augmented and Virtual Reality, can support industrial applications in the age
of digitalization. Starting from a historical perspective on Augmented and
Virtual Reality, this article surveys recent developments in the domain of
Mixed Reality, relevant for industrial use cases.
  ---
  Die Digitalisierung bietet f\""ur Industrieunternehmen neue Chancen, stellt
diese jedoch auch vor Herausforderungen. Dieser Artikel beleuchtet wie das
Gebiet der vermischten Realit\""at mit seinen Auspr\""agungen der erweiterten
Realit\""at und der virtuellen Realit\""at f\""ur industriellen Anwendungen im
Zeitalter der Digitalisierung Vorteile schaffen kann. Ausgehend von einer
historischen Betrachtung, werden aktuelle Entwicklungen auf dem Gebiet der
erweiterten und virtuellen Realit\""at diskutiert.",['Jens Grubert'],2017-09-04T16:07:35Z,http://arxiv.org/abs/1709.01020v1,['cs.HC']
Context-Aware Mixed Reality: A Framework for Ubiquitous Interaction,"Mixed Reality (MR) is a powerful interactive technology that yields new types
of user experience. We present a semantic based interactive MR framework that
exceeds the current geometry level approaches, a step change in generating
high-level context-aware interactions. Our key insight is to build semantic
understanding in MR that not only can greatly enhance user experience through
object-specific behaviours, but also pave the way for solving complex
interaction design challenges. The framework generates semantic properties of
the real world environment through dense scene reconstruction and deep image
understanding. We demonstrate our approach with a material-aware prototype
system for generating context-aware physical interactions between the real and
the virtual objects. Quantitative and qualitative evaluations are carried out
and the results show that the framework delivers accurate and fast semantic
information in interactive MR environment, providing effective semantic level
interactions.","['Long Chen', 'Wen Tang', 'Nigel John', 'Tao Ruan Wan', 'Jian Jun Zhang']",2018-03-14T23:38:54Z,http://arxiv.org/abs/1803.05541v1,['cs.CV']
"Immercity: a curation content application in Virtual and Augmented
  reality","When working with emergent and appealing technologies as Virtual Reality,
Mixed Reality and Augmented Reality, the issue of definitions appear very
often. Indeed, our experience with various publics allows us to notice that
technology definitions pose ambiguity and representation problems for informed
as well as novice users. In this paper we present Immercity, a content curation
system designed in the context of a collaboration between the University of
Montpellier and CapGemi-ni, to deliver a technology watch. It is also used as a
testbed for our experiences with Virtual, Mixed and Augmented reality to
explore new interaction techniques and devices, artificial intelligence
integration, visual affordances, performance , etc. But another, very
interesting goal appeared: use Immercity to communicate about Virtual, Mixed
and Augmented Reality by using them as a support.","['Jean-Daniel Taupiac', 'Nancy Rodriguez', 'Olivier Strauss']",2018-10-24T06:23:46Z,http://arxiv.org/abs/1810.10206v1,"['cs.GR', 'cs.HC']"
"SlingDrone: Mixed Reality System for Pointing and Interaction Using a
  Single Drone","We propose SlingDrone, a novel Mixed Reality interaction paradigm that
utilizes a micro-quadrotor as both pointing controller and interactive robot
with a slingshot motion type. The drone attempts to hover at a given position
while the human pulls it in desired direction using a hand grip and a leash.
Based on the displacement, a virtual trajectory is defined. To allow for
intuitive and simple control, we use virtual reality (VR) technology to trace
the path of the drone based on the displacement input. The user receives force
feedback propagated through the leash. Force feedback from SlingDrone coupled
with visualized trajectory in VR creates an intuitive and user friendly
pointing device. When the drone is released, it follows the trajectory that was
shown in VR. Onboard payload (e.g. magnetic gripper) can perform various
scenarios for real interaction with the surroundings, e.g. manipulation or
sensing. Unlike HTC Vive controller, SlingDrone does not require handheld
devices, thus it can be used as a standalone pointing technology in VR.","['Evgeny Tsykunov', 'Roman Ibrahimov', 'Derek Vasquez', 'Dzmitry Tsetserukou']",2019-11-12T05:30:24Z,http://arxiv.org/abs/1911.04680v1,"['cs.RO', 'cs.HC']"
"Exploring the Effectiveness of Face-to-face Mixed Reality for Teaching
  with Chalktalk","Teaching that uses projected presentation media such as slide-shows lacks
support for dynamic content whose form and behaviors require live changes
during a lecture. Recent software alternatives such as the Chalktalk software
platform allow the creation of interactive simulations in arbitrary sequences
and combinations within presentations. These more dynamic solutions, however,
do not optimize for face-to-face interactions: eye-contact, gaze direction, and
concurrent awareness of another person's movements together with the presented
content. To explore the extent to which these face-to-face interactions may
improve learning and engagement during a lecture, we propose a Mixed Reality
(MR) platform that places Chalktalk's behaviors and simulations within a
mirrored virtual world environment designed for face-to-face, one-on-one
interactions. We compare our system with projected Chalktalk to evaluate its
relative effectiveness for learning, retention, and level of engagement.","['Zhenyi He', 'Ken Perlin']",2019-12-09T05:54:59Z,http://arxiv.org/abs/1912.03863v5,['cs.HC']
"Switchable Virtual, Augmented, and Mixed Reality through Optical
  Cloaking","A switchable virtual reality (VR), augmented reality (AR), and mixed reality
(MR) system is proposed using digital optical cloaking. Optical cloaking allows
completely opaque VR devices to be ""cloaked,"" switching to AR or MR while
providing correct three-dimensional (3D) parallax and perspective of the real
world, without the need for transparent optics. On the other hand, 3D capture
and display devices with non-zero thicknesses, require optical cloaking to
properly display captured reality. A simplified stereoscopic system with two
cameras and existing VR systems can be an approximation for limited VR, AR, or
MR. To provide true 3D visual effects, multiple input cameras, a 3D display,
and a simple linear calculation amounting to cloaking can be used. Since the
display size requirements for VR, AR, and MR are usually small, with increasing
computing power and pixel densities, the framework presented here can provide a
widely deployable VR, AR, MR design.",['Joseph S. Choi'],2018-02-06T07:18:51Z,http://arxiv.org/abs/1802.01826v1,['physics.optics']
"Scene Synchronization for Real-Time Interaction in Distributed Mixed
  Reality and Virtual Reality Environments","Advances in computer networks and rendering systems facilitate the creation
of distributed collaborative environments in which the distribution of
information at remote locations allows efficient communication. One of the
challenges in networked virtual environments is maintaining a consistent view
of the shared state in the presence of inevitable network latency and jitter. A
consistent view in a shared scene may significantly increase the sense of
presence among participants and facilitate their interactivity. The dynamic
shared state is directly affected by the frequency of actions applied on the
objects in the scene. Mixed Reality (MR) and Virtual Reality (VR) environments
contain several types of action producers including human users, a wide range
of electronic motion sensors, and haptic devices. In this paper, the authors
propose a novel criterion for categorization of distributed MR/VR systems and
present an adaptive synchronization algorithm for distributed MR/VR
collaborative environments. In spite of significant network latency, results
show that for low levels of update frequencies the dynamic shared state can be
maintained consistent at multiple remotely located sites.","['Felix G. Hamza-Lup', 'Jannick P. Rolland']",2018-12-08T14:01:49Z,http://arxiv.org/abs/1812.03322v1,"['cs.NI', 'cs.MM']"
"A Parametric Perceptual Deficit Modeling and Diagnostics Framework for
  Retina Damage using Mixed Reality","Age-related Macular Degeneration (AMD) is a progressive visual impairment
affecting millions of individuals. Since there is no current treatment for the
disease, the only means of improving the lives of individuals suffering from
the disease is via assistive technologies. In this paper we propose a novel and
effective methodology to accurately generate a parametric model for the
perceptual deficit caused by the physiological deterioration of a patient's
retina due to AMD. Based on the parameters of the model, a mechanism is
developed to simulate the patient's perception as a result of the disease. This
simulation can effectively deliver the perceptual impact and its progression to
the patient's eye doctor. In addition, we propose a mixed-reality apparatus and
interface to allow the patient recover functional vision and to compensate for
the perceptual loss caused by the physiological damage. The results obtained by
the proposed approach show the superiority of our framework over the
state-of-the-art low-vision systems.","['Prithul Aniruddha', 'Nasif Zaman', 'Alireza Tavakkoli', 'Stewart Zuckerbrod']",2019-10-17T02:54:26Z,http://arxiv.org/abs/1910.07688v1,"['eess.IV', 'cs.CV']"
"Cloud Rendering-based Volumetric Video Streaming System for Mixed
  Reality Services","Volumetric video is an emerging technology for immersive representation of 3D
spaces that captures objects from all directions using multiple cameras and
creates a dynamic 3D model of the scene. However, processing volumetric content
requires high amounts of processing power and is still a very demanding task
for today's mobile devices. To mitigate this, we propose a volumetric video
streaming system that offloads the rendering to a powerful cloud/edge server
and only sends the rendered 2D view to the client instead of the full
volumetric content. We use 6DoF head movement prediction techniques, WebRTC
protocol and hardware video encoding to ensure low-latency in different parts
of the processing chain. We demonstrate our system using both a browser-based
client and a Microsoft HoloLens client. Our application contains generic
interfaces that allow for easy deployment of various augmented/mixed reality
clients using the same server implementation.","['Serhan Gül', 'Dimitri Podborski', 'Jangwoo Son', 'Gurdeep Singh Bhullar', 'Thomas Buchholz', 'Thomas Schierl', 'Cornelius Hellge']",2020-03-05T10:44:37Z,http://arxiv.org/abs/2003.02526v2,"['cs.MM', 'eess.IV']"
Delay Induced Swarm Pattern Bifurcations in Mixed Reality Experiments,"Swarms of coupled mobile agents subject to inter-agent wireless communication
delays are known to exhibit multiple dynamic patterns in space that depend on
the strength of the interactions and the magnitude of the communication delays.
We experimentally demonstrate communication delay-induced bifurcations in the
spatio-temporal patterns of robot swarms using two distinct hardware platforms
in a mixed reality framework. Additionally, we make steps toward experimentally
validating theoretically predicted parameter regions where transitions between
swarm patterns occur. We show that multiple rotation patterns persist even when
collision-avoidance strategies are incorporated, and we show the existence of
multi-stable, co-existing rotational patterns not predicted by usual mean field
dynamics. Our experiments are the first significant steps towards validating
existing theory and the existence and robustness of the delay-induced patterns
in real robotic swarms.","['Victoria Edwards', 'Philip deZonia', 'M. Ani Hsieh', 'Jason Hindes', 'Ioana Triandaf', 'Ira B Schwartz']",2020-03-12T19:27:16Z,http://arxiv.org/abs/2003.05986v1,['nlin.AO']
MagicEyes: A Large Scale Eye Gaze Estimation Dataset for Mixed Reality,"With the emergence of Virtual and Mixed Reality (XR) devices, eye tracking
has received significant attention in the computer vision community. Eye gaze
estimation is a crucial component in XR -- enabling energy efficient rendering,
multi-focal displays, and effective interaction with content. In head-mounted
XR devices, the eyes are imaged off-axis to avoid blocking the field of view.
This leads to increased challenges in inferring eye related quantities and
simultaneously provides an opportunity to develop accurate and robust learning
based approaches. To this end, we present MagicEyes, the first large scale eye
dataset collected using real MR devices with comprehensive ground truth
labeling. MagicEyes includes $587$ subjects with $80,000$ images of
human-labeled ground truth and over $800,000$ images with gaze target labels.
We evaluate several state-of-the-art methods on MagicEyes and also propose a
new multi-task EyeNet model designed for detecting the cornea, glints and pupil
along with eye segmentation in a single forward pass.","['Zhengyang Wu', 'Srivignesh Rajendran', 'Tarrence van As', 'Joelle Zimmermann', 'Vijay Badrinarayanan', 'Andrew Rabinovich']",2020-03-18T08:23:57Z,http://arxiv.org/abs/2003.08806v1,"['cs.CV', 'cs.HC', 'cs.LG', 'eess.IV']"
"Mobile Delivery Robots: Mixed Reality-Based Simulation Relying on ROS
  and Unity 3D","In the context of Intelligent Transportation Systems and the delivery of
goods, new technology approaches need to be developed in order to cope with
certain challenges that last mile delivery entails, such as navigation in an
urban environment. Autonomous delivery robots can help overcome these
challenges. We propose a method for performing mixed reality (MR) simulation
with ROS-based robots using Unity, which synchronizes the real and virtual
environment, and simultaneously uses the sensor information of the real robots
to locate themselves and project them into the virtual environment, so that
they can use their virtual doppelganger to perceive the virtual world. Using
this method, real and virtual robots can perceive each other and the
environment in which the other party is located, thereby enabling the exchange
of information between virtual and real objects. Through this approach a more
realistic and reliable simulation can be obtained. Results of the demonstrated
use-cases verified the feasibility and efficiency as well as the stability of
implementing MR using Unity for ROS-based robots.","['Yuzhou Liu', 'Georg Novotny', 'Nikita Smirnov', 'Walter Morales-Alvarez', 'Cristina Olaverri-Monreal']",2020-06-16T08:59:38Z,http://arxiv.org/abs/2006.09002v2,['cs.RO']
"Mixed-Reality Robotic Games: Design Guidelines for Effective
  Entertainment with Consumer Robots","In recent years, there has been an increasing interest in the use of robotic
technology at home. A number of service robots appeared on the market,
supporting customers in the execution of everyday tasks. Roughly at the same
time, consumer level robots started to be used also as toys or gaming
companions. However, gaming possibilities provided by current off-the-shelf
robotic products are generally quite limited, and this fact makes them quickly
loose their attractiveness. A way that has been proven capable to boost robotic
gaming and related devices consists in creating playful experiences in which
physical and digital elements are combined together using Mixed Reality
technologies. However, these games differ significantly from digital- or
physical only experiences, and new design principles are required to support
developers in their creative work. This papers addresses such need, by drafting
a set of guidelines which summarize developments carried out by the research
community and their findings.","['F. Gabriele Pratticò', 'Fabrizio Lamberti']",2020-07-30T15:47:17Z,http://arxiv.org/abs/2007.15538v1,"['cs.HC', 'cs.GR', 'cs.RO']"
HoloLens 2 Research Mode as a Tool for Computer Vision Research,"Mixed reality headsets, such as the Microsoft HoloLens 2, are powerful
sensing devices with integrated compute capabilities, which makes it an ideal
platform for computer vision research. In this technical report, we present
HoloLens 2 Research Mode, an API and a set of tools enabling access to the raw
sensor streams. We provide an overview of the API and explain how it can be
used to build mixed reality applications based on processing sensor data. We
also show how to combine the Research Mode sensor data with the built-in eye
and hand tracking capabilities provided by HoloLens 2. By releasing the
Research Mode API and a set of open-source tools, we aim to foster further
research in the fields of computer vision as well as robotics and encourage
contributions from the research community.","['Dorin Ungureanu', 'Federica Bogo', 'Silvano Galliani', 'Pooja Sama', 'Xin Duan', 'Casey Meekhof', 'Jan Stühmer', 'Thomas J. Cashman', 'Bugra Tekin', 'Johannes L. Schönberger', 'Pawel Olszta', 'Marc Pollefeys']",2020-08-25T19:05:38Z,http://arxiv.org/abs/2008.11239v1,['cs.CV']
A privacy-preserving approach to streaming eye-tracking data,"Eye-tracking technology is being increasingly integrated into mixed reality
devices. Although critical applications are being enabled, there are
significant possibilities for violating user privacy expectations. We show that
there is an appreciable risk of unique user identification even under natural
viewing conditions in virtual reality. This identification would allow an app
to connect a user's personal ID with their work ID without needing their
consent, for example. To mitigate such risks we propose a framework that
incorporates gatekeeping via the design of the application programming
interface and via software-implemented privacy mechanisms. Our results indicate
that these mechanisms can reduce the rate of identification from as much as 85%
to as low as 30%. The impact of introducing these mechanisms is less than
1.5$^\circ$ error in gaze position for gaze prediction. Gaze data streams can
thus be made private while still allowing for gaze prediction, for example,
during foveated rendering. Our approach is the first to support
privacy-by-design in the flow of eye-tracking data within mixed reality use
cases.","['Brendan David-John', 'Diane Hosfelt', 'Kevin Butler', 'Eakta Jain']",2021-02-02T21:43:01Z,http://arxiv.org/abs/2102.01770v2,['cs.HC']
"A Survey on Synchronous Augmented, Virtual and Mixed Reality Remote
  Collaboration Systems","Remote collaboration systems have become increasingly important in today's
society, especially during times where physical distancing is advised.
Industry, research and individuals face the challenging task of collaborating
and networking over long distances. While video and teleconferencing are
already widespread, collaboration systems in augmented, virtual, and mixed
reality are still a niche technology. We provide an overview of recent
developments of synchronous remote collaboration systems and create a taxonomy
by dividing them into three main components that form such systems:
Environment, Avatars, and Interaction. A thorough overview of existing systems
is given, categorising their main contributions in order to help researchers
working in different fields by providing concise information about specific
topics such as avatars, virtual environment, visualisation styles and
interaction. The focus of this work is clearly on synchronised collaboration
from a distance. A total of 82 unique systems for remote collaboration are
discussed, including more than 100 publications and 25 commercial systems.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2021-02-11T13:33:51Z,http://arxiv.org/abs/2102.05998v1,"['cs.HC', 'cs.CV', 'cs.GT']"
Lets Make A Story Measuring MR Child Engagement,"We present the result of a pilot study measuring child engagement with the
Lets Make A Story system, a novel mixed reality, MR, collaborative storytelling
system designed for grandparents and grandchildren. We compare our MR
experience against an equivalent paper story experience. The goal of our pilot
was to test the system with actual child users and assess the goodness of using
metrics of time, user generated story content and facial expression analysis as
metrics of child engagement. We find that multiple confounding variables make
these metrics problematic including attribution of engagement time, spontaneous
non-story related conversation and having the childs full forward face
continuously in view during the story. We present our platform and experiences
and our finding that the strongest metric was user comments in the
post-experiential interview.","['Duotun Wang', 'Jennifer Healey', 'Jing Qian', 'Curtis Wigington', 'Tong Sun', 'Huaishu Peng']",2021-04-13T22:36:50Z,http://arxiv.org/abs/2104.06536v1,"['cs.HC', 'H.5.1; H.5.2']"
Towards Real-World Category-level Articulation Pose Estimation,"Human life is populated with articulated objects. Current Category-level
Articulation Pose Estimation (CAPE) methods are studied under the
single-instance setting with a fixed kinematic structure for each category.
Considering these limitations, we reform this problem setting for real-world
environments and suggest a CAPE-Real (CAPER) task setting. This setting allows
varied kinematic structures within a semantic category, and multiple instances
to co-exist in an observation of real world. To support this task, we build an
articulated model repository ReArt-48 and present an efficient dataset
generation pipeline, which contains Fast Articulated Object Modeling (FAOM) and
Semi-Authentic MixEd Reality Technique (SAMERT). Accompanying the pipeline, we
build a large-scale mixed reality dataset ReArtMix and a real world dataset
ReArtVal. We also propose an effective framework ReArtNOCS that exploits RGB-D
input to estimate part-level pose for multiple instances in a single forward
pass. Extensive experiments demonstrate that the proposed ReArtNOCS can achieve
good performance on both CAPER and CAPE settings. We believe it could serve as
a strong baseline for future research on the CAPER task.","['Liu Liu', 'Han Xue', 'Wenqiang Xu', 'Haoyuan Fu', 'Cewu Lu']",2021-05-07T13:41:16Z,http://arxiv.org/abs/2105.03260v1,"['cs.CV', 'cs.RO']"
"Investigating Modes of Activity and Guidance for Mediating Museum
  Exhibits in Mixed Reality","We present an exploratory case study describing the design and realisation of
a ''pure mixed reality'' application in a museum setting, where we investigate
the potential of using Microsoft's HoloLens for object-centred museum
mediation. Our prototype supports non-expert visitors observing a sculpture by
offering interpretation that is linked to visual properties of the museum
object. The design and development of our research prototype is based on a
two-stage visitor observation study and a formative study we conducted prior to
the design of the application. We present a summary of our findings from these
studies and explain how they have influenced our user-centred content creation
and the interaction design of our prototype. We are specifically interested in
investigating to what extent different constructs of initiative influence the
learning and user experience. Thus, we detail three modes of activity that we
realised in our prototype. Our case study is informed by research in the area
of human-computer interaction, the humanities and museum practice. Accordingly,
we discuss core concepts, such as gaze-based interaction, object-centred
learning, presence, and modes of activity and guidance with a transdisciplinary
perspective.","['Katrin Glinka', 'Patrick Tobias Fischer', 'Claudia Müller-Birn', 'Silke Krohn']",2021-06-25T08:17:34Z,http://arxiv.org/abs/2106.13494v1,['cs.HC']
"Mixed reality technologies for people with dementia: Participatory
  evaluation methods","Technologies can support people with early onset dementia (PwD) to aid them
in Instrumental Activities of Daily Living (IADL). The integration of physical
and virtual realities in Mixed reality technologies (MRTs) could provide
scalable and deployable options in developing prompting systems for PwD.
However, these emerging technologies should be evaluated and investigated for
feasibility with PwD. Survey instruments such as SUS, SUPR-Q and ethnographic
methods that are used for usability evaluation of websites and apps are used to
evaluate and study MRTs. However, PwD who cannot provide written and verbal
feedback are unable to participate in these studies. MRTs also present
challenges due to different ways in which physical and virtual realities could
be coupled. Experiences with physical, virtual and the couplings between the
two are to be considered in evaluating MRTs.","['Shital Desai', 'Arlene Astell']",2021-06-07T00:00:15Z,http://arxiv.org/abs/2107.07336v1,['cs.HC']
Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications,"We address the problem of estimating the shape of a person's head, defined as
the geometry of the complete head surface, from a video taken with a single
moving camera, and determining the alignment of the fitted 3D head for all
video frames, irrespective of the person's pose. 3D head reconstructions
commonly tend to focus on perfecting the face reconstruction, leaving the scalp
to a statistical approximation. Our goal is to reconstruct the head model of
each person to enable future mixed reality applications. To do this, we recover
a dense 3D reconstruction and camera information via structure-from-motion and
multi-view stereo. These are then used in a new two-stage fitting process to
recover the 3D head shape by iteratively fitting a 3D morphable model of the
head with the dense reconstruction in canonical space and fitting it to each
person's head, using both traditional facial landmarks and scalp features
extracted from the head's segmentation mask. Our approach recovers consistent
geometry for varying head shapes, from videos taken by different people, with
different smartphones, and in a variety of environments from living rooms to
outdoor spaces.","['Tejas Mane', 'Aylar Bayramova', 'Kostas Daniilidis', 'Philippos Mordohai', 'Elena Bernardis']",2021-09-06T21:03:52Z,http://arxiv.org/abs/2109.02740v2,['cs.CV']
Eye-Tracking-Based Design of Mixed Reality Learning Environments in STEM,"With the advent of commercially available Mixed-Reality(MR)-headsets in
recent years MR-assisted learning started to play a vital role in educational
research, especially related to STEM (science, technology, engineering and
mathematics) education. Along with these developments it seems viable to
further frameworks and structured design processes for MR-based learning
environments. Instead of a widely applicable framework for designing
educational MR applications, we here consider the case of virtually enhancing
physical hands-on experiments in STEM, where students are given a certain
problem to solve, and how to design these. For this focused realm, we suggest
an empirically driven problemand user-centred design process for MR
applications to get novices to act more like experts and exemplify it for a
specific experiment and problem set containing a non-trivial electric circuit
with capacitors and coils.","['Dörte Sonntag', 'Oliver Bodensiek']",2021-09-07T08:48:01Z,http://arxiv.org/abs/2109.02940v1,['physics.ed-ph']
"The Object at Hand: Automated Editing for Mixed Reality Video Guidance
  from Hand-Object Interactions","In this paper, we concern with the problem of how to automatically extract
the steps that compose real-life hand activities. This is a key competence
towards processing, monitoring and providing video guidance in Mixed Reality
systems. We use egocentric vision to observe hand-object interactions in
real-world tasks and automatically decompose a video into its constituent
steps. Our approach combines hand-object interaction (HOI) detection, object
similarity measurement and a finite state machine (FSM) representation to
automatically edit videos into steps. We use a combination of Convolutional
Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments
while observing real hand activities. We evaluate quantitatively and
qualitatively our algorithm on two datasets: the GTEA\cite{li2015delving}, and
a new dataset we introduce for Chinese Tea making. Results show our method is
able to segment hand-object interaction videos into key step segments with high
levels of precision.","['Yao Lu', 'Walterio W. Mayol-Cuevas']",2021-09-29T22:24:25Z,http://arxiv.org/abs/2109.14744v1,['cs.CV']
Learning robot motor skills with mixed reality,"Mixed Reality (MR) has recently shown great success as an intuitive interface
for enabling end-users to teach robots. Related works have used MR interfaces
to communicate robot intents and beliefs to a co-located human, as well as
developed algorithms for taking multi-modal human input and learning complex
motor behaviors. Even with these successes, enabling end-users to teach robots
complex motor tasks still poses a challenge because end-user communication is
highly task dependent and world knowledge is highly varied. We propose a
learning framework where end-users teach robots a) motion demonstrations, b)
task constraints, c) planning representations, and d) object information, all
of which are integrated into a single motor skill learning framework based on
Dynamic Movement Primitives (DMPs). We hypothesize that conveying this world
knowledge will be intuitive with an MR interface, and that a sample-efficient
motor skill learning framework which incorporates varied modalities of world
knowledge will enable robots to effectively solve complex tasks.","['Eric Rosen', 'Sreehari Rammohan', 'Devesh Jha']",2022-03-21T20:25:40Z,http://arxiv.org/abs/2203.11324v1,"['cs.RO', 'cs.LG']"
"ShapeFindAR: Exploring In-Situ Spatial Search for Physical Artifact
  Retrieval using Mixed Reality","Personal fabrication is made more accessible through repositories like
Thingiverse, as they replace modeling with retrieval. However, they require
users to translate spatial requirements to keywords, which paints an incomplete
picture of physical artifacts: proportions or morphology are non-trivially
encoded through text only. We explore a vision of in-situ spatial search for
(future) physical artifacts, and present ShapeFindAR, a mixed-reality tool to
search for 3D models using in-situ sketches blended with textual queries. With
ShapeFindAR, users search for geometry, and not necessarily precise labels,
while coupling the search process to the physical environment (e.g., by
sketching in-situ, extracting search terms from objects present, or tracing
them). We developed ShapeFindAR for HoloLens 2, connected to a database of
3D-printable artifacts. We specify in-situ spatial search, describe its
advantages, and present walkthroughs using ShapeFindAR, which highlight novel
ways for users to articulate their wishes, without requiring complex modeling
tools or profound domain knowledge.","['Evgeny Stemasov', 'Tobias Wagner', 'Jan Gugenheimer', 'Enrico Rukzio']",2022-03-31T17:37:26Z,http://arxiv.org/abs/2203.17211v1,"['cs.HC', 'cs.GR', 'H.5.2']"
Mixed Reality as Communication Medium for Human-Robot Collaboration,"Humans engaged in collaborative activities are naturally able to convey their
intentions to teammates through multi-modal communication, which is made up of
explicit and implicit cues. Similarly, a more natural form of human-robot
collaboration may be achieved by enabling robots to convey their intentions to
human teammates via multiple communication channels. In this paper, we
postulate that a better communication may take place should collaborative
robots be able to anticipate their movements to human teammates in an intuitive
way. In order to support such a claim, we propose a robot system's architecture
through which robots can communicate planned motions to human teammates
leveraging a Mixed Reality interface powered by modern head-mounted displays.
Specifically, the robot's hologram, which is superimposed to the real robot in
the human teammate's point of view, shows the robot's future movements,
allowing the human to understand them in advance, and possibly react to them in
an appropriate way. We conduct a preliminary user study to evaluate the
effectiveness of the proposed anticipatory visualization during a complex
collaborative task. The experimental results suggest that an improved and more
natural collaboration can be achieved by employing this anticipatory
communication mode.","['Simone Macciò', 'Alessandro Carfì', 'Fulvio Mastrogiovanni']",2022-06-30T15:57:17Z,http://arxiv.org/abs/2206.15380v1,['cs.RO']
Real Time Egocentric Segmentation for Video-self Avatar in Mixed Reality,"In this work we present our real-time egocentric body segmentation algorithm.
Our algorithm achieves a frame rate of 66 fps for an input resolution of
640x480, thanks to our shallow network inspired in Thundernet's architecture.
Besides, we put a strong emphasis on the variability of the training data. More
concretely, we describe the creation process of our Egocentric Bodies
(EgoBodies) dataset, composed of almost 10,000 images from three datasets,
created both from synthetic methods and real capturing. We conduct experiments
to understand the contribution of the individual datasets; compare Thundernet
model trained with EgoBodies with simpler and more complex previous approaches
and discuss their corresponding performance in a real-life setup in terms of
segmentation quality and inference times. The described trained semantic
segmentation algorithm is already integrated in an end-to-end system for Mixed
Reality (MR), making it possible for users to see his/her own body while being
immersed in a MR scene.","['Ester Gonzalez-Sosa', 'Andrija Gajic', 'Diego Gonzalez-Morin', 'Guillermo Robledo', 'Pablo Perez', 'Alvaro Villegas']",2022-07-04T10:00:16Z,http://arxiv.org/abs/2207.01296v1,['cs.CV']
HoloLens 2 Technical Evaluation as Mixed Reality Guide,"Mixed Reality (MR) is an evolving technology lying in the continuum spanned
by related technologies such as Virtual Reality (VR) and Augmented Reality
(AR), and creates an exciting way of interacting with people and the
environment. This technology is fast becoming a tool used by many people,
potentially improving living environments and work efficiency. Microsoft
HoloLens has played an important role in the progress of MR, from the first
generation to the second generation. In this paper, we systematically evaluate
the functions of applicable functions in HoloLens 2. These evaluations can
serve as a performance benchmark that can help people who need to use this
instrument for research or applications in the future. The detailed tests and
the performance evaluation of the different functionalities show the usability
and possible limitations of each function. We mainly divide the experiment into
the existing functions of the HoloLens 1, the new functions of the HoloLens 2,
and the use of research mode. This research results will be useful for MR
researchers who want to use HoloLens 2 as a research tool to design their own
MR applications.","['Hung-Jui Guo', 'Balakrishnan Prabhakaran']",2022-07-19T21:19:23Z,http://arxiv.org/abs/2207.09554v1,['cs.HC']
Mixed Reality for Mechanical Design and Assembly Planning,"Design for Manufacturing and Assembly (DFMA) is a crucial design stage within
the heavy vehicle manufacturing process that involves optimising the order and
feasibility of the parts assembly process to reduce manufacturing complexity
and overall cost. Existing work has focused on conducting DFMA within virtual
environments to reduce manufacturing costs, but users are less able to relate
and compare physical characteristics of a virtual component with real physical
objects. Therefore, a Mixed Reality (MR) application is developed for engineers
to visualise and manipulate assembly parts virtually, conduct and plan out an
assembly within its intended physical environment. Two pilot evaluations were
conducted with both engineering professionals and non-engineers to assess
effectiveness of the software for assembly planning. Usability results suggest
that the application is overall usable (M=56.1, SD=7.89), and participants felt
a sense of involvement in the activity (M=13.1, SD=3.3). Engineering
professionals see the application as a useful and cost-effective tool for
optimising their mechanical assembly designs.","['Emran Poh', 'Kyrin Liong', 'Jeannie Lee']",2022-09-02T19:41:29Z,http://arxiv.org/abs/2209.01252v1,['cs.HC']
MR4MR: Mixed Reality for Melody Reincarnation,"There is a long history of an effort made to explore musical elements with
the entities and spaces around us, such as musique concr\`ete and ambient
music. In the context of computer music and digital art, interactive
experiences that concentrate on the surrounding objects and physical spaces
have also been designed. In recent years, with the development and
popularization of devices, an increasing number of works have been designed in
Extended Reality to create such musical experiences. In this paper, we describe
MR4MR, a sound installation work that allows users to experience melodies
produced from interactions with their surrounding space in the context of Mixed
Reality (MR). Using HoloLens, an MR head-mounted display, users can bump
virtual objects that emit sound against real objects in their surroundings.
Then, by continuously creating a melody following the sound made by the object
and re-generating randomly and gradually changing melody using music generation
machine learning models, users can feel their ambient melody ""reincarnating"".","['Atsuya Kobayashi', 'Ryogo Ishino', 'Ryuku Nobusue', 'Takumi Inoue', 'Keisuke Okazaki', 'Shoma Sawa', 'Nao Tokui']",2022-09-15T03:23:29Z,http://arxiv.org/abs/2209.07023v1,"['cs.HC', 'cs.AI']"
Holo-Dex: Teaching Dexterity with Immersive Mixed Reality,"A fundamental challenge in teaching robots is to provide an effective
interface for human teachers to demonstrate useful skills to a robot. This
challenge is exacerbated in dexterous manipulation, where teaching
high-dimensional, contact-rich behaviors often require esoteric teleoperation
tools. In this work, we present Holo-Dex, a framework for dexterous
manipulation that places a teacher in an immersive mixed reality through
commodity VR headsets. The high-fidelity hand pose estimator onboard the
headset is used to teleoperate the robot and collect demonstrations for a
variety of general-purpose dexterous tasks. Given these demonstrations, we use
powerful feature learning combined with non-parametric imitation to train
dexterous skills. Our experiments on six common dexterous tasks, including
in-hand rotation, spinning, and bottle opening, indicate that Holo-Dex can both
collect high-quality demonstration data and train skills in a matter of hours.
Finally, we find that our trained skills can exhibit generalization on objects
not seen in training. Videos of Holo-Dex are available at
https://holo-dex.github.io.","['Sridhar Pandian Arunachalam', 'Irmak Güzey', 'Soumith Chintala', 'Lerrel Pinto']",2022-10-12T17:59:02Z,http://arxiv.org/abs/2210.06463v1,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.HC', 'cs.LG']"
"Augmented Reality and Mixed Reality Measurement Under Different
  Environments: A Survey on Head-Mounted Devices","Augmented Reality (AR) and Mixed Reality (MR) have been two of the most
explosive research topics in the last few years. Head-Mounted Devices (HMDs)
are essential intermediums for using AR and MR technology, playing an important
role in the research progress in these two areas. Behavioral research with
users is one way of evaluating the technical progress and effectiveness of
HMDs. In addition, AR and MR technology is dependent upon virtual interactions
with the real environment. Thus, conditions in real environments can be a
significant factor for AR and MR measurements with users. In this paper, we
survey 87 environmental-related HMD papers with measurements from users,
spanning over 32 years. We provide a thorough review of AR- and MR-related user
experiments with HMDs under different environmental factors. Then, we summarize
trends in this literature over time using a new classification method with four
environmental factors, the presence or absence of user feedback in behavioral
experiments, and ten main categories to subdivide these papers (e.g., domain
and method of user assessment). We also categorize characteristics of the
behavioral experiments, showing similarities and differences among papers.","['Hung-Jui Guo', 'Jonathan Z. Bakdash', 'Laura R. Marusich', 'Balakrishnan Prabhakaran']",2022-10-29T02:03:56Z,http://arxiv.org/abs/2210.16463v1,['cs.HC']
"Imitation Learning based Auto-Correction of Extrinsic Parameters for A
  Mixed-Reality Setup","In this paper, we discuss an imitation learning based method for reducing the
calibration error for a mixed reality system consisting of a vision sensor and
a projector. Unlike a head mounted display, in this setup, augmented
information is available to a human subject via the projection of a scene into
the real world. Inherently, the camera and projector need to be calibrated as a
stereo setup to project accurate information in 3D space. Previous calibration
processes require multiple recording and parameter tuning steps to achieve the
desired calibration, which is usually time consuming process. In order to avoid
such tedious calibration, we train a CNN model to iteratively correct the
extrinsic offset given a QR code and a projected pattern. We discuss the
overall system setup, data collection for training, and results of the
auto-correction model.","['Shubham Sonawani', 'Yifan Zhou', 'Heni Ben Amor']",2022-12-16T21:34:33Z,http://arxiv.org/abs/2212.08720v1,['cs.RO']
A Framework for Active Haptic Guidance Using Robotic Haptic Proxies,"Haptic feedback is an important component of creating an immersive mixed
reality experience. Traditionally, haptic forces are rendered in response to
the user's interactions with the virtual environment. In this work, we explore
the idea of rendering haptic forces in a proactive manner, with the explicit
intention to influence the user's behavior through compelling haptic forces. To
this end, we present a framework for active haptic guidance in mixed reality,
using one or more robotic haptic proxies to influence user behavior and deliver
a safer and more immersive virtual experience. We provide details on common
challenges that need to be overcome when implementing active haptic guidance,
and discuss example applications that show how active haptic guidance can be
used to influence the user's behavior. Finally, we apply active haptic guidance
to a virtual reality navigation problem, and conduct a user study that
demonstrates how active haptic guidance creates a safer and more immersive
experience for users.","['Niall L. Williams', 'Nicholas Rewkowski', 'Jiasheng Li', 'Ming C. Lin']",2023-01-12T21:43:34Z,http://arxiv.org/abs/2301.05311v2,"['cs.RO', 'cs.GR']"
"MR.Brick: Designing A Remote Mixed-reality Educational Game System for
  Promoting Children's Social & Collaborative Skills","Children are one of the groups most influenced by COVID-19-related social
distancing, and a lack of contact with peers can limit their opportunities to
develop social and collaborative skills. However, remote socialization and
collaboration as an alternative approach is still a great challenge for
children. This paper presents MR.Brick, a Mixed Reality (MR) educational game
system that helps children adapt to remote collaboration. A controlled
experimental study involving 24 children aged six to ten was conducted to
compare MR.Brick with the traditional video game by measuring their social and
collaborative skills and analyzing their multi-modal playing behaviours. The
results showed that MR.Brick was more conducive to children's remote
collaboration experience than the traditional video game. Given the lack of
training systems designed for children to collaborate remotely, this study may
inspire interaction design and educational research in related fields.","['Yudan Wu', 'Shanhe You', 'Zixuan Guo', 'Xiangyang Li', 'Guyue Zhou', 'Jiangtao Gong']",2023-01-18T05:14:57Z,http://arxiv.org/abs/2301.07310v2,"['cs.HC', 'H.5.2']"
"ChameleonControl: Teleoperating Real Human Surrogates through Mixed
  Reality Gestural Guidance for Remote Hands-on Classrooms","We present ChameleonControl, a real-human teleoperation system for scalable
remote instruction in hands-on classrooms. In contrast to existing video or
AR/VR-based remote hands-on education, ChameleonControl uses a real human as a
surrogate of a remote instructor. Building on existing human-based telepresence
approaches, we contribute a novel method to teleoperate a human surrogate
through synchronized mixed reality hand gestural navigation and verbal
communication. By overlaying the remote instructor's virtual hands in the local
user's MR view, the remote instructor can guide and control the local user as
if they were physically present. This allows the local user/surrogate to
synchronize their hand movements and gestures with the remote instructor,
effectively teleoperating a real human. We deploy and evaluate our system in
classrooms of physiotherapy training, as well as other application domains such
as mechanical assembly, sign language and cooking lessons. The study results
confirm that our approach can increase engagement and the sense of co-presence,
showing potential for the future of remote hands-on classrooms.","['Mehrad Faridan', 'Bheesha Kumari', 'Ryo Suzuki']",2023-02-21T23:11:41Z,http://arxiv.org/abs/2302.11053v1,['cs.HC']
CAstelet in Virtual reality for shadOw AVatars (CAVOAV),"After an overview of the use of digital shadows in computing science research
projects with cultural and social impacts and a focus on recent researches and
insights on virtual theaters, this paper introduces a research mixing the
manipulation of shadow avatars and the building of a virtual theater setup
inspired by traditional shadow theater (or ``castelet'' in french) in a mixed
reality environment. It describes the virtual 3D setup, the nature of the
shadow avatars and the issues of directing believable interactions between
virtual avatars and physical performers on stage. Two modalities of shadow
avatars direction are exposed. Some results of the research are illustrated in
two use cases: the development of theatrical creativity in mixed reality
through pedagogical workshops; and an artistic achievement in ''The Shadow''
performance, after H. C. Andersen.","['Georges Gagneré', 'Anastasiia Ternova']",2023-03-13T10:31:09Z,http://arxiv.org/abs/2303.06981v1,['cs.GR']
"HoloTouch: Interacting with Mixed Reality Visualizations Through
  Smartphone Proxies","We contribute interaction techniques for augmenting mixed reality (MR)
visualizations with smartphone proxies. By combining head-mounted displays
(HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D
charts with precise touch input, haptics feedback, high-resolution 2D graphics,
and physical manipulation. Our approach aims to complement both MR and physical
visualizations. Most current MR visualizations suffer from unreliable tracking,
low visual resolution, and imprecise input. Data physicalizations on the other
hand, although allowing for natural physical manipulation, are limited in
dynamic and interactive modification. We demonstrate how mobile devices such as
smartphones or tablets can serve as physical proxies for MR data interactions,
creating dynamic visualizations that support precise manipulation and rich
input and output. We describe 6 interaction techniques that leverage the
combined physicality, sensing, and output capabilities of HMDs and smartphones,
and demonstrate those interactions via a prototype system. Based on an
evaluation, we outline opportunities for combining the advantages of both MR
and physical charts.","['Neil Chulpongsatorn', 'Wesley Willett', 'Ryo Suzuki']",2023-03-15T20:19:13Z,http://arxiv.org/abs/2303.08916v1,['cs.HC']
Accessible Robot Control in Mixed Reality,"A novel method to control the Spot robot of Boston Dynamics by Hololens 2 is
proposed. This method is mainly designed for people with physical disabilities,
users can control the robot's movement and robot arm without using their hands.
The eye gaze tracking and head motion tracking technologies of Hololens 2 are
utilized for sending control commands. The movement of the robot would follow
the eye gaze and the robot arm would mimic the pose of the user's head. Through
our experiment, our method is comparable with the traditional control method by
joystick in both time efficiency and user experience. Demo can be found on our
project webpage: https://zhangganlin.github.io/Holo-Spot-Page/index.html","['Ganlin Zhang', 'Deheng Zhang', 'Longteng Duan', 'Guo Han']",2023-06-04T16:05:26Z,http://arxiv.org/abs/2306.02393v1,"['cs.RO', 'cs.CV']"
"Affordance segmentation of hand-occluded containers from exocentric
  images","Visual affordance segmentation identifies the surfaces of an object an agent
can interact with. Common challenges for the identification of affordances are
the variety of the geometry and physical properties of these surfaces as well
as occlusions. In this paper, we focus on occlusions of an object that is
hand-held by a person manipulating it. To address this challenge, we propose an
affordance segmentation model that uses auxiliary branches to process the
object and hand regions separately. The proposed model learns affordance
features under hand-occlusion by weighting the feature map through hand and
object segmentation. To train the model, we annotated the visual affordances of
an existing dataset with mixed-reality images of hand-held containers in
third-person (exocentric) images. Experiments on both real and mixed-reality
images show that our model achieves better affordance segmentation and
generalisation than existing models.","['Tommaso Apicella', 'Alessio Xompero', 'Edoardo Ragusa', 'Riccardo Berta', 'Andrea Cavallaro', 'Paolo Gastaldo']",2023-08-22T07:14:29Z,http://arxiv.org/abs/2308.11233v1,['cs.CV']
"Realistic Volume Rendering with Environment-Synced Illumination in Mixed
  Reality","Interactive volume visualization using a mixed reality (MR) system helps
provide users with an intuitive spatial perception of volumetric data. Due to
sophisticated requirements of user interaction and vision when using MR
head-mounted display (HMD) devices, the conflict between the realisticness and
efficiency of direct volume rendering (DVR) is yet to be resolved. In this
paper, a new MR visualization framework that supports interactive realistic DVR
is proposed. An efficient illumination estimation method is used to identify
the high dynamic range (HDR) environment illumination captured using a panorama
camera. To improve the visual quality of Monte Carlo-based DVR, a new
spatio-temporal denoising algorithm is designed. Based on a reprojection
strategy, it makes full use of temporal coherence between adjacent frames and
spatial coherence between the two screens of an HMD to optimize MR rendering
quality. Several MR development modules are also developed for related devices
to efficiently and stably display the DVR results in an MR HMD. Experimental
results demonstrate that our framework can better support immersive and
intuitive user perception during MR viewing than existing MR solutions.","['Haojie Cheng', 'Chunxiao Xu', 'Xujing Chen', 'Zhenxin Chen', 'Jiajun Wang', 'Lingxiao Zhao']",2023-09-05T03:11:38Z,http://arxiv.org/abs/2309.01916v1,['cs.GR']
"LLMR: Real-time Prompting of Interactive Worlds using Large Language
  Models","We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.","['Fernanda De La Torre', 'Cathy Mengying Fang', 'Han Huang', 'Andrzej Banburski-Fahey', 'Judith Amores Fernandez', 'Jaron Lanier']",2023-09-21T17:37:01Z,http://arxiv.org/abs/2309.12276v3,"['cs.HC', 'cs.AI', 'cs.CL', 'cs.ET']"
"Mixed Reality Environment and High-Dimensional Continuification Control
  for Swarm Robotics","Many new methodologies for the control of large-scale multi-agent systems are
based on macroscopic representations of the emerging systemdynamics, in the
form of continuum approximations of large ensembles. These techniques, that are
typically developed in the limit case of an infinite number of agents, are
usually validated only through numerical simulations. In this paper, we
introduce a mixed reality set-up for testing swarm robotics techniques,
focusing on the macroscopic collective motion of robotic swarms. This hybrid
apparatus combines both real differential drive robots and virtual agents to
create a heterogeneous swarm of tunable size. We also extend
continuification-based control methods for swarms to higher dimensions, and
assess experimentally their validity in the new platform. Our study
demonstrates the effectiveness of the platform for conducting large-scale swarm
robotics experiments, and it contributes new theoretical insights into control
algorithms exploiting continuification approaches.","['Gian Carlo Maffettone', 'Lorenzo Liguori', 'Eduardo Palermo', 'Mario di Bernardo', 'Maurizio Porfiri']",2023-10-02T19:05:24Z,http://arxiv.org/abs/2310.01573v3,"['cs.RO', 'cs.SY', 'eess.SY']"
"Visualizing Causality in Mixed Reality for Manual Task Learning: An
  Exploratory Study","Mixed Reality (MR) is gaining prominence in manual task skill learning due to
its in-situ, embodied, and immersive experience. To teach manual tasks, current
methodologies break the task into hierarchies (tasks into subtasks) and
visualize the current subtask and future in terms of causality. Existing
psychology literature also shows that humans learn tasks by breaking them into
hierarchies. In order to understand the design space of information visualized
to the learner for better task understanding, we conducted a user study with 48
users. The study was conducted using a complex assembly task, which involves
learning of both actions and tool usage. We aim to explore the effect of
visualization of causality in the hierarchy for manual task learning in MR by
four options: no causality, event level causality, interaction level causality,
and gesture level causality. The results show that the user understands and
performs best when all the level of causality is shown to the user. Based on
the results, we further provide design recommendations and in-depth discussions
for future manual task learning systems.","['Rahul Jain', 'Jingyu Shi', 'Andrew Benton', 'Moiz Rasheed', 'Hyungjun Doh', 'Subramanian Chidambaram', 'Karthik Ramani']",2023-10-19T21:35:11Z,http://arxiv.org/abs/2310.13167v3,['cs.HC']
"SoundShift: Exploring Sound Manipulations for Accessible Mixed-Reality
  Awareness","Mixed-reality (MR) soundscapes blend real-world sound with virtual audio from
hearing devices, presenting intricate auditory information that is hard to
discern and differentiate. This is particularly challenging for blind or
visually impaired individuals, who rely on sounds and descriptions in their
everyday lives. To understand how complex audio information is consumed, we
analyzed online forum posts within the blind community, identifying prevailing
challenges, needs, and desired solutions. We synthesized the results and
propose SoundShift for increasing MR sound awareness, which includes six sound
manipulations: Transparency Shift, Envelope Shift, Position Shift, Style Shift,
Time Shift, and Sound Append. To evaluate the effectiveness of SoundShift, we
conducted a user study with 18 blind participants across three simulated MR
scenarios, where participants identified specific sounds within intricate
soundscapes. We found that SoundShift increased MR sound awareness and
minimized cognitive load. Finally, we developed three real-world example
applications to demonstrate the practicality of SoundShift.","['Ruei-Che Chang', 'Chia-Sheng Hung', 'Bing-Yu Chen', 'Dhruv Jain', 'Anhong Guo']",2024-01-20T02:57:50Z,http://arxiv.org/abs/2401.11095v2,"['cs.HC', 'cs.SD', 'eess.AS']"
"BioNet-XR: Biological Network Visualization Framework for Virtual
  Reality and Mixed Reality Environments","Protein-protein interaction networks (PPIN) enable the study of cellular
processes in organisms. Visualizing PPINs in extended reality (XR), including
virtual reality (VR) and mixed reality (MR), is crucial for exploring
subnetworks, evaluating protein positions, and collaboratively analyzing and
discussing on networks with the help of recent technological advancements.
Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in
VR and MR environments. BioNet-XR was developed with the Unity3D game engine.
Our framework provides state-of-the-art methods and visualization features
including teleportation between nodes, general and first-person view to explore
the network, subnetwork construction via PageRank, Steiner tree, and all-pair
shortest path algorithms for a given set of initial nodes. We used usability
tests to gather feedback from both specialists (bioinformaticians) and
generalists (multidisciplinary groups), addressing the need for usability
evaluations of visualization tools. In the MR version of BioNet-XR, users can
seamlessly transition to real-world environments and interact with protein
interaction networks. BioNet-XR is highly modular and adaptable for
visualization of other biological networks, such as metabolic and regulatory
networks, and extension with additional network methods.","['Busra Senderin', 'Nurcan Tuncbag', 'Elif Surer']",2024-02-06T12:20:10Z,http://arxiv.org/abs/2402.03946v1,['cs.MM']
"Spatial Assisted Human-Drone Collaborative Navigation and Interaction
  through Immersive Mixed Reality","Aerial robots have the potential to play a crucial role in assisting humans
with complex and dangerous tasks. Nevertheless, the future industry demands
innovative solutions to streamline the interaction process between humans and
drones to enable seamless collaboration and efficient co-working. In this
paper, we present a novel tele-immersive framework that promotes cognitive and
physical collaboration between humans and robots through Mixed Reality (MR).
This framework incorporates a novel bi-directional spatial awareness and a
multi-modal virtual-physical interaction approaches. The former seamlessly
integrates the physical and virtual worlds, offering bidirectional egocentric
and exocentric environmental representations. The latter, leveraging the
proposed spatial representation, further enhances the collaboration combining a
robot planning algorithm for obstacle avoidance with a variable admittance
control. This allows users to issue commands based on virtual forces while
maintaining compatibility with the environment map. We validate the proposed
approach by performing several collaborative planning and exploration tasks
involving a drone and an user equipped with a MR headset.","['Luca Morando', 'Giuseppe Loianno']",2024-02-06T15:17:09Z,http://arxiv.org/abs/2402.04070v2,"['cs.RO', 'cs.SY', 'eess.SY']"
Another Body in the World: Flusserian Freedom in Mixed Reality,"In Flusserian view of media history, humans often misperceive the world
projected by media to be the world itself, leading to a loss of freedom. This
paper examines Flusserian Freedom in the context of Mixed Reality (MR) and
explores how humans can recognize the obscuration of the world within the media
(i.e., MR) and understand their relationship. The authors investigate the
concept of playing against apparatus and deliberately alienating the perception
of the projected world through an artwork titled ""Surrealism Me."" This artwork
enables the user to have another body within MR through interactive and
immersive experiences based on the definition of Sense of Embodiment. The
purpose of this work is to raise awareness of the domination of media and to
approach Flusserian freedom within contemporary technical arrangements.","['Aven Le Zhou', 'Lei Xi', 'Kang Zhang']",2024-02-16T15:19:35Z,http://arxiv.org/abs/2402.10751v1,['cs.CY']
"Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D
  Spatio-Temporal Data in Mixed Reality","Tangible interfaces in mixed reality (MR) environments allow for intuitive
data interactions. Tangible cubes, with their rich interaction affordances,
high maneuverability, and stable structure, are particularly well-suited for
exploring multi-dimensional data types. However, the design potential of these
cubes is underexplored. This study introduces a design space for tangible cubes
in MR, focusing on interaction space, visualization space, sizes, and
multiplicity. Using spatio-temporal data, we explored the interaction
affordances of these cubes in a workshop (N=24). We identified unique
interactions like rotating, tapping, and stacking, which are linked to
augmented reality (AR) visualization commands. Integrating user-identified
interactions, we created a design space for tangible-cube interactions and
visualization. A prototype visualizing global health spending with small cubes
was developed and evaluated, supporting both individual and combined cube
manipulation. This research enhances our grasp of tangible interaction in MR,
offering insights for future design and application in diverse data contexts.","['Shuqi He', 'Haonan Yao', 'Luyan Jiang', 'Kaiwen Li', 'Nan Xiang', 'Yue Li', 'Hai-Ning Liang', 'Lingyun Yu']",2024-03-11T16:47:39Z,http://arxiv.org/abs/2403.06891v1,['cs.HC']
Gaze-based Human-Robot Interaction System for Infrastructure Inspections,"Routine inspections for critical infrastructures such as bridges are required
in most jurisdictions worldwide. Such routine inspections are largely visual in
nature, which are qualitative, subjective, and not repeatable. Although robotic
infrastructure inspections address such limitations, they cannot replace the
superior ability of experts to make decisions in complex situations, thus
making human-robot interaction systems a promising technology. This study
presents a novel gaze-based human-robot interaction system, designed to augment
the visual inspection performance through mixed reality. Through holograms from
a mixed reality device, gaze can be utilized effectively to estimate the
properties of the defect in real-time. Additionally, inspectors can monitor the
inspection progress online, which enhances the speed of the entire inspection
process. Limited controlled experiments demonstrate its effectiveness across
various users and defect types. To our knowledge, this is the first
demonstration of the real-time application of eye gaze in civil infrastructure
inspections.","['Sunwoong Choi', 'Zaid Abbas Al-Sabbag', 'Sriram Narasimhan', 'Chul Min Yeum']",2024-03-12T20:26:51Z,http://arxiv.org/abs/2403.08061v1,['cs.RO']
"On the Fly Robotic-Assisted Medical Instrument Planning and Execution
  Using Mixed Reality","Robotic-assisted medical systems (RAMS) have gained significant attention for
their advantages in alleviating surgeons' fatigue and improving patients'
outcomes. These systems comprise a range of human-computer interactions,
including medical scene monitoring, anatomical target planning, and robot
manipulation. However, despite its versatility and effectiveness, RAMS demands
expertise in robotics, leading to a high learning cost for the operator. In
this work, we introduce a novel framework using mixed reality technologies to
ease the use of RAMS. The proposed framework achieves real-time planning and
execution of medical instruments by providing 3D anatomical image overlay,
human-robot collision detection, and robot programming interface. These
features, integrated with an easy-to-use calibration method for head-mounted
display, improve the effectiveness of human-robot interactions. To assess the
feasibility of the framework, two medical applications are presented in this
work: 1) coil placement during transcranial magnetic stimulation and 2) drill
and injector device positioning during femoroplasty. Results from these use
cases demonstrate its potential to extend to a wider range of medical
scenarios.","['Letian Ai', 'Yihao Liu', 'Mehran Armand', 'Amir Kheradmand', 'Alejandro Martin-Gomez']",2024-04-08T21:58:25Z,http://arxiv.org/abs/2404.05887v1,['cs.RO']
"Quantifying Social Presence in Mixed Reality: A Contemporary Review of
  Techniques and Innovations","This literature review investigates the transformative potential of mixed
reality (MR) technology, where we explore the intersection of contemporary
technological advancements, modern deep learning recommendation systems, and
social psychology frameworks. This interdisciplinary study informs the
understanding of MR's role in improving social presence, catalyzing novel
social interactions, and enhancing the quality of interpersonal communication
in the real world. We also discuss the challenges and barriers blocking the
wide-spread adoption of social networking in MR, such as device constraints,
privacy and accessibility concerns, and social norms. Through carefully
structured, closed-environment experiments with diverse participants of varying
levels of digital literacy, we measure the differences in social dynamics,
frequency, quality, and duration of interactions, and levels of social anxiety
between MR-enhanced, mobile-enhanced, and control condition participants.",['Sparsh Srivastava'],2024-04-05T16:16:12Z,http://arxiv.org/abs/2404.15325v2,['cs.HC']
"Practice-informed Patterns for Organising Large Groups in Distributed
  Mixed Reality Collaboration","Collaborating across dissimilar, distributed spaces presents numerous
challenges for computer-aided spatial communication. Mixed reality (MR) can
blend selected surfaces, allowing collaborators to work in blended f-formations
(facing formations), even when their workstations are physically misaligned.
Since collaboration often involves more than just participant pairs, this
research examines how we might scale MR experiences for large-group
collaboration. To do so, this study recruited collaboration designers (CDs) to
evaluate and reimagine MR for large-scale collaboration. These CDs were engaged
in a four-part user study that involved a technology probe, a semi-structured
interview, a speculative low-fidelity prototyping activity and a validation
session. The outcomes of this paper contribute (1) a set of collaboration
design principles to inspire future computer-supported collaborative work, (2)
eight collaboration patterns for blended f-formations and collaboration at
scale and (3) theoretical implications for f-formations and space-place
relationships. As a result, this work creates a blueprint for scaling
collaboration across distributed spaces.","['Emily Wong', 'Juan Sánchez Esquivel', 'Jens Emil Grønbæk', 'Germán Leiva', 'Eduardo Velloso']",2024-05-08T08:06:22Z,http://arxiv.org/abs/2405.04873v2,['cs.HC']
"Harms in Repurposing Real-World Sensory Cues for Mixed Reality: A Causal
  Perspective","The rise of Mixed Reality (MR) stimulates new interactive techniques that
seamlessly blend the virtual and physical environments. Just as virtual content
could be overlayed onto the physical world for providing adaptive user
interfaces [5, 8], emergent techniques ""repurpose"" everyday environments and
sensory cues to support the virtual content [7, 9, 13-15]. For instance, a
strong wind gust in the real world, rather than being distracting to the
virtual experience, can be mapped with trees swaying in MR to achieve a
unifying experience [15], as shown in Figure 1. Such techniques introduce
stronger immersion, but they also expose users to overlooked perceptual
manipulations, where safety risks arise from misperception of real-world
events. In this work, we apply a causal inference perspective to understand the
harms of repurposing real-world sensory cues for MR. We argue that by viewing
the MR experience as a causal inference process of interpreting cues arising
from both the virtual and physical world, MR designers and researchers can gain
a new lens to understand potential perceptual manipulation harms.","['Yujie Tao', 'Sean Follmer']",2024-04-23T17:34:46Z,http://arxiv.org/abs/2405.05931v1,['cs.HC']
"RealitySummary: On-Demand Mixed Reality Document Enhancement using Large
  Language Models","We introduce RealitySummary, a mixed reality reading assistant that can
enhance any printed or digital document using on-demand text extraction,
summarization, and augmentation. While augmented reading tools promise to
enhance physical reading experiences with overlaid digital content, prior
systems have typically required pre-processed documents, which limits their
generalizability and real-world use cases. In this paper, we explore on-demand
document augmentation by leveraging large language models. To understand
generalizable techniques for diverse documents, we first conducted an
exploratory design study which identified five categories of document
enhancements (summarization, augmentation, navigation, comparison, and
extraction). Based on this, we developed a proof-of-concept system that can
automatically extract and summarize text using Google Cloud OCR and GPT-4, then
embed information around documents using a Microsoft Hololens 2 and Apple
Vision Pro. We demonstrate real-time examples of six specific document
augmentations: 1) summaries, 2) comparison tables, 3) timelines, 4) keyword
lists, 5) summary highlighting, and 6) information cards. Results from a
usability study (N=12) and in-the-wild study (N=11) highlight the potential
benefits of on-demand MR document enhancement and opportunities for future
research.","['Aditya Gunturu', 'Shivesh Jadon', 'Nandi Zhang', 'Jarin Thundathil', 'Wesley Willett', 'Ryo Suzuki']",2024-05-28T21:59:56Z,http://arxiv.org/abs/2405.18620v1,"['cs.HC', 'cs.AI', 'cs.CL']"
Live-action Virtual Reality Games,"This paper proposes the concept of ""live-action virtual reality games"" as a
new genre of digital games based on an innovative combination of live-action,
mixed-reality, context-awareness, and interaction paradigms that comprise
tangible objects, context-aware input devices, and embedded/embodied
interactions. Live-action virtual reality games are ""live-action games"" because
a player physically acts out (using his/her real body and senses) his/her
""avatar"" (his/her virtual representation) in the game stage, which is the
mixed-reality environment where the game happens. The game stage is a kind of
""augmented virtuality""; a mixed-reality where the virtual world is augmented
with real-world information. In live-action virtual reality games, players wear
HMD devices and see a virtual world that is constructed using the physical
world architecture as the basic geometry and context information. Physical
objects that reside in the physical world are also mapped to virtual elements.
Live-action virtual reality games keeps the virtual and real-worlds
superimposed, requiring players to physically move in the environment and to
use different interaction paradigms (such as tangible and embodied interaction)
to complete game activities. This setup enables the players to touch physical
architectural elements (such as walls) and other objects, ""feeling"" the game
stage. Players have free movement and may interact with physical objects placed
in the game stage, implicitly and explicitly. Live-action virtual reality games
differ from similar game concepts because they sense and use contextual
information to create unpredictable game experiences, giving rise to emergent
gameplay.","['Luis Valente', 'Esteban Clua', 'Alexandre Ribeiro Silva', 'Bruno Feijó']",2016-01-07T19:30:37Z,http://arxiv.org/abs/1601.01645v1,['cs.HC']
Inverse Augmented Reality: A Virtual Agent's Perspective,"We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']",2018-08-10T05:23:37Z,http://arxiv.org/abs/1808.03413v1,['cs.HC']
"Where's My Drink? Enabling Peripheral Real World Interactions While
  Using HMDs","Head Mounted Displays (HMDs) allow users to experience virtual reality with a
great level of immersion. However, even simple physical tasks like drinking a
beverage can be difficult and awkward while in a virtual reality experience. We
explore mixed reality renderings that selectively incorporate the physical
world into the virtual world for interactions with physical objects. We
conducted a user study comparing four rendering techniques that balances
immersion in a virtual world with ease of interaction with the physical world.
Finally, we discuss the pros and cons of each approach, suggesting guidelines
for future rendering techniques that bring physical objects into virtual
reality.","['Pulkit Budhiraja', 'Rajinder Sodhi', 'Brett Jones', 'Kevin Karsch', 'Brian Bailey', 'David Forsyth']",2015-02-16T22:50:03Z,http://arxiv.org/abs/1502.04744v1,['cs.HC']
Notes on Pervasive Virtuality,"This paper summarizes current notes about a new mixed-reality paradigm that
we named as ""pervasive virtuality"". This paradigm has emerged recently in
industry and academia through different initiatives. In this paper we intend to
explore this new area by proposing a set of features that we identified as
important or helpful to realize pervasive virtuality in games and entertainment
applications.","['Luis Valente', 'Bruno Feijo', 'Alexandre Ribeiro Silva', 'Esteban Clua']",2016-05-25T12:01:46Z,http://arxiv.org/abs/1605.08035v1,['cs.HC']
"Augmented Reality with Hololens: Experiential Architectures Embedded in
  the Real World","Early hands-on experiences with the Microsoft Hololens augmented/mixed
reality device are reported and discussed, with a general aim of exploring
basic 3D visualization. A range of usage cases are tested, including data
visualization and immersive data spaces, in-situ visualization of 3D models and
full scale architectural form visualization. Ultimately, the Hololens is found
to provide a remarkable tool for moving from traditional visualization of 3D
objects on a 2D screen, to fully experiential 3D visualizations embedded in the
real world.","['Paul Hockett', 'Tim Ingleby']",2016-10-13T22:32:08Z,http://arxiv.org/abs/1610.04281v1,"['cs.GR', 'physics.data-an']"
Authoring and Living Next-Generation Location-Based Experiences,"Authoring location-based experiences involving multiple participants,
collaborating or competing in both indoor and outdoor mixed realities, is
extremely complex and bound to serious technical challenges. In this work, we
present the first results of the MAGELLAN European project and how these
greatly simplify this creative process using novel authoring, augmented reality
(AR) and indoor geolocalisation techniques.","['Olivier Balet', 'Boriana Koleva', 'Jens Grubert', 'Kwang Moo Yi', 'Marco Gunia', 'Angelos Katsis', 'Julien Castet']",2017-09-05T09:04:05Z,http://arxiv.org/abs/1709.01293v1,['cs.HC']
Real-time Egocentric Gesture Recognition on Mobile Head Mounted Displays,"Mobile virtual reality (VR) head mounted displays (HMD) have become popular
among consumers in recent years. In this work, we demonstrate real-time
egocentric hand gesture detection and localization on mobile HMDs. Our main
contributions are: 1) A novel mixed-reality data collection tool to automatic
annotate bounding boxes and gesture labels; 2) The largest-to-date egocentric
hand gesture and bounding box dataset with more than 400,000 annotated frames;
3) A neural network that runs real time on modern mobile CPUs, and achieves
higher than 76% precision on gesture recognition across 8 classes.","['Rohit Pandey', 'Marie White', 'Pavel Pidlypenskyi', 'Xue Wang', 'Christine Kaeser-Chen']",2017-12-13T19:06:37Z,http://arxiv.org/abs/1712.04961v1,['cs.CV']
"Deep Neural Network and Data Augmentation Methodology for off-axis iris
  segmentation in wearable headsets","A data augmentation methodology is presented and applied to generate a large
dataset of off-axis iris regions and train a low-complexity deep neural
network. Although of low complexity the resulting network achieves a high level
of accuracy in iris region segmentation for challenging off-axis eye-patches.
Interestingly, this network is also shown to achieve high levels of performance
for regular, frontal, segmentation of iris regions, comparing favorably with
state-of-the-art techniques of significantly higher complexity. Due to its
lower complexity, this network is well suited for deployment in embedded
applications such as augmented and mixed reality headsets.","['Viktor Varkarakis', 'Shabab Bazrafkan', 'Peter Corcoran']",2019-03-01T16:17:00Z,http://arxiv.org/abs/1903.00389v1,['cs.CV']
Making ethical decisions for the immersive web,"Mixed reality (MR) ethics occupies a space that intersects with web ethics,
emerging tech ethics, healthcare ethics and product ethics (among others). This
paper focuses on how we can build an immersive web that encourages ethical
development and usage. The technology is beyond emerging (footnote: generally,
the ethics of emerging technologies are focused on ethical assessments of
research and innovation), but not quite entrenched. We're still in a position
to intervene in the development process, instead of attempting to retrofit
ethical decisions into an established design. While we have a wider range of
data to analyze than most emerging technologies, we're still in a much more
speculative state than entrenched technologies. This space is a challenge and
an opportunity.",['Diane Hosfelt'],2019-05-14T14:57:20Z,http://arxiv.org/abs/1905.06995v1,['cs.HC']
"Real Time Egocentric Object Segmentation: THU-READ Labeling and
  Benchmarking Results","Egocentric segmentation has attracted recent interest in the computer vision
community due to their potential in Mixed Reality (MR) applications. While most
previous works have been focused on segmenting egocentric human body parts
(mainly hands), little attention has been given to egocentric objects. Due to
the lack of datasets of pixel-wise annotations of egocentric objects, in this
paper we contribute with a semantic-wise labeling of a subset of 2124 images
from the RGB-D THU-READ Dataset. We also report benchmarking results using
Thundernet, a real-time semantic segmentation network, that could allow future
integration with end-to-end MR applications.","['E. Gonzalez-Sosa', 'G. Robledo', 'D. Gonzalez-Morin', 'P. Perez-Garcia', 'A. Villegas']",2021-06-09T10:10:02Z,http://arxiv.org/abs/2106.04957v1,['cs.CV']
"Comparing Controller With the Hand Gestures Pinch and Grab for Picking
  Up and Placing Virtual Objects","Grabbing virtual objects is one of the essential tasks for Augmented,
Virtual, and Mixed Reality applications. Modern applications usually use a
simple pinch gesture for grabbing and moving objects. However, picking up
objects by pinching has disadvantages. It can be an unnatural gesture to pick
up objects and prevents the implementation of other gestures which would be
performed with thumb and index. Therefore it is not the optimal choice for many
applications. In this work, different implementations for grabbing and placing
virtual objects are proposed and compared. Performance and accuracy of the
proposed techniques are measured and compared.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2022-02-22T15:12:06Z,http://arxiv.org/abs/2202.10964v1,"['cs.HC', 'cs.CV']"
"Cross-Reality for Extending the Metaverse: Designing Hyper-Connected
  Immersive Environments with XRI","The Metaverse comprises technologies to enable virtual twins of the real
world, via mixed reality, internet of things, and others. As it matures unique
challenges arise such as a lack of strong connections between virtual and
physical worlds. This work presents design frameworks for cross-reality hybrid
spaces. Contributions include: i) clarifying the metaverse ""disconnect"", ii)
extended metaverse design frameworks, iii) prototypes, and iv) discussions
toward new metaverse smart environments.","['Jie Guan', 'Alexis Morris', 'Jay Irizawa']",2023-06-01T19:55:34Z,http://arxiv.org/abs/2306.01113v1,['cs.HC']
"Beyond the Screen: Reshaping the Workplace with Virtual and Augmented
  Reality","Although extended reality technologies have enjoyed an explosion in
popularity in recent years, few applications are effectively used outside the
entertainment or academic contexts. This work consists of a literature review
regarding the effective integration of such technologies in the workplace. It
aims to provide an updated view of how they are being used in that context.
First, we examine existing research concerning virtual, augmented, and
mixed-reality applications. We also analyze which have made their way to the
workflows of companies and institutions. Furthermore, we circumscribe the
aspects of extended reality technologies that determined this applicability.","['Nuno Verdelho Trindade', 'Alfredo Ferreira', 'João Madeiras Pereira']",2023-12-01T08:05:22Z,http://arxiv.org/abs/2312.00408v1,['cs.HC']
"OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using
  Semantic Understanding in Mixed Reality","One key challenge in augmented reality is the placement of virtual content in
natural locations. Existing automated techniques are only able to work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce a new
open-vocabulary method for object placement. Our eight-stage pipeline leverages
recent advances in segmentation models, vision-language models, and LLMs to
place any virtual object in any AR camera frame or scene. In a preliminary user
study, we show that our method performs at least as well as human experts 57%
of the time.","['Luke Yoffe', 'Aditya Sharma', 'Tobias Höllerer']",2023-12-20T07:34:20Z,http://arxiv.org/abs/2312.12815v1,"['cs.CV', 'cs.AI', 'cs.CL']"
"Virtual World, Defined from a Technological Perspective, and Applied to
  Video Games, Mixed Reality and the Metaverse","There is no generally accepted definition for a virtual world, with many
complimentary terms and acronyms having emerged implying a virtual world.
Advances in systems architecture techniques such as, host migration of
instances, mobile ad-hoc networking, and distributed computing, bring in to
question whether those architectures can actually support a virtual world.
Without a concrete definition, controversy ensues and it is problematic to
design an architecture for a virtual world. Several researchers provided a
definition but aspects of each definition are still problematic and simply can
not be applied to contemporary technologies. The approach of this article is to
sample technologies using grounded theory, and obtain a definition for a
`virtual world' that is directly applicable to technology. The obtained
definition is compared with related work and used to classify advanced
technologies, such as: a pseudo-persistent video game, a MANet, virtual and
mixed reality, and the Metaverse. The results of this article include: a break
down of which properties set apart the various technologies; a definition that
is validated by comparing it with other definitions; an ontology showing the
relation of the different complimentary terms and acronyms; and, the usage of
pseudo-persistence to categories those technologies which only mimic
persistence.",['Kim J. L. Nevelsteen'],2015-11-26T18:05:02Z,http://arxiv.org/abs/1511.08464v2,"['cs.HC', 'cs.CY']"
"Augmenting the thermal flux experiment: a mixed reality approach with
  the HoloLens","In the field of Virtual Reality (VR) and Augmented Reality (AR) technologies
have made huge progress during the last years and also reached the field of
education. The virtuality continuum, ranging from pure virtuality on one side
to the real world on the other has been successfully covered by the use of
immersive technologies like head-mounted displays, which allow to embed virtual
objects into the real surroundings, leading to a Mixed Reality (MR) experience.
In such an environment digital and real objects do not only co-exist, but
moreover are also able to interact with each other in real-time. These concepts
can be used to merge human perception of reality with digitally visualized
sensor data and thereby making the invisible visible. As a first example, in
this paper we introduce alongside the basic idea of this column an
MR-experiment in thermodynamics for a laboratory course for freshman students
in physics or other science and engineering subjects which uses physical data
from mobile devices for analyzing and displaying physical phenomena to
students.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-09-05T11:52:18Z,http://arxiv.org/abs/1709.01342v1,['physics.ed-ph']
Security and Privacy Approaches in Mixed Reality: A Literature Survey,"Mixed reality (MR) technology development is now gaining momentum due to
advances in computer vision, sensor fusion, and realistic display technologies.
With most of the research and development focused on delivering the promise of
MR, there is only barely a few working on the privacy and security implications
of this technology. This survey paper aims to put in to light these risks, and
to look into the latest security and privacy work on MR. Specifically, we list
and review the different protection approaches that have been proposed to
ensure user and data security and privacy in MR. We extend the scope to include
work on related technologies such as augmented reality (AR), virtual reality
(VR), and human-computer interaction (HCI) as crucial components, if not the
origins, of MR, as well as numerous related work from the larger area of mobile
devices, wearables, and Internet-of-Things (IoT). We highlight the lack of
investigation, implementation, and evaluation of data protection approaches in
MR. Further challenges and directions on MR security and privacy are also
discussed.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-02-15T23:33:45Z,http://arxiv.org/abs/1802.05797v3,"['cs.CR', 'cs.CY', 'cs.HC']"
"Conservative Plane Releasing for Spatial Privacy Protection in Mixed
  Reality","Augmented reality (AR) or mixed reality (MR) platforms require spatial
understanding to detect objects or surfaces, often including their structural
(i.e. spatial geometry) and photometric (e.g. color, and texture) attributes,
to allow applications to place virtual or synthetic objects seemingly
""anchored"" on to real world objects; in some cases, even allowing interactions
between the physical and virtual objects. These functionalities require AR/MR
platforms to capture the 3D spatial information with high resolution and
frequency; however, these pose unprecedented risks to user privacy. Aside from
objects being detected, spatial information also reveals the location of the
user with high specificity, e.g. in which part of the house the user is. In
this work, we propose to leverage spatial generalizations coupled with
conservative releasing to provide spatial privacy while maintaining data
utility. We designed an adversary that builds up on existing place and shape
recognition methods over 3D data as attackers to which the proposed spatial
privacy approach can be evaluated against. Then, we simulate user movement
within spaces which reveals more of their space as they move around utilizing
3D point clouds collected from Microsoft HoloLens. Results show that revealing
no more than 11 generalized planes--accumulated from successively revealed
spaces with large enough radius, i.e. $r\leq1.0m$--can make an adversary fail
in identifying the spatial location of the user for at least half of the time.
Furthermore, if the accumulated spaces are of smaller radius, i.e. each
successively revealed space is $r\leq 0.5m$, we can release up to 29
generalized planes while enjoying both better data utility and privacy.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2020-04-17T01:57:58Z,http://arxiv.org/abs/2004.08029v1,"['cs.CV', 'cs.CR', 'cs.HC']"
Kalman Filter-based Head Motion Prediction for Cloud-based Mixed Reality,"Volumetric video allows viewers to experience highly-realistic 3D content
with six degrees of freedom in mixed reality (MR) environments. Rendering
complex volumetric videos can require a prohibitively high amount of
computational power for mobile devices. A promising technique to reduce the
computational burden on mobile devices is to perform the rendering at a cloud
server. However, cloud-based rendering systems suffer from an increased
interaction (motion-to-photon) latency that may cause registration errors in MR
environments. One way of reducing the effective latency is to predict the
viewer's head pose and render the corresponding view from the volumetric video
in advance. In this paper, we design a Kalman filter for head motion prediction
in our cloud-based volumetric video streaming system. We analyze the
performance of our approach using recorded head motion traces and compare its
performance to an autoregression model for different prediction intervals
(look-ahead times). Our results show that the Kalman filter can predict head
orientations 0.5 degrees more accurately than the autoregression model for a
look-ahead time of 60 ms.","['Serhan Gül', 'Sebastian Bosse', 'Dimitri Podborski', 'Thomas Schierl', 'Cornelius Hellge']",2020-07-28T09:41:22Z,http://arxiv.org/abs/2007.14084v1,"['cs.MM', 'eess.IV', 'eess.SP']"
"Jointly Optimizing Sensing Pipelines for Multimodal Mixed Reality
  Interaction","Natural human interactions for Mixed Reality Applications are overwhelmingly
multimodal: humans communicate intent and instructions via a combination of
visual, aural and gestural cues. However, supporting low-latency and accurate
comprehension of such multimodal instructions (MMI), on resource-constrained
wearable devices, remains an open challenge, especially as the state-of-the-art
comprehension techniques for each individual modality increasingly utilize
complex Deep Neural Network models. We demonstrate the possibility of
overcoming the core limitation of latency--vs.--accuracy tradeoff by exploiting
cross-modal dependencies -- i.e., by compensating for the inferior performance
of one model with an increased accuracy of more complex model of a different
modality. We present a sensor fusion architecture that performs MMI
comprehension in a quasi-synchronous fashion, by fusing visual, speech and
gestural input. The architecture is reconfigurable and supports dynamic
modification of the complexity of the data processing pipeline for each
individual modality in response to contextual changes. Using a representative
""classroom"" context and a set of four common interaction primitives, we then
demonstrate how the choices between low and high complexity models for each
individual modality are coupled. In particular, we show that (a) a judicious
combination of low and high complexity models across modalities can offer a
dramatic 3-fold decrease in comprehension latency together with an increase
10-15% in accuracy, and (b) the right collective choice of models is context
dependent, with the performance of some model combinations being significantly
more sensitive to changes in scene context or choice of interaction.","['Darshana Rathnayake', 'Ashen de Silva', 'Dasun Puwakdandawa', 'Lakmal Meegahapola', 'Archan Misra', 'Indika Perera']",2020-10-13T10:13:24Z,http://arxiv.org/abs/2010.06584v2,['cs.HC']
"A Tool for Organizing Key Characteristics of Virtual, Augmented, and
  Mixed Reality for Human-Robot Interaction Systems: Synthesizing VAM-HRI
  Trends and Takeaways","Frameworks have begun to emerge to categorize Virtual, Augmented, and Mixed
Reality (VAM) technologies that provide immersive, intuitive interfaces to
facilitate Human-Robot Interaction. These frameworks, however, fail to capture
key characteristics of the growing subfield of VAM-HRI and can be difficult to
consistently apply due to continuous scales. This work builds upon these prior
frameworks through the creation of a Tool for Organizing Key Characteristics of
VAM-HRI Systems (TOKCS). TOKCS discretizes the continuous scales used within
prior works for more consistent classification and adds additional
characteristics related to a robot's internal model, anchor locations,
manipulability, and the system's software and hardware. To showcase the tool's
capability, TOKCS is applied to the ten papers from the fourth VAM-HRI workshop
and examined for key trends and takeaways. These trends highlight the
expressive capability of TOKCS while also helping frame newer trends and future
work recommendations for VAM-HRI research.","['Thomas R. Groechel', 'Michael E. Walker', 'Christine T. Chang', 'Eric Rosen', 'Jessica Zosa Forde']",2021-08-07T16:01:42Z,http://arxiv.org/abs/2108.03477v3,"['cs.RO', 'cs.HC']"
"Mixed Reality using Illumination-aware Gradient Mixing in Surgical
  Telepresence: Enhanced Multi-layer Visualization","Background and aim: Surgical telepresence using augmented perception has been
applied, but mixed reality is still being researched and is only theoretical.
The aim of this work is to propose a solution to improve the visualization in
the final merged video by producing globally consistent videos when the
intensity of illumination in the input source and target video varies.
Methodology: The proposed system uses an enhanced multi-layer visualization
with illumination-aware gradient mixing using Illumination Aware Video
Composition algorithm. Particle Swarm Optimization Algorithm is used to find
the best sample pair from foreground and background region and image pixel
correlation to estimate the alpha matte. Particle Swarm Optimization algorithm
helps to get the original colour and depth of the unknown pixel in the unknown
region. Result: Our results showed improved accuracy caused by reducing the
Mean squared Error for selecting the best sample pair for unknown region in 10
each sample for bowel, jaw and breast. The amount of this reduction is 16.48%
from the state of art system. As a result, the visibility accuracy is improved
from 89.4 to 97.7% which helped to clear the hand vision even in the difference
of light. Conclusion: Illumination effect and alpha pixel correlation improves
the visualization accuracy and produces a globally consistent composition
results and maintains the temporal coherency when compositing two videos with
high and inverse illumination effect. In addition, this paper provides a
solution for selecting the best sampling pair for the unknown region to obtain
the original colour and depth.","['Nirakar Puri', 'Abeer Alsadoon', 'P. W. C. Prasad', 'Nada Alsalami', 'Tarik A. Rashid']",2021-08-21T11:59:24Z,http://arxiv.org/abs/2110.09318v1,"['cs.MM', 'cs.AI', 'cs.CV']"
"Mixed reality hologram slicer (mxdR-HS): a marker-less tangible user
  interface for interactive holographic volume visualization","Mixed reality head-mounted displays (mxdR-HMD) have the potential to
visualize volumetric medical imaging data in holograms to provide a true sense
of volumetric depth. An effective user interface, however, has yet to be
thoroughly studied. Tangible user interfaces (TUIs) enable a tactile
interaction with a hologram through an object. The object has physical
properties indicating how it might be used with multiple degrees-of-freedom. We
propose a TUI using a planar object (PO) for the holographic medical volume
visualization and exploration. We refer to it as mxdR hologram slicer
(mxdR-HS). Users can slice the hologram to examine particular regions of
interest (ROIs) and intermix complementary data and annotations. The mxdR-HS
introduces a novel real-time ad-hoc marker-less PO tracking method that works
with any PO where corners are visible. The aim of mxdR-HS is to maintain
minimum computational latency while preserving practical tracking accuracy to
enable seamless TUI integration in the commercial mxdR-HMD, which has limited
computational resources. We implemented the mxdR-HS on a commercial Microsoft
HoloLens with a built-in depth camera. Our experimental results showed our
mxdR-HS had a superior computational latency but marginally lower tracking
accuracy than two marker-based tracking methods and resulted in enhanced
computational latency and tracking accuracy than 10 marker-less tracking
methods. Our mxdR-HS, in a medical environment, can be suggested as a visual
guide to display complex volumetric medical imaging data.","['Hoijoon Jung', 'Younhyun Jung', 'Michael Fulham', 'Jinman Kim']",2022-01-26T01:59:53Z,http://arxiv.org/abs/2201.10704v1,['cs.HC']
"Mixed Reality Depth Contour Occlusion Using Binocular Similarity
  Matching and Three-dimensional Contour Optimisation","Mixed reality applications often require virtual objects that are partly
occluded by real objects. However, previous research and commercial products
have limitations in terms of performance and efficiency. To address these
challenges, we propose a novel depth contour occlusion (DCO) algorithm. The
proposed method is based on the sensitivity of contour occlusion and a
binocular stereoscopic vision device. In this method, a depth contour map is
combined with a sparse depth map obtained from a two-stage adaptive filter area
stereo matching algorithm and the depth contour information of the objects
extracted by a digital image stabilisation optical flow method. We also propose
a quadratic optimisation model with three constraints to generate an accurate
dense map of the depth contour for high-quality real-virtual occlusion. The
whole process is accelerated by GPU. To evaluate the effectiveness of the
algorithm, we demonstrate a time con-sumption statistical analysis for each
stage of the DCO algorithm execution. To verify the relia-bility of the
real-virtual occlusion effect, we conduct an experimental analysis on
single-sided, enclosed, and complex occlusions; subsequently, we compare it
with the occlusion method without quadratic optimisation. With our GPU
implementation for real-time DCO, the evaluation indicates that applying the
presented DCO algorithm can enhance the real-time performance and the visual
quality of real-virtual occlusion.","['Naye Ji', 'Fan Zhang', 'Haoxiang Zhang', 'Youbing Zhao', 'Dingguo Yu']",2022-03-04T13:16:40Z,http://arxiv.org/abs/2203.02300v1,"['cs.CV', 'cs.LG']"
"Augmented Reality Appendages for Robots: Design Considerations and
  Recommendations for Maximizing Social and Functional Perception","In order to address the limitations of gestural capabilities in physical
robots, researchers in Virtual, Augmented, Mixed Reality Human-Robot
Interaction (VAM-HRI) have been using augmented-reality visualizations that
increase robot expressivity and improve user perception (e.g., social
presence). While a multitude of virtual robot deictic gestures (e.g., pointing
to an object) have been implemented to improve interactions within VAM-HRI,
such systems are often reported to have tradeoffs between functional and social
user perceptions of robots, creating a need for a unified approach that
considers both attributes. We performed a literature analysis that selected
factors that were noted to significantly influence either user perception or
task efficiency and propose a set of design considerations and recommendations
that address those factors by combining anthropomorphic and non-anthropomorphic
virtual gestures based on the motivation of the interaction, visibility of the
target and robot, salience of the target, and distance between the target and
robot. The proposed recommendations provide the VAM-HRI community with starting
points for selecting appropriate gesture types for a multitude of interaction
contexts.","['Ipek Goktan', 'Karen Ly', 'Thomas R. Groechel', 'Maja J. Mataric']",2022-05-13T16:30:53Z,http://arxiv.org/abs/2205.06747v1,['cs.RO']
"Comparison of synthetic dataset generation methods for medical
  intervention rooms using medical clothing detection as an example","The availability of real data from areas with high privacy requirements, such
as the medical intervention space, is low and the acquisition legally complex.
Therefore, this work presents a way to create a synthetic dataset for the
medical context, using medical clothing as an example. The goal is to close the
reality gap between the synthetic and real data. For this purpose, methods of
3D-scanned clothing and designed clothing are compared in a
Domain-Randomization and Structured-Domain-Randomization scenario using an
Unreal-Engine plugin or Unity. Additionally a Mixed-Reality dataset in front of
a greenscreen and a target domain dataset were used. Our experiments show, that
Structured-Domain-Randomization of designed clothing together with
Mixed-Reality data provide a baseline achieving 72.0% mAP on a test dataset of
the clinical target domain. When additionally using 15% of available target
domain train data, the gap towards 100% (660 images) target domain train data
could be nearly closed 80.05% mAP (81.95% mAP). Finally we show that when
additionally using 100% target domain train data the accuracy could be
increased to 83.35% mAP.","['Patrick Schülein', 'Hannah Teufel', 'Ronja Vorpahl', 'Indira Emter', 'Yannick Bukschat', 'Marcus Pfister', 'Anke Siebert', 'Nils Rathmann', 'Steffen Diehl', 'Marcus Vetter']",2022-09-23T09:36:23Z,http://arxiv.org/abs/2209.11493v1,"['cs.CV', 'I.5; I.2']"
Automated Reconstruction of 3D Open Surfaces from Sparse Point Clouds,"Real-world 3D data may contain intricate details defined by salient surface
gaps. Automated reconstruction of these open surfaces (e.g., non-watertight
meshes) is a challenging problem for environment synthesis in mixed reality
applications. Current learning-based implicit techniques can achieve high
fidelity on closed-surface reconstruction. However, their dependence on the
distinction between the inside and outside of a surface makes them incapable of
reconstructing open surfaces. Recently, a new class of implicit functions have
shown promise in reconstructing open surfaces by regressing an unsigned
distance field. Yet, these methods rely on a discretized representation of the
raw data, which loses important surface details and can lead to outliers in the
reconstruction. We propose IPVNet, a learning-based implicit model that
predicts the unsigned distance between a surface and a query point in 3D space
by leveraging both raw point cloud data and its discretized voxel counterpart.
Experiments on synthetic and real-world public datasets demonstrates that
IPVNet outperforms the state of the art while producing far fewer outliers in
the reconstruction.","['Mohammad Samiul Arshad', 'William J. Beksi']",2022-10-26T22:02:45Z,http://arxiv.org/abs/2210.15059v2,"['cs.CV', 'cs.GR']"
"Complementary Textures. A Novel Approach to Object Alignment in Mixed
  Reality","Alignment between real and virtual objects is a challenging task required for
the deployment of Mixed Reality (MR) into manufacturing, medical, and
construction applications. To face this challenge, a series of methods have
been proposed. While many approaches use dynamic augmentations such as
animations, arrows, or text to assist users, they require tracking the position
of real objects. In contrast, when tracking of the real objects is not
available or desired, alternative approaches use virtual replicas of real
objects to allow for interactive, perceptual virtual-to-real, and/or
real-to-virtual alignment. In these cases, the accuracy achieved strongly
depends on the quality of the perceptual information provided to the user. This
paper proposes a novel set of perceptual alignment concepts that go beyond the
use of traditional visualization of virtual replicas, introducing the concept
of COMPLEMENTARY TEXTURES to improve interactive alignment in MR applications.
To showcase the advantages of using COMPLEMENTARY TEXTURES, we describe three
different implementations that provide highly salient visual cues when
misalignment is observed; or present semantic augmentations that, when combined
with a real object, provide contextual information that can be used during the
alignment process. The authors aim to open new paths for the community to
explore rather than describing end-to-end solutions. The objective is to show
the multitude of opportunities such concepts could provide for further research
and development.","['Alejandro Martin-Gomez', 'Alexander Winkler', 'Rafael de la Tijera Obert', 'Javad Fotouhi', 'Daniel Roth', 'Ulrich Eck', 'Nassir Navab']",2022-11-16T16:50:21Z,http://arxiv.org/abs/2211.09037v1,"['cs.HC', 'cs.GR']"
"Generative AI-empowered Simulation for Autonomous Driving in Vehicular
  Mixed Reality Metaverses","In the vehicular mixed reality (MR) Metaverse, the distance between physical
and virtual entities can be overcome by fusing the physical and virtual
environments with multi-dimensional communications in autonomous driving
systems. Assisted by digital twin (DT) technologies, connected autonomous
vehicles (AVs), roadside units (RSU), and virtual simulators can maintain the
vehicular MR Metaverse via digital simulations for sharing data and making
driving decisions collaboratively. However, large-scale traffic and driving
simulation via realistic data collection and fusion from the physical world for
online prediction and offline training in autonomous driving systems are
difficult and costly. In this paper, we propose an autonomous driving
architecture, where generative AI is leveraged to synthesize unlimited
conditioned traffic and driving data in simulations for improving driving
safety and traffic efficiency. First, we propose a multi-task DT offloading
model for the reliable execution of heterogeneous DT tasks with different
requirements at RSUs. Then, based on the preferences of AV's DTs and collected
realistic data, virtual simulators can synthesize unlimited conditioned driving
and traffic datasets to further improve robustness. Finally, we propose a
multi-task enhanced auction-based mechanism to provide fine-grained incentives
for RSUs in providing resources for autonomous driving. The property analysis
and experimental results demonstrate that the proposed mechanism and
architecture are strategy-proof and effective, respectively.","['Minrui Xu', 'Dusit Niyato', 'Junlong Chen', 'Hongliang Zhang', 'Jiawen Kang', 'Zehui Xiong', 'Shiwen Mao', 'Zhu Han']",2023-02-16T16:54:10Z,http://arxiv.org/abs/2302.08418v1,"['cs.AI', 'cs.NI']"
HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations,"Generating both plausible and accurate full body avatar motion is the key to
the quality of immersive experiences in mixed reality scenarios. Head-Mounted
Devices (HMDs) typically only provide a few input signals, such as head and
hands 6-DoF. Recently, different approaches achieved impressive performance in
generating full body motion given only head and hands signal. However, to the
best of our knowledge, all existing approaches rely on full hand visibility.
While this is the case when, e.g., using motion controllers, a considerable
proportion of mixed reality experiences do not involve motion controllers and
instead rely on egocentric hand tracking. This introduces the challenge of
partial hand visibility owing to the restricted field of view of the HMD. In
this paper, we propose the first unified approach, HMD-NeMo, that addresses
plausible and accurate full body motion generation even when the hands may be
only partially visible. HMD-NeMo is a lightweight neural network that predicts
the full body motion in an online and real-time fashion. At the heart of
HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask
tokens that encourage plausible motion in the absence of hand observations. We
perform extensive analysis of the impact of different components in HMD-NeMo
and introduce a new state-of-the-art on AMASS dataset through our evaluation.","['Sadegh Aliakbarian', 'Fatemeh Saleh', 'David Collier', 'Pashmina Cameron', 'Darren Cosker']",2023-08-22T08:07:12Z,http://arxiv.org/abs/2308.11261v1,['cs.CV']
"HoloPOCUS: Portable Mixed-Reality 3D Ultrasound Tracking, Reconstruction
  and Overlay","Ultrasound (US) imaging provides a safe and accessible solution to procedural
guidance and diagnostic imaging. The effective usage of conventional 2D US for
interventional guidance requires extensive experience to project the image
plane onto the patient, and the interpretation of images in diagnostics suffers
from high intra- and inter-user variability. 3D US reconstruction allows for
more consistent diagnosis and interpretation, but existing solutions are
limited in terms of equipment and applicability in real-time navigation. To
address these issues, we propose HoloPOCUS - a mixed reality US system (MR-US)
that overlays rich US information onto the user's vision in a point-of-care
setting. HoloPOCUS extends existing MR-US methods beyond placing a US plane in
the user's vision to include a 3D reconstruction and projection that can aid in
procedural guidance using conventional probes. We validated a tracking pipeline
that demonstrates higher accuracy compared to existing MR-US works.
Furthermore, user studies conducted via a phantom task showed significant
improvements in navigation duration when using our proposed methods.","['Kian Wei Ng', 'Yujia Gao', 'Shaheryar Mohammed Furqan', 'Zachery Yeo', 'Joel Lau', 'Kee Yuan Ngiam', 'Eng Tat Khoo']",2023-08-26T09:28:20Z,http://arxiv.org/abs/2308.13823v1,"['cs.CV', 'cs.HC']"
"Investigating the Correlation Between Presence and Reaction Time in
  Mixed Reality","Measuring presence is critical to improving user involvement and performance
in Mixed Reality (MR). \emph{Presence}, a crucial aspect of MR, is
traditionally gauged using subjective questionnaires, leading to a lack of
time-varying responses and susceptibility to user bias. Inspired by the
existing literature on the relationship between presence and human performance,
the proposed methodology systematically measures a user's reaction time to a
visual stimulus as they interact within a manipulated MR environment. We
explore the user reaction time as a quantity that can be easily measured using
the systemic tools available in modern MR devices. We conducted an exploratory
study (N=40) with two experiments designed to alter the users' sense of
presence by manipulating \emph{place illusion} and \emph{plausibility
illusion}. We found a significant correlation between presence scores and
reaction times with a correlation coefficient -0.65, suggesting that users with
a higher sense of presence responded more swiftly to stimuli. We develop a
model that estimates a user's presence level using the reaction time values
with high accuracy of up to 80\%. While our study suggests that reaction time
can be used as a measure of presence, further investigation is needed to
improve the accuracy of the model.","['Yasra Chandio', 'Noman Bashir', 'Victoria Interrante', 'Fatima M. Anwar']",2023-09-20T22:02:38Z,http://arxiv.org/abs/2309.11662v1,"['cs.HC', 'cs.ET']"
"BrickStARt: Enabling In-situ Design and Tangible Exploration for
  Personal Fabrication using Mixed Reality","3D printers enable end-users to design and fabricate unique physical
artifacts but maintain an increased entry barrier and friction. End users must
design tangible artifacts through intangible media away from the main problem
space (ex-situ) and transfer spatial requirements to an abstract software
environment. To allow users to evaluate dimensions, balance, or fit early and
in-situ, we developed BrickStARt, a design tool using tangible construction
blocks paired with a mixed-reality headset. Users assemble a physical block
model at the envisioned location of the fabricated artifact. Designs can be
tested tangibly, refined, and digitally post-processed, remaining continuously
in-situ. We implemented BrickStARt using a Magic Leap headset and present
walkthroughs, highlighting novel interactions for 3D design. In a user study
(n=16), first-time 3D modelers succeeded more often using BrickStARt than
Tinkercad. Our results suggest that BrickStARt provides an accessible and
explorative process while facilitating quick, tangible design iterations that
allow users to detect physics-related issues (e.g., clearance) early on.","['Evgeny Stemasov', 'Jessica Hohn', 'Maurice Cordts', 'Anja Schikorr', 'Enrico Rukzio', 'Jan Gugenheimer']",2023-10-05T17:18:13Z,http://arxiv.org/abs/2310.03700v1,"['cs.HC', 'H.5.1; H.5.2; H.5.m']"
GaitGuard: Towards Private Gait in Mixed Reality,"Augmented/Mixed Reality (AR/MR) devices are unique from other mobile systems
because of their capability to offer an immersive multi-user collaborative
experience. While previous studies have explored privacy and security aspects
of multiple user interactions in AR/MR, a less-explored area is the
vulnerability of gait privacy. Gait is considered a private state because it is
a highly individualistic and a distinctive biometric trait. Thus, preserving
gait privacy in emerging AR/MR systems is crucial to safeguard individuals from
potential identity tracking and unauthorized profiling.
  This paper first introduces GaitExtract, a framework designed to
automatically detect gait information in humans, shedding light on the nuances
of gait privacy in AR/MR. In this paper, we designed GaitExtract, a framework
that can automatically detect the outside gait information of a human and
investigate the vulnerability of gait privacy in AR. In a user study with 20
participants, our findings reveal that participants were uniquely identifiable
with an accuracy of up to 78% using GaitExtract. Consequently, we propose
GaitGuard, a system that safeguards gait information of people appearing in the
camera view of the AR/MR device.
  Furthermore, we tested GaitGuard in an MR collaborative application,
achieving 22 fps while streaming mitigated frames to the collaborative server.
Our user-study survey indicated that users are more comfortable with releasing
videos of them walking when GaitGuard is applied to the frames. These results
underscore the efficacy and practicality of GaitGuard in mitigating gait
privacy concerns in MR contexts.","['Diana Romero', 'Ruchi Jagdish Patel', 'Athina Markopoulou', 'Salma Elmalaki']",2023-12-07T17:42:04Z,http://arxiv.org/abs/2312.04470v2,"['cs.HC', 'cs.CR']"
"Collaborative System Design of Mixed Reality Communication for Medical
  Training","We present the design of a mixed reality (MR) telehealth training system that
aims to close the gap between in-person and distance training and re-training
for medical procedures. Our system uses real-time volumetric capture as a means
for communicating and relating spatial information between the non-colocated
trainee and instructor. The system's design is based on a requirements
elicitation study performed in situ, at a medical school simulation training
center. The focus is on the lightweight real-time transmission of volumetric
data - meaning the use of consumer hardware, easy and quick deployment, and
low-demand computations. We evaluate the MR system design by analyzing the
workload for the users during medical training. We compare in-person, video,
and MR training workloads. The results indicate that the overall workload for
central line placement training with MR does not increase significantly
compared to video communication. Our work shows that, when designed
strategically together with domain experts, an MR communication system can be
used effectively for complex medical procedural training without increasing the
overall workload for users significantly. Moreover, MR systems offer new
opportunities for teaching due to spatial information, hand tracking, and
augmented communication.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka', 'Christian Guetl']",2023-12-14T22:36:56Z,http://arxiv.org/abs/2312.09382v1,['cs.HC']
"ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering
  Perceived Texture Roughness in Mixed Reality","Extensive research has been done in haptic feedback for texture simulation in
virtual reality (VR). However, it is challenging to modify the perceived
tactile texture of existing physical objects which usually serve as anchors for
virtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a
finger-worn haptic device that uses vibratory-pneumatic feedback to modulate
(i.e., increase and decrease) the perceived roughness of the material surface
contacted by the user's fingerpad while supporting the perceived sensation of
other haptic properties (e.g., temperature or stickiness) in MR. Our device
includes a silicone-based pneumatic actuator that can lift the user's fingerpad
on the physical surface to reduce the contact area for roughness decreasing,
and an on-finger vibrator for roughness increasing. Our user-perception
experimental results showed that the participants could perceive changes in
roughness, both increasing and decreasing, compared to the original material
surface. We also observed the overlapping roughness ratings among certain
haptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived
roughness of some materials without any haptic feedback. This suggests the
potential to alter the perceived texture of one type of material to another in
terms of roughness (e.g., modifying the perceived texture of ceramics as
glass). Lastly, a user study of MR experience showed that ViboPneumo could
significantly improve the MR user experience, particularly for visual-haptic
matching, compared to the condition of a bare finger. We also demonstrated a
few application scenarios for ViboPneumo.","['Shaoyu Cai', 'Zhenlin Chen', 'Haichen Gao', 'Ya Huang', 'Qi Zhang', 'Xinge Yu', 'Kening Zhu']",2024-03-08T09:48:05Z,http://arxiv.org/abs/2403.05182v1,"['cs.HC', 'cs.GR']"
"Explainable Interfaces for Rapid Gaze-Based Interactions in Mixed
  Reality","Gaze-based interactions offer a potential way for users to naturally engage
with mixed reality (XR) interfaces. Black-box machine learning models enabled
higher accuracy for gaze-based interactions. However, due to the black-box
nature of the model, users might not be able to understand and effectively
adapt their gaze behaviour to achieve high quality interaction. We posit that
explainable AI (XAI) techniques can facilitate understanding of and interaction
with gaze-based model-driven system in XR. To study this, we built a real-time,
multi-level XAI interface for gaze-based interaction using a deep learning
model, and evaluated it during a visual search task in XR. A between-subjects
study revealed that participants who interacted with XAI made more accurate
selections compared to those who did not use the XAI system (i.e., F1 score
increase of 10.8%). Additionally, participants who used the XAI system adapted
their gaze behavior over time to make more effective selections. These findings
suggest that XAI can potentially be used to assist users in more effective
collaboration with model-driven interactions in XR.","['Mengjie Yu', 'Dustin Harris', 'Ian Jones', 'Ting Zhang', 'Yue Liu', 'Naveen Sendhilnathan', 'Narine Kokhlikyan', 'Fulton Wang', 'Co Tran', 'Jordan L. Livingston', 'Krista E. Taylor', 'Zhenhong Hu', 'Mary A. Hood', 'Hrvoje Benko', 'Tanya R. Jonker']",2024-04-21T21:13:46Z,http://arxiv.org/abs/2404.13777v1,['cs.HC']
"Alignment of the Virtual Scene to the Tracking Space of a Mixed Reality
  Head-Mounted Display","With the mounting global interest for optical see-through head-mounted
displays (OST-HMDs) across medical, industrial and entertainment settings, many
systems with different capabilities are rapidly entering the market. Despite
such variety, they all require display calibration to create a proper mixed
reality environment. With the aid of tracking systems, it is possible to
register rendered graphics with tracked objects in the real world. We propose a
calibration procedure to properly align the coordinate system of a 3D virtual
scene that the user sees with that of the tracker. Our method takes a blackbox
approach towards the HMD calibration, where the tracker's data is its input and
the 3D coordinates of a virtual object in the observer's eye is the output; the
objective is thus to find the 3D projection that aligns the virtual content
with its real counterpart. In addition, a faster and more intuitive version of
this calibration is introduced in which the user simultaneously aligns multiple
points of a single virtual 3D object with its real counterpart; this reduces
the number of required repetitions in the alignment from 20 to only 4, which
leads to a much easier calibration task for the user. In this paper, both
internal (HMD camera) and external tracking systems are studied. We perform
experiments with Microsoft HoloLens, taking advantage of its self localization
and spatial mapping capabilities to eliminate the requirement for line of sight
from the HMD to the object or external tracker. The experimental results
indicate an accuracy of up to 4 mm in the average reprojection error based on
two separate evaluation methods. We further perform experiments with the
internal tracking on the Epson Moverio BT-300 to demonstrate that the method
can provide similar results with other HMDs.","['Ehsan Azimi', 'Long Qian', 'Nassir Navab', 'Peter Kazanzides']",2017-03-16T21:51:23Z,http://arxiv.org/abs/1703.05834v4,['cs.HC']
"Augment Yourself: Mixed Reality Self-Augmentation Using Optical
  See-through Head-mounted Displays and Physical Mirrors","Optical see-though head-mounted displays (OST HMDs) are one of the key
technologies for merging virtual objects and physical scenes to provide an
immersive mixed reality (MR) environment to its user. A fundamental limitation
of HMDs is, that the user itself cannot be augmented conveniently as, in casual
posture, only the distal upper extremities are within the field of view of the
HMD. Consequently, most MR applications that are centered around the user, such
as virtual dressing rooms or learning of body movements, cannot be realized
with HMDs. In this paper, we propose a novel concept and prototype system that
combines OST HMDs and physical mirrors to enable self-augmentation and provide
an immersive MR environment centered around the user. Our system, to the best
of our knowledge the first of its kind, estimates the user's pose in the
virtual image generated by the mirror using an RGBD camera attached to the HMD
and anchors virtual objects to the reflection rather than the user directly. We
evaluate our system quantitatively with respect to calibration accuracy and
infrared signal degradation effects due to the mirror, and show its potential
in applications where large mirrors are already an integral part of the
facility. Particularly, we demonstrate its use for virtual fitting rooms,
gaming applications, anatomy learning, and personal fitness. In contrast to
competing devices such as LCD-equipped smart mirrors, the proposed system
consists of only an HMD with RGBD camera and, thus, does not require a prepared
environment making it very flexible and generic. In future work, we will aim to
investigate how the system can be optimally used for physical rehabilitation
and personal training as a promising application.","['Mathias Unberath', 'Kevin Yu', 'Roghayeh Barmaki', 'Alex Johnson', 'Nassir Navab']",2020-07-06T16:53:47Z,http://arxiv.org/abs/2007.02884v1,"['cs.HC', 'cs.CV']"
"A Novel Solution of Using Mixed Reality in Bowel and Oral and
  Maxillofacial Surgical Telepresence: 3D Mean Value Cloning algorithm","Background and aim: Most of the Mixed Reality models used in the surgical
telepresence are suffering from discrepancies in the boundary area and
spatial-temporal inconsistency due to the illumination variation in the video
frames. The aim behind this work is to propose a new solution that helps
produce the composite video by merging the augmented video of the surgery site
and the virtual hand of the remote expertise surgeon. The purpose of the
proposed solution is to decrease the processing time and enhance the accuracy
of merged video by decreasing the overlay and visualization error and removing
occlusion and artefacts. Methodology: The proposed system enhanced the mean
value cloning algorithm that helps to maintain the spatial-temporal consistency
of the final composite video. The enhanced algorithm includes the 3D mean value
coordinates and improvised mean value interpolant in the image cloning process,
which helps to reduce the sawtooth, smudging and discolouration artefacts
around the blending region. Results: As compared to the state of the art
solution, the accuracy in terms of overlay error of the proposed solution is
improved from 1.01mm to 0.80mm whereas the accuracy in terms of visualization
error is improved from 98.8% to 99.4%. The processing time is reduced to 0.173
seconds from 0.211 seconds. Conclusion: Our solution helps make the object of
interest consistent with the light intensity of the target image by adding the
space distance that helps maintain the spatial consistency in the final merged
video.","['Arjina Maharjan', 'Abeer Alsadoon', 'P. W. C. Prasad', 'Nada AlSallami', 'Tarik A. Rashid', 'Ahmad Alrubaie', 'Sami Haddad']",2021-03-17T10:01:06Z,http://arxiv.org/abs/2104.06316v1,"['physics.med-ph', 'cs.CV', 'cs.GR', 'cs.RO']"
"Deep Learning-based Framework for Automatic Cranial Defect
  Reconstruction and Implant Modeling","The goal of this work is to propose a robust, fast, and fully automatic
method for personalized cranial defect reconstruction and implant modeling.
  We propose a two-step deep learning-based method using a modified U-Net
architecture to perform the defect reconstruction, and a dedicated iterative
procedure to improve the implant geometry, followed by automatic generation of
models ready for 3-D printing. We propose a cross-case augmentation based on
imperfect image registration combining cases from different datasets. We
perform ablation studies regarding different augmentation strategies and
compare them to other state-of-the-art methods.
  We evaluate the method on three datasets introduced during the AutoImplant
2021 challenge, organized jointly with the MICCAI conference. We perform the
quantitative evaluation using the Dice and boundary Dice coefficients, and the
Hausdorff distance. The average Dice coefficient, boundary Dice coefficient,
and the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm
respectively. We perform an additional qualitative evaluation by 3-D printing
and visualization in mixed reality to confirm the implant's usefulness.
  We propose a complete pipeline that enables one to create the cranial implant
model ready for 3-D printing. The described method is a greatly extended
version of the method that scored 1st place in all AutoImplant 2021 challenge
tasks. We freely release the source code, that together with the open datasets,
makes the results fully reproducible. The automatic reconstruction of cranial
defects may enable manufacturing personalized implants in a significantly
shorter time, possibly allowing one to perform the 3-D printing process
directly during a given intervention. Moreover, we show the usability of the
defect reconstruction in mixed reality that may further reduce the surgery
time.","['Marek Wodzinski', 'Mateusz Daniol', 'Miroslaw Socha', 'Daria Hemmerling', 'Maciej Stanuch', 'Andrzej Skalski']",2022-04-13T11:33:26Z,http://arxiv.org/abs/2204.06310v1,"['eess.IV', 'cs.CV', 'cs.LG']"
"Full Body Video-Based Self-Avatars for Mixed Reality: from E2E System to
  User Study","In this work we explore the creation of self-avatars through video
pass-through in Mixed Reality (MR) applications. We present our end-to-end
system, including: custom MR video pass-through implementation on a commercial
head mounted display (HMD), our deep learning-based real-time egocentric body
segmentation algorithm, and our optimized offloading architecture, to
communicate the segmentation server with the HMD. To validate this technology,
we designed an immersive VR experience where the user has to walk through a
narrow tiles path over an active volcano crater. The study was performed under
three body representation conditions: virtual hands, video pass-through with
color-based full-body segmentation and video pass-through with deep learning
full-body segmentation. This immersive experience was carried out by 30 women
and 28 men. To the best of our knowledge, this is the first user study focused
on evaluating video-based self-avatars to represent the user in a MR scene.
Results showed no significant differences between the different body
representations in terms of presence, with moderate improvements in some
Embodiment components between the virtual hands and full-body representations.
Visual Quality results showed better results from the deep-learning algorithms
in terms of the whole body perception and overall segmentation quality. We
provide some discussion regarding the use of video-based self-avatars, and some
reflections on the evaluation methodology. The proposed E2E solution is in the
boundary of the state of the art, so there is still room for improvement before
it reaches maturity. However, this solution serves as a crucial starting point
for novel MR distributed solutions.","['Diego Gonzalez Morin', 'Ester Gonzalez-Sosa', 'Pablo Perez', 'Alvaro Villegas']",2022-08-24T20:59:17Z,http://arxiv.org/abs/2208.12639v1,['cs.CV']
ArK: Augmented Reality with Knowledge Interactive Emergent Ability,"Despite the growing adoption of mixed reality and interactive AI agents, it
remains challenging for these systems to generate high quality 2D/3D scenes in
unseen environments. The common practice requires deploying an AI agent to
collect large amounts of data for model training for every new task. This
process is costly, or even impossible, for many domains. In this study, we
develop an infinite agent that learns to transfer knowledge memory from general
foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene
understanding and generation in the physical or virtual world. The heart of our
approach is an emerging mechanism, dubbed Augmented Reality with Knowledge
Inference Interaction (ArK), which leverages knowledge-memory to generate
scenes in unseen physical world and virtual reality environments. The knowledge
interactive emergent ability (Figure 1) is demonstrated as the observation
learns i) micro-action of cross-modality: in multi-modality models to collect a
large amount of relevant knowledge memory data for each interaction task (e.g.,
unseen scene understanding) from the physical reality; and ii) macro-behavior
of reality-agnostic: in mix-reality environments to improve interactions that
tailor to different characterized roles, target variables, collaborative
information, and so on. We validate the effectiveness of ArK on the scene
generation and editing tasks. We show that our ArK approach, combined with
large foundation models, significantly improves the quality of generated 2D/3D
scenes, compared to baselines, demonstrating the potential benefit of
incorporating ArK in generative AI for applications such as metaverse and
gaming simulation.","['Qiuyuan Huang', 'Jae Sung Park', 'Abhinav Gupta', 'Paul Bennett', 'Ran Gong', 'Subhojit Som', 'Baolin Peng', 'Owais Khan Mohammed', 'Chris Pal', 'Yejin Choi', 'Jianfeng Gao']",2023-05-01T17:57:01Z,http://arxiv.org/abs/2305.00970v1,['cs.CV']
"Which architecture should be implemented to manage data from the real
  world, in an Unreal Engine 5 simulator and in the context of mixed reality?","Due to its ability to generate millions of particles, massively detailed
scenes and confusing artificial illumination with reality, the version 5 of
Unreal Engine promises unprecedented industrial applications. The paradigms and
aims of Unreal Engine contrast with the industrial simulators typically used by
the scientific community. The visual quality and performance of its rendering
engine increase the opportunities, especially for industries and simulation
business: where interoperability and scalability are required. The study of the
following issue `` Which architecture should we implement to integrate
real-world data, in an Unreal Engine 5 simulator and in a mixed-reality
environment? '' offers a point of view. The topic is reexamined in an
innovative and conceptual way, such as the generalization of mixedreality
technologies, Internet of Things, digital twins, Big Data but providing a
solution for simple and actual use cases. This paper gives a detailed analysis
of the issue, at both theoretical and operational level. Then, the document
goes deep into Unreal Engine's operation in order to extract the vanilla
capabilities. Next, the C++ Plugin system is reviewed in details as well as the
third-party library integration: pitfalls to be avoided are shown. Finally, the
last chapter proposes a generic architecture, useful in large-scale industrial
3D applications, such as collaborative work or hyper-connected simulators. This
document might be of interest to an Unreal Engine expert who would like to
discover about server architectures. Conversely, it could be relevant for an
expert in backend servers who wants to learn about Unreal Engine capabilities.
This research concludes that Unreal Engine's modularity enables integration
with almost any protocol. The features to integrate external real data are
numerous but depend on use cases. Distributed systems for Big Data require a
scalable architecture, possibly without the use of the Unreal Engine dedicated
server. Environments, which require sub-second latency need to implement direct
connections, bypassing any intermediate servers.",['Jonathan Cassaing'],2023-05-16T07:51:54Z,http://arxiv.org/abs/2305.09244v1,['cs.SE']
"Forging the Industrial Metaverse -- Where Industry 5.0, Augmented and
  Mixed Reality, IIoT, Opportunistic Edge Computing and Digital Twins Meet","The Metaverse is a concept that proposes to immerse users into real-time
rendered 3D content virtual worlds delivered through Extended Reality (XR)
devices like Augmented and Mixed Reality (AR/MR) smart glasses and Virtual
Reality (VR) headsets. When the Metaverse concept is applied to industrial
environments, it is called Industrial Metaverse, a hybrid world where
industrial operators work by using some of the latest technologies. Currently,
such technologies are related to the ones fostered by Industry 4.0, which is
evolving towards Industry 5.0, a paradigm that enhances Industry 4.0 by
creating a sustainable and resilient world of industrial human-centric
applications. The Industrial Metaverse can benefit from Industry 5.0, since it
implies making use of dynamic and up-to-date content, as well as fast
human-to-machine interactions. To enable such enhancements, this article
proposes the concept of Meta-Operator: an Industry 5.0 worker that interacts
with Industrial Metaverse applications and with his/her surroundings through
advanced XR devices. This article provides a description of the technologies
that support Meta-Operators: the main components of the Industrial Metaverse,
the latest XR technologies and the use of Opportunistic Edge Computing
communications (to interact with surrounding IoT/IioT devices). Moreover, this
paper analyzes how to create the next generation of Industrial Metaverse
applications based on Industry 5.0, including the integration of AR/MR devices
with IoT/IIoT solutions, the development of advanced communications or the
creation of shared experiences. Finally, this article provides a list of
potential Industry 5.0 applications for the Industrial Metaverse and analyzes
the main challenges and research lines. Thus, this article provides useful
guidelines for the researchers that will create the next generation of
applications for the Industrial Metaverse.","['Tiago M. Fernández-Caramés', 'Paula Fraga-Lamas']",2024-03-17T19:14:28Z,http://arxiv.org/abs/2403.11312v1,"['cs.ET', 'cs.HC']"
"A Distributed Software Architecture for Collaborative Teleoperation
  based on a VR Platform and Web Application Interoperability","Augmented Reality and Virtual Reality can provide to a Human Operator (HO) a
real help to complete complex tasks, such as robot teleoperation and
cooperative teleassistance. Using appropriate augmentations, the HO can
interact faster, safer and easier with the remote real world. In this paper, we
present an extension of an existing distributed software and network
architecture for collaborative teleoperation based on networked human-scaled
mixed reality and mobile platform. The first teleoperation system was composed
by a VR application and a Web application. However the 2 systems cannot be used
together and it is impossible to control a distant robot simultaneously. Our
goal is to update the teleoperation system to permit a heterogeneous
collaborative teleoperation between the 2 platforms. An important feature of
this interface is based on different Mobile platforms to control one or many
robots.","['Christophe Domingues', 'Samir Otmane', 'Frédéric Davesne', 'Malik Mallem']",2009-04-14T11:21:47Z,http://arxiv.org/abs/0904.2096v1,"['cs.HC', 'cs.GR', 'cs.MM', 'cs.RO']"
Immersive Augmented Reality Training for Complex Manufacturing Scenarios,"In the complex manufacturing sector a considerable amount of resources are
focused on developing new skills and training workers. In that context,
increasing the effectiveness of those processes and reducing the investment
required is an outstanding issue. In this paper we present an experiment that
shows how modern Human Computer Interaction (HCI) metaphors such as
collaborative mixed-reality can be used to transmit procedural knowledge and
could eventually replace other forms of face-to-face training. We implement a
real-time Immersive Augmented Reality (IAR) setup with see-through cameras that
allows for collaborative interactions that can simulate conventional forms of
training. The obtained results indicate that people who took the IAR training
achieved the same performance than people in the conventional face-to-face
training condition. These results, their implications for future training and
the use of HCI paradigms in this context are discussed in this paper.","['Mar Gonzalez-Franco', 'Julio Cermeron', 'Katie Li', 'Rodrigo Pizarro', 'Jacob Thorn', 'Windo Hutabarat', 'Ashutosh Tiwari', 'Pablo Bermell-Garcia']",2016-02-05T07:50:25Z,http://arxiv.org/abs/1602.01944v2,['cs.HC']
"Automated capture and delivery of assistive task guidance with an
  eyewear computer: The GlaciAR system","In this paper we describe and evaluate a mixed reality system that aims to
augment users in task guidance applications by combining automated and
unsupervised information collection with minimally invasive video guides. The
result is a self-contained system that we call GlaciAR (Glass-enabled
Contextual Interactions for Augmented Reality), that operates by extracting
contextual interactions from observing users performing actions. GlaciAR is
able to i) automatically determine moments of relevance based on a head motion
attention model, ii) automatically produce video guidance information, iii)
trigger these video guides based on an object detection method, iv) learn
without supervision from observing multiple users and v) operate fully on-board
a current eyewear computer (Google Glass). We describe the components of
GlaciAR together with evaluations on how users are able to use the system to
achieve three tasks. We see this work as a first step toward the development of
systems that aim to scale up the notoriously difficult authoring problem in
guidance systems and where people's natural abilities are enhanced via
minimally invasive visual guidance.","['Teesid Leelasawassuk', 'Dima Damen', 'Walterio Mayol-Cuevas']",2016-12-29T01:10:54Z,http://arxiv.org/abs/1701.02586v1,['cs.HC']
BodyDigitizer: An Open Source Photogrammetry-based 3D Body Scanner,"With the rising popularity of Augmented and Virtual Reality, there is a need
for representing humans as virtual avatars in various application domains
ranging from remote telepresence, games to medical applications. Besides
explicitly modelling 3D avatars, sensing approaches that create person-specific
avatars are becoming popular. However, affordable solutions typically suffer
from a low visual quality and professional solution are often too expensive to
be deployed in nonprofit projects.
  We present an open-source project, BodyDigitizer, which aims at providing
both build instructions and configuration software for a high-resolution
photogrammetry-based 3D body scanner. Our system encompasses up to 96 Rasperry
PI cameras, active LED lighting, a sturdy frame construction and open-source
configuration software. %We demonstrate the applicability of the body scanner
in a nonprofit Mixed Reality health project. The detailed build instruction and
software are available at http://www.bodydigitizer.org.","['Travis Gesslein', 'Daniel Scherer', 'Jens Grubert']",2017-10-03T20:10:10Z,http://arxiv.org/abs/1710.01370v2,"['cs.CV', 'cs.HC']"
"Air Mounted Eyepiece: Design Methods for Aerial Optical Functions of
  Near-Eye and See-Through Display using Transmissive Mirror Device","We propose a novel method to implement an optical see-through head mounted
display which renders real aerial images with a wide viewing angle, called an
Air Mounted Eyepiece (AME). To achieve the AMD design, we employ an
off-the-shelf head mounted display and Transmissive Mirror Device (TMD) which
is usually used in aerial real imaging systems. In the proposed method, we
replicate the function of the head mounted display (HMD) itself, which is used
in the air by using the TMD and presenting a real image of eyepiece in front of
the eye. Moreover, it can realize a wide viewing angle 3D display by placing a
virtual lens in front of the eye without wearing an HMD. In addition to
enhancing the experience of mixed reality and augmented reality, our proposed
method can be used as a 3D imaging method for use in other applications such as
in automobiles and desktop work. We aim to contribute to the field of
human-computer interaction and the research on eyepiece interfaces by
discussing the advantages and the limitations of this near-eye optical system.","['Yoichi Ochiai', 'Kazuki Otao', 'Hiroyuki Osone']",2017-10-11T03:08:02Z,http://arxiv.org/abs/1710.03889v1,['cs.HC']
Beyond One Glance: Gated Recurrent Architecture for Hand Segmentation,"As mixed reality is gaining increased momentum, the development of effective
and efficient solutions to egocentric hand segmentation is becoming critical.
Traditional segmentation techniques typically follow a one-shot approach, where
the image is passed forward only once through a model that produces a
segmentation mask. This strategy, however, does not reflect the perception of
humans, who continuously refine their representation of the world. In this
paper, we therefore introduce a novel gated recurrent architecture. It goes
beyond both iteratively passing the predicted segmentation mask through the
network and adding a standard recurrent unit to it. Instead, it incorporates
multiple encoder-decoder layers of the segmentation network, so as to keep
track of its internal state in the refinement process. As evidenced by our
results on standard hand segmentation benchmarks and on our own dataset, our
approach outperforms these other, simpler recurrent segmentation techniques, as
well as the state-of-the-art hand segmentation one. Furthermore, we demonstrate
the generality of our approach by applying it to road segmentation, where it
also outperforms other baseline methods.","['Wei Wang', 'Kaicheng Yu', 'Joachim Hugonot', 'Pascal Fua', 'Mathieu Salzmann']",2018-11-27T11:16:41Z,http://arxiv.org/abs/1811.10914v3,"['cs.CV', 'cs.LG']"
Interoperable GPU Kernels as Latency Improver for MEC,"Mixed reality (MR) applications are expected to become common when 5G goes
mainstream. However, the latency requirements are challenging to meet due to
the resources required by video-based remoting of graphics, that is, decoding
video codecs. We propose an approach towards tackling this challenge: a
client-server implementation for transacting intermediate representation (IR)
between a mobile UE and a MEC server instead of video codecs and this way
avoiding video decoding. We demonstrate the ability to address latency
bottlenecks on edge computing workloads that transact graphics. We select
SPIR-V compatible GPU kernels as the intermediate representation. Our approach
requires know-how in GPU architecture and GPU domain-specific languages (DSLs),
but compared to video-based edge graphics, it decreases UE device delay by
sevenfold. Further, we find that due to low cold-start times on both UEs and
MEC servers, application migration can happen in milliseconds. We imply that
graphics-based location-aware applications, such as MR, can benefit from this
kind of approach.","['Juuso Haavisto', 'Jukka Riekki']",2020-01-25T19:07:58Z,http://arxiv.org/abs/2001.09352v1,"['cs.DC', 'cs.GR']"
"Blending Entropy: A Term for Addressing Information Density in Mediated
  Reality","The virtuality continuum describes the degrees of positive virtuality under
the umbrella term mixed reality. Besides adding virtual information within a
mixed environment, diminished reality aims at reducing real world information.
Mann defined the term mediated reality (MR), which also considered diminished
reality, but without the possibility to describe different degrees of fusion
between a mixed and a diminished reality. That is why this work defines the new
term blending entropy that captures the relations between a mixed and a
diminished reality. The blending entropy is based on the information density of
the mediated reality and the actual area the user has to comprehend, which is
named perceptual frustum. We describe the blending entropy's twodimensional
dependencies and detail important points in the blending entropy's space.","['Philipp Tiefenbacher', 'Gerhard Rigoll']",2016-09-13T06:29:47Z,http://arxiv.org/abs/1609.03695v2,['cs.HC']
"Towards high-throughput 3D insect capture for species discovery and
  diagnostics","Digitisation of natural history collections not only preserves precious
information about biological diversity, it also enables us to share, analyse,
annotate and compare specimens to gain new insights. High-resolution,
full-colour 3D capture of biological specimens yields color and geometry
information complementary to other techniques (e.g., 2D capture, electron
scanning and micro computed tomography). However 3D colour capture of small
specimens is slow for reasons including specimen handling, the narrow depth of
field of high magnification optics, and the large number of images required to
resolve complex shapes of specimens. In this paper, we outline techniques to
accelerate 3D image capture, including using a desktop robotic arm to automate
the insect handling process; using a calibrated pan-tilt rig to avoid attaching
calibration targets to specimens; using light field cameras to capture images
at an extended depth of field in one shot; and using 3D Web and mixed reality
tools to facilitate the annotation, distribution and visualisation of 3D
digital models.","['Chuong Nguyen', 'Matt Adcock', 'Stuart Anderson', 'David Lovell', 'Nicole Fisher', 'John La Salle']",2017-09-07T00:31:03Z,http://arxiv.org/abs/1709.02033v1,['cs.CV']
"CoAug-MR: An MR-based Interactive Office Workstation Design System via
  Augmented Multi-Person Collaboration","Digital prototyping and evaluation using 3D modeling and digital human models
are becoming more practical for customizing products to the preference of a
user. However, the 3D modeling is less accessible to casual users, and digital
human models suffer from insufficient body data and less intuitive illustration
on how people use the product or how it accommodates to their body. Recently,
VR-supported 'Do It Yourself' design has achieved real-time ergonomic
evaluation with users themselves by capturing their poses, however, it lacks
reliability and quality of design. In this paper, we explore a multi-person
interactive design approach that enables designers, users, and even ergonomists
to collaborate to achieve effective and reliable design and prototyping tasks.
Mixed Reality that utilizes Hololens and motion tracking devices had been
developed to provide instant design feedback and evaluation and to experience
prototyping in physical space. We evaluate the system based on the usability
study, where casual users and designers are engaged in the interactive process
of designing items with respect to the body information, the preference, and
the environment.","['Lin Wang', 'Kuk-Jin Yoon']",2019-07-06T10:19:04Z,http://arxiv.org/abs/1907.03107v3,"['cs.HC', 'cs.GR']"
CityScopeAR: Urban Design and Crowdsourced Engagement Platform,"Processes of urban planning, urban design and architecture are inherently
tangible, iterative and collaborative. Nevertheless, the majority of tools in
these fields offer virtual environments and single user experience. This paper
presents CityScopeAR: a computational-tangible mixed-reality platform designed
for collaborative urban design processes. It portrays the evolution of the tool
and presents an overview of the history and limitations of notable CAD and TUI
platforms. As well, it depicts the development of a distributed networking
system between TUIs and CityScopeAR, as a key in design collaboration. It
shares the potential advantage of broad and decentralized community-engagement
process using such tools. Finally, this paper demonstrates several real-world
tests and deployments of CityScopeAR and proposes a path to future integration
of AR/MR devices in urban design and public participation.","['Ariel Noyman', 'Yasushi Sakai', 'Kent Larson']",2019-07-19T17:30:26Z,http://arxiv.org/abs/1907.08586v1,"['cs.HC', 'H.5']"
An Agent-Based Intelligent HCI Information System in Mixed Reality,"This paper presents a design of agent-based intelligent HCI (iHCI) system
using collaborative information for MR to improve user experience and
information security based on context-aware computing. In order to implement
target awareness system, we propose the use of non-parameter stochastic
adaptive learning and a kernel learning strategy for improving the adaptivity
of the recognition. The proposed design involves the use of a context-aware
computing strategy to recognize patterns for simulating human awareness and
processing of stereo pattern analysis. It provides a flexible customization
method for scene creation and manipulation. It also enables several types of
awareness related to the interactive target, user-experience, system
performance, confidentiality, and agent identification by applying several
strategies, such as context pattern analysis, scalable learning, data-aware
confidential computing.","['Hamed Alqahtani', 'Charles Z. Liu', 'Manolya Kavakli-Thorne', 'Yuzhi Kang']",2019-11-07T02:46:55Z,http://arxiv.org/abs/1911.02726v1,['cs.HC']
Improving pixel differentiation in holographic images,"Computer generated holography (CGH) has seen a resurgence in recent years
due, in part, to the rise of virtual and mixed reality systems. The majority of
approaches for CGH are based on a sampled Discrete Fourier Transform (DFT) and
ignore the interstitial behaviour between sampling points in the replay field.
In this paper we demonstrate that neighbouring replay field pixels can
interfere significantly giving the visual impression of pixel movement and
increased noise. We also demonstrate that increasing the separation between
target pixels reduces the interference and improves pixel quality. This
phenomena is demonstrated experimentally with close agreement between model and
measured result.
  This work begins by introducing the concept of pixel differentiation before
showing simulated models of pixel differentiation issues. Two mitigation
approaches are introduced and an experimental system is then used to validate
the simulation. Finally results are discussed and conclusions drawn.","['Peter J. Christopher', 'Ralf Mouthaan', 'John P. Freeman', 'Timothy D. Wilkinson']",2019-12-23T14:38:56Z,http://arxiv.org/abs/1912.12196v1,"['physics.optics', 'eess.IV']"
"Tangible Holograms: Towards Mobile Physical Augmentation of Virtual
  Objects","The last two decades have seen the emergence and steady development of
tangible user interfaces. While most of these interfaces are applied for input
- with output still on traditional computer screens - the goal of programmable
matter and actuated shape-changing materials is to directly use the physical
objects for visual or tangible feedback. Advances in material sciences and
flexible display technologies are investigated to enable such reconfigurable
physical objects. While existing solutions aim for making physical objects more
controllable via the digital world, we propose an approach where holograms
(virtual objects) in a mixed reality environment are augmented with physical
variables such as shape, texture or temperature. As such, the support for
mobility forms an important contribution of the proposed solution since it
enables users to freely move within and across environments. Furthermore, our
augmented virtual objects can co-exist in a single environment with
programmable matter and other actuated shape-changing solutions. The future
potential of the proposed approach is illustrated in two usage scenarios and we
hope that the presentation of our work in progress on a novel way to realise
tangible holograms will foster some lively discussions in the CHI community.","['Beat Signer', 'Timothy J. Curtin']",2017-03-24T05:31:56Z,http://arxiv.org/abs/1703.08288v1,['cs.HC']
"Google Cardboard Dates Augmented Reality : Issues, Challenges and Future
  Opportunities","The Google's frugal Cardboard solution for immersive Virtual Reality
experiences has come a long way in the VR market. The Google Cardboard VR
applications will support us in the fields such as education, virtual tourism,
entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has
introduced support for developing mixed reality applications for Google
Cardboard which can combine Virtual and Augmented Reality to develop exciting
and immersive experiences. In this work, we present a comprehensive review of
Google Cardboard for AR and also highlight its technical and subjective
limitations by conducting a feasibility study through the inspection of a
Desktop computer use-case. Additionally, we recommend the future avenues for
the Google Cardboard in AR. This work also serves as a guide for Android/iOS
developers as there are no published scholarly articles or well documented
studies exclusively on Google Cardboard with both user and developer's
experience captured at one place.","['Ramakrishna Perla', 'Ramya Hebbalaguppe']",2017-06-05T06:26:25Z,http://arxiv.org/abs/1706.03851v1,['cs.HC']
"MixedPeds: Pedestrian Detection in Unannotated Videos using
  Synthetically Generated Human-agents for Training","We present a new method for training pedestrian detectors on an unannotated
set of images. We produce a mixed reality dataset that is composed of
real-world background images and synthetically generated static human-agents.
Our approach is general, robust, and makes no other assumptions about the
unannotated dataset regarding the number or location of pedestrians. We
automatically extract from the dataset: i) the vanishing point to calibrate the
virtual camera, and ii) the pedestrians' scales to generate a Spawn Probability
Map, which is a novel concept that guides our algorithm to place the
pedestrians at appropriate locations. After putting synthetic human-agents in
the unannotated images, we use these augmented images to train a Pedestrian
Detector, with the annotations generated along with the synthetic agents. We
conducted our experiments using Faster R-CNN by comparing the detection results
on the unannotated dataset performed by the detector trained using our approach
and detectors trained with other manually labeled datasets. We showed that our
approach improves the average precision by 5-13% over these detectors.","['Ernest C. Cheung', 'Tsan Kwong Wong', 'Aniket Bera', 'Dinesh Manocha']",2017-07-28T04:05:33Z,http://arxiv.org/abs/1707.09100v2,['cs.CV']
"Specifying, Monitoring, and Executing Workflows in Linked Data
  Environments","We present an ontology for representing workflows over components with
Read-Write Linked Data interfaces and give an operational semantics to the
ontology via a rule language. Workflow languages have been successfully applied
for modelling behaviour in enterprise information systems, in which the data is
often managed in a relational database. Linked Data interfaces have been widely
deployed on the web to support data integration in very diverse domains,
increasingly also in scenarios involving the Internet of Things, in which
application behaviour is often specified using imperative programming
languages. With our work we aim to combine workflow languages, which allow for
the high-level specification of application behaviour by non-expert users, with
Linked Data, which allows for decentralised data publication and integrated
data access. We show that our ontology is expressive enough to cover the basic
workflow patterns and demonstrate the applicability of our approach with a
prototype system that observes pilots carrying out tasks in a mixed-reality
aircraft cockpit. On a synthetic benchmark from the building automation domain,
the runtime scales linearly with the size of the number of Internet of Things
devices.","['Tobias Käfer', 'Andreas Harth']",2018-04-13T17:22:17Z,http://arxiv.org/abs/1804.05044v4,"['cs.AI', 'cs.SE']"
"Varifocal zoom imaging with large area focal length adjustable
  metalenses","Varifocal lenses are essential components of dynamic optical systems with
applications in photography, mixed reality, and microscopy. Metasurface optics
has strong potential for creating tunable flat optics. Existing tunable
metalenses, however, typically require microelectromechanical actuators, which
cannot be scaled to large area devices, or rely on high voltages to stretch a
flexible substrate and achieve a sufficient tuning range. Here, we build a 1 cm
aperture varifocal metalens system at 1550 nm wavelength inspired by an Alvarez
lens, fabricated using high-throughput stepper photolithography. We demonstrate
a nonlinear change in focal length by minimally actuating two cubic phase
metasurfaces laterally, with focusing efficiency as high as 57% and a wide
focal length change of more than 6 cm (> 200%). We also test a lens design at
visible wavelength and conduct varifocal zoom imaging with a demonstrated 4x
zoom capability without any other optical elements in the imaging path.","['Shane Colburn', 'Alan Zhan', 'Arka Majumdar']",2018-05-20T22:46:28Z,http://arxiv.org/abs/1805.07832v1,['physics.optics']
Robust Point Light Source Estimation Using Differentiable Rendering,"Illumination estimation is often used in mixed reality to re-render a scene
from another point of view, to change the color/texture of an object, or to
insert a virtual object consistently lit into a real video or photograph.
Specifically, the estimation of a point light source is required for the
shadows cast by the inserted object to be consistent with the real scene. We
tackle the problem of illumination retrieval given an RGBD image of the scene
as an inverse problem: we aim to find the illumination that minimizes the
photometric error between the rendered image and the observation. In particular
we propose a novel differentiable renderer based on the Blinn-Phong model with
cast shadows. We compare our differentiable renderer to state-of-the-art
methods and demonstrate its robustness to an incorrect reflectance estimation.","['Grégoire Nieto', 'Salma Jiddi', 'Philippe Robert']",2018-12-12T09:05:11Z,http://arxiv.org/abs/1812.04857v1,['cs.CV']
"MVC-3D: Adaptive Design Pattern for Virtual and Augmented Reality
  Systems","In this paper, we present MVC-3D design pattern to develop virtual and
augmented (or mixed) reality interfaces that use new types of sensors,
modalities and implement specific algorithms and simulation models. The
proposed pattern represents the extension of classic MVC pattern by enriching
the View component (interactive View) and adding a specific component
(Library). The results obtained on the development of augmented reality
interfaces showed that the complexity of M, iV and C components is reduced. The
complexity increases only on the Library component (L). This helps the
programmers to well structure their models even if the interface complexity
increases. The proposed design pattern is also used in a design process called
MVC-3D in the loop that enables a seamless evolution from initial prototype to
the final system.","['Samir Benbelkacem', 'Djamel Aouam', 'Nadia Zenati-Henda', 'Abdelkader Bellarbi', 'Ahmed Bouhena', 'Samir Otmane']",2019-03-01T07:38:23Z,http://arxiv.org/abs/1903.00185v1,"['cs.HC', 'cs.SE']"
Privacy Preserving Image-Based Localization,"Image-based localization is a core component of many augmented/mixed reality
(AR/MR) and autonomous robotic systems. Current localization systems rely on
the persistent storage of 3D point clouds of the scene to enable camera pose
estimation, but such data reveals potentially sensitive scene information. This
gives rise to significant privacy risks, especially as for many applications 3D
mapping is a background process that the user might not be fully aware of. We
pose the following question: How can we avoid disclosing confidential
information about the captured 3D scene, and yet allow reliable camera pose
estimation? This paper proposes the first solution to what we call privacy
preserving image-based localization. The key idea of our approach is to lift
the map representation from a 3D point cloud to a 3D line cloud. This novel
representation obfuscates the underlying scene geometry while providing
sufficient geometric constraints to enable robust and accurate 6-DOF camera
pose estimation. Extensive experiments on several datasets and localization
scenarios underline the high practical relevance of our proposed approach.","['Pablo Speciale', 'Johannes L. Schönberger', 'Sing Bing Kang', 'Sudipta N. Sinha', 'Marc Pollefeys']",2019-03-13T16:12:04Z,http://arxiv.org/abs/1903.05572v1,['cs.CV']
"Understanding the Limitations of CNN-based Absolute Camera Pose
  Regression","Visual localization is the task of accurate camera pose estimation in a known
scene. It is a key problem in computer vision and robotics, with applications
including self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality.
Traditionally, the localization problem has been tackled using 3D geometry.
Recently, end-to-end approaches based on convolutional neural networks have
become popular. These methods learn to directly regress the camera pose from an
input image. However, they do not achieve the same level of pose accuracy as 3D
structure-based methods. To understand this behavior, we develop a theoretical
model for camera pose regression. We use our model to predict failure cases for
pose regression techniques and verify our predictions through experiments. We
furthermore use our model to show that pose regression is more closely related
to pose approximation via image retrieval than to accurate pose estimation via
3D structure. A key result is that current approaches do not consistently
outperform a handcrafted image retrieval baseline. This clearly shows that
additional research is needed before pose regression algorithms are ready to
compete with structure-based methods.","['Torsten Sattler', 'Qunjie Zhou', 'Marc Pollefeys', 'Laura Leal-Taixe']",2019-03-18T15:24:11Z,http://arxiv.org/abs/1903.07504v1,['cs.CV']
"Learning Convolutional Transforms for Lossy Point Cloud Geometry
  Compression","Efficient point cloud compression is fundamental to enable the deployment of
virtual and mixed reality applications, since the number of points to code can
range in the order of millions. In this paper, we present a novel data-driven
geometry compression method for static point clouds based on learned
convolutional transforms and uniform quantization. We perform joint
optimization of both rate and distortion using a trade-off parameter. In
addition, we cast the decoding process as a binary classification of the point
cloud occupancy map. Our method outperforms the MPEG reference solution in
terms of rate-distortion on the Microsoft Voxelized Upper Bodies dataset with
51.5% BDBR savings on average. Moreover, while octree-based methods face
exponential diminution of the number of points at low bitrates, our method
still produces high resolution outputs even at low bitrates. Code and
supplementary material are available at
https://github.com/mauriceqch/pcc_geo_cnn .","['Maurice Quach', 'Giuseppe Valenzise', 'Frederic Dufaux']",2019-03-20T15:14:15Z,http://arxiv.org/abs/1903.08548v2,"['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
Omnipotent Virtual Giant for Remote Human-Swarm Interaction,"This paper proposes an intuitive human-swarm interaction framework inspired
by our childhood memory in which we interacted with living ants by changing
their positions and environments as if we were omnipotent relative to the ants.
In virtual reality, analogously, we can be a super-powered virtual giant who
can supervise a swarm of mobile robots in a vast and remote environment by
flying over or resizing the world and coordinate them by picking and placing a
robot or creating virtual walls. This work implements this idea by using
Virtual Reality along with Leap Motion, which is then validated by
proof-of-concept experiments using real and virtual mobile robots in mixed
reality. We conduct a usability analysis to quantify the effectiveness of the
overall system as well as the individual interfaces proposed in this work. The
results revealed that the proposed method is intuitive and feasible for
interaction with swarm robots, but may require appropriate training for the new
end-user interface device.","['Inmo Jang', 'Junyan Hu', 'Farshad Arvin', 'Joaquin Carrasco', 'Barry Lennox']",2019-03-24T21:39:49Z,http://arxiv.org/abs/1903.10064v2,['cs.RO']
3D Virtual Garment Modeling from RGB Images,"We present a novel approach that constructs 3D virtual garment models from
photos. Unlike previous methods that require photos of a garment on a human
model or a mannequin, our approach can work with various states of the garment:
on a model, on a mannequin, or on a flat surface. To construct a complete 3D
virtual model, our approach only requires two images as input, one front view
and one back view. We first apply a multi-task learning network called JFNet
that jointly predicts fashion landmarks and parses a garment image into
semantic parts. The predicted landmarks are used for estimating sizing
information of the garment. Then, a template garment mesh is deformed based on
the sizing information to generate the final 3D model. The semantic parts are
utilized for extracting color textures from input images. The results of our
approach can be used in various Virtual Reality and Mixed Reality applications.","['Yi Xu', 'Shanglin Yang', 'Wei Sun', 'Li Tan', 'Kefeng Li', 'Hui Zhou']",2019-07-31T21:47:52Z,http://arxiv.org/abs/1908.00114v1,['cs.CV']
To Learn or Not to Learn: Visual Localization from Essential Matrices,"Visual localization is the problem of estimating a camera within a scene and
a key component in computer vision applications such as self-driving cars and
Mixed Reality. State-of-the-art approaches for accurate visual localization use
scene-specific representations, resulting in the overhead of constructing these
models when applying the techniques to new scenes. Recently, deep
learning-based approaches based on relative pose estimation have been proposed,
carrying the promise of easily adapting to new scenes. However, it has been
shown such approaches are currently significantly less accurate than
state-of-the-art approaches. In this paper, we are interested in analyzing this
behavior. To this end, we propose a novel framework for visual localization
from relative poses. Using a classical feature-based approach within this
framework, we show state-of-the-art performance. Replacing the classical
approach with learned alternatives at various levels, we then identify the
reasons for why deep learned approaches do not perform well. Based on our
analysis, we make recommendations for future work.","['Qunjie Zhou', 'Torsten Sattler', 'Marc Pollefeys', 'Laura Leal-Taixe']",2019-08-04T08:23:44Z,http://arxiv.org/abs/1908.01293v2,['cs.CV']
"Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in
  Human-Robot Interaction","In socially assistive robotics, an important research area is the development
of adaptation techniques and their effect on human-robot interaction. We
present a meta-learning based policy gradient method for addressing the problem
of adaptation in human-robot interaction and also investigate its role as a
mechanism for trust modelling. By building an escape room scenario in mixed
reality with a robot, we test our hypothesis that bi-directional trust can be
influenced by different adaptation algorithms. We found that our proposed model
increased the perceived trustworthiness of the robot and influenced the
dynamics of gaining human's trust. Additionally, participants evaluated that
the robot perceived them as more trustworthy during the interactions with the
meta-learning based adaptation compared to the previously studied statistical
adaptation model.","['Yuan Gao', 'Elena Sibirtseva', 'Ginevra Castellano', 'Danica Kragic']",2019-08-12T11:06:28Z,http://arxiv.org/abs/1908.04087v1,"['cs.RO', 'cs.HC', 'cs.LG']"
"EyeNet: A Multi-Task Network for Off-Axis Eye Gaze Estimation and User
  Understanding","Eye gaze estimation and simultaneous semantic understanding of a user through
eye images is a crucial component in Virtual and Mixed Reality; enabling energy
efficient rendering, multi-focal displays and effective interaction with 3D
content. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid
blocking the user's gaze, this view-point makes drawing eye related inferences
very challenging. In this work, we present EyeNet, the first single deep neural
network which solves multiple heterogeneous tasks related to eye gaze
estimation and semantic user understanding for an off-axis camera setting. The
tasks include eye segmentation, blink detection, emotive expression
classification, IR LED glints detection, pupil and cornea center estimation. To
train EyeNet end-to-end we employ both hand labelled supervision and model
based supervision. We benchmark all tasks on MagicEyes, a large and new dataset
of 587 subjects with varying morphology, gender, skin-color, make-up and
imaging conditions.","['Zhengyang Wu', 'Srivignesh Rajendran', 'Tarrence van As', 'Joelle Zimmermann', 'Vijay Badrinarayanan', 'Andrew Rabinovich']",2019-08-24T00:47:39Z,http://arxiv.org/abs/1908.09060v1,"['cs.CV', 'cs.LG', 'eess.IV']"
"Spatial Data Science: Closing the human-spatial computing-environment
  loop","Over the last decade, the term spatial computing has grown to have two
different, though not entirely unrelated, definitions. The first definition of
spatial computing stems from industry, where it refers primarily to new kinds
of augmented, virtual, mixed-reality, and natural user interface technologies.
A second definition coming out of academia takes a broader perspective that
includes active research in geographic information science as well as the
aforementioned novel UI technologies. Both senses reflect an ongoing shift
toward increased interaction with computing interfaces and sensors embedded in
the environment and how the use of these technologies influence how we behave
and make sense of and even change the world we live in. Regardless of the
definition, research in spatial computing is humming along nicely without the
need to identify new research agendas or new labels for communities of
researchers. However, as a field of research, it could be helpful to view
spatial data science as the glue that coheres spatial computing with
problem-solving and learning in the real world into a more holistic discipline.",['Benjamin Adams'],2019-10-15T02:19:19Z,http://arxiv.org/abs/1910.06484v1,"['cs.SI', 'cs.CY']"
C-D Ratio in multi-display environments,"Research in user interaction with mixed reality environments using multiple
displays has become increasingly relevant with the prevalence of mobile devices
in everyday life and increased commoditization of large display area
technologies using projectors or large displays. Previous work often combines
touch-based input with other approaches, such as gesture-based input, to expand
the possible interaction space or deal with limitations of other
two-dimensional input methods. In contrast to previous methods, we examine the
possibilities when the control-display (C-D) ratio is significantly smaller
than one and small input movements result in large output movements. To this
end one specific multi-display configuration is implemented in the form of a
spatial-augmented reality sandbox environment, and used to explore various
interaction techniques based on a variety of mobile device touch-based input
and optical marker tracking-based finger input. A small pilot study determines
the most promising input candidate, which is compared to traditional
touch-input based techniques in a user study that tests it for practical
relevance. Results and conclusions of the study are presented.","['Travis Gesslein', 'Jens Grubert']",2020-02-12T13:37:24Z,http://arxiv.org/abs/2002.04980v1,['cs.HC']
"A New Exocentric Metaphor for Complex Path Following to Control a UAV
  Using Mixed Reality","Teleoperation of Unmanned Aerial Vehicles (UAVs) has recently become an
noteworthly research topic in the field of human robot interaction. Each year,
a variety of devices is being studied to design adapted interface for diverse
purpose such as view taking, search and rescue operation or suveillance. New
interfaces have to be precise, simple and intuitive even for complex path
planning. Moreover, when teleoperation involves long distance control, user
needs to get proper feedbacks and avoid motion sickness. In order to overcome
all these challenges, a new interaction metaphor named DrEAM (Drone Exocentric
Advanced Metaphor) was designed. User can see the UAV he is controlling in a
virtual environment mapped to the real world. He can interact with it as a
simple object in a classical virtual world. An experiment was lead in order to
evaluate the perfomances of this metaphor, comparing performance of novice user
using either a direct-view joystick control or using DrEAM.","['Baptiste Wojtkowski', 'Pedro Castillo', 'Indira Thouvenin']",2020-02-13T11:02:33Z,http://arxiv.org/abs/2002.05721v1,"['cs.HC', 'cs.GR', 'cs.RO']"
Egocentric Human Segmentation for Mixed Reality,"The objective of this work is to segment human body parts from egocentric
video using semantic segmentation networks. Our contribution is two-fold: i) we
create a semi-synthetic dataset composed of more than 15, 000 realistic images
and associated pixel-wise labels of egocentric human body parts, such as arms
or legs including different demographic factors; ii) building upon the
ThunderNet architecture, we implement a deep learning semantic segmentation
algorithm that is able to perform beyond real-time requirements (16 ms for 720
x 720 images). It is believed that this method will enhance sense of presence
of Virtual Environments and will constitute a more realistic solution to the
standard virtual avatars.","['Andrija Gajic', 'Ester Gonzalez-Sosa', 'Diego Gonzalez-Morin', 'Marcos Escudero-Viñolo', 'Alvaro Villegas']",2020-05-25T12:34:47Z,http://arxiv.org/abs/2005.12074v2,['cs.CV']
Improved Deep Point Cloud Geometry Compression,"Point clouds have been recognized as a crucial data structure for 3D content
and are essential in a number of applications such as virtual and mixed
reality, autonomous driving, cultural heritage, etc. In this paper, we propose
a set of contributions to improve deep point cloud compression, i.e.: using a
scale hyperprior model for entropy coding; employing deeper transforms; a
different balancing weight in the focal loss; optimal thresholding for
decoding; and sequential model training. In addition, we present an extensive
ablation study on the impact of each of these factors, in order to provide a
better understanding about why they improve RD performance. An optimal
combination of the proposed improvements achieves BD-PSNR gains over G-PCC
trisoup and octree of 5.50 (6.48) dB and 6.84 (5.95) dB, respectively, when
using the point-to-point (point-to-plane) metric. Code is available at
https://github.com/mauriceqch/pcc_geo_cnn_v2 .","['Maurice Quach', 'Giuseppe Valenzise', 'Frederic Dufaux']",2020-06-16T10:03:14Z,http://arxiv.org/abs/2006.09043v2,"['cs.CV', 'cs.LG', 'eess.IV', 'eess.SP', 'stat.ML']"
Structure and Design of HoloGen,"Increasing popularity of augmented and mixed reality systems has seen a
similar increase of interest in 2D and 3D computer generated holography (CGH).
Unlike stereoscopic approaches, CGH can fully represent a light field including
depth of focus, accommodation and vergence. Along with existing
telecommunications, imaging, projection, lithography, beam shaping and optical
tweezing applications, CGH is an exciting technique applicable to a wide array
of photonic problems including full 3D representation. Traditionally, the
primary roadblock to acceptance has been the significant numerical processing
required to generate holograms requiring both significant expertise and
significant computational power. This article discusses the structure and
design of HoloGen. HoloGen is an MIT licensed application that may be used to
generate holograms using a wide array of algorithms without expert guidance.
HoloGen uses a Cuda C and C++ backend with a C# and Windows Presentation
Framework graphical user interface. The article begins by introducing HoloGen
before providing an in-depth discussion of its design and structure. Particular
focus is given to the communication, data transfer and algorithmic aspects.","['Peter J. Christopher', 'Timothy D. Wilkinson']",2020-06-18T13:29:46Z,http://arxiv.org/abs/2006.10509v1,"['cs.GR', 'cs.CV']"
An Advert Creation System for 3D Product Placements,"Over the past decade, the evolution of video-sharing platforms has attracted
a significant amount of investments on contextual advertising. The common
contextual advertising platforms utilize the information provided by users to
integrate 2D visual ads into videos. The existing platforms face many technical
challenges such as ad integration with respect to occluding objects and 3D ad
placement. This paper presents a Video Advertisement Placement & Integration
(Adverts) framework, which is capable of perceiving the 3D geometry of the
scene and camera motion to blend 3D virtual objects in videos and create the
illusion of reality. The proposed framework contains several modules such as
monocular depth estimation, object segmentation, background-foreground
separation, alpha matting and camera tracking. Our experiments conducted using
Adverts framework indicates the significant potential of this system in
contextual ad integration, and pushing the limits of advertising industry using
mixed reality technologies.","['Ivan Bacher', 'Hossein Javidnia', 'Soumyabrata Dev', 'Rahul Agrahari', 'Murhaf Hossari', 'Matthew Nicholson', 'Clare Conran', 'Jian Tang', 'Peng Song', 'David Corrigan', 'François Pitié']",2020-06-26T17:41:50Z,http://arxiv.org/abs/2006.15131v1,"['cs.CV', 'cs.MM']"
The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization,"Realtime perceptual and interaction capabilities in mixed reality require a
range of 3D tracking problems to be solved at low latency on
resource-constrained hardware such as head-mounted devices. Indeed, for devices
such as HoloLens 2 where the CPU and GPU are left available for applications,
multiple tracking subsystems are required to run on a continuous, real-time
basis while sharing a single Digital Signal Processor. To solve model-fitting
problems for HoloLens 2 hand tracking, where the computational budget is
approximately 100 times smaller than an iPhone 7, we introduce a new surface
model: the `Phong surface'. Using ideas from computer graphics, the Phong
surface describes the same 3D shape as a triangulated mesh model, but with
continuous surface normals which enable the use of lifting-based optimization,
providing significant efficiency gains over ICP-based methods. We show that
Phong surfaces retain the convergence benefits of smoother surface models,
while triangle meshes do not.","['Jingjing Shen', 'Thomas J. Cashman', 'Qi Ye', 'Tim Hutton', 'Toby Sharp', 'Federica Bogo', 'Andrew William Fitzgibbon', 'Jamie Shotton']",2020-07-09T17:10:11Z,http://arxiv.org/abs/2007.04940v1,['cs.CV']
"XR-Ed Framework: Designing Instruction-driven andLearner-centered
  Extended Reality Systems for Education","Recently, the HCI community has seen an increased interest in applying
Virtual Reality (VR), AugmentedReality (AR) and Mixed Reality (MR) into
educational settings. Despite many literature reviews, there stilllacks a clear
framework that reveals the different design dimensions in educational Extended
Reality (XR)systems. Addressing this gap, we synthesize a broad range of
educational XR to propose the XR-Ed framework,which reveals design space in six
dimensions (Physical Accessibility, Scenario, Social Interactivity,
Agency,Virtuality Degree, Assessment). Within each dimension, we contextualize
the framework using existing designcases. Based on the XR-Ed Design framework,
we incorporated instructional design approaches to proposeXR-Ins, an
instruction-oriented, step-by-step guideline in educational XR instruction
design. Jointly, they aimto support practitioners by revealing implicit design
choices, offering design inspirations as well as guide themto design
instructional activities for XR technologies in a more instruction-oriented and
learner-centered way.","['Kexin Yang', 'Xiaofei Zhou', 'Iulian Radu']",2020-10-24T03:18:05Z,http://arxiv.org/abs/2010.13779v1,['cs.HC']
Efficient Scene Compression for Visual-based Localization,"Estimating the pose of a camera with respect to a 3D reconstruction or scene
representation is a crucial step for many mixed reality and robotics
applications. Given the vast amount of available data nowadays, many
applications constrain storage and/or bandwidth to work efficiently. To satisfy
these constraints, many applications compress a scene representation by
reducing its number of 3D points. While state-of-the-art methods use
$K$-cover-based algorithms to compress a scene, they are slow and hard to tune.
To enhance speed and facilitate parameter tuning, this work introduces a novel
approach that compresses a scene representation by means of a constrained
quadratic program (QP). Because this QP resembles a one-class support vector
machine, we derive a variant of the sequential minimal optimization to solve
it. Our approach uses the points corresponding to the support vectors as the
subset of points to represent a scene. We also present an efficient
initialization method that allows our method to converge quickly. Our
experiments on publicly available datasets show that our approach compresses a
scene representation quickly while delivering accurate pose estimates.","['Marcela Mera-Trujillo', 'Benjamin Smith', 'Victor Fragoso']",2020-11-27T18:36:06Z,http://arxiv.org/abs/2011.13894v1,['cs.CV']
"Sonic Sculpture: Activating Engagement with Head-Mounted Augmented
  Reality","This work examines how head-mounted AR can be used to build an interactive
sonic landscape to engage with a public sculpture. We describe a sonic artwork,
""Listening To Listening"", that has been designed to accompany a real-world
sculpture with two prototype interaction schemes. Our artwork is created for
the HoloLens platform so that users can have an individual experience in a
mixed reality context. Personal head-mounted AR systems have recently become
available and practical for integration into public art projects, however
research into sonic sculpture works has yet to account for the affordances of
current portable and mainstream AR systems. In this work, we take advantage of
the HoloLens' spatial awareness to build sonic spaces that have a precise
spatial relationship to a given sculpture and where the sculpture itself is
modelled in the augmented scene as an ""invisible hologram"". We describe the
artistic rationale for our artwork, the design of the two interaction schemes,
and the technical and usability feedback that we have obtained from
demonstrations during iterative development.","['Charles Patrick Martin', 'Zeruo Liu', 'Yichen Wang', 'Wennan He', 'Henry Gardner']",2020-12-03T22:35:49Z,http://arxiv.org/abs/2012.02311v1,"['cs.HC', 'cs.SD', 'eess.AS', 'H.5.5; H.5.1']"
"No Shadow Left Behind: Removing Objects and their Shadows using
  Approximate Lighting and Geometry","Removing objects from images is a challenging problem that is important for
many applications, including mixed reality. For believable results, the shadows
that the object casts should also be removed. Current inpainting-based methods
only remove the object itself, leaving shadows behind, or at best require
specifying shadow regions to inpaint. We introduce a deep learning pipeline for
removing a shadow along with its caster. We leverage rough scene models in
order to remove a wide variety of shadows (hard or soft, dark or subtle, large
or thin) from surfaces with a wide variety of textures. We train our pipeline
on synthetically rendered data, and show qualitative and quantitative results
on both synthetic and real scenes.","['Edward Zhang', 'Ricardo Martin-Brualla', 'Janne Kontkanen', 'Brian Curless']",2020-12-19T01:05:40Z,http://arxiv.org/abs/2012.10565v1,"['cs.CV', 'cs.GR']"
Analysing ocular parameters for web browsing and graph visualization,"This paper proposes a set of techniques to investigate eye gaze and fixation
patterns while users interact with electronic user interfaces. In particular,
two case studies are presented - one on analysing eye gaze while interacting
with deceptive materials in web pages and another on analysing graphs in
standard computer monitor and virtual reality displays. We analysed spatial and
temporal distributions of eye gaze fixations and sequence of eye gaze
movements. We used this information to propose new design guidelines to avoid
deceptive materials in web and user-friendly representation of data in 2D
graphs. In 2D graph study we identified that area graph has lowest number of
clusters for user's gaze fixations and lowest average response time. The
results of 2D graph study were implemented in virtual and mixed reality
environment. Along with this, it was ob-served that the duration while
interacting with deceptive materials in web pages is independent of the number
of fixations. Furthermore, web-based data visualization tool for analysing eye
tracking data from single and multiple users was developed.","['Somnath Arjun', 'KamalPreet Singh Saluja', 'Pradipta Biswas']",2021-01-04T06:20:02Z,http://arxiv.org/abs/2101.00794v1,"['cs.HC', 'D.2.2; H.1.2; I.3.6']"
Discussing the Risks of Adaptive Virtual Environments for User Autonomy,"Adaptive virtual environments are an opportunity to support users and
increase their flow, presence, immersion, and overall experience. Possible
fields of application are adaptive individual education, gameplay adjustment,
professional work, and personalized content. But who benefits more from this
adaptivity, the users who can enjoy a greater user experience or the companies
or governments who are completely in control of the provided content. While the
user autonomy decreases for individuals, the power of institutions raises, and
the risk exists that personal opinions are precisely controlled. In this
position paper, we will argue that researchers should not only propose the
benefits of their work but also critically discuss what are possible abusive
use cases. Therefore, we will examine two use cases in the fields of
professional work and personalized content and show possible abusive use.","['Tobias Drey', 'Enrico Rukzio']",2021-01-07T14:55:09Z,http://arxiv.org/abs/2101.02576v1,['cs.HC']
"Color Contrast Enhanced Rendering for Optical See-through Head-mounted
  Displays","Most commercially available optical see-through head-mounted displays
(OST-HMDs) utilize optical combiners to simultaneously visualize the physical
background and virtual objects. The displayed images perceived by users are a
blend of rendered pixels and background colors. Enabling high fidelity color
perception in mixed reality (MR) scenarios using OST-HMDs is an important but
challenging task. We propose a real-time rendering scheme to enhance the color
contrast between virtual objects and the surrounding background for OST-HMDs.
Inspired by the discovery of color perception in psychophysics, we first
formulate the color contrast enhancement as a constrained optimization problem.
We then design an end-to-end algorithm to search the optimal complementary
shift in both chromaticity and luminance of the displayed color. This aims at
enhancing the contrast between virtual objects and the real background as well
as keeping the consistency with the original color. We assess the performance
of our approach using a simulated OST-HMD environment and an off-the-shelf
OST-HMD. Experimental results from objective evaluations and subjective user
studies demonstrate that the proposed approach makes rendered virtual objects
more distinguishable from the surrounding background, thereby bringing a better
visual experience.","['Yunjin Zhang', 'Rui Wang', 'Yifan', 'Peng', 'Wei Hua', 'Hujun Bao']",2021-01-08T04:42:39Z,http://arxiv.org/abs/2101.02847v1,['cs.GR']
Adaptive Accessible AR/VR Systems,"Augmented, virtual and mixed reality technologies offer new ways of
interacting with digital media. However, such technologies are not well
explored for people with different ranges of abilities beyond a few specific
navigation and gaming applications. While new standardization activities are
investigating accessibility issues with existing AR/VR systems, commercial
systems are still confined to specialized hardware and software limiting their
widespread adoption among people with disabilities as well as seniors. This
proposal takes a novel approach by exploring the application of user
model-based personalization for AR/VR systems to improve accessibility. The
workshop will be organized by experienced researchers in the field of human
computer interaction, robotics control, assistive technology, and AR/VR
systems, and will consist of peer reviewed papers and hands-on demonstrations.
Keynote speeches and demonstrations will cover latest accessibility research at
Microsoft, Google, Verizon and leading universities.","['Pradipta Biswas', 'Pilar Orero', 'Manohar Swaminathan', 'Kavita Krishnaswamy', 'Peter Robinson']",2021-01-08T10:01:21Z,http://arxiv.org/abs/2101.02936v1,"['cs.HC', 'D.2.2; H.1.2; I.3.6']"
"Streaming VR Games to the Broad Audience: A Comparison of the
  First-Person and Third-Person Perspectives","The spectatorship experience for virtual reality (VR) games differs strongly
from its non-VR precursor. When watching non-VR games on platforms such as
Twitch, spectators just see what the player sees, as the physical interaction
is mostly unimportant for the overall impression. In VR, the immersive
full-body interaction is a crucial part of the player experience. Hence,
content creators, such as streamers, often rely on green screens or similar
solutions to offer a mixed-reality third-person view to disclose their
full-body actions. Our work compares the most popular realizations of the
first-person and the third-person perspective in an online survey (N=217) with
three different VR games. Contrary to the current trend to stream in
third-person, our key result is that most viewers prefer the first-person
version, which they attribute mostly to the better focus on in-game actions and
higher involvement. Based on the study insights, we provide design
recommendations for both perspectives.","['Katharina Emmerich', 'Andrey Krekhov', 'Sebastian Cmentowski', 'Jens Krueger']",2021-01-12T12:46:04Z,http://arxiv.org/abs/2101.04449v1,['cs.HC']
HAIR: Head-mounted AR Intention Recognition,"Human teams exhibit both implicit and explicit intention sharing. To further
development of human-robot collaboration, intention recognition is crucial on
both sides. Present approaches rely on a vast sensor suite on and around the
robot to achieve intention recognition. This relegates intuitive human-robot
collaboration purely to such bulky systems, which are inadequate for
large-scale, real-world scenarios due to their complexity and cost. In this
paper we propose an intention recognition system that is based purely on a
portable head-mounted display. In addition robot intention visualisation is
also supported. We present experiments to show the quality of our human goal
estimation component and some basic interactions with an industrial robot. HAIR
should raise the quality of interaction between robots and humans, instead of
such interactions raising the hair on the necks of the human coworkers.","['David Puljiz', 'Bowen Zhou', 'Ke Ma', 'Björn Hein']",2021-02-22T16:38:22Z,http://arxiv.org/abs/2102.11162v1,"['cs.RO', 'cs.HC']"
"Text-driven object affordance for guiding grasp-type recognition in
  multimodal robot teaching","This study investigates how text-driven object affordance, which provides
prior knowledge about grasp types for each object, affects image-based
grasp-type recognition in robot teaching. The researchers created labeled
datasets of first-person hand images to examine the impact of object affordance
on recognition performance. They evaluated scenarios with real and illusory
objects, considering mixed reality teaching conditions where visual object
information may be limited. The results demonstrate that object affordance
improves image-based recognition by filtering out unlikely grasp types and
emphasizing likely ones. The effectiveness of object affordance was more
pronounced when there was a stronger bias towards specific grasp types for each
object. These findings highlight the significance of object affordance in
multimodal robot teaching, regardless of whether real objects are present in
the images. Sample code is available on
https://github.com/microsoft/arr-grasp-type-recognition.","['Naoki Wake', 'Daichi Saito', 'Kazuhiro Sasabuchi', 'Hideki Koike', 'Katsushi Ikeuchi']",2021-02-27T17:03:32Z,http://arxiv.org/abs/2103.00268v2,"['cs.RO', 'cs.CV', 'cs.HC']"
A Full Body Avatar-Based Telepresence System for Dissimilar Spaces,"We present a novel mixed reality (MR) telepresence system enabling a local
user to interact with a remote user through full-body avatars in their own
rooms. If the remote rooms have different sizes and furniture arrangements,
directly applying a user's motion to an avatar leads to a mismatch of placement
and deictic gesture. To overcome this problem, we retarget the placement, arm
gesture, and head movement of a local user to an avatar in a remote room to
preserve a local user's environment and interaction context. This allows
avatars to utilize real furniture and interact with a local user and shared
objects as if they were in the same room. This paper describes our system's
design and implementation in detail and a set of example scenarios in the
living room and office room. A qualitative user study delves into a user
experience, challenges, and possible extensions of the proposed system.","['Leonard Yoon', 'Dongseok Yang', 'Choongho Chung', 'Sung-Hee Lee']",2021-03-07T15:43:57Z,http://arxiv.org/abs/2103.04380v1,['cs.HC']
FastNeRF: High-Fidelity Neural Rendering at 200FPS,"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
be used to encode complex 3D environments that can be rendered
photorealistically from novel viewpoints. Rendering these images is very
computationally demanding and recent improvements are still a long way from
enabling interactive rates, even on high-end hardware. Motivated by scenarios
on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
system capable of rendering high fidelity photorealistic images at 200Hz on a
high-end consumer GPU. The core of our method is a graphics-inspired
factorization that allows for (i) compactly caching a deep radiance map at each
position in space, (ii) efficiently querying that map using ray directions to
estimate the pixel values in the rendered image. Extensive experiments show
that the proposed method is 3000 times faster than the original NeRF algorithm
and at least an order of magnitude faster than existing work on accelerating
NeRF, while maintaining visual quality and extensibility.","['Stephan J. Garbin', 'Marek Kowalski', 'Matthew Johnson', 'Jamie Shotton', 'Julien Valentin']",2021-03-18T17:09:12Z,http://arxiv.org/abs/2103.10380v2,['cs.CV']
"Congruence and Plausibility, not Presence?! Pivotal Conditions for XR
  Experiences and Effects, a Novel Model","Presence often is considered the most important quale describing the
subjective feeling of being in a computer-generated and/or computer-mediated
virtual environment. The identification and separation of orthogonal presence
components, i.e., the place illusion and the plausibility illusion, has been an
accepted theoretical model describing Virtual Reality (VR) experiences for some
time. This perspective article challenges this presence-oriented VR theory.
First, we argue that a place illusion cannot be the major construct to describe
the much wider scope of Virtual, Augmented, and Mixed Reality (VR, AR, MR: or
XR for short). Second, we argue that there is no plausibility illusion but
merely plausibility, and we derive the place illusion caused by congruent and
plausible generation of spatial cues, and similarly for all the current model's
so-defined illusions. Finally, we propose congruence and plausibility to become
the central essential conditions in a novel theoretical model describing XR
experiences and effects.","['Marc Erich Latoschik', 'Carolin Wienrich']",2021-04-10T19:25:17Z,http://arxiv.org/abs/2104.04846v5,"['cs.HC', 'H.5.1']"
Implementing Virtual Reality for Teleoperation of a Humanoid Robot,"Our research explores the potential of a humanoid robot for work in
unpredictable environments, but controlling a humanoid robot remains a very
difficult problem. In our previous work, we designed a prototype virtual
reality (VR) interface to allow an operator to command a humanoid robot.
However, while usable, the initial interface was not sufficient for commanding
the robot to perform the tasks; for example, in some cases, there was a lack of
precision available for robot control. The interface was overly cumbersome in
some areas as well. In this paper, we discuss numerous additions, inspired by
traditional interfaces and virtual reality video games, to our prior
implementation, providing additional ways to visualize and command a humanoid
robot to perform difficult tasks within a virtual world.","['Jordan Allspaw', 'Gregory LeMasurier', 'Holly Yanco']",2021-04-23T21:44:25Z,http://arxiv.org/abs/2104.11826v1,"['cs.RO', 'I.2.9']"
Semi-Autonomous Planning and Visualization in Virtual Reality,"Virtual reality (VR) interfaces for robots provide a three-dimensional (3D)
view of the robot in its environment, which allows people to better plan
complex robot movements in tight or cluttered spaces. In our prior work, we
created a VR interface to allow for the teleoperation of a humanoid robot. As
detailed in this paper, we have now focused on a human-in-the-loop planner
where the operator can send higher level manipulation and navigation goals in
VR through functional waypoints, visualize the results of a robot planner in
the 3D virtual space, and then deny, alter or confirm the plan to send to the
robot. In addition, we have adapted our interface to also work for a mobile
manipulation robot in addition to the humanoid robot. For a video demonstration
please see the accompanying video at https://youtu.be/wEHZug_fxrA.","['Gregory LeMasurier', 'Jordan Allspaw', 'Holly A. Yanco']",2021-04-23T21:48:05Z,http://arxiv.org/abs/2104.11827v1,"['cs.RO', 'I.2.9']"
"Procedural animations in interactive art experiences -- A state of the
  art review","The state of the art review broadly oversees the use of novel research
utilized in the creation of virtual environments applied in interactive art
experiences, with a specific focus on the application of procedural animation
in spatially augmented reality (SAR) exhibitions. These art exhibitions
frequently combine sensory displays that appeal, replace, and augment the
visual, auditory and touch or haptic senses. We analyze and break down
art-technology related innovations in the last three years, and thoroughly
identify the most recent and vibrant applications of interactive art
experiences in the review of numerous installation applications, studies, and
events. Display mediums such as virtual reality, augmented reality, mixed
reality, and robotics are overviewed in the context of art experiences such as
visual art museums, park or historic site tours, live concerts, and theatre. We
explore research and extrapolate how recent innovations can lead to different
applications that will be seen in the future.",['C. Tollola'],2021-05-16T05:14:56Z,http://arxiv.org/abs/2105.09153v1,"['cs.HC', 'cs.MM']"
"CLEDGE: A Hybrid Cloud-Edge Computing Framework over Information Centric
  Networking","In today's era of Internet of Things (IoT), where massive amounts of data are
produced by IoT and other devices, edge computing has emerged as a prominent
paradigm for low-latency data processing. However, applications may have
diverse latency requirements: certain latency-sensitive processing operations
may need to be performed at the edge, while delay-tolerant operations can be
performed on the cloud, without occupying the potentially limited edge
computing resources. To achieve that, we envision an environment where
computing resources are distributed across edge and cloud offerings. In this
paper, we present the design of CLEDGE (CLoud + EDGE), an information-centric
hybrid cloud-edge framework, aiming to maximize the on-time completion of
computational tasks offloaded by applications with diverse latency
requirements. The design of CLEDGE is motivated by the networking challenges
that mixed reality researchers face. Our evaluation demonstrates that CLEDGE
can complete on-time more than 90% of offloaded tasks with modest overheads.","['Md Washik Al Azad', 'Susmit Shannigrahi', 'Nicholas Stergiou', 'Francisco R. Ortega', 'Spyridon Mastorakis']",2021-07-15T20:52:37Z,http://arxiv.org/abs/2107.07604v1,['cs.NI']
"Physics-informed neural networks for one-dimensional sound field
  predictions with parameterized sources and impedance boundaries","Realistic sound is essential in virtual environments, such as computer games
and mixed reality. Efficient and accurate numerical methods for pre-calculating
acoustics have been developed over the last decade; however, pre-calculating
acoustics makes handling dynamic scenes with moving sources challenging,
requiring intractable memory storage. A physics-informed neural network (PINN)
method in 1D is presented, which learns a compact and efficient surrogate model
with parameterized moving Gaussian sources and impedance boundaries, and
satisfies a system of coupled equations. The model shows relative mean errors
below 2%/0.2 dB and proposes a first step in developing PINNs for realistic 3D
scenes.","['Nikolas Borrel-Jensen', 'Allan P. Engsig-Karup', 'Cheol-Ho Jeong']",2021-09-23T11:59:26Z,http://arxiv.org/abs/2109.11313v5,"['cs.SD', 'eess.AS', 'physics.comp-ph']"
"CAESynth: Real-Time Timbre Interpolation and Pitch Control with
  Conditional Autoencoders","In this paper, we present a novel audio synthesizer, CAESynth, based on a
conditional autoencoder. CAESynth synthesizes timbre in real-time by
interpolating the reference sounds in their shared latent feature space, while
controlling a pitch independently. We show that training a conditional
autoencoder based on accuracy in timbre classification together with
adversarial regularization of pitch content allows timbre distribution in
latent space to be more effective and stable for timbre interpolation and pitch
conditioning. The proposed method is applicable not only to creation of musical
cues but also to exploration of audio affordance in mixed reality based on
novel timbre mixtures with environmental sounds. We demonstrate by experiments
that CAESynth achieves smooth and high-fidelity audio synthesis in real-time
through timbre interpolation and independent yet accurate pitch control for
musical cues as well as for audio affordance with environmental sound. A Python
implementation along with some generated samples are shared online.","['Aaron Valero Puche', 'Sukhan Lee']",2021-11-09T14:36:31Z,http://arxiv.org/abs/2111.05174v1,"['cs.SD', 'cs.LG', 'eess.AS']"
Ditto: Building Digital Twins of Articulated Objects from Interaction,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto","['Zhenyu Jiang', 'Cheng-Chun Hsu', 'Yuke Zhu']",2022-02-16T18:12:14Z,http://arxiv.org/abs/2202.08227v3,"['cs.CV', 'cs.AI', 'cs.RO']"
"Dynamic Object Comprehension: A Framework For Evaluating Artificial
  Visual Perception","Augmented and Mixed Reality are emerging as likely successors to the mobile
internet. However, many technical challenges remain. One of the key
requirements of these systems is the ability to create a continuity between
physical and virtual worlds, with the user's visual perception as the primary
interface medium. Building this continuity requires the system to develop a
visual understanding of the physical world. While there has been significant
recent progress in computer vision and AI techniques such as image
classification and object detection, success in these areas has not yet led to
the visual perception required for these critical MR and AR applications. A
significant issue is that current evaluation criteria are insufficient for
these applications. To motivate and evaluate progress in this emerging area,
there is a need for new metrics. In this paper we outline limitations of
current evaluation criteria and propose new criteria.","['Scott Y. L. Chin', 'Bradley R. Quinton']",2022-02-17T07:49:49Z,http://arxiv.org/abs/2202.08490v1,"['cs.CV', 'cs.AI', 'cs.LG']"
"Virtual Reality Digital Twin and Environment for Troubleshooting
  Lunar-based Infrastructure Assembly Failures","Humans and robots will need to collaborate in order to create a sustainable
human lunar presence by the end of the 2020s. This includes cases in which a
human will be required to teleoperate an autonomous rover that has encountered
an instrument assembly failure. To aid teleoperators in the troubleshooting
process, we propose a virtual reality digital twin placed in a simulated
environment. Here, the operator can virtually interact with a digital version
of the rover and mechanical arm that uses the same controls and kinematic
model. The user can also adopt the egocentric (a first person view through
using stereoscopic passthrough) and exocentric (a third person view where the
operator can virtually walk around the environment and rover as if they were on
site) view. We also discuss our metrics for evaluating the differences between
our digital and physical robot, as well as the experimental concept based on
real and applicable missions, and future work that would compare our platform
to traditional troubleshooting methods.","['Phaedra S. Curlin', 'Madaline A. Muniz', 'Mason M. Bell', 'Alexis A. Muniz', 'Jack O. Burns']",2022-03-05T19:36:16Z,http://arxiv.org/abs/2203.02810v1,['cs.RO']
"Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced
  Human-Robot Interaction and Robotic Interfaces","This paper contributes to a taxonomy of augmented reality and robotics based
on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have
emerged as a new way to enhance human-robot interaction (HRI) and robotic
interfaces (e.g., actuated and shape-changing interfaces). Recently, an
increasing number of studies in HCI, HRI, and robotics have demonstrated how AR
enables better interactions between people and robots. However, often research
remains focused on individual explorations and key design strategies, and
research questions are rarely analyzed systematically. In this paper, we
synthesize and categorize this research field in the following dimensions: 1)
approaches to augmenting reality; 2) characteristics of robots; 3) purposes and
benefits; 4) classification of presented information; 5) design components and
strategies for visual augmentation; 6) interaction techniques and modalities;
7) application domains; and 8) evaluation strategies. We formulate key
challenges and opportunities to guide and inform future research in AR and
robotics.","['Ryo Suzuki', 'Adnan Karim', 'Tian Xia', 'Hooman Hedayati', 'Nicolai Marquardt']",2022-03-07T10:22:59Z,http://arxiv.org/abs/2203.03254v1,"['cs.RO', 'cs.CV', 'cs.HC']"
FLAG: Flow-based 3D Avatar Generation from Sparse Observations,"To represent people in mixed reality applications for collaboration and
communication, we need to generate realistic and faithful avatar poses.
However, the signal streams that can be applied for this task from head-mounted
devices (HMDs) are typically limited to head pose and hand pose estimates.
While these signals are valuable, they are an incomplete representation of the
human body, making it challenging to generate a faithful full-body avatar. We
address this challenge by developing a flow-based generative model of the 3D
human body from sparse observations, wherein we learn not only a conditional
distribution of 3D human pose, but also a probabilistic mapping from
observations to the latent space from which we can generate a plausible pose
along with uncertainty estimates for the joints. We show that our approach is
not only a strong predictive model, but can also act as an efficient pose prior
in different optimization settings where a good initial latent code plays a
major role.","['Sadegh Aliakbarian', 'Pashmina Cameron', 'Federica Bogo', 'Andrew Fitzgibbon', 'Thomas J. Cashman']",2022-03-11T08:07:09Z,http://arxiv.org/abs/2203.05789v1,"['cs.CV', 'cs.LG']"
"Re-shaping Post-COVID-19 Teaching and Learning: A Blueprint of
  Virtual-Physical Blended Classrooms in the Metaverse Era","During the COVID-19 pandemic, most countries have experienced some form of
remote education through video conferencing software platforms. However, these
software platforms fail to reduce immersion and replicate the classroom
experience. The currently emerging Metaverse addresses many of such limitations
by offering blended physical-digital environments. This paper aims to assess
how the Metaverse can support and improve e-learning. We first survey the
latest applications of blended environments in education and highlight the
primary challenges and opportunities. Accordingly, we derive our proposal for a
virtual-physical blended classroom configuration that brings students and
teachers into a shared educational Metaverse. We focus on the system
architecture of the Metaverse classroom to achieve real-time synchronization of
a large number of participants and activities across physical (mixed reality
classrooms) and virtual (remote VR platform) learning spaces. Our proposal
attempts to transform the traditional physical classroom into virtual-physical
cyberspace as a new social network of learners and educators connected at an
unprecedented scale.","['Yuyang Wang', 'Lik-Hang Lee', 'Tristan Braud', 'Pan Hui']",2022-03-17T10:35:45Z,http://arxiv.org/abs/2203.09228v3,"['cs.HC', 'cs.MM', 'K.3; I.3']"
BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion,"Dense 3D reconstruction from a stream of depth images is the key to many
mixed reality and robotic applications. Although methods based on Truncated
Signed Distance Function (TSDF) Fusion have advanced the field over the years,
the TSDF volume representation is confronted with striking a balance between
the robustness to noisy measurements and maintaining the level of detail. We
present Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent
advances in neural implicit representations and neural rendering for dense 3D
reconstruction. In order to incrementally integrate new depth maps into a
global neural implicit representation, we propose a novel bi-level fusion
strategy that considers both efficiency and reconstruction quality by design.
We evaluate the proposed method on multiple datasets quantitatively and
qualitatively, demonstrating a significant improvement over existing methods.","['Kejie Li', 'Yansong Tang', 'Victor Adrian Prisacariu', 'Philip H. S. Torr']",2022-04-03T19:33:09Z,http://arxiv.org/abs/2204.01139v1,['cs.CV']
Context-Aware Sequence Alignment using 4D Skeletal Augmentation,"Temporal alignment of fine-grained human actions in videos is important for
numerous applications in computer vision, robotics, and mixed reality.
State-of-the-art methods directly learn image-based embedding space by
leveraging powerful deep convolutional neural networks. While being
straightforward, their results are far from satisfactory, the aligned videos
exhibit severe temporal discontinuity without additional post-processing steps.
The recent advancements in human body and hand pose estimation in the wild
promise new ways of addressing the task of human action alignment in videos. In
this work, based on off-the-shelf human pose estimators, we propose a novel
context-aware self-supervised learning architecture to align sequences of
actions. We name it CASA. Specifically, CASA employs self-attention and
cross-attention mechanisms to incorporate the spatial and temporal context of
human actions, which can solve the temporal discontinuity problem. Moreover, we
introduce a self-supervised learning scheme that is empowered by novel 4D
augmentation techniques for 3D skeleton representations. We systematically
evaluate the key components of our method. Our experiments on three public
datasets demonstrate CASA significantly improves phase progress and Kendall's
Tau scores over the previous state-of-the-art methods.","['Taein Kwon', 'Bugra Tekin', 'Siyu Tang', 'Marc Pollefeys']",2022-04-26T10:59:29Z,http://arxiv.org/abs/2204.12223v1,['cs.CV']
"The Gesture Authoring Space: Authoring Customised Hand Gestures for
  Grasping Virtual Objects in Immersive Virtual Environments","Natural user interfaces are on the rise. Manufacturers for Augmented,
Virtual, and Mixed Reality head mounted displays are increasingly integrating
new sensors into their consumer grade products, allowing gesture recognition
without additional hardware. This offers new possibilities for bare handed
interaction within virtual environments. This work proposes a hand gesture
authoring tool for object specific grab gestures allowing virtual objects to be
grabbed as in the real world. The presented solution uses template matching for
gesture recognition and requires no technical knowledge to design and create
custom tailored hand gestures. In a user study, the proposed approach is
compared with the pinch gesture and the controller for grasping virtual
objects. The different grasping techniques are compared in terms of accuracy,
task completion time, usability, and naturalness. The study showed that
gestures created with the proposed approach are perceived by users as a more
natural input modality than the others.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2022-07-03T18:33:33Z,http://arxiv.org/abs/2207.01092v1,"['cs.HC', 'cs.CV']"
SHREC 2022 Track on Online Detection of Heterogeneous Gestures,"This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.","['Ariel Caputo', 'Marco Emporio', 'Andrea Giachetti', 'Marco Cristani', 'Guido Borghi', ""Andrea D'Eusanio"", 'Minh-Quan Le', 'Hai-Dang Nguyen', 'Minh-Triet Tran', 'F. Ambellan', 'M. Hanik', 'E. Nava-Yazdani', 'C. von Tycowicz']",2022-07-14T07:24:02Z,http://arxiv.org/abs/2207.06706v2,"['cs.CV', '68T10', 'I.5.2']"
"Virtual Reality Therapy for the Psychological Well-being of Palliative
  Care Patients in Hong Kong","In this paper we introduce novel Virtual Reality (VR) and Augmented Reality
(AR) treatments to improve the psychological well being of patients in
palliative care, based on interviews with a clinical psychologist who has
successfully implemented VR assisted interventions on palliative care patients
in the Hong Kong hospital system. Our VR and AR assisted interventions are
adaptations of traditional palliative care therapies which simultaneously
facilitate patients communication with family and friends while isolated in
hospital due to physical weakness and COVID-19 related restrictions. The first
system we propose is a networked, metaverse platform for palliative care
patients to create customized virtual environments with therapists, family and
friends which function as immersive and collaborative versions of 'life review'
and 'reminiscence therapy'. The second proposed system will investigate the use
of Mixed Reality telepresence and haptic touch in an AR environment, which will
allow palliative care patients to physically feel friends and family in a
virtual space, adding to the sense of presence and immersion in that
environment.","['Daniel Eckhoff', 'Royce Ng', 'Alvaro Cassinelli']",2022-07-24T14:31:52Z,http://arxiv.org/abs/2207.11754v1,['cs.HC']
"IPDAE: Improved Patch-Based Deep Autoencoder for Lossy Point Cloud
  Geometry Compression","Point cloud is a crucial representation of 3D contents, which has been widely
used in many areas such as virtual reality, mixed reality, autonomous driving,
etc. With the boost of the number of points in the data, how to efficiently
compress point cloud becomes a challenging problem. In this paper, we propose a
set of significant improvements to patch-based point cloud compression, i.e., a
learnable context model for entropy coding, octree coding for sampling centroid
points, and an integrated compression and training process. In addition, we
propose an adversarial network to improve the uniformity of points during
reconstruction. Our experiments show that the improved patch-based autoencoder
outperforms the state-of-the-art in terms of rate-distortion performance, on
both sparse and large-scale point clouds. More importantly, our method can
maintain a short compression time while ensuring the reconstruction quality.","['Kang You', 'Pan Gao', 'Qing Li']",2022-08-04T08:12:35Z,http://arxiv.org/abs/2208.02519v1,"['cs.CV', 'cs.IT', 'cs.MM', 'eess.IV', 'math.IT']"
"When Internet of Things meets Metaverse: Convergence of Physical and
  Cyber Worlds","In recent years, the Internet of Things (IoT) is studied in the context of
the Metaverse to provide users immersive cyber-virtual experiences in mixed
reality environments. This survey introduces six typical IoT applications in
the Metaverse, including collaborative healthcare, education, smart city,
entertainment, real estate, and socialization. In the IoT-inspired Metaverse,
we also comprehensively survey four pillar technologies that enable augmented
reality (AR) and virtual reality (VR), namely, responsible artificial
intelligence (AI), high-speed data communications, cost-effective mobile edge
computing (MEC), and digital twins. According to the physical-world demands, we
outline the current industrial efforts and seven key requirements for building
the IoT-inspired Metaverse: immersion, variety, economy, civility,
interactivity, authenticity, and independence. In addition, this survey
describes the open issues in the IoT-inspired Metaverse, which need to be
addressed to eventually achieve the convergence of physical and cyber worlds.","['Kai Li', 'Yingping Cui', 'Weicai Li', 'Tiejun Lv', 'Xin Yuan', 'Shenghong Li', 'Wei Ni', 'Meryem Simsek', 'Falko Dressler']",2022-08-29T11:17:54Z,http://arxiv.org/abs/2208.13501v1,['cs.NI']
Mixed-Reality Robot Behavior Replay: A System Implementation,"As robots become increasingly complex, they must explain their behaviors to
gain trust and acceptance. However, it may be difficult through verbal
explanation alone to fully convey information about past behavior, especially
regarding objects no longer present due to robots' or humans' actions. Humans
often try to physically mimic past movements to accompany verbal explanations.
Inspired by this human-human interaction, we describe the technical
implementation of a system for past behavior replay for robots in this tool
paper. Specifically, we used Behavior Trees to encode and separate robot
behaviors, and schemaless MongoDB to structurally store and query the
underlying sensor data and joint control messages for future replay. Our
approach generalizes to different types of replays, including both manipulation
and navigation replay, and visual (i.e., augmented reality (AR)) and auditory
replay. Additionally, we briefly summarize a user study to further provide
empirical evidence of its effectiveness and efficiency. Sample code and
instructions are available on GitHub at
https://github.com/umhan35/robot-behavior-replay.","['Zhao Han', 'Tom Williams', 'Holly A. Yanco']",2022-09-30T20:19:07Z,http://arxiv.org/abs/2210.00075v1,"['cs.RO', 'cs.HC']"
"Light-weighted CNN-Attention based architecture for Hand Gesture
  Recognition via ElectroMyography","Advancements in Biological Signal Processing (BSP) and Machine-Learning (ML)
models have paved the path for development of novel immersive Human-Machine
Interfaces (HMI). In this context, there has been a surge of significant
interest in Hand Gesture Recognition (HGR) utilizing Surface-Electromyogram
(sEMG) signals. This is due to its unique potential for decoding wearable data
to interpret human intent for immersion in Mixed Reality (MR) environments. To
achieve the highest possible accuracy, complicated and heavy-weighted Deep
Neural Networks (DNNs) are typically developed, which restricts their practical
application in low-power and resource-constrained wearable systems. In this
work, we propose a light-weighted hybrid architecture (HDCAM) based on
Convolutional Neural Network (CNN) and attention mechanism to effectively
extract local and global representations of the input. The proposed HDCAM model
with 58,441 parameters reached a new state-of-the-art (SOTA) performance with
82.91% and 81.28% accuracy on window sizes of 300 ms and 200 ms for classifying
17 hand gestures. The number of parameters to train the proposed HDCAM
architecture is 18.87 times less than its previous SOTA counterpart.","['Soheil Zabihi', 'Elahe Rahimian', 'Amir Asif', 'Arash Mohammadi']",2022-10-27T02:12:07Z,http://arxiv.org/abs/2210.15119v1,"['cs.LG', 'eess.SP']"
"A new benchmark for group distribution shifts in hand grasp regression
  for object manipulation. Can meta-learning raise the bar?","Understanding hand-object pose with computer vision opens the door to new
applications in mixed reality, assisted living or human-robot interaction. Most
methods are trained and evaluated on balanced datasets. This is of limited use
in real-world applications; how do these methods perform in the wild on unknown
objects? We propose a novel benchmark for object group distribution shifts in
hand and object pose regression. We then test the hypothesis that meta-learning
a baseline pose regression neural network can adapt to these shifts and
generalize better to unknown objects. Our results show measurable improvements
over the baseline, depending on the amount of prior knowledge. For the task of
joint hand-object pose regression, we observe optimization interference for the
meta-learner. To address this issue and improve the method further, we provide
a comprehensive analysis which should serve as a basis for future work on this
benchmark.","['Théo Morales', 'Gerard Lacey']",2022-10-31T19:32:14Z,http://arxiv.org/abs/2211.00110v1,"['cs.CV', 'cs.LG']"
A Quantum-Powered Photorealistic Rendering,"Achieving photorealistic rendering of real-world scenes poses a significant
challenge with diverse applications, including mixed reality and virtual
reality. Neural networks, extensively explored in solving differential
equations, have previously been introduced as implicit representations for
photorealistic rendering. However, achieving realism through traditional
computing methods is arduous due to the time-consuming optical ray tracing, as
it necessitates extensive numerical integration of color, transparency, and
opacity values for each sampling point during the rendering process. In this
paper, we introduce Quantum Radiance Fields (QRF), which incorporate quantum
circuits, quantum activation functions, and quantum volume rendering to
represent scenes implicitly. Our results demonstrate that QRF effectively
confronts the computational challenges associated with extensive numerical
integration by harnessing the parallelism capabilities of quantum computing.
Furthermore, current neural networks struggle with capturing fine signal
details and accurately modeling high-frequency information and higher-order
derivatives. Quantum computing's higher order of nonlinearity provides a
distinct advantage in this context. Consequently, QRF leverages two key
strengths of quantum computing: highly non-linear processing and extensive
parallelism, making it a potent tool for achieving photorealistic rendering of
real-world scenes.","['YuanFu Yang', 'Min Sun']",2022-11-07T10:23:32Z,http://arxiv.org/abs/2211.03418v5,['cs.CV']
The Value Chain of Education Metaverse,"Since the end of 2021, the Metaverse has been booming. Many unknown
possibilities are gradually being realized, but many people only determined
that they use Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality
(MR) in the Metaverse. It is even considered that as long as the above
realities (VR, AR, MR) are used, it is equal to the Metaverse. However, this is
not true, for Reality-based display tools are only one of the presentation
methods of the Metaverse. If we cannot return to the three main characteristics
of the Metaverse: ""digital avatars,"" a decentralized ""consensus value system,""
and ""Immersive experience,"" the practice and imagination of the Metaverse will
become very narrow. Since 2022, the concept of Metaverse has also been widely
used in classroom teaching to integrate into teaching activities. Therefore, to
prevent teachers and students from understanding the Metaverse not only in the
""Using VR, AR, MR is equivalent to Metaverse"" but also pay more attention to
the other two characteristics of the Metaverse: ""digital avatars"" and a
decentralized ""consensus value system.""",['Yun-Cheng Tsai'],2022-11-07T15:28:06Z,http://arxiv.org/abs/2211.05833v2,"['cs.CY', 'cs.SI']"
"The Ball is in Our Court: Conducting Visualization Research with Sports
  Experts","Most sports visualizations rely on a combination of spatial, highly temporal,
and user-centric data, making sports a challenging target for visualization.
Emerging technologies, such as augmented and mixed reality (AR/XR), have
brought exciting opportunities along with new challenges for sports
visualization. We share our experience working with sports domain experts and
present lessons learned from conducting visualization research in SportsXR. In
our previous work, we have targeted different types of users in sports,
including athletes, game analysts, and fans. Each user group has unique design
constraints and requirements, such as obtaining real-time visual feedback in
training, automating the low-level video analysis workflow, or personalizing
embedded visualizations for live game data analysis. In this paper, we
synthesize our best practices and pitfalls we identified while working on
SportsXR. We highlight lessons learned in working with sports domain experts in
designing and evaluating sports visualizations and in working with emerging
AR/XR technologies. We envision that sports visualization research will benefit
the larger visualization community through its unique challenges and
opportunities for immersive and situated analytics.","['Tica Lin', 'Zhutian Chen', 'Johanna Beyer', 'Yincai Wu', 'Hanspeter Pfister', 'Yalong Yang']",2022-11-15T01:17:27Z,http://arxiv.org/abs/2211.07832v1,['cs.HC']
Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes,"We present a efficient multi-view inverse rendering method for large-scale
real-world indoor scenes that reconstructs global illumination and
physically-reasonable SVBRDFs. Unlike previous representations, where the
global illumination of large scenes is simplified as multiple environment maps,
we propose a compact representation called Texture-based Lighting (TBL). It
consists of 3D mesh and HDR textures, and efficiently models direct and
infinite-bounce indirect lighting of the entire large scene. Based on TBL, we
further propose a hybrid lighting representation with precomputed irradiance,
which significantly improves the efficiency and alleviates the rendering noise
in the material optimization. To physically disentangle the ambiguity between
materials, we propose a three-stage material optimization strategy based on the
priors of semantic segmentation and room segmentation. Extensive experiments
show that the proposed method outperforms the state-of-the-art quantitatively
and qualitatively, and enables physically-reasonable mixed-reality applications
such as material editing, editable novel view synthesis and relighting. The
project page is at https://lzleejean.github.io/TexIR.","['Zhen Li', 'Lingli Wang', 'Mofang Cheng', 'Cihui Pan', 'Jiaqi Yang']",2022-11-18T12:53:10Z,http://arxiv.org/abs/2211.10206v4,['cs.CV']
"Towards Live 3D Reconstruction from Wearable Video: An Evaluation of
  V-SLAM, NeRF, and Videogrammetry Techniques","Mixed reality (MR) is a key technology which promises to change the future of
warfare. An MR hybrid of physical outdoor environments and virtual military
training will enable engagements with long distance enemies, both real and
simulated. To enable this technology, a large-scale 3D model of a physical
environment must be maintained based on live sensor observations. 3D
reconstruction algorithms should utilize the low cost and pervasiveness of
video camera sensors, from both overhead and soldier-level perspectives.
Mapping speed and 3D quality can be balanced to enable live MR training in
dynamic environments. Given these requirements, we survey several 3D
reconstruction algorithms for large-scale mapping for military applications
given only live video. We measure 3D reconstruction performance from common
structure from motion, visual-SLAM, and photogrammetry techniques. This
includes the open source algorithms COLMAP, ORB-SLAM3, and NeRF using
Instant-NGP. We utilize the autonomous driving academic benchmark KITTI, which
includes both dashboard camera video and lidar produced 3D ground truth. With
the KITTI data, our primary contribution is a quantitative evaluation of 3D
reconstruction computational speed when considering live video.","['David Ramirez', 'Suren Jayasuriya', 'Andreas Spanias']",2022-11-21T19:57:51Z,http://arxiv.org/abs/2211.11836v1,"['eess.IV', 'cs.CV']"
"Mixed Cloud Control Testbed: Validating Vehicle-Road-Cloud Integration
  via Mixed Digital Twin","Reliable and efficient validation technologies are critical for the recent
development of multi-vehicle cooperation and vehicle-road-cloud integration. In
this paper, we introduce our miniature experimental platform, Mixed Cloud
Control Testbed (MCCT), developed based on a new notion of Mixed Digital Twin
(mixedDT). Combining Mixed Reality with Digital Twin, mixedDT integrates the
virtual and physical spaces into a mixed one, where physical entities coexist
and interact with virtual entities via their digital counterparts. Under the
framework of mixedDT, MCCT contains three major experimental platforms in the
physical, virtual and mixed spaces respectively, and provides a unified access
for various human-machine interfaces and external devices such as driving
simulators. A cloud unit, where the mixed experimental platform is deployed, is
responsible for fusing multi-platform information and assigning control
instructions, contributing to synchronous operation and real-time
cross-platform interaction. Particularly, MCCT allows for multi-vehicle
coordination composed of different multi-source vehicles (\eg, physical
vehicles, virtual vehicles and human-driven vehicles). Validations on vehicle
platooning demonstrate the flexibility and scalability of MCCT.","['Jianghong Dong', 'Qing Xu', 'Jiawei Wang', 'Chunying Yang', 'Mengchi Cai', 'Chaoyi Chen', 'Jianqiang Wang', 'Keqiang Li']",2022-12-05T03:39:31Z,http://arxiv.org/abs/2212.02007v1,"['cs.RO', 'cs.SY', 'eess.SY']"
Interactive Segmentation of Radiance Fields,"Radiance Fields (RF) are popular to represent casually-captured scenes for
new view synthesis and several applications beyond it. Mixed reality on
personal spaces needs understanding and manipulating scenes represented as RFs,
with semantic segmentation of objects as an important step. Prior segmentation
efforts show promise but don't scale to complex objects with diverse
appearance. We present the ISRF method to interactively segment objects with
fine structure and appearance. Nearest neighbor feature matching using
distilled semantic features identifies high-confidence seed regions. Bilateral
search in a joint spatio-semantic space grows the region to recover accurate
segmentation. We show state-of-the-art results of segmenting objects from RFs
and compositing them to another scene, changing appearance, etc., and an
interactive segmentation tool that others can use.
  Project Page: https://rahul-goel.github.io/isrf/","['Rahul Goel', 'Dhawal Sirikonda', 'Saurabh Saini', 'PJ Narayanan']",2022-12-27T16:33:19Z,http://arxiv.org/abs/2212.13545v2,['cs.CV']
PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,"Touch plays a fundamental role in manipulation for humans; however, machine
perception of contact and pressure typically requires invasive sensors. Recent
research has shown that deep models can estimate hand pressure based on a
single RGB image. However, evaluations have been limited to controlled settings
since collecting diverse data with ground-truth pressure measurements is
difficult. We present a novel approach that enables diverse data to be captured
with only an RGB camera and a cooperative participant. Our key insight is that
people can be prompted to apply pressure in a certain way, and this prompt can
serve as a weak label to supervise models to perform well under varied
conditions. We collect a novel dataset with 51 participants making fingertip
contact with diverse objects. Our network, PressureVision++, outperforms human
annotators and prior work. We also demonstrate an application of
PressureVision++ to mixed reality where pressure estimation allows everyday
surfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and
models are available online.","['Patrick Grady', 'Jeremy A. Collins', 'Chengcheng Tang', 'Christopher D. Twigg', 'Kunal Aneja', 'James Hays', 'Charles C. Kemp']",2023-01-05T21:48:33Z,http://arxiv.org/abs/2301.02310v3,['cs.CV']
"Event-Triggered Optimal Formation Tracking Control Using Reinforcement
  Learning for Large-Scale UAV Systems","Large-scale UAV switching formation tracking control has been widely applied
in many fields such as search and rescue, cooperative transportation, and UAV
light shows. In order to optimize the control performance and reduce the
computational burden of the system, this study proposes an event-triggered
optimal formation tracking controller for discrete-time large-scale UAV systems
(UASs). And an optimal decision - optimal control framework is completed by
introducing the Hungarian algorithm and actor-critic neural networks (NNs)
implementation. Finally, a large-scale mixed reality experimental platform is
built to verify the effectiveness of the proposed algorithm, which includes
large-scale virtual UAV nodes and limited physical UAV nodes. This compensates
for the limitations of the experimental field and equipment in realworld
scenario, ensures the experimental safety, significantly reduces the
experimental cost, and is suitable for realizing largescale UAV formation light
shows.","['Ziwei Yan', 'Liang Han', 'Xiaoduo Li', 'Jinjie Li', 'Zhang Ren']",2023-01-17T08:24:54Z,http://arxiv.org/abs/2301.06749v2,['cs.MA']
"Developing a Framework for Heterotopias as Discursive Playgrounds: A
  Comparative Analysis of Non-Immersive and Immersive Technologies","The discursive space represents the reordering of knowledge gained through
accumulation. In the digital age, multimedia has become the language of
information, and the space for archival practices is provided by non-immersive
technologies, resulting in the disappearance of several layers from discursive
activities. Heterotopias are unique, multilayered epistemic contexts that
connect other systems through the exchange of information. This paper describes
a process to create a framework for Virtual Reality, Mixed Reality, and
personal computer environments based on heterotopias to provide absent layers.
This study provides virtual museum space as an informational terrain that
contains a ""world within worlds"" and presents place production as a layer of
heterotopia and the subject of discourse. Automation for the individual
multimedia content is provided via various sorting and grouping algorithms, and
procedural content generation algorithms such as Binary Space Partitioning,
Cellular Automata, Growth Algorithm, and Procedural Room Generation. Versions
of the framework were comparatively evaluated through a user study involving 30
participants, considering factors such as usability, technology acceptance, and
presence. The results of the study show that the framework can serve diverse
contexts to construct multilayered digital habitats and is flexible for
integration into professional and daily life practices.","['Elif Hilal Korkut', 'Elif Surer']",2023-01-20T13:26:36Z,http://arxiv.org/abs/2301.08565v1,"['cs.HC', 'cs.MM']"
"Assessment HTN (A-HTN) for Automated Task Performance Assessment in 3D
  Serious Games","In the recent years, various 3D mixed reality serious games have been
developed for different applications such as physical training, rehabilitation,
and education. Task performance in a serious game is a measurement of how
efficiently and accurately users accomplish the game's objectives. Prior
research includes a graph-based representation of tasks, e.g. Hierarchical Task
Network (HTN), which only models a game's tasks but does not perform
assessment. In this paper, we propose Assessment HTN (A-HTN), which both models
the task efficiently and incorporates assessment logic for game objectives.
Based on how the task performance is evaluated, A-HTN automatically performs:
(a) Task-level Assessment by comparing object manipulations and (b)
Action-level Assessment by comparing motion trajectories. The system can also
categorize the task performance assessment into single user or multi-user based
on who is being assessed. We showcase the effectiveness of the A-HTN using two
3D VR serious games: a hydrometer experiment and a multi-user chemistry
experiment. The A-HTN experiments show a high correlation between instructor
scores and the system generated scores indicating that the proposed A-HTN
generalizes automatic assessment at par with Subject Matter Experts.","['Kevin Desai', 'Omeed Ashtiani', 'Balakrishnan Prabhakaran']",2023-02-11T22:13:16Z,http://arxiv.org/abs/2302.05795v1,"['cs.HC', 'cs.MM']"
Optimization-Based Eye Tracking using Deflectometric Information,"Eye tracking is an important tool with a wide range of applications in
Virtual, Augmented, and Mixed Reality (VR/AR/MR) technologies. State-of-the-art
eye tracking methods are either reflection-based and track reflections of
sparse point light sources, or image-based and exploit 2D features of the
acquired eye image. In this work, we attempt to significantly improve
reflection-based methods by utilizing pixel-dense deflectometric surface
measurements in combination with optimization-based inverse rendering
algorithms. Utilizing the known geometry of our deflectometric setup, we
develop a differentiable rendering pipeline based on PyTorch3D that simulates a
virtual eye under screen illumination. Eventually, we exploit the
image-screen-correspondence information from the captured measurements to find
the eye's rotation, translation, and shape parameters with our renderer via
gradient descent. In general, our method does not require a specific pattern
and can work with ordinary video frames of the main VR/AR/MR screen itself. We
demonstrate real-world experiments with evaluated mean relative gaze errors
below 0.45 degrees at a precision better than 0.11 degrees. Moreover, we show
an improvement of 6X over a representative reflection-based state-of-the-art
method in simulation.","['Tianfu Wang', 'Jiazhang Wang', 'Oliver Cossairt', 'Florian Willomitzer']",2023-03-09T02:41:13Z,http://arxiv.org/abs/2303.04997v1,['cs.CV']
"GazePair: Efficient Pairing of Augmented Reality Devices Using Gaze
  Tracking","As Augmented Reality (AR) devices become more prevalent and commercially
viable, the need for quick, efficient, and secure schemes for pairing these
devices has become more pressing. Current methods to securely exchange
holograms require users to send this information through large data centers,
creating security and privacy concerns. Existing techniques to pair these
devices on a local network and share information fall short in terms of
usability and scalability. These techniques either require hardware not
available on AR devices, intricate physical gestures, removal of the device
from the head, do not scale to multiple pairing partners, or rely on methods
with low entropy to create encryption keys. To that end, we propose a novel
pairing system, called GazePair, that improves on all existing local pairing
techniques by creating an efficient, effective, and intuitive pairing protocol.
GazePair uses eye gaze tracking and a spoken key sequence cue (KSC) to generate
identical, independently generated symmetric encryption keys with 64 bits of
entropy. GazePair also achieves improvements in pairing success rates and times
over current methods. Additionally, we show that GazePair can extend to
multiple users. Finally, we assert that GazePair can be used on any Mixed
Reality (MR) device equipped with eye gaze tracking.","['Matthew Corbett', 'Jiacheng Shang', 'Bo Ji']",2023-03-13T18:32:32Z,http://arxiv.org/abs/2303.07404v1,['cs.CR']
"Inside-out Infrared Marker Tracking via Head Mounted Displays for Smart
  Robot Programming","Intuitive robot programming through use of tracked smart input devices relies
on fixed, external tracking systems, most often employing infra-red markers.
Such an approach is frequently combined with projector-based augmented reality
for better visualisation and interface. The combined system, although providing
an intuitive programming platform with short cycle times even for inexperienced
users, is immobile, expensive and requires extensive calibration. When faced
with a changing environment and large number of robots it becomes sorely
impractical. Here we present our work on infra-red marker tracking using the
Microsoft HoloLens head-mounted display. The HoloLens can map the environment,
register the robot on-line, and track smart devices equipped with infra-red
markers in the robot coordinate system. We envision our work to provide the
basis to transfer many of the paradigms developed over the years for systems
requiring a projector and a tracked input device into a highly-portable system
that does not require any calibration or special set-up. We test the quality of
the marker-tracking in an industrial robot cell and compare our tracking with a
ground truth obtained via an ART-3 tracking system.","['David Puljiz', 'Alexandru-George Vasilache', 'Michael Mende', 'Björn Hein']",2023-03-28T14:45:03Z,http://arxiv.org/abs/2303.16017v1,['cs.RO']
"Exploiting the Complementarity of 2D and 3D Networks to Address
  Domain-Shift in 3D Semantic Segmentation","3D semantic segmentation is a critical task in many real-world applications,
such as autonomous driving, robotics, and mixed reality. However, the task is
extremely challenging due to ambiguities coming from the unstructured, sparse,
and uncolored nature of the 3D point clouds. A possible solution is to combine
the 3D information with others coming from sensors featuring a different
modality, such as RGB cameras. Recent multi-modal 3D semantic segmentation
networks exploit these modalities relying on two branches that process the 2D
and 3D information independently, striving to maintain the strength of each
modality. In this work, we first explain why this design choice is effective
and then show how it can be improved to make the multi-modal semantic
segmentation more robust to domain shift. Our surprisingly simple contribution
achieves state-of-the-art performances on four popular multi-modal unsupervised
domain adaptation benchmarks, as well as better results in a domain
generalization scenario.","['Adriano Cardace', 'Pierluigi Zama Ramirez', 'Samuele Salti', 'Luigi Di Stefano']",2023-04-06T10:59:43Z,http://arxiv.org/abs/2304.02991v1,['cs.CV']
"OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for
  Real-time 3D Hand Gesture Classification and Segmentation","Continuous mid-air hand gesture recognition based on captured hand pose
streams is fundamental for human-computer interaction, particularly in AR / VR.
However, many of the methods proposed to recognize heterogeneous hand gestures
are tested only on the classification task, and the real-time low-latency
gesture segmentation in a continuous stream is not well addressed in the
literature. For this task, we propose the On-Off deep Multi-View Multi-Task
paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related
to hand pose and movement to generate rich gesture descriptions, along with
using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the
classical MVMT paradigm, where all of the multiple tasks have to be active at
each time, by allowing specific tasks to switch on/off depending on whether
they can apply to the input. We show that OO-dMVMT defines the new SotA on
continuous/online 3D skeleton-based gesture recognition in terms of gesture
classification accuracy, segmentation accuracy, false positives, and decision
latency while maintaining real-time operation.","['Federico Cunico', 'Federico Girella', 'Andrea Avogaro', 'Marco Emporio', 'Andrea Giachetti', 'Marco Cristani']",2023-04-12T16:28:29Z,http://arxiv.org/abs/2304.05956v1,['cs.CV']
Traffic Characteristics of Extended Reality,"This tutorial paper analyzes the traffic characteristics of immersive
experiences with extended reality (XR) technologies, including Augmented
reality (AR), virtual reality (VR), and mixed reality (MR). The current trend
in XR applications is to offload the computation and rendering to an external
server and use wireless communications between the XR head-mounted display
(HMD) and the access points. This paradigm becomes essential owing to (1) its
high flexibility (in terms of user mobility) compared to remote rendering
through a wired connection, and (2) the high computing power available on the
server compared to local rendering (on HMD). The requirements to facilitate a
pleasant XR experience are analyzed in three aspects: capacity (throughput),
latency, and reliability. For capacity, two VR experiences are analyzed: a
human eye-like experience and an experience with the Oculus Quest 2 HMD. For
latency, the key components of the motion-to-photon (MTP) delay are discussed.
For reliability, the maximum packet loss rate (or the minimum packet delivery
rate) is studied for different XR scenarios. Specifically, the paper reviews
optimization techniques that were proposed to reduce the latency, conserve the
bandwidth, extend the scalability, and/or increase the reliability to satisfy
the stringent requirements of the emerging XR applications.","['Abdullah Alnajim', 'Seyedmohammad Salehi', 'Chien-Chung Shen', 'Malcolm Smith']",2023-04-16T22:17:29Z,http://arxiv.org/abs/2304.07908v1,"['cs.NI', 'cs.GR']"
"BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion
  Synthesis","Mixed reality applications require tracking the user's full-body motion to
enable an immersive experience. However, typical head-mounted devices can only
track head and hand movements, leading to a limited reconstruction of full-body
motion due to variability in lower body configurations. We propose BoDiffusion
-- a generative diffusion model for motion synthesis to tackle this
under-constrained reconstruction problem. We present a time and space
conditioning scheme that allows BoDiffusion to leverage sparse tracking inputs
while generating smooth and realistic full-body motion sequences. To the best
of our knowledge, this is the first approach that uses the reverse diffusion
process to model full-body tracking as a conditional sequence generation task.
We conduct experiments on the large-scale motion-capture dataset AMASS and show
that our approach outperforms the state-of-the-art approaches by a significant
margin in terms of full-body motion realism and joint reconstruction error.","['Angela Castillo', 'Maria Escobar', 'Guillaume Jeanneret', 'Albert Pumarola', 'Pablo Arbeláez', 'Ali Thabet', 'Artsiom Sanakoyeu']",2023-04-21T16:39:05Z,http://arxiv.org/abs/2304.11118v1,"['cs.CV', 'cs.AI']"
"A Comprehensive Survey on Affective Computing; Challenges, Trends,
  Applications, and Future Directions","As the name suggests, affective computing aims to recognize human emotions,
sentiments, and feelings. There is a wide range of fields that study affective
computing, including languages, sociology, psychology, computer science, and
physiology. However, no research has ever been done to determine how machine
learning (ML) and mixed reality (XR) interact together. This paper discusses
the significance of affective computing, as well as its ideas, conceptions,
methods, and outcomes. By using approaches of ML and XR, we survey and discuss
recent methodologies in affective computing. We survey the state-of-the-art
approaches along with current affective data resources. Further, we discuss
various applications where affective computing has a significant impact, which
will aid future scholars in gaining a better understanding of its significance
and practical relevance.","['Sitara Afzal', 'Haseeb Ali Khan', 'Imran Ullah Khan', 'Md. Jalil Piran', 'Jong Weon Lee']",2023-05-08T10:42:46Z,http://arxiv.org/abs/2305.07665v1,['cs.AI']
"A Fusion Model: Towards a Virtual, Physical and Cognitive Integration
  and its Principles","Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital
twin, Metaverse and other related digital technologies have attracted much
attention in recent years. These new emerging technologies are changing the
world significantly. This research introduces a fusion model, i.e. Fusion
Universe (FU), where the virtual, physical, and cognitive worlds are merged
together. Therefore, it is crucial to establish a set of principles for the
fusion model that is compatible with our physical universe laws and principles.
This paper investigates several aspects that could affect immersive and
interactive experience; and proposes the fundamental principles for Fusion
Universe that can integrate physical and virtual world seamlessly.","['Hao Lan Zhang', 'Yun Xue', 'Yifan Lu', 'Sanghyuk Lee']",2023-05-17T06:34:22Z,http://arxiv.org/abs/2305.09992v1,"['cs.AI', 'cs.HC']"
TextSLAM: Visual SLAM with Semantic Planar Text Features,"We propose a novel visual SLAM method that integrates text objects tightly by
treating them as semantic features via fully exploring their geometric and
semantic prior. The text object is modeled as a texture-rich planar patch whose
semantic meaning is extracted and updated on the fly for better data
association. With the full exploration of locally planar characteristics and
semantic meaning of text objects, the SLAM system becomes more accurate and
robust even under challenging conditions such as image blurring, large
viewpoint changes, and significant illumination variations (day and night). We
tested our method in various scenes with the ground truth data. The results
show that integrating texture features leads to a more superior SLAM system
that can match images across day and night. The reconstructed semantic 3D text
map could be useful for navigation and scene understanding in robotic and mixed
reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .","['Boying Li', 'Danping Zou', 'Yuan Huang', 'Xinghan Niu', 'Ling Pei', 'Wenxian Yu']",2023-05-17T08:16:26Z,http://arxiv.org/abs/2305.10029v2,['cs.CV']
Extended-XRI Body Interfaces for Hyper-Connected Metaverse Environments,"Hybrid mixed-reality (XR) internet-of-things (IoT) research, here called XRI,
aims at a strong integration between physical and virtual objects,
environments, and agents wherein IoT-enabled edge devices are deployed for
sensing, context understanding, networked communication and control of device
actuators. Likewise, as augmented reality systems provide an immersive overlay
on the environments, and virtual reality provides fully immersive environments,
the merger of these domains leads to immersive smart spaces that are
hyper-connected, adaptive and dynamic components that anchor the metaverse to
real-world constructs. Enabling the human-in-the-loop to remain engaged and
connected across these virtual-physical hybrid environments requires advances
in user interaction that are multi-dimensional. This work investigates the
potential to transition the user interface to the human body as an
extended-reality avatar with hybrid extended-body interfaces that can interact
both with the physical and virtual sides of the metaverse. It contributes: i)
an overview of metaverses, XRI, and avatarization concepts, ii) a taxonomy
landscape for extended XRI body interfaces, iii) an architecture and potential
interactions for XRI body designs, iv) a prototype XRI body implementation
based on the architecture, v) a design-science evaluation, toward enabling
future design research directions.","['Jie Guan', 'Alexis Morris']",2023-06-01T19:11:18Z,http://arxiv.org/abs/2306.01096v1,['cs.HC']
"An XRI Mixed-Reality Internet-of-Things Architectural Framework Toward
  Immersive and Adaptive Smart Environments","The internet-of-things (IoT) refers to the growing number of embedded
interconnected devices within everyday ubiquitous objects and environments,
especially their networks, edge controllers, data gathering and management,
sharing, and contextual analysis capabilities. However, the IoT suffers from
inherent limitations in terms of human-computer interaction. In this landscape,
there is a need for interfaces that have the potential to translate the IoT
more solidly into the foreground of everyday smart environments, where its
users are multimodal, multifaceted, and where new forms of presentation,
adaptation, and immersion are essential. This work highlights the synergetic
opportunities for both IoT and XR to converge toward hybrid XR objects with
strong real-world connectivity, and IoT objects with rich XR interfaces. The
paper contributes i) an understanding of this multi-disciplinary domain XR-IoT
(XRI); ii) a theoretical perspective on how to design XRI agents based on the
literature; iii) a system design architectural framework for XRI smart
environment development; and iv) an early discussion of this process. It is
hoped that this research enables future researchers in both communities to
better understand and deploy hybrid smart XRI environments.","['Alexis Morris', 'Jie Guan', 'Amna Azhar']",2023-06-01T20:47:07Z,http://arxiv.org/abs/2306.01139v1,['cs.HC']
"Evolution of 3GPP Standards Towards True Extended Reality (XR) Support
  in 6G Networks","Extended reality (XR) is a key innovation of 5G-advanced and beyond networks.
The diverse XR use-cases, including virtual reality, augmented reality, and
mixed reality, transform the way humans interact with surrounding environments.
Thus, XR technology enables true immersive experiences of novel services
spanning, e.g., e-commerce, healthcare, and education, respectively. However,
the efficient support of XR services over existing and future cellular systems
is highly challenging and requires multiple radio design improvements, due to
the unique XR traffic and performance characteristics. Thus, this article
surveys the state-of-art 3GPP standardization activities (release-18) for
integrating the XR service class into the 5G-advanced specifications,
highlighting the major XR performance challenges. Furthermore, the paper
introduces valuable insights and research directions for supporting true XR
services over the next-generation 6G networks, where multiple novel radio
design mindsets and protocol enhancements are proposed and evaluated using
extensive system level simulations, including solutions for application-native
dynamic performance reporting, traffic-dependent control channel design,
collaborative device aggregation for XR capacity boosting and offload,
respectively.","['Ali A. Esswie', 'Morris Repeta']",2023-06-06T20:57:35Z,http://arxiv.org/abs/2306.04012v1,"['eess.SP', 'cs.NI']"
Large AI Model-Based Semantic Communications,"Semantic communication (SC) is an emerging intelligent paradigm, offering
solutions for various future applications like metaverse, mixed-reality, and
the Internet of everything. However, in current SC systems, the construction of
the knowledge base (KB) faces several issues, including limited knowledge
representation, frequent knowledge updates, and insecure knowledge sharing.
Fortunately, the development of the large AI model provides new solutions to
overcome above issues. Here, we propose a large AI model-based SC framework
(LAM-SC) specifically designed for image data, where we first design the
segment anything model (SAM)-based KB (SKB) that can split the original image
into different semantic segments by universal semantic knowledge. Then, we
present an attention-based semantic integration (ASI) to weigh the semantic
segments generated by SKB without human participation and integrate them as the
semantic-aware image. Additionally, we propose an adaptive semantic compression
(ASC) encoding to remove redundant information in semantic features, thereby
reducing communication overhead. Finally, through simulations, we demonstrate
the effectiveness of the LAM-SC framework and the significance of the large AI
model-based KB development in future SC paradigms.","['Feibo Jiang', 'Yubo Peng', 'Li Dong', 'Kezhi Wang', 'Kun Yang', 'Cunhua Pan', 'Xiaohu You']",2023-07-07T10:01:08Z,http://arxiv.org/abs/2307.03492v1,"['cs.AI', 'cs.NI']"
"Accessibility and Inclusiveness of New Information and Communication
  Technologies for Disabled Users and Content Creators in the Metaverse","Despite the proliferation of Blockchain Metaverse projects, the inclusion of
physically disabled individuals in the Metaverse remains distant, with limited
standards and regulations in place. However, the article proposes a concept of
the Metaverse that leverages emerging technologies, such as Virtual and
Augmented Reality, and the Internet of Things, to enable greater engagement of
disabled creatives. This approach aims to enhance inclusiveness in the
Metaverse landscape. Based on the findings, the paper concludes that the active
involvement of physically disabled individuals in the design and development of
Metaverse platforms is crucial for promoting inclusivity. The proposed
framework for accessibility and inclusiveness in Virtual, Augmented, and Mixed
realities of decentralised Metaverses provides a basis for the meaningful
participation of disabled creatives. The article emphasises the importance of
addressing the mechanisms for art production by individuals with disabilities
in the emerging Metaverse landscape. Additionally, it highlights the need for
further research and collaboration to establish standards and regulations that
facilitate the inclusion of physically disabled individuals in Metaverse
projects.","['Petar Radanliev', 'David De Roure', 'Peter Novitzky', 'Ivo Sluganovic']",2023-08-01T18:39:12Z,http://arxiv.org/abs/2308.01925v1,"['cs.CY', 'cs.CV', 'cs.MM', 'cs.SI']"
"Open Medical Gesture: An Open-Source Experiment in Naturalistic Physical
  Interactions for Mixed and Virtual Reality Simulations","Mixed Reality (MR) and Virtual Reality (VR) simulations are hampered by
requirements for hand controllers or attempts to perseverate in use of
two-dimensional computer interface paradigms from the 1980s. From our efforts
to produce more naturalistic interactions for combat medic training for the
military, USC has developed an open-source toolkit that enables direct hand
controlled responsive interactions that is sensor independent and can function
with depth sensing cameras, webcams or sensory gloves. Natural approaches we
have examined include the ability to manipulate virtual smart objects in a
similar manner to how they are used in the real world. From this research and
review of current literature, we have discerned several best approaches for
hand-based human computer interactions which provide intuitive, responsive,
useful, and low frustration experiences for VR users.","['Thomas B Talbot', 'Chinmay Chinara']",2023-08-14T21:56:41Z,http://arxiv.org/abs/2308.07472v1,['cs.HC']
"Projecting Robot Intentions Through Visual Cues: Static vs. Dynamic
  Signaling","Augmented and mixed-reality techniques harbor a great potential for improving
human-robot collaboration. Visual signals and cues may be projected to a human
partner in order to explicitly communicate robot intentions and goals. However,
it is unclear what type of signals support such a process and whether signals
can be combined without adding additional cognitive stress to the partner. This
paper focuses on identifying the effective types of visual signals and quantify
their impact through empirical evaluations. In particular, the study compares
static and dynamic visual signals within a collaborative object sorting task
and assesses their ability to shape human behavior. Furthermore, an
information-theoretic analysis is performed to numerically quantify the degree
of information transfer between visual signals and human behavior. The results
of a human subject experiment show that there are significant advantages to
combining multiple visual signals within a single task, i.e., increased task
efficiency and reduced cognitive load.","['Shubham Sonawani', 'Yifan Zhou', 'Heni Ben Amor']",2023-08-19T01:18:37Z,http://arxiv.org/abs/2308.09871v1,"['cs.RO', 'cs.GR']"
Towards Ubiquitous Intelligent Hand Interaction,"The development of ubiquitous computing and sensing devices has brought about
novel interaction scenarios such as mixed reality and IoT (e.g., smart home),
which pose new demands for the next generation of natural user interfaces
(NUI). Human hand, benefit for the large degree-of-freedom, serves as a medium
through which people interact with the external world in their daily lives,
thus also being regarded as the main entry of NUI. Unfortunately, current hand
tracking system is largely confined on first perspective vision-based
solutions, which suffer from optical artifacts and are not practical in
ubiquitous environments. In my thesis, I rethink this problem by analyzing the
underlying logic in terms of sensor, behavior, and semantics, constituting a
research framework for achieving ubiquitous intelligent hand interaction. Then
I summarize my previous research topics and illustrated the future research
directions based on my research framework.",['Chen Liang'],2023-08-21T07:52:16Z,http://arxiv.org/abs/2308.13543v1,"['cs.HC', 'cs.CV']"
"PaperToPlace: Transforming Instruction Documents into Spatialized and
  Context-Aware Mixed Reality Experiences","While paper instructions are one of the mainstream medium for sharing
knowledge, consuming such instructions and translating them into activities are
inefficient due to the lack of connectivity with physical environment. We
present PaperToPlace, a novel workflow comprising an authoring pipeline, which
allows the authors to rapidly transform and spatialize existing paper
instructions into MR experience, and a consumption pipeline, which
computationally place each instruction step at an optimal location that is easy
to read and do not occlude key interaction areas. Our evaluations of the
authoring pipeline with 12 participants demonstrated the usability of our
workflow and the effectiveness of using a machine learning based approach to
help extracting the spatial locations associated with each steps. A second
within-subject study with another 12 participants demonstrates the merits of
our consumption pipeline by reducing efforts of context switching, delivering
the segmented instruction steps and offering the hands-free affordances.","['Chen Chen', 'Cuong Nguyen', 'Jane Hoffswell', 'Jennifer Healey', 'Trung Bui', 'Nadir Weibel']",2023-08-26T17:51:12Z,http://arxiv.org/abs/2308.13924v1,"['cs.HC', 'H.4.m; H.5.2; I.7.m']"
D-VAT: End-to-End Visual Active Tracking for Micro Aerial Vehicles,"Visual active tracking is a growing research topic in robotics due to its key
role in applications such as human assistance, disaster recovery, and
surveillance. In contrast to passive tracking, active tracking approaches
combine vision and control capabilities to detect and actively track the
target. Most of the work in this area focuses on ground robots, while the very
few contributions on aerial platforms still pose important design constraints
that limit their applicability. To overcome these limitations, in this paper we
propose D-VAT, a novel end-to-end visual active tracking methodology based on
deep reinforcement learning that is tailored to micro aerial vehicle platforms.
The D-VAT agent computes the vehicle thrust and angular velocity commands
needed to track the target by directly processing monocular camera
measurements. We show that the proposed approach allows for precise and
collision-free tracking operations, outperforming different state-of-the-art
baselines on simulated environments which differ significantly from those
encountered during training. Moreover, we demonstrate a smooth real-world
transition to a quadrotor platform with mixed-reality.","['Alberto Dionigi', 'Simone Felicioni', 'Mirko Leomanni', 'Gabriele Costante']",2023-08-31T17:21:18Z,http://arxiv.org/abs/2308.16874v2,['cs.RO']
Poster: Enabling Flexible Edge-assisted XR,"Extended reality (XR) is touted as the next frontier of the digital future.
XR includes all immersive technologies of augmented reality (AR), virtual
reality (VR), and mixed reality (MR). XR applications obtain the real-world
context of the user from an underlying system, and provide rich, immersive, and
interactive virtual experiences based on the user's context in real-time. XR
systems process streams of data from device sensors, and provide
functionalities including perceptions and graphics required by the
applications. These processing steps are computationally intensive, and the
challenge is that they must be performed within the strict latency requirements
of XR. This poses limitations on the possible XR experiences that can be
supported on mobile devices with limited computing resources.
  In this XR context, edge computing is an effective approach to address this
problem for mobile users. The edge is located closer to the end users and
enables processing and storing data near them. In addition, the development of
high bandwidth and low latency network technologies such as 5G facilitates the
application of edge computing for latency-critical use cases [4, 11]. This work
presents an XR system for enabling flexible edge-assisted XR.","['Jin Heo', 'Ketan Bhardwaj', 'Ada Gavrilovska']",2023-09-08T18:34:34Z,http://arxiv.org/abs/2309.04548v1,"['cs.DC', 'cs.MM']"
"Improving Human Legibility in Collaborative Robot Tasks through
  Augmented Reality and Workspace Preparation","Understanding the intentions of human teammates is critical for safe and
effective human-robot interaction. The canonical approach for human-aware robot
motion planning is to first predict the human's goal or path, and then
construct a robot plan that avoids collision with the human. This method can
generate unsafe interactions if the human model and subsequent predictions are
inaccurate. In this work, we present an algorithmic approach for both arranging
the configuration of objects in a shared human-robot workspace, and projecting
``virtual obstacles'' in augmented reality, optimizing for legibility in a
given task. These changes to the workspace result in more legible human
behavior, improving robot predictions of human goals, thereby improving task
fluency and safety. To evaluate our approach, we propose two user studies
involving a collaborative tabletop task with a manipulator robot, and a
warehouse navigation task with a mobile robot.","['Yi-Shiuan Tung', 'Matthew B. Luebbers', 'Alessandro Roncone', 'Bradley Hayes']",2023-11-09T18:18:28Z,http://arxiv.org/abs/2311.05562v1,['cs.RO']
Mixed Reality UI Adaptations with Inaccurate and Incomplete Objectives,"This position paper outlines a new approach to adapting 3D user interface
(UI) layouts given the complex nature of end-user preferences. Current
optimization techniques, which mainly rely on weighted sum methods, can be
inflexible and result in unsatisfactory adaptations. We propose using
multi-objective optimization and interactive preference elicitation to provide
semi-automated, flexible, and effective adaptations of 3D UIs. Our approach is
demonstrated using an example of single-element 3D layout adaptation with
ergonomic objectives. Future work is needed to address questions around the
presentation and selection of optimal solutions, the impact on cognitive load,
and the integration of preference learning. We conclude that, to make adaptive
3D UIs truly effective, we must acknowledge the limitations of our optimization
objectives and techniques and emphasize the importance of user control.","['Christoph Albert Johns', 'João Marcelo Evangelista Belo']",2023-11-17T11:45:46Z,http://arxiv.org/abs/2311.10466v1,['cs.HC']
"A Comparison of Interfaces for Learning How to Play a Mixed Reality
  Handpan","In the realm of music therapy, Virtual Reality (VR) has a long-standing
history of enriching human experiences through immersive applications, spanning
entertainment games, serious games, and professional training in various
fields. However, the untapped potential lies in using VR games to support
mindfulness through music. We present a new approach utilizing a virtual
environment to facilitate learning how to play the handpan -- an instrument in
the shape of a spherical dish with harmonically tuned notes used commonly in
the sound healing practice of mindfulness. In a preliminary study, we compared
six interfaces, where the highlighted path interface performed best. However,
participants expressed preference for the standard interface inspired by rhythm
games like Guitar Hero.","['Gavin Gosling', 'Ivan-teofil Catovic', 'Ghazal Bangash', 'Daniel MacCormick', 'Loutfouz Zaman']",2023-12-12T02:11:13Z,http://arxiv.org/abs/2312.06936v1,['cs.HC']
"Testing Human-Robot Interaction in Virtual Reality: Experience from a
  Study on Speech Act Classification","In recent years, an increasing number of Human-Robot Interaction (HRI)
approaches have been implemented and evaluated in Virtual Reality (VR), as it
allows to speed-up design iterations and makes it safer for the final user to
evaluate and master the HRI primitives. However, identifying the most suitable
VR experience is not straightforward. In this work, we evaluate how, in a smart
agriculture scenario, immersive and non-immersive VR are perceived by users
with respect to a speech act understanding task. In particular, we collect
opinions and suggestions from the 81 participants involved in both experiments
to highlight the strengths and weaknesses of these different experiences.","['Sara Kaszuba', 'Sandeep Reddy Sabbella', 'Francesco Leotta', 'Pascal Serrarens', 'Daniele Nardi']",2024-01-09T13:08:13Z,http://arxiv.org/abs/2401.04534v1,"['cs.RO', 'cs.HC']"
"OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed
  Reality","One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.","['Aditya Sharma', 'Luke Yoffe', 'Tobias Höllerer']",2024-01-17T04:52:40Z,http://arxiv.org/abs/2401.08973v1,"['cs.CV', 'cs.AI', 'cs.CL']"
Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case,"Multi-perspective cameras with potentially non-overlapping fields of view
have become an important exteroceptive sensing modality in a number of
applications such as intelligent vehicles, drones, and mixed reality headsets.
In this work, we challenge one of the basic assumptions made in these
scenarios, which is that the multi-camera rig is rigid. More specifically, we
are considering the problem of estimating the relative pose between a static
non-rigid rig in different spatial orientations while taking into account the
effect of gravity onto the system. The deformable physical connections between
each camera and the body center are approximated by a simple cantilever model,
and inserted into the generalized epipolar constraint. Our results lead us to
the important insight that the latent parameters of the deformation model,
meaning the gravity vector in both views, become observable. We present a
concise analysis of the observability of all variables based on noise,
outliers, and rig rigidity for two different algorithms. The first one is a
vision-only alternative, while the second one makes use of additional gravity
measurements. To conclude, we demonstrate the ability to sense gravity in a
real-world example, and discuss practical implications.","['Min Li', 'Jiaqi Yang', 'Laurent Kneip']",2024-01-17T11:28:28Z,http://arxiv.org/abs/2401.09140v1,"['cs.RO', 'cs.CV']"
"Design Frameworks for Spatial Zone Agents in XRI Metaverse Smart
  Environments","The spatial XR-IoT (XRI) Zone Agents concept combines Extended Reality (XR),
the Internet of Things (IoT), and spatial computing concepts to create
hyper-connected spaces for metaverse applications; envisioning space as zones
that are social, smart, scalable, expressive, and agent-based. These zone
agents serve as applications and agents (partners, assistants, or guides) for
users co-living and co-operating together in a shared spatial context. The zone
agent concept is toward reducing the gap between the physical environment
(space) and the classical two-dimensional user interface, through space-based
interactions for future metaverse applications. This integration aims to enrich
user engagement with their environments through intuitive and immersive
experiences and pave the way for innovative human-machine interaction in smart
spaces. Contributions include: i) a theoretical framework for creating XRI
zone/space-agents using Mixed-Reality Agents (MiRAs) and XRI theory, ii) agent
and scene design for spatial zone agents, and iii) prototype and user
interaction design scenario concepts for human-to-space agent relationships in
an early immersive smart-space application.","['Jie Guan', 'Jiamin Liu', 'Alexis Morris']",2024-01-19T22:03:36Z,http://arxiv.org/abs/2401.11040v1,['cs.HC']
Portobello: Extending Driving Simulation from the Lab to the Road,"In automotive user interface design, testing often starts with lab-based
driving simulators and migrates toward on-road studies to mitigate risks. Mixed
reality (XR) helps translate virtual study designs to the real road to increase
ecological validity. However, researchers rarely run the same study in both
in-lab and on-road simulators due to the challenges of replicating studies in
both physical and virtual worlds. To provide a common infrastructure to port
in-lab study designs on-road, we built a platform-portable infrastructure,
Portobello, to enable us to run twinned physical-virtual studies. As a
proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We
ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32)
both in-lab and on-road to investigate study design portability and
platform-driven influences on study outcomes. To our knowledge, this is the
first system that enables the twinning of studies originally designed for
in-lab simulators to be carried out in an on-road platform.","['Fanjun Bu', 'Stacey Li', 'David Goedicke', 'Mark Colley', 'Gyanendra Sharma', 'Hiroshi Yasuda', 'Wendy Ju']",2024-02-12T21:11:34Z,http://arxiv.org/abs/2402.08061v1,['cs.HC']
"Mixed-Reality-Guided Teleoperation of a Collaborative Robot for Surgical
  Procedures","The development of advanced surgical systems embedding the Master-Slave
control strategy introduced the possibility of remote interaction between the
surgeon and the patient, also known as teleoperation. The present paper aims to
integrate innovative technologies into the teleoperation process to enhance
workflow during surgeries. The proposed system incorporates a collaborative
robot, Kuka IIWA LBR, and Hololens 2 (an augmented reality device), allowing
the user to control the robot in an expansive environment that integrates
actual (real data) with additional digital information imported via Hololens 2.
Experimental data demonstrate the user's ability to control the Kuka IIWA using
various gestures to position it with respect to real or digital objects. Thus,
this system offers a novel solution to manipulate robots used in surgeries in a
more intuitive manner, contributing to the reduction of the learning curve for
surgeons. Calibration and testing in multiple scenarios demonstrate the
efficiency of the system in providing seamless movements.","['Gabriela Rus', 'Nadim Al Hajjar', 'Paul Tucan', 'Andra Ciocan', 'Calin Vaida', 'Corina Radu', 'Damien Chablat', 'Doina Pisla']",2024-02-19T09:51:03Z,http://arxiv.org/abs/2402.12002v1,['cs.RO']
"Enabling Waypoint Generation for Collaborative Robots using LLMs and
  Mixed Reality","Programming a robotic is a complex task, as it demands the user to have a
good command of specific programming languages and awareness of the robot's
physical constraints. We propose a framework that simplifies robot deployment
by allowing direct communication using natural language. It uses large language
models (LLM) for prompt processing, workspace understanding, and waypoint
generation. It also employs Augmented Reality (AR) to provide visual feedback
of the planned outcome. We showcase the effectiveness of our framework with a
simple pick-and-place task, which we implement on a real robot. Moreover, we
present an early concept of expressive robot behavior and skill generation that
can be used to communicate with the user and learn new skills (e.g., object
grasping).","['Cathy Mengying Fang', 'Krzysztof Zieliński', 'Pattie Maes', 'Joe Paradiso', 'Bruce Blumberg', 'Mikkel Baun Kjærgaard']",2024-03-14T11:59:07Z,http://arxiv.org/abs/2403.09308v1,"['cs.HC', 'cs.RO']"
"Towards Massive Interaction with Generalist Robotics: A Systematic
  Review of XR-enabled Remote Human-Robot Interaction Systems","The rising interest of generalist robots seek to create robots with
versatility to handle multiple tasks in a variety of environments, and human
will interact with such robots through immersive interfaces. In the context of
human-robot interaction (HRI), this survey provides an exhaustive review of the
applications of extended reality (XR) technologies in the field of remote HRI.
We developed a systematic search strategy based on the PRISMA methodology. From
the initial 2,561 articles selected, 100 research papers that met our inclusion
criteria were included. We categorized and summarized the domain in detail,
delving into XR technologies, including augmented reality (AR), virtual reality
(VR), and mixed reality (MR), and their applications in facilitating intuitive
and effective remote control and interaction with robotic systems. The survey
highlights existing articles on the application of XR technologies, user
experience enhancement, and various interaction designs for XR in remote HRI,
providing insights into current trends and future directions. We also
identified potential gaps and opportunities for future research to improve
remote HRI systems through XR technology to guide and inform future XR and
robotics research.","['Xian Wang', 'Luyao Shen', 'Lik-Hang Lee']",2024-03-18T00:22:30Z,http://arxiv.org/abs/2403.11384v3,"['cs.HC', 'cs.RO']"
Experimental Studies of Metaverse Streaming,"Metaverse aims to construct a large, unified, immersive, and shared digital
realm by combining various technologies, namely XR (extended reality),
blockchain, and digital twin, among others. This article explores the Metaverse
from the perspective of multimedia communication by conducting and analyzing
real-world experiments on four different Metaverse platforms: VR (virtual
reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual
City. We first investigate the traffic patterns and network performance in the
three VR platforms. After raising the challenges of the Metaverse streaming and
investigating the potential methods to enhance Metaverse performance, we
propose a remote rendering architecture and verify its advantages through a
prototype involving the campus network and MR multimodal interaction by
comparison with local rendering.","['Haopeng Wang', 'Roberto Martinez-Velazquez', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",2024-03-22T14:57:12Z,http://arxiv.org/abs/2403.15256v1,"['cs.MM', 'cs.NI']"
Multiway Point Cloud Mosaicking with Diffusion and Global Optimization,"We introduce a novel framework for multiway point cloud mosaicking (named
Wednesday), designed to co-align sets of partially overlapping point clouds --
typically obtained from 3D scanners or moving RGB-D cameras -- into a unified
coordinate system. At the core of our approach is ODIN, a learned pairwise
registration algorithm that iteratively identifies overlaps and refines
attention scores, employing a diffusion-based process for denoising pairwise
correlation matrices to enhance matching accuracy. Further steps include
constructing a pose graph from all point clouds, performing rotation averaging,
a novel robust algorithm for re-estimating translations optimally in terms of
consensus maximization and translation optimization. Finally, the point cloud
rotations and positions are optimized jointly by a diffusion-based approach.
Tested on four diverse, large-scale datasets, our method achieves
state-of-the-art pairwise and multiway registration results by a large margin
on all benchmarks. Our code and models are available at
https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization.","['Shengze Jin', 'Iro Armeni', 'Marc Pollefeys', 'Daniel Barath']",2024-03-30T17:29:13Z,http://arxiv.org/abs/2404.00429v1,['cs.CV']
SARA: Smart AI Reading Assistant for Reading Comprehension,"SARA integrates Eye Tracking and state-of-the-art large language models in a
mixed reality framework to enhance the reading experience by providing
personalized assistance in real-time. By tracking eye movements, SARA
identifies the text segments that attract the user's attention the most and
potentially indicate uncertain areas and comprehension issues. The process
involves these key steps: text detection and extraction, gaze tracking and
alignment, and assessment of detected reading difficulty. The results are
customized solutions presented directly within the user's field of view as
virtual overlays on identified difficult text areas. This support enables users
to overcome challenges like unfamiliar vocabulary and complex sentences by
offering additional context, rephrased solutions, and multilingual help. SARA's
innovative approach demonstrates it has the potential to transform the reading
experience and improve reading proficiency.","['Enkeleda Thaqi', 'Mohamed Mantawy', 'Enkelejda Kasneci']",2024-04-10T10:57:18Z,http://arxiv.org/abs/2404.06906v1,['cs.HC']
"MixLight: Borrowing the Best of both Spherical Harmonics and Gaussian
  Models","Accurately estimating scene lighting is critical for applications such as
mixed reality. Existing works estimate illumination by generating illumination
maps or regressing illumination parameters. However, the method of generating
illumination maps has poor generalization performance and parametric models
such as Spherical Harmonic (SH) and Spherical Gaussian (SG) fall short in
capturing high-frequency or low-frequency components. This paper presents
MixLight, a joint model that utilizes the complementary characteristics of SH
and SG to achieve a more complete illumination representation, which uses SH
and SG to capture low-frequency ambient and high-frequency light sources
respectively. In addition, a special spherical light source sparsemax
(SLSparsemax) module that refers to the position and brightness relationship
between spherical light sources is designed to improve their sparsity, which is
significant but omitted by prior works. Extensive experiments demonstrate that
MixLight surpasses state-of-the-art (SOTA) methods on multiple metrics. In
addition, experiments on Web Dataset also show that MixLight as a parametric
method has better generalization performance than non-parametric methods.","['Xinlong Ji', 'Fangneng Zhan', 'Shijian Lu', 'Shi-Sheng Huang', 'Hua Huang']",2024-04-19T10:17:10Z,http://arxiv.org/abs/2404.12768v1,"['cs.CV', 'cs.AI', 'cs.GR']"
"Protecting Human Users Against Cognitive Attacks in Immersive
  Environments","Integrating mixed reality (MR) with artificial intelligence (AI)
technologies, including vision, language, audio, reasoning, and planning,
enables the AI-powered MR assistant [1] to substantially elevate human
efficiency. This enhancement comes from situational awareness, quick access to
essential information, and support in learning new skills in the right context
throughout everyday tasks. This blend transforms interactions with both the
virtual and physical environments, catering to a range of skill levels and
personal preferences. For instance, computer vision enables the understanding
of the user's environment, allowing for the provision of timely and relevant
digital overlays in MR systems. At the same time, language models enhance
comprehension of contextual information and support voice-activated dialogue to
answer user questions. However, as AI-driven MR systems advance, they also
unveil new vulnerabilities, posing a threat to user safety by potentially
exposing them to grave dangers [5, 6].","['Yan-Ming Chiou', 'Bob Price', 'Chien-Chung Shen', 'Syed Ali Asif']",2024-04-23T17:42:15Z,http://arxiv.org/abs/2405.05919v1,['cs.HC']
A First Look at Immersive Telepresence on Apple Vision Pro,"Due to the widespread adoption of ""work-from-home"" policies,
videoconferencing applications (e.g., Zoom) have become indispensable for
remote communication. However, these systems lack immersiveness, leading to the
so-called ""Zoom fatigue"" and degrading communication efficiency. The recent
debut of Apple Vision Pro, a mixed reality headset that supports ""spatial
persona"", aims to offer an immersive telepresence experience with these
applications. In this paper, we conduct a first-of-its-kind in-depth and
empirical study to analyze the performance of immersive telepresence with four
applications, Apple FaceTime, Cisco Webex, Microsoft Teams, and Zoom, on Vision
Pro. We find that only FaceTime provides a truly immersive experience with
spatial personas, whereas other applications still operate 2D personas. Our
measurement results reveal that (1) FaceTime delivers semantic information to
optimize bandwidth consumption, which is even lower than that of 2D persona for
other applications, and (2) it employs visibility-aware optimizations to reduce
rendering overhead. However, the scalability of FaceTime remains limited, with
a simple server allocation strategy that potentially leads to high network
delay among users.","['Ruizhi Cheng', 'Nan Wu', 'Matteo Varvello', 'Eugene Chai', 'Songqing Chen', 'Bo Han']",2024-05-16T20:03:03Z,http://arxiv.org/abs/2405.10422v1,['cs.NI']
Distributed Technology-Sustained Pervasive Applications,"Technology-sustained pervasive games, contrary to technology-supported
pervasive games, can be understood as computer games interfacing with the
physical world. Pervasive games are known to make use of 'non-standard input
devices' and with the rise of the Internet of Things (IoT), pervasive
applications can be expected to move beyond games. This dissertation is
requirements- and development-focused Design Science research for distributed
technology-sustained pervasive applications, incorporating knowledge from the
domains of Distributed Computing, Mixed Reality, Context-Aware Computing,
Geographical Information Systems and IoT. Computer video games have existed for
decades, with a reusable game engine to drive them. If pervasive games can be
understood as computer games interfacing with the physical world, can computer
game engines be used to stage pervasive games? Considering the use of
non-standard input devices in pervasive games and the rise of IoT, how will
this affect the architectures supporting the broader set of pervasive
applications? The use of a game engine can be found in some existing pervasive
game projects, but general research into how the domain of pervasive games
overlaps with that of video games is lacking. When an engine is used, a
discussion of, what type of engine is most suitable and what properties are
being fulfilled by the engine, is often not part of the discourse. This
dissertation uses multiple iterations of the method framework for Design
Science for the design and development of three software system architectures.
In the face of IoT, the problem of extending pervasive games into a fourth
software architecture, accommodating a broader set of pervasive applications,
is explicated. The requirements, for technology-sustained pervasive games, are
verified through the design, development and demonstration of the three
software system architectures. The ...",['Kim J. L. Nevelsteen'],2016-04-11T11:33:20Z,http://arxiv.org/abs/1604.02892v1,"['cs.CY', 'cs.HC']"
"Exploring the Pathways of Adaptation an Avatar 3D Animation Procedures
  and Virtual Reality Arenas in Research of Human Courtship Behaviour and
  Sexual Reactivity in Psychological Research","There are many reasons for utilising 3D animation and virtual reality in
sexuality research. Apart from providing a mean with which to (re)experience
certain situations there are four main advantages: a) bespoke animated stimuli
can be created and customized, which is especially important when researching
paraphilia and sexual preferences, b) stimulus production is less expensive and
easier to produce compared to real world stimuli, c) virtual reality allows us
to capture data such as physiological reasons to stimuli, that we would not be
able to otherwise (without resorting to self-report measures which are
especially problematic in this research domain), d) ethical, legal, and health
and safety issues are less complex since neither physical nor psychological
harm is caused to animated characters allowing for the safe presentation of
stimuli involving vulnerable targets. The animation sub-group has been
exploring so far several production quality levels and various animation
procedures in a number of available software. The aim is to develop static as
well as dynamic, interactive sexual stimuli for sexual diagnostic and
therapeutic purposes. We are aware of number of ethical issues related to the
use of virtual reality in proposed research are analysed in this chapter.","['Jakub Binter', 'Kateřina Klapilová', 'Tereza Zikánová', 'Tommy Nilsson', 'Klára Bártová', 'Lucie Krejcová', 'Renata Androvicová', 'Jitka Lindová', 'Denisa Prušová', 'Timothy Wells', 'Daniel Riha']",2016-11-06T18:27:09Z,http://arxiv.org/abs/1611.01817v1,['cs.HC']
"Physics holo.lab learning experience: Using Smartglasses for Augmented
  Reality labwork to foster the concepts of heat conduction","Fundamental concepts of thermodynamics rely on abstract physical quantities
such as energy, heat and entropy, which play an important role in the process
of interpreting thermal phenomena and statistical mechanics. However, these
quantities are not covered by human (visual) perception and thus, an intuitive
understanding often is lacking. Today immersive technologies like head-mounted
displays of the newest generation, especially HoloLens, allow for high quality
augmented reality learning experiences, which can overcome this perception gap
and simultaneously avoid a split attention effect. In a mixed reality (MR)
scenario as presented in this paper---which we call a holo.lab---human
perception can be extended to the thermal regime by presenting false-color
representations of the temperature of objects as a virtual augmentation
directly on the real object itself in real-time. Direct feedback to
experimental actions of the users in form of different representations allows
for immediate comparison to theoretical principles and predictions and
therefore is supposed to intensify the theory-experiment interactions and to
increase the conceptual understanding. We tested this technology for an
experiment on thermal conduction of metals in the framework of undergraduate
laboratories. A pilot study with treatment and control groups (N = 59) showed a
small positive effect of MR on students' performance measured with a
standardized concept test for thermodynamics, indicating an improvement of the
understanding of the underlying physical concepts.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Klein', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-11-14T12:50:35Z,http://arxiv.org/abs/1711.05087v2,['physics.ed-ph']
"Generating Classes of 3D Virtual Mandibles for AR-Based Medical
  Simulation","Simulation and modeling represent promising tools for several application
domains from engineering to forensic science and medicine. Advances in 3D
imaging technology convey paradigms such as augmented reality (AR) and mixed
reality inside promising simulation tools for the training industry. Motivated
by the requirement for superimposing anatomically correct 3D models on a Human
Patient Simulator (HPS) and visualizing them in an AR environment, the purpose
of this research effort is to derive method for scaling a source human mandible
to a target human mandible. Results show that, given a distance between two
same landmarks on two different mandibles, a relative scaling factor may be
computed. Using this scaling factor, results show that a 3D virtual mandible
model can be made morphometrically equivalent to a real target-specific
mandible within a 1.30 millimeter average error bound. The virtual mandible may
be further used as a reference target for registering other anatomical models,
such as the lungs, on the HPS. Such registration will be made possible by
physical constraints among the mandible and the spinal column in the horizontal
normal rest position.","['Neha R. Hippalgaonkar', 'Alexa D. Sider', 'Felix G. Hamza-Lup', 'Anand P. Santhanam', 'Bala Jaganathan', 'Celina Imielinska', 'Jannick P. Rolland']",2018-11-20T03:29:56Z,http://arxiv.org/abs/1811.08053v1,['cs.GR']
"RetinaMatch: Efficient Template Matching of Retina Images for
  Teleophthalmology","Retinal template matching and registration is an important challenge in
teleophthalmology with low-cost imaging devices. However, the images from such
devices generally have a small field of view (FOV) and image quality
degradations, making matching difficult. In this work, we develop an efficient
and accurate retinal matching technique that combines dimension reduction and
mutual information (MI), called RetinaMatch. The dimension reduction
initializes the MI optimization as a coarse localization process, which narrows
the optimization domain and avoids local optima. The effectiveness of
RetinaMatch is demonstrated on the open fundus image database STARE with
simulated reduced FOV and anticipated degradations, and on retinal images
acquired by adapter-based optics attached to a smartphone. RetinaMatch achieves
a success rate over 94\% on human retinal images with the matched target
registration errors below 2 pixels on average, excluding the observer
variability. It outperforms the standard template matching solutions. In the
application of measuring vessel diameter repeatedly, single pixel errors are
expected. In addition, our method can be used in the process of image
mosaicking with area-based registration, providing a robust approach when the
feature based methods fail. To the best of our knowledge, this is the first
template matching algorithm for retina images with small template images from
unconstrained retinal areas. In the context of the emerging mixed reality
market, we envision automated retinal image matching and registration methods
as transformative for advanced teleophthalmology and long-term retinal
monitoring.","['Chen Gong', 'N. Benjamin Erichson', 'John P. Kelly', 'Laura Trutoiu', 'Brian T. Schowengerdt', 'Steven L. Brunton', 'Eric J. Seibel']",2018-11-28T23:06:54Z,http://arxiv.org/abs/1811.11874v1,"['eess.IV', 'cs.CV']"
"AirPen: A Touchless Fingertip Based Gestural Interface for Smartphones
  and Head-Mounted Devices","Hand gestures are an intuitive, socially acceptable, and a non-intrusive
interaction modality in Mixed Reality (MR) and smartphone based applications.
Unlike speech interfaces, they tend to perform well even in shared and public
spaces. Hand gestures can also be used to interact with smartphones in
situations where the user's ability to physically touch the device is impaired.
However, accurate gesture recognition can be achieved through state-of-the-art
deep learning models or with the use of expensive sensors. Despite the
robustness of these deep learning models, they are computationally heavy and
memory hungry, and obtaining real-time performance on-device without additional
hardware is still a challenge. To address this, we propose AirPen: an analogue
to pen on paper, but in air, for in-air writing and gestural commands that
works seamlessly in First and Second Person View. The models are trained on a
GPU machine and ported on an Android smartphone. AirPen comprises of three deep
learning models that work in tandem: MobileNetV2 for hand localisation, our
custom fingertip regression architecture followed by a Bi-LSTM model for
gesture classification. The overall framework works in real-time on mobile
devices and achieves a classification accuracy of 80% with an average latency
of only 0.12 s.","['Varun Jain', 'Ramya Hebbalaguppe']",2019-04-12T09:29:01Z,http://arxiv.org/abs/1904.06122v1,['cs.HC']
"GestARLite: An On-Device Pointing Finger Based Gestural Interface for
  Smartphones and Video See-Through Head-Mounts","Hand gestures form an intuitive means of interaction in Mixed Reality (MR)
applications. However, accurate gesture recognition can be achieved only
through state-of-the-art deep learning models or with the use of expensive
sensors. Despite the robustness of these deep learning models, they are
generally computationally expensive and obtaining real-time performance
on-device is still a challenge. To this end, we propose a novel lightweight
hand gesture recognition framework that works in First Person View for wearable
devices. The models are trained on a GPU machine and ported on an Android
smartphone for its use with frugal wearable devices such as the Google
Cardboard and VR Box. The proposed hand gesture recognition framework is driven
by a cascade of state-of-the-art deep learning models: MobileNetV2 for hand
localisation, our custom fingertip regression architecture followed by a
Bi-LSTM model for gesture classification. We extensively evaluate the framework
on our EgoGestAR dataset. The overall framework works in real-time on mobile
devices and achieves a classification accuracy of 80% on EgoGestAR video
dataset with an average latency of only 0.12 s.","['Varun Jain', 'Gaurav Garg', 'Ramakrishna Perla', 'Ramya Hebbalaguppe']",2019-04-19T14:32:40Z,http://arxiv.org/abs/1904.09843v1,['cs.CV']
"Conceptual Design and Preliminary Results of a VR-based Radiation Safety
  Training System for Interventional Radiologists","Recent studies have reported an increased risk of developing brain and neck
tumors, as well as cataracts, in practitioners in interventional radiology
(IR). Occupational radiation protection in IR has been a top concern for
regulatory agencies and professional societies. To help minimize occupational
radiation exposure in IR, we conceptualized a virtual reality (VR) based
radiation safety training system to help operators understand complex radiation
fields and to avoid high radiation areas through game-like interactive
simulations. The preliminary development of the system has yielded results
suggesting that the training system can calculate and report the radiation
exposure after each training session based on a database precalculated from
computational phantoms and Monte Carlo simulations and the position information
provided in real-time by the MS Hololens headset worn by trainee. In addition,
real-time dose rate and cumulative dose will be displayed to the trainee by MS
Hololens to help them adjust their practice. This paper presents the conceptual
design of the overall hardware and software design, as well as preliminary
results to combine MS HoloLens headset and complex 3D X-ray field spatial
distribution data to create a mixed reality environment for safety training
purpose in IR.","['Yi Guo', 'Li Mao', 'Gongsen Zhang', 'Zhi Chen', 'Xi Pei', 'X. George Xu']",2020-01-14T15:02:47Z,http://arxiv.org/abs/2001.04839v1,"['physics.med-ph', 'cs.HC']"
"Toward the Internet of No Things: The Role of O2O Communications and
  Extended Reality","Future fully interconnected virtual reality (VR) systems and the Tactile
Internet diminish the boundary between virtual (online) and real (offline)
worlds, while extending the digital and physical capabilities of humans via
edge computing and teleoperated robots, respectively. In this paper, we focus
on the Internet of No Things as an extension of immersive VR from virtual to
real environments, where human-intended Internet services - either digital or
physical - appear when needed and disappear when not needed. We first introduce
the concept of integrated online-to-offline (O2O) communications, which treats
online and offline channels as complementary to bridge the virtual and physical
worlds and provide O2O multichannel experiences. We then elaborate on the
emerging extended reality (XR), which brings the different forms of
virtual/augmented/mixed reality together to realize the entire
reality-virtuality continuum and, more importantly, supports human-machine
interaction as envisioned by the Tactile Internet, while posing challenges to
conventional handhelds, e.g., smartphones. Building on the so-called
invisible-to-visible (I2V) technology concept, we present our extrasensory
perception network (ESPN) and investigate how O2O communications and XR can be
combined for the nonlocal extension of human ""sixth-sense"" experiences in space
and time. We conclude by putting our ideas in perspective of the 6G vision.","['Martin Maier', 'Amin Ebrahimzadeh']",2019-06-16T17:41:23Z,http://arxiv.org/abs/1906.06738v1,['cs.NI']
LIME: Live Intrinsic Material Estimation,"We present the first end to end approach for real time material estimation
for general object shapes with uniform material that only requires a single
color image as input. In addition to Lambertian surface properties, our
approach fully automatically computes the specular albedo, material shininess,
and a foreground segmentation. We tackle this challenging and ill posed inverse
rendering problem using recent advances in image to image translation
techniques based on deep convolutional encoder decoder architectures. The
underlying core representations of our approach are specular shading, diffuse
shading and mirror images, which allow to learn the effective and accurate
separation of diffuse and specular albedo. In addition, we propose a novel
highly efficient perceptual rendering loss that mimics real world image
formation and obtains intermediate results even during run time. The estimation
of material parameters at real time frame rates enables exciting mixed reality
applications, such as seamless illumination consistent integration of virtual
objects into real world scenes, and virtual material cloning. We demonstrate
our approach in a live setup, compare it to the state of the art, and
demonstrate its effectiveness through quantitative and qualitative evaluation.","['Abhimitra Meka', 'Maxim Maximov', 'Michael Zollhoefer', 'Avishek Chatterjee', 'Hans-Peter Seidel', 'Christian Richardt', 'Christian Theobalt']",2018-01-03T16:55:31Z,http://arxiv.org/abs/1801.01075v2,['cs.CV']
Cost-benefit Analysis of Visualization in Virtual Environments,"Visualization and virtual environments (VEs) have been two interconnected
parallel strands in visual computing for decades. Some VEs have been purposely
developed for visualization applications, while many visualization applications
are exemplary showcases in general-purpose VEs. Because of the development and
operation costs of VEs, the majority of visualization applications in practice
are yet to benefit from the capacity of VEs. In this paper, we examine this
perplexity from an information-theoretic perspective. Our objectives are to
conduct cost-benefit analysis on typical VE systems (including augmented and
mixed reality, theatre-based systems, and large powerwalls), to explain why
some visualization applications benefit more from VEs than others, and to
sketch out pathways for the future development of visualization applications in
VEs. We support our theoretical propositions and analysis using theories and
discoveries in the literature of cognitive sciences and the practical evidence
reported in the literatures of visualization and VEs.","['Min Chen', 'Kelly Gaither', 'Nigel W. John', 'Brian McCann']",2018-02-25T14:14:42Z,http://arxiv.org/abs/1802.09012v2,"['cs.HC', 'cs.GR']"
"Virtualized Application Function Chaining: Maximizing the Wearable
  System Lifetime","The number of smart devices wear and carry by users is growing rapidly which
is driven by innovative new smart wearables and interesting service o erings.
This has led to applications that utilize multiple devices around the body to
provide immersive environments such as mixed reality. These applications rely
on a number of di erent types of functions such as sensing, communication and
various types of processing, that require considerable resources. Thus one of
the major challenges in supporting of these applications is dependent on the
battery lifetime of the devices that provide the necessary functionality. The
battery lifetime can be extended by either incorporating a battery with larger
capacity and/or by utilizing the available resources e ciently. However, the
increases in battery capacity are not keeping up with the demand and larger
batteries add to both the weight and size of the device. Thus, the focus of
this paper is to improve the battery e ciency through intelligent resources
utilization. We show that, when the same resource is available on multiple
devices that form part of the wearable system, and or is in close proximity, it
is possible consider them as a resource pool and further utilize them
intelligently to improve the system lifetime. Speci cally, we formulate the
function allocation algorithm as a Mixed Integer Linear Programming (MILP)
optimization problem and propose an e cient heuristic solution. The
experimental data driven simulation results show that approximately 40-50%
system battery life improvement can be achieved with proper function allocation
and orchestration.","['Harini Kolamunna', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-04-02T21:43:59Z,http://arxiv.org/abs/1804.00739v1,['cs.NI']
"Offline and Online calibration of Mobile Robot and SLAM Device for
  Navigation","Robot navigation technology is required to accomplish difficult tasks in
various environments. In navigation, it is necessary to know the information of
the external environments and the state of the robot under the environment. On
the other hand, various studies have been done on SLAM technology, which is
also used for navigation, but also applied to devices for Mixed Reality and the
like.
  In this paper, we propose a robot-device calibration method for navigation
with a device using SLAM technology on a robot. The calibration is performed by
using the position and orientation information given by the robot and the
device. In the calibration, the most efficient way of movement is clarified
according to the restriction of the robot movement. Furthermore, we also show a
method to dynamically correct the position and orientation of the robot so that
the information of the external environment and the shape information of the
robot maintain consistency in order to reduce the dynamic error occurring
during navigation.
  Our method can be easily used for various kinds of robots and localization
with sufficient precision for navigation is possible with offline calibration
and online position correction. In the experiments, we confirm the parameters
obtained by two types of offline calibration according to the degree of freedom
of robot movement and validate the effectiveness of online correction method by
plotting localized position error during robot's intense movement. Finally, we
show the demonstration of navigation using SLAM device.","['Ryoichi Ishikawa', 'Takeshi Oishi', 'Katsushi Ikeuchi']",2018-04-13T07:48:44Z,http://arxiv.org/abs/1804.04817v1,"['cs.CV', 'cs.RO']"
"Normalized Object Coordinate Space for Category-Level 6D Object Pose and
  Size Estimation","The goal of this paper is to estimate the 6D pose and dimensions of unseen
object instances in an RGB-D image. Contrary to ""instance-level"" 6D pose
estimation tasks, our problem assumes that no exact object CAD models are
available during either training or testing time. To handle different and
unseen object instances in a given category, we introduce a Normalized Object
Coordinate Space (NOCS)---a shared canonical representation for all possible
object instances within a category. Our region-based neural network is then
trained to directly infer the correspondence from observed pixels to this
shared object representation (NOCS) along with other object information such as
class label and instance mask. These predictions can be combined with the depth
map to jointly estimate the metric 6D pose and dimensions of multiple objects
in a cluttered scene. To train our network, we present a new context-aware
technique to generate large amounts of fully annotated mixed reality data. To
further improve our model and evaluate its performance on real data, we also
provide a fully annotated real-world dataset with large environment and
instance variation. Extensive experiments demonstrate that the proposed method
is able to robustly estimate the pose and size of unseen object instances in
real environments while also achieving state-of-the-art performance on standard
6D pose estimation benchmarks.","['He Wang', 'Srinath Sridhar', 'Jingwei Huang', 'Julien Valentin', 'Shuran Song', 'Leonidas J. Guibas']",2019-01-09T23:31:40Z,http://arxiv.org/abs/1901.02970v2,['cs.CV']
Maps and Globes in Virtual Reality,"This paper explores different ways to render world-wide geographic maps in
virtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's
viewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c)
an egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved
map, created by projecting the map onto a section of a sphere which curves
around the user. In all four visualisations the geographic centre can be
smoothly adjusted with a standard handheld VR controller and the user, through
a head-tracked headset, can physically move around the visualisation. For
distance comparison, exocentric globe is more accurate than egocentric globe
and flat map. For area comparison, more time is required with exocentric and
egocentric globes than with flat and curved maps. For direction estimation, the
exocentric globe is more accurate and faster than the other visual
presentations. Our study participants had a weak preference for the exocentric
globe. Generally, the curved map had benefits over the flat map. In almost all
cases the egocentric globe was found to be the least effective visualisation.
Overall, our results provide support for the use of exocentric globes for
geographic visualisation in mixed-reality.","['Yalong Yang', 'Bernhard Jenny', 'Tim Dwyer', 'Kim Marriott', 'Haohui Chen', 'Maxime Cordeil']",2019-08-06T11:45:51Z,http://arxiv.org/abs/1908.02088v1,"['cs.HC', 'cs.GR', 'cs.MM']"
Mixing realities for sketch retrieval in Virtual Reality,"Drawing tools for Virtual Reality (VR) enable users to model 3D designs from
within the virtual environment itself. These tools employ sketching and
sculpting techniques known from desktop-based interfaces and apply them to
hand-based controller interaction. While these techniques allow for mid-air
sketching of basic shapes, it remains difficult for users to create detailed
and comprehensive 3D models. In our work, we focus on supporting the user in
designing the virtual environment around them by enhancing sketch-based
interfaces with a supporting system for interactive model retrieval. Through
sketching, an immersed user can query a database containing detailed 3D models
and replace them into the virtual environment. To understand supportive
sketching within a virtual environment, we compare different methods of sketch
interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D
sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet.
%using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and
3D mid-air sketching. Our results show that 3D mid-air sketching is considered
to be a more intuitive method to search a collection of models while the
addition of physical devices creates confusion due to the complications of
their inclusion within a virtual environment. While we pose our work as a
retrieval problem for 3D models of chairs, our results can be extrapolated to
other sketching tasks for virtual environments.","['Daniele Giunchi', 'Stuart james', 'Donald Degraen', 'Anthony Steed']",2019-10-25T11:52:25Z,http://arxiv.org/abs/1910.11637v2,"['cs.HC', 'cs.CV']"
Rig-space Neural Rendering,"Movie productions use high resolution 3d characters with complex proprietary
rigs to create the highest quality images possible for large displays.
Unfortunately, these 3d assets are typically not compatible with real-time
graphics engines used for games, mixed reality and real-time pre-visualization.
Consequently, the 3d characters need to be re-modeled and re-rigged for these
new applications, requiring weeks of work and artistic approval. Our solution
to this problem is to learn a compact image-based rendering of the original 3d
character, conditioned directly on the rig parameters. Our idea is to render
the character in many different poses and views, and to train a deep neural
network to render high resolution images, from the rig parameters directly.
Many neural rendering techniques have been proposed to render from 2d
skeletons, or geometry and UV maps. However these require manual work, and to
do not remain compatible with the animator workflow of manipulating rig
widgets, as well as the real-time game engine pipeline of interpolating rig
parameters. We extend our architecture to support dynamic re-lighting and
composition with other 3d objects in the scene. We designed a network that
efficiently generates multiple scene feature maps such as normals, depth,
albedo and mask, which are composed with other scene objects to form the final
image.","['Dominik Borer', 'Lu Yuhang', 'Laura Wuelfroth', 'Jakob Buhmann', 'Martin Guay']",2020-03-22T06:28:22Z,http://arxiv.org/abs/2003.09820v1,['cs.GR']
"Enhanced Self-Perception in Mixed Reality: Egocentric Arm Segmentation
  and Database with Automatic Labelling","In this study, we focus on the egocentric segmentation of arms to improve
self-perception in Augmented Virtuality (AV). The main contributions of this
work are: i) a comprehensive survey of segmentation algorithms for AV; ii) an
Egocentric Arm Segmentation Dataset, composed of more than 10, 000 images,
comprising variations of skin color, and gender, among others. We provide all
details required for the automated generation of groundtruth and semi-synthetic
images; iii) the use of deep learning for the first time for segmenting arms in
AV; iv) to showcase the usefulness of this database, we report results on
different real egocentric hand datasets, including GTEA Gaze+, EDSH, EgoHands,
Ego Youtube Hands, THU-Read, TEgO, FPAB, and Ego Gesture, which allow for
direct comparisons with existing approaches utilizing color or depth. Results
confirm the suitability of the EgoArm dataset for this task, achieving
improvement up to 40% with respect to the original network, depending on the
particular dataset. Results also suggest that, while approaches based on color
or depth can work in controlled conditions (lack of occlusion, uniform
lighting, only objects of interest in the near range, controlled background,
etc.), egocentric segmentation based on deep learning is more robust in real AV
applications.","['Ester Gonzalez-Sosa', 'Pablo Perez', 'Ruben Tolosana', 'Redouane Kachach', 'Alvaro Villegas']",2020-03-27T12:09:27Z,http://arxiv.org/abs/2003.12352v1,['cs.CV']
Lookup tables for phase randomisation in hardware generated holograms,"The rise in virtual and mixed reality systems has prompted a resurgence of
interest in two-dimensional and three-dimensional real-time computer generated
holography. Phase randomisation is an integral part of holographic projection
as it ensures independence in sub-frame techniques and reduces the edge
enhancement seen in flat-phase images. Phase randomisation requires, however,
the availability of a pseudo-random number generator as well as trigonometric
functions such as cos and sin. On embedded devices such as field programmable
gate arrays and digital signal processors this can be an unacceptable load and
necessitate the use of proprietary intellectual property cores. Lookup tables
are able to reduce the computational load but can run to many megabytes for
even low-resolution systems.
  This paper introduces the use of lookup tables (LUTs) in the context of two
common algorithms used for real-time holographic projection: Gerchberg-Saxton
and One-Step Phase-Retrieval. A simulated study is carried out to investigate
the use of relatively small lookup tables where random numbers are repeated in
sequence. We find that the increase in error is low and tunable to under 5\%
even for small look up tables. This result is also demonstrated experimentally.
Finally, the implications of this study are discussed and conclusions drawn.","['Peter J. Christopher', 'Timothy D. Wilkinson']",2020-03-31T11:31:55Z,http://arxiv.org/abs/2004.04049v1,"['eess.SP', 'eess.IV']"
"Anchors Based Method for Fingertips Position Estimation from a Monocular
  RGB Image using Deep Neural Network","In Virtual, augmented, and mixed reality, the use of hand gestures is
increasingly becoming popular to reduce the difference between the virtual and
real world. The precise location of the fingertip is essential/crucial for a
seamless experience. Much of the research work is based on using depth
information for the estimation of the fingertips position. However, most of the
work using RGB images for fingertips detection is limited to a single finger.
The detection of multiple fingertips from a single RGB image is very
challenging due to various factors. In this paper, we propose a deep neural
network (DNN) based methodology to estimate the fingertips position. We
christened this methodology as an Anchor based Fingertips Position Estimation
(ABFPE), and it is a two-step process. The fingertips location is estimated
using regression by computing the difference in the location of a fingertip
from the nearest anchor point. The proposed framework performs the best with
limited dependence on hand detection results. In our experiments on the
SCUT-Ego-Gesture dataset, we achieved the fingertips detection error of 2.3552
pixels on a video frame with a resolution of $640 \times 480$ and about
$92.98\%$ of test images have average pixel errors of five pixels.","['Purnendu Mishra', 'Kishor Sarawadekar']",2020-05-04T09:45:56Z,http://arxiv.org/abs/2005.01351v2,"['cs.CV', 'cs.HC', 'eess.IV']"
Deep Lighting Environment Map Estimation from Spherical Panoramas,"Estimating a scene's lighting is a very important task when compositing
synthetic content within real environments, with applications in mixed reality
and post-production. In this work we present a data-driven model that estimates
an HDR lighting environment map from a single LDR monocular spherical panorama.
In addition to being a challenging and ill-posed problem, the lighting
estimation task also suffers from a lack of facile illumination ground truth
data, a fact that hinders the applicability of data-driven methods. We approach
this problem differently, exploiting the availability of surface geometry to
employ image-based relighting as a data generator and supervision mechanism.
This relies on a global Lambertian assumption that helps us overcome issues
related to pre-baked lighting. We relight our training data and complement the
model's supervision with a photometric loss, enabled by a differentiable
image-based relighting technique. Finally, since we predict spherical spectral
coefficients, we show that by imposing a distribution prior on the predicted
coefficients, we can greatly boost performance. Code and models available at
https://vcl3d.github.io/DeepPanoramaLighting.","['Vasileios Gkitsas', 'Nikolaos Zioulis', 'Federico Alvarez', 'Dimitrios Zarpalas', 'Petros Daras']",2020-05-16T14:23:05Z,http://arxiv.org/abs/2005.08000v1,"['cs.CV', 'cs.GR']"
"A survey on applications of augmented, mixed and virtual reality for
  nature and environment","Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are
technologies of great potential due to the engaging and enriching experiences
they are capable of providing. Their use is rapidly increasing in diverse
fields such as medicine, manufacturing or entertainment. However, the
possibilities that AR, VR and MR offer in the area of environmental
applications are not yet widely explored. In this paper we present the outcome
of a survey meant to discover and classify existing AR/VR/MR applications that
can benefit the environment or increase awareness on environmental issues. We
performed an exhaustive search over several online publication access platforms
and past proceedings of major conferences in the fields of AR/VR/MR. Identified
relevant papers were filtered based on novelty, technical soundness, impact and
topic relevance, and classified into different categories. Referring to the
selected papers, we discuss how the applications of each category are
contributing to environmental protection, preservation and sensitization
purposes. We further analyse these approaches as well as possible future
directions in the scope of existing and upcoming AR/VR/MR enabling
technologies.","['Jason Rambach', 'Gergana Lilligreen', 'Alexander Schäfer', 'Ramya Bankanal', 'Alexander Wiebel', 'Didier Stricker']",2020-08-27T09:59:27Z,http://arxiv.org/abs/2008.12024v2,"['cs.HC', 'cs.CV', 'cs.CY', 'cs.GT']"
HoloGen: An open source toolbox for high-speed hologram generation,"The rise of mixed reality systems such as Microsoft HoloLens has prompted an
increase in interest in the fields of 2D and 3D holography. Already applied in
fields including telecommunications, imaging, projection, lithography, beam
shaping and optical tweezing, Computer Generated Holography (CGH) offers an
exciting approach to a wide range of light shaping problems. The numerical
processing required to generate a hologram is high and requires significant
domain expertise. This has historically slowed the adoption of holographic
techniques in emerging fields. In this paper we present HoloGen, an open-source
Cuda C and C ++ framework for computer generated holography. HoloGen unites,
for the first time, a wide array of existing hologram generation algorithms
with state of the art performance while attempting to remain intuitive and easy
to use. This is enabled by a C # and Windows Presentation Framework (WPF)
graphical user interface (GUI). A novel reflection based parameter hierarchy is
used to ensure ease of modification. Extensive use of C ++ templates based on
the Standard Template Library (STL), compile time flexibility is preserved
while maintaining runtime performance. The current release of HoloGen unites
implementations of well known generation algorithms including Gerchberg-Saxton
(GS), Liu-Taghizadeh (LT), Direct Search (DS), Simulated Annealing (SA) and
One-Step Phase-Retrieval (OSPR) with less known specialist variants including
Weighted GS and Adaptive OSPR. Benchmarking results are presented for several
key algorithms. The software is freely available under an MIT license.","['Peter J. Christopher', 'Andrew Kadis', 'George S. D. Gordon', 'Timothy D. Wilkinson']",2020-08-24T11:22:07Z,http://arxiv.org/abs/2008.12214v2,"['eess.IV', 'physics.optics']"
Cross-Descriptor Visual Localization and Mapping,"Visual localization and mapping is the key technology underlying the majority
of mixed reality and robotics systems. Most state-of-the-art approaches rely on
local features to establish correspondences between images. In this paper, we
present three novel scenarios for localization and mapping which require the
continuous update of feature representations and the ability to match across
different feature types. While localization and mapping is a fundamental
computer vision problem, the traditional setup supposes the same local features
are used throughout the evolution of a map. Thus, whenever the underlying
features are changed, the whole process is repeated from scratch. However, this
is typically impossible in practice, because raw images are often not stored
and re-building the maps could lead to loss of the attached digital content. To
overcome the limitations of current approaches, we present the first principled
solution to cross-descriptor localization and mapping. Our data-driven approach
is agnostic to the feature descriptor type, has low computational requirements,
and scales linearly with the number of description algorithms. Extensive
experiments demonstrate the effectiveness of our approach on state-of-the-art
benchmarks for a variety of handcrafted and learned features.","['Mihai Dusmanu', 'Ondrej Miksik', 'Johannes L. Schönberger', 'Marc Pollefeys']",2020-12-02T18:19:51Z,http://arxiv.org/abs/2012.01377v2,['cs.CV']
Evaluating User Experiences in Mixed Reality,"Measure user experience in MR (i.e., AR/VR) user studies is essential.
Researchers apply a wide range of measuring methods using objective (e.g.,
biosignals, time logging), behavioral (e.g., gaze direction, movement
amplitude), and subjective (e.g., standardized questionnaires) metrics. Many of
these measurement instruments were adapted from use-cases outside of MR but
have not been validated for usage in MR experiments. However, researchers are
faced with various challenges and design alternatives when measuring immersive
experiences. These challenges become even more diverse when running out-of-the
lab studies. Measurement methods of VR experience recently received much
attention. For example, research has started embedding questionnaires in the VE
for various applications, allowing users to stay closer to the ongoing
experience while filling out the survey. However, there is a diversity in the
interaction methods and practices on how the assessment procedure is conducted.
This diversity in methods underlines a missing shared agreement of standardized
measurement tools for VR experiences. AR research strongly orients on the
research methods from VR, e.g., using the same type of subjective
questionnaires. However, some crucial technical differences require careful
considerations during the evaluation. This workshop at CHI 2021 provides a
foundation to exchange expertise and address challenges and opportunities of
research methods in MR user studies. By this, our workshop launches a
discussion of research methods that should lead to standardizing assessment
methods in MR user studies. The outcomes of the workshop will be aggregated
into a collective special issue journal article.","['Dmitry Alexandrovsky', 'Susanne Putze', 'Valentin Schwind', 'Elisa D. Mekler', 'Jan David Smeddinck', 'Denise Kahl', 'Antonio Krüger', 'Rainer Malaka']",2021-01-16T12:38:01Z,http://arxiv.org/abs/2101.06444v1,['cs.HC']
"Defining Preferred and Natural Robot Motions in Immersive Telepresence
  from a First-Person Perspective","This paper presents some early work and future plans regarding how the
autonomous motions of a telepresence robot affect a person embodied in the
robot through a head-mounted display. We consider the preferences, comfort, and
the perceived naturalness of aspects of piecewise linear paths compared to the
same aspects on a smooth path. In a user study, thirty-six subjects (eighteen
females) watched panoramic videos of three different paths through a simulated
museum in virtual reality and responded to questionnaires regarding each path.
We found that comfort had a strong effect on path preference, and that the
subjective feeling of naturalness also had a strong effect on path preference,
even though people consider different things as natural. We describe a
categorization of the responses regarding the naturalness of the robot's motion
and provide a recommendation on how this can be applied more broadly. Although
immersive robotic telepresence is increasingly being used for remote education,
clinical care, and to assist people with disabilities or mobility
complications, the full potential of this technology is limited by issues
related to user experience. Our work addresses these shortcomings and will
enable the future personalization of telepresence experiences for the
improvement of overall remote communication and the enhancement of the feeling
of presence in a remote location.","['Katherine J. Mimnaugh', 'Markku Suomalainen', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-02-25T07:40:23Z,http://arxiv.org/abs/2102.12719v1,['cs.RO']
Experiences with User Studies in Augmented Reality,"The research field of augmented reality (AR) is of increasing popularity, as
seen, among others, in several recently published surveys. To produce further
advancements in AR, it is not only necessary to create new systems or
applications, but also to evaluate them. One important aspect in regards to the
evaluation is the general understanding of how users experience a given AR
application, which can also be seen by the increased number of papers focusing
on this topic that were published in the last years. With the steadily growing
understanding and development of AR in general, it is only a matter of time
until AR devices make the leap into the consumer market where such an in-depth
user understanding is even more essential. Thus, a better understanding of
factors that could influence the design and results of user experience studies
can help us to make them more robust and dependable in the future.
  In this position paper, we describe three challenges which researchers face
while designing and conducting AR users studies. We encountered these
challenges in our past and current research, including papers that focus on
perceptual studies of visualizations, interaction studies, and studies
exploring the use of AR applications and their design spaces.","['Marc Satkowski', 'Wolfgang Büschel', 'Raimund Dachselt']",2021-04-08T14:18:51Z,http://arxiv.org/abs/2104.03795v1,['cs.HC']
Neural RGB-D Surface Reconstruction,"Obtaining high-quality 3D reconstructions of room-scale scenes is of
paramount importance for upcoming applications in AR or VR. These range from
mixed reality applications for teleconferencing, virtual measuring, virtual
room planing, to robotic applications. While current volume-based view
synthesis methods that use neural radiance fields (NeRFs) show promising
results in reproducing the appearance of an object or scene, they do not
reconstruct an actual surface. The volumetric representation of the surface
based on densities leads to artifacts when a surface is extracted using
Marching Cubes, since during optimization, densities are accumulated along the
ray and are not used at a single sample point in isolation. Instead of this
volumetric representation of the surface, we propose to represent the surface
using an implicit function (truncated signed distance function). We show how to
incorporate this representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such as a Kinect. In
addition, we propose a pose and camera refinement technique which improves the
overall reconstruction quality. In contrast to concurrent work on integrating
depth priors in NeRF which concentrates on novel view synthesis, our approach
is able to reconstruct high-quality, metrical 3D reconstructions.","['Dejan Azinović', 'Ricardo Martin-Brualla', 'Dan B Goldman', 'Matthias Nießner', 'Justus Thies']",2021-04-09T18:00:01Z,http://arxiv.org/abs/2104.04532v3,['cs.CV']
DeepCompress: Efficient Point Cloud Geometry Compression,"Point clouds are a basic data type that is increasingly of interest as 3D
content becomes more ubiquitous. Applications using point clouds include
virtual, augmented, and mixed reality and autonomous driving. We propose a more
efficient deep learning-based encoder architecture for point clouds compression
that incorporates principles from established 3D object detection and image
compression architectures. Through an ablation study, we show that
incorporating the learned activation function from Computational Efficient
Neural Image Compression (CENIC) and designing more parameter-efficient
convolutional blocks yields dramatic gains in efficiency and performance. Our
proposed architecture incorporates Generalized Divisive Normalization
activations and propose a spatially separable InceptionV4-inspired block. We
then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized
Full Bodies dataset to evaluate our model's performance. Our proposed
modifications outperform the baseline approaches by a small margin in terms of
Bjontegard delta rate and PSNR values, yet reduces necessary encoder
convolution operations by 8 percent and reduces total encoder parameters by 20
percent. Our proposed architecture, when considered on its own, has a small
penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit
rate in Point to Plane Distance for the same peak signal-to-noise ratio.","['Ryan Killea', 'Yun Li', 'Saeed Bastani', 'Paul McLachlan']",2021-06-02T23:18:11Z,http://arxiv.org/abs/2106.01504v1,"['cs.CV', 'cs.GR', 'cs.LG', 'eess.IV']"
"3D Visualisations Should Not be Displayed Alone - Encouraging a Need for
  Multivocality in Visualisation","We believe that 3D visualisations should not be used alone; by coincidentally
displaying alternative views the user can gain the best understanding of all
situations. The different presentations signify manifold meanings and afford
different tasks. Natural 3D worlds implicitly tell many stories. For instance,
walking into a living room, seeing the TV, types of magazines, pictures on the
wall, tells us much about the occupiers: their occupation, standards of living,
taste in design, whether they have kids, and so on. How can we similarly create
rich and diverse 3D visualisation presentations? How can we create
visualisations that allow people to understand different stories from the data?
In a multivariate 2D visualisation a developer may coordinate and link many
views together to provide exploratory visualisation functionality. But how can
this be achieved in 3D and in immersive visualisations? Different visualisation
types, each have specific uses, and each has the potential to tell or evoke a
different story. Through several use-cases, we discuss challenges of 3D
visualisation, and present our argument for concurrent and coordinated
visualisations of alternative styles, and encourage developers to consider
using alternative representations with any 3D view, even if that view is
displayed in a virtual, augmented or mixed reality setup.","['J. C. Roberts', 'J. W. Mearman', 'P. W. S. Butcher', 'H. M. Al-Maneea', 'P. D. Ritsos']",2021-08-10T13:37:04Z,http://arxiv.org/abs/2108.04680v1,"['cs.HC', 'cs.GR', 'I.3.0; H.5.2; I.6.3; J.2']"
"Towards Efficient Point Cloud Graph Neural Networks Through
  Architectural Simplification","In recent years graph neural network (GNN)-based approaches have become a
popular strategy for processing point cloud data, regularly achieving
state-of-the-art performance on a variety of tasks. To date, the research
community has primarily focused on improving model expressiveness, with
secondary thought given to how to design models that can run efficiently on
resource constrained mobile devices including smartphones or mixed reality
headsets. In this work we make a step towards improving the efficiency of these
models by making the observation that these GNN models are heavily limited by
the representational power of their first, feature extracting, layer. We find
that it is possible to radically simplify these models so long as the feature
extraction layer is retained with minimal degradation to model performance;
further, we discover that it is possible to improve performance overall on
ModelNet40 and S3DIS by improving the design of the feature extractor. Our
approach reduces memory consumption by 20$\times$ and latency by up to
9.9$\times$ for graph layers in models such as DGCNN; overall, we achieve
speed-ups of up to 4.5$\times$ and peak memory reductions of 72.5%.","['Shyam A. Tailor', 'René de Jong', 'Tiago Azevedo', 'Matthew Mattina', 'Partha Maji']",2021-08-13T17:04:54Z,http://arxiv.org/abs/2108.06317v1,"['cs.CV', 'cs.LG']"
"FreeStyleGAN: Free-view Editable Portrait Rendering with the Camera
  Manifold","Current Generative Adversarial Networks (GANs) produce photorealistic
renderings of portrait images. Embedding real images into the latent space of
such models enables high-level image editing. While recent methods provide
considerable semantic control over the (re-)generated images, they can only
generate a limited set of viewpoints and cannot explicitly control the camera.
Such 3D camera control is required for 3D virtual and mixed reality
applications. In our solution, we use a few images of a face to perform 3D
reconstruction, and we introduce the notion of the GAN camera manifold, the key
element allowing us to precisely define the range of images that the GAN can
reproduce in a stable manner. We train a small face-specific neural implicit
representation network to map a captured face to this manifold and complement
it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show
how our approach - due to its precise camera control - enables the integration
of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g.,
stereo rendering or consistent insertion of faces in synthetic 3D environments.
Our solution proposes the first truly free-viewpoint rendering of realistic
faces at interactive rates, using only a small number of casual photos as
input, while simultaneously allowing semantic editing capabilities, such as
facial expression or lighting changes.","['Thomas Leimkühler', 'George Drettakis']",2021-09-20T08:59:21Z,http://arxiv.org/abs/2109.09378v1,"['cs.GR', 'cs.CV']"
"Struct-MRT: Immersive Learning and Teaching of Design and Verification
  in Structural Civil Engineering using Mixed Reality","Our goal is to transform traditional paper-based instruction into an
immersive lesson. This paper presents the conception, workflow and deployment
of two MR applications for verification of typical yet geometrically complex
structural members: a reinforced concrete corbel and a steel frame. The aim of
this research is threefold: (i) to develop and implement the technological
feasibility of such applications, (ii) to demonstrate possible use cases in the
context of structural engineering lectures and (iii) to evaluate the presented
MR examples and the future potential of such MR applications in structural
engineering lectures through a survey. The workflow and MR teaching
applications were developed with Apple's ARKit. The verification process was
reproduced in the MR applications based on conventional exercises taught on
paper. Users can navigate independently through the applications and review
every single step, including a true-to-scale, spatial representation of the
specific component as well as associated verification formulas in the
respective step. The applications were used to assess the demand and
expectations for immersive teaching techniques among students and instructors
through a survey. The participants were asked to test the MR applications on
their devices or watch pre-recorded video demonstrations, afterwards perception
was elicited through a questionnaire. The results of subsequent data analysis
show generally positive judgement of the MR application over the six questioned
categories (style, usefulness, ease of use, enjoyment, attitude as well as
intention towards using). The statistical analysis revealed (positivity) biases
for users with prior XR experience w.r.t. to usage and navigation, while
inexperienced users underlined increased enjoyment or excitement with this
learning format. The outlook covers identified shortcomings and future
developments in this field.","['Michael Kraus', 'Irfan Custovic', 'Walter Kaufmann']",2021-09-20T12:46:52Z,http://arxiv.org/abs/2109.09489v1,['cs.HC']
"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']",2022-01-13T16:54:36Z,http://arxiv.org/abs/2201.07003v1,['cs.CY']
"DeepMix: Mobility-aware, Lightweight, and Hybrid 3D Object Detection for
  Headsets","Mobile headsets should be capable of understanding 3D physical environments
to offer a truly immersive experience for augmented/mixed reality (AR/MR).
However, their small form-factor and limited computation resources make it
extremely challenging to execute in real-time 3D vision algorithms, which are
known to be more compute-intensive than their 2D counterparts. In this paper,
we propose DeepMix, a mobility-aware, lightweight, and hybrid 3D object
detection framework for improving the user experience of AR/MR on mobile
headsets. Motivated by our analysis and evaluation of state-of-the-art 3D
object detection models, DeepMix intelligently combines edge-assisted 2D object
detection and novel, on-device 3D bounding box estimations that leverage depth
data captured by headsets. This leads to low end-to-end latency and
significantly boosts detection accuracy in mobile scenarios. A unique feature
of DeepMix is that it fully exploits the mobility of headsets to fine-tune
detection results and boost detection accuracy. To the best of our knowledge,
DeepMix is the first 3D object detection that achieves 30 FPS (an end-to-end
latency much lower than the 100 ms stringent requirement of interactive AR/MR).
We implement a prototype of DeepMix on Microsoft HoloLens and evaluate its
performance via both extensive controlled experiments and a user study with 30+
participants. DeepMix not only improves detection accuracy by 9.1--37.3% but
also reduces end-to-end latency by 2.68--9.15x, compared to the baseline that
uses existing 3D object detection models.","['Yongjie Guan', 'Xueyu Hou', 'Nan Wu', 'Bo Han', 'Tao Han']",2022-01-15T05:50:18Z,http://arxiv.org/abs/2201.08812v2,['cs.CV']
"Standardization of Extended Reality (XR) over 5G and 5G-Advanced 3GPP
  New Radio","Extended Reality (XR) is one of the major innovations to be introduced in
5G/5G-Advanced communication systems. A combination of augmented reality,
virtual reality, and mixed reality, supplemented by cloud gaming, revisits the
way how humans interact with computers, networks, and each other. However,
efficient support of XR services imposes new challenges for existing and future
wireless networks. This article presents a tutorial on integrating support for
the XR into the 3GPP New Radio (NR), summarizing a range of activities handled
within various 3GPP Service and Systems Aspects (SA) and Radio Access Networks
(RAN) groups. The article also delivers a case study evaluating the performance
of different XR services in state-of-the-art NR Release 17. The paper concludes
with a vision of further enhancements to better support XR in future NR
releases and outlines open problems in this area.","['Margarita Gapeyenko', 'Vitaly Petrov', 'Stefano Paris', 'Andrea Marcano', 'Klaus I. Pedersen']",2022-03-04T11:17:34Z,http://arxiv.org/abs/2203.02242v3,['cs.NI']
Learning Online Multi-Sensor Depth Fusion,"Many hand-held or mixed reality devices are used with a single sensor for 3D
reconstruction, although they often comprise multiple sensors. Multi-sensor
depth fusion is able to substantially improve the robustness and accuracy of 3D
reconstruction methods, but existing techniques are not robust enough to handle
sensors which operate with diverse value ranges as well as noise and outlier
statistics. To this end, we introduce SenFuNet, a depth fusion approach that
learns sensor-specific noise and outlier statistics and combines the data
streams of depth frames from different sensors in an online fashion. Our method
fuses multi-sensor depth streams regardless of time synchronization and
calibration and generalizes well with little training data. We conduct
experiments with various sensor combinations on the real-world CoRBS and
Scene3D datasets, as well as the Replica dataset. Experiments demonstrate that
our fusion strategy outperforms traditional and recent online depth fusion
approaches. In addition, the combination of multiple sensors yields more robust
outlier handling and more precise surface reconstruction than the use of a
single sensor. The source code and data are available at
https://github.com/tfy14esa/SenFuNet.","['Erik Sandström', 'Martin R. Oswald', 'Suryansh Kumar', 'Silvan Weder', 'Fisher Yu', 'Cristian Sminchisescu', 'Luc Van Gool']",2022-04-07T10:45:32Z,http://arxiv.org/abs/2204.03353v2,['cs.CV']
BEHAVE: Dataset and Method for Tracking Human Object Interactions,"Modelling interactions between humans and objects in natural environments is
central to many applications including gaming, virtual and mixed reality, as
well as human behavior analysis and human-robot collaboration. This challenging
operation scenario requires generalization to vast number of objects, scenes,
and human actions. Unfortunately, there exist no such dataset. Moreover, this
data needs to be acquired in diverse natural environments, which rules out 4D
scanners and marker based capture systems. We present BEHAVE dataset, the first
full body human- object interaction dataset with multi-view RGBD frames and
corresponding 3D SMPL and object fits along with the annotated contacts between
them. We record around 15k frames at 5 locations with 8 subjects performing a
wide range of interactions with 20 common objects. We use this data to learn a
model that can jointly track humans and objects in natural environments with an
easy-to-use portable multi-camera setup. Our key insight is to predict
correspondences from the human and the object to a statistical body model to
obtain human-object contacts during interactions. Our approach can record and
track not just the humans and objects but also their interactions, modeled as
surface contacts, in 3D. Our code and data can be found at:
http://virtualhumans.mpi-inf.mpg.de/behave","['Bharat Lal Bhatnagar', 'Xianghui Xie', 'Ilya A. Petrov', 'Cristian Sminchisescu', 'Christian Theobalt', 'Gerard Pons-Moll']",2022-04-14T13:21:19Z,http://arxiv.org/abs/2204.06950v1,['cs.CV']
"Virtual and Augmented Reality-Based Assistive Interfaces for Upper-limb
  Prosthesis Control and Rehabilitation","Functional upper-limb prosthetic training can improve users performance in
controlling prostheses and has been incorporated into occupational therapy for
individuals in need. In recent years, virtual reality (VR) and augmented
reality (AR) technologies have been shown to be promising avenues to improve
the convenience of rehabilitative prosthesis training systems. However, it is
uncertain if the comprehensive efficacy and effectiveness of VR or AR assistive
tools are adequate compared to conventional prosthetic tools and if not,
whether enhancements can be made through incorporation of other technical
paradigms.
  This work first presents a mixed reality system we developed for prosthesis
control and training. Five able-bodied subjects are involved to perform
three-dimensional object manipulation tasks in analogous AR and VR
environments. Multiple evaluation metrics are applied to assess subjects
performances within the two paradigms. Based on the comparative analysis, we
find that VR-based environment promotes more efficient motion along with higher
task completion rate and path efficiency while AR paradigm allows subjects to
perform motor tasks with shorter time consumed. Another study is conducted to
evaluate the efficiency and feasibility of AR-facilitated prosthesis control
system compared to that in real-world and if any technical additions can be
applied to improve the AR-based system. Three able-bodied subjects were engaged
in the experiment to perform object manipulation tasks in a) physical
environment, b) AR-without-bypass environment, and c) AR-with-bypass
environment. Based on the results obtained from the assessment, we conclude
that while our AR-based system modestly lags behind the effectiveness of
physical systems, the study conducted using a bypass prosthesis suggests that
AR system has the potential to improve the efficacy of prosthesis control.",['Yinghe Sun'],2022-04-28T03:26:12Z,http://arxiv.org/abs/2205.02227v1,['cs.HC']
"Visual Guidance for User Placement in Avatar-Mediated Telepresence
  between Dissimilar Spaces","Rapid advances in technology gradually realize immersive mixed-reality (MR)
telepresence between distant spaces. This paper presents a novel visual
guidance system for avatar-mediated telepresence, directing users to optimal
placements that facilitate the clear transfer of gaze and pointing contexts
through remote avatars in dissimilar spaces, where the spatial relationship
between the remote avatar and the interaction targets may differ from that of
the local user. Representing the spatial relationship between the user/avatar
and interaction targets with angle-based interaction features, we assign
recommendation scores of sampled local placements as their maximum feature
similarity with remote placements. These scores are visualized as color-coded
2D sectors to inform the users of better placements for interaction with
selected targets. In addition, virtual objects of the remote space are
overlapped with the local space for the user to better understand the
recommendations. We examine whether the proposed score measure agrees with the
actual user perception of the partner's interaction context and find a score
threshold for recommendation through user experiments in virtual reality (VR).
A subsequent user study in VR investigates the effectiveness and perceptual
overload of different combinations of visualizations. Finally, we conduct a
user study in an MR telepresence scenario to evaluate the effectiveness of our
method in real-world applications.","['Dongseok Yang', 'Jiho Kang', 'Taehei Kim', 'Sung-Hee Lee']",2022-06-20T02:41:39Z,http://arxiv.org/abs/2206.09542v3,"['cs.HC', 'cs.GR']"
Inter-Frame Compression for Dynamic Point Cloud Geometry Coding,"Efficient point cloud compression is essential for applications like virtual
and mixed reality, autonomous driving, and cultural heritage. In this paper, we
propose a deep learning-based inter-frame encoding scheme for dynamic point
cloud geometry compression. We propose a lossy geometry compression scheme that
predicts the latent representation of the current frame using the previous
frame by employing a novel prediction network. Our proposed network utilizes
sparse convolutions with hierarchical multiscale 3D feature learning to encode
the current frame using the previous frame. We employ convolution on target
coordinates to map the latent representation of the previous frame to the
downsampled coordinates of the current frame to predict the current frame's
feature embedding. Our framework transmits the residual of the predicted
features and the actual features by compressing them using a learned
probabilistic factorized entropy model. At the receiver, the decoder
hierarchically reconstructs the current frame by progressively rescaling the
feature embedding. We compared our model to the state-of-the-art Video-based
Point Cloud Compression (V-PCC) and Geometry-based Point Cloud Compression
(G-PCC) schemes standardized by the Moving Picture Experts Group (MPEG). Our
method achieves more than 91% BD-Rate Bjontegaard Delta Rate) reduction against
G-PCC, more than 62% BD-Rate reduction against V-PCC intra-frame encoding mode,
and more than 52% BD-Rate savings against V-PCC P-frame-based inter-frame
encoding mode using HEVC.","['Anique Akhtar', 'Zhu Li', 'Geert Van der Auwera']",2022-07-25T22:17:19Z,http://arxiv.org/abs/2207.12554v1,"['cs.CV', 'cs.MM', 'eess.IV']"
"AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
  Sensing","Today's Mixed Reality head-mounted displays track the user's head pose in
world space as well as the user's hands for interaction in both Augmented
Reality and Virtual Reality scenarios. While this is adequate to support user
input, it unfortunately limits users' virtual representations to just their
upper bodies. Current systems thus resort to floating avatars, whose limitation
is particularly evident in collaborative settings. To estimate full-body poses
from the sparse input sources, prior work has incorporated additional trackers
and sensors at the pelvis or lower body, which increases setup complexity and
limits practical application in mobile settings. In this paper, we present
AvatarPoser, the first learning-based method that predicts full-body poses in
world coordinates using only motion input from the user's head and hands. Our
method builds on a Transformer encoder to extract deep features from the input
signals and decouples global motion from the learned local joint orientations
to guide pose estimation. To obtain accurate full-body motions that resemble
motion capture animations, we refine the arm joints' positions using an
optimization routine with inverse kinematics to match the original tracking
input. In our evaluation, AvatarPoser achieved new state-of-the-art results in
evaluations on large motion capture datasets (AMASS). At the same time, our
method's inference speed supports real-time operation, providing a practical
interface to support holistic avatar control and representation for Metaverse
applications.","['Jiaxi Jiang', 'Paul Streli', 'Huajian Qiu', 'Andreas Fender', 'Larissa Laich', 'Patrick Snape', 'Christian Holz']",2022-07-27T20:52:39Z,http://arxiv.org/abs/2207.13784v1,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.HC', '68T07, 68T45, 68U01', 'I.2; I.3; I.4; I.5']"
Harmonization and Evaluation; Tweaking the Parameters on Human Listeners,"Kansei models were used to study the connotative meaning of music. In
multimedia and mixed reality, automatically generated melodies are increasingly
being used. It is important to consider whether and what feelings are
communicated by this music. Evaluation of computer-generated melodies is not a
trivial task. Considered the difficulty of defining useful quantitative metrics
of the quality of a generated musical piece, researchers often resort to human
evaluation. In these evaluations, often the judges are required to evaluate a
set of generated pieces along with some benchmark pieces. The latter are often
composed by humans. While this kind of evaluation is relatively common, it is
known that care should be taken when designing the experiment, as humans can be
influenced by a variety of factors. In this paper, we examine the impact of the
presence of harmony in audio files that judges must evaluate, to see whether
having an accompaniment can change the evaluation of generated melodies. To do
so, we generate melodies with two different algorithms and harmonize them with
an automatic tool that we designed for this experiment, and ask more than sixty
participants to evaluate the melodies. By using statistical analyses, we show
harmonization does impact the evaluation process, by emphasizing the
differences among judgements.","['Filippo Carnovalini', 'Alessandro Pelizzo', 'Antonio Rodà', 'Sergio Canazza']",2022-08-31T09:54:06Z,http://arxiv.org/abs/2208.14750v1,"['cs.SD', 'cs.MM', 'eess.AS']"
"Haptic Feedback Relocation from the Fingertips to the Wrist for
  Two-Finger Manipulation in Virtual Reality","Relocation of haptic feedback from the fingertips to the wrist has been
considered as a way to enable haptic interaction with mixed reality virtual
environments while leaving the fingers free for other tasks. We present a pair
of wrist-worn tactile haptic devices and a virtual environment to study how
various mappings between fingers and tactors affect task performance. The
haptic feedback rendered to the wrist reflects the interaction forces occurring
between a virtual object and virtual avatars controlled by the index finger and
thumb. We performed a user study comparing four different finger-to-tactor
haptic feedback mappings and one no-feedback condition as a control. We
evaluated users' ability to perform a simple pick-and-place task via the
metrics of task completion time, path length of the fingers and virtual cube,
and magnitudes of normal and shear forces at the fingertips. We found that
multiple mappings were effective, and there was a greater impact when visual
cues were limited. We discuss the limitations of our approach and describe next
steps toward multi-degree-of-freedom haptic rendering for wrist-worn devices to
improve task performance in virtual environments.","['Jasmin E. Palmer', 'Mine Sarac', 'Aaron A. Garza', 'Allison M. Okamura']",2022-09-15T22:46:25Z,http://arxiv.org/abs/2209.07640v2,"['cs.HC', 'cs.RO']"
"A Novel Light Field Coding Scheme Based on Deep Belief Network &
  Weighted Binary Images for Additive Layered Displays","Light-field displays create an immersive experience by providing binocular
depth sensation and motion parallax. Stacking light attenuating layers is one
approach to implement a light field display with a broader depth of field, wide
viewing angles and high resolution. Due to the transparent holographic optical
element (HOE) layers, additive layered displays can be integrated into
augmented reality (AR) wearables to overlay virtual objects onto the real
world, creating a seamless mixed reality (XR) experience. This paper proposes a
novel framework for light field representation and coding that utilizes Deep
Belief Network (DBN) and weighted binary images suitable for additive layered
displays. The weighted binary representation of layers makes the framework more
flexible for adaptive bitrate encoding. The framework effectively captures
intrinsic redundancies in the light field data, and thus provides a scalable
solution for light field coding suitable for XR display applications. The
latent code is encoded by H.265 codec generating a rate-scalable bit-stream. We
achieve adaptive bitrate decoding by varying the number of weighted binary
images and the H.265 quantization parameter, while maintaining an optimal
reconstruction quality. The framework is tested on real and synthetic benchmark
datasets, and the results validate the rate-scalable property of the proposed
scheme.","['Sally Khaidem', 'Mansi Sharma']",2022-10-04T08:18:06Z,http://arxiv.org/abs/2210.01447v2,"['cs.CV', 'eess.IV']"
"A DirectX-Based DICOM Viewer for Multi-User Surgical Planning in
  Augmented Reality","Preoperative medical imaging is an essential part of surgical planning. The
data from medical imaging devices, such as CT and MRI scanners, consist of
stacks of 2D images in DICOM format. Conversely, advances in 3D data
visualization provide further information by assembling cross-sections into 3D
volumetric datasets. As Microsoft unveiled the HoloLens 2 (HL2), which is
considered one of the best Mixed Reality (XR) headsets in the market, it
promised to enhance visualization in 3D by providing an immersive experience to
users. This paper introduces a prototype holographic XR DICOM Viewer for the 3D
visualization of DICOM image sets on HL2 for surgical planning. We first
developed a standalone graphical C++ engine using the native DirectX11 API and
HLSL shaders. Based on that, the prototype further applies the OpenXR API for
potential deployment on a wide range of devices from vendors across the XR
spectrum. With native access to the device, our prototype unravels the
limitation of hardware capabilities on HL2 for 3D volume rendering and
interaction. Moreover, smartphones can act as input devices to provide another
user interaction method by connecting to our server. In this paper, we present
a holographic DICOM viewer for the HoloLens 2 and contribute (i) a prototype
that renders the DICOM image stacks in real-time on HL2, (ii) three types of
user interactions in XR, and (iii) a preliminary qualitative evaluation of our
prototype.","['Menghe Zhang', 'Weichen Liu', 'Nadir Weibel', 'Jurgen Schulze']",2022-10-25T21:22:00Z,http://arxiv.org/abs/2210.14349v1,"['cs.MM', 'cs.HC']"
Big Data Meets Metaverse: A Survey,"We are living in the era of big data. The Metaverse is an emerging technology
in the future, and it has a combination of big data, AI (artificial
intelligence), VR (Virtual Reality), AR (Augmented Reality), MR (mixed
reality), and other technologies that will diminish the difference between
online and real-life interaction. It has the goal of becoming a platform where
we can work, go shopping, play around, and socialize. Each user who enters the
Metaverse interacts with the virtual world in a data way. With the development
and application of the Metaverse, the data will continue to grow, thus forming
a big data network, which will bring huge data processing pressure to the
digital world. Therefore, big data processing technology is one of the key
technologies to implement the Metaverse. In this survey, we provide a
comprehensive review of how Metaverse is changing big data. Moreover, we
discuss the key security and privacy of Metaverse big data in detail. Finally,
we summarize the open problems and opportunities of Metaverse, as well as the
future of Metaverse with big data. We hope that this survey will provide
researchers with the research direction and prospects of applying big data in
the Metaverse.","['Jiayi Sun', 'Wensheng Gan', 'Zefeng Chen', 'Junhui Li', 'Philip S. Yu']",2022-10-28T17:22:20Z,http://arxiv.org/abs/2210.16282v1,"['cs.DB', 'cs.CY']"
Analyzing Performance Issues of Virtual Reality Applications,"Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)
and Mixed Reality (MR). XR is an emerging technology that simulates a realistic
environment for users. XR techniques have provided revolutionary user
experiences in various application scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). Due to the
high computational cost of rendering real-time animation in limited-resource
devices and constant interaction with user activity, XR applications often face
performance bottlenecks, and these bottlenecks create a negative impact on the
user experience of XR software. Thus, performance optimization plays an
essential role in many industry-standard XR applications. Even though
identifying performance bottlenecks in traditional software (e.g., desktop
applications) is a widely explored topic, those approaches cannot be directly
applied within XR software due to the different nature of XR applications.
Moreover, XR applications developed in different frameworks such as Unity and
Unreal Engine show different performance bottleneck patterns and thus,
bottleneck patterns of Unity projects can't be applied for Unreal Engine
(UE)-based XR projects. To fill the knowledge gap for XR performance
optimizations of Unreal Engine-based XR projects, we present the first
empirical study on performance optimizations from seven UE XR projects, 78 UE
XR discussion issues and three sources of UE documentation. Our analysis
identified 14 types of performance bugs, including 12 types of bugs related to
UE settings issues and two types of CPP source code-related issues. To further
assist developers in detecting performance bugs based on the identified bug
patterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect
performance bugs in both configuration files and source code.","['Jason Hogan', 'Aaron Salo', 'Dhia Elhaq Rzig', 'Foyzul Hassan', 'Bruce Maxim']",2022-11-03T17:27:36Z,http://arxiv.org/abs/2211.02013v1,['cs.SE']
Twin-S: A Digital Twin for Skull-base Surgery,"Purpose: Digital twins are virtual interactive models of the real world,
exhibiting identical behavior and properties. In surgical applications,
computational analysis from digital twins can be used, for example, to enhance
situational awareness. Methods: We present a digital twin framework for
skull-base surgeries, named Twin-S, which can be integrated within various
image-guided interventions seamlessly. Twin-S combines high-precision optical
tracking and real-time simulation. We rely on rigorous calibration routines to
ensure that the digital twin representation precisely mimics all real-world
processes. Twin-S models and tracks the critical components of skull-base
surgery, including the surgical tool, patient anatomy, and surgical camera.
Significantly, Twin-S updates and reflects real-world drilling of the
anatomical model in frame rate. Results: We extensively evaluate the accuracy
of Twin-S, which achieves an average 1.39 mm error during the drilling process.
We further illustrate how segmentation masks derived from the continuously
updated digital twin can augment the surgical microscope view in a mixed
reality setting, where bone requiring ablation is highlighted to provide
surgeons additional situational awareness. Conclusion: We present Twin-S, a
digital twin environment for skull-base surgery. Twin-S tracks and updates the
virtual model in real-time given measurements from modern tracking
technologies. Future research on complementing optical tracking with
higher-precision vision-based approaches may further increase the accuracy of
Twin-S.","['Hongchao Shu', 'Ruixing Liang', 'Zhaoshuo Li', 'Anna Goodridge', 'Xiangyu Zhang', 'Hao Ding', 'Nimesh Nagururu', 'Manish Sahu', 'Francis X. Creighton', 'Russell H. Taylor', 'Adnan Munawar', 'Mathias Unberath']",2022-11-21T21:33:51Z,http://arxiv.org/abs/2211.11863v2,"['cs.HC', 'cs.CV', 'cs.RO']"
"Performance Analysis of Free-Space Information Sharing in Full-Duplex
  Semantic Communications","In next-generation Internet services, such as Metaverse, the mixed reality
(MR) technique plays a vital role. Yet the limited computing capacity of the
user-side MR headset-mounted device (HMD) prevents its further application,
especially in scenarios that require a lot of computation. One way out of this
dilemma is to design an efficient information sharing scheme among users to
replace the heavy and repetitive computation. In this paper, we propose a
free-space information sharing mechanism based on full-duplex device-to-device
(D2D) semantic communications. Specifically, the view images of MR users in the
same real-world scenario may be analogous. Therefore, when one user (i.e., a
device) completes some computation tasks, the user can send his own calculation
results and the semantic features extracted from the user's own view image to
nearby users (i.e., other devices). On this basis, other users can use the
received semantic features to obtain the spatial matching of the computational
results under their own view images without repeating the computation. Using
generalized small-scale fading models, we analyze the key performance
indicators of full-duplex D2D communications, including channel capacity and
bit error probability, which directly affect the transmission of semantic
information. Finally, the numerical analysis experiment proves the
effectiveness of our proposed methods.","['Hongyang Du', 'Jiacheng Wang', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Dong In Kim', 'Boon Hee Soong']",2022-11-27T09:18:42Z,http://arxiv.org/abs/2211.14771v1,['eess.SP']
Fast and Lightweight Scene Regressor for Camera Relocalization,"Camera relocalization involving a prior 3D reconstruction plays a crucial
role in many mixed reality and robotics applications. Estimating the camera
pose directly with respect to pre-built 3D models can be prohibitively
expensive for several applications with limited storage and/or communication
bandwidth. Although recent scene and absolute pose regression methods have
become popular for efficient camera localization, most of them are
computation-resource intensive and difficult to obtain a real-time inference
with high accuracy constraints. This study proposes a simple scene regression
method that requires only a multi-layer perceptron network for mapping scene
coordinates to achieve accurate camera pose estimations. The proposed approach
uses sparse descriptors to regress the scene coordinates, instead of a dense
RGB image. The use of sparse features provides several advantages. First, the
proposed regressor network is substantially smaller than those reported in
previous studies. This makes our system highly efficient and scalable. Second,
the pre-built 3D models provide the most reliable and robust 2D-3D matches.
Therefore, learning from them can lead to an awareness of equivalent features
and substantially improve the generalization performance. A detailed analysis
of our approach and extensive evaluations using existing datasets are provided
to support the proposed method. The implementation detail is available at
https://github.com/aislab/feat2map","['Thuan B. Bui', 'Dinh-Tuan Tran', 'Joo-Ho Lee']",2022-12-04T14:41:20Z,http://arxiv.org/abs/2212.01830v1,"['cs.CV', 'cs.RO']"
"CNN-based real-time 2D-3D deformable registration from a single X-ray
  projection","Purpose: The purpose of this paper is to present a method for real-time 2D-3D
non-rigid registration using a single fluoroscopic image. Such a method can
find applications in surgery, interventional radiology and radiotherapy. By
estimating a three-dimensional displacement field from a 2D X-ray image,
anatomical structures segmented in the preoperative scan can be projected onto
the 2D image, thus providing a mixed reality view. Methods: A dataset composed
of displacement fields and 2D projections of the anatomy is generated from the
preoperative scan. From this dataset, a neural network is trained to recover
the unknown 3D displacement field from a single projection image. Results: Our
method is validated on lung 4D CT data at different stages of the lung
deformation. The training is performed on a 3D CT using random (non
domain-specific) diffeomorphic deformations, to which perturbations mimicking
the pose uncertainty are added. The model achieves a mean TRE over a series of
landmarks ranging from 2.3 to 5.5 mm depending on the amplitude of deformation.
Conclusion: In this paper, a CNN-based method for real-time 2D-3D non-rigid
registration is presented. This method is able to cope with pose estimation
uncertainties, making it applicable to actual clinical scenarios, such as lung
surgery, where the C-arm pose is planned before the intervention.","['François Lecomte', 'Jean-Louis Dillenseger', 'Stéphane Cotin']",2022-12-15T09:57:19Z,http://arxiv.org/abs/2212.07692v2,"['eess.IV', 'cs.CV']"
"Playing with Data: An Augmented Reality Approach to Interact with
  Visualizations of Industrial Process Tomography","Industrial process tomography (IPT) is a specialized imaging technique widely
used in industrial scenarios for process supervision and control. Today,
augmented/mixed reality (AR/MR) is increasingly being adopted in many
industrial occasions, even though there is still an obvious gap when it comes
to IPT. To bridge this gap, we propose the first systematic AR approach using
optical see-through (OST) head mounted displays (HMDs) with comparative
evaluation for domain users towards IPT visualization analysis. The
proof-of-concept was demonstrated by a within-subject user study (n=20) with
counterbalancing design. Both qualitative and quantitative measurements were
investigated. The results showed that our AR approach outperformed conventional
settings for IPT data visualization analysis in bringing higher
understandability, reduced task completion time, lower error rates for domain
tasks, increased usability with enhanced user experience, and a better
recommendation level. We summarize the findings and suggest future research
directions for benefiting IPT users with AR/MR.","['Yuchong Zhang', 'Yueming Xuan', 'Rahul Yadav', 'Adel Omrani', 'Morten Fjeld']",2023-02-03T12:19:01Z,http://arxiv.org/abs/2302.01686v5,['cs.HC']
"KuberneTSN: a Deterministic Overlay Network for Time-Sensitive
  Containerized Environments","The emerging paradigm of resource disaggregation enables the deployment of
cloud-like services across a pool of physical and virtualized resources,
interconnected using a network fabric. This design embodies several benefits in
terms of resource efficiency and cost-effectiveness, service elasticity and
adaptability, etc. Application domains benefiting from such a trend include
cyber-physical systems (CPS), tactile internet, 5G networks and beyond, or
mixed reality applications, all generally embodying heterogeneous Quality of
Service (QoS) requirements. In this context, a key enabling factor to fully
support those mixed-criticality scenarios will be the network and the
system-level support for time-sensitive communication. Although a lot of work
has been conducted on devising efficient orchestration and CPU scheduling
strategies, the networking aspects of performance-critical components remain
largely unstudied. Bridging this gap, we propose KuberneTSN, an original
solution built on the Kubernetes platform, providing support for time-sensitive
traffic to unmodified application binaries. We define an architecture for an
accelerated and deterministic overlay network, which includes kernel-bypassing
networking features as well as a novel userspace packet scheduler compliant
with the Time-Sensitive Networking (TSN) standard. The solution is implemented
as tsn-cni, a Kubernetes network plugin that can coexist alongside popular
alternatives. To assess the validity of the approach, we conduct an
experimental analysis on a real distributed testbed, demonstrating that
KuberneTSN enables applications to easily meet deterministic deadlines,
provides the same guarantees of bare-metal deployments, and outperforms overlay
networks built using the Flannel plugin.","['Andrea Garbugli', 'Lorenzo Rosa', 'Armir Bujari', 'Luca Foschini']",2023-02-16T16:16:28Z,http://arxiv.org/abs/2302.08398v1,"['cs.NI', 'cs.DC']"
"AI-Generated Incentive Mechanism and Full-Duplex Semantic Communications
  for Information Sharing","The next generation of Internet services, such as Metaverse, rely on mixed
reality (MR) technology to provide immersive user experiences. However, the
limited computation power of MR headset-mounted devices (HMDs) hinders the
deployment of such services. Therefore, we propose an efficient information
sharing scheme based on full-duplex device-to-device (D2D) semantic
communications to address this issue. Our approach enables users to avoid heavy
and repetitive computational tasks, such as artificial intelligence-generated
content (AIGC) in the view images of all MR users. Specifically, a user can
transmit the generated content and semantic information extracted from their
view image to nearby users, who can then use this information to obtain the
spatial matching of computation results under their view images. We analyze the
performance of full-duplex D2D communications, including the achievable rate
and bit error probability, by using generalized small-scale fading models. To
facilitate semantic information sharing among users, we design a contract
theoretic AI-generated incentive mechanism. The proposed diffusion model
generates the optimal contract design, outperforming two deep reinforcement
learning algorithms, i.e., proximal policy optimization and soft actor-critic
algorithms. Our numerical analysis experiment proves the effectiveness of our
proposed methods. The code for this paper is available at
https://github.com/HongyangDu/SemSharing","['Hongyang Du', 'Jiacheng Wang', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Dong In Kim']",2023-03-03T12:47:34Z,http://arxiv.org/abs/2303.01896v2,['eess.SP']
Modular 3D Interface Design for Accessible VR Applications,"Designed with an accessible first design approach, the presented paper
describes how exploiting humans proprioception ability in 3D space can result
in a more natural interaction experience when using a 3D graphical user
interface in a virtual environment. The modularity of the designed interface
empowers the user to decide where they want to place interface elements in 3D
space allowing for a highly customizable experience, both in the context of the
player and the virtual space. Drawing inspiration from todays tangible
interfaces used, such as those in aircraft cockpits, a modular interface is
presented taking advantage of our natural understanding of interacting with 3D
objects and exploiting capabilities that otherwise have not been used in 2D
interaction. Additionally, the designed interface supports multimodal input
mechanisms which also demonstrates the opportunity for the design to cross over
to augmented reality applications. A focus group study was completed to better
understand the usability and constraints of the designed 3D GUI.","['Corrie Green', 'Dr Yang Jiang', 'Dr John Isaacs']",2023-04-08T17:07:46Z,http://arxiv.org/abs/2304.10541v2,['cs.HC']
"Meta-Optimization for Higher Model Generalizability in Single-Image
  Depth Prediction","Model generalizability to unseen datasets, concerned with in-the-wild
robustness, is less studied for indoor single-image depth prediction. We
leverage gradient-based meta-learning for higher generalizability on zero-shot
cross-dataset inference. Unlike the most-studied image classification in
meta-learning, depth is pixel-level continuous range values, and mappings from
each image to depth vary widely across environments. Thus no explicit task
boundaries exist. We instead propose fine-grained task that treats each RGB-D
pair as a task in our meta-optimization. We first show meta-learning on limited
data induces much better prior (max +29.4\%). Using meta-learned weights as
initialization for following supervised learning, without involving extra data
or information, it consistently outperforms baselines without the method.
Compared to most indoor-depth methods that only train/ test on a single
dataset, we propose zero-shot cross-dataset protocols, closely evaluate
robustness, and show consistently higher generalizability and accuracy by our
meta-initialization. The work at the intersection of depth and meta-learning
potentially drives both research streams to step closer to practical use.","['Cho-Ying Wu', 'Yiqi Zhong', 'Junying Wang', 'Ulrich Neumann']",2023-05-12T06:17:13Z,http://arxiv.org/abs/2305.07269v2,['cs.CV']
Enhancing Transformer Backbone for Egocentric Video Action Segmentation,"Egocentric temporal action segmentation in videos is a crucial task in
computer vision with applications in various fields such as mixed reality,
human behavior analysis, and robotics. Although recent research has utilized
advanced visual-language frameworks, transformers remain the backbone of action
segmentation models. Therefore, it is necessary to improve transformers to
enhance the robustness of action segmentation models. In this work, we propose
two novel ideas to enhance the state-of-the-art transformer for action
segmentation. First, we introduce a dual dilated attention mechanism to
adaptively capture hierarchical representations in both local-to-global and
global-to-local contexts. Second, we incorporate cross-connections between the
encoder and decoder blocks to prevent the loss of local context by the decoder.
We also utilize state-of-the-art visual-language representation learning
techniques to extract richer and more compact features for our transformer. Our
proposed approach outperforms other state-of-the-art methods on the Georgia
Tech Egocentric Activities (GTEA) and HOI4D Office Tools datasets, and we
validate our introduced components with ablation studies. The source code and
supplementary materials are publicly available on
https://www.sail-nu.com/dxformer.","['Sakib Reza', 'Balaji Sundareshan', 'Mohsen Moghaddam', 'Octavia Camps']",2023-05-19T01:00:08Z,http://arxiv.org/abs/2305.11365v2,['cs.CV']
"Design Frameworks for Hyper-Connected Social XRI Immersive Metaverse
  Environments","The metaverse refers to the merger of technologies for providing a digital
twin of the real world and the underlying connectivity and interactions for the
many kinds of agents within. As this set of technology paradigms - involving
artificial intelligence, mixed reality, the internet-of-things and others -
gains in scale, maturity, and utility there are rapidly emerging design
challenges and new research opportunities. In particular is the metaverse
disconnect problem, the gap in task switching that inevitably occurs when a
user engages with multiple virtual and physical environments simultaneously.
Addressing this gap remains an open issue that affects the user experience and
must be overcome to increase overall utility of the metaverse. This article
presents design frameworks that consider how to address the metaverse as a
hyper-connected meta-environment that connects and expands multiple user
environments, modalities, contexts, and the many objects and relationships
within them. This article contributes to i) a framing of the metaverse as a
social XR-IoT (XRI) concept, ii) design Considerations for XRI metaverse
experiences, iii) a design architecture for social multi-user XRI metaverse
environments, and iv) descriptive exploration of social interaction scenarios
within XRI multi-user metaverses. These contribute a new design framework for
metaverse researchers and creators to consider the coming wave of
interconnected and immersive smart environments.","['Jie Guan', 'Alexis Morris']",2023-06-09T20:02:26Z,http://arxiv.org/abs/2306.06230v3,['cs.HC']
"Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities","In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.","['Kai Li', 'Billy Pik Lik Lau', 'Xin Yuan', 'Wei Ni', 'Mohsen Guizani', 'Chau Yuen']",2023-07-13T11:14:46Z,http://arxiv.org/abs/2307.06687v2,"['cs.HC', 'cs.AI', 'cs.NI']"
"POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation
  During Surgical Activities","The surgical usage of Mixed Reality (MR) has received growing attention in
areas such as surgical navigation systems, skill assessment, and robot-assisted
surgeries. For such applications, pose estimation for hand and surgical
instruments from an egocentric perspective is a fundamental task and has been
studied extensively in the computer vision field in recent years. However, the
development of this field has been impeded by a lack of datasets, especially in
the surgical field, where bloody gloves and reflective metallic tools make it
hard to obtain 3D pose annotations for hands and objects using conventional
methods. To address this issue, we propose POV-Surgery, a large-scale,
synthetic, egocentric dataset focusing on pose estimation for hands with
different surgical gloves and three orthopedic surgical instruments, namely
scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329
frames, featuring high-resolution RGB-D video streams with activity
annotations, accurate 3D and 2D annotations for hand-object pose, and 2D
hand-object segmentation masks. We fine-tune the current SOTA methods on
POV-Surgery and further show the generalizability when applying to real-life
cases with surgical gloves and tools by extensive evaluations. The code and the
dataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.","['Rui Wang', 'Sophokles Ktistakis', 'Siwei Zhang', 'Mirko Meboldt', 'Quentin Lohmeyer']",2023-07-19T18:00:32Z,http://arxiv.org/abs/2307.10387v1,['cs.CV']
"A Review of Gaps between Web 4.0 and Web 3.0 Intelligent Network
  Infrastructure","World Wide Web is speeding up its pace into an intelligent and decentralized
ecosystem, as seen in the campaign of Web 3.0 and forthcoming Web 4.0. Marked
by the Europe Commission's latest mention of Web 4.0, a race towards strategic
Web 4.0 success has started. Web 4.0 is committed to bringing the next
technological transition with an open, secure, trustworthy fairness and digital
ecosystem for individuals and businesses in private and public sectors. Despite
overlapping scopes and objectives of Web 3.0 and Web 4.0 from academic and
industrial perspectives, there are distinct and definitive features and gaps
for the next generation of WWW. In this review, a brief introduction to WWW
development unravels the entangled but consistent requirement of a more vivid
web experience, enhancing human-centric experience in both societal and
technical aspects. Moreover, the review brings a decentralized intelligence
prospect of view on native AI entities for Web 4.0, envisioning sustainable,
autonomous and decentralized AI services for the entire Web 4.0 environment,
powering a self-sustainable Decentralized Physical and Software Infrastructure
for Computing Force Network, Semantic Network, Virtual/Mixed Reality, and
Privacy-preserving content presumption.
  The review aims to reveal that Web 4.0 offers native intelligence with
focused thinking on utilizing decentralized physical infrastructure, in
addition to sole requirements on decentralization, bridging the gap between Web
4.0 and Web 3.0 advances with the latest future-shaping blockchain-enabled
computing and network routing protocols.","['Zihan Zhou', 'Zihao Li', 'Xiaoshuai Zhang', 'Yunqing Sun', 'Hao Xu']",2023-08-06T02:49:35Z,http://arxiv.org/abs/2308.02996v1,"['cs.NI', 'cs.CY', 'cs.DC']"
"GRIP: Generating Interaction Poses Using Latent Consistency and Spatial
  Cues","Hands are dexterous and highly versatile manipulators that are central to how
humans interact with objects and their environment. Consequently, modeling
realistic hand-object interactions, including the subtle motion of individual
fingers, is critical for applications in computer graphics, computer vision,
and mixed reality. Prior work on capturing and modeling humans interacting with
objects in 3D focuses on the body and object motion, often ignoring hand pose.
In contrast, we introduce GRIP, a learning-based method that takes, as input,
the 3D motion of the body and the object, and synthesizes realistic motion for
both hands before, during, and after object interaction. As a preliminary step
before synthesizing the hand motion, we first use a network, ANet, to denoise
the arm motion. Then, we leverage the spatio-temporal relationship between the
body and the object to extract two types of novel temporal interaction cues,
and use them in a two-stage inference pipeline to generate the hand motion. In
the first stage, we introduce a new approach to enforce motion temporal
consistency in the latent space (LTC), and generate consistent interaction
motions. In the second stage, GRIP generates refined hand poses to avoid
hand-object penetrations. Given sequences of noisy body and object motion, GRIP
upgrades them to include hand-object interaction. Quantitative experiments and
perceptual studies demonstrate that GRIP outperforms baseline methods and
generalizes to unseen objects and motions from different motion-capture
datasets.","['Omid Taheri', 'Yi Zhou', 'Dimitrios Tzionas', 'Yang Zhou', 'Duygu Ceylan', 'Soren Pirk', 'Michael J. Black']",2023-08-22T17:59:51Z,http://arxiv.org/abs/2308.11617v1,['cs.CV']
"Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended
  Reality","Real-time Stereo Matching is a cornerstone algorithm for many Extended
Reality (XR) applications, such as indoor 3D understanding, video pass-through,
and mixed-reality games. Despite significant advancements in deep stereo
methods, achieving real-time depth inference with high accuracy on a low-power
device remains a major challenge. One of the major difficulties is the lack of
high-quality indoor video stereo training datasets captured by head-mounted
VR/AR glasses. To address this issue, we introduce a novel video stereo
synthetic dataset that comprises photorealistic renderings of various indoor
scenes and realistic camera motion captured by a 6-DoF moving VR/AR
head-mounted display (HMD). This facilitates the evaluation of existing
approaches and promotes further research on indoor augmented reality scenarios.
Our newly proposed dataset enables us to develop a novel framework for
continuous video-rate stereo matching.
  As another contribution, our dataset enables us to proposed a new video-based
stereo matching approach tailored for XR applications, which achieves real-time
inference at an impressive 134fps on a standard desktop computer, or 30fps on a
battery-powered HMD. Our key insight is that disparity and contextual
information are highly correlated and redundant between consecutive stereo
frames. By unrolling an iterative cost aggregation in time (i.e. in the
temporal dimension), we are able to distribute and reuse the aggregated
features over time. This approach leads to a substantial reduction in
computation without sacrificing accuracy. We conducted extensive evaluations
and comparisons and demonstrated that our method achieves superior performance
compared to the current state-of-the-art, making it a strong contender for
real-time stereo matching in VR/AR applications.","['Ziang Cheng', 'Jiayu Yang', 'Hongdong Li']",2023-09-08T07:53:58Z,http://arxiv.org/abs/2309.04183v1,['cs.CV']
"HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI
  Assistants in the Real World","Building an interactive AI assistant that can perceive, reason, and
collaborate with humans in the real world has been a long-standing pursuit in
the AI community. This work is part of a broader research effort to develop
intelligent agents that can interactively guide humans through performing tasks
in the physical world. As a first step in this direction, we introduce
HoloAssist, a large-scale egocentric human interaction dataset, where two
people collaboratively complete physical manipulation tasks. The task performer
executes the task while wearing a mixed-reality headset that captures seven
synchronized data streams. The task instructor watches the performer's
egocentric video in real time and guides them verbally. By augmenting the data
with action and conversational annotations and observing the rich behaviors of
various participants, we present key insights into how human assistants correct
mistakes, intervene in the task completion procedure, and ground their
instructions to the environment. HoloAssist spans 166 hours of data captured by
350 unique instructor-performer pairs. Furthermore, we construct and present
benchmarks on mistake detection, intervention type prediction, and hand
forecasting, along with detailed analysis. We expect HoloAssist will provide an
important resource for building AI assistants that can fluidly collaborate with
humans in the real world. Data can be downloaded at
https://holoassist.github.io/.","['Xin Wang', 'Taein Kwon', 'Mahdi Rad', 'Bowen Pan', 'Ishani Chakraborty', 'Sean Andrist', 'Dan Bohus', 'Ashley Feniello', 'Bugra Tekin', 'Felipe Vieira Frujeri', 'Neel Joshi', 'Marc Pollefeys']",2023-09-29T07:17:43Z,http://arxiv.org/abs/2309.17024v1,['cs.CV']
"Generative AI-driven Semantic Communication Framework for NextG Wireless
  Network","This work designs a novel semantic communication (SemCom) framework for the
next-generation wireless network to tackle the challenges of unnecessary
transmission of vast amounts that cause high bandwidth consumption, more
latency, and experience with bad quality of services (QoS). In particular,
these challenges hinder applications like intelligent transportation systems
(ITS), metaverse, mixed reality, and the Internet of Everything, where
real-time and efficient data transmission is paramount. Therefore, to reduce
communication overhead and maintain the QoS of emerging applications such as
metaverse, ITS, and digital twin creation, this work proposes a novel semantic
communication framework. First, an intelligent semantic transmitter is designed
to capture the meaningful information (e.g., the rode-side image in ITS) by
designing a domain-specific Mobile Segment Anything Model (MSAM)-based
mechanism to reduce the potential communication traffic while QoS remains
intact. Second, the concept of generative AI is introduced for building the
SemCom to reconstruct and denoise the received semantic data frame at the
receiver end. In particular, the Generative Adversarial Network (GAN) mechanism
is designed to maintain a superior quality reconstruction under different
signal-to-noise (SNR) channel conditions. Finally, we have tested and evaluated
the proposed semantic communication (SemCom) framework with the real-world 6G
scenario of ITS; in particular, the base station equipped with an RGB camera
and a mmWave phased array. Experimental results demonstrate the efficacy of the
proposed SemCom framework by achieving high-quality reconstruction across
various SNR channel conditions, resulting in 93.45% data reduction in
communication.","['Avi Deb Raha', 'Md. Shirajum Munir', 'Apurba Adhikary', 'Yu Qiao', 'Choong Seon Hong']",2023-10-13T11:33:54Z,http://arxiv.org/abs/2310.09021v1,['cs.NI']
"Privacy Preservation in Artificial Intelligence and Extended Reality
  (AI-XR) Metaverses: A Survey","The metaverse is a nascent concept that envisions a virtual universe, a
collaborative space where individuals can interact, create, and participate in
a wide range of activities. Privacy in the metaverse is a critical concern as
the concept evolves and immersive virtual experiences become more prevalent.
The metaverse privacy problem refers to the challenges and concerns surrounding
the privacy of personal information and data within Virtual Reality (VR)
environments as the concept of a shared VR space becomes more accessible.
Metaverse will harness advancements from various technologies such as
Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and
5G/6G-based communication to provide personalized and immersive services to its
users. Moreover, to enable more personalized experiences, the metaverse relies
on the collection of fine-grained user data that leads to various privacy
issues. Therefore, before the potential of the metaverse can be fully realized,
privacy concerns related to personal information and data within VR
environments must be addressed. This includes safeguarding users' control over
their data, ensuring the security of their personal information, and protecting
in-world actions and interactions from unauthorized sharing. In this paper, we
explore various privacy challenges that future metaverses are expected to face,
given their reliance on AI for tracking users, creating XR and MR experiences,
and facilitating interactions. Moreover, we thoroughly analyze technical
solutions such as differential privacy, Homomorphic Encryption (HE), and
Federated Learning (FL) and discuss related sociotechnical issues regarding
privacy.","['Mahdi Alkaeed', 'Adnan Qayyum', 'Junaid Qadir']",2023-09-19T11:56:12Z,http://arxiv.org/abs/2310.10665v1,"['cs.CR', 'cs.AI', 'cs.LG']"
"Experiencing Urban Air Mobility: How Passengers evaluate a simulated
  flight with an Air Taxi","For the successful development and implementation of novel concepts and
technology, the acceptance of potential users is crucial. Therefore, within the
project HorizonUAM, we investigated passengers' acceptance of air taxis. One
challenge is that not many people have real experiences with urban air mobility
(UAM) at the moment and thus requirements formulated by potential users refer
to rather abstract concepts. To allow participants to gain realistic
impressions of UAM concepts, a Mixed Reality Air Taxi Simulator was set up. It
allows participants to experience an inner-city business shuttle flight. A
study with 30 participants assessed the information needs and the influence of
another person on board on wellbeing in nominal situations (experiment 1) as
well as one non-nominal situation (experiment 2). For the latter, participants
experienced a re-routing of the flight due to an unavailability of landing
sites at the vertidrome. During and after the flights, participants answered
questionnaires and extensive interviews were conducted. The study produced
first empirical data on relevant factors regarding interaction, information
needs and comfort within an air taxi. The findings show that passengers want to
be informed about intentions of the vehicle. The presence of a steward on board
is not necessary but can increase wellbeing especially during non-nominal
situations.","['Anne Papenfuss', 'Maria Stolz', 'Nele Riedesel', 'Franziska Dunkel', 'Johannes Maria Ernst', 'Tim Laudien', 'Helge Lenz', 'Aytek Korkmaz', 'Albert End', 'Bianca Isabella Schuchardt']",2023-11-02T08:43:52Z,http://arxiv.org/abs/2311.01079v1,"['eess.SY', 'cs.SY', 'physics.soc-ph']"
"Enabling In-Situ Resources Utilisation by leveraging collaborative
  robotics and astronaut-robot interaction","Space exploration and establishing human presence on other planets demand
advanced technology and effective collaboration between robots and astronauts.
Efficient space resource utilization is also vital for extraterrestrial
settlements. The Collaborative In-Situ Resources Utilisation (CISRU) project
has developed a software suite comprising five key modules. The first module
manages multi-agent autonomy, facilitating communication between agents and
mission control. The second focuses on environment perception, employing AI
algorithms for tasks like environment segmentation and object pose estimation.
The third module ensures safe navigation, covering obstacle avoidance, social
navigation with astronauts, and cooperation among robots. The fourth module
addresses manipulation functions, including multi-tool capabilities and
tool-changer design for diverse tasks in In-Situ Resources Utilization (ISRU)
scenarios. Finally, the fifth module controls cooperative behaviour,
incorporating astronaut commands, Mixed Reality interfaces, map fusion, task
supervision, and error control. The suite was tested using an astronaut-rover
interaction dataset in a planetary environment and GMV SPoT analogue
environments. Results demonstrate the advantages of E4 autonomy and AI in space
systems, benefiting astronaut-robot collaboration. This paper details CISRU's
development, field test preparation, and analysis, highlighting its potential
to revolutionize planetary exploration through AI-powered technology.","['Silvia Romero-Azpitarte', 'Cristina Luna', 'Alba Guerra', 'Mercedes Alonso', 'Pablo Romeo Manrique', 'Marina L. Seoane', 'Daniel Olayo', 'Almudena Moreno', 'Pablo Castellanos', 'Fernando Gandía', 'Gianfranco Visentin']",2023-11-06T14:43:03Z,http://arxiv.org/abs/2311.03146v1,['cs.RO']
"Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information","Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.","['Jie Li', 'Zhixin Li', 'Zhi Liu', 'Pengyuan Zhou', 'Richang Hong', 'Qiyue Li', 'Han Hu']",2023-11-28T03:45:29Z,http://arxiv.org/abs/2311.16462v1,"['cs.CV', 'cs.MM']"
MANUS: Markerless Grasp Capture using Articulated 3D Gaussians,"Understanding how we grasp objects with our hands has important applications
in areas like robotics and mixed reality. However, this challenging problem
requires accurate modeling of the contact between hands and objects. To capture
grasps, existing methods use skeletons, meshes, or parametric models that does
not represent hand shape accurately resulting in inaccurate contacts. We
present MANUS, a method for Markerless Hand-Object Grasp Capture using
Articulated 3D Gaussians. We build a novel articulated 3D Gaussians
representation that extends 3D Gaussian splatting for high-fidelity
representation of articulating hands. Since our representation uses Gaussian
primitives, it enables us to efficiently and accurately estimate contacts
between the hand and the object. For the most accurate results, our method
requires tens of camera views that current datasets do not provide. We
therefore build MANUS-Grasps, a new dataset that contains hand-object grasps
viewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7M
frames. In addition to extensive qualitative results, we also show that our
method outperforms others on a quantitative contact evaluation method that uses
paint transfer from the object to the hand.","['Chandradeep Pokhariya', 'Ishaan N Shah', 'Angela Xing', 'Zekun Li', 'Kefan Chen', 'Avinash Sharma', 'Srinath Sridhar']",2023-12-04T18:56:22Z,http://arxiv.org/abs/2312.02137v2,['cs.CV']
"Exploring the current applications and potential of extended reality for
  environmental sustainability in manufacturing","In response to the transformation towards Industry 5.0, there is a growing
call for manufacturing systems that prioritize environmental sustainability,
alongside the emerging application of digital tools. Extended Reality (XR) -
including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) -
is one of the technologies identified as an enabler for Industry 5.0. XR could
potentially also be a driver for more sustainable manufacturing: however, its
potential environmental benefits have received limited attention. This paper
aims to explore the current manufacturing applications and research within the
field of XR technology connected to the environmental sustainability principle.
The objectives of this paper are two-fold: (1) Identify the currently explored
use cases of XR technology in literature and research, addressing environmental
sustainability in manufacturing; (2) Provide guidance and references for
industry and companies to use cases, toolboxes, methodologies, and workflows
for implementing XR in environmental sustainable manufacturing practices. Based
on the categorization of sustainability indicators, developed by the National
Institute of Standards and Technology (NIST), the authors analyzed and mapped
the current literature, with criteria of pragmatic XR use cases for
manufacturing. The exploration resulted in a mapping of the current
applications and use cases of XR technology within manufacturing that has the
potential to drive environmental sustainability. The results are presented as
stated use-cases with reference to the literature, contributing as guidance and
inspiration for future researchers or implementations in industry, using XR as
a driver for environmental sustainability. Furthermore, the authors open up the
discussion for future work and research to increase the attention of XR as a
driver for environmental sustainability.","['Huizhong Cao', 'Henrik Söderlund', 'Mélanie Derspeisse', 'Björn Johansson']",2023-12-29T13:18:01Z,http://arxiv.org/abs/2312.17595v1,"['cs.CY', 'cs.HC']"
On the Emergence of Symmetrical Reality,"Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.","['Zhenliang Zhang', 'Zeyu Zhang', 'Ziyuan Jiao', 'Yao Su', 'Hangxin Liu', 'Wei Wang', 'Song-Chun Zhu']",2024-01-26T16:09:39Z,http://arxiv.org/abs/2401.15132v1,"['cs.HC', 'cs.AI']"
"VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System
  in Virtual Reality","As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain
momentum, there's a growing focus on the development of engagements with 3D
virtual content. Unfortunately, traditional techniques for content creation,
editing, and interaction within these virtual spaces are fraught with
difficulties. They tend to be not only engineering-intensive but also require
extensive expertise, which adds to the frustration and inefficiency in virtual
object manipulation. Our proposed VR-GS system represents a leap forward in
human-centered 3D content interaction, offering a seamless and intuitive user
experience. By developing a physical dynamics-aware interactive Gaussian
Splatting in a Virtual Reality setting, and constructing a highly efficient
two-level embedding strategy alongside deformable body simulations, VR-GS
ensures real-time execution with highly realistic dynamic responses. The
components of our Virtual Reality system are designed for high efficiency and
effectiveness, starting from detailed scene reconstruction and object
segmentation, advancing through multi-view image in-painting, and extending to
interactive physics-based editing. The system also incorporates real-time
deformation embedding and dynamic shadow casting, ensuring a comprehensive and
engaging virtual experience.Our project page is available at:
https://yingjiang96.github.io/VR-GS/.","['Ying Jiang', 'Chang Yu', 'Tianyi Xie', 'Xuan Li', 'Yutao Feng', 'Huamin Wang', 'Minchen Li', 'Henry Lau', 'Feng Gao', 'Yin Yang', 'Chenfanfu Jiang']",2024-01-30T01:28:36Z,http://arxiv.org/abs/2401.16663v2,"['cs.HC', 'cs.CV']"
"HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined
  RGB and Depth Inpainting","Inpainting involves filling in missing pixels or areas in an image, a crucial
technique employed in Mixed Reality environments for various applications,
particularly in Diminished Reality (DR) where content is removed from a user's
visual environment. Existing methods rely on digital replacement techniques
which necessitate multiple cameras and incur high costs. AR devices and
smartphones use ToF depth sensors to capture scene depth maps aligned with RGB
images. Despite speed and affordability, ToF cameras create imperfect depth
maps with missing pixels. To address the above challenges, we propose
Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in
a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked
edge and segmentation label images respectively, while CombinedRGBD-GAN
combines their latent representation outputs and performs RGB and Depth
inpainting. Edge images and particularly segmentation label images as auxiliary
inputs significantly enhance inpainting performance by complementary context
and hierarchical optimization. We believe we make the first attempt to
incorporate label images into inpainting process.Unlike previous approaches
requiring multiple sequential models and separate outputs, our work operates in
an end-to-end manner, training all three models simultaneously and
hierarchically. Specifically, EdgeGAN and LabelGAN are first optimized
separately and further optimized inside CombinedRGBD-GAN to enhance inpainting
quality. Experiments demonstrate that HI-GAN works seamlessly and achieves
overall superior performance compared with existing approaches.","['Ankan Dash', 'Jingyi Gu', 'Guiling Wang']",2024-02-15T21:43:56Z,http://arxiv.org/abs/2402.10334v1,"['cs.CV', 'cs.AI', 'cs.LG']"
"The Value of Extended Reality Techniques to Improve Remote Collaborative
  Maintenance Operations: A User Study","In the Architecture, Engineering and Construction (AEC) sector, data
extracted from building information modelling (BIM) can be used to create a
digital twin (DT). The algorithms of a BIM-based DT can facilitate the
retrieval of information, which can then be used to improve building operation
and maintenance procedures. However, with the increased complexity and
automation of the building, maintenance operations are likely to become more
complex and may require expert intervention. Collaboration and interaction
between the operator and the expert may be limited as the latter may not be on
site or within the company. Recently, extended reality (XR) technologies have
proven to be effective in improving collaboration during maintenance
operations,through data display and shared interactions. This paper presents a
new collaborative solution using these technologies to enhance collaboration
during remote maintenance operations. The proposed approach consists of a mixed
reality (MR) set-up for the operator, a virtual reality (VR) set-up for the
remote expert and a shared Digital Model of a heat exchanger. The MR set-up is
used for tracking and displaying specific information, provided by the VR
module. A user study was carried out to compare the efficiency of our solution
with a standard audio-video collaboration. Our approach demonstrated
substantial enhancements in collaborative inspection, resulting in a
significative reduction in both the overall completion time of the inspection
and the frequency of errors committed by the operators.","['Corentin Coupry', 'Paul Richard', 'David Bigaud', 'Sylvain Noblecourt', 'David Baudry']",2024-02-29T12:28:40Z,http://arxiv.org/abs/2403.05580v1,"['cs.HC', 'cs.GR']"
OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation,"Open-sourced, user-friendly tools form the bedrock of scientific advancement
across disciplines. The widespread adoption of data-driven learning has led to
remarkable progress in multi-fingered dexterity, bimanual manipulation, and
applications ranging from logistics to home robotics. However, existing data
collection platforms are often proprietary, costly, or tailored to specific
robotic morphologies. We present OPEN TEACH, a new teleoperation system
leveraging VR headsets to immerse users in mixed reality for intuitive robot
control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH
enables real-time control of various robots, including multi-fingered hands and
bimanual arms, through an easy-to-use app. Using natural hand gestures and
movements, users can manipulate robots at up to 90Hz with smooth visual
feedback and interface widgets offering closeup environment views. We
demonstrate the versatility of OPEN TEACH across 38 tasks on different robots.
A comprehensive user study indicates significant improvement in teleoperation
capability over the AnyTeleop framework. Further experiments exhibit that the
collected data is compatible with policy learning on 10 dexterous and
contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and
Allegro platforms, OPEN TEACH is fully open-sourced to promote broader
adoption. Videos are available at https://open-teach.github.io/.","['Aadhithya Iyer', 'Zhuoran Peng', 'Yinlong Dai', 'Irmak Guzey', 'Siddhant Haldar', 'Soumith Chintala', 'Lerrel Pinto']",2024-03-12T17:58:38Z,http://arxiv.org/abs/2403.07870v1,['cs.RO']
Segment Any Medical Model Extended,"The Segment Anything Model (SAM) has drawn significant attention from
researchers who work on medical image segmentation because of its
generalizability. However, researchers have found that SAM may have limited
performance on medical images compared to state-of-the-art non-foundation
models. Regardless, the community sees potential in extending, fine-tuning,
modifying, and evaluating SAM for analysis of medical imaging. An increasing
number of works have been published focusing on the mentioned four directions,
where variants of SAM are proposed. To this end, a unified platform helps push
the boundary of the foundation model for medical images, facilitating the use,
modification, and validation of SAM and its variants in medical image
segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that
integrates new SAM variant models, adopts faster communication protocols,
accommodates new interactive modes, and allows for fine-tuning of subcomponents
of the models. These features can expand the potential of foundation models
like SAM, and the results can be translated to applications such as
image-guided therapy, mixed reality interaction, robotic navigation, and data
augmentation.","['Yihao Liu', 'Jiaming Zhang', 'Andres Diaz-Pinto', 'Haowei Li', 'Alejandro Martin-Gomez', 'Amir Kheradmand', 'Mehran Armand']",2024-03-26T21:37:25Z,http://arxiv.org/abs/2403.18114v1,['cs.CV']
3D Human Scan With A Moving Event Camera,"Capturing a 3D human body is one of the important tasks in computer vision
with a wide range of applications such as virtual reality and sports analysis.
However, conventional frame cameras are limited by their temporal resolution
and dynamic range, which imposes constraints in real-world application setups.
Event cameras have the advantages of high temporal resolution and high dynamic
range (HDR), but the development of event-based methods is necessary to handle
data with different characteristics. This paper proposes a novel event-based
method for 3D pose estimation and human mesh recovery. Prior work on
event-based human mesh recovery require frames (images) as well as event data.
The proposed method solely relies on events; it carves 3D voxels by moving the
event camera around a stationary body, reconstructs the human pose and mesh by
attenuated rays, and fit statistical body models, preserving high-frequency
details. The experimental results show that the proposed method outperforms
conventional frame-based methods in the estimation accuracy of both pose and
body mesh. We also demonstrate results in challenging situations where a
conventional camera has motion blur. This is the first to demonstrate
event-only human mesh recovery, and we hope that it is the first step toward
achieving robust and accurate 3D human body scanning from vision sensors.
https://florpeng.github.io/event-based-human-scan/","['Kai Kohyama', 'Shintaro Shiba', 'Yoshimitsu Aoki']",2024-04-12T14:34:24Z,http://arxiv.org/abs/2404.08504v2,['cs.CV']
"Exploring Proactive Interventions toward Harmful Behavior in Embodied
  Virtual Spaces","Technological advancements have undoubtedly revolutionized various aspects of
human life, altering the ways we perceive the world, engage with others, build
relationships, and conduct our daily work routines. Among the recent
advancements, the proliferation of virtual and mixed reality technologies
stands out as a significant leap forward, promising to elevate our experiences
and interactions to unprecedented levels. However, alongside the benefits,
these emerging technologies also introduce novel avenues for harm and misuse,
particularly in virtual and embodied spaces such as Zoom and virtual reality
(VR) environments.
  The immersive nature of virtual reality environments raises unique challenges
regarding psychological and emotional well-being. While VR can offer
captivating and immersive experiences, prolonged exposure to virtual
environments may lead to phenomena like cybersickness, disorientation, and even
psychological distress in susceptible individuals. Additionally, the blurring
of boundaries between virtual and real-world interactions in VR raises ethical
concerns regarding consent, harassment, and the potential for virtual
experiences to influence real-life behavior. Additionally, the increasing
integration of artificial intelligence (AI) and machine learning algorithms in
virtual spaces introduces risks related to algorithmic bias, discrimination,
and manipulation. In VR environments, AI-driven systems may inadvertently
perpetuate stereotypes, amplify inequalities, or manipulate user behavior
through personalized content recommendations and targeted advertising, posing
ethical dilemmas and societal risks.",['Ruchi Panchanadikar'],2024-04-23T17:38:27Z,http://arxiv.org/abs/2405.05920v1,['cs.HC']
"A First Step in Using Machine Learning Methods to Enhance Interaction
  Analysis for Embodied Learning Environments","Investigating children's embodied learning in mixed-reality environments,
where they collaboratively simulate scientific processes, requires analyzing
complex multimodal data to interpret their learning and coordination behaviors.
Learning scientists have developed Interaction Analysis (IA) methodologies for
analyzing such data, but this requires researchers to watch hours of videos to
extract and interpret students' learning patterns. Our study aims to simplify
researchers' tasks, using Machine Learning and Multimodal Learning Analytics to
support the IA processes. Our study combines machine learning algorithms and
multimodal analyses to support and streamline researcher efforts in developing
a comprehensive understanding of students' scientific engagement through their
movements, gaze, and affective responses in a simulated scenario. To facilitate
an effective researcher-AI partnership, we present an initial case study to
determine the feasibility of visually representing students' states, actions,
gaze, affect, and movement on a timeline. Our case study focuses on a specific
science scenario where students learn about photosynthesis. The timeline allows
us to investigate the alignment of critical learning moments identified by
multimodal and interaction analysis, and uncover insights into students'
temporal learning progressions.","['Joyce Fonteles', 'Eduardo Davalos', 'Ashwin T. S.', 'Yike Zhang', 'Mengxi Zhou', 'Efrat Ayalon', 'Alicia Lane', 'Selena Steinberg', 'Gabriella Anton', 'Joshua Danish', 'Noel Enyedy', 'Gautam Biswas']",2024-05-10T02:40:24Z,http://arxiv.org/abs/2405.06203v1,['cs.AI']
"Autonomous Workflow for Multimodal Fine-Grained Training Assistants
  Towards Mixed Reality","Autonomous artificial intelligence (AI) agents have emerged as promising
protocols for automatically understanding the language-based environment,
particularly with the exponential development of large language models (LLMs).
However, a fine-grained, comprehensive understanding of multimodal environments
remains under-explored. This work designs an autonomous workflow tailored for
integrating AI agents seamlessly into extended reality (XR) applications for
fine-grained training. We present a demonstration of a multimodal fine-grained
training assistant for LEGO brick assembly in a pilot XR environment.
Specifically, we design a cerebral language agent that integrates LLM with
memory, planning, and interaction with XR tools and a vision-language agent,
enabling agents to decide their actions based on past experiences. Furthermore,
we introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset
synthesized automatically in the workflow served by a commercial LLM. This
dataset comprises multimodal instruction manuals, conversations, XR responses,
and vision question answering. Last, we present several prevailing
open-resource LLMs as benchmarks, assessing their performance with and without
fine-tuning on the proposed dataset. We anticipate that the broader impact of
this workflow will advance the development of smarter assistants for seamless
user interaction in XR environments, fostering research in both AI and HCI
communities.","['Jiahuan Pei', 'Irene Viola', 'Haochen Huang', 'Junxiao Wang', 'Moonisa Ahsan', 'Fanghua Ye', 'Jiang Yiming', 'Yao Sai', 'Di Wang', 'Zhumin Chen', 'Pengjie Ren', 'Pablo Cesar']",2024-05-16T14:20:30Z,http://arxiv.org/abs/2405.13034v1,"['cs.CL', 'cs.AI', 'cs.HC']"
"HoloDevice: Holographic Cross-Device Interactions for Remote
  Collaboration","This paper introduces holographic cross-device interaction, a new class of
remote cross-device interactions between local physical devices and
holographically rendered remote devices. Cross-device interactions have enabled
a rich set of interactions with device ecologies. Most existing research
focuses on co-located settings (meaning when users and devices are in the same
physical space) to achieve these rich interactions and affordances. In
contrast, holographic cross-device interaction allows remote interactions
between devices at distant locations by providing a rich visual affordance
through real-time holographic rendering of the device's motion, content, and
interactions on mixed reality head-mounted displays. This maintains the
advantages of having a physical device, such as precise input through touch and
pen interaction. Through holographic rendering, not only can remote devices
interact as if they are co-located, but they can also be virtually augmented to
further enrich interactions, going beyond what is possible with existing
cross-device systems. To demonstrate this concept, we developed HoloDevice, a
prototype system for holographic cross-device interaction using the Microsoft
Hololens 2 augmented reality headset. Our contribution is threefold. First, we
introduce the concept of holographic cross-device interaction. Second, we
present a design space containing three unique benefits, which include: (1)
spatial visualization of interaction and motion, (2) rich visual affordances
for intermediate transition, and (3) dynamic and fluid configuration. Last we
discuss a set of implementation demonstrations and use-case scenarios that
further explore the space.","['Neil Chulpongsatorn', 'Thien-Kim Nguyen', 'Nicolai Marquardt', 'Ryo Suzuki']",2024-05-28T22:49:01Z,http://arxiv.org/abs/2405.19377v1,['cs.HC']
"BundleFusion: Real-time Globally Consistent 3D Reconstruction using
  On-the-fly Surface Re-integration","Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed
reality and robotic applications. However, scalability brings challenges of
drift in pose estimation, introducing significant errors in the accumulated
model. Approaches often require hours of offline processing to globally correct
model errors. Recent online methods demonstrate compelling results, but suffer
from: (1) needing minutes to perform online correction preventing true
real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation
resulting in many tracking failures; or (3) supporting only unstructured
point-based representations, which limit scan quality and applicability. We
systematically address these issues with a novel, real-time, end-to-end
reconstruction framework. At its core is a robust pose estimation strategy,
optimizing per frame for a global set of camera poses by considering the
complete history of RGB-D input with an efficient hierarchical approach. We
remove the heavy reliance on temporal tracking, and continually localize to the
globally optimized frames instead. We contribute a parallelizable optimization
framework, which employs correspondences based on sparse features and dense
geometric and photometric matching. Our approach estimates globally optimized
(i.e., bundle adjusted) poses in real-time, supports robust tracking with
recovery from gross tracking failures (i.e., relocalization), and re-estimates
the 3D model in real-time to ensure global consistency; all within a single
framework. Our approach outperforms state-of-the-art online systems with
quality on par to offline methods, but with unprecedented speed and scan
completeness. Our framework leads to a comprehensive online scanning solution
for large indoor environments, enabling ease of use and high-quality results.","['Angela Dai', 'Matthias Nießner', 'Michael Zollhöfer', 'Shahram Izadi', 'Christian Theobalt']",2016-04-05T00:06:39Z,http://arxiv.org/abs/1604.01093v3,"['cs.GR', 'cs.CV']"
"XR: Enabling training mode in the human brain XR: Enabling training mode
  in the human brain","The face of simulation-based training has greatly evolved, with the most
recent tools giving the ability to create virtual environments that rival
realism. At first glance, it might appear that what the training sector needs
is the most realistic simulators possible, but traditional simulators are not
necessarily the most efficient or practical training tools. With all that these
new technologies have to offer; the challenge is to go back to the core of
training needs and identify the right vector of sensory cues that will most
effectively enable training mode in the human brain. Bigger and Pricier doesn't
necessarily mean better. Simulation with cross-reality content (XR), which by
definition encompasses virtual reality (VR), mixed reality (MR), and augmented
reality (AR), is the most practical solution for deploying any kind of
simulation-based training. The authors of this paper (a teacher and a
technology expert) share their experiences and expose XR-specific best
practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :
Starting his career in the modeling and simulation community more than 15 years
ago, S{\'e}bastien has focused on learning about the latest simulation
innovations and sharing information on how experts have solved their
challenges. He worked on the COTS integration at CAE and the Presagis focusing
on Simulation and Visualization products. More recently, Sebastien put together
simulation and training teams and strategies for emerging companies like CM
Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games,
focusing on helping companies develop real-time solutions for simulation-based
training. Philippe Lepinard: Former military helicopter pilot and simulation
officer, Philippe L{\'e}pinard is now an associate professor at the University
of Paris-Est Cr{\'e}teil (UPEC). His research is focusing on playful learning
and training through simulation. He is one of the founding members of the
French simulation association.","['Philippe Lépinard', 'Sébastien Lozé']",2019-04-26T07:55:39Z,http://arxiv.org/abs/1904.11704v1,"['cs.GR', 'cs.HC']"
"All Reality: Virtual, Augmented, Mixed (X), Mediated (X,Y), and
  Multimediated Reality","The contributions of this paper are: (1) a taxonomy of the ""Realities""
(Virtual, Augmented, Mixed, Mediated, etc.), and (2) some new kinds of
""reality"" that come from nature itself, i.e. that expand our notion beyond
synthetic realities to include also phenomenological realities.
  VR (Virtual Reality) replaces the real world with a simulated experience
(virtual world). AR (Augmented Reality) allows a virtual world to be
experienced while also experiencing the real world at the same time. Mixed
Reality provides blends that interpolate between real and virtual worlds in
various proportions, along a ""Virtuality"" axis, and extrapolate to an ""X-axis"".
Mediated Reality goes a step further by mixing/blending and also modifying
reality. This modifying of reality introduces a second axis. Mediated Reality
is useful as a seeing aid (e.g. modifying reality to make it easier to
understand), and for psychology experiments like Stratton's 1896 upside-down
eyeglasses experiment.
  We propose Multimediated Reality as a multidimensional multisensory mediated
reality that includes not just interactive multimedia-based reality for our
five senses, but also includes additional senses (like sensory sonar, sensory
radar, etc.), as well as our human actions/actuators. These extra senses are
mapped to our human senses using synthetic synesthesia. This allows us to
directly experience real (but otherwise invisible) phenomena, such as wave
propagation and wave interference patterns, so that we can see radio waves and
sound waves and how they interact with objects and each other. Multimediated
reality is multidimensional, multimodal, multisensory, and multiscale. It is
also multidisciplinary, in that we must consider not just the user, but also
how the technology affects others, e.g. how its physical appearance affects
social situations.","['Steve Mann', 'Tom Furness', 'Yu Yuan', 'Jay Iorio', 'Zixin Wang']",2018-04-20T15:40:39Z,http://arxiv.org/abs/1804.08386v1,['cs.HC']
SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in the Wild,"Gesture recognition is a fundamental tool to enable novel interaction
paradigms in a variety of application scenarios like Mixed Reality
environments, touchless public kiosks, entertainment systems, and more.
Recognition of hand gestures can be nowadays performed directly from the stream
of hand skeletons estimated by software provided by low-cost trackers
(Ultraleap) and MR headsets (Hololens, Oculus Quest) or by video processing
software modules (e.g. Google Mediapipe). Despite the recent advancements in
gesture and action recognition from skeletons, it is unclear how well the
current state-of-the-art techniques can perform in a real-world scenario for
the recognition of a wide set of heterogeneous gestures, as many benchmarks do
not test online recognition and use limited dictionaries. This motivated the
proposal of the SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in
the Wild. For this contest, we created a novel dataset with heterogeneous
gestures featuring different types and duration. These gestures have to be
found inside sequences in an online recognition scenario. This paper presents
the result of the contest, showing the performances of the techniques proposed
by four research groups on the challenging task compared with a simple baseline
method.","['Ariel Caputo', 'Andrea Giachetti', 'Simone Soso', 'Deborah Pintani', ""Andrea D'Eusanio"", 'Stefano Pini', 'Guido Borghi', 'Alessandro Simoni', 'Roberto Vezzani', 'Rita Cucchiara', 'Andrea Ranieri', 'Franca Giannini', 'Katia Lupinetti', 'Marina Monti', 'Mehran Maghoumi', 'Joseph J. LaViola Jr', 'Minh-Quan Le', 'Hai-Dang Nguyen', 'Minh-Triet Tran']",2021-06-21T10:57:49Z,http://arxiv.org/abs/2106.10980v1,"['cs.CV', 'cs.LG']"
Egocentric Videoconferencing,"We introduce a method for egocentric videoconferencing that enables
hands-free video calls, for instance by people wearing smart glasses or other
mixed-reality devices. Videoconferencing portrays valuable non-verbal
communication and face expression cues, but usually requires a front-facing
camera. Using a frontal camera in a hands-free setting when a person is on the
move is impractical. Even holding a mobile phone camera in the front of the
face while sitting for a long duration is not convenient. To overcome these
issues, we propose a low-cost wearable egocentric camera setup that can be
integrated into smart glasses. Our goal is to mimic a classical video call, and
therefore, we transform the egocentric perspective of this camera into a front
facing video. To this end, we employ a conditional generative adversarial
neural network that learns a transition from the highly distorted egocentric
views to frontal views common in videoconferencing. Our approach learns to
transfer expression details directly from the egocentric view without using a
complex intermediate parametric expressions model, as it is used by related
face reenactment methods. We successfully handle subtle expressions, not easily
captured by parametric blendshape-based solutions, e.g., tongue movement, eye
movements, eye blinking, strong expressions and depth varying movements. To get
control over the rigid head movements in the target view, we condition the
generator on synthetic renderings of a moving neutral face. This allows us to
synthesis results at different head poses. Our technique produces temporally
smooth video-realistic renderings in real-time using a video-to-video
translation network in conjunction with a temporal discriminator. We
demonstrate the improved capabilities of our technique by comparing against
related state-of-the art approaches.","['Mohamed Elgharib', 'Mohit Mendiratta', 'Justus Thies', 'Matthias Nießner', 'Hans-Peter Seidel', 'Ayush Tewari', 'Vladislav Golyanik', 'Christian Theobalt']",2021-07-07T09:49:39Z,http://arxiv.org/abs/2107.03109v1,"['cs.GR', 'cs.CV']"
Fristograms: Revealing and Exploiting Light Field Internals,"In recent years, light field (LF) capture and processing has become an
integral part of media production. The richness of information available in LFs
has enabled novel applications like post-capture depth-of-field editing, 3D
reconstruction, segmentation and matting, saliency detection, object detection
and recognition, and mixed reality. The efficacy of such applications depends
on certain underlying requirements, which are often ignored. For example, some
operations such as noise-reduction, or hyperfan-filtering are only possible if
a scene point Lambertian radiator. Some other operations such as the removal of
obstacles or looking behind objects are only possible if there is at least one
ray capturing the required scene point. Consequently, the ray distribution
representing a certain scene point is an important characteristic for
evaluating processing possibilities. The primary idea in this paper is to
establish a relation between the capturing setup and the rays of the LF. To
this end, we discretize the view frustum. Traditionally, a uniform
discretization of the view frustum results in voxels that represents a single
sample on a regularly spaced, 3-D grid. Instead, we use frustum-shaped voxels
(froxels), by using depth and capturing-setup dependent discretization of the
view frustum. Based on such discretization, we count the number of rays mapping
to the same pixel on the capturing device(s). By means of this count, we
propose histograms of ray-counts over the froxels (fristograms). Fristograms
can be used as a tool to analyze and reveal interesting aspects of the
underlying LF, like the number of rays originating from a scene point and the
color distribution of these rays. As an example, we show its ability by
significantly reducing the number of rays which enables noise reduction while
maintaining the realistic rendering of non-Lambertian or partially occluded
regions.","['Thorsten Herfet', 'Kelvin Chelli', 'Tobias Lange', 'Robin Kremer']",2021-07-22T10:33:13Z,http://arxiv.org/abs/2107.10563v1,"['eess.IV', 'cs.CV']"
"The Impact of Machine Learning on 2D/3D Registration for Image-guided
  Interventions: A Systematic Review and Perspective","Image-based navigation is widely considered the next frontier of minimally
invasive surgery. It is believed that image-based navigation will increase the
access to reproducible, safe, and high-precision surgery as it may then be
performed at acceptable costs and effort. This is because image-based
techniques avoid the need of specialized equipment and seamlessly integrate
with contemporary workflows. Further, it is expected that image-based
navigation will play a major role in enabling mixed reality environments and
autonomous, robotic workflows. A critical component of image guidance is 2D/3D
registration, a technique to estimate the spatial relationships between 3D
structures, e.g., volumetric imagery or tool models, and 2D images thereof,
such as fluoroscopy or endoscopy. While image-based 2D/3D registration is a
mature technique, its transition from the bench to the bedside has been
restrained by well-known challenges, including brittleness of the optimization
objective, hyperparameter selection, and initialization, difficulties around
inconsistencies or multiple objects, and limited single-view performance. One
reason these challenges persist today is that analytical solutions are likely
inadequate considering the complexity, variability, and high-dimensionality of
generic 2D/3D registration problems. The recent advent of machine
learning-based approaches to imaging problems that, rather than specifying the
desired functional mapping, approximate it using highly expressive parametric
models holds promise for solving some of the notorious challenges in 2D/3D
registration. In this manuscript, we review the impact of machine learning on
2D/3D registration to systematically summarize the recent advances made by
introduction of this novel technology. Grounded in these insights, we then
offer our perspective on the most pressing needs, significant open problems,
and possible next steps.","['Mathias Unberath', 'Cong Gao', 'Yicheng Hu', 'Max Judish', 'Russell H Taylor', 'Mehran Armand', 'Robert Grupp']",2021-08-04T18:31:29Z,http://arxiv.org/abs/2108.02238v1,"['cs.CV', 'cs.RO', 'physics.med-ph']"
"Comfort and Sickness while Virtually Aboard an Autonomous Telepresence
  Robot","In this paper, we analyze how different path aspects affect a user's
experience, mainly VR sickness and overall comfort, while immersed in an
autonomously moving telepresence robot through a virtual reality headset. In
particular, we focus on how the robot turns and the distance it keeps from
objects, with the goal of planning suitable trajectories for an autonomously
moving immersive telepresence robot in mind; rotational acceleration is known
for causing the majority of VR sickness, and distance to objects modulates the
optical flow. We ran a within-subjects user study (n = 36, women = 18) in which
the participants watched three panoramic videos recorded in a virtual museum
while aboard an autonomously moving telepresence robot taking three different
paths varying in aspects such as turns, speeds, or distances to walls and
objects. We found a moderate correlation between the users' sickness as
measured by the SSQ and comfort on a 6-point Likert scale across all paths.
However, we detected no association between sickness and the choice of the most
comfortable path, showing that sickness is not the only factor affecting the
comfort of the user. The subjective experience of turn speed did not correlate
with either the SSQ scores or comfort, even though people often mentioned
turning speed as a source of discomfort in the open-ended questions. Through
exploring the open-ended answers more carefully, a possible reason is that the
length and lack of predictability also play a large role in making people
observe turns as uncomfortable. A larger subjective distance from walls and
objects increased comfort and decreased sickness both in quantitative and
qualitative data. Finally, the SSQ subscales and total weighted scores showed
differences by age group and by gender.","['Markku Suomalainen', 'Katherine J. Mimnaugh', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-09-09T11:30:17Z,http://arxiv.org/abs/2109.04177v1,"['cs.HC', 'cs.MM', 'cs.RO']"
Extended Reality for Mental Health Evaluation -A Scoping Review,"Mental health disorders are the leading cause of health-related problems
globally. It is projected that mental health disorders will be the leading
cause of morbidity among adults as the incidence rates of anxiety and
depression grows globally. Recently, extended reality (XR), a general term
covering virtual reality (VR), augmented reality (AR) and mixed reality (MR),
is paving a new way to deliver mental health care. In this paper, we conduct a
scoping review on the development and application of XR in the area of mental
disorders. We performed a scoping database search to identify the relevant
studies indexed in Google Scholar, PubMed, and the ACM Digital Library. A
search period between August 2016 and December 2023 was defined to select
articles related to the usage of VR, AR, and MR in a mental health context. We
identified a total of 85 studies from 27 countries across the globe. By
performing data analysis, we found that most of the studies focused on
developed countries such as the US (16.47%) and Germany (12.94%). None of the
studies were for African countries. The majority of the articles reported that
XR techniques led to a significant reduction in symptoms of anxiety or
depression. More studies were published in the year 2021, i.e., 31.76% (n =
31). This could indicate that mental disorder intervention received a higher
attention when COVID-19 emerged. Most studies (n = 65) focused on a population
between 18 and 65 years old, only a few studies focused on teenagers (n = 2).
Also, more studies were done experimentally (n = 67, 78.82%) rather than by
analytical and modeling approaches (n = 8, 9.41%). This shows that there is a
rapid development of XR technology for mental health care. Furthermore, these
studies showed that XR technology can effectively be used for evaluating mental
disorders in similar or better way as the conventional approaches.","['Omisore Olatunji', 'Ifeanyi Odenigbo', 'Joseph Orji', 'Amelia Beltran', 'Nilufar Baghaei', 'Meier Sandra', 'Rita Orji']",2022-04-04T09:46:30Z,http://arxiv.org/abs/2204.01348v2,"['cs.HC', 'cs.CV']"
Updating Industrial Robots for Emerging Technologies,"Industrial arms need to evolve beyond their standard shape to embrace new and
emerging technologies. In this paper, we shall first perform an analysis of
four popular but different modern industrial robot arms. By seeing the common
trends we will try to extrapolate and expand these trends for the future. Here,
particular focus will be on interaction based on augmented reality (AR) through
head-mounted displays (HMD), but also through smartphones. Long-term
human-robot interaction and personalization of said interaction will also be
considered. The use of AR in human-robot interaction has proven to enhance
communication and information exchange. A basic addition to industrial arm
design would be the integration of QR markers on the robot, both for accessing
information and adding tracking capabilities to more easily display AR
overlays. In a recent example of information access, Mercedes Benz added QR
markers on their cars to help rescue workers estimate the best places to cut
and evacuate people after car crashes. One has also to deal with safety in an
environment that will be more and more about collaboration. The QR markers can
therefore be combined with RF-based ranging modules, developed in the
EU-project SafeLog, that can be used both for safety as well as for tracking of
human positions while in close proximity interactions with the industrial arms.
The industrial arms of the future should also be intuitive to program and
interact with. This would be achieved through AR and head mounted displays as
well as the already mentioned RF-based person tracking. Finally, a more
personalized interaction between the robots and humans can be achieved through
life-long learning AI and disembodied, personalized agents. We propose a design
that not only exists in the physical world, but also partly in the digital
world of mixed reality.","['David Puljiz', 'Björn Hein']",2022-04-07T16:08:02Z,http://arxiv.org/abs/2204.03538v2,['cs.RO']
"Convergence and Disruption in Digital Society -- Money, Secure
  Communication, Digital Objects and Generative AI in Spatial Mixed Reality","In the digital society's evolving landscape, open-source tooling and
generative AI are pivotal in transforming global collaboration. These
technologies promise to dismantle traditional barriers of accessibility,
language, and governance, fostering an inclusive digital ecosystem. However,
the journey towards a fully integrated digital society faces significant
challenges, including trust, accessibility, and sustainable development.
Emerging technologies like global ledgers and blockchain propose novel methods
for transferring digital goods and personal data across diverse digital spaces.
This development, coupled with augmented intelligence tools, aims to create
decentralized and federated environments where creativity and collaboration can
flourish. An ""open metaverse"" concept is gaining traction, promoting an
alternative to restrictive proprietary platforms and emphasizing user
empowerment and equity. Despite the opportunities, governance and ethical
considerations remain paramount. The digital society must navigate the fine
balance between innovation and the potential risks associated with new
technologies. The drive for an inclusive, innovative, and secure digital
society necessitates a commitment to open-source principles and ethical AI
application. It also involves overcoming cultural, legislative, and technical
barriers that impede global collaboration. The future digital society envisions
a collaborative, inclusive, and innovative global community. By focusing on
augmented intelligence and supported creativity, it aims to unlock new
possibilities for economic empowerment, cultural exchange, and technological
advancement. This vision is not without its challenges, but with continued
commitment to ethical, open, and inclusive development, a more connected and
empowered global community is within reach.","[""John Joseph O'Hare"", 'Allen Fairchild', 'Umran Ali']",2022-07-19T15:33:24Z,http://arxiv.org/abs/2207.09460v11,['cs.CR']
"Sampling, Communication, and Prediction Co-Design for Synchronizing the
  Real-World Device and Digital Model in Metaverse","The metaverse has the potential to revolutionize the next generation of the
Internet by supporting highly interactive services with the help of Mixed
Reality (MR) technologies; still, to provide a satisfactory experience for
users, the synchronization between the physical world and its digital models is
crucial. This work proposes a sampling, communication and prediction co-design
framework to minimize the communication load subject to a constraint on
tracking the Mean Squared Error (MSE) between a real-world device and its
digital model in the metaverse. To optimize the sampling rate and the
prediction horizon, we exploit expert knowledge and develop a constrained Deep
Reinforcement Learning (DRL) algorithm, named Knowledge-assisted Constrained
Twin-Delayed Deep Deterministic (KC-TD3) policy gradient algorithm. We validate
our framework on a prototype composed of a real-world robotic arm and its
digital model. Compared with existing approaches: (1) When the tracking error
constraint is stringent (MSE=0.002 degrees), our policy degenerates into the
policy in the sampling-communication co-design framework. (2) When the tracking
error constraint is mild (MSE=0.007 degrees), our policy degenerates into the
policy in the prediction-communication co-design framework. (3) Our framework
achieves a better trade-off between the average MSE and the average
communication load compared with a communication system without sampling and
prediction. For example, the average communication load can be reduced up to
87% when the track error constraint is 0.002 degrees. (4) Our policy
outperforms the benchmark with the static sampling rate and prediction horizon
optimized by exhaustive search, in terms of the tail probability of the
tracking error. Furthermore, with the assistance of expert knowledge, the
proposed algorithm KC-TD3 achieves better convergence time, stability, and
final policy performance.","['Zhen Meng', 'Changyang She', 'Guodong Zhao', 'Daniele De Martini']",2022-07-31T20:17:31Z,http://arxiv.org/abs/2208.04233v1,"['cs.RO', 'cs.AI', 'cs.HC', 'cs.LG']"
"AI and 6G into the Metaverse: Fundamentals, Challenges and Future
  Research Trends","Since Facebook was renamed Meta, a lot of attention, debate, and exploration
have intensified about what the Metaverse is, how it works, and the possible
ways to exploit it. It is anticipated that Metaverse will be a continuum of
rapidly emerging technologies, usecases, capabilities, and experiences that
will make it up for the next evolution of the Internet. Several researchers
have already surveyed the literature on artificial intelligence (AI) and
wireless communications in realizing the Metaverse. However, due to the rapid
emergence and continuous evolution of technologies, there is a need for a
comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both
in realizing the immersive experiences of Metaverse. Therefore, in this survey,
we first introduce the background and ongoing progress in augmented reality
(AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed
by the technical aspects of AI and 6G. Then, we survey the role of AI in the
Metaverse by reviewing the state-of-the-art in deep learning, computer vision,
and Edge AI to extract the requirements of 6G in Metaverse. Next, we
investigate the promising services of B5G/6G towards Metaverse, followed by
identifying the role of AI in 6G networks and 6G networks for AI in support of
Metaverse applications, and the need for sustainability in Metaverse. Finally,
we enlist the existing and potential applications, usecases, and projects to
highlight the importance of progress in the Metaverse. Moreover, in order to
provide potential research directions to researchers, we underline the
challenges, research gaps, and lessons learned identified from the literature
review of the aforementioned technologies.","['Muhammad Zawish', 'Fayaz Ali Dharejo', 'Sunder Ali Khowaja', 'Kapal Dev', 'Steven Davy', 'Nawab Muhammad Faseeh Qureshi', 'Paolo Bellavista']",2022-08-23T12:48:53Z,http://arxiv.org/abs/2208.10921v2,"['cs.AI', 'cs.HC', 'cs.NI', 'cs.SI']"
"Apple Vision Pro for Healthcare: ""The Ultimate Display""? -- Entering the
  Wonderland of Precision Medicine","At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user's eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers' eyes to ""outsiders"" or a button on
the top, called ""Digital Crown"", that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the ""Ultimate
Display"", which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.","['Jan Egger', 'Christina Gsaxner', 'Xiaojun Chen', 'Jiang Bian', 'Jens Kleesiek', 'Behrus Puladi']",2023-08-08T15:01:51Z,http://arxiv.org/abs/2308.04313v4,"['cs.AI', 'cs.GR', 'cs.HC']"
"Modeling novel physics in virtual reality labs: An affective analysis of
  student learning","We report on a study of the effects of laboratory activities that model
fictitious laws of physics in a virtual reality environment on (1) students'
epistemology about the role of experimental physics in class and in the world;
(2) students' self-efficacy; and (3) the quality of student engagement with the
lab activities. We create opportunities for students to practice physics as a
means of creating and validating new knowledge by simulating real and
fictitious physics in virtual reality (VR). This approach seeks to steer
students away from a confirmation mindset in labs by eliminating any form of
prior or outside models to confirm. We refer to the activities using this
approach as Novel Observations in Mixed Reality (NOMR) labs. We examined NOMR's
effects in 100-level and 200-level undergraduate courses. Using pre-post
measurements we find that after NOMR labs, students in both populations were
more expertlike in their epistemology about experimental physics and held
stronger self-efficacy about their abilities to do the kinds of things
experimental physicists do. Through the lens of the psychological theory of
flow, we found that students engage as productively with NOMR labs as with
traditional hands-on labs. This engagement persisted after the novelty of VR in
the classroom wore off, suggesting that these effects are due to the
pedagogical design rather than the medium of the intervention. We conclude that
these NOMR labs offer an approach to physics laboratory instruction that
centers the development of students' understanding of and comfort with the
authentic practice of science.","['Jared P. Canright', 'Suzanne White Brahmia']",2023-10-12T00:27:24Z,http://arxiv.org/abs/2310.07952v1,['physics.ed-ph']
"ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic
  Reconstruction","We propose an online 3D semantic segmentation method that incrementally
reconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline
methods, ours is directly applicable to scenarios with real-time constraints,
such as robotics or mixed reality. To overcome the inherent challenges of
online methods, we make two main contributions. First, to effectively extract
information from the input RGB-D video stream, we jointly estimate geometry and
semantic labels per frame in 3D. A key focus of our approach is to reason about
semantic entities both in the 2D input and the local 3D domain to leverage
differences in spatial context and network architectures. Our method predicts
2D features using an off-the-shelf segmentation network. The extracted 2D
features are refined by a lightweight 3D network to enable reasoning about the
local 3D structure. Second, to efficiently deal with an infinite stream of
input RGB-D frames, a subsequent network serves as a temporal expert predicting
the incremental scene updates by leveraging 2D, 3D, and past information in a
learned manner. These updates are then integrated into a global scene
representation. Using these main contributions, our method can enable scenarios
with real-time constraints and can scale to arbitrary scene sizes by processing
and updating the scene only in a local region defined by the new measurement.
Our experiments demonstrate improved results compared to existing online
methods that purely operate in local regions and show that complementary
sources of information can boost the performance. We provide a thorough
ablation study on the benefits of different architectural as well as
algorithmic design decisions. Our method yields competitive results on the
popular ScanNet benchmark and SceneNN dataset.","['Silvan Weder', 'Francis Engelmann', 'Johannes L. Schönberger', 'Akihito Seki', 'Marc Pollefeys', 'Martin R. Oswald']",2023-11-29T20:30:18Z,http://arxiv.org/abs/2311.18068v2,['cs.CV']
"Leveraging Artificial Intelligence to Promote Awareness in Augmented
  Reality Systems","Recent developments in artificial intelligence (AI) have permeated through an
array of different immersive environments, including virtual, augmented, and
mixed realities. AI brings a wealth of potential that centers on its ability to
critically analyze environments, identify relevant artifacts to a goal or
action, and then autonomously execute decision-making strategies to optimize
the reward-to-risk ratio. However, the inherent benefits of AI are not without
disadvantages as the autonomy and communication methodology can interfere with
the human's awareness of their environment. More specifically in the case of
autonomy, the relevant human-computer interaction literature cites that high
autonomy results in an ""out-of-the-loop"" experience for the human such that
they are not aware of critical artifacts or situational changes that require
their attention. At the same time, low autonomy of an AI system can limit the
human's own autonomy with repeated requests to approve its decisions. In these
circumstances, humans enter into supervisor roles, which tend to increase their
workload and, therefore, decrease their awareness in a multitude of ways. In
this position statement, we call for the development of human-centered AI in
immersive environments to sustain and promote awareness. It is our position
then that we believe with the inherent risk presented in both AI and AR/VR
systems, we need to examine the interaction between them when we integrate the
two to create a new system for any unforeseen risks, and that it is crucial to
do so because of its practical application in many high-risk environments.","['Wangfan Li', 'Rohit Mallick', 'Carlos Toxtli-Hernandez', 'Christopher Flathmann', 'Nathan J. McNeese']",2024-04-23T17:47:51Z,http://arxiv.org/abs/2405.05916v1,['cs.HC']
"MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer
  Vision","Prior to the deep learning era, shape was commonly used to describe the
objects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging are
predominantly diverging from computer vision, where voxel grids, meshes, point
clouds, and implicit surface models are used. This is seen from numerous
shape-related publications in premier vision conferences as well as the growing
popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915
models). For the medical domain, we present a large collection of anatomical
shapes (e.g., bones, organs, vessels) and 3D models of surgical instrument,
called MedShapeNet, created to facilitate the translation of data-driven vision
algorithms to medical applications and to adapt SOTA vision algorithms to
medical problems. As a unique feature, we directly model the majority of shapes
on the imaging data of real patients. As of today, MedShapeNet includes 23
dataset with more than 100,000 shapes that are paired with annotations (ground
truth). Our data is freely accessible via a web interface and a Python
application programming interface (API) and can be used for discriminative,
reconstructive, and variational benchmarks as well as various applications in
virtual, augmented, or mixed reality, and 3D printing. Exemplary, we present
use cases in the fields of classification of brain tumors, facial and skull
reconstructions, multi-class anatomy completion, education, and 3D printing. In
future, we will extend the data and improve the interfaces. The project pages
are: https://medshapenet.ikim.nrw/ and
https://github.com/Jianningli/medshapenet-feedback","['Jianning Li', 'Zongwei Zhou', 'Jiancheng Yang', 'Antonio Pepe', 'Christina Gsaxner', 'Gijs Luijten', 'Chongyu Qu', 'Tiezheng Zhang', 'Xiaoxi Chen', 'Wenxuan Li', 'Marek Wodzinski', 'Paul Friedrich', 'Kangxian Xie', 'Yuan Jin', 'Narmada Ambigapathy', 'Enrico Nasca', 'Naida Solak', 'Gian Marco Melito', 'Viet Duc Vu', 'Afaque R. Memon', 'Christopher Schlachta', 'Sandrine De Ribaupierre', 'Rajnikant Patel', 'Roy Eagleson', 'Xiaojun Chen', 'Heinrich Mächler', 'Jan Stefan Kirschke', 'Ezequiel de la Rosa', 'Patrick Ferdinand Christ', 'Hongwei Bran Li', 'David G. Ellis', 'Michele R. Aizenberg', 'Sergios Gatidis', 'Thomas Küstner', 'Nadya Shusharina', 'Nicholas Heller', 'Vincent Andrearczyk', 'Adrien Depeursinge', 'Mathieu Hatt', 'Anjany Sekuboyina', 'Maximilian Löffler', 'Hans Liebl', 'Reuben Dorent', 'Tom Vercauteren', 'Jonathan Shapey', 'Aaron Kujawa', 'Stefan Cornelissen', 'Patrick Langenhuizen', 'Achraf Ben-Hamadou', 'Ahmed Rekik', 'Sergi Pujades', 'Edmond Boyer', 'Federico Bolelli', 'Costantino Grana', 'Luca Lumetti', 'Hamidreza Salehi', 'Jun Ma', 'Yao Zhang', 'Ramtin Gharleghi', 'Susann Beier', 'Arcot Sowmya', 'Eduardo A. Garza-Villarreal', 'Thania Balducci', 'Diego Angeles-Valdez', 'Roberto Souza', 'Leticia Rittner', 'Richard Frayne', 'Yuanfeng Ji', 'Vincenzo Ferrari', 'Soumick Chatterjee', 'Florian Dubost', 'Stefanie Schreiber', 'Hendrik Mattern', 'Oliver Speck', 'Daniel Haehn', 'Christoph John', 'Andreas Nürnberger', 'João Pedrosa', 'Carlos Ferreira', 'Guilherme Aresta', 'António Cunha', 'Aurélio Campilho', 'Yannick Suter', 'Jose Garcia', 'Alain Lalande', 'Vicky Vandenbossche', 'Aline Van Oevelen', 'Kate Duquesne', 'Hamza Mekhzoum', 'Jef Vandemeulebroucke', 'Emmanuel Audenaert', 'Claudia Krebs', 'Timo van Leeuwen', 'Evie Vereecke', 'Hauke Heidemeyer', 'Rainer Röhrig', 'Frank Hölzle', 'Vahid Badeli', 'Kathrin Krieger', 'Matthias Gunzer', 'Jianxu Chen', 'Timo van Meegdenburg', 'Amin Dada', 'Miriam Balzer', 'Jana Fragemann', 'Frederic Jonske', 'Moritz Rempe', 'Stanislav Malorodov', 'Fin H. Bahnsen', 'Constantin Seibold', 'Alexander Jaus', 'Zdravko Marinov', 'Paul F. Jaeger', 'Rainer Stiefelhagen', 'Ana Sofia Santos', 'Mariana Lindo', 'André Ferreira', 'Victor Alves', 'Michael Kamp', 'Amr Abourayya', 'Felix Nensa', 'Fabian Hörst', 'Alexander Brehmer', 'Lukas Heine', 'Yannik Hanusrichter', 'Martin Weßling', 'Marcel Dudda', 'Lars E. Podleska', 'Matthias A. Fink', 'Julius Keyl', 'Konstantinos Tserpes', 'Moon-Sung Kim', 'Shireen Elhabian', 'Hans Lamecker', 'Dženan Zukić', 'Beatriz Paniagua', 'Christian Wachinger', 'Martin Urschler', 'Luc Duong', 'Jakob Wasserthal', 'Peter F. Hoyer', 'Oliver Basu', 'Thomas Maal', 'Max J. H. Witjes', 'Gregor Schiele', 'Ti-chiun Chang', 'Seyed-Ahmad Ahmadi', 'Ping Luo', 'Bjoern Menze', 'Mauricio Reyes', 'Thomas M. Deserno', 'Christos Davatzikos', 'Behrus Puladi', 'Pascal Fua', 'Alan L. Yuille', 'Jens Kleesiek', 'Jan Egger']",2023-08-30T16:52:20Z,http://arxiv.org/abs/2308.16139v5,"['cs.CV', 'cs.DB', 'cs.LG', '68T01']"
"A Survey on Synchronous Augmented, Virtual and Mixed Reality Remote
  Collaboration Systems","Remote collaboration systems have become increasingly important in today's
society, especially during times where physical distancing is advised.
Industry, research and individuals face the challenging task of collaborating
and networking over long distances. While video and teleconferencing are
already widespread, collaboration systems in augmented, virtual, and mixed
reality are still a niche technology. We provide an overview of recent
developments of synchronous remote collaboration systems and create a taxonomy
by dividing them into three main components that form such systems:
Environment, Avatars, and Interaction. A thorough overview of existing systems
is given, categorising their main contributions in order to help researchers
working in different fields by providing concise information about specific
topics such as avatars, virtual environment, visualisation styles and
interaction. The focus of this work is clearly on synchronised collaboration
from a distance. A total of 82 unique systems for remote collaboration are
discussed, including more than 100 publications and 25 commercial systems.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2021-02-11T13:33:51Z,http://arxiv.org/abs/2102.05998v1,"['cs.HC', 'cs.CV', 'cs.GT']"
Lets Make A Story Measuring MR Child Engagement,"We present the result of a pilot study measuring child engagement with the
Lets Make A Story system, a novel mixed reality, MR, collaborative storytelling
system designed for grandparents and grandchildren. We compare our MR
experience against an equivalent paper story experience. The goal of our pilot
was to test the system with actual child users and assess the goodness of using
metrics of time, user generated story content and facial expression analysis as
metrics of child engagement. We find that multiple confounding variables make
these metrics problematic including attribution of engagement time, spontaneous
non-story related conversation and having the childs full forward face
continuously in view during the story. We present our platform and experiences
and our finding that the strongest metric was user comments in the
post-experiential interview.","['Duotun Wang', 'Jennifer Healey', 'Jing Qian', 'Curtis Wigington', 'Tong Sun', 'Huaishu Peng']",2021-04-13T22:36:50Z,http://arxiv.org/abs/2104.06536v1,"['cs.HC', 'H.5.1; H.5.2']"
Towards Real-World Category-level Articulation Pose Estimation,"Human life is populated with articulated objects. Current Category-level
Articulation Pose Estimation (CAPE) methods are studied under the
single-instance setting with a fixed kinematic structure for each category.
Considering these limitations, we reform this problem setting for real-world
environments and suggest a CAPE-Real (CAPER) task setting. This setting allows
varied kinematic structures within a semantic category, and multiple instances
to co-exist in an observation of real world. To support this task, we build an
articulated model repository ReArt-48 and present an efficient dataset
generation pipeline, which contains Fast Articulated Object Modeling (FAOM) and
Semi-Authentic MixEd Reality Technique (SAMERT). Accompanying the pipeline, we
build a large-scale mixed reality dataset ReArtMix and a real world dataset
ReArtVal. We also propose an effective framework ReArtNOCS that exploits RGB-D
input to estimate part-level pose for multiple instances in a single forward
pass. Extensive experiments demonstrate that the proposed ReArtNOCS can achieve
good performance on both CAPER and CAPE settings. We believe it could serve as
a strong baseline for future research on the CAPER task.","['Liu Liu', 'Han Xue', 'Wenqiang Xu', 'Haoyuan Fu', 'Cewu Lu']",2021-05-07T13:41:16Z,http://arxiv.org/abs/2105.03260v1,"['cs.CV', 'cs.RO']"
"Investigating Modes of Activity and Guidance for Mediating Museum
  Exhibits in Mixed Reality","We present an exploratory case study describing the design and realisation of
a ''pure mixed reality'' application in a museum setting, where we investigate
the potential of using Microsoft's HoloLens for object-centred museum
mediation. Our prototype supports non-expert visitors observing a sculpture by
offering interpretation that is linked to visual properties of the museum
object. The design and development of our research prototype is based on a
two-stage visitor observation study and a formative study we conducted prior to
the design of the application. We present a summary of our findings from these
studies and explain how they have influenced our user-centred content creation
and the interaction design of our prototype. We are specifically interested in
investigating to what extent different constructs of initiative influence the
learning and user experience. Thus, we detail three modes of activity that we
realised in our prototype. Our case study is informed by research in the area
of human-computer interaction, the humanities and museum practice. Accordingly,
we discuss core concepts, such as gaze-based interaction, object-centred
learning, presence, and modes of activity and guidance with a transdisciplinary
perspective.","['Katrin Glinka', 'Patrick Tobias Fischer', 'Claudia Müller-Birn', 'Silke Krohn']",2021-06-25T08:17:34Z,http://arxiv.org/abs/2106.13494v1,['cs.HC']
"Mixed reality technologies for people with dementia: Participatory
  evaluation methods","Technologies can support people with early onset dementia (PwD) to aid them
in Instrumental Activities of Daily Living (IADL). The integration of physical
and virtual realities in Mixed reality technologies (MRTs) could provide
scalable and deployable options in developing prompting systems for PwD.
However, these emerging technologies should be evaluated and investigated for
feasibility with PwD. Survey instruments such as SUS, SUPR-Q and ethnographic
methods that are used for usability evaluation of websites and apps are used to
evaluate and study MRTs. However, PwD who cannot provide written and verbal
feedback are unable to participate in these studies. MRTs also present
challenges due to different ways in which physical and virtual realities could
be coupled. Experiences with physical, virtual and the couplings between the
two are to be considered in evaluating MRTs.","['Shital Desai', 'Arlene Astell']",2021-06-07T00:00:15Z,http://arxiv.org/abs/2107.07336v1,['cs.HC']
Single-Camera 3D Head Fitting for Mixed Reality Clinical Applications,"We address the problem of estimating the shape of a person's head, defined as
the geometry of the complete head surface, from a video taken with a single
moving camera, and determining the alignment of the fitted 3D head for all
video frames, irrespective of the person's pose. 3D head reconstructions
commonly tend to focus on perfecting the face reconstruction, leaving the scalp
to a statistical approximation. Our goal is to reconstruct the head model of
each person to enable future mixed reality applications. To do this, we recover
a dense 3D reconstruction and camera information via structure-from-motion and
multi-view stereo. These are then used in a new two-stage fitting process to
recover the 3D head shape by iteratively fitting a 3D morphable model of the
head with the dense reconstruction in canonical space and fitting it to each
person's head, using both traditional facial landmarks and scalp features
extracted from the head's segmentation mask. Our approach recovers consistent
geometry for varying head shapes, from videos taken by different people, with
different smartphones, and in a variety of environments from living rooms to
outdoor spaces.","['Tejas Mane', 'Aylar Bayramova', 'Kostas Daniilidis', 'Philippos Mordohai', 'Elena Bernardis']",2021-09-06T21:03:52Z,http://arxiv.org/abs/2109.02740v2,['cs.CV']
Eye-Tracking-Based Design of Mixed Reality Learning Environments in STEM,"With the advent of commercially available Mixed-Reality(MR)-headsets in
recent years MR-assisted learning started to play a vital role in educational
research, especially related to STEM (science, technology, engineering and
mathematics) education. Along with these developments it seems viable to
further frameworks and structured design processes for MR-based learning
environments. Instead of a widely applicable framework for designing
educational MR applications, we here consider the case of virtually enhancing
physical hands-on experiments in STEM, where students are given a certain
problem to solve, and how to design these. For this focused realm, we suggest
an empirically driven problemand user-centred design process for MR
applications to get novices to act more like experts and exemplify it for a
specific experiment and problem set containing a non-trivial electric circuit
with capacitors and coils.","['Dörte Sonntag', 'Oliver Bodensiek']",2021-09-07T08:48:01Z,http://arxiv.org/abs/2109.02940v1,['physics.ed-ph']
"The Object at Hand: Automated Editing for Mixed Reality Video Guidance
  from Hand-Object Interactions","In this paper, we concern with the problem of how to automatically extract
the steps that compose real-life hand activities. This is a key competence
towards processing, monitoring and providing video guidance in Mixed Reality
systems. We use egocentric vision to observe hand-object interactions in
real-world tasks and automatically decompose a video into its constituent
steps. Our approach combines hand-object interaction (HOI) detection, object
similarity measurement and a finite state machine (FSM) representation to
automatically edit videos into steps. We use a combination of Convolutional
Neural Networks (CNNs) and the FSM to discover, edit cuts and merge segments
while observing real hand activities. We evaluate quantitatively and
qualitatively our algorithm on two datasets: the GTEA\cite{li2015delving}, and
a new dataset we introduce for Chinese Tea making. Results show our method is
able to segment hand-object interaction videos into key step segments with high
levels of precision.","['Yao Lu', 'Walterio W. Mayol-Cuevas']",2021-09-29T22:24:25Z,http://arxiv.org/abs/2109.14744v1,['cs.CV']
Learning robot motor skills with mixed reality,"Mixed Reality (MR) has recently shown great success as an intuitive interface
for enabling end-users to teach robots. Related works have used MR interfaces
to communicate robot intents and beliefs to a co-located human, as well as
developed algorithms for taking multi-modal human input and learning complex
motor behaviors. Even with these successes, enabling end-users to teach robots
complex motor tasks still poses a challenge because end-user communication is
highly task dependent and world knowledge is highly varied. We propose a
learning framework where end-users teach robots a) motion demonstrations, b)
task constraints, c) planning representations, and d) object information, all
of which are integrated into a single motor skill learning framework based on
Dynamic Movement Primitives (DMPs). We hypothesize that conveying this world
knowledge will be intuitive with an MR interface, and that a sample-efficient
motor skill learning framework which incorporates varied modalities of world
knowledge will enable robots to effectively solve complex tasks.","['Eric Rosen', 'Sreehari Rammohan', 'Devesh Jha']",2022-03-21T20:25:40Z,http://arxiv.org/abs/2203.11324v1,"['cs.RO', 'cs.LG']"
"ShapeFindAR: Exploring In-Situ Spatial Search for Physical Artifact
  Retrieval using Mixed Reality","Personal fabrication is made more accessible through repositories like
Thingiverse, as they replace modeling with retrieval. However, they require
users to translate spatial requirements to keywords, which paints an incomplete
picture of physical artifacts: proportions or morphology are non-trivially
encoded through text only. We explore a vision of in-situ spatial search for
(future) physical artifacts, and present ShapeFindAR, a mixed-reality tool to
search for 3D models using in-situ sketches blended with textual queries. With
ShapeFindAR, users search for geometry, and not necessarily precise labels,
while coupling the search process to the physical environment (e.g., by
sketching in-situ, extracting search terms from objects present, or tracing
them). We developed ShapeFindAR for HoloLens 2, connected to a database of
3D-printable artifacts. We specify in-situ spatial search, describe its
advantages, and present walkthroughs using ShapeFindAR, which highlight novel
ways for users to articulate their wishes, without requiring complex modeling
tools or profound domain knowledge.","['Evgeny Stemasov', 'Tobias Wagner', 'Jan Gugenheimer', 'Enrico Rukzio']",2022-03-31T17:37:26Z,http://arxiv.org/abs/2203.17211v1,"['cs.HC', 'cs.GR', 'H.5.2']"
Mixed Reality as Communication Medium for Human-Robot Collaboration,"Humans engaged in collaborative activities are naturally able to convey their
intentions to teammates through multi-modal communication, which is made up of
explicit and implicit cues. Similarly, a more natural form of human-robot
collaboration may be achieved by enabling robots to convey their intentions to
human teammates via multiple communication channels. In this paper, we
postulate that a better communication may take place should collaborative
robots be able to anticipate their movements to human teammates in an intuitive
way. In order to support such a claim, we propose a robot system's architecture
through which robots can communicate planned motions to human teammates
leveraging a Mixed Reality interface powered by modern head-mounted displays.
Specifically, the robot's hologram, which is superimposed to the real robot in
the human teammate's point of view, shows the robot's future movements,
allowing the human to understand them in advance, and possibly react to them in
an appropriate way. We conduct a preliminary user study to evaluate the
effectiveness of the proposed anticipatory visualization during a complex
collaborative task. The experimental results suggest that an improved and more
natural collaboration can be achieved by employing this anticipatory
communication mode.","['Simone Macciò', 'Alessandro Carfì', 'Fulvio Mastrogiovanni']",2022-06-30T15:57:17Z,http://arxiv.org/abs/2206.15380v1,['cs.RO']
Real Time Egocentric Segmentation for Video-self Avatar in Mixed Reality,"In this work we present our real-time egocentric body segmentation algorithm.
Our algorithm achieves a frame rate of 66 fps for an input resolution of
640x480, thanks to our shallow network inspired in Thundernet's architecture.
Besides, we put a strong emphasis on the variability of the training data. More
concretely, we describe the creation process of our Egocentric Bodies
(EgoBodies) dataset, composed of almost 10,000 images from three datasets,
created both from synthetic methods and real capturing. We conduct experiments
to understand the contribution of the individual datasets; compare Thundernet
model trained with EgoBodies with simpler and more complex previous approaches
and discuss their corresponding performance in a real-life setup in terms of
segmentation quality and inference times. The described trained semantic
segmentation algorithm is already integrated in an end-to-end system for Mixed
Reality (MR), making it possible for users to see his/her own body while being
immersed in a MR scene.","['Ester Gonzalez-Sosa', 'Andrija Gajic', 'Diego Gonzalez-Morin', 'Guillermo Robledo', 'Pablo Perez', 'Alvaro Villegas']",2022-07-04T10:00:16Z,http://arxiv.org/abs/2207.01296v1,['cs.CV']
HoloLens 2 Technical Evaluation as Mixed Reality Guide,"Mixed Reality (MR) is an evolving technology lying in the continuum spanned
by related technologies such as Virtual Reality (VR) and Augmented Reality
(AR), and creates an exciting way of interacting with people and the
environment. This technology is fast becoming a tool used by many people,
potentially improving living environments and work efficiency. Microsoft
HoloLens has played an important role in the progress of MR, from the first
generation to the second generation. In this paper, we systematically evaluate
the functions of applicable functions in HoloLens 2. These evaluations can
serve as a performance benchmark that can help people who need to use this
instrument for research or applications in the future. The detailed tests and
the performance evaluation of the different functionalities show the usability
and possible limitations of each function. We mainly divide the experiment into
the existing functions of the HoloLens 1, the new functions of the HoloLens 2,
and the use of research mode. This research results will be useful for MR
researchers who want to use HoloLens 2 as a research tool to design their own
MR applications.","['Hung-Jui Guo', 'Balakrishnan Prabhakaran']",2022-07-19T21:19:23Z,http://arxiv.org/abs/2207.09554v1,['cs.HC']
Mixed Reality for Mechanical Design and Assembly Planning,"Design for Manufacturing and Assembly (DFMA) is a crucial design stage within
the heavy vehicle manufacturing process that involves optimising the order and
feasibility of the parts assembly process to reduce manufacturing complexity
and overall cost. Existing work has focused on conducting DFMA within virtual
environments to reduce manufacturing costs, but users are less able to relate
and compare physical characteristics of a virtual component with real physical
objects. Therefore, a Mixed Reality (MR) application is developed for engineers
to visualise and manipulate assembly parts virtually, conduct and plan out an
assembly within its intended physical environment. Two pilot evaluations were
conducted with both engineering professionals and non-engineers to assess
effectiveness of the software for assembly planning. Usability results suggest
that the application is overall usable (M=56.1, SD=7.89), and participants felt
a sense of involvement in the activity (M=13.1, SD=3.3). Engineering
professionals see the application as a useful and cost-effective tool for
optimising their mechanical assembly designs.","['Emran Poh', 'Kyrin Liong', 'Jeannie Lee']",2022-09-02T19:41:29Z,http://arxiv.org/abs/2209.01252v1,['cs.HC']
MR4MR: Mixed Reality for Melody Reincarnation,"There is a long history of an effort made to explore musical elements with
the entities and spaces around us, such as musique concr\`ete and ambient
music. In the context of computer music and digital art, interactive
experiences that concentrate on the surrounding objects and physical spaces
have also been designed. In recent years, with the development and
popularization of devices, an increasing number of works have been designed in
Extended Reality to create such musical experiences. In this paper, we describe
MR4MR, a sound installation work that allows users to experience melodies
produced from interactions with their surrounding space in the context of Mixed
Reality (MR). Using HoloLens, an MR head-mounted display, users can bump
virtual objects that emit sound against real objects in their surroundings.
Then, by continuously creating a melody following the sound made by the object
and re-generating randomly and gradually changing melody using music generation
machine learning models, users can feel their ambient melody ""reincarnating"".","['Atsuya Kobayashi', 'Ryogo Ishino', 'Ryuku Nobusue', 'Takumi Inoue', 'Keisuke Okazaki', 'Shoma Sawa', 'Nao Tokui']",2022-09-15T03:23:29Z,http://arxiv.org/abs/2209.07023v1,"['cs.HC', 'cs.AI']"
Holo-Dex: Teaching Dexterity with Immersive Mixed Reality,"A fundamental challenge in teaching robots is to provide an effective
interface for human teachers to demonstrate useful skills to a robot. This
challenge is exacerbated in dexterous manipulation, where teaching
high-dimensional, contact-rich behaviors often require esoteric teleoperation
tools. In this work, we present Holo-Dex, a framework for dexterous
manipulation that places a teacher in an immersive mixed reality through
commodity VR headsets. The high-fidelity hand pose estimator onboard the
headset is used to teleoperate the robot and collect demonstrations for a
variety of general-purpose dexterous tasks. Given these demonstrations, we use
powerful feature learning combined with non-parametric imitation to train
dexterous skills. Our experiments on six common dexterous tasks, including
in-hand rotation, spinning, and bottle opening, indicate that Holo-Dex can both
collect high-quality demonstration data and train skills in a matter of hours.
Finally, we find that our trained skills can exhibit generalization on objects
not seen in training. Videos of Holo-Dex are available at
https://holo-dex.github.io.","['Sridhar Pandian Arunachalam', 'Irmak Güzey', 'Soumith Chintala', 'Lerrel Pinto']",2022-10-12T17:59:02Z,http://arxiv.org/abs/2210.06463v1,"['cs.RO', 'cs.AI', 'cs.CV', 'cs.HC', 'cs.LG']"
"Augmented Reality and Mixed Reality Measurement Under Different
  Environments: A Survey on Head-Mounted Devices","Augmented Reality (AR) and Mixed Reality (MR) have been two of the most
explosive research topics in the last few years. Head-Mounted Devices (HMDs)
are essential intermediums for using AR and MR technology, playing an important
role in the research progress in these two areas. Behavioral research with
users is one way of evaluating the technical progress and effectiveness of
HMDs. In addition, AR and MR technology is dependent upon virtual interactions
with the real environment. Thus, conditions in real environments can be a
significant factor for AR and MR measurements with users. In this paper, we
survey 87 environmental-related HMD papers with measurements from users,
spanning over 32 years. We provide a thorough review of AR- and MR-related user
experiments with HMDs under different environmental factors. Then, we summarize
trends in this literature over time using a new classification method with four
environmental factors, the presence or absence of user feedback in behavioral
experiments, and ten main categories to subdivide these papers (e.g., domain
and method of user assessment). We also categorize characteristics of the
behavioral experiments, showing similarities and differences among papers.","['Hung-Jui Guo', 'Jonathan Z. Bakdash', 'Laura R. Marusich', 'Balakrishnan Prabhakaran']",2022-10-29T02:03:56Z,http://arxiv.org/abs/2210.16463v1,['cs.HC']
"Imitation Learning based Auto-Correction of Extrinsic Parameters for A
  Mixed-Reality Setup","In this paper, we discuss an imitation learning based method for reducing the
calibration error for a mixed reality system consisting of a vision sensor and
a projector. Unlike a head mounted display, in this setup, augmented
information is available to a human subject via the projection of a scene into
the real world. Inherently, the camera and projector need to be calibrated as a
stereo setup to project accurate information in 3D space. Previous calibration
processes require multiple recording and parameter tuning steps to achieve the
desired calibration, which is usually time consuming process. In order to avoid
such tedious calibration, we train a CNN model to iteratively correct the
extrinsic offset given a QR code and a projected pattern. We discuss the
overall system setup, data collection for training, and results of the
auto-correction model.","['Shubham Sonawani', 'Yifan Zhou', 'Heni Ben Amor']",2022-12-16T21:34:33Z,http://arxiv.org/abs/2212.08720v1,['cs.RO']
A Framework for Active Haptic Guidance Using Robotic Haptic Proxies,"Haptic feedback is an important component of creating an immersive mixed
reality experience. Traditionally, haptic forces are rendered in response to
the user's interactions with the virtual environment. In this work, we explore
the idea of rendering haptic forces in a proactive manner, with the explicit
intention to influence the user's behavior through compelling haptic forces. To
this end, we present a framework for active haptic guidance in mixed reality,
using one or more robotic haptic proxies to influence user behavior and deliver
a safer and more immersive virtual experience. We provide details on common
challenges that need to be overcome when implementing active haptic guidance,
and discuss example applications that show how active haptic guidance can be
used to influence the user's behavior. Finally, we apply active haptic guidance
to a virtual reality navigation problem, and conduct a user study that
demonstrates how active haptic guidance creates a safer and more immersive
experience for users.","['Niall L. Williams', 'Nicholas Rewkowski', 'Jiasheng Li', 'Ming C. Lin']",2023-01-12T21:43:34Z,http://arxiv.org/abs/2301.05311v2,"['cs.RO', 'cs.GR']"
"MR.Brick: Designing A Remote Mixed-reality Educational Game System for
  Promoting Children's Social & Collaborative Skills","Children are one of the groups most influenced by COVID-19-related social
distancing, and a lack of contact with peers can limit their opportunities to
develop social and collaborative skills. However, remote socialization and
collaboration as an alternative approach is still a great challenge for
children. This paper presents MR.Brick, a Mixed Reality (MR) educational game
system that helps children adapt to remote collaboration. A controlled
experimental study involving 24 children aged six to ten was conducted to
compare MR.Brick with the traditional video game by measuring their social and
collaborative skills and analyzing their multi-modal playing behaviours. The
results showed that MR.Brick was more conducive to children's remote
collaboration experience than the traditional video game. Given the lack of
training systems designed for children to collaborate remotely, this study may
inspire interaction design and educational research in related fields.","['Yudan Wu', 'Shanhe You', 'Zixuan Guo', 'Xiangyang Li', 'Guyue Zhou', 'Jiangtao Gong']",2023-01-18T05:14:57Z,http://arxiv.org/abs/2301.07310v2,"['cs.HC', 'H.5.2']"
"ChameleonControl: Teleoperating Real Human Surrogates through Mixed
  Reality Gestural Guidance for Remote Hands-on Classrooms","We present ChameleonControl, a real-human teleoperation system for scalable
remote instruction in hands-on classrooms. In contrast to existing video or
AR/VR-based remote hands-on education, ChameleonControl uses a real human as a
surrogate of a remote instructor. Building on existing human-based telepresence
approaches, we contribute a novel method to teleoperate a human surrogate
through synchronized mixed reality hand gestural navigation and verbal
communication. By overlaying the remote instructor's virtual hands in the local
user's MR view, the remote instructor can guide and control the local user as
if they were physically present. This allows the local user/surrogate to
synchronize their hand movements and gestures with the remote instructor,
effectively teleoperating a real human. We deploy and evaluate our system in
classrooms of physiotherapy training, as well as other application domains such
as mechanical assembly, sign language and cooking lessons. The study results
confirm that our approach can increase engagement and the sense of co-presence,
showing potential for the future of remote hands-on classrooms.","['Mehrad Faridan', 'Bheesha Kumari', 'Ryo Suzuki']",2023-02-21T23:11:41Z,http://arxiv.org/abs/2302.11053v1,['cs.HC']
CAstelet in Virtual reality for shadOw AVatars (CAVOAV),"After an overview of the use of digital shadows in computing science research
projects with cultural and social impacts and a focus on recent researches and
insights on virtual theaters, this paper introduces a research mixing the
manipulation of shadow avatars and the building of a virtual theater setup
inspired by traditional shadow theater (or ``castelet'' in french) in a mixed
reality environment. It describes the virtual 3D setup, the nature of the
shadow avatars and the issues of directing believable interactions between
virtual avatars and physical performers on stage. Two modalities of shadow
avatars direction are exposed. Some results of the research are illustrated in
two use cases: the development of theatrical creativity in mixed reality
through pedagogical workshops; and an artistic achievement in ''The Shadow''
performance, after H. C. Andersen.","['Georges Gagneré', 'Anastasiia Ternova']",2023-03-13T10:31:09Z,http://arxiv.org/abs/2303.06981v1,['cs.GR']
"HoloTouch: Interacting with Mixed Reality Visualizations Through
  Smartphone Proxies","We contribute interaction techniques for augmenting mixed reality (MR)
visualizations with smartphone proxies. By combining head-mounted displays
(HMDs) with mobile touchscreens, we can augment low-resolution holographic 3D
charts with precise touch input, haptics feedback, high-resolution 2D graphics,
and physical manipulation. Our approach aims to complement both MR and physical
visualizations. Most current MR visualizations suffer from unreliable tracking,
low visual resolution, and imprecise input. Data physicalizations on the other
hand, although allowing for natural physical manipulation, are limited in
dynamic and interactive modification. We demonstrate how mobile devices such as
smartphones or tablets can serve as physical proxies for MR data interactions,
creating dynamic visualizations that support precise manipulation and rich
input and output. We describe 6 interaction techniques that leverage the
combined physicality, sensing, and output capabilities of HMDs and smartphones,
and demonstrate those interactions via a prototype system. Based on an
evaluation, we outline opportunities for combining the advantages of both MR
and physical charts.","['Neil Chulpongsatorn', 'Wesley Willett', 'Ryo Suzuki']",2023-03-15T20:19:13Z,http://arxiv.org/abs/2303.08916v1,['cs.HC']
Accessible Robot Control in Mixed Reality,"A novel method to control the Spot robot of Boston Dynamics by Hololens 2 is
proposed. This method is mainly designed for people with physical disabilities,
users can control the robot's movement and robot arm without using their hands.
The eye gaze tracking and head motion tracking technologies of Hololens 2 are
utilized for sending control commands. The movement of the robot would follow
the eye gaze and the robot arm would mimic the pose of the user's head. Through
our experiment, our method is comparable with the traditional control method by
joystick in both time efficiency and user experience. Demo can be found on our
project webpage: https://zhangganlin.github.io/Holo-Spot-Page/index.html","['Ganlin Zhang', 'Deheng Zhang', 'Longteng Duan', 'Guo Han']",2023-06-04T16:05:26Z,http://arxiv.org/abs/2306.02393v1,"['cs.RO', 'cs.CV']"
"Affordance segmentation of hand-occluded containers from exocentric
  images","Visual affordance segmentation identifies the surfaces of an object an agent
can interact with. Common challenges for the identification of affordances are
the variety of the geometry and physical properties of these surfaces as well
as occlusions. In this paper, we focus on occlusions of an object that is
hand-held by a person manipulating it. To address this challenge, we propose an
affordance segmentation model that uses auxiliary branches to process the
object and hand regions separately. The proposed model learns affordance
features under hand-occlusion by weighting the feature map through hand and
object segmentation. To train the model, we annotated the visual affordances of
an existing dataset with mixed-reality images of hand-held containers in
third-person (exocentric) images. Experiments on both real and mixed-reality
images show that our model achieves better affordance segmentation and
generalisation than existing models.","['Tommaso Apicella', 'Alessio Xompero', 'Edoardo Ragusa', 'Riccardo Berta', 'Andrea Cavallaro', 'Paolo Gastaldo']",2023-08-22T07:14:29Z,http://arxiv.org/abs/2308.11233v1,['cs.CV']
"Realistic Volume Rendering with Environment-Synced Illumination in Mixed
  Reality","Interactive volume visualization using a mixed reality (MR) system helps
provide users with an intuitive spatial perception of volumetric data. Due to
sophisticated requirements of user interaction and vision when using MR
head-mounted display (HMD) devices, the conflict between the realisticness and
efficiency of direct volume rendering (DVR) is yet to be resolved. In this
paper, a new MR visualization framework that supports interactive realistic DVR
is proposed. An efficient illumination estimation method is used to identify
the high dynamic range (HDR) environment illumination captured using a panorama
camera. To improve the visual quality of Monte Carlo-based DVR, a new
spatio-temporal denoising algorithm is designed. Based on a reprojection
strategy, it makes full use of temporal coherence between adjacent frames and
spatial coherence between the two screens of an HMD to optimize MR rendering
quality. Several MR development modules are also developed for related devices
to efficiently and stably display the DVR results in an MR HMD. Experimental
results demonstrate that our framework can better support immersive and
intuitive user perception during MR viewing than existing MR solutions.","['Haojie Cheng', 'Chunxiao Xu', 'Xujing Chen', 'Zhenxin Chen', 'Jiajun Wang', 'Lingxiao Zhao']",2023-09-05T03:11:38Z,http://arxiv.org/abs/2309.01916v1,['cs.GR']
"LLMR: Real-time Prompting of Interactive Worlds using Large Language
  Models","We present Large Language Model for Mixed Reality (LLMR), a framework for the
real-time creation and modification of interactive Mixed Reality experiences
using LLMs. LLMR leverages novel strategies to tackle difficult cases where
ideal training data is scarce, or where the design goal requires the synthesis
of internal dynamics, intuitive analysis, or advanced interactivity. Our
framework relies on text interaction and the Unity game engine. By
incorporating techniques for scene understanding, task planning,
self-debugging, and memory management, LLMR outperforms the standard GPT-4 by
4x in average error rate. We demonstrate LLMR's cross-platform interoperability
with several example worlds, and evaluate it on a variety of creation and
modification tasks to show that it can produce and edit diverse objects, tools,
and scenes. Finally, we conducted a usability study (N=11) with a diverse set
that revealed participants had positive experiences with the system and would
use it again.","['Fernanda De La Torre', 'Cathy Mengying Fang', 'Han Huang', 'Andrzej Banburski-Fahey', 'Judith Amores Fernandez', 'Jaron Lanier']",2023-09-21T17:37:01Z,http://arxiv.org/abs/2309.12276v3,"['cs.HC', 'cs.AI', 'cs.CL', 'cs.ET']"
"Mixed Reality Environment and High-Dimensional Continuification Control
  for Swarm Robotics","Many new methodologies for the control of large-scale multi-agent systems are
based on macroscopic representations of the emerging systemdynamics, in the
form of continuum approximations of large ensembles. These techniques, that are
typically developed in the limit case of an infinite number of agents, are
usually validated only through numerical simulations. In this paper, we
introduce a mixed reality set-up for testing swarm robotics techniques,
focusing on the macroscopic collective motion of robotic swarms. This hybrid
apparatus combines both real differential drive robots and virtual agents to
create a heterogeneous swarm of tunable size. We also extend
continuification-based control methods for swarms to higher dimensions, and
assess experimentally their validity in the new platform. Our study
demonstrates the effectiveness of the platform for conducting large-scale swarm
robotics experiments, and it contributes new theoretical insights into control
algorithms exploiting continuification approaches.","['Gian Carlo Maffettone', 'Lorenzo Liguori', 'Eduardo Palermo', 'Mario di Bernardo', 'Maurizio Porfiri']",2023-10-02T19:05:24Z,http://arxiv.org/abs/2310.01573v3,"['cs.RO', 'cs.SY', 'eess.SY']"
"Visualizing Causality in Mixed Reality for Manual Task Learning: An
  Exploratory Study","Mixed Reality (MR) is gaining prominence in manual task skill learning due to
its in-situ, embodied, and immersive experience. To teach manual tasks, current
methodologies break the task into hierarchies (tasks into subtasks) and
visualize the current subtask and future in terms of causality. Existing
psychology literature also shows that humans learn tasks by breaking them into
hierarchies. In order to understand the design space of information visualized
to the learner for better task understanding, we conducted a user study with 48
users. The study was conducted using a complex assembly task, which involves
learning of both actions and tool usage. We aim to explore the effect of
visualization of causality in the hierarchy for manual task learning in MR by
four options: no causality, event level causality, interaction level causality,
and gesture level causality. The results show that the user understands and
performs best when all the level of causality is shown to the user. Based on
the results, we further provide design recommendations and in-depth discussions
for future manual task learning systems.","['Rahul Jain', 'Jingyu Shi', 'Andrew Benton', 'Moiz Rasheed', 'Hyungjun Doh', 'Subramanian Chidambaram', 'Karthik Ramani']",2023-10-19T21:35:11Z,http://arxiv.org/abs/2310.13167v3,['cs.HC']
"SoundShift: Exploring Sound Manipulations for Accessible Mixed-Reality
  Awareness","Mixed-reality (MR) soundscapes blend real-world sound with virtual audio from
hearing devices, presenting intricate auditory information that is hard to
discern and differentiate. This is particularly challenging for blind or
visually impaired individuals, who rely on sounds and descriptions in their
everyday lives. To understand how complex audio information is consumed, we
analyzed online forum posts within the blind community, identifying prevailing
challenges, needs, and desired solutions. We synthesized the results and
propose SoundShift for increasing MR sound awareness, which includes six sound
manipulations: Transparency Shift, Envelope Shift, Position Shift, Style Shift,
Time Shift, and Sound Append. To evaluate the effectiveness of SoundShift, we
conducted a user study with 18 blind participants across three simulated MR
scenarios, where participants identified specific sounds within intricate
soundscapes. We found that SoundShift increased MR sound awareness and
minimized cognitive load. Finally, we developed three real-world example
applications to demonstrate the practicality of SoundShift.","['Ruei-Che Chang', 'Chia-Sheng Hung', 'Bing-Yu Chen', 'Dhruv Jain', 'Anhong Guo']",2024-01-20T02:57:50Z,http://arxiv.org/abs/2401.11095v2,"['cs.HC', 'cs.SD', 'eess.AS']"
"BioNet-XR: Biological Network Visualization Framework for Virtual
  Reality and Mixed Reality Environments","Protein-protein interaction networks (PPIN) enable the study of cellular
processes in organisms. Visualizing PPINs in extended reality (XR), including
virtual reality (VR) and mixed reality (MR), is crucial for exploring
subnetworks, evaluating protein positions, and collaboratively analyzing and
discussing on networks with the help of recent technological advancements.
Here, we present BioNet-XR, a 3D visualization framework, to visualize PPINs in
VR and MR environments. BioNet-XR was developed with the Unity3D game engine.
Our framework provides state-of-the-art methods and visualization features
including teleportation between nodes, general and first-person view to explore
the network, subnetwork construction via PageRank, Steiner tree, and all-pair
shortest path algorithms for a given set of initial nodes. We used usability
tests to gather feedback from both specialists (bioinformaticians) and
generalists (multidisciplinary groups), addressing the need for usability
evaluations of visualization tools. In the MR version of BioNet-XR, users can
seamlessly transition to real-world environments and interact with protein
interaction networks. BioNet-XR is highly modular and adaptable for
visualization of other biological networks, such as metabolic and regulatory
networks, and extension with additional network methods.","['Busra Senderin', 'Nurcan Tuncbag', 'Elif Surer']",2024-02-06T12:20:10Z,http://arxiv.org/abs/2402.03946v1,['cs.MM']
"Spatial Assisted Human-Drone Collaborative Navigation and Interaction
  through Immersive Mixed Reality","Aerial robots have the potential to play a crucial role in assisting humans
with complex and dangerous tasks. Nevertheless, the future industry demands
innovative solutions to streamline the interaction process between humans and
drones to enable seamless collaboration and efficient co-working. In this
paper, we present a novel tele-immersive framework that promotes cognitive and
physical collaboration between humans and robots through Mixed Reality (MR).
This framework incorporates a novel bi-directional spatial awareness and a
multi-modal virtual-physical interaction approaches. The former seamlessly
integrates the physical and virtual worlds, offering bidirectional egocentric
and exocentric environmental representations. The latter, leveraging the
proposed spatial representation, further enhances the collaboration combining a
robot planning algorithm for obstacle avoidance with a variable admittance
control. This allows users to issue commands based on virtual forces while
maintaining compatibility with the environment map. We validate the proposed
approach by performing several collaborative planning and exploration tasks
involving a drone and an user equipped with a MR headset.","['Luca Morando', 'Giuseppe Loianno']",2024-02-06T15:17:09Z,http://arxiv.org/abs/2402.04070v2,"['cs.RO', 'cs.SY', 'eess.SY']"
Another Body in the World: Flusserian Freedom in Mixed Reality,"In Flusserian view of media history, humans often misperceive the world
projected by media to be the world itself, leading to a loss of freedom. This
paper examines Flusserian Freedom in the context of Mixed Reality (MR) and
explores how humans can recognize the obscuration of the world within the media
(i.e., MR) and understand their relationship. The authors investigate the
concept of playing against apparatus and deliberately alienating the perception
of the projected world through an artwork titled ""Surrealism Me."" This artwork
enables the user to have another body within MR through interactive and
immersive experiences based on the definition of Sense of Embodiment. The
purpose of this work is to raise awareness of the domination of media and to
approach Flusserian freedom within contemporary technical arrangements.","['Aven Le Zhou', 'Lei Xi', 'Kang Zhang']",2024-02-16T15:19:35Z,http://arxiv.org/abs/2402.10751v1,['cs.CY']
"Data Cubes in Hand: A Design Space of Tangible Cubes for Visualizing 3D
  Spatio-Temporal Data in Mixed Reality","Tangible interfaces in mixed reality (MR) environments allow for intuitive
data interactions. Tangible cubes, with their rich interaction affordances,
high maneuverability, and stable structure, are particularly well-suited for
exploring multi-dimensional data types. However, the design potential of these
cubes is underexplored. This study introduces a design space for tangible cubes
in MR, focusing on interaction space, visualization space, sizes, and
multiplicity. Using spatio-temporal data, we explored the interaction
affordances of these cubes in a workshop (N=24). We identified unique
interactions like rotating, tapping, and stacking, which are linked to
augmented reality (AR) visualization commands. Integrating user-identified
interactions, we created a design space for tangible-cube interactions and
visualization. A prototype visualizing global health spending with small cubes
was developed and evaluated, supporting both individual and combined cube
manipulation. This research enhances our grasp of tangible interaction in MR,
offering insights for future design and application in diverse data contexts.","['Shuqi He', 'Haonan Yao', 'Luyan Jiang', 'Kaiwen Li', 'Nan Xiang', 'Yue Li', 'Hai-Ning Liang', 'Lingyun Yu']",2024-03-11T16:47:39Z,http://arxiv.org/abs/2403.06891v1,['cs.HC']
Gaze-based Human-Robot Interaction System for Infrastructure Inspections,"Routine inspections for critical infrastructures such as bridges are required
in most jurisdictions worldwide. Such routine inspections are largely visual in
nature, which are qualitative, subjective, and not repeatable. Although robotic
infrastructure inspections address such limitations, they cannot replace the
superior ability of experts to make decisions in complex situations, thus
making human-robot interaction systems a promising technology. This study
presents a novel gaze-based human-robot interaction system, designed to augment
the visual inspection performance through mixed reality. Through holograms from
a mixed reality device, gaze can be utilized effectively to estimate the
properties of the defect in real-time. Additionally, inspectors can monitor the
inspection progress online, which enhances the speed of the entire inspection
process. Limited controlled experiments demonstrate its effectiveness across
various users and defect types. To our knowledge, this is the first
demonstration of the real-time application of eye gaze in civil infrastructure
inspections.","['Sunwoong Choi', 'Zaid Abbas Al-Sabbag', 'Sriram Narasimhan', 'Chul Min Yeum']",2024-03-12T20:26:51Z,http://arxiv.org/abs/2403.08061v1,['cs.RO']
"On the Fly Robotic-Assisted Medical Instrument Planning and Execution
  Using Mixed Reality","Robotic-assisted medical systems (RAMS) have gained significant attention for
their advantages in alleviating surgeons' fatigue and improving patients'
outcomes. These systems comprise a range of human-computer interactions,
including medical scene monitoring, anatomical target planning, and robot
manipulation. However, despite its versatility and effectiveness, RAMS demands
expertise in robotics, leading to a high learning cost for the operator. In
this work, we introduce a novel framework using mixed reality technologies to
ease the use of RAMS. The proposed framework achieves real-time planning and
execution of medical instruments by providing 3D anatomical image overlay,
human-robot collision detection, and robot programming interface. These
features, integrated with an easy-to-use calibration method for head-mounted
display, improve the effectiveness of human-robot interactions. To assess the
feasibility of the framework, two medical applications are presented in this
work: 1) coil placement during transcranial magnetic stimulation and 2) drill
and injector device positioning during femoroplasty. Results from these use
cases demonstrate its potential to extend to a wider range of medical
scenarios.","['Letian Ai', 'Yihao Liu', 'Mehran Armand', 'Amir Kheradmand', 'Alejandro Martin-Gomez']",2024-04-08T21:58:25Z,http://arxiv.org/abs/2404.05887v1,['cs.RO']
"Quantifying Social Presence in Mixed Reality: A Contemporary Review of
  Techniques and Innovations","This literature review investigates the transformative potential of mixed
reality (MR) technology, where we explore the intersection of contemporary
technological advancements, modern deep learning recommendation systems, and
social psychology frameworks. This interdisciplinary study informs the
understanding of MR's role in improving social presence, catalyzing novel
social interactions, and enhancing the quality of interpersonal communication
in the real world. We also discuss the challenges and barriers blocking the
wide-spread adoption of social networking in MR, such as device constraints,
privacy and accessibility concerns, and social norms. Through carefully
structured, closed-environment experiments with diverse participants of varying
levels of digital literacy, we measure the differences in social dynamics,
frequency, quality, and duration of interactions, and levels of social anxiety
between MR-enhanced, mobile-enhanced, and control condition participants.",['Sparsh Srivastava'],2024-04-05T16:16:12Z,http://arxiv.org/abs/2404.15325v2,['cs.HC']
"Practice-informed Patterns for Organising Large Groups in Distributed
  Mixed Reality Collaboration","Collaborating across dissimilar, distributed spaces presents numerous
challenges for computer-aided spatial communication. Mixed reality (MR) can
blend selected surfaces, allowing collaborators to work in blended f-formations
(facing formations), even when their workstations are physically misaligned.
Since collaboration often involves more than just participant pairs, this
research examines how we might scale MR experiences for large-group
collaboration. To do so, this study recruited collaboration designers (CDs) to
evaluate and reimagine MR for large-scale collaboration. These CDs were engaged
in a four-part user study that involved a technology probe, a semi-structured
interview, a speculative low-fidelity prototyping activity and a validation
session. The outcomes of this paper contribute (1) a set of collaboration
design principles to inspire future computer-supported collaborative work, (2)
eight collaboration patterns for blended f-formations and collaboration at
scale and (3) theoretical implications for f-formations and space-place
relationships. As a result, this work creates a blueprint for scaling
collaboration across distributed spaces.","['Emily Wong', 'Juan Sánchez Esquivel', 'Jens Emil Grønbæk', 'Germán Leiva', 'Eduardo Velloso']",2024-05-08T08:06:22Z,http://arxiv.org/abs/2405.04873v2,['cs.HC']
"Harms in Repurposing Real-World Sensory Cues for Mixed Reality: A Causal
  Perspective","The rise of Mixed Reality (MR) stimulates new interactive techniques that
seamlessly blend the virtual and physical environments. Just as virtual content
could be overlayed onto the physical world for providing adaptive user
interfaces [5, 8], emergent techniques ""repurpose"" everyday environments and
sensory cues to support the virtual content [7, 9, 13-15]. For instance, a
strong wind gust in the real world, rather than being distracting to the
virtual experience, can be mapped with trees swaying in MR to achieve a
unifying experience [15], as shown in Figure 1. Such techniques introduce
stronger immersion, but they also expose users to overlooked perceptual
manipulations, where safety risks arise from misperception of real-world
events. In this work, we apply a causal inference perspective to understand the
harms of repurposing real-world sensory cues for MR. We argue that by viewing
the MR experience as a causal inference process of interpreting cues arising
from both the virtual and physical world, MR designers and researchers can gain
a new lens to understand potential perceptual manipulation harms.","['Yujie Tao', 'Sean Follmer']",2024-04-23T17:34:46Z,http://arxiv.org/abs/2405.05931v1,['cs.HC']
"RealitySummary: On-Demand Mixed Reality Document Enhancement using Large
  Language Models","We introduce RealitySummary, a mixed reality reading assistant that can
enhance any printed or digital document using on-demand text extraction,
summarization, and augmentation. While augmented reading tools promise to
enhance physical reading experiences with overlaid digital content, prior
systems have typically required pre-processed documents, which limits their
generalizability and real-world use cases. In this paper, we explore on-demand
document augmentation by leveraging large language models. To understand
generalizable techniques for diverse documents, we first conducted an
exploratory design study which identified five categories of document
enhancements (summarization, augmentation, navigation, comparison, and
extraction). Based on this, we developed a proof-of-concept system that can
automatically extract and summarize text using Google Cloud OCR and GPT-4, then
embed information around documents using a Microsoft Hololens 2 and Apple
Vision Pro. We demonstrate real-time examples of six specific document
augmentations: 1) summaries, 2) comparison tables, 3) timelines, 4) keyword
lists, 5) summary highlighting, and 6) information cards. Results from a
usability study (N=12) and in-the-wild study (N=11) highlight the potential
benefits of on-demand MR document enhancement and opportunities for future
research.","['Aditya Gunturu', 'Shivesh Jadon', 'Nandi Zhang', 'Jarin Thundathil', 'Wesley Willett', 'Ryo Suzuki']",2024-05-28T21:59:56Z,http://arxiv.org/abs/2405.18620v1,"['cs.HC', 'cs.AI', 'cs.CL']"
Live-action Virtual Reality Games,"This paper proposes the concept of ""live-action virtual reality games"" as a
new genre of digital games based on an innovative combination of live-action,
mixed-reality, context-awareness, and interaction paradigms that comprise
tangible objects, context-aware input devices, and embedded/embodied
interactions. Live-action virtual reality games are ""live-action games"" because
a player physically acts out (using his/her real body and senses) his/her
""avatar"" (his/her virtual representation) in the game stage, which is the
mixed-reality environment where the game happens. The game stage is a kind of
""augmented virtuality""; a mixed-reality where the virtual world is augmented
with real-world information. In live-action virtual reality games, players wear
HMD devices and see a virtual world that is constructed using the physical
world architecture as the basic geometry and context information. Physical
objects that reside in the physical world are also mapped to virtual elements.
Live-action virtual reality games keeps the virtual and real-worlds
superimposed, requiring players to physically move in the environment and to
use different interaction paradigms (such as tangible and embodied interaction)
to complete game activities. This setup enables the players to touch physical
architectural elements (such as walls) and other objects, ""feeling"" the game
stage. Players have free movement and may interact with physical objects placed
in the game stage, implicitly and explicitly. Live-action virtual reality games
differ from similar game concepts because they sense and use contextual
information to create unpredictable game experiences, giving rise to emergent
gameplay.","['Luis Valente', 'Esteban Clua', 'Alexandre Ribeiro Silva', 'Bruno Feijó']",2016-01-07T19:30:37Z,http://arxiv.org/abs/1601.01645v1,['cs.HC']
Inverse Augmented Reality: A Virtual Agent's Perspective,"We propose a framework called inverse augmented reality (IAR) which describes
the scenario that a virtual agent living in the virtual world can observe both
virtual objects and real objects. This is different from the traditional
augmented reality. The traditional virtual reality, mixed reality and augmented
reality are all generated for humans, i.e., they are human-centered frameworks.
On the contrary, the proposed inverse augmented reality is a virtual
agent-centered framework, which represents and analyzes the reality from a
virtual agent's perspective. In this paper, we elaborate the framework of
inverse augmented reality to argue the equivalence of the virtual world and the
physical world regarding the whole physical structure.","['Zhenliang Zhang', 'Dongdong Weng', 'Haiyan Jiang', 'Yue Liu', 'Yongtian Wang']",2018-08-10T05:23:37Z,http://arxiv.org/abs/1808.03413v1,['cs.HC']
"Where's My Drink? Enabling Peripheral Real World Interactions While
  Using HMDs","Head Mounted Displays (HMDs) allow users to experience virtual reality with a
great level of immersion. However, even simple physical tasks like drinking a
beverage can be difficult and awkward while in a virtual reality experience. We
explore mixed reality renderings that selectively incorporate the physical
world into the virtual world for interactions with physical objects. We
conducted a user study comparing four rendering techniques that balances
immersion in a virtual world with ease of interaction with the physical world.
Finally, we discuss the pros and cons of each approach, suggesting guidelines
for future rendering techniques that bring physical objects into virtual
reality.","['Pulkit Budhiraja', 'Rajinder Sodhi', 'Brett Jones', 'Kevin Karsch', 'Brian Bailey', 'David Forsyth']",2015-02-16T22:50:03Z,http://arxiv.org/abs/1502.04744v1,['cs.HC']
Notes on Pervasive Virtuality,"This paper summarizes current notes about a new mixed-reality paradigm that
we named as ""pervasive virtuality"". This paradigm has emerged recently in
industry and academia through different initiatives. In this paper we intend to
explore this new area by proposing a set of features that we identified as
important or helpful to realize pervasive virtuality in games and entertainment
applications.","['Luis Valente', 'Bruno Feijo', 'Alexandre Ribeiro Silva', 'Esteban Clua']",2016-05-25T12:01:46Z,http://arxiv.org/abs/1605.08035v1,['cs.HC']
"Augmented Reality with Hololens: Experiential Architectures Embedded in
  the Real World","Early hands-on experiences with the Microsoft Hololens augmented/mixed
reality device are reported and discussed, with a general aim of exploring
basic 3D visualization. A range of usage cases are tested, including data
visualization and immersive data spaces, in-situ visualization of 3D models and
full scale architectural form visualization. Ultimately, the Hololens is found
to provide a remarkable tool for moving from traditional visualization of 3D
objects on a 2D screen, to fully experiential 3D visualizations embedded in the
real world.","['Paul Hockett', 'Tim Ingleby']",2016-10-13T22:32:08Z,http://arxiv.org/abs/1610.04281v1,"['cs.GR', 'physics.data-an']"
Authoring and Living Next-Generation Location-Based Experiences,"Authoring location-based experiences involving multiple participants,
collaborating or competing in both indoor and outdoor mixed realities, is
extremely complex and bound to serious technical challenges. In this work, we
present the first results of the MAGELLAN European project and how these
greatly simplify this creative process using novel authoring, augmented reality
(AR) and indoor geolocalisation techniques.","['Olivier Balet', 'Boriana Koleva', 'Jens Grubert', 'Kwang Moo Yi', 'Marco Gunia', 'Angelos Katsis', 'Julien Castet']",2017-09-05T09:04:05Z,http://arxiv.org/abs/1709.01293v1,['cs.HC']
Real-time Egocentric Gesture Recognition on Mobile Head Mounted Displays,"Mobile virtual reality (VR) head mounted displays (HMD) have become popular
among consumers in recent years. In this work, we demonstrate real-time
egocentric hand gesture detection and localization on mobile HMDs. Our main
contributions are: 1) A novel mixed-reality data collection tool to automatic
annotate bounding boxes and gesture labels; 2) The largest-to-date egocentric
hand gesture and bounding box dataset with more than 400,000 annotated frames;
3) A neural network that runs real time on modern mobile CPUs, and achieves
higher than 76% precision on gesture recognition across 8 classes.","['Rohit Pandey', 'Marie White', 'Pavel Pidlypenskyi', 'Xue Wang', 'Christine Kaeser-Chen']",2017-12-13T19:06:37Z,http://arxiv.org/abs/1712.04961v1,['cs.CV']
"Deep Neural Network and Data Augmentation Methodology for off-axis iris
  segmentation in wearable headsets","A data augmentation methodology is presented and applied to generate a large
dataset of off-axis iris regions and train a low-complexity deep neural
network. Although of low complexity the resulting network achieves a high level
of accuracy in iris region segmentation for challenging off-axis eye-patches.
Interestingly, this network is also shown to achieve high levels of performance
for regular, frontal, segmentation of iris regions, comparing favorably with
state-of-the-art techniques of significantly higher complexity. Due to its
lower complexity, this network is well suited for deployment in embedded
applications such as augmented and mixed reality headsets.","['Viktor Varkarakis', 'Shabab Bazrafkan', 'Peter Corcoran']",2019-03-01T16:17:00Z,http://arxiv.org/abs/1903.00389v1,['cs.CV']
Making ethical decisions for the immersive web,"Mixed reality (MR) ethics occupies a space that intersects with web ethics,
emerging tech ethics, healthcare ethics and product ethics (among others). This
paper focuses on how we can build an immersive web that encourages ethical
development and usage. The technology is beyond emerging (footnote: generally,
the ethics of emerging technologies are focused on ethical assessments of
research and innovation), but not quite entrenched. We're still in a position
to intervene in the development process, instead of attempting to retrofit
ethical decisions into an established design. While we have a wider range of
data to analyze than most emerging technologies, we're still in a much more
speculative state than entrenched technologies. This space is a challenge and
an opportunity.",['Diane Hosfelt'],2019-05-14T14:57:20Z,http://arxiv.org/abs/1905.06995v1,['cs.HC']
"Real Time Egocentric Object Segmentation: THU-READ Labeling and
  Benchmarking Results","Egocentric segmentation has attracted recent interest in the computer vision
community due to their potential in Mixed Reality (MR) applications. While most
previous works have been focused on segmenting egocentric human body parts
(mainly hands), little attention has been given to egocentric objects. Due to
the lack of datasets of pixel-wise annotations of egocentric objects, in this
paper we contribute with a semantic-wise labeling of a subset of 2124 images
from the RGB-D THU-READ Dataset. We also report benchmarking results using
Thundernet, a real-time semantic segmentation network, that could allow future
integration with end-to-end MR applications.","['E. Gonzalez-Sosa', 'G. Robledo', 'D. Gonzalez-Morin', 'P. Perez-Garcia', 'A. Villegas']",2021-06-09T10:10:02Z,http://arxiv.org/abs/2106.04957v1,['cs.CV']
"Comparing Controller With the Hand Gestures Pinch and Grab for Picking
  Up and Placing Virtual Objects","Grabbing virtual objects is one of the essential tasks for Augmented,
Virtual, and Mixed Reality applications. Modern applications usually use a
simple pinch gesture for grabbing and moving objects. However, picking up
objects by pinching has disadvantages. It can be an unnatural gesture to pick
up objects and prevents the implementation of other gestures which would be
performed with thumb and index. Therefore it is not the optimal choice for many
applications. In this work, different implementations for grabbing and placing
virtual objects are proposed and compared. Performance and accuracy of the
proposed techniques are measured and compared.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2022-02-22T15:12:06Z,http://arxiv.org/abs/2202.10964v1,"['cs.HC', 'cs.CV']"
"Cross-Reality for Extending the Metaverse: Designing Hyper-Connected
  Immersive Environments with XRI","The Metaverse comprises technologies to enable virtual twins of the real
world, via mixed reality, internet of things, and others. As it matures unique
challenges arise such as a lack of strong connections between virtual and
physical worlds. This work presents design frameworks for cross-reality hybrid
spaces. Contributions include: i) clarifying the metaverse ""disconnect"", ii)
extended metaverse design frameworks, iii) prototypes, and iv) discussions
toward new metaverse smart environments.","['Jie Guan', 'Alexis Morris', 'Jay Irizawa']",2023-06-01T19:55:34Z,http://arxiv.org/abs/2306.01113v1,['cs.HC']
"Beyond the Screen: Reshaping the Workplace with Virtual and Augmented
  Reality","Although extended reality technologies have enjoyed an explosion in
popularity in recent years, few applications are effectively used outside the
entertainment or academic contexts. This work consists of a literature review
regarding the effective integration of such technologies in the workplace. It
aims to provide an updated view of how they are being used in that context.
First, we examine existing research concerning virtual, augmented, and
mixed-reality applications. We also analyze which have made their way to the
workflows of companies and institutions. Furthermore, we circumscribe the
aspects of extended reality technologies that determined this applicability.","['Nuno Verdelho Trindade', 'Alfredo Ferreira', 'João Madeiras Pereira']",2023-12-01T08:05:22Z,http://arxiv.org/abs/2312.00408v1,['cs.HC']
"OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using
  Semantic Understanding in Mixed Reality","One key challenge in augmented reality is the placement of virtual content in
natural locations. Existing automated techniques are only able to work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce a new
open-vocabulary method for object placement. Our eight-stage pipeline leverages
recent advances in segmentation models, vision-language models, and LLMs to
place any virtual object in any AR camera frame or scene. In a preliminary user
study, we show that our method performs at least as well as human experts 57%
of the time.","['Luke Yoffe', 'Aditya Sharma', 'Tobias Höllerer']",2023-12-20T07:34:20Z,http://arxiv.org/abs/2312.12815v1,"['cs.CV', 'cs.AI', 'cs.CL']"
"Virtual World, Defined from a Technological Perspective, and Applied to
  Video Games, Mixed Reality and the Metaverse","There is no generally accepted definition for a virtual world, with many
complimentary terms and acronyms having emerged implying a virtual world.
Advances in systems architecture techniques such as, host migration of
instances, mobile ad-hoc networking, and distributed computing, bring in to
question whether those architectures can actually support a virtual world.
Without a concrete definition, controversy ensues and it is problematic to
design an architecture for a virtual world. Several researchers provided a
definition but aspects of each definition are still problematic and simply can
not be applied to contemporary technologies. The approach of this article is to
sample technologies using grounded theory, and obtain a definition for a
`virtual world' that is directly applicable to technology. The obtained
definition is compared with related work and used to classify advanced
technologies, such as: a pseudo-persistent video game, a MANet, virtual and
mixed reality, and the Metaverse. The results of this article include: a break
down of which properties set apart the various technologies; a definition that
is validated by comparing it with other definitions; an ontology showing the
relation of the different complimentary terms and acronyms; and, the usage of
pseudo-persistence to categories those technologies which only mimic
persistence.",['Kim J. L. Nevelsteen'],2015-11-26T18:05:02Z,http://arxiv.org/abs/1511.08464v2,"['cs.HC', 'cs.CY']"
"Augmenting the thermal flux experiment: a mixed reality approach with
  the HoloLens","In the field of Virtual Reality (VR) and Augmented Reality (AR) technologies
have made huge progress during the last years and also reached the field of
education. The virtuality continuum, ranging from pure virtuality on one side
to the real world on the other has been successfully covered by the use of
immersive technologies like head-mounted displays, which allow to embed virtual
objects into the real surroundings, leading to a Mixed Reality (MR) experience.
In such an environment digital and real objects do not only co-exist, but
moreover are also able to interact with each other in real-time. These concepts
can be used to merge human perception of reality with digitally visualized
sensor data and thereby making the invisible visible. As a first example, in
this paper we introduce alongside the basic idea of this column an
MR-experiment in thermodynamics for a laboratory course for freshman students
in physics or other science and engineering subjects which uses physical data
from mobile devices for analyzing and displaying physical phenomena to
students.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-09-05T11:52:18Z,http://arxiv.org/abs/1709.01342v1,['physics.ed-ph']
Security and Privacy Approaches in Mixed Reality: A Literature Survey,"Mixed reality (MR) technology development is now gaining momentum due to
advances in computer vision, sensor fusion, and realistic display technologies.
With most of the research and development focused on delivering the promise of
MR, there is only barely a few working on the privacy and security implications
of this technology. This survey paper aims to put in to light these risks, and
to look into the latest security and privacy work on MR. Specifically, we list
and review the different protection approaches that have been proposed to
ensure user and data security and privacy in MR. We extend the scope to include
work on related technologies such as augmented reality (AR), virtual reality
(VR), and human-computer interaction (HCI) as crucial components, if not the
origins, of MR, as well as numerous related work from the larger area of mobile
devices, wearables, and Internet-of-Things (IoT). We highlight the lack of
investigation, implementation, and evaluation of data protection approaches in
MR. Further challenges and directions on MR security and privacy are also
discussed.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-02-15T23:33:45Z,http://arxiv.org/abs/1802.05797v3,"['cs.CR', 'cs.CY', 'cs.HC']"
"Conservative Plane Releasing for Spatial Privacy Protection in Mixed
  Reality","Augmented reality (AR) or mixed reality (MR) platforms require spatial
understanding to detect objects or surfaces, often including their structural
(i.e. spatial geometry) and photometric (e.g. color, and texture) attributes,
to allow applications to place virtual or synthetic objects seemingly
""anchored"" on to real world objects; in some cases, even allowing interactions
between the physical and virtual objects. These functionalities require AR/MR
platforms to capture the 3D spatial information with high resolution and
frequency; however, these pose unprecedented risks to user privacy. Aside from
objects being detected, spatial information also reveals the location of the
user with high specificity, e.g. in which part of the house the user is. In
this work, we propose to leverage spatial generalizations coupled with
conservative releasing to provide spatial privacy while maintaining data
utility. We designed an adversary that builds up on existing place and shape
recognition methods over 3D data as attackers to which the proposed spatial
privacy approach can be evaluated against. Then, we simulate user movement
within spaces which reveals more of their space as they move around utilizing
3D point clouds collected from Microsoft HoloLens. Results show that revealing
no more than 11 generalized planes--accumulated from successively revealed
spaces with large enough radius, i.e. $r\leq1.0m$--can make an adversary fail
in identifying the spatial location of the user for at least half of the time.
Furthermore, if the accumulated spaces are of smaller radius, i.e. each
successively revealed space is $r\leq 0.5m$, we can release up to 29
generalized planes while enjoying both better data utility and privacy.","['Jaybie A. de Guzman', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2020-04-17T01:57:58Z,http://arxiv.org/abs/2004.08029v1,"['cs.CV', 'cs.CR', 'cs.HC']"
Kalman Filter-based Head Motion Prediction for Cloud-based Mixed Reality,"Volumetric video allows viewers to experience highly-realistic 3D content
with six degrees of freedom in mixed reality (MR) environments. Rendering
complex volumetric videos can require a prohibitively high amount of
computational power for mobile devices. A promising technique to reduce the
computational burden on mobile devices is to perform the rendering at a cloud
server. However, cloud-based rendering systems suffer from an increased
interaction (motion-to-photon) latency that may cause registration errors in MR
environments. One way of reducing the effective latency is to predict the
viewer's head pose and render the corresponding view from the volumetric video
in advance. In this paper, we design a Kalman filter for head motion prediction
in our cloud-based volumetric video streaming system. We analyze the
performance of our approach using recorded head motion traces and compare its
performance to an autoregression model for different prediction intervals
(look-ahead times). Our results show that the Kalman filter can predict head
orientations 0.5 degrees more accurately than the autoregression model for a
look-ahead time of 60 ms.","['Serhan Gül', 'Sebastian Bosse', 'Dimitri Podborski', 'Thomas Schierl', 'Cornelius Hellge']",2020-07-28T09:41:22Z,http://arxiv.org/abs/2007.14084v1,"['cs.MM', 'eess.IV', 'eess.SP']"
"Jointly Optimizing Sensing Pipelines for Multimodal Mixed Reality
  Interaction","Natural human interactions for Mixed Reality Applications are overwhelmingly
multimodal: humans communicate intent and instructions via a combination of
visual, aural and gestural cues. However, supporting low-latency and accurate
comprehension of such multimodal instructions (MMI), on resource-constrained
wearable devices, remains an open challenge, especially as the state-of-the-art
comprehension techniques for each individual modality increasingly utilize
complex Deep Neural Network models. We demonstrate the possibility of
overcoming the core limitation of latency--vs.--accuracy tradeoff by exploiting
cross-modal dependencies -- i.e., by compensating for the inferior performance
of one model with an increased accuracy of more complex model of a different
modality. We present a sensor fusion architecture that performs MMI
comprehension in a quasi-synchronous fashion, by fusing visual, speech and
gestural input. The architecture is reconfigurable and supports dynamic
modification of the complexity of the data processing pipeline for each
individual modality in response to contextual changes. Using a representative
""classroom"" context and a set of four common interaction primitives, we then
demonstrate how the choices between low and high complexity models for each
individual modality are coupled. In particular, we show that (a) a judicious
combination of low and high complexity models across modalities can offer a
dramatic 3-fold decrease in comprehension latency together with an increase
10-15% in accuracy, and (b) the right collective choice of models is context
dependent, with the performance of some model combinations being significantly
more sensitive to changes in scene context or choice of interaction.","['Darshana Rathnayake', 'Ashen de Silva', 'Dasun Puwakdandawa', 'Lakmal Meegahapola', 'Archan Misra', 'Indika Perera']",2020-10-13T10:13:24Z,http://arxiv.org/abs/2010.06584v2,['cs.HC']
"A Tool for Organizing Key Characteristics of Virtual, Augmented, and
  Mixed Reality for Human-Robot Interaction Systems: Synthesizing VAM-HRI
  Trends and Takeaways","Frameworks have begun to emerge to categorize Virtual, Augmented, and Mixed
Reality (VAM) technologies that provide immersive, intuitive interfaces to
facilitate Human-Robot Interaction. These frameworks, however, fail to capture
key characteristics of the growing subfield of VAM-HRI and can be difficult to
consistently apply due to continuous scales. This work builds upon these prior
frameworks through the creation of a Tool for Organizing Key Characteristics of
VAM-HRI Systems (TOKCS). TOKCS discretizes the continuous scales used within
prior works for more consistent classification and adds additional
characteristics related to a robot's internal model, anchor locations,
manipulability, and the system's software and hardware. To showcase the tool's
capability, TOKCS is applied to the ten papers from the fourth VAM-HRI workshop
and examined for key trends and takeaways. These trends highlight the
expressive capability of TOKCS while also helping frame newer trends and future
work recommendations for VAM-HRI research.","['Thomas R. Groechel', 'Michael E. Walker', 'Christine T. Chang', 'Eric Rosen', 'Jessica Zosa Forde']",2021-08-07T16:01:42Z,http://arxiv.org/abs/2108.03477v3,"['cs.RO', 'cs.HC']"
"Mixed Reality using Illumination-aware Gradient Mixing in Surgical
  Telepresence: Enhanced Multi-layer Visualization","Background and aim: Surgical telepresence using augmented perception has been
applied, but mixed reality is still being researched and is only theoretical.
The aim of this work is to propose a solution to improve the visualization in
the final merged video by producing globally consistent videos when the
intensity of illumination in the input source and target video varies.
Methodology: The proposed system uses an enhanced multi-layer visualization
with illumination-aware gradient mixing using Illumination Aware Video
Composition algorithm. Particle Swarm Optimization Algorithm is used to find
the best sample pair from foreground and background region and image pixel
correlation to estimate the alpha matte. Particle Swarm Optimization algorithm
helps to get the original colour and depth of the unknown pixel in the unknown
region. Result: Our results showed improved accuracy caused by reducing the
Mean squared Error for selecting the best sample pair for unknown region in 10
each sample for bowel, jaw and breast. The amount of this reduction is 16.48%
from the state of art system. As a result, the visibility accuracy is improved
from 89.4 to 97.7% which helped to clear the hand vision even in the difference
of light. Conclusion: Illumination effect and alpha pixel correlation improves
the visualization accuracy and produces a globally consistent composition
results and maintains the temporal coherency when compositing two videos with
high and inverse illumination effect. In addition, this paper provides a
solution for selecting the best sampling pair for the unknown region to obtain
the original colour and depth.","['Nirakar Puri', 'Abeer Alsadoon', 'P. W. C. Prasad', 'Nada Alsalami', 'Tarik A. Rashid']",2021-08-21T11:59:24Z,http://arxiv.org/abs/2110.09318v1,"['cs.MM', 'cs.AI', 'cs.CV']"
"Mixed reality hologram slicer (mxdR-HS): a marker-less tangible user
  interface for interactive holographic volume visualization","Mixed reality head-mounted displays (mxdR-HMD) have the potential to
visualize volumetric medical imaging data in holograms to provide a true sense
of volumetric depth. An effective user interface, however, has yet to be
thoroughly studied. Tangible user interfaces (TUIs) enable a tactile
interaction with a hologram through an object. The object has physical
properties indicating how it might be used with multiple degrees-of-freedom. We
propose a TUI using a planar object (PO) for the holographic medical volume
visualization and exploration. We refer to it as mxdR hologram slicer
(mxdR-HS). Users can slice the hologram to examine particular regions of
interest (ROIs) and intermix complementary data and annotations. The mxdR-HS
introduces a novel real-time ad-hoc marker-less PO tracking method that works
with any PO where corners are visible. The aim of mxdR-HS is to maintain
minimum computational latency while preserving practical tracking accuracy to
enable seamless TUI integration in the commercial mxdR-HMD, which has limited
computational resources. We implemented the mxdR-HS on a commercial Microsoft
HoloLens with a built-in depth camera. Our experimental results showed our
mxdR-HS had a superior computational latency but marginally lower tracking
accuracy than two marker-based tracking methods and resulted in enhanced
computational latency and tracking accuracy than 10 marker-less tracking
methods. Our mxdR-HS, in a medical environment, can be suggested as a visual
guide to display complex volumetric medical imaging data.","['Hoijoon Jung', 'Younhyun Jung', 'Michael Fulham', 'Jinman Kim']",2022-01-26T01:59:53Z,http://arxiv.org/abs/2201.10704v1,['cs.HC']
"Mixed Reality Depth Contour Occlusion Using Binocular Similarity
  Matching and Three-dimensional Contour Optimisation","Mixed reality applications often require virtual objects that are partly
occluded by real objects. However, previous research and commercial products
have limitations in terms of performance and efficiency. To address these
challenges, we propose a novel depth contour occlusion (DCO) algorithm. The
proposed method is based on the sensitivity of contour occlusion and a
binocular stereoscopic vision device. In this method, a depth contour map is
combined with a sparse depth map obtained from a two-stage adaptive filter area
stereo matching algorithm and the depth contour information of the objects
extracted by a digital image stabilisation optical flow method. We also propose
a quadratic optimisation model with three constraints to generate an accurate
dense map of the depth contour for high-quality real-virtual occlusion. The
whole process is accelerated by GPU. To evaluate the effectiveness of the
algorithm, we demonstrate a time con-sumption statistical analysis for each
stage of the DCO algorithm execution. To verify the relia-bility of the
real-virtual occlusion effect, we conduct an experimental analysis on
single-sided, enclosed, and complex occlusions; subsequently, we compare it
with the occlusion method without quadratic optimisation. With our GPU
implementation for real-time DCO, the evaluation indicates that applying the
presented DCO algorithm can enhance the real-time performance and the visual
quality of real-virtual occlusion.","['Naye Ji', 'Fan Zhang', 'Haoxiang Zhang', 'Youbing Zhao', 'Dingguo Yu']",2022-03-04T13:16:40Z,http://arxiv.org/abs/2203.02300v1,"['cs.CV', 'cs.LG']"
"Augmented Reality Appendages for Robots: Design Considerations and
  Recommendations for Maximizing Social and Functional Perception","In order to address the limitations of gestural capabilities in physical
robots, researchers in Virtual, Augmented, Mixed Reality Human-Robot
Interaction (VAM-HRI) have been using augmented-reality visualizations that
increase robot expressivity and improve user perception (e.g., social
presence). While a multitude of virtual robot deictic gestures (e.g., pointing
to an object) have been implemented to improve interactions within VAM-HRI,
such systems are often reported to have tradeoffs between functional and social
user perceptions of robots, creating a need for a unified approach that
considers both attributes. We performed a literature analysis that selected
factors that were noted to significantly influence either user perception or
task efficiency and propose a set of design considerations and recommendations
that address those factors by combining anthropomorphic and non-anthropomorphic
virtual gestures based on the motivation of the interaction, visibility of the
target and robot, salience of the target, and distance between the target and
robot. The proposed recommendations provide the VAM-HRI community with starting
points for selecting appropriate gesture types for a multitude of interaction
contexts.","['Ipek Goktan', 'Karen Ly', 'Thomas R. Groechel', 'Maja J. Mataric']",2022-05-13T16:30:53Z,http://arxiv.org/abs/2205.06747v1,['cs.RO']
"Comparison of synthetic dataset generation methods for medical
  intervention rooms using medical clothing detection as an example","The availability of real data from areas with high privacy requirements, such
as the medical intervention space, is low and the acquisition legally complex.
Therefore, this work presents a way to create a synthetic dataset for the
medical context, using medical clothing as an example. The goal is to close the
reality gap between the synthetic and real data. For this purpose, methods of
3D-scanned clothing and designed clothing are compared in a
Domain-Randomization and Structured-Domain-Randomization scenario using an
Unreal-Engine plugin or Unity. Additionally a Mixed-Reality dataset in front of
a greenscreen and a target domain dataset were used. Our experiments show, that
Structured-Domain-Randomization of designed clothing together with
Mixed-Reality data provide a baseline achieving 72.0% mAP on a test dataset of
the clinical target domain. When additionally using 15% of available target
domain train data, the gap towards 100% (660 images) target domain train data
could be nearly closed 80.05% mAP (81.95% mAP). Finally we show that when
additionally using 100% target domain train data the accuracy could be
increased to 83.35% mAP.","['Patrick Schülein', 'Hannah Teufel', 'Ronja Vorpahl', 'Indira Emter', 'Yannick Bukschat', 'Marcus Pfister', 'Anke Siebert', 'Nils Rathmann', 'Steffen Diehl', 'Marcus Vetter']",2022-09-23T09:36:23Z,http://arxiv.org/abs/2209.11493v1,"['cs.CV', 'I.5; I.2']"
Automated Reconstruction of 3D Open Surfaces from Sparse Point Clouds,"Real-world 3D data may contain intricate details defined by salient surface
gaps. Automated reconstruction of these open surfaces (e.g., non-watertight
meshes) is a challenging problem for environment synthesis in mixed reality
applications. Current learning-based implicit techniques can achieve high
fidelity on closed-surface reconstruction. However, their dependence on the
distinction between the inside and outside of a surface makes them incapable of
reconstructing open surfaces. Recently, a new class of implicit functions have
shown promise in reconstructing open surfaces by regressing an unsigned
distance field. Yet, these methods rely on a discretized representation of the
raw data, which loses important surface details and can lead to outliers in the
reconstruction. We propose IPVNet, a learning-based implicit model that
predicts the unsigned distance between a surface and a query point in 3D space
by leveraging both raw point cloud data and its discretized voxel counterpart.
Experiments on synthetic and real-world public datasets demonstrates that
IPVNet outperforms the state of the art while producing far fewer outliers in
the reconstruction.","['Mohammad Samiul Arshad', 'William J. Beksi']",2022-10-26T22:02:45Z,http://arxiv.org/abs/2210.15059v2,"['cs.CV', 'cs.GR']"
"Complementary Textures. A Novel Approach to Object Alignment in Mixed
  Reality","Alignment between real and virtual objects is a challenging task required for
the deployment of Mixed Reality (MR) into manufacturing, medical, and
construction applications. To face this challenge, a series of methods have
been proposed. While many approaches use dynamic augmentations such as
animations, arrows, or text to assist users, they require tracking the position
of real objects. In contrast, when tracking of the real objects is not
available or desired, alternative approaches use virtual replicas of real
objects to allow for interactive, perceptual virtual-to-real, and/or
real-to-virtual alignment. In these cases, the accuracy achieved strongly
depends on the quality of the perceptual information provided to the user. This
paper proposes a novel set of perceptual alignment concepts that go beyond the
use of traditional visualization of virtual replicas, introducing the concept
of COMPLEMENTARY TEXTURES to improve interactive alignment in MR applications.
To showcase the advantages of using COMPLEMENTARY TEXTURES, we describe three
different implementations that provide highly salient visual cues when
misalignment is observed; or present semantic augmentations that, when combined
with a real object, provide contextual information that can be used during the
alignment process. The authors aim to open new paths for the community to
explore rather than describing end-to-end solutions. The objective is to show
the multitude of opportunities such concepts could provide for further research
and development.","['Alejandro Martin-Gomez', 'Alexander Winkler', 'Rafael de la Tijera Obert', 'Javad Fotouhi', 'Daniel Roth', 'Ulrich Eck', 'Nassir Navab']",2022-11-16T16:50:21Z,http://arxiv.org/abs/2211.09037v1,"['cs.HC', 'cs.GR']"
"Generative AI-empowered Simulation for Autonomous Driving in Vehicular
  Mixed Reality Metaverses","In the vehicular mixed reality (MR) Metaverse, the distance between physical
and virtual entities can be overcome by fusing the physical and virtual
environments with multi-dimensional communications in autonomous driving
systems. Assisted by digital twin (DT) technologies, connected autonomous
vehicles (AVs), roadside units (RSU), and virtual simulators can maintain the
vehicular MR Metaverse via digital simulations for sharing data and making
driving decisions collaboratively. However, large-scale traffic and driving
simulation via realistic data collection and fusion from the physical world for
online prediction and offline training in autonomous driving systems are
difficult and costly. In this paper, we propose an autonomous driving
architecture, where generative AI is leveraged to synthesize unlimited
conditioned traffic and driving data in simulations for improving driving
safety and traffic efficiency. First, we propose a multi-task DT offloading
model for the reliable execution of heterogeneous DT tasks with different
requirements at RSUs. Then, based on the preferences of AV's DTs and collected
realistic data, virtual simulators can synthesize unlimited conditioned driving
and traffic datasets to further improve robustness. Finally, we propose a
multi-task enhanced auction-based mechanism to provide fine-grained incentives
for RSUs in providing resources for autonomous driving. The property analysis
and experimental results demonstrate that the proposed mechanism and
architecture are strategy-proof and effective, respectively.","['Minrui Xu', 'Dusit Niyato', 'Junlong Chen', 'Hongliang Zhang', 'Jiawen Kang', 'Zehui Xiong', 'Shiwen Mao', 'Zhu Han']",2023-02-16T16:54:10Z,http://arxiv.org/abs/2302.08418v1,"['cs.AI', 'cs.NI']"
HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations,"Generating both plausible and accurate full body avatar motion is the key to
the quality of immersive experiences in mixed reality scenarios. Head-Mounted
Devices (HMDs) typically only provide a few input signals, such as head and
hands 6-DoF. Recently, different approaches achieved impressive performance in
generating full body motion given only head and hands signal. However, to the
best of our knowledge, all existing approaches rely on full hand visibility.
While this is the case when, e.g., using motion controllers, a considerable
proportion of mixed reality experiences do not involve motion controllers and
instead rely on egocentric hand tracking. This introduces the challenge of
partial hand visibility owing to the restricted field of view of the HMD. In
this paper, we propose the first unified approach, HMD-NeMo, that addresses
plausible and accurate full body motion generation even when the hands may be
only partially visible. HMD-NeMo is a lightweight neural network that predicts
the full body motion in an online and real-time fashion. At the heart of
HMD-NeMo is the spatio-temporal encoder with novel temporally adaptable mask
tokens that encourage plausible motion in the absence of hand observations. We
perform extensive analysis of the impact of different components in HMD-NeMo
and introduce a new state-of-the-art on AMASS dataset through our evaluation.","['Sadegh Aliakbarian', 'Fatemeh Saleh', 'David Collier', 'Pashmina Cameron', 'Darren Cosker']",2023-08-22T08:07:12Z,http://arxiv.org/abs/2308.11261v1,['cs.CV']
"HoloPOCUS: Portable Mixed-Reality 3D Ultrasound Tracking, Reconstruction
  and Overlay","Ultrasound (US) imaging provides a safe and accessible solution to procedural
guidance and diagnostic imaging. The effective usage of conventional 2D US for
interventional guidance requires extensive experience to project the image
plane onto the patient, and the interpretation of images in diagnostics suffers
from high intra- and inter-user variability. 3D US reconstruction allows for
more consistent diagnosis and interpretation, but existing solutions are
limited in terms of equipment and applicability in real-time navigation. To
address these issues, we propose HoloPOCUS - a mixed reality US system (MR-US)
that overlays rich US information onto the user's vision in a point-of-care
setting. HoloPOCUS extends existing MR-US methods beyond placing a US plane in
the user's vision to include a 3D reconstruction and projection that can aid in
procedural guidance using conventional probes. We validated a tracking pipeline
that demonstrates higher accuracy compared to existing MR-US works.
Furthermore, user studies conducted via a phantom task showed significant
improvements in navigation duration when using our proposed methods.","['Kian Wei Ng', 'Yujia Gao', 'Shaheryar Mohammed Furqan', 'Zachery Yeo', 'Joel Lau', 'Kee Yuan Ngiam', 'Eng Tat Khoo']",2023-08-26T09:28:20Z,http://arxiv.org/abs/2308.13823v1,"['cs.CV', 'cs.HC']"
"Investigating the Correlation Between Presence and Reaction Time in
  Mixed Reality","Measuring presence is critical to improving user involvement and performance
in Mixed Reality (MR). \emph{Presence}, a crucial aspect of MR, is
traditionally gauged using subjective questionnaires, leading to a lack of
time-varying responses and susceptibility to user bias. Inspired by the
existing literature on the relationship between presence and human performance,
the proposed methodology systematically measures a user's reaction time to a
visual stimulus as they interact within a manipulated MR environment. We
explore the user reaction time as a quantity that can be easily measured using
the systemic tools available in modern MR devices. We conducted an exploratory
study (N=40) with two experiments designed to alter the users' sense of
presence by manipulating \emph{place illusion} and \emph{plausibility
illusion}. We found a significant correlation between presence scores and
reaction times with a correlation coefficient -0.65, suggesting that users with
a higher sense of presence responded more swiftly to stimuli. We develop a
model that estimates a user's presence level using the reaction time values
with high accuracy of up to 80\%. While our study suggests that reaction time
can be used as a measure of presence, further investigation is needed to
improve the accuracy of the model.","['Yasra Chandio', 'Noman Bashir', 'Victoria Interrante', 'Fatima M. Anwar']",2023-09-20T22:02:38Z,http://arxiv.org/abs/2309.11662v1,"['cs.HC', 'cs.ET']"
"BrickStARt: Enabling In-situ Design and Tangible Exploration for
  Personal Fabrication using Mixed Reality","3D printers enable end-users to design and fabricate unique physical
artifacts but maintain an increased entry barrier and friction. End users must
design tangible artifacts through intangible media away from the main problem
space (ex-situ) and transfer spatial requirements to an abstract software
environment. To allow users to evaluate dimensions, balance, or fit early and
in-situ, we developed BrickStARt, a design tool using tangible construction
blocks paired with a mixed-reality headset. Users assemble a physical block
model at the envisioned location of the fabricated artifact. Designs can be
tested tangibly, refined, and digitally post-processed, remaining continuously
in-situ. We implemented BrickStARt using a Magic Leap headset and present
walkthroughs, highlighting novel interactions for 3D design. In a user study
(n=16), first-time 3D modelers succeeded more often using BrickStARt than
Tinkercad. Our results suggest that BrickStARt provides an accessible and
explorative process while facilitating quick, tangible design iterations that
allow users to detect physics-related issues (e.g., clearance) early on.","['Evgeny Stemasov', 'Jessica Hohn', 'Maurice Cordts', 'Anja Schikorr', 'Enrico Rukzio', 'Jan Gugenheimer']",2023-10-05T17:18:13Z,http://arxiv.org/abs/2310.03700v1,"['cs.HC', 'H.5.1; H.5.2; H.5.m']"
GaitGuard: Towards Private Gait in Mixed Reality,"Augmented/Mixed Reality (AR/MR) devices are unique from other mobile systems
because of their capability to offer an immersive multi-user collaborative
experience. While previous studies have explored privacy and security aspects
of multiple user interactions in AR/MR, a less-explored area is the
vulnerability of gait privacy. Gait is considered a private state because it is
a highly individualistic and a distinctive biometric trait. Thus, preserving
gait privacy in emerging AR/MR systems is crucial to safeguard individuals from
potential identity tracking and unauthorized profiling.
  This paper first introduces GaitExtract, a framework designed to
automatically detect gait information in humans, shedding light on the nuances
of gait privacy in AR/MR. In this paper, we designed GaitExtract, a framework
that can automatically detect the outside gait information of a human and
investigate the vulnerability of gait privacy in AR. In a user study with 20
participants, our findings reveal that participants were uniquely identifiable
with an accuracy of up to 78% using GaitExtract. Consequently, we propose
GaitGuard, a system that safeguards gait information of people appearing in the
camera view of the AR/MR device.
  Furthermore, we tested GaitGuard in an MR collaborative application,
achieving 22 fps while streaming mitigated frames to the collaborative server.
Our user-study survey indicated that users are more comfortable with releasing
videos of them walking when GaitGuard is applied to the frames. These results
underscore the efficacy and practicality of GaitGuard in mitigating gait
privacy concerns in MR contexts.","['Diana Romero', 'Ruchi Jagdish Patel', 'Athina Markopoulou', 'Salma Elmalaki']",2023-12-07T17:42:04Z,http://arxiv.org/abs/2312.04470v2,"['cs.HC', 'cs.CR']"
"Collaborative System Design of Mixed Reality Communication for Medical
  Training","We present the design of a mixed reality (MR) telehealth training system that
aims to close the gap between in-person and distance training and re-training
for medical procedures. Our system uses real-time volumetric capture as a means
for communicating and relating spatial information between the non-colocated
trainee and instructor. The system's design is based on a requirements
elicitation study performed in situ, at a medical school simulation training
center. The focus is on the lightweight real-time transmission of volumetric
data - meaning the use of consumer hardware, easy and quick deployment, and
low-demand computations. We evaluate the MR system design by analyzing the
workload for the users during medical training. We compare in-person, video,
and MR training workloads. The results indicate that the overall workload for
central line placement training with MR does not increase significantly
compared to video communication. Our work shows that, when designed
strategically together with domain experts, an MR communication system can be
used effectively for complex medical procedural training without increasing the
overall workload for users significantly. Moreover, MR systems offer new
opportunities for teaching due to spatial information, hand tracking, and
augmented communication.","['Manuel Rebol', 'Krzysztof Pietroszek', 'Claudia Ranniger', 'Colton Hood', 'Adam Rutenberg', 'Neal Sikka', 'Christian Guetl']",2023-12-14T22:36:56Z,http://arxiv.org/abs/2312.09382v1,['cs.HC']
"ViboPneumo: A Vibratory-Pneumatic Finger-Worn Haptic Device for Altering
  Perceived Texture Roughness in Mixed Reality","Extensive research has been done in haptic feedback for texture simulation in
virtual reality (VR). However, it is challenging to modify the perceived
tactile texture of existing physical objects which usually serve as anchors for
virtual objects in mixed reality (MR). In this paper, we present ViboPneumo, a
finger-worn haptic device that uses vibratory-pneumatic feedback to modulate
(i.e., increase and decrease) the perceived roughness of the material surface
contacted by the user's fingerpad while supporting the perceived sensation of
other haptic properties (e.g., temperature or stickiness) in MR. Our device
includes a silicone-based pneumatic actuator that can lift the user's fingerpad
on the physical surface to reduce the contact area for roughness decreasing,
and an on-finger vibrator for roughness increasing. Our user-perception
experimental results showed that the participants could perceive changes in
roughness, both increasing and decreasing, compared to the original material
surface. We also observed the overlapping roughness ratings among certain
haptic stimuli (i.e., vibrotactile and pneumatic) and the originally perceived
roughness of some materials without any haptic feedback. This suggests the
potential to alter the perceived texture of one type of material to another in
terms of roughness (e.g., modifying the perceived texture of ceramics as
glass). Lastly, a user study of MR experience showed that ViboPneumo could
significantly improve the MR user experience, particularly for visual-haptic
matching, compared to the condition of a bare finger. We also demonstrated a
few application scenarios for ViboPneumo.","['Shaoyu Cai', 'Zhenlin Chen', 'Haichen Gao', 'Ya Huang', 'Qi Zhang', 'Xinge Yu', 'Kening Zhu']",2024-03-08T09:48:05Z,http://arxiv.org/abs/2403.05182v1,"['cs.HC', 'cs.GR']"
"Explainable Interfaces for Rapid Gaze-Based Interactions in Mixed
  Reality","Gaze-based interactions offer a potential way for users to naturally engage
with mixed reality (XR) interfaces. Black-box machine learning models enabled
higher accuracy for gaze-based interactions. However, due to the black-box
nature of the model, users might not be able to understand and effectively
adapt their gaze behaviour to achieve high quality interaction. We posit that
explainable AI (XAI) techniques can facilitate understanding of and interaction
with gaze-based model-driven system in XR. To study this, we built a real-time,
multi-level XAI interface for gaze-based interaction using a deep learning
model, and evaluated it during a visual search task in XR. A between-subjects
study revealed that participants who interacted with XAI made more accurate
selections compared to those who did not use the XAI system (i.e., F1 score
increase of 10.8%). Additionally, participants who used the XAI system adapted
their gaze behavior over time to make more effective selections. These findings
suggest that XAI can potentially be used to assist users in more effective
collaboration with model-driven interactions in XR.","['Mengjie Yu', 'Dustin Harris', 'Ian Jones', 'Ting Zhang', 'Yue Liu', 'Naveen Sendhilnathan', 'Narine Kokhlikyan', 'Fulton Wang', 'Co Tran', 'Jordan L. Livingston', 'Krista E. Taylor', 'Zhenhong Hu', 'Mary A. Hood', 'Hrvoje Benko', 'Tanya R. Jonker']",2024-04-21T21:13:46Z,http://arxiv.org/abs/2404.13777v1,['cs.HC']
"Alignment of the Virtual Scene to the Tracking Space of a Mixed Reality
  Head-Mounted Display","With the mounting global interest for optical see-through head-mounted
displays (OST-HMDs) across medical, industrial and entertainment settings, many
systems with different capabilities are rapidly entering the market. Despite
such variety, they all require display calibration to create a proper mixed
reality environment. With the aid of tracking systems, it is possible to
register rendered graphics with tracked objects in the real world. We propose a
calibration procedure to properly align the coordinate system of a 3D virtual
scene that the user sees with that of the tracker. Our method takes a blackbox
approach towards the HMD calibration, where the tracker's data is its input and
the 3D coordinates of a virtual object in the observer's eye is the output; the
objective is thus to find the 3D projection that aligns the virtual content
with its real counterpart. In addition, a faster and more intuitive version of
this calibration is introduced in which the user simultaneously aligns multiple
points of a single virtual 3D object with its real counterpart; this reduces
the number of required repetitions in the alignment from 20 to only 4, which
leads to a much easier calibration task for the user. In this paper, both
internal (HMD camera) and external tracking systems are studied. We perform
experiments with Microsoft HoloLens, taking advantage of its self localization
and spatial mapping capabilities to eliminate the requirement for line of sight
from the HMD to the object or external tracker. The experimental results
indicate an accuracy of up to 4 mm in the average reprojection error based on
two separate evaluation methods. We further perform experiments with the
internal tracking on the Epson Moverio BT-300 to demonstrate that the method
can provide similar results with other HMDs.","['Ehsan Azimi', 'Long Qian', 'Nassir Navab', 'Peter Kazanzides']",2017-03-16T21:51:23Z,http://arxiv.org/abs/1703.05834v4,['cs.HC']
"Augment Yourself: Mixed Reality Self-Augmentation Using Optical
  See-through Head-mounted Displays and Physical Mirrors","Optical see-though head-mounted displays (OST HMDs) are one of the key
technologies for merging virtual objects and physical scenes to provide an
immersive mixed reality (MR) environment to its user. A fundamental limitation
of HMDs is, that the user itself cannot be augmented conveniently as, in casual
posture, only the distal upper extremities are within the field of view of the
HMD. Consequently, most MR applications that are centered around the user, such
as virtual dressing rooms or learning of body movements, cannot be realized
with HMDs. In this paper, we propose a novel concept and prototype system that
combines OST HMDs and physical mirrors to enable self-augmentation and provide
an immersive MR environment centered around the user. Our system, to the best
of our knowledge the first of its kind, estimates the user's pose in the
virtual image generated by the mirror using an RGBD camera attached to the HMD
and anchors virtual objects to the reflection rather than the user directly. We
evaluate our system quantitatively with respect to calibration accuracy and
infrared signal degradation effects due to the mirror, and show its potential
in applications where large mirrors are already an integral part of the
facility. Particularly, we demonstrate its use for virtual fitting rooms,
gaming applications, anatomy learning, and personal fitness. In contrast to
competing devices such as LCD-equipped smart mirrors, the proposed system
consists of only an HMD with RGBD camera and, thus, does not require a prepared
environment making it very flexible and generic. In future work, we will aim to
investigate how the system can be optimally used for physical rehabilitation
and personal training as a promising application.","['Mathias Unberath', 'Kevin Yu', 'Roghayeh Barmaki', 'Alex Johnson', 'Nassir Navab']",2020-07-06T16:53:47Z,http://arxiv.org/abs/2007.02884v1,"['cs.HC', 'cs.CV']"
"A Novel Solution of Using Mixed Reality in Bowel and Oral and
  Maxillofacial Surgical Telepresence: 3D Mean Value Cloning algorithm","Background and aim: Most of the Mixed Reality models used in the surgical
telepresence are suffering from discrepancies in the boundary area and
spatial-temporal inconsistency due to the illumination variation in the video
frames. The aim behind this work is to propose a new solution that helps
produce the composite video by merging the augmented video of the surgery site
and the virtual hand of the remote expertise surgeon. The purpose of the
proposed solution is to decrease the processing time and enhance the accuracy
of merged video by decreasing the overlay and visualization error and removing
occlusion and artefacts. Methodology: The proposed system enhanced the mean
value cloning algorithm that helps to maintain the spatial-temporal consistency
of the final composite video. The enhanced algorithm includes the 3D mean value
coordinates and improvised mean value interpolant in the image cloning process,
which helps to reduce the sawtooth, smudging and discolouration artefacts
around the blending region. Results: As compared to the state of the art
solution, the accuracy in terms of overlay error of the proposed solution is
improved from 1.01mm to 0.80mm whereas the accuracy in terms of visualization
error is improved from 98.8% to 99.4%. The processing time is reduced to 0.173
seconds from 0.211 seconds. Conclusion: Our solution helps make the object of
interest consistent with the light intensity of the target image by adding the
space distance that helps maintain the spatial consistency in the final merged
video.","['Arjina Maharjan', 'Abeer Alsadoon', 'P. W. C. Prasad', 'Nada AlSallami', 'Tarik A. Rashid', 'Ahmad Alrubaie', 'Sami Haddad']",2021-03-17T10:01:06Z,http://arxiv.org/abs/2104.06316v1,"['physics.med-ph', 'cs.CV', 'cs.GR', 'cs.RO']"
"Deep Learning-based Framework for Automatic Cranial Defect
  Reconstruction and Implant Modeling","The goal of this work is to propose a robust, fast, and fully automatic
method for personalized cranial defect reconstruction and implant modeling.
  We propose a two-step deep learning-based method using a modified U-Net
architecture to perform the defect reconstruction, and a dedicated iterative
procedure to improve the implant geometry, followed by automatic generation of
models ready for 3-D printing. We propose a cross-case augmentation based on
imperfect image registration combining cases from different datasets. We
perform ablation studies regarding different augmentation strategies and
compare them to other state-of-the-art methods.
  We evaluate the method on three datasets introduced during the AutoImplant
2021 challenge, organized jointly with the MICCAI conference. We perform the
quantitative evaluation using the Dice and boundary Dice coefficients, and the
Hausdorff distance. The average Dice coefficient, boundary Dice coefficient,
and the 95th percentile of Hausdorff distance are 0.91, 0.94, and 1.53 mm
respectively. We perform an additional qualitative evaluation by 3-D printing
and visualization in mixed reality to confirm the implant's usefulness.
  We propose a complete pipeline that enables one to create the cranial implant
model ready for 3-D printing. The described method is a greatly extended
version of the method that scored 1st place in all AutoImplant 2021 challenge
tasks. We freely release the source code, that together with the open datasets,
makes the results fully reproducible. The automatic reconstruction of cranial
defects may enable manufacturing personalized implants in a significantly
shorter time, possibly allowing one to perform the 3-D printing process
directly during a given intervention. Moreover, we show the usability of the
defect reconstruction in mixed reality that may further reduce the surgery
time.","['Marek Wodzinski', 'Mateusz Daniol', 'Miroslaw Socha', 'Daria Hemmerling', 'Maciej Stanuch', 'Andrzej Skalski']",2022-04-13T11:33:26Z,http://arxiv.org/abs/2204.06310v1,"['eess.IV', 'cs.CV', 'cs.LG']"
"Full Body Video-Based Self-Avatars for Mixed Reality: from E2E System to
  User Study","In this work we explore the creation of self-avatars through video
pass-through in Mixed Reality (MR) applications. We present our end-to-end
system, including: custom MR video pass-through implementation on a commercial
head mounted display (HMD), our deep learning-based real-time egocentric body
segmentation algorithm, and our optimized offloading architecture, to
communicate the segmentation server with the HMD. To validate this technology,
we designed an immersive VR experience where the user has to walk through a
narrow tiles path over an active volcano crater. The study was performed under
three body representation conditions: virtual hands, video pass-through with
color-based full-body segmentation and video pass-through with deep learning
full-body segmentation. This immersive experience was carried out by 30 women
and 28 men. To the best of our knowledge, this is the first user study focused
on evaluating video-based self-avatars to represent the user in a MR scene.
Results showed no significant differences between the different body
representations in terms of presence, with moderate improvements in some
Embodiment components between the virtual hands and full-body representations.
Visual Quality results showed better results from the deep-learning algorithms
in terms of the whole body perception and overall segmentation quality. We
provide some discussion regarding the use of video-based self-avatars, and some
reflections on the evaluation methodology. The proposed E2E solution is in the
boundary of the state of the art, so there is still room for improvement before
it reaches maturity. However, this solution serves as a crucial starting point
for novel MR distributed solutions.","['Diego Gonzalez Morin', 'Ester Gonzalez-Sosa', 'Pablo Perez', 'Alvaro Villegas']",2022-08-24T20:59:17Z,http://arxiv.org/abs/2208.12639v1,['cs.CV']
ArK: Augmented Reality with Knowledge Interactive Emergent Ability,"Despite the growing adoption of mixed reality and interactive AI agents, it
remains challenging for these systems to generate high quality 2D/3D scenes in
unseen environments. The common practice requires deploying an AI agent to
collect large amounts of data for model training for every new task. This
process is costly, or even impossible, for many domains. In this study, we
develop an infinite agent that learns to transfer knowledge memory from general
foundation models (e.g. GPT4, DALLE) to novel domains or scenarios for scene
understanding and generation in the physical or virtual world. The heart of our
approach is an emerging mechanism, dubbed Augmented Reality with Knowledge
Inference Interaction (ArK), which leverages knowledge-memory to generate
scenes in unseen physical world and virtual reality environments. The knowledge
interactive emergent ability (Figure 1) is demonstrated as the observation
learns i) micro-action of cross-modality: in multi-modality models to collect a
large amount of relevant knowledge memory data for each interaction task (e.g.,
unseen scene understanding) from the physical reality; and ii) macro-behavior
of reality-agnostic: in mix-reality environments to improve interactions that
tailor to different characterized roles, target variables, collaborative
information, and so on. We validate the effectiveness of ArK on the scene
generation and editing tasks. We show that our ArK approach, combined with
large foundation models, significantly improves the quality of generated 2D/3D
scenes, compared to baselines, demonstrating the potential benefit of
incorporating ArK in generative AI for applications such as metaverse and
gaming simulation.","['Qiuyuan Huang', 'Jae Sung Park', 'Abhinav Gupta', 'Paul Bennett', 'Ran Gong', 'Subhojit Som', 'Baolin Peng', 'Owais Khan Mohammed', 'Chris Pal', 'Yejin Choi', 'Jianfeng Gao']",2023-05-01T17:57:01Z,http://arxiv.org/abs/2305.00970v1,['cs.CV']
"Which architecture should be implemented to manage data from the real
  world, in an Unreal Engine 5 simulator and in the context of mixed reality?","Due to its ability to generate millions of particles, massively detailed
scenes and confusing artificial illumination with reality, the version 5 of
Unreal Engine promises unprecedented industrial applications. The paradigms and
aims of Unreal Engine contrast with the industrial simulators typically used by
the scientific community. The visual quality and performance of its rendering
engine increase the opportunities, especially for industries and simulation
business: where interoperability and scalability are required. The study of the
following issue `` Which architecture should we implement to integrate
real-world data, in an Unreal Engine 5 simulator and in a mixed-reality
environment? '' offers a point of view. The topic is reexamined in an
innovative and conceptual way, such as the generalization of mixedreality
technologies, Internet of Things, digital twins, Big Data but providing a
solution for simple and actual use cases. This paper gives a detailed analysis
of the issue, at both theoretical and operational level. Then, the document
goes deep into Unreal Engine's operation in order to extract the vanilla
capabilities. Next, the C++ Plugin system is reviewed in details as well as the
third-party library integration: pitfalls to be avoided are shown. Finally, the
last chapter proposes a generic architecture, useful in large-scale industrial
3D applications, such as collaborative work or hyper-connected simulators. This
document might be of interest to an Unreal Engine expert who would like to
discover about server architectures. Conversely, it could be relevant for an
expert in backend servers who wants to learn about Unreal Engine capabilities.
This research concludes that Unreal Engine's modularity enables integration
with almost any protocol. The features to integrate external real data are
numerous but depend on use cases. Distributed systems for Big Data require a
scalable architecture, possibly without the use of the Unreal Engine dedicated
server. Environments, which require sub-second latency need to implement direct
connections, bypassing any intermediate servers.",['Jonathan Cassaing'],2023-05-16T07:51:54Z,http://arxiv.org/abs/2305.09244v1,['cs.SE']
"Forging the Industrial Metaverse -- Where Industry 5.0, Augmented and
  Mixed Reality, IIoT, Opportunistic Edge Computing and Digital Twins Meet","The Metaverse is a concept that proposes to immerse users into real-time
rendered 3D content virtual worlds delivered through Extended Reality (XR)
devices like Augmented and Mixed Reality (AR/MR) smart glasses and Virtual
Reality (VR) headsets. When the Metaverse concept is applied to industrial
environments, it is called Industrial Metaverse, a hybrid world where
industrial operators work by using some of the latest technologies. Currently,
such technologies are related to the ones fostered by Industry 4.0, which is
evolving towards Industry 5.0, a paradigm that enhances Industry 4.0 by
creating a sustainable and resilient world of industrial human-centric
applications. The Industrial Metaverse can benefit from Industry 5.0, since it
implies making use of dynamic and up-to-date content, as well as fast
human-to-machine interactions. To enable such enhancements, this article
proposes the concept of Meta-Operator: an Industry 5.0 worker that interacts
with Industrial Metaverse applications and with his/her surroundings through
advanced XR devices. This article provides a description of the technologies
that support Meta-Operators: the main components of the Industrial Metaverse,
the latest XR technologies and the use of Opportunistic Edge Computing
communications (to interact with surrounding IoT/IioT devices). Moreover, this
paper analyzes how to create the next generation of Industrial Metaverse
applications based on Industry 5.0, including the integration of AR/MR devices
with IoT/IIoT solutions, the development of advanced communications or the
creation of shared experiences. Finally, this article provides a list of
potential Industry 5.0 applications for the Industrial Metaverse and analyzes
the main challenges and research lines. Thus, this article provides useful
guidelines for the researchers that will create the next generation of
applications for the Industrial Metaverse.","['Tiago M. Fernández-Caramés', 'Paula Fraga-Lamas']",2024-03-17T19:14:28Z,http://arxiv.org/abs/2403.11312v1,"['cs.ET', 'cs.HC']"
"A Distributed Software Architecture for Collaborative Teleoperation
  based on a VR Platform and Web Application Interoperability","Augmented Reality and Virtual Reality can provide to a Human Operator (HO) a
real help to complete complex tasks, such as robot teleoperation and
cooperative teleassistance. Using appropriate augmentations, the HO can
interact faster, safer and easier with the remote real world. In this paper, we
present an extension of an existing distributed software and network
architecture for collaborative teleoperation based on networked human-scaled
mixed reality and mobile platform. The first teleoperation system was composed
by a VR application and a Web application. However the 2 systems cannot be used
together and it is impossible to control a distant robot simultaneously. Our
goal is to update the teleoperation system to permit a heterogeneous
collaborative teleoperation between the 2 platforms. An important feature of
this interface is based on different Mobile platforms to control one or many
robots.","['Christophe Domingues', 'Samir Otmane', 'Frédéric Davesne', 'Malik Mallem']",2009-04-14T11:21:47Z,http://arxiv.org/abs/0904.2096v1,"['cs.HC', 'cs.GR', 'cs.MM', 'cs.RO']"
Immersive Augmented Reality Training for Complex Manufacturing Scenarios,"In the complex manufacturing sector a considerable amount of resources are
focused on developing new skills and training workers. In that context,
increasing the effectiveness of those processes and reducing the investment
required is an outstanding issue. In this paper we present an experiment that
shows how modern Human Computer Interaction (HCI) metaphors such as
collaborative mixed-reality can be used to transmit procedural knowledge and
could eventually replace other forms of face-to-face training. We implement a
real-time Immersive Augmented Reality (IAR) setup with see-through cameras that
allows for collaborative interactions that can simulate conventional forms of
training. The obtained results indicate that people who took the IAR training
achieved the same performance than people in the conventional face-to-face
training condition. These results, their implications for future training and
the use of HCI paradigms in this context are discussed in this paper.","['Mar Gonzalez-Franco', 'Julio Cermeron', 'Katie Li', 'Rodrigo Pizarro', 'Jacob Thorn', 'Windo Hutabarat', 'Ashutosh Tiwari', 'Pablo Bermell-Garcia']",2016-02-05T07:50:25Z,http://arxiv.org/abs/1602.01944v2,['cs.HC']
"Automated capture and delivery of assistive task guidance with an
  eyewear computer: The GlaciAR system","In this paper we describe and evaluate a mixed reality system that aims to
augment users in task guidance applications by combining automated and
unsupervised information collection with minimally invasive video guides. The
result is a self-contained system that we call GlaciAR (Glass-enabled
Contextual Interactions for Augmented Reality), that operates by extracting
contextual interactions from observing users performing actions. GlaciAR is
able to i) automatically determine moments of relevance based on a head motion
attention model, ii) automatically produce video guidance information, iii)
trigger these video guides based on an object detection method, iv) learn
without supervision from observing multiple users and v) operate fully on-board
a current eyewear computer (Google Glass). We describe the components of
GlaciAR together with evaluations on how users are able to use the system to
achieve three tasks. We see this work as a first step toward the development of
systems that aim to scale up the notoriously difficult authoring problem in
guidance systems and where people's natural abilities are enhanced via
minimally invasive visual guidance.","['Teesid Leelasawassuk', 'Dima Damen', 'Walterio Mayol-Cuevas']",2016-12-29T01:10:54Z,http://arxiv.org/abs/1701.02586v1,['cs.HC']
BodyDigitizer: An Open Source Photogrammetry-based 3D Body Scanner,"With the rising popularity of Augmented and Virtual Reality, there is a need
for representing humans as virtual avatars in various application domains
ranging from remote telepresence, games to medical applications. Besides
explicitly modelling 3D avatars, sensing approaches that create person-specific
avatars are becoming popular. However, affordable solutions typically suffer
from a low visual quality and professional solution are often too expensive to
be deployed in nonprofit projects.
  We present an open-source project, BodyDigitizer, which aims at providing
both build instructions and configuration software for a high-resolution
photogrammetry-based 3D body scanner. Our system encompasses up to 96 Rasperry
PI cameras, active LED lighting, a sturdy frame construction and open-source
configuration software. %We demonstrate the applicability of the body scanner
in a nonprofit Mixed Reality health project. The detailed build instruction and
software are available at http://www.bodydigitizer.org.","['Travis Gesslein', 'Daniel Scherer', 'Jens Grubert']",2017-10-03T20:10:10Z,http://arxiv.org/abs/1710.01370v2,"['cs.CV', 'cs.HC']"
"Air Mounted Eyepiece: Design Methods for Aerial Optical Functions of
  Near-Eye and See-Through Display using Transmissive Mirror Device","We propose a novel method to implement an optical see-through head mounted
display which renders real aerial images with a wide viewing angle, called an
Air Mounted Eyepiece (AME). To achieve the AMD design, we employ an
off-the-shelf head mounted display and Transmissive Mirror Device (TMD) which
is usually used in aerial real imaging systems. In the proposed method, we
replicate the function of the head mounted display (HMD) itself, which is used
in the air by using the TMD and presenting a real image of eyepiece in front of
the eye. Moreover, it can realize a wide viewing angle 3D display by placing a
virtual lens in front of the eye without wearing an HMD. In addition to
enhancing the experience of mixed reality and augmented reality, our proposed
method can be used as a 3D imaging method for use in other applications such as
in automobiles and desktop work. We aim to contribute to the field of
human-computer interaction and the research on eyepiece interfaces by
discussing the advantages and the limitations of this near-eye optical system.","['Yoichi Ochiai', 'Kazuki Otao', 'Hiroyuki Osone']",2017-10-11T03:08:02Z,http://arxiv.org/abs/1710.03889v1,['cs.HC']
Beyond One Glance: Gated Recurrent Architecture for Hand Segmentation,"As mixed reality is gaining increased momentum, the development of effective
and efficient solutions to egocentric hand segmentation is becoming critical.
Traditional segmentation techniques typically follow a one-shot approach, where
the image is passed forward only once through a model that produces a
segmentation mask. This strategy, however, does not reflect the perception of
humans, who continuously refine their representation of the world. In this
paper, we therefore introduce a novel gated recurrent architecture. It goes
beyond both iteratively passing the predicted segmentation mask through the
network and adding a standard recurrent unit to it. Instead, it incorporates
multiple encoder-decoder layers of the segmentation network, so as to keep
track of its internal state in the refinement process. As evidenced by our
results on standard hand segmentation benchmarks and on our own dataset, our
approach outperforms these other, simpler recurrent segmentation techniques, as
well as the state-of-the-art hand segmentation one. Furthermore, we demonstrate
the generality of our approach by applying it to road segmentation, where it
also outperforms other baseline methods.","['Wei Wang', 'Kaicheng Yu', 'Joachim Hugonot', 'Pascal Fua', 'Mathieu Salzmann']",2018-11-27T11:16:41Z,http://arxiv.org/abs/1811.10914v3,"['cs.CV', 'cs.LG']"
Interoperable GPU Kernels as Latency Improver for MEC,"Mixed reality (MR) applications are expected to become common when 5G goes
mainstream. However, the latency requirements are challenging to meet due to
the resources required by video-based remoting of graphics, that is, decoding
video codecs. We propose an approach towards tackling this challenge: a
client-server implementation for transacting intermediate representation (IR)
between a mobile UE and a MEC server instead of video codecs and this way
avoiding video decoding. We demonstrate the ability to address latency
bottlenecks on edge computing workloads that transact graphics. We select
SPIR-V compatible GPU kernels as the intermediate representation. Our approach
requires know-how in GPU architecture and GPU domain-specific languages (DSLs),
but compared to video-based edge graphics, it decreases UE device delay by
sevenfold. Further, we find that due to low cold-start times on both UEs and
MEC servers, application migration can happen in milliseconds. We imply that
graphics-based location-aware applications, such as MR, can benefit from this
kind of approach.","['Juuso Haavisto', 'Jukka Riekki']",2020-01-25T19:07:58Z,http://arxiv.org/abs/2001.09352v1,"['cs.DC', 'cs.GR']"
"Blending Entropy: A Term for Addressing Information Density in Mediated
  Reality","The virtuality continuum describes the degrees of positive virtuality under
the umbrella term mixed reality. Besides adding virtual information within a
mixed environment, diminished reality aims at reducing real world information.
Mann defined the term mediated reality (MR), which also considered diminished
reality, but without the possibility to describe different degrees of fusion
between a mixed and a diminished reality. That is why this work defines the new
term blending entropy that captures the relations between a mixed and a
diminished reality. The blending entropy is based on the information density of
the mediated reality and the actual area the user has to comprehend, which is
named perceptual frustum. We describe the blending entropy's twodimensional
dependencies and detail important points in the blending entropy's space.","['Philipp Tiefenbacher', 'Gerhard Rigoll']",2016-09-13T06:29:47Z,http://arxiv.org/abs/1609.03695v2,['cs.HC']
"Towards high-throughput 3D insect capture for species discovery and
  diagnostics","Digitisation of natural history collections not only preserves precious
information about biological diversity, it also enables us to share, analyse,
annotate and compare specimens to gain new insights. High-resolution,
full-colour 3D capture of biological specimens yields color and geometry
information complementary to other techniques (e.g., 2D capture, electron
scanning and micro computed tomography). However 3D colour capture of small
specimens is slow for reasons including specimen handling, the narrow depth of
field of high magnification optics, and the large number of images required to
resolve complex shapes of specimens. In this paper, we outline techniques to
accelerate 3D image capture, including using a desktop robotic arm to automate
the insect handling process; using a calibrated pan-tilt rig to avoid attaching
calibration targets to specimens; using light field cameras to capture images
at an extended depth of field in one shot; and using 3D Web and mixed reality
tools to facilitate the annotation, distribution and visualisation of 3D
digital models.","['Chuong Nguyen', 'Matt Adcock', 'Stuart Anderson', 'David Lovell', 'Nicole Fisher', 'John La Salle']",2017-09-07T00:31:03Z,http://arxiv.org/abs/1709.02033v1,['cs.CV']
"CoAug-MR: An MR-based Interactive Office Workstation Design System via
  Augmented Multi-Person Collaboration","Digital prototyping and evaluation using 3D modeling and digital human models
are becoming more practical for customizing products to the preference of a
user. However, the 3D modeling is less accessible to casual users, and digital
human models suffer from insufficient body data and less intuitive illustration
on how people use the product or how it accommodates to their body. Recently,
VR-supported 'Do It Yourself' design has achieved real-time ergonomic
evaluation with users themselves by capturing their poses, however, it lacks
reliability and quality of design. In this paper, we explore a multi-person
interactive design approach that enables designers, users, and even ergonomists
to collaborate to achieve effective and reliable design and prototyping tasks.
Mixed Reality that utilizes Hololens and motion tracking devices had been
developed to provide instant design feedback and evaluation and to experience
prototyping in physical space. We evaluate the system based on the usability
study, where casual users and designers are engaged in the interactive process
of designing items with respect to the body information, the preference, and
the environment.","['Lin Wang', 'Kuk-Jin Yoon']",2019-07-06T10:19:04Z,http://arxiv.org/abs/1907.03107v3,"['cs.HC', 'cs.GR']"
CityScopeAR: Urban Design and Crowdsourced Engagement Platform,"Processes of urban planning, urban design and architecture are inherently
tangible, iterative and collaborative. Nevertheless, the majority of tools in
these fields offer virtual environments and single user experience. This paper
presents CityScopeAR: a computational-tangible mixed-reality platform designed
for collaborative urban design processes. It portrays the evolution of the tool
and presents an overview of the history and limitations of notable CAD and TUI
platforms. As well, it depicts the development of a distributed networking
system between TUIs and CityScopeAR, as a key in design collaboration. It
shares the potential advantage of broad and decentralized community-engagement
process using such tools. Finally, this paper demonstrates several real-world
tests and deployments of CityScopeAR and proposes a path to future integration
of AR/MR devices in urban design and public participation.","['Ariel Noyman', 'Yasushi Sakai', 'Kent Larson']",2019-07-19T17:30:26Z,http://arxiv.org/abs/1907.08586v1,"['cs.HC', 'H.5']"
An Agent-Based Intelligent HCI Information System in Mixed Reality,"This paper presents a design of agent-based intelligent HCI (iHCI) system
using collaborative information for MR to improve user experience and
information security based on context-aware computing. In order to implement
target awareness system, we propose the use of non-parameter stochastic
adaptive learning and a kernel learning strategy for improving the adaptivity
of the recognition. The proposed design involves the use of a context-aware
computing strategy to recognize patterns for simulating human awareness and
processing of stereo pattern analysis. It provides a flexible customization
method for scene creation and manipulation. It also enables several types of
awareness related to the interactive target, user-experience, system
performance, confidentiality, and agent identification by applying several
strategies, such as context pattern analysis, scalable learning, data-aware
confidential computing.","['Hamed Alqahtani', 'Charles Z. Liu', 'Manolya Kavakli-Thorne', 'Yuzhi Kang']",2019-11-07T02:46:55Z,http://arxiv.org/abs/1911.02726v1,['cs.HC']
Improving pixel differentiation in holographic images,"Computer generated holography (CGH) has seen a resurgence in recent years
due, in part, to the rise of virtual and mixed reality systems. The majority of
approaches for CGH are based on a sampled Discrete Fourier Transform (DFT) and
ignore the interstitial behaviour between sampling points in the replay field.
In this paper we demonstrate that neighbouring replay field pixels can
interfere significantly giving the visual impression of pixel movement and
increased noise. We also demonstrate that increasing the separation between
target pixels reduces the interference and improves pixel quality. This
phenomena is demonstrated experimentally with close agreement between model and
measured result.
  This work begins by introducing the concept of pixel differentiation before
showing simulated models of pixel differentiation issues. Two mitigation
approaches are introduced and an experimental system is then used to validate
the simulation. Finally results are discussed and conclusions drawn.","['Peter J. Christopher', 'Ralf Mouthaan', 'John P. Freeman', 'Timothy D. Wilkinson']",2019-12-23T14:38:56Z,http://arxiv.org/abs/1912.12196v1,"['physics.optics', 'eess.IV']"
"Tangible Holograms: Towards Mobile Physical Augmentation of Virtual
  Objects","The last two decades have seen the emergence and steady development of
tangible user interfaces. While most of these interfaces are applied for input
- with output still on traditional computer screens - the goal of programmable
matter and actuated shape-changing materials is to directly use the physical
objects for visual or tangible feedback. Advances in material sciences and
flexible display technologies are investigated to enable such reconfigurable
physical objects. While existing solutions aim for making physical objects more
controllable via the digital world, we propose an approach where holograms
(virtual objects) in a mixed reality environment are augmented with physical
variables such as shape, texture or temperature. As such, the support for
mobility forms an important contribution of the proposed solution since it
enables users to freely move within and across environments. Furthermore, our
augmented virtual objects can co-exist in a single environment with
programmable matter and other actuated shape-changing solutions. The future
potential of the proposed approach is illustrated in two usage scenarios and we
hope that the presentation of our work in progress on a novel way to realise
tangible holograms will foster some lively discussions in the CHI community.","['Beat Signer', 'Timothy J. Curtin']",2017-03-24T05:31:56Z,http://arxiv.org/abs/1703.08288v1,['cs.HC']
"Google Cardboard Dates Augmented Reality : Issues, Challenges and Future
  Opportunities","The Google's frugal Cardboard solution for immersive Virtual Reality
experiences has come a long way in the VR market. The Google Cardboard VR
applications will support us in the fields such as education, virtual tourism,
entertainment, gaming, design etc. Recently, Qualcomm's Vuforia SDK has
introduced support for developing mixed reality applications for Google
Cardboard which can combine Virtual and Augmented Reality to develop exciting
and immersive experiences. In this work, we present a comprehensive review of
Google Cardboard for AR and also highlight its technical and subjective
limitations by conducting a feasibility study through the inspection of a
Desktop computer use-case. Additionally, we recommend the future avenues for
the Google Cardboard in AR. This work also serves as a guide for Android/iOS
developers as there are no published scholarly articles or well documented
studies exclusively on Google Cardboard with both user and developer's
experience captured at one place.","['Ramakrishna Perla', 'Ramya Hebbalaguppe']",2017-06-05T06:26:25Z,http://arxiv.org/abs/1706.03851v1,['cs.HC']
"MixedPeds: Pedestrian Detection in Unannotated Videos using
  Synthetically Generated Human-agents for Training","We present a new method for training pedestrian detectors on an unannotated
set of images. We produce a mixed reality dataset that is composed of
real-world background images and synthetically generated static human-agents.
Our approach is general, robust, and makes no other assumptions about the
unannotated dataset regarding the number or location of pedestrians. We
automatically extract from the dataset: i) the vanishing point to calibrate the
virtual camera, and ii) the pedestrians' scales to generate a Spawn Probability
Map, which is a novel concept that guides our algorithm to place the
pedestrians at appropriate locations. After putting synthetic human-agents in
the unannotated images, we use these augmented images to train a Pedestrian
Detector, with the annotations generated along with the synthetic agents. We
conducted our experiments using Faster R-CNN by comparing the detection results
on the unannotated dataset performed by the detector trained using our approach
and detectors trained with other manually labeled datasets. We showed that our
approach improves the average precision by 5-13% over these detectors.","['Ernest C. Cheung', 'Tsan Kwong Wong', 'Aniket Bera', 'Dinesh Manocha']",2017-07-28T04:05:33Z,http://arxiv.org/abs/1707.09100v2,['cs.CV']
"Specifying, Monitoring, and Executing Workflows in Linked Data
  Environments","We present an ontology for representing workflows over components with
Read-Write Linked Data interfaces and give an operational semantics to the
ontology via a rule language. Workflow languages have been successfully applied
for modelling behaviour in enterprise information systems, in which the data is
often managed in a relational database. Linked Data interfaces have been widely
deployed on the web to support data integration in very diverse domains,
increasingly also in scenarios involving the Internet of Things, in which
application behaviour is often specified using imperative programming
languages. With our work we aim to combine workflow languages, which allow for
the high-level specification of application behaviour by non-expert users, with
Linked Data, which allows for decentralised data publication and integrated
data access. We show that our ontology is expressive enough to cover the basic
workflow patterns and demonstrate the applicability of our approach with a
prototype system that observes pilots carrying out tasks in a mixed-reality
aircraft cockpit. On a synthetic benchmark from the building automation domain,
the runtime scales linearly with the size of the number of Internet of Things
devices.","['Tobias Käfer', 'Andreas Harth']",2018-04-13T17:22:17Z,http://arxiv.org/abs/1804.05044v4,"['cs.AI', 'cs.SE']"
"Varifocal zoom imaging with large area focal length adjustable
  metalenses","Varifocal lenses are essential components of dynamic optical systems with
applications in photography, mixed reality, and microscopy. Metasurface optics
has strong potential for creating tunable flat optics. Existing tunable
metalenses, however, typically require microelectromechanical actuators, which
cannot be scaled to large area devices, or rely on high voltages to stretch a
flexible substrate and achieve a sufficient tuning range. Here, we build a 1 cm
aperture varifocal metalens system at 1550 nm wavelength inspired by an Alvarez
lens, fabricated using high-throughput stepper photolithography. We demonstrate
a nonlinear change in focal length by minimally actuating two cubic phase
metasurfaces laterally, with focusing efficiency as high as 57% and a wide
focal length change of more than 6 cm (> 200%). We also test a lens design at
visible wavelength and conduct varifocal zoom imaging with a demonstrated 4x
zoom capability without any other optical elements in the imaging path.","['Shane Colburn', 'Alan Zhan', 'Arka Majumdar']",2018-05-20T22:46:28Z,http://arxiv.org/abs/1805.07832v1,['physics.optics']
Robust Point Light Source Estimation Using Differentiable Rendering,"Illumination estimation is often used in mixed reality to re-render a scene
from another point of view, to change the color/texture of an object, or to
insert a virtual object consistently lit into a real video or photograph.
Specifically, the estimation of a point light source is required for the
shadows cast by the inserted object to be consistent with the real scene. We
tackle the problem of illumination retrieval given an RGBD image of the scene
as an inverse problem: we aim to find the illumination that minimizes the
photometric error between the rendered image and the observation. In particular
we propose a novel differentiable renderer based on the Blinn-Phong model with
cast shadows. We compare our differentiable renderer to state-of-the-art
methods and demonstrate its robustness to an incorrect reflectance estimation.","['Grégoire Nieto', 'Salma Jiddi', 'Philippe Robert']",2018-12-12T09:05:11Z,http://arxiv.org/abs/1812.04857v1,['cs.CV']
"MVC-3D: Adaptive Design Pattern for Virtual and Augmented Reality
  Systems","In this paper, we present MVC-3D design pattern to develop virtual and
augmented (or mixed) reality interfaces that use new types of sensors,
modalities and implement specific algorithms and simulation models. The
proposed pattern represents the extension of classic MVC pattern by enriching
the View component (interactive View) and adding a specific component
(Library). The results obtained on the development of augmented reality
interfaces showed that the complexity of M, iV and C components is reduced. The
complexity increases only on the Library component (L). This helps the
programmers to well structure their models even if the interface complexity
increases. The proposed design pattern is also used in a design process called
MVC-3D in the loop that enables a seamless evolution from initial prototype to
the final system.","['Samir Benbelkacem', 'Djamel Aouam', 'Nadia Zenati-Henda', 'Abdelkader Bellarbi', 'Ahmed Bouhena', 'Samir Otmane']",2019-03-01T07:38:23Z,http://arxiv.org/abs/1903.00185v1,"['cs.HC', 'cs.SE']"
Privacy Preserving Image-Based Localization,"Image-based localization is a core component of many augmented/mixed reality
(AR/MR) and autonomous robotic systems. Current localization systems rely on
the persistent storage of 3D point clouds of the scene to enable camera pose
estimation, but such data reveals potentially sensitive scene information. This
gives rise to significant privacy risks, especially as for many applications 3D
mapping is a background process that the user might not be fully aware of. We
pose the following question: How can we avoid disclosing confidential
information about the captured 3D scene, and yet allow reliable camera pose
estimation? This paper proposes the first solution to what we call privacy
preserving image-based localization. The key idea of our approach is to lift
the map representation from a 3D point cloud to a 3D line cloud. This novel
representation obfuscates the underlying scene geometry while providing
sufficient geometric constraints to enable robust and accurate 6-DOF camera
pose estimation. Extensive experiments on several datasets and localization
scenarios underline the high practical relevance of our proposed approach.","['Pablo Speciale', 'Johannes L. Schönberger', 'Sing Bing Kang', 'Sudipta N. Sinha', 'Marc Pollefeys']",2019-03-13T16:12:04Z,http://arxiv.org/abs/1903.05572v1,['cs.CV']
"Understanding the Limitations of CNN-based Absolute Camera Pose
  Regression","Visual localization is the task of accurate camera pose estimation in a known
scene. It is a key problem in computer vision and robotics, with applications
including self-driving cars, Structure-from-Motion, SLAM, and Mixed Reality.
Traditionally, the localization problem has been tackled using 3D geometry.
Recently, end-to-end approaches based on convolutional neural networks have
become popular. These methods learn to directly regress the camera pose from an
input image. However, they do not achieve the same level of pose accuracy as 3D
structure-based methods. To understand this behavior, we develop a theoretical
model for camera pose regression. We use our model to predict failure cases for
pose regression techniques and verify our predictions through experiments. We
furthermore use our model to show that pose regression is more closely related
to pose approximation via image retrieval than to accurate pose estimation via
3D structure. A key result is that current approaches do not consistently
outperform a handcrafted image retrieval baseline. This clearly shows that
additional research is needed before pose regression algorithms are ready to
compete with structure-based methods.","['Torsten Sattler', 'Qunjie Zhou', 'Marc Pollefeys', 'Laura Leal-Taixe']",2019-03-18T15:24:11Z,http://arxiv.org/abs/1903.07504v1,['cs.CV']
"Learning Convolutional Transforms for Lossy Point Cloud Geometry
  Compression","Efficient point cloud compression is fundamental to enable the deployment of
virtual and mixed reality applications, since the number of points to code can
range in the order of millions. In this paper, we present a novel data-driven
geometry compression method for static point clouds based on learned
convolutional transforms and uniform quantization. We perform joint
optimization of both rate and distortion using a trade-off parameter. In
addition, we cast the decoding process as a binary classification of the point
cloud occupancy map. Our method outperforms the MPEG reference solution in
terms of rate-distortion on the Microsoft Voxelized Upper Bodies dataset with
51.5% BDBR savings on average. Moreover, while octree-based methods face
exponential diminution of the number of points at low bitrates, our method
still produces high resolution outputs even at low bitrates. Code and
supplementary material are available at
https://github.com/mauriceqch/pcc_geo_cnn .","['Maurice Quach', 'Giuseppe Valenzise', 'Frederic Dufaux']",2019-03-20T15:14:15Z,http://arxiv.org/abs/1903.08548v2,"['cs.CV', 'cs.LG', 'eess.IV', 'stat.ML']"
Omnipotent Virtual Giant for Remote Human-Swarm Interaction,"This paper proposes an intuitive human-swarm interaction framework inspired
by our childhood memory in which we interacted with living ants by changing
their positions and environments as if we were omnipotent relative to the ants.
In virtual reality, analogously, we can be a super-powered virtual giant who
can supervise a swarm of mobile robots in a vast and remote environment by
flying over or resizing the world and coordinate them by picking and placing a
robot or creating virtual walls. This work implements this idea by using
Virtual Reality along with Leap Motion, which is then validated by
proof-of-concept experiments using real and virtual mobile robots in mixed
reality. We conduct a usability analysis to quantify the effectiveness of the
overall system as well as the individual interfaces proposed in this work. The
results revealed that the proposed method is intuitive and feasible for
interaction with swarm robots, but may require appropriate training for the new
end-user interface device.","['Inmo Jang', 'Junyan Hu', 'Farshad Arvin', 'Joaquin Carrasco', 'Barry Lennox']",2019-03-24T21:39:49Z,http://arxiv.org/abs/1903.10064v2,['cs.RO']
3D Virtual Garment Modeling from RGB Images,"We present a novel approach that constructs 3D virtual garment models from
photos. Unlike previous methods that require photos of a garment on a human
model or a mannequin, our approach can work with various states of the garment:
on a model, on a mannequin, or on a flat surface. To construct a complete 3D
virtual model, our approach only requires two images as input, one front view
and one back view. We first apply a multi-task learning network called JFNet
that jointly predicts fashion landmarks and parses a garment image into
semantic parts. The predicted landmarks are used for estimating sizing
information of the garment. Then, a template garment mesh is deformed based on
the sizing information to generate the final 3D model. The semantic parts are
utilized for extracting color textures from input images. The results of our
approach can be used in various Virtual Reality and Mixed Reality applications.","['Yi Xu', 'Shanglin Yang', 'Wei Sun', 'Li Tan', 'Kefeng Li', 'Hui Zhou']",2019-07-31T21:47:52Z,http://arxiv.org/abs/1908.00114v1,['cs.CV']
To Learn or Not to Learn: Visual Localization from Essential Matrices,"Visual localization is the problem of estimating a camera within a scene and
a key component in computer vision applications such as self-driving cars and
Mixed Reality. State-of-the-art approaches for accurate visual localization use
scene-specific representations, resulting in the overhead of constructing these
models when applying the techniques to new scenes. Recently, deep
learning-based approaches based on relative pose estimation have been proposed,
carrying the promise of easily adapting to new scenes. However, it has been
shown such approaches are currently significantly less accurate than
state-of-the-art approaches. In this paper, we are interested in analyzing this
behavior. To this end, we propose a novel framework for visual localization
from relative poses. Using a classical feature-based approach within this
framework, we show state-of-the-art performance. Replacing the classical
approach with learned alternatives at various levels, we then identify the
reasons for why deep learned approaches do not perform well. Based on our
analysis, we make recommendations for future work.","['Qunjie Zhou', 'Torsten Sattler', 'Marc Pollefeys', 'Laura Leal-Taixe']",2019-08-04T08:23:44Z,http://arxiv.org/abs/1908.01293v2,['cs.CV']
"Fast Adaptation with Meta-Reinforcement Learning for Trust Modelling in
  Human-Robot Interaction","In socially assistive robotics, an important research area is the development
of adaptation techniques and their effect on human-robot interaction. We
present a meta-learning based policy gradient method for addressing the problem
of adaptation in human-robot interaction and also investigate its role as a
mechanism for trust modelling. By building an escape room scenario in mixed
reality with a robot, we test our hypothesis that bi-directional trust can be
influenced by different adaptation algorithms. We found that our proposed model
increased the perceived trustworthiness of the robot and influenced the
dynamics of gaining human's trust. Additionally, participants evaluated that
the robot perceived them as more trustworthy during the interactions with the
meta-learning based adaptation compared to the previously studied statistical
adaptation model.","['Yuan Gao', 'Elena Sibirtseva', 'Ginevra Castellano', 'Danica Kragic']",2019-08-12T11:06:28Z,http://arxiv.org/abs/1908.04087v1,"['cs.RO', 'cs.HC', 'cs.LG']"
"EyeNet: A Multi-Task Network for Off-Axis Eye Gaze Estimation and User
  Understanding","Eye gaze estimation and simultaneous semantic understanding of a user through
eye images is a crucial component in Virtual and Mixed Reality; enabling energy
efficient rendering, multi-focal displays and effective interaction with 3D
content. In head-mounted VR/MR devices the eyes are imaged off-axis to avoid
blocking the user's gaze, this view-point makes drawing eye related inferences
very challenging. In this work, we present EyeNet, the first single deep neural
network which solves multiple heterogeneous tasks related to eye gaze
estimation and semantic user understanding for an off-axis camera setting. The
tasks include eye segmentation, blink detection, emotive expression
classification, IR LED glints detection, pupil and cornea center estimation. To
train EyeNet end-to-end we employ both hand labelled supervision and model
based supervision. We benchmark all tasks on MagicEyes, a large and new dataset
of 587 subjects with varying morphology, gender, skin-color, make-up and
imaging conditions.","['Zhengyang Wu', 'Srivignesh Rajendran', 'Tarrence van As', 'Joelle Zimmermann', 'Vijay Badrinarayanan', 'Andrew Rabinovich']",2019-08-24T00:47:39Z,http://arxiv.org/abs/1908.09060v1,"['cs.CV', 'cs.LG', 'eess.IV']"
"Spatial Data Science: Closing the human-spatial computing-environment
  loop","Over the last decade, the term spatial computing has grown to have two
different, though not entirely unrelated, definitions. The first definition of
spatial computing stems from industry, where it refers primarily to new kinds
of augmented, virtual, mixed-reality, and natural user interface technologies.
A second definition coming out of academia takes a broader perspective that
includes active research in geographic information science as well as the
aforementioned novel UI technologies. Both senses reflect an ongoing shift
toward increased interaction with computing interfaces and sensors embedded in
the environment and how the use of these technologies influence how we behave
and make sense of and even change the world we live in. Regardless of the
definition, research in spatial computing is humming along nicely without the
need to identify new research agendas or new labels for communities of
researchers. However, as a field of research, it could be helpful to view
spatial data science as the glue that coheres spatial computing with
problem-solving and learning in the real world into a more holistic discipline.",['Benjamin Adams'],2019-10-15T02:19:19Z,http://arxiv.org/abs/1910.06484v1,"['cs.SI', 'cs.CY']"
C-D Ratio in multi-display environments,"Research in user interaction with mixed reality environments using multiple
displays has become increasingly relevant with the prevalence of mobile devices
in everyday life and increased commoditization of large display area
technologies using projectors or large displays. Previous work often combines
touch-based input with other approaches, such as gesture-based input, to expand
the possible interaction space or deal with limitations of other
two-dimensional input methods. In contrast to previous methods, we examine the
possibilities when the control-display (C-D) ratio is significantly smaller
than one and small input movements result in large output movements. To this
end one specific multi-display configuration is implemented in the form of a
spatial-augmented reality sandbox environment, and used to explore various
interaction techniques based on a variety of mobile device touch-based input
and optical marker tracking-based finger input. A small pilot study determines
the most promising input candidate, which is compared to traditional
touch-input based techniques in a user study that tests it for practical
relevance. Results and conclusions of the study are presented.","['Travis Gesslein', 'Jens Grubert']",2020-02-12T13:37:24Z,http://arxiv.org/abs/2002.04980v1,['cs.HC']
"A New Exocentric Metaphor for Complex Path Following to Control a UAV
  Using Mixed Reality","Teleoperation of Unmanned Aerial Vehicles (UAVs) has recently become an
noteworthly research topic in the field of human robot interaction. Each year,
a variety of devices is being studied to design adapted interface for diverse
purpose such as view taking, search and rescue operation or suveillance. New
interfaces have to be precise, simple and intuitive even for complex path
planning. Moreover, when teleoperation involves long distance control, user
needs to get proper feedbacks and avoid motion sickness. In order to overcome
all these challenges, a new interaction metaphor named DrEAM (Drone Exocentric
Advanced Metaphor) was designed. User can see the UAV he is controlling in a
virtual environment mapped to the real world. He can interact with it as a
simple object in a classical virtual world. An experiment was lead in order to
evaluate the perfomances of this metaphor, comparing performance of novice user
using either a direct-view joystick control or using DrEAM.","['Baptiste Wojtkowski', 'Pedro Castillo', 'Indira Thouvenin']",2020-02-13T11:02:33Z,http://arxiv.org/abs/2002.05721v1,"['cs.HC', 'cs.GR', 'cs.RO']"
Egocentric Human Segmentation for Mixed Reality,"The objective of this work is to segment human body parts from egocentric
video using semantic segmentation networks. Our contribution is two-fold: i) we
create a semi-synthetic dataset composed of more than 15, 000 realistic images
and associated pixel-wise labels of egocentric human body parts, such as arms
or legs including different demographic factors; ii) building upon the
ThunderNet architecture, we implement a deep learning semantic segmentation
algorithm that is able to perform beyond real-time requirements (16 ms for 720
x 720 images). It is believed that this method will enhance sense of presence
of Virtual Environments and will constitute a more realistic solution to the
standard virtual avatars.","['Andrija Gajic', 'Ester Gonzalez-Sosa', 'Diego Gonzalez-Morin', 'Marcos Escudero-Viñolo', 'Alvaro Villegas']",2020-05-25T12:34:47Z,http://arxiv.org/abs/2005.12074v2,['cs.CV']
Improved Deep Point Cloud Geometry Compression,"Point clouds have been recognized as a crucial data structure for 3D content
and are essential in a number of applications such as virtual and mixed
reality, autonomous driving, cultural heritage, etc. In this paper, we propose
a set of contributions to improve deep point cloud compression, i.e.: using a
scale hyperprior model for entropy coding; employing deeper transforms; a
different balancing weight in the focal loss; optimal thresholding for
decoding; and sequential model training. In addition, we present an extensive
ablation study on the impact of each of these factors, in order to provide a
better understanding about why they improve RD performance. An optimal
combination of the proposed improvements achieves BD-PSNR gains over G-PCC
trisoup and octree of 5.50 (6.48) dB and 6.84 (5.95) dB, respectively, when
using the point-to-point (point-to-plane) metric. Code is available at
https://github.com/mauriceqch/pcc_geo_cnn_v2 .","['Maurice Quach', 'Giuseppe Valenzise', 'Frederic Dufaux']",2020-06-16T10:03:14Z,http://arxiv.org/abs/2006.09043v2,"['cs.CV', 'cs.LG', 'eess.IV', 'eess.SP', 'stat.ML']"
Structure and Design of HoloGen,"Increasing popularity of augmented and mixed reality systems has seen a
similar increase of interest in 2D and 3D computer generated holography (CGH).
Unlike stereoscopic approaches, CGH can fully represent a light field including
depth of focus, accommodation and vergence. Along with existing
telecommunications, imaging, projection, lithography, beam shaping and optical
tweezing applications, CGH is an exciting technique applicable to a wide array
of photonic problems including full 3D representation. Traditionally, the
primary roadblock to acceptance has been the significant numerical processing
required to generate holograms requiring both significant expertise and
significant computational power. This article discusses the structure and
design of HoloGen. HoloGen is an MIT licensed application that may be used to
generate holograms using a wide array of algorithms without expert guidance.
HoloGen uses a Cuda C and C++ backend with a C# and Windows Presentation
Framework graphical user interface. The article begins by introducing HoloGen
before providing an in-depth discussion of its design and structure. Particular
focus is given to the communication, data transfer and algorithmic aspects.","['Peter J. Christopher', 'Timothy D. Wilkinson']",2020-06-18T13:29:46Z,http://arxiv.org/abs/2006.10509v1,"['cs.GR', 'cs.CV']"
An Advert Creation System for 3D Product Placements,"Over the past decade, the evolution of video-sharing platforms has attracted
a significant amount of investments on contextual advertising. The common
contextual advertising platforms utilize the information provided by users to
integrate 2D visual ads into videos. The existing platforms face many technical
challenges such as ad integration with respect to occluding objects and 3D ad
placement. This paper presents a Video Advertisement Placement & Integration
(Adverts) framework, which is capable of perceiving the 3D geometry of the
scene and camera motion to blend 3D virtual objects in videos and create the
illusion of reality. The proposed framework contains several modules such as
monocular depth estimation, object segmentation, background-foreground
separation, alpha matting and camera tracking. Our experiments conducted using
Adverts framework indicates the significant potential of this system in
contextual ad integration, and pushing the limits of advertising industry using
mixed reality technologies.","['Ivan Bacher', 'Hossein Javidnia', 'Soumyabrata Dev', 'Rahul Agrahari', 'Murhaf Hossari', 'Matthew Nicholson', 'Clare Conran', 'Jian Tang', 'Peng Song', 'David Corrigan', 'François Pitié']",2020-06-26T17:41:50Z,http://arxiv.org/abs/2006.15131v1,"['cs.CV', 'cs.MM']"
The Phong Surface: Efficient 3D Model Fitting using Lifted Optimization,"Realtime perceptual and interaction capabilities in mixed reality require a
range of 3D tracking problems to be solved at low latency on
resource-constrained hardware such as head-mounted devices. Indeed, for devices
such as HoloLens 2 where the CPU and GPU are left available for applications,
multiple tracking subsystems are required to run on a continuous, real-time
basis while sharing a single Digital Signal Processor. To solve model-fitting
problems for HoloLens 2 hand tracking, where the computational budget is
approximately 100 times smaller than an iPhone 7, we introduce a new surface
model: the `Phong surface'. Using ideas from computer graphics, the Phong
surface describes the same 3D shape as a triangulated mesh model, but with
continuous surface normals which enable the use of lifting-based optimization,
providing significant efficiency gains over ICP-based methods. We show that
Phong surfaces retain the convergence benefits of smoother surface models,
while triangle meshes do not.","['Jingjing Shen', 'Thomas J. Cashman', 'Qi Ye', 'Tim Hutton', 'Toby Sharp', 'Federica Bogo', 'Andrew William Fitzgibbon', 'Jamie Shotton']",2020-07-09T17:10:11Z,http://arxiv.org/abs/2007.04940v1,['cs.CV']
"XR-Ed Framework: Designing Instruction-driven andLearner-centered
  Extended Reality Systems for Education","Recently, the HCI community has seen an increased interest in applying
Virtual Reality (VR), AugmentedReality (AR) and Mixed Reality (MR) into
educational settings. Despite many literature reviews, there stilllacks a clear
framework that reveals the different design dimensions in educational Extended
Reality (XR)systems. Addressing this gap, we synthesize a broad range of
educational XR to propose the XR-Ed framework,which reveals design space in six
dimensions (Physical Accessibility, Scenario, Social Interactivity,
Agency,Virtuality Degree, Assessment). Within each dimension, we contextualize
the framework using existing designcases. Based on the XR-Ed Design framework,
we incorporated instructional design approaches to proposeXR-Ins, an
instruction-oriented, step-by-step guideline in educational XR instruction
design. Jointly, they aimto support practitioners by revealing implicit design
choices, offering design inspirations as well as guide themto design
instructional activities for XR technologies in a more instruction-oriented and
learner-centered way.","['Kexin Yang', 'Xiaofei Zhou', 'Iulian Radu']",2020-10-24T03:18:05Z,http://arxiv.org/abs/2010.13779v1,['cs.HC']
Efficient Scene Compression for Visual-based Localization,"Estimating the pose of a camera with respect to a 3D reconstruction or scene
representation is a crucial step for many mixed reality and robotics
applications. Given the vast amount of available data nowadays, many
applications constrain storage and/or bandwidth to work efficiently. To satisfy
these constraints, many applications compress a scene representation by
reducing its number of 3D points. While state-of-the-art methods use
$K$-cover-based algorithms to compress a scene, they are slow and hard to tune.
To enhance speed and facilitate parameter tuning, this work introduces a novel
approach that compresses a scene representation by means of a constrained
quadratic program (QP). Because this QP resembles a one-class support vector
machine, we derive a variant of the sequential minimal optimization to solve
it. Our approach uses the points corresponding to the support vectors as the
subset of points to represent a scene. We also present an efficient
initialization method that allows our method to converge quickly. Our
experiments on publicly available datasets show that our approach compresses a
scene representation quickly while delivering accurate pose estimates.","['Marcela Mera-Trujillo', 'Benjamin Smith', 'Victor Fragoso']",2020-11-27T18:36:06Z,http://arxiv.org/abs/2011.13894v1,['cs.CV']
"Sonic Sculpture: Activating Engagement with Head-Mounted Augmented
  Reality","This work examines how head-mounted AR can be used to build an interactive
sonic landscape to engage with a public sculpture. We describe a sonic artwork,
""Listening To Listening"", that has been designed to accompany a real-world
sculpture with two prototype interaction schemes. Our artwork is created for
the HoloLens platform so that users can have an individual experience in a
mixed reality context. Personal head-mounted AR systems have recently become
available and practical for integration into public art projects, however
research into sonic sculpture works has yet to account for the affordances of
current portable and mainstream AR systems. In this work, we take advantage of
the HoloLens' spatial awareness to build sonic spaces that have a precise
spatial relationship to a given sculpture and where the sculpture itself is
modelled in the augmented scene as an ""invisible hologram"". We describe the
artistic rationale for our artwork, the design of the two interaction schemes,
and the technical and usability feedback that we have obtained from
demonstrations during iterative development.","['Charles Patrick Martin', 'Zeruo Liu', 'Yichen Wang', 'Wennan He', 'Henry Gardner']",2020-12-03T22:35:49Z,http://arxiv.org/abs/2012.02311v1,"['cs.HC', 'cs.SD', 'eess.AS', 'H.5.5; H.5.1']"
"No Shadow Left Behind: Removing Objects and their Shadows using
  Approximate Lighting and Geometry","Removing objects from images is a challenging problem that is important for
many applications, including mixed reality. For believable results, the shadows
that the object casts should also be removed. Current inpainting-based methods
only remove the object itself, leaving shadows behind, or at best require
specifying shadow regions to inpaint. We introduce a deep learning pipeline for
removing a shadow along with its caster. We leverage rough scene models in
order to remove a wide variety of shadows (hard or soft, dark or subtle, large
or thin) from surfaces with a wide variety of textures. We train our pipeline
on synthetically rendered data, and show qualitative and quantitative results
on both synthetic and real scenes.","['Edward Zhang', 'Ricardo Martin-Brualla', 'Janne Kontkanen', 'Brian Curless']",2020-12-19T01:05:40Z,http://arxiv.org/abs/2012.10565v1,"['cs.CV', 'cs.GR']"
Analysing ocular parameters for web browsing and graph visualization,"This paper proposes a set of techniques to investigate eye gaze and fixation
patterns while users interact with electronic user interfaces. In particular,
two case studies are presented - one on analysing eye gaze while interacting
with deceptive materials in web pages and another on analysing graphs in
standard computer monitor and virtual reality displays. We analysed spatial and
temporal distributions of eye gaze fixations and sequence of eye gaze
movements. We used this information to propose new design guidelines to avoid
deceptive materials in web and user-friendly representation of data in 2D
graphs. In 2D graph study we identified that area graph has lowest number of
clusters for user's gaze fixations and lowest average response time. The
results of 2D graph study were implemented in virtual and mixed reality
environment. Along with this, it was ob-served that the duration while
interacting with deceptive materials in web pages is independent of the number
of fixations. Furthermore, web-based data visualization tool for analysing eye
tracking data from single and multiple users was developed.","['Somnath Arjun', 'KamalPreet Singh Saluja', 'Pradipta Biswas']",2021-01-04T06:20:02Z,http://arxiv.org/abs/2101.00794v1,"['cs.HC', 'D.2.2; H.1.2; I.3.6']"
Discussing the Risks of Adaptive Virtual Environments for User Autonomy,"Adaptive virtual environments are an opportunity to support users and
increase their flow, presence, immersion, and overall experience. Possible
fields of application are adaptive individual education, gameplay adjustment,
professional work, and personalized content. But who benefits more from this
adaptivity, the users who can enjoy a greater user experience or the companies
or governments who are completely in control of the provided content. While the
user autonomy decreases for individuals, the power of institutions raises, and
the risk exists that personal opinions are precisely controlled. In this
position paper, we will argue that researchers should not only propose the
benefits of their work but also critically discuss what are possible abusive
use cases. Therefore, we will examine two use cases in the fields of
professional work and personalized content and show possible abusive use.","['Tobias Drey', 'Enrico Rukzio']",2021-01-07T14:55:09Z,http://arxiv.org/abs/2101.02576v1,['cs.HC']
"Color Contrast Enhanced Rendering for Optical See-through Head-mounted
  Displays","Most commercially available optical see-through head-mounted displays
(OST-HMDs) utilize optical combiners to simultaneously visualize the physical
background and virtual objects. The displayed images perceived by users are a
blend of rendered pixels and background colors. Enabling high fidelity color
perception in mixed reality (MR) scenarios using OST-HMDs is an important but
challenging task. We propose a real-time rendering scheme to enhance the color
contrast between virtual objects and the surrounding background for OST-HMDs.
Inspired by the discovery of color perception in psychophysics, we first
formulate the color contrast enhancement as a constrained optimization problem.
We then design an end-to-end algorithm to search the optimal complementary
shift in both chromaticity and luminance of the displayed color. This aims at
enhancing the contrast between virtual objects and the real background as well
as keeping the consistency with the original color. We assess the performance
of our approach using a simulated OST-HMD environment and an off-the-shelf
OST-HMD. Experimental results from objective evaluations and subjective user
studies demonstrate that the proposed approach makes rendered virtual objects
more distinguishable from the surrounding background, thereby bringing a better
visual experience.","['Yunjin Zhang', 'Rui Wang', 'Yifan', 'Peng', 'Wei Hua', 'Hujun Bao']",2021-01-08T04:42:39Z,http://arxiv.org/abs/2101.02847v1,['cs.GR']
Adaptive Accessible AR/VR Systems,"Augmented, virtual and mixed reality technologies offer new ways of
interacting with digital media. However, such technologies are not well
explored for people with different ranges of abilities beyond a few specific
navigation and gaming applications. While new standardization activities are
investigating accessibility issues with existing AR/VR systems, commercial
systems are still confined to specialized hardware and software limiting their
widespread adoption among people with disabilities as well as seniors. This
proposal takes a novel approach by exploring the application of user
model-based personalization for AR/VR systems to improve accessibility. The
workshop will be organized by experienced researchers in the field of human
computer interaction, robotics control, assistive technology, and AR/VR
systems, and will consist of peer reviewed papers and hands-on demonstrations.
Keynote speeches and demonstrations will cover latest accessibility research at
Microsoft, Google, Verizon and leading universities.","['Pradipta Biswas', 'Pilar Orero', 'Manohar Swaminathan', 'Kavita Krishnaswamy', 'Peter Robinson']",2021-01-08T10:01:21Z,http://arxiv.org/abs/2101.02936v1,"['cs.HC', 'D.2.2; H.1.2; I.3.6']"
"Streaming VR Games to the Broad Audience: A Comparison of the
  First-Person and Third-Person Perspectives","The spectatorship experience for virtual reality (VR) games differs strongly
from its non-VR precursor. When watching non-VR games on platforms such as
Twitch, spectators just see what the player sees, as the physical interaction
is mostly unimportant for the overall impression. In VR, the immersive
full-body interaction is a crucial part of the player experience. Hence,
content creators, such as streamers, often rely on green screens or similar
solutions to offer a mixed-reality third-person view to disclose their
full-body actions. Our work compares the most popular realizations of the
first-person and the third-person perspective in an online survey (N=217) with
three different VR games. Contrary to the current trend to stream in
third-person, our key result is that most viewers prefer the first-person
version, which they attribute mostly to the better focus on in-game actions and
higher involvement. Based on the study insights, we provide design
recommendations for both perspectives.","['Katharina Emmerich', 'Andrey Krekhov', 'Sebastian Cmentowski', 'Jens Krueger']",2021-01-12T12:46:04Z,http://arxiv.org/abs/2101.04449v1,['cs.HC']
HAIR: Head-mounted AR Intention Recognition,"Human teams exhibit both implicit and explicit intention sharing. To further
development of human-robot collaboration, intention recognition is crucial on
both sides. Present approaches rely on a vast sensor suite on and around the
robot to achieve intention recognition. This relegates intuitive human-robot
collaboration purely to such bulky systems, which are inadequate for
large-scale, real-world scenarios due to their complexity and cost. In this
paper we propose an intention recognition system that is based purely on a
portable head-mounted display. In addition robot intention visualisation is
also supported. We present experiments to show the quality of our human goal
estimation component and some basic interactions with an industrial robot. HAIR
should raise the quality of interaction between robots and humans, instead of
such interactions raising the hair on the necks of the human coworkers.","['David Puljiz', 'Bowen Zhou', 'Ke Ma', 'Björn Hein']",2021-02-22T16:38:22Z,http://arxiv.org/abs/2102.11162v1,"['cs.RO', 'cs.HC']"
"Text-driven object affordance for guiding grasp-type recognition in
  multimodal robot teaching","This study investigates how text-driven object affordance, which provides
prior knowledge about grasp types for each object, affects image-based
grasp-type recognition in robot teaching. The researchers created labeled
datasets of first-person hand images to examine the impact of object affordance
on recognition performance. They evaluated scenarios with real and illusory
objects, considering mixed reality teaching conditions where visual object
information may be limited. The results demonstrate that object affordance
improves image-based recognition by filtering out unlikely grasp types and
emphasizing likely ones. The effectiveness of object affordance was more
pronounced when there was a stronger bias towards specific grasp types for each
object. These findings highlight the significance of object affordance in
multimodal robot teaching, regardless of whether real objects are present in
the images. Sample code is available on
https://github.com/microsoft/arr-grasp-type-recognition.","['Naoki Wake', 'Daichi Saito', 'Kazuhiro Sasabuchi', 'Hideki Koike', 'Katsushi Ikeuchi']",2021-02-27T17:03:32Z,http://arxiv.org/abs/2103.00268v2,"['cs.RO', 'cs.CV', 'cs.HC']"
A Full Body Avatar-Based Telepresence System for Dissimilar Spaces,"We present a novel mixed reality (MR) telepresence system enabling a local
user to interact with a remote user through full-body avatars in their own
rooms. If the remote rooms have different sizes and furniture arrangements,
directly applying a user's motion to an avatar leads to a mismatch of placement
and deictic gesture. To overcome this problem, we retarget the placement, arm
gesture, and head movement of a local user to an avatar in a remote room to
preserve a local user's environment and interaction context. This allows
avatars to utilize real furniture and interact with a local user and shared
objects as if they were in the same room. This paper describes our system's
design and implementation in detail and a set of example scenarios in the
living room and office room. A qualitative user study delves into a user
experience, challenges, and possible extensions of the proposed system.","['Leonard Yoon', 'Dongseok Yang', 'Choongho Chung', 'Sung-Hee Lee']",2021-03-07T15:43:57Z,http://arxiv.org/abs/2103.04380v1,['cs.HC']
FastNeRF: High-Fidelity Neural Rendering at 200FPS,"Recent work on Neural Radiance Fields (NeRF) showed how neural networks can
be used to encode complex 3D environments that can be rendered
photorealistically from novel viewpoints. Rendering these images is very
computationally demanding and recent improvements are still a long way from
enabling interactive rates, even on high-end hardware. Motivated by scenarios
on mobile and mixed reality devices, we propose FastNeRF, the first NeRF-based
system capable of rendering high fidelity photorealistic images at 200Hz on a
high-end consumer GPU. The core of our method is a graphics-inspired
factorization that allows for (i) compactly caching a deep radiance map at each
position in space, (ii) efficiently querying that map using ray directions to
estimate the pixel values in the rendered image. Extensive experiments show
that the proposed method is 3000 times faster than the original NeRF algorithm
and at least an order of magnitude faster than existing work on accelerating
NeRF, while maintaining visual quality and extensibility.","['Stephan J. Garbin', 'Marek Kowalski', 'Matthew Johnson', 'Jamie Shotton', 'Julien Valentin']",2021-03-18T17:09:12Z,http://arxiv.org/abs/2103.10380v2,['cs.CV']
"Congruence and Plausibility, not Presence?! Pivotal Conditions for XR
  Experiences and Effects, a Novel Model","Presence often is considered the most important quale describing the
subjective feeling of being in a computer-generated and/or computer-mediated
virtual environment. The identification and separation of orthogonal presence
components, i.e., the place illusion and the plausibility illusion, has been an
accepted theoretical model describing Virtual Reality (VR) experiences for some
time. This perspective article challenges this presence-oriented VR theory.
First, we argue that a place illusion cannot be the major construct to describe
the much wider scope of Virtual, Augmented, and Mixed Reality (VR, AR, MR: or
XR for short). Second, we argue that there is no plausibility illusion but
merely plausibility, and we derive the place illusion caused by congruent and
plausible generation of spatial cues, and similarly for all the current model's
so-defined illusions. Finally, we propose congruence and plausibility to become
the central essential conditions in a novel theoretical model describing XR
experiences and effects.","['Marc Erich Latoschik', 'Carolin Wienrich']",2021-04-10T19:25:17Z,http://arxiv.org/abs/2104.04846v5,"['cs.HC', 'H.5.1']"
Implementing Virtual Reality for Teleoperation of a Humanoid Robot,"Our research explores the potential of a humanoid robot for work in
unpredictable environments, but controlling a humanoid robot remains a very
difficult problem. In our previous work, we designed a prototype virtual
reality (VR) interface to allow an operator to command a humanoid robot.
However, while usable, the initial interface was not sufficient for commanding
the robot to perform the tasks; for example, in some cases, there was a lack of
precision available for robot control. The interface was overly cumbersome in
some areas as well. In this paper, we discuss numerous additions, inspired by
traditional interfaces and virtual reality video games, to our prior
implementation, providing additional ways to visualize and command a humanoid
robot to perform difficult tasks within a virtual world.","['Jordan Allspaw', 'Gregory LeMasurier', 'Holly Yanco']",2021-04-23T21:44:25Z,http://arxiv.org/abs/2104.11826v1,"['cs.RO', 'I.2.9']"
Semi-Autonomous Planning and Visualization in Virtual Reality,"Virtual reality (VR) interfaces for robots provide a three-dimensional (3D)
view of the robot in its environment, which allows people to better plan
complex robot movements in tight or cluttered spaces. In our prior work, we
created a VR interface to allow for the teleoperation of a humanoid robot. As
detailed in this paper, we have now focused on a human-in-the-loop planner
where the operator can send higher level manipulation and navigation goals in
VR through functional waypoints, visualize the results of a robot planner in
the 3D virtual space, and then deny, alter or confirm the plan to send to the
robot. In addition, we have adapted our interface to also work for a mobile
manipulation robot in addition to the humanoid robot. For a video demonstration
please see the accompanying video at https://youtu.be/wEHZug_fxrA.","['Gregory LeMasurier', 'Jordan Allspaw', 'Holly A. Yanco']",2021-04-23T21:48:05Z,http://arxiv.org/abs/2104.11827v1,"['cs.RO', 'I.2.9']"
"Procedural animations in interactive art experiences -- A state of the
  art review","The state of the art review broadly oversees the use of novel research
utilized in the creation of virtual environments applied in interactive art
experiences, with a specific focus on the application of procedural animation
in spatially augmented reality (SAR) exhibitions. These art exhibitions
frequently combine sensory displays that appeal, replace, and augment the
visual, auditory and touch or haptic senses. We analyze and break down
art-technology related innovations in the last three years, and thoroughly
identify the most recent and vibrant applications of interactive art
experiences in the review of numerous installation applications, studies, and
events. Display mediums such as virtual reality, augmented reality, mixed
reality, and robotics are overviewed in the context of art experiences such as
visual art museums, park or historic site tours, live concerts, and theatre. We
explore research and extrapolate how recent innovations can lead to different
applications that will be seen in the future.",['C. Tollola'],2021-05-16T05:14:56Z,http://arxiv.org/abs/2105.09153v1,"['cs.HC', 'cs.MM']"
"CLEDGE: A Hybrid Cloud-Edge Computing Framework over Information Centric
  Networking","In today's era of Internet of Things (IoT), where massive amounts of data are
produced by IoT and other devices, edge computing has emerged as a prominent
paradigm for low-latency data processing. However, applications may have
diverse latency requirements: certain latency-sensitive processing operations
may need to be performed at the edge, while delay-tolerant operations can be
performed on the cloud, without occupying the potentially limited edge
computing resources. To achieve that, we envision an environment where
computing resources are distributed across edge and cloud offerings. In this
paper, we present the design of CLEDGE (CLoud + EDGE), an information-centric
hybrid cloud-edge framework, aiming to maximize the on-time completion of
computational tasks offloaded by applications with diverse latency
requirements. The design of CLEDGE is motivated by the networking challenges
that mixed reality researchers face. Our evaluation demonstrates that CLEDGE
can complete on-time more than 90% of offloaded tasks with modest overheads.","['Md Washik Al Azad', 'Susmit Shannigrahi', 'Nicholas Stergiou', 'Francisco R. Ortega', 'Spyridon Mastorakis']",2021-07-15T20:52:37Z,http://arxiv.org/abs/2107.07604v1,['cs.NI']
"Physics-informed neural networks for one-dimensional sound field
  predictions with parameterized sources and impedance boundaries","Realistic sound is essential in virtual environments, such as computer games
and mixed reality. Efficient and accurate numerical methods for pre-calculating
acoustics have been developed over the last decade; however, pre-calculating
acoustics makes handling dynamic scenes with moving sources challenging,
requiring intractable memory storage. A physics-informed neural network (PINN)
method in 1D is presented, which learns a compact and efficient surrogate model
with parameterized moving Gaussian sources and impedance boundaries, and
satisfies a system of coupled equations. The model shows relative mean errors
below 2%/0.2 dB and proposes a first step in developing PINNs for realistic 3D
scenes.","['Nikolas Borrel-Jensen', 'Allan P. Engsig-Karup', 'Cheol-Ho Jeong']",2021-09-23T11:59:26Z,http://arxiv.org/abs/2109.11313v5,"['cs.SD', 'eess.AS', 'physics.comp-ph']"
"CAESynth: Real-Time Timbre Interpolation and Pitch Control with
  Conditional Autoencoders","In this paper, we present a novel audio synthesizer, CAESynth, based on a
conditional autoencoder. CAESynth synthesizes timbre in real-time by
interpolating the reference sounds in their shared latent feature space, while
controlling a pitch independently. We show that training a conditional
autoencoder based on accuracy in timbre classification together with
adversarial regularization of pitch content allows timbre distribution in
latent space to be more effective and stable for timbre interpolation and pitch
conditioning. The proposed method is applicable not only to creation of musical
cues but also to exploration of audio affordance in mixed reality based on
novel timbre mixtures with environmental sounds. We demonstrate by experiments
that CAESynth achieves smooth and high-fidelity audio synthesis in real-time
through timbre interpolation and independent yet accurate pitch control for
musical cues as well as for audio affordance with environmental sound. A Python
implementation along with some generated samples are shared online.","['Aaron Valero Puche', 'Sukhan Lee']",2021-11-09T14:36:31Z,http://arxiv.org/abs/2111.05174v1,"['cs.SD', 'cs.LG', 'eess.AS']"
Ditto: Building Digital Twins of Articulated Objects from Interaction,"Digitizing physical objects into the virtual world has the potential to
unlock new research and applications in embodied AI and mixed reality. This
work focuses on recreating interactive digital twins of real-world articulated
objects, which can be directly imported into virtual environments. We introduce
Ditto to learn articulation model estimation and 3D geometry reconstruction of
an articulated object through interactive perception. Given a pair of visual
observations of an articulated object before and after interaction, Ditto
reconstructs part-level geometry and estimates the articulation model of the
object. We employ implicit neural representations for joint geometry and
articulation modeling. Our experiments show that Ditto effectively builds
digital twins of articulated objects in a category-agnostic way. We also apply
Ditto to real-world objects and deploy the recreated digital twins in physical
simulation. Code and additional results are available at
https://ut-austin-rpl.github.io/Ditto","['Zhenyu Jiang', 'Cheng-Chun Hsu', 'Yuke Zhu']",2022-02-16T18:12:14Z,http://arxiv.org/abs/2202.08227v3,"['cs.CV', 'cs.AI', 'cs.RO']"
"Dynamic Object Comprehension: A Framework For Evaluating Artificial
  Visual Perception","Augmented and Mixed Reality are emerging as likely successors to the mobile
internet. However, many technical challenges remain. One of the key
requirements of these systems is the ability to create a continuity between
physical and virtual worlds, with the user's visual perception as the primary
interface medium. Building this continuity requires the system to develop a
visual understanding of the physical world. While there has been significant
recent progress in computer vision and AI techniques such as image
classification and object detection, success in these areas has not yet led to
the visual perception required for these critical MR and AR applications. A
significant issue is that current evaluation criteria are insufficient for
these applications. To motivate and evaluate progress in this emerging area,
there is a need for new metrics. In this paper we outline limitations of
current evaluation criteria and propose new criteria.","['Scott Y. L. Chin', 'Bradley R. Quinton']",2022-02-17T07:49:49Z,http://arxiv.org/abs/2202.08490v1,"['cs.CV', 'cs.AI', 'cs.LG']"
"Virtual Reality Digital Twin and Environment for Troubleshooting
  Lunar-based Infrastructure Assembly Failures","Humans and robots will need to collaborate in order to create a sustainable
human lunar presence by the end of the 2020s. This includes cases in which a
human will be required to teleoperate an autonomous rover that has encountered
an instrument assembly failure. To aid teleoperators in the troubleshooting
process, we propose a virtual reality digital twin placed in a simulated
environment. Here, the operator can virtually interact with a digital version
of the rover and mechanical arm that uses the same controls and kinematic
model. The user can also adopt the egocentric (a first person view through
using stereoscopic passthrough) and exocentric (a third person view where the
operator can virtually walk around the environment and rover as if they were on
site) view. We also discuss our metrics for evaluating the differences between
our digital and physical robot, as well as the experimental concept based on
real and applicable missions, and future work that would compare our platform
to traditional troubleshooting methods.","['Phaedra S. Curlin', 'Madaline A. Muniz', 'Mason M. Bell', 'Alexis A. Muniz', 'Jack O. Burns']",2022-03-05T19:36:16Z,http://arxiv.org/abs/2203.02810v1,['cs.RO']
"Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced
  Human-Robot Interaction and Robotic Interfaces","This paper contributes to a taxonomy of augmented reality and robotics based
on a survey of 460 research papers. Augmented and mixed reality (AR/MR) have
emerged as a new way to enhance human-robot interaction (HRI) and robotic
interfaces (e.g., actuated and shape-changing interfaces). Recently, an
increasing number of studies in HCI, HRI, and robotics have demonstrated how AR
enables better interactions between people and robots. However, often research
remains focused on individual explorations and key design strategies, and
research questions are rarely analyzed systematically. In this paper, we
synthesize and categorize this research field in the following dimensions: 1)
approaches to augmenting reality; 2) characteristics of robots; 3) purposes and
benefits; 4) classification of presented information; 5) design components and
strategies for visual augmentation; 6) interaction techniques and modalities;
7) application domains; and 8) evaluation strategies. We formulate key
challenges and opportunities to guide and inform future research in AR and
robotics.","['Ryo Suzuki', 'Adnan Karim', 'Tian Xia', 'Hooman Hedayati', 'Nicolai Marquardt']",2022-03-07T10:22:59Z,http://arxiv.org/abs/2203.03254v1,"['cs.RO', 'cs.CV', 'cs.HC']"
FLAG: Flow-based 3D Avatar Generation from Sparse Observations,"To represent people in mixed reality applications for collaboration and
communication, we need to generate realistic and faithful avatar poses.
However, the signal streams that can be applied for this task from head-mounted
devices (HMDs) are typically limited to head pose and hand pose estimates.
While these signals are valuable, they are an incomplete representation of the
human body, making it challenging to generate a faithful full-body avatar. We
address this challenge by developing a flow-based generative model of the 3D
human body from sparse observations, wherein we learn not only a conditional
distribution of 3D human pose, but also a probabilistic mapping from
observations to the latent space from which we can generate a plausible pose
along with uncertainty estimates for the joints. We show that our approach is
not only a strong predictive model, but can also act as an efficient pose prior
in different optimization settings where a good initial latent code plays a
major role.","['Sadegh Aliakbarian', 'Pashmina Cameron', 'Federica Bogo', 'Andrew Fitzgibbon', 'Thomas J. Cashman']",2022-03-11T08:07:09Z,http://arxiv.org/abs/2203.05789v1,"['cs.CV', 'cs.LG']"
"Re-shaping Post-COVID-19 Teaching and Learning: A Blueprint of
  Virtual-Physical Blended Classrooms in the Metaverse Era","During the COVID-19 pandemic, most countries have experienced some form of
remote education through video conferencing software platforms. However, these
software platforms fail to reduce immersion and replicate the classroom
experience. The currently emerging Metaverse addresses many of such limitations
by offering blended physical-digital environments. This paper aims to assess
how the Metaverse can support and improve e-learning. We first survey the
latest applications of blended environments in education and highlight the
primary challenges and opportunities. Accordingly, we derive our proposal for a
virtual-physical blended classroom configuration that brings students and
teachers into a shared educational Metaverse. We focus on the system
architecture of the Metaverse classroom to achieve real-time synchronization of
a large number of participants and activities across physical (mixed reality
classrooms) and virtual (remote VR platform) learning spaces. Our proposal
attempts to transform the traditional physical classroom into virtual-physical
cyberspace as a new social network of learners and educators connected at an
unprecedented scale.","['Yuyang Wang', 'Lik-Hang Lee', 'Tristan Braud', 'Pan Hui']",2022-03-17T10:35:45Z,http://arxiv.org/abs/2203.09228v3,"['cs.HC', 'cs.MM', 'K.3; I.3']"
BNV-Fusion: Dense 3D Reconstruction using Bi-level Neural Volume Fusion,"Dense 3D reconstruction from a stream of depth images is the key to many
mixed reality and robotic applications. Although methods based on Truncated
Signed Distance Function (TSDF) Fusion have advanced the field over the years,
the TSDF volume representation is confronted with striking a balance between
the robustness to noisy measurements and maintaining the level of detail. We
present Bi-level Neural Volume Fusion (BNV-Fusion), which leverages recent
advances in neural implicit representations and neural rendering for dense 3D
reconstruction. In order to incrementally integrate new depth maps into a
global neural implicit representation, we propose a novel bi-level fusion
strategy that considers both efficiency and reconstruction quality by design.
We evaluate the proposed method on multiple datasets quantitatively and
qualitatively, demonstrating a significant improvement over existing methods.","['Kejie Li', 'Yansong Tang', 'Victor Adrian Prisacariu', 'Philip H. S. Torr']",2022-04-03T19:33:09Z,http://arxiv.org/abs/2204.01139v1,['cs.CV']
Context-Aware Sequence Alignment using 4D Skeletal Augmentation,"Temporal alignment of fine-grained human actions in videos is important for
numerous applications in computer vision, robotics, and mixed reality.
State-of-the-art methods directly learn image-based embedding space by
leveraging powerful deep convolutional neural networks. While being
straightforward, their results are far from satisfactory, the aligned videos
exhibit severe temporal discontinuity without additional post-processing steps.
The recent advancements in human body and hand pose estimation in the wild
promise new ways of addressing the task of human action alignment in videos. In
this work, based on off-the-shelf human pose estimators, we propose a novel
context-aware self-supervised learning architecture to align sequences of
actions. We name it CASA. Specifically, CASA employs self-attention and
cross-attention mechanisms to incorporate the spatial and temporal context of
human actions, which can solve the temporal discontinuity problem. Moreover, we
introduce a self-supervised learning scheme that is empowered by novel 4D
augmentation techniques for 3D skeleton representations. We systematically
evaluate the key components of our method. Our experiments on three public
datasets demonstrate CASA significantly improves phase progress and Kendall's
Tau scores over the previous state-of-the-art methods.","['Taein Kwon', 'Bugra Tekin', 'Siyu Tang', 'Marc Pollefeys']",2022-04-26T10:59:29Z,http://arxiv.org/abs/2204.12223v1,['cs.CV']
"The Gesture Authoring Space: Authoring Customised Hand Gestures for
  Grasping Virtual Objects in Immersive Virtual Environments","Natural user interfaces are on the rise. Manufacturers for Augmented,
Virtual, and Mixed Reality head mounted displays are increasingly integrating
new sensors into their consumer grade products, allowing gesture recognition
without additional hardware. This offers new possibilities for bare handed
interaction within virtual environments. This work proposes a hand gesture
authoring tool for object specific grab gestures allowing virtual objects to be
grabbed as in the real world. The presented solution uses template matching for
gesture recognition and requires no technical knowledge to design and create
custom tailored hand gestures. In a user study, the proposed approach is
compared with the pinch gesture and the controller for grasping virtual
objects. The different grasping techniques are compared in terms of accuracy,
task completion time, usability, and naturalness. The study showed that
gestures created with the proposed approach are perceived by users as a more
natural input modality than the others.","['Alexander Schäfer', 'Gerd Reis', 'Didier Stricker']",2022-07-03T18:33:33Z,http://arxiv.org/abs/2207.01092v1,"['cs.HC', 'cs.CV']"
SHREC 2022 Track on Online Detection of Heterogeneous Gestures,"This paper presents the outcomes of a contest organized to evaluate methods
for the online recognition of heterogeneous gestures from sequences of 3D hand
poses. The task is the detection of gestures belonging to a dictionary of 16
classes characterized by different pose and motion features. The dataset
features continuous sequences of hand tracking data where the gestures are
interleaved with non-significant motions. The data have been captured using the
Hololens 2 finger tracking system in a realistic use-case of mixed reality
interaction. The evaluation is based not only on the detection performances but
also on the latency and the false positives, making it possible to understand
the feasibility of practical interaction tools based on the algorithms
proposed. The outcomes of the contest's evaluation demonstrate the necessity of
further research to reduce recognition errors, while the computational cost of
the algorithms proposed is sufficiently low.","['Ariel Caputo', 'Marco Emporio', 'Andrea Giachetti', 'Marco Cristani', 'Guido Borghi', ""Andrea D'Eusanio"", 'Minh-Quan Le', 'Hai-Dang Nguyen', 'Minh-Triet Tran', 'F. Ambellan', 'M. Hanik', 'E. Nava-Yazdani', 'C. von Tycowicz']",2022-07-14T07:24:02Z,http://arxiv.org/abs/2207.06706v2,"['cs.CV', '68T10', 'I.5.2']"
"Virtual Reality Therapy for the Psychological Well-being of Palliative
  Care Patients in Hong Kong","In this paper we introduce novel Virtual Reality (VR) and Augmented Reality
(AR) treatments to improve the psychological well being of patients in
palliative care, based on interviews with a clinical psychologist who has
successfully implemented VR assisted interventions on palliative care patients
in the Hong Kong hospital system. Our VR and AR assisted interventions are
adaptations of traditional palliative care therapies which simultaneously
facilitate patients communication with family and friends while isolated in
hospital due to physical weakness and COVID-19 related restrictions. The first
system we propose is a networked, metaverse platform for palliative care
patients to create customized virtual environments with therapists, family and
friends which function as immersive and collaborative versions of 'life review'
and 'reminiscence therapy'. The second proposed system will investigate the use
of Mixed Reality telepresence and haptic touch in an AR environment, which will
allow palliative care patients to physically feel friends and family in a
virtual space, adding to the sense of presence and immersion in that
environment.","['Daniel Eckhoff', 'Royce Ng', 'Alvaro Cassinelli']",2022-07-24T14:31:52Z,http://arxiv.org/abs/2207.11754v1,['cs.HC']
"IPDAE: Improved Patch-Based Deep Autoencoder for Lossy Point Cloud
  Geometry Compression","Point cloud is a crucial representation of 3D contents, which has been widely
used in many areas such as virtual reality, mixed reality, autonomous driving,
etc. With the boost of the number of points in the data, how to efficiently
compress point cloud becomes a challenging problem. In this paper, we propose a
set of significant improvements to patch-based point cloud compression, i.e., a
learnable context model for entropy coding, octree coding for sampling centroid
points, and an integrated compression and training process. In addition, we
propose an adversarial network to improve the uniformity of points during
reconstruction. Our experiments show that the improved patch-based autoencoder
outperforms the state-of-the-art in terms of rate-distortion performance, on
both sparse and large-scale point clouds. More importantly, our method can
maintain a short compression time while ensuring the reconstruction quality.","['Kang You', 'Pan Gao', 'Qing Li']",2022-08-04T08:12:35Z,http://arxiv.org/abs/2208.02519v1,"['cs.CV', 'cs.IT', 'cs.MM', 'eess.IV', 'math.IT']"
"When Internet of Things meets Metaverse: Convergence of Physical and
  Cyber Worlds","In recent years, the Internet of Things (IoT) is studied in the context of
the Metaverse to provide users immersive cyber-virtual experiences in mixed
reality environments. This survey introduces six typical IoT applications in
the Metaverse, including collaborative healthcare, education, smart city,
entertainment, real estate, and socialization. In the IoT-inspired Metaverse,
we also comprehensively survey four pillar technologies that enable augmented
reality (AR) and virtual reality (VR), namely, responsible artificial
intelligence (AI), high-speed data communications, cost-effective mobile edge
computing (MEC), and digital twins. According to the physical-world demands, we
outline the current industrial efforts and seven key requirements for building
the IoT-inspired Metaverse: immersion, variety, economy, civility,
interactivity, authenticity, and independence. In addition, this survey
describes the open issues in the IoT-inspired Metaverse, which need to be
addressed to eventually achieve the convergence of physical and cyber worlds.","['Kai Li', 'Yingping Cui', 'Weicai Li', 'Tiejun Lv', 'Xin Yuan', 'Shenghong Li', 'Wei Ni', 'Meryem Simsek', 'Falko Dressler']",2022-08-29T11:17:54Z,http://arxiv.org/abs/2208.13501v1,['cs.NI']
Mixed-Reality Robot Behavior Replay: A System Implementation,"As robots become increasingly complex, they must explain their behaviors to
gain trust and acceptance. However, it may be difficult through verbal
explanation alone to fully convey information about past behavior, especially
regarding objects no longer present due to robots' or humans' actions. Humans
often try to physically mimic past movements to accompany verbal explanations.
Inspired by this human-human interaction, we describe the technical
implementation of a system for past behavior replay for robots in this tool
paper. Specifically, we used Behavior Trees to encode and separate robot
behaviors, and schemaless MongoDB to structurally store and query the
underlying sensor data and joint control messages for future replay. Our
approach generalizes to different types of replays, including both manipulation
and navigation replay, and visual (i.e., augmented reality (AR)) and auditory
replay. Additionally, we briefly summarize a user study to further provide
empirical evidence of its effectiveness and efficiency. Sample code and
instructions are available on GitHub at
https://github.com/umhan35/robot-behavior-replay.","['Zhao Han', 'Tom Williams', 'Holly A. Yanco']",2022-09-30T20:19:07Z,http://arxiv.org/abs/2210.00075v1,"['cs.RO', 'cs.HC']"
"Light-weighted CNN-Attention based architecture for Hand Gesture
  Recognition via ElectroMyography","Advancements in Biological Signal Processing (BSP) and Machine-Learning (ML)
models have paved the path for development of novel immersive Human-Machine
Interfaces (HMI). In this context, there has been a surge of significant
interest in Hand Gesture Recognition (HGR) utilizing Surface-Electromyogram
(sEMG) signals. This is due to its unique potential for decoding wearable data
to interpret human intent for immersion in Mixed Reality (MR) environments. To
achieve the highest possible accuracy, complicated and heavy-weighted Deep
Neural Networks (DNNs) are typically developed, which restricts their practical
application in low-power and resource-constrained wearable systems. In this
work, we propose a light-weighted hybrid architecture (HDCAM) based on
Convolutional Neural Network (CNN) and attention mechanism to effectively
extract local and global representations of the input. The proposed HDCAM model
with 58,441 parameters reached a new state-of-the-art (SOTA) performance with
82.91% and 81.28% accuracy on window sizes of 300 ms and 200 ms for classifying
17 hand gestures. The number of parameters to train the proposed HDCAM
architecture is 18.87 times less than its previous SOTA counterpart.","['Soheil Zabihi', 'Elahe Rahimian', 'Amir Asif', 'Arash Mohammadi']",2022-10-27T02:12:07Z,http://arxiv.org/abs/2210.15119v1,"['cs.LG', 'eess.SP']"
"A new benchmark for group distribution shifts in hand grasp regression
  for object manipulation. Can meta-learning raise the bar?","Understanding hand-object pose with computer vision opens the door to new
applications in mixed reality, assisted living or human-robot interaction. Most
methods are trained and evaluated on balanced datasets. This is of limited use
in real-world applications; how do these methods perform in the wild on unknown
objects? We propose a novel benchmark for object group distribution shifts in
hand and object pose regression. We then test the hypothesis that meta-learning
a baseline pose regression neural network can adapt to these shifts and
generalize better to unknown objects. Our results show measurable improvements
over the baseline, depending on the amount of prior knowledge. For the task of
joint hand-object pose regression, we observe optimization interference for the
meta-learner. To address this issue and improve the method further, we provide
a comprehensive analysis which should serve as a basis for future work on this
benchmark.","['Théo Morales', 'Gerard Lacey']",2022-10-31T19:32:14Z,http://arxiv.org/abs/2211.00110v1,"['cs.CV', 'cs.LG']"
A Quantum-Powered Photorealistic Rendering,"Achieving photorealistic rendering of real-world scenes poses a significant
challenge with diverse applications, including mixed reality and virtual
reality. Neural networks, extensively explored in solving differential
equations, have previously been introduced as implicit representations for
photorealistic rendering. However, achieving realism through traditional
computing methods is arduous due to the time-consuming optical ray tracing, as
it necessitates extensive numerical integration of color, transparency, and
opacity values for each sampling point during the rendering process. In this
paper, we introduce Quantum Radiance Fields (QRF), which incorporate quantum
circuits, quantum activation functions, and quantum volume rendering to
represent scenes implicitly. Our results demonstrate that QRF effectively
confronts the computational challenges associated with extensive numerical
integration by harnessing the parallelism capabilities of quantum computing.
Furthermore, current neural networks struggle with capturing fine signal
details and accurately modeling high-frequency information and higher-order
derivatives. Quantum computing's higher order of nonlinearity provides a
distinct advantage in this context. Consequently, QRF leverages two key
strengths of quantum computing: highly non-linear processing and extensive
parallelism, making it a potent tool for achieving photorealistic rendering of
real-world scenes.","['YuanFu Yang', 'Min Sun']",2022-11-07T10:23:32Z,http://arxiv.org/abs/2211.03418v5,['cs.CV']
The Value Chain of Education Metaverse,"Since the end of 2021, the Metaverse has been booming. Many unknown
possibilities are gradually being realized, but many people only determined
that they use Virtual Reality (VR), Augmented Reality (AR), and Mixed Reality
(MR) in the Metaverse. It is even considered that as long as the above
realities (VR, AR, MR) are used, it is equal to the Metaverse. However, this is
not true, for Reality-based display tools are only one of the presentation
methods of the Metaverse. If we cannot return to the three main characteristics
of the Metaverse: ""digital avatars,"" a decentralized ""consensus value system,""
and ""Immersive experience,"" the practice and imagination of the Metaverse will
become very narrow. Since 2022, the concept of Metaverse has also been widely
used in classroom teaching to integrate into teaching activities. Therefore, to
prevent teachers and students from understanding the Metaverse not only in the
""Using VR, AR, MR is equivalent to Metaverse"" but also pay more attention to
the other two characteristics of the Metaverse: ""digital avatars"" and a
decentralized ""consensus value system.""",['Yun-Cheng Tsai'],2022-11-07T15:28:06Z,http://arxiv.org/abs/2211.05833v2,"['cs.CY', 'cs.SI']"
"The Ball is in Our Court: Conducting Visualization Research with Sports
  Experts","Most sports visualizations rely on a combination of spatial, highly temporal,
and user-centric data, making sports a challenging target for visualization.
Emerging technologies, such as augmented and mixed reality (AR/XR), have
brought exciting opportunities along with new challenges for sports
visualization. We share our experience working with sports domain experts and
present lessons learned from conducting visualization research in SportsXR. In
our previous work, we have targeted different types of users in sports,
including athletes, game analysts, and fans. Each user group has unique design
constraints and requirements, such as obtaining real-time visual feedback in
training, automating the low-level video analysis workflow, or personalizing
embedded visualizations for live game data analysis. In this paper, we
synthesize our best practices and pitfalls we identified while working on
SportsXR. We highlight lessons learned in working with sports domain experts in
designing and evaluating sports visualizations and in working with emerging
AR/XR technologies. We envision that sports visualization research will benefit
the larger visualization community through its unique challenges and
opportunities for immersive and situated analytics.","['Tica Lin', 'Zhutian Chen', 'Johanna Beyer', 'Yincai Wu', 'Hanspeter Pfister', 'Yalong Yang']",2022-11-15T01:17:27Z,http://arxiv.org/abs/2211.07832v1,['cs.HC']
Multi-view Inverse Rendering for Large-scale Real-world Indoor Scenes,"We present a efficient multi-view inverse rendering method for large-scale
real-world indoor scenes that reconstructs global illumination and
physically-reasonable SVBRDFs. Unlike previous representations, where the
global illumination of large scenes is simplified as multiple environment maps,
we propose a compact representation called Texture-based Lighting (TBL). It
consists of 3D mesh and HDR textures, and efficiently models direct and
infinite-bounce indirect lighting of the entire large scene. Based on TBL, we
further propose a hybrid lighting representation with precomputed irradiance,
which significantly improves the efficiency and alleviates the rendering noise
in the material optimization. To physically disentangle the ambiguity between
materials, we propose a three-stage material optimization strategy based on the
priors of semantic segmentation and room segmentation. Extensive experiments
show that the proposed method outperforms the state-of-the-art quantitatively
and qualitatively, and enables physically-reasonable mixed-reality applications
such as material editing, editable novel view synthesis and relighting. The
project page is at https://lzleejean.github.io/TexIR.","['Zhen Li', 'Lingli Wang', 'Mofang Cheng', 'Cihui Pan', 'Jiaqi Yang']",2022-11-18T12:53:10Z,http://arxiv.org/abs/2211.10206v4,['cs.CV']
"Towards Live 3D Reconstruction from Wearable Video: An Evaluation of
  V-SLAM, NeRF, and Videogrammetry Techniques","Mixed reality (MR) is a key technology which promises to change the future of
warfare. An MR hybrid of physical outdoor environments and virtual military
training will enable engagements with long distance enemies, both real and
simulated. To enable this technology, a large-scale 3D model of a physical
environment must be maintained based on live sensor observations. 3D
reconstruction algorithms should utilize the low cost and pervasiveness of
video camera sensors, from both overhead and soldier-level perspectives.
Mapping speed and 3D quality can be balanced to enable live MR training in
dynamic environments. Given these requirements, we survey several 3D
reconstruction algorithms for large-scale mapping for military applications
given only live video. We measure 3D reconstruction performance from common
structure from motion, visual-SLAM, and photogrammetry techniques. This
includes the open source algorithms COLMAP, ORB-SLAM3, and NeRF using
Instant-NGP. We utilize the autonomous driving academic benchmark KITTI, which
includes both dashboard camera video and lidar produced 3D ground truth. With
the KITTI data, our primary contribution is a quantitative evaluation of 3D
reconstruction computational speed when considering live video.","['David Ramirez', 'Suren Jayasuriya', 'Andreas Spanias']",2022-11-21T19:57:51Z,http://arxiv.org/abs/2211.11836v1,"['eess.IV', 'cs.CV']"
"Mixed Cloud Control Testbed: Validating Vehicle-Road-Cloud Integration
  via Mixed Digital Twin","Reliable and efficient validation technologies are critical for the recent
development of multi-vehicle cooperation and vehicle-road-cloud integration. In
this paper, we introduce our miniature experimental platform, Mixed Cloud
Control Testbed (MCCT), developed based on a new notion of Mixed Digital Twin
(mixedDT). Combining Mixed Reality with Digital Twin, mixedDT integrates the
virtual and physical spaces into a mixed one, where physical entities coexist
and interact with virtual entities via their digital counterparts. Under the
framework of mixedDT, MCCT contains three major experimental platforms in the
physical, virtual and mixed spaces respectively, and provides a unified access
for various human-machine interfaces and external devices such as driving
simulators. A cloud unit, where the mixed experimental platform is deployed, is
responsible for fusing multi-platform information and assigning control
instructions, contributing to synchronous operation and real-time
cross-platform interaction. Particularly, MCCT allows for multi-vehicle
coordination composed of different multi-source vehicles (\eg, physical
vehicles, virtual vehicles and human-driven vehicles). Validations on vehicle
platooning demonstrate the flexibility and scalability of MCCT.","['Jianghong Dong', 'Qing Xu', 'Jiawei Wang', 'Chunying Yang', 'Mengchi Cai', 'Chaoyi Chen', 'Jianqiang Wang', 'Keqiang Li']",2022-12-05T03:39:31Z,http://arxiv.org/abs/2212.02007v1,"['cs.RO', 'cs.SY', 'eess.SY']"
Interactive Segmentation of Radiance Fields,"Radiance Fields (RF) are popular to represent casually-captured scenes for
new view synthesis and several applications beyond it. Mixed reality on
personal spaces needs understanding and manipulating scenes represented as RFs,
with semantic segmentation of objects as an important step. Prior segmentation
efforts show promise but don't scale to complex objects with diverse
appearance. We present the ISRF method to interactively segment objects with
fine structure and appearance. Nearest neighbor feature matching using
distilled semantic features identifies high-confidence seed regions. Bilateral
search in a joint spatio-semantic space grows the region to recover accurate
segmentation. We show state-of-the-art results of segmenting objects from RFs
and compositing them to another scene, changing appearance, etc., and an
interactive segmentation tool that others can use.
  Project Page: https://rahul-goel.github.io/isrf/","['Rahul Goel', 'Dhawal Sirikonda', 'Saurabh Saini', 'PJ Narayanan']",2022-12-27T16:33:19Z,http://arxiv.org/abs/2212.13545v2,['cs.CV']
PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images,"Touch plays a fundamental role in manipulation for humans; however, machine
perception of contact and pressure typically requires invasive sensors. Recent
research has shown that deep models can estimate hand pressure based on a
single RGB image. However, evaluations have been limited to controlled settings
since collecting diverse data with ground-truth pressure measurements is
difficult. We present a novel approach that enables diverse data to be captured
with only an RGB camera and a cooperative participant. Our key insight is that
people can be prompted to apply pressure in a certain way, and this prompt can
serve as a weak label to supervise models to perform well under varied
conditions. We collect a novel dataset with 51 participants making fingertip
contact with diverse objects. Our network, PressureVision++, outperforms human
annotators and prior work. We also demonstrate an application of
PressureVision++ to mixed reality where pressure estimation allows everyday
surfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and
models are available online.","['Patrick Grady', 'Jeremy A. Collins', 'Chengcheng Tang', 'Christopher D. Twigg', 'Kunal Aneja', 'James Hays', 'Charles C. Kemp']",2023-01-05T21:48:33Z,http://arxiv.org/abs/2301.02310v3,['cs.CV']
"Event-Triggered Optimal Formation Tracking Control Using Reinforcement
  Learning for Large-Scale UAV Systems","Large-scale UAV switching formation tracking control has been widely applied
in many fields such as search and rescue, cooperative transportation, and UAV
light shows. In order to optimize the control performance and reduce the
computational burden of the system, this study proposes an event-triggered
optimal formation tracking controller for discrete-time large-scale UAV systems
(UASs). And an optimal decision - optimal control framework is completed by
introducing the Hungarian algorithm and actor-critic neural networks (NNs)
implementation. Finally, a large-scale mixed reality experimental platform is
built to verify the effectiveness of the proposed algorithm, which includes
large-scale virtual UAV nodes and limited physical UAV nodes. This compensates
for the limitations of the experimental field and equipment in realworld
scenario, ensures the experimental safety, significantly reduces the
experimental cost, and is suitable for realizing largescale UAV formation light
shows.","['Ziwei Yan', 'Liang Han', 'Xiaoduo Li', 'Jinjie Li', 'Zhang Ren']",2023-01-17T08:24:54Z,http://arxiv.org/abs/2301.06749v2,['cs.MA']
"Developing a Framework for Heterotopias as Discursive Playgrounds: A
  Comparative Analysis of Non-Immersive and Immersive Technologies","The discursive space represents the reordering of knowledge gained through
accumulation. In the digital age, multimedia has become the language of
information, and the space for archival practices is provided by non-immersive
technologies, resulting in the disappearance of several layers from discursive
activities. Heterotopias are unique, multilayered epistemic contexts that
connect other systems through the exchange of information. This paper describes
a process to create a framework for Virtual Reality, Mixed Reality, and
personal computer environments based on heterotopias to provide absent layers.
This study provides virtual museum space as an informational terrain that
contains a ""world within worlds"" and presents place production as a layer of
heterotopia and the subject of discourse. Automation for the individual
multimedia content is provided via various sorting and grouping algorithms, and
procedural content generation algorithms such as Binary Space Partitioning,
Cellular Automata, Growth Algorithm, and Procedural Room Generation. Versions
of the framework were comparatively evaluated through a user study involving 30
participants, considering factors such as usability, technology acceptance, and
presence. The results of the study show that the framework can serve diverse
contexts to construct multilayered digital habitats and is flexible for
integration into professional and daily life practices.","['Elif Hilal Korkut', 'Elif Surer']",2023-01-20T13:26:36Z,http://arxiv.org/abs/2301.08565v1,"['cs.HC', 'cs.MM']"
"Assessment HTN (A-HTN) for Automated Task Performance Assessment in 3D
  Serious Games","In the recent years, various 3D mixed reality serious games have been
developed for different applications such as physical training, rehabilitation,
and education. Task performance in a serious game is a measurement of how
efficiently and accurately users accomplish the game's objectives. Prior
research includes a graph-based representation of tasks, e.g. Hierarchical Task
Network (HTN), which only models a game's tasks but does not perform
assessment. In this paper, we propose Assessment HTN (A-HTN), which both models
the task efficiently and incorporates assessment logic for game objectives.
Based on how the task performance is evaluated, A-HTN automatically performs:
(a) Task-level Assessment by comparing object manipulations and (b)
Action-level Assessment by comparing motion trajectories. The system can also
categorize the task performance assessment into single user or multi-user based
on who is being assessed. We showcase the effectiveness of the A-HTN using two
3D VR serious games: a hydrometer experiment and a multi-user chemistry
experiment. The A-HTN experiments show a high correlation between instructor
scores and the system generated scores indicating that the proposed A-HTN
generalizes automatic assessment at par with Subject Matter Experts.","['Kevin Desai', 'Omeed Ashtiani', 'Balakrishnan Prabhakaran']",2023-02-11T22:13:16Z,http://arxiv.org/abs/2302.05795v1,"['cs.HC', 'cs.MM']"
Optimization-Based Eye Tracking using Deflectometric Information,"Eye tracking is an important tool with a wide range of applications in
Virtual, Augmented, and Mixed Reality (VR/AR/MR) technologies. State-of-the-art
eye tracking methods are either reflection-based and track reflections of
sparse point light sources, or image-based and exploit 2D features of the
acquired eye image. In this work, we attempt to significantly improve
reflection-based methods by utilizing pixel-dense deflectometric surface
measurements in combination with optimization-based inverse rendering
algorithms. Utilizing the known geometry of our deflectometric setup, we
develop a differentiable rendering pipeline based on PyTorch3D that simulates a
virtual eye under screen illumination. Eventually, we exploit the
image-screen-correspondence information from the captured measurements to find
the eye's rotation, translation, and shape parameters with our renderer via
gradient descent. In general, our method does not require a specific pattern
and can work with ordinary video frames of the main VR/AR/MR screen itself. We
demonstrate real-world experiments with evaluated mean relative gaze errors
below 0.45 degrees at a precision better than 0.11 degrees. Moreover, we show
an improvement of 6X over a representative reflection-based state-of-the-art
method in simulation.","['Tianfu Wang', 'Jiazhang Wang', 'Oliver Cossairt', 'Florian Willomitzer']",2023-03-09T02:41:13Z,http://arxiv.org/abs/2303.04997v1,['cs.CV']
"GazePair: Efficient Pairing of Augmented Reality Devices Using Gaze
  Tracking","As Augmented Reality (AR) devices become more prevalent and commercially
viable, the need for quick, efficient, and secure schemes for pairing these
devices has become more pressing. Current methods to securely exchange
holograms require users to send this information through large data centers,
creating security and privacy concerns. Existing techniques to pair these
devices on a local network and share information fall short in terms of
usability and scalability. These techniques either require hardware not
available on AR devices, intricate physical gestures, removal of the device
from the head, do not scale to multiple pairing partners, or rely on methods
with low entropy to create encryption keys. To that end, we propose a novel
pairing system, called GazePair, that improves on all existing local pairing
techniques by creating an efficient, effective, and intuitive pairing protocol.
GazePair uses eye gaze tracking and a spoken key sequence cue (KSC) to generate
identical, independently generated symmetric encryption keys with 64 bits of
entropy. GazePair also achieves improvements in pairing success rates and times
over current methods. Additionally, we show that GazePair can extend to
multiple users. Finally, we assert that GazePair can be used on any Mixed
Reality (MR) device equipped with eye gaze tracking.","['Matthew Corbett', 'Jiacheng Shang', 'Bo Ji']",2023-03-13T18:32:32Z,http://arxiv.org/abs/2303.07404v1,['cs.CR']
"Inside-out Infrared Marker Tracking via Head Mounted Displays for Smart
  Robot Programming","Intuitive robot programming through use of tracked smart input devices relies
on fixed, external tracking systems, most often employing infra-red markers.
Such an approach is frequently combined with projector-based augmented reality
for better visualisation and interface. The combined system, although providing
an intuitive programming platform with short cycle times even for inexperienced
users, is immobile, expensive and requires extensive calibration. When faced
with a changing environment and large number of robots it becomes sorely
impractical. Here we present our work on infra-red marker tracking using the
Microsoft HoloLens head-mounted display. The HoloLens can map the environment,
register the robot on-line, and track smart devices equipped with infra-red
markers in the robot coordinate system. We envision our work to provide the
basis to transfer many of the paradigms developed over the years for systems
requiring a projector and a tracked input device into a highly-portable system
that does not require any calibration or special set-up. We test the quality of
the marker-tracking in an industrial robot cell and compare our tracking with a
ground truth obtained via an ART-3 tracking system.","['David Puljiz', 'Alexandru-George Vasilache', 'Michael Mende', 'Björn Hein']",2023-03-28T14:45:03Z,http://arxiv.org/abs/2303.16017v1,['cs.RO']
"Exploiting the Complementarity of 2D and 3D Networks to Address
  Domain-Shift in 3D Semantic Segmentation","3D semantic segmentation is a critical task in many real-world applications,
such as autonomous driving, robotics, and mixed reality. However, the task is
extremely challenging due to ambiguities coming from the unstructured, sparse,
and uncolored nature of the 3D point clouds. A possible solution is to combine
the 3D information with others coming from sensors featuring a different
modality, such as RGB cameras. Recent multi-modal 3D semantic segmentation
networks exploit these modalities relying on two branches that process the 2D
and 3D information independently, striving to maintain the strength of each
modality. In this work, we first explain why this design choice is effective
and then show how it can be improved to make the multi-modal semantic
segmentation more robust to domain shift. Our surprisingly simple contribution
achieves state-of-the-art performances on four popular multi-modal unsupervised
domain adaptation benchmarks, as well as better results in a domain
generalization scenario.","['Adriano Cardace', 'Pierluigi Zama Ramirez', 'Samuele Salti', 'Luigi Di Stefano']",2023-04-06T10:59:43Z,http://arxiv.org/abs/2304.02991v1,['cs.CV']
"OO-dMVMT: A Deep Multi-view Multi-task Classification Framework for
  Real-time 3D Hand Gesture Classification and Segmentation","Continuous mid-air hand gesture recognition based on captured hand pose
streams is fundamental for human-computer interaction, particularly in AR / VR.
However, many of the methods proposed to recognize heterogeneous hand gestures
are tested only on the classification task, and the real-time low-latency
gesture segmentation in a continuous stream is not well addressed in the
literature. For this task, we propose the On-Off deep Multi-View Multi-Task
paradigm (OO-dMVMT). The idea is to exploit multiple time-local views related
to hand pose and movement to generate rich gesture descriptions, along with
using heterogeneous tasks to achieve high accuracy. OO-dMVMT extends the
classical MVMT paradigm, where all of the multiple tasks have to be active at
each time, by allowing specific tasks to switch on/off depending on whether
they can apply to the input. We show that OO-dMVMT defines the new SotA on
continuous/online 3D skeleton-based gesture recognition in terms of gesture
classification accuracy, segmentation accuracy, false positives, and decision
latency while maintaining real-time operation.","['Federico Cunico', 'Federico Girella', 'Andrea Avogaro', 'Marco Emporio', 'Andrea Giachetti', 'Marco Cristani']",2023-04-12T16:28:29Z,http://arxiv.org/abs/2304.05956v1,['cs.CV']
Traffic Characteristics of Extended Reality,"This tutorial paper analyzes the traffic characteristics of immersive
experiences with extended reality (XR) technologies, including Augmented
reality (AR), virtual reality (VR), and mixed reality (MR). The current trend
in XR applications is to offload the computation and rendering to an external
server and use wireless communications between the XR head-mounted display
(HMD) and the access points. This paradigm becomes essential owing to (1) its
high flexibility (in terms of user mobility) compared to remote rendering
through a wired connection, and (2) the high computing power available on the
server compared to local rendering (on HMD). The requirements to facilitate a
pleasant XR experience are analyzed in three aspects: capacity (throughput),
latency, and reliability. For capacity, two VR experiences are analyzed: a
human eye-like experience and an experience with the Oculus Quest 2 HMD. For
latency, the key components of the motion-to-photon (MTP) delay are discussed.
For reliability, the maximum packet loss rate (or the minimum packet delivery
rate) is studied for different XR scenarios. Specifically, the paper reviews
optimization techniques that were proposed to reduce the latency, conserve the
bandwidth, extend the scalability, and/or increase the reliability to satisfy
the stringent requirements of the emerging XR applications.","['Abdullah Alnajim', 'Seyedmohammad Salehi', 'Chien-Chung Shen', 'Malcolm Smith']",2023-04-16T22:17:29Z,http://arxiv.org/abs/2304.07908v1,"['cs.NI', 'cs.GR']"
"BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion
  Synthesis","Mixed reality applications require tracking the user's full-body motion to
enable an immersive experience. However, typical head-mounted devices can only
track head and hand movements, leading to a limited reconstruction of full-body
motion due to variability in lower body configurations. We propose BoDiffusion
-- a generative diffusion model for motion synthesis to tackle this
under-constrained reconstruction problem. We present a time and space
conditioning scheme that allows BoDiffusion to leverage sparse tracking inputs
while generating smooth and realistic full-body motion sequences. To the best
of our knowledge, this is the first approach that uses the reverse diffusion
process to model full-body tracking as a conditional sequence generation task.
We conduct experiments on the large-scale motion-capture dataset AMASS and show
that our approach outperforms the state-of-the-art approaches by a significant
margin in terms of full-body motion realism and joint reconstruction error.","['Angela Castillo', 'Maria Escobar', 'Guillaume Jeanneret', 'Albert Pumarola', 'Pablo Arbeláez', 'Ali Thabet', 'Artsiom Sanakoyeu']",2023-04-21T16:39:05Z,http://arxiv.org/abs/2304.11118v1,"['cs.CV', 'cs.AI']"
"A Comprehensive Survey on Affective Computing; Challenges, Trends,
  Applications, and Future Directions","As the name suggests, affective computing aims to recognize human emotions,
sentiments, and feelings. There is a wide range of fields that study affective
computing, including languages, sociology, psychology, computer science, and
physiology. However, no research has ever been done to determine how machine
learning (ML) and mixed reality (XR) interact together. This paper discusses
the significance of affective computing, as well as its ideas, conceptions,
methods, and outcomes. By using approaches of ML and XR, we survey and discuss
recent methodologies in affective computing. We survey the state-of-the-art
approaches along with current affective data resources. Further, we discuss
various applications where affective computing has a significant impact, which
will aid future scholars in gaining a better understanding of its significance
and practical relevance.","['Sitara Afzal', 'Haseeb Ali Khan', 'Imran Ullah Khan', 'Md. Jalil Piran', 'Jong Weon Lee']",2023-05-08T10:42:46Z,http://arxiv.org/abs/2305.07665v1,['cs.AI']
"A Fusion Model: Towards a Virtual, Physical and Cognitive Integration
  and its Principles","Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital
twin, Metaverse and other related digital technologies have attracted much
attention in recent years. These new emerging technologies are changing the
world significantly. This research introduces a fusion model, i.e. Fusion
Universe (FU), where the virtual, physical, and cognitive worlds are merged
together. Therefore, it is crucial to establish a set of principles for the
fusion model that is compatible with our physical universe laws and principles.
This paper investigates several aspects that could affect immersive and
interactive experience; and proposes the fundamental principles for Fusion
Universe that can integrate physical and virtual world seamlessly.","['Hao Lan Zhang', 'Yun Xue', 'Yifan Lu', 'Sanghyuk Lee']",2023-05-17T06:34:22Z,http://arxiv.org/abs/2305.09992v1,"['cs.AI', 'cs.HC']"
TextSLAM: Visual SLAM with Semantic Planar Text Features,"We propose a novel visual SLAM method that integrates text objects tightly by
treating them as semantic features via fully exploring their geometric and
semantic prior. The text object is modeled as a texture-rich planar patch whose
semantic meaning is extracted and updated on the fly for better data
association. With the full exploration of locally planar characteristics and
semantic meaning of text objects, the SLAM system becomes more accurate and
robust even under challenging conditions such as image blurring, large
viewpoint changes, and significant illumination variations (day and night). We
tested our method in various scenes with the ground truth data. The results
show that integrating texture features leads to a more superior SLAM system
that can match images across day and night. The reconstructed semantic 3D text
map could be useful for navigation and scene understanding in robotic and mixed
reality applications. Our project page: https://github.com/SJTU-ViSYS/TextSLAM .","['Boying Li', 'Danping Zou', 'Yuan Huang', 'Xinghan Niu', 'Ling Pei', 'Wenxian Yu']",2023-05-17T08:16:26Z,http://arxiv.org/abs/2305.10029v2,['cs.CV']
Extended-XRI Body Interfaces for Hyper-Connected Metaverse Environments,"Hybrid mixed-reality (XR) internet-of-things (IoT) research, here called XRI,
aims at a strong integration between physical and virtual objects,
environments, and agents wherein IoT-enabled edge devices are deployed for
sensing, context understanding, networked communication and control of device
actuators. Likewise, as augmented reality systems provide an immersive overlay
on the environments, and virtual reality provides fully immersive environments,
the merger of these domains leads to immersive smart spaces that are
hyper-connected, adaptive and dynamic components that anchor the metaverse to
real-world constructs. Enabling the human-in-the-loop to remain engaged and
connected across these virtual-physical hybrid environments requires advances
in user interaction that are multi-dimensional. This work investigates the
potential to transition the user interface to the human body as an
extended-reality avatar with hybrid extended-body interfaces that can interact
both with the physical and virtual sides of the metaverse. It contributes: i)
an overview of metaverses, XRI, and avatarization concepts, ii) a taxonomy
landscape for extended XRI body interfaces, iii) an architecture and potential
interactions for XRI body designs, iv) a prototype XRI body implementation
based on the architecture, v) a design-science evaluation, toward enabling
future design research directions.","['Jie Guan', 'Alexis Morris']",2023-06-01T19:11:18Z,http://arxiv.org/abs/2306.01096v1,['cs.HC']
"An XRI Mixed-Reality Internet-of-Things Architectural Framework Toward
  Immersive and Adaptive Smart Environments","The internet-of-things (IoT) refers to the growing number of embedded
interconnected devices within everyday ubiquitous objects and environments,
especially their networks, edge controllers, data gathering and management,
sharing, and contextual analysis capabilities. However, the IoT suffers from
inherent limitations in terms of human-computer interaction. In this landscape,
there is a need for interfaces that have the potential to translate the IoT
more solidly into the foreground of everyday smart environments, where its
users are multimodal, multifaceted, and where new forms of presentation,
adaptation, and immersion are essential. This work highlights the synergetic
opportunities for both IoT and XR to converge toward hybrid XR objects with
strong real-world connectivity, and IoT objects with rich XR interfaces. The
paper contributes i) an understanding of this multi-disciplinary domain XR-IoT
(XRI); ii) a theoretical perspective on how to design XRI agents based on the
literature; iii) a system design architectural framework for XRI smart
environment development; and iv) an early discussion of this process. It is
hoped that this research enables future researchers in both communities to
better understand and deploy hybrid smart XRI environments.","['Alexis Morris', 'Jie Guan', 'Amna Azhar']",2023-06-01T20:47:07Z,http://arxiv.org/abs/2306.01139v1,['cs.HC']
"Evolution of 3GPP Standards Towards True Extended Reality (XR) Support
  in 6G Networks","Extended reality (XR) is a key innovation of 5G-advanced and beyond networks.
The diverse XR use-cases, including virtual reality, augmented reality, and
mixed reality, transform the way humans interact with surrounding environments.
Thus, XR technology enables true immersive experiences of novel services
spanning, e.g., e-commerce, healthcare, and education, respectively. However,
the efficient support of XR services over existing and future cellular systems
is highly challenging and requires multiple radio design improvements, due to
the unique XR traffic and performance characteristics. Thus, this article
surveys the state-of-art 3GPP standardization activities (release-18) for
integrating the XR service class into the 5G-advanced specifications,
highlighting the major XR performance challenges. Furthermore, the paper
introduces valuable insights and research directions for supporting true XR
services over the next-generation 6G networks, where multiple novel radio
design mindsets and protocol enhancements are proposed and evaluated using
extensive system level simulations, including solutions for application-native
dynamic performance reporting, traffic-dependent control channel design,
collaborative device aggregation for XR capacity boosting and offload,
respectively.","['Ali A. Esswie', 'Morris Repeta']",2023-06-06T20:57:35Z,http://arxiv.org/abs/2306.04012v1,"['eess.SP', 'cs.NI']"
Large AI Model-Based Semantic Communications,"Semantic communication (SC) is an emerging intelligent paradigm, offering
solutions for various future applications like metaverse, mixed-reality, and
the Internet of everything. However, in current SC systems, the construction of
the knowledge base (KB) faces several issues, including limited knowledge
representation, frequent knowledge updates, and insecure knowledge sharing.
Fortunately, the development of the large AI model provides new solutions to
overcome above issues. Here, we propose a large AI model-based SC framework
(LAM-SC) specifically designed for image data, where we first design the
segment anything model (SAM)-based KB (SKB) that can split the original image
into different semantic segments by universal semantic knowledge. Then, we
present an attention-based semantic integration (ASI) to weigh the semantic
segments generated by SKB without human participation and integrate them as the
semantic-aware image. Additionally, we propose an adaptive semantic compression
(ASC) encoding to remove redundant information in semantic features, thereby
reducing communication overhead. Finally, through simulations, we demonstrate
the effectiveness of the LAM-SC framework and the significance of the large AI
model-based KB development in future SC paradigms.","['Feibo Jiang', 'Yubo Peng', 'Li Dong', 'Kezhi Wang', 'Kun Yang', 'Cunhua Pan', 'Xiaohu You']",2023-07-07T10:01:08Z,http://arxiv.org/abs/2307.03492v1,"['cs.AI', 'cs.NI']"
"Accessibility and Inclusiveness of New Information and Communication
  Technologies for Disabled Users and Content Creators in the Metaverse","Despite the proliferation of Blockchain Metaverse projects, the inclusion of
physically disabled individuals in the Metaverse remains distant, with limited
standards and regulations in place. However, the article proposes a concept of
the Metaverse that leverages emerging technologies, such as Virtual and
Augmented Reality, and the Internet of Things, to enable greater engagement of
disabled creatives. This approach aims to enhance inclusiveness in the
Metaverse landscape. Based on the findings, the paper concludes that the active
involvement of physically disabled individuals in the design and development of
Metaverse platforms is crucial for promoting inclusivity. The proposed
framework for accessibility and inclusiveness in Virtual, Augmented, and Mixed
realities of decentralised Metaverses provides a basis for the meaningful
participation of disabled creatives. The article emphasises the importance of
addressing the mechanisms for art production by individuals with disabilities
in the emerging Metaverse landscape. Additionally, it highlights the need for
further research and collaboration to establish standards and regulations that
facilitate the inclusion of physically disabled individuals in Metaverse
projects.","['Petar Radanliev', 'David De Roure', 'Peter Novitzky', 'Ivo Sluganovic']",2023-08-01T18:39:12Z,http://arxiv.org/abs/2308.01925v1,"['cs.CY', 'cs.CV', 'cs.MM', 'cs.SI']"
"Open Medical Gesture: An Open-Source Experiment in Naturalistic Physical
  Interactions for Mixed and Virtual Reality Simulations","Mixed Reality (MR) and Virtual Reality (VR) simulations are hampered by
requirements for hand controllers or attempts to perseverate in use of
two-dimensional computer interface paradigms from the 1980s. From our efforts
to produce more naturalistic interactions for combat medic training for the
military, USC has developed an open-source toolkit that enables direct hand
controlled responsive interactions that is sensor independent and can function
with depth sensing cameras, webcams or sensory gloves. Natural approaches we
have examined include the ability to manipulate virtual smart objects in a
similar manner to how they are used in the real world. From this research and
review of current literature, we have discerned several best approaches for
hand-based human computer interactions which provide intuitive, responsive,
useful, and low frustration experiences for VR users.","['Thomas B Talbot', 'Chinmay Chinara']",2023-08-14T21:56:41Z,http://arxiv.org/abs/2308.07472v1,['cs.HC']
"Projecting Robot Intentions Through Visual Cues: Static vs. Dynamic
  Signaling","Augmented and mixed-reality techniques harbor a great potential for improving
human-robot collaboration. Visual signals and cues may be projected to a human
partner in order to explicitly communicate robot intentions and goals. However,
it is unclear what type of signals support such a process and whether signals
can be combined without adding additional cognitive stress to the partner. This
paper focuses on identifying the effective types of visual signals and quantify
their impact through empirical evaluations. In particular, the study compares
static and dynamic visual signals within a collaborative object sorting task
and assesses their ability to shape human behavior. Furthermore, an
information-theoretic analysis is performed to numerically quantify the degree
of information transfer between visual signals and human behavior. The results
of a human subject experiment show that there are significant advantages to
combining multiple visual signals within a single task, i.e., increased task
efficiency and reduced cognitive load.","['Shubham Sonawani', 'Yifan Zhou', 'Heni Ben Amor']",2023-08-19T01:18:37Z,http://arxiv.org/abs/2308.09871v1,"['cs.RO', 'cs.GR']"
Towards Ubiquitous Intelligent Hand Interaction,"The development of ubiquitous computing and sensing devices has brought about
novel interaction scenarios such as mixed reality and IoT (e.g., smart home),
which pose new demands for the next generation of natural user interfaces
(NUI). Human hand, benefit for the large degree-of-freedom, serves as a medium
through which people interact with the external world in their daily lives,
thus also being regarded as the main entry of NUI. Unfortunately, current hand
tracking system is largely confined on first perspective vision-based
solutions, which suffer from optical artifacts and are not practical in
ubiquitous environments. In my thesis, I rethink this problem by analyzing the
underlying logic in terms of sensor, behavior, and semantics, constituting a
research framework for achieving ubiquitous intelligent hand interaction. Then
I summarize my previous research topics and illustrated the future research
directions based on my research framework.",['Chen Liang'],2023-08-21T07:52:16Z,http://arxiv.org/abs/2308.13543v1,"['cs.HC', 'cs.CV']"
"PaperToPlace: Transforming Instruction Documents into Spatialized and
  Context-Aware Mixed Reality Experiences","While paper instructions are one of the mainstream medium for sharing
knowledge, consuming such instructions and translating them into activities are
inefficient due to the lack of connectivity with physical environment. We
present PaperToPlace, a novel workflow comprising an authoring pipeline, which
allows the authors to rapidly transform and spatialize existing paper
instructions into MR experience, and a consumption pipeline, which
computationally place each instruction step at an optimal location that is easy
to read and do not occlude key interaction areas. Our evaluations of the
authoring pipeline with 12 participants demonstrated the usability of our
workflow and the effectiveness of using a machine learning based approach to
help extracting the spatial locations associated with each steps. A second
within-subject study with another 12 participants demonstrates the merits of
our consumption pipeline by reducing efforts of context switching, delivering
the segmented instruction steps and offering the hands-free affordances.","['Chen Chen', 'Cuong Nguyen', 'Jane Hoffswell', 'Jennifer Healey', 'Trung Bui', 'Nadir Weibel']",2023-08-26T17:51:12Z,http://arxiv.org/abs/2308.13924v1,"['cs.HC', 'H.4.m; H.5.2; I.7.m']"
D-VAT: End-to-End Visual Active Tracking for Micro Aerial Vehicles,"Visual active tracking is a growing research topic in robotics due to its key
role in applications such as human assistance, disaster recovery, and
surveillance. In contrast to passive tracking, active tracking approaches
combine vision and control capabilities to detect and actively track the
target. Most of the work in this area focuses on ground robots, while the very
few contributions on aerial platforms still pose important design constraints
that limit their applicability. To overcome these limitations, in this paper we
propose D-VAT, a novel end-to-end visual active tracking methodology based on
deep reinforcement learning that is tailored to micro aerial vehicle platforms.
The D-VAT agent computes the vehicle thrust and angular velocity commands
needed to track the target by directly processing monocular camera
measurements. We show that the proposed approach allows for precise and
collision-free tracking operations, outperforming different state-of-the-art
baselines on simulated environments which differ significantly from those
encountered during training. Moreover, we demonstrate a smooth real-world
transition to a quadrotor platform with mixed-reality.","['Alberto Dionigi', 'Simone Felicioni', 'Mirko Leomanni', 'Gabriele Costante']",2023-08-31T17:21:18Z,http://arxiv.org/abs/2308.16874v2,['cs.RO']
Poster: Enabling Flexible Edge-assisted XR,"Extended reality (XR) is touted as the next frontier of the digital future.
XR includes all immersive technologies of augmented reality (AR), virtual
reality (VR), and mixed reality (MR). XR applications obtain the real-world
context of the user from an underlying system, and provide rich, immersive, and
interactive virtual experiences based on the user's context in real-time. XR
systems process streams of data from device sensors, and provide
functionalities including perceptions and graphics required by the
applications. These processing steps are computationally intensive, and the
challenge is that they must be performed within the strict latency requirements
of XR. This poses limitations on the possible XR experiences that can be
supported on mobile devices with limited computing resources.
  In this XR context, edge computing is an effective approach to address this
problem for mobile users. The edge is located closer to the end users and
enables processing and storing data near them. In addition, the development of
high bandwidth and low latency network technologies such as 5G facilitates the
application of edge computing for latency-critical use cases [4, 11]. This work
presents an XR system for enabling flexible edge-assisted XR.","['Jin Heo', 'Ketan Bhardwaj', 'Ada Gavrilovska']",2023-09-08T18:34:34Z,http://arxiv.org/abs/2309.04548v1,"['cs.DC', 'cs.MM']"
"Improving Human Legibility in Collaborative Robot Tasks through
  Augmented Reality and Workspace Preparation","Understanding the intentions of human teammates is critical for safe and
effective human-robot interaction. The canonical approach for human-aware robot
motion planning is to first predict the human's goal or path, and then
construct a robot plan that avoids collision with the human. This method can
generate unsafe interactions if the human model and subsequent predictions are
inaccurate. In this work, we present an algorithmic approach for both arranging
the configuration of objects in a shared human-robot workspace, and projecting
``virtual obstacles'' in augmented reality, optimizing for legibility in a
given task. These changes to the workspace result in more legible human
behavior, improving robot predictions of human goals, thereby improving task
fluency and safety. To evaluate our approach, we propose two user studies
involving a collaborative tabletop task with a manipulator robot, and a
warehouse navigation task with a mobile robot.","['Yi-Shiuan Tung', 'Matthew B. Luebbers', 'Alessandro Roncone', 'Bradley Hayes']",2023-11-09T18:18:28Z,http://arxiv.org/abs/2311.05562v1,['cs.RO']
Mixed Reality UI Adaptations with Inaccurate and Incomplete Objectives,"This position paper outlines a new approach to adapting 3D user interface
(UI) layouts given the complex nature of end-user preferences. Current
optimization techniques, which mainly rely on weighted sum methods, can be
inflexible and result in unsatisfactory adaptations. We propose using
multi-objective optimization and interactive preference elicitation to provide
semi-automated, flexible, and effective adaptations of 3D UIs. Our approach is
demonstrated using an example of single-element 3D layout adaptation with
ergonomic objectives. Future work is needed to address questions around the
presentation and selection of optimal solutions, the impact on cognitive load,
and the integration of preference learning. We conclude that, to make adaptive
3D UIs truly effective, we must acknowledge the limitations of our optimization
objectives and techniques and emphasize the importance of user control.","['Christoph Albert Johns', 'João Marcelo Evangelista Belo']",2023-11-17T11:45:46Z,http://arxiv.org/abs/2311.10466v1,['cs.HC']
"A Comparison of Interfaces for Learning How to Play a Mixed Reality
  Handpan","In the realm of music therapy, Virtual Reality (VR) has a long-standing
history of enriching human experiences through immersive applications, spanning
entertainment games, serious games, and professional training in various
fields. However, the untapped potential lies in using VR games to support
mindfulness through music. We present a new approach utilizing a virtual
environment to facilitate learning how to play the handpan -- an instrument in
the shape of a spherical dish with harmonically tuned notes used commonly in
the sound healing practice of mindfulness. In a preliminary study, we compared
six interfaces, where the highlighted path interface performed best. However,
participants expressed preference for the standard interface inspired by rhythm
games like Guitar Hero.","['Gavin Gosling', 'Ivan-teofil Catovic', 'Ghazal Bangash', 'Daniel MacCormick', 'Loutfouz Zaman']",2023-12-12T02:11:13Z,http://arxiv.org/abs/2312.06936v1,['cs.HC']
"Testing Human-Robot Interaction in Virtual Reality: Experience from a
  Study on Speech Act Classification","In recent years, an increasing number of Human-Robot Interaction (HRI)
approaches have been implemented and evaluated in Virtual Reality (VR), as it
allows to speed-up design iterations and makes it safer for the final user to
evaluate and master the HRI primitives. However, identifying the most suitable
VR experience is not straightforward. In this work, we evaluate how, in a smart
agriculture scenario, immersive and non-immersive VR are perceived by users
with respect to a speech act understanding task. In particular, we collect
opinions and suggestions from the 81 participants involved in both experiments
to highlight the strengths and weaknesses of these different experiences.","['Sara Kaszuba', 'Sandeep Reddy Sabbella', 'Francesco Leotta', 'Pascal Serrarens', 'Daniele Nardi']",2024-01-09T13:08:13Z,http://arxiv.org/abs/2401.04534v1,"['cs.RO', 'cs.HC']"
"OCTO+: A Suite for Automatic Open-Vocabulary Object Placement in Mixed
  Reality","One key challenge in Augmented Reality is the placement of virtual content in
natural locations. Most existing automated techniques can only work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce and
evaluate several methods for automatic object placement using recent advances
in open-vocabulary vision-language models. Through a multifaceted evaluation,
we identify a new state-of-the-art method, OCTO+. We also introduce a benchmark
for automatically evaluating the placement of virtual objects in augmented
reality, alleviating the need for costly user studies. Through this, in
addition to human evaluations, we find that OCTO+ places objects in a valid
region over 70% of the time, outperforming other methods on a range of metrics.","['Aditya Sharma', 'Luke Yoffe', 'Tobias Höllerer']",2024-01-17T04:52:40Z,http://arxiv.org/abs/2401.08973v1,"['cs.CV', 'cs.AI', 'cs.CL']"
Relative Pose for Nonrigid Multi-Perspective Cameras: The Static Case,"Multi-perspective cameras with potentially non-overlapping fields of view
have become an important exteroceptive sensing modality in a number of
applications such as intelligent vehicles, drones, and mixed reality headsets.
In this work, we challenge one of the basic assumptions made in these
scenarios, which is that the multi-camera rig is rigid. More specifically, we
are considering the problem of estimating the relative pose between a static
non-rigid rig in different spatial orientations while taking into account the
effect of gravity onto the system. The deformable physical connections between
each camera and the body center are approximated by a simple cantilever model,
and inserted into the generalized epipolar constraint. Our results lead us to
the important insight that the latent parameters of the deformation model,
meaning the gravity vector in both views, become observable. We present a
concise analysis of the observability of all variables based on noise,
outliers, and rig rigidity for two different algorithms. The first one is a
vision-only alternative, while the second one makes use of additional gravity
measurements. To conclude, we demonstrate the ability to sense gravity in a
real-world example, and discuss practical implications.","['Min Li', 'Jiaqi Yang', 'Laurent Kneip']",2024-01-17T11:28:28Z,http://arxiv.org/abs/2401.09140v1,"['cs.RO', 'cs.CV']"
"Design Frameworks for Spatial Zone Agents in XRI Metaverse Smart
  Environments","The spatial XR-IoT (XRI) Zone Agents concept combines Extended Reality (XR),
the Internet of Things (IoT), and spatial computing concepts to create
hyper-connected spaces for metaverse applications; envisioning space as zones
that are social, smart, scalable, expressive, and agent-based. These zone
agents serve as applications and agents (partners, assistants, or guides) for
users co-living and co-operating together in a shared spatial context. The zone
agent concept is toward reducing the gap between the physical environment
(space) and the classical two-dimensional user interface, through space-based
interactions for future metaverse applications. This integration aims to enrich
user engagement with their environments through intuitive and immersive
experiences and pave the way for innovative human-machine interaction in smart
spaces. Contributions include: i) a theoretical framework for creating XRI
zone/space-agents using Mixed-Reality Agents (MiRAs) and XRI theory, ii) agent
and scene design for spatial zone agents, and iii) prototype and user
interaction design scenario concepts for human-to-space agent relationships in
an early immersive smart-space application.","['Jie Guan', 'Jiamin Liu', 'Alexis Morris']",2024-01-19T22:03:36Z,http://arxiv.org/abs/2401.11040v1,['cs.HC']
Portobello: Extending Driving Simulation from the Lab to the Road,"In automotive user interface design, testing often starts with lab-based
driving simulators and migrates toward on-road studies to mitigate risks. Mixed
reality (XR) helps translate virtual study designs to the real road to increase
ecological validity. However, researchers rarely run the same study in both
in-lab and on-road simulators due to the challenges of replicating studies in
both physical and virtual worlds. To provide a common infrastructure to port
in-lab study designs on-road, we built a platform-portable infrastructure,
Portobello, to enable us to run twinned physical-virtual studies. As a
proof-of-concept, we extended the on-road simulator XR-OOM with Portobello. We
ran a within-subjects, autonomous-vehicle crosswalk cooperation study (N=32)
both in-lab and on-road to investigate study design portability and
platform-driven influences on study outcomes. To our knowledge, this is the
first system that enables the twinning of studies originally designed for
in-lab simulators to be carried out in an on-road platform.","['Fanjun Bu', 'Stacey Li', 'David Goedicke', 'Mark Colley', 'Gyanendra Sharma', 'Hiroshi Yasuda', 'Wendy Ju']",2024-02-12T21:11:34Z,http://arxiv.org/abs/2402.08061v1,['cs.HC']
"Mixed-Reality-Guided Teleoperation of a Collaborative Robot for Surgical
  Procedures","The development of advanced surgical systems embedding the Master-Slave
control strategy introduced the possibility of remote interaction between the
surgeon and the patient, also known as teleoperation. The present paper aims to
integrate innovative technologies into the teleoperation process to enhance
workflow during surgeries. The proposed system incorporates a collaborative
robot, Kuka IIWA LBR, and Hololens 2 (an augmented reality device), allowing
the user to control the robot in an expansive environment that integrates
actual (real data) with additional digital information imported via Hololens 2.
Experimental data demonstrate the user's ability to control the Kuka IIWA using
various gestures to position it with respect to real or digital objects. Thus,
this system offers a novel solution to manipulate robots used in surgeries in a
more intuitive manner, contributing to the reduction of the learning curve for
surgeons. Calibration and testing in multiple scenarios demonstrate the
efficiency of the system in providing seamless movements.","['Gabriela Rus', 'Nadim Al Hajjar', 'Paul Tucan', 'Andra Ciocan', 'Calin Vaida', 'Corina Radu', 'Damien Chablat', 'Doina Pisla']",2024-02-19T09:51:03Z,http://arxiv.org/abs/2402.12002v1,['cs.RO']
"Enabling Waypoint Generation for Collaborative Robots using LLMs and
  Mixed Reality","Programming a robotic is a complex task, as it demands the user to have a
good command of specific programming languages and awareness of the robot's
physical constraints. We propose a framework that simplifies robot deployment
by allowing direct communication using natural language. It uses large language
models (LLM) for prompt processing, workspace understanding, and waypoint
generation. It also employs Augmented Reality (AR) to provide visual feedback
of the planned outcome. We showcase the effectiveness of our framework with a
simple pick-and-place task, which we implement on a real robot. Moreover, we
present an early concept of expressive robot behavior and skill generation that
can be used to communicate with the user and learn new skills (e.g., object
grasping).","['Cathy Mengying Fang', 'Krzysztof Zieliński', 'Pattie Maes', 'Joe Paradiso', 'Bruce Blumberg', 'Mikkel Baun Kjærgaard']",2024-03-14T11:59:07Z,http://arxiv.org/abs/2403.09308v1,"['cs.HC', 'cs.RO']"
"Towards Massive Interaction with Generalist Robotics: A Systematic
  Review of XR-enabled Remote Human-Robot Interaction Systems","The rising interest of generalist robots seek to create robots with
versatility to handle multiple tasks in a variety of environments, and human
will interact with such robots through immersive interfaces. In the context of
human-robot interaction (HRI), this survey provides an exhaustive review of the
applications of extended reality (XR) technologies in the field of remote HRI.
We developed a systematic search strategy based on the PRISMA methodology. From
the initial 2,561 articles selected, 100 research papers that met our inclusion
criteria were included. We categorized and summarized the domain in detail,
delving into XR technologies, including augmented reality (AR), virtual reality
(VR), and mixed reality (MR), and their applications in facilitating intuitive
and effective remote control and interaction with robotic systems. The survey
highlights existing articles on the application of XR technologies, user
experience enhancement, and various interaction designs for XR in remote HRI,
providing insights into current trends and future directions. We also
identified potential gaps and opportunities for future research to improve
remote HRI systems through XR technology to guide and inform future XR and
robotics research.","['Xian Wang', 'Luyao Shen', 'Lik-Hang Lee']",2024-03-18T00:22:30Z,http://arxiv.org/abs/2403.11384v3,"['cs.HC', 'cs.RO']"
Experimental Studies of Metaverse Streaming,"Metaverse aims to construct a large, unified, immersive, and shared digital
realm by combining various technologies, namely XR (extended reality),
blockchain, and digital twin, among others. This article explores the Metaverse
from the perspective of multimedia communication by conducting and analyzing
real-world experiments on four different Metaverse platforms: VR (virtual
reality) Vircadia, VR Mozilla Hubs, VRChat, and MR (mixed reality) Virtual
City. We first investigate the traffic patterns and network performance in the
three VR platforms. After raising the challenges of the Metaverse streaming and
investigating the potential methods to enhance Metaverse performance, we
propose a remote rendering architecture and verify its advantages through a
prototype involving the campus network and MR multimodal interaction by
comparison with local rendering.","['Haopeng Wang', 'Roberto Martinez-Velazquez', 'Haiwei Dong', 'Abdulmotaleb El Saddik']",2024-03-22T14:57:12Z,http://arxiv.org/abs/2403.15256v1,"['cs.MM', 'cs.NI']"
Multiway Point Cloud Mosaicking with Diffusion and Global Optimization,"We introduce a novel framework for multiway point cloud mosaicking (named
Wednesday), designed to co-align sets of partially overlapping point clouds --
typically obtained from 3D scanners or moving RGB-D cameras -- into a unified
coordinate system. At the core of our approach is ODIN, a learned pairwise
registration algorithm that iteratively identifies overlaps and refines
attention scores, employing a diffusion-based process for denoising pairwise
correlation matrices to enhance matching accuracy. Further steps include
constructing a pose graph from all point clouds, performing rotation averaging,
a novel robust algorithm for re-estimating translations optimally in terms of
consensus maximization and translation optimization. Finally, the point cloud
rotations and positions are optimized jointly by a diffusion-based approach.
Tested on four diverse, large-scale datasets, our method achieves
state-of-the-art pairwise and multiway registration results by a large margin
on all benchmarks. Our code and models are available at
https://github.com/jinsz/Multiway-Point-Cloud-Mosaicking-with-Diffusion-and-Global-Optimization.","['Shengze Jin', 'Iro Armeni', 'Marc Pollefeys', 'Daniel Barath']",2024-03-30T17:29:13Z,http://arxiv.org/abs/2404.00429v1,['cs.CV']
SARA: Smart AI Reading Assistant for Reading Comprehension,"SARA integrates Eye Tracking and state-of-the-art large language models in a
mixed reality framework to enhance the reading experience by providing
personalized assistance in real-time. By tracking eye movements, SARA
identifies the text segments that attract the user's attention the most and
potentially indicate uncertain areas and comprehension issues. The process
involves these key steps: text detection and extraction, gaze tracking and
alignment, and assessment of detected reading difficulty. The results are
customized solutions presented directly within the user's field of view as
virtual overlays on identified difficult text areas. This support enables users
to overcome challenges like unfamiliar vocabulary and complex sentences by
offering additional context, rephrased solutions, and multilingual help. SARA's
innovative approach demonstrates it has the potential to transform the reading
experience and improve reading proficiency.","['Enkeleda Thaqi', 'Mohamed Mantawy', 'Enkelejda Kasneci']",2024-04-10T10:57:18Z,http://arxiv.org/abs/2404.06906v1,['cs.HC']
"MixLight: Borrowing the Best of both Spherical Harmonics and Gaussian
  Models","Accurately estimating scene lighting is critical for applications such as
mixed reality. Existing works estimate illumination by generating illumination
maps or regressing illumination parameters. However, the method of generating
illumination maps has poor generalization performance and parametric models
such as Spherical Harmonic (SH) and Spherical Gaussian (SG) fall short in
capturing high-frequency or low-frequency components. This paper presents
MixLight, a joint model that utilizes the complementary characteristics of SH
and SG to achieve a more complete illumination representation, which uses SH
and SG to capture low-frequency ambient and high-frequency light sources
respectively. In addition, a special spherical light source sparsemax
(SLSparsemax) module that refers to the position and brightness relationship
between spherical light sources is designed to improve their sparsity, which is
significant but omitted by prior works. Extensive experiments demonstrate that
MixLight surpasses state-of-the-art (SOTA) methods on multiple metrics. In
addition, experiments on Web Dataset also show that MixLight as a parametric
method has better generalization performance than non-parametric methods.","['Xinlong Ji', 'Fangneng Zhan', 'Shijian Lu', 'Shi-Sheng Huang', 'Hua Huang']",2024-04-19T10:17:10Z,http://arxiv.org/abs/2404.12768v1,"['cs.CV', 'cs.AI', 'cs.GR']"
"Protecting Human Users Against Cognitive Attacks in Immersive
  Environments","Integrating mixed reality (MR) with artificial intelligence (AI)
technologies, including vision, language, audio, reasoning, and planning,
enables the AI-powered MR assistant [1] to substantially elevate human
efficiency. This enhancement comes from situational awareness, quick access to
essential information, and support in learning new skills in the right context
throughout everyday tasks. This blend transforms interactions with both the
virtual and physical environments, catering to a range of skill levels and
personal preferences. For instance, computer vision enables the understanding
of the user's environment, allowing for the provision of timely and relevant
digital overlays in MR systems. At the same time, language models enhance
comprehension of contextual information and support voice-activated dialogue to
answer user questions. However, as AI-driven MR systems advance, they also
unveil new vulnerabilities, posing a threat to user safety by potentially
exposing them to grave dangers [5, 6].","['Yan-Ming Chiou', 'Bob Price', 'Chien-Chung Shen', 'Syed Ali Asif']",2024-04-23T17:42:15Z,http://arxiv.org/abs/2405.05919v1,['cs.HC']
A First Look at Immersive Telepresence on Apple Vision Pro,"Due to the widespread adoption of ""work-from-home"" policies,
videoconferencing applications (e.g., Zoom) have become indispensable for
remote communication. However, these systems lack immersiveness, leading to the
so-called ""Zoom fatigue"" and degrading communication efficiency. The recent
debut of Apple Vision Pro, a mixed reality headset that supports ""spatial
persona"", aims to offer an immersive telepresence experience with these
applications. In this paper, we conduct a first-of-its-kind in-depth and
empirical study to analyze the performance of immersive telepresence with four
applications, Apple FaceTime, Cisco Webex, Microsoft Teams, and Zoom, on Vision
Pro. We find that only FaceTime provides a truly immersive experience with
spatial personas, whereas other applications still operate 2D personas. Our
measurement results reveal that (1) FaceTime delivers semantic information to
optimize bandwidth consumption, which is even lower than that of 2D persona for
other applications, and (2) it employs visibility-aware optimizations to reduce
rendering overhead. However, the scalability of FaceTime remains limited, with
a simple server allocation strategy that potentially leads to high network
delay among users.","['Ruizhi Cheng', 'Nan Wu', 'Matteo Varvello', 'Eugene Chai', 'Songqing Chen', 'Bo Han']",2024-05-16T20:03:03Z,http://arxiv.org/abs/2405.10422v1,['cs.NI']
Distributed Technology-Sustained Pervasive Applications,"Technology-sustained pervasive games, contrary to technology-supported
pervasive games, can be understood as computer games interfacing with the
physical world. Pervasive games are known to make use of 'non-standard input
devices' and with the rise of the Internet of Things (IoT), pervasive
applications can be expected to move beyond games. This dissertation is
requirements- and development-focused Design Science research for distributed
technology-sustained pervasive applications, incorporating knowledge from the
domains of Distributed Computing, Mixed Reality, Context-Aware Computing,
Geographical Information Systems and IoT. Computer video games have existed for
decades, with a reusable game engine to drive them. If pervasive games can be
understood as computer games interfacing with the physical world, can computer
game engines be used to stage pervasive games? Considering the use of
non-standard input devices in pervasive games and the rise of IoT, how will
this affect the architectures supporting the broader set of pervasive
applications? The use of a game engine can be found in some existing pervasive
game projects, but general research into how the domain of pervasive games
overlaps with that of video games is lacking. When an engine is used, a
discussion of, what type of engine is most suitable and what properties are
being fulfilled by the engine, is often not part of the discourse. This
dissertation uses multiple iterations of the method framework for Design
Science for the design and development of three software system architectures.
In the face of IoT, the problem of extending pervasive games into a fourth
software architecture, accommodating a broader set of pervasive applications,
is explicated. The requirements, for technology-sustained pervasive games, are
verified through the design, development and demonstration of the three
software system architectures. The ...",['Kim J. L. Nevelsteen'],2016-04-11T11:33:20Z,http://arxiv.org/abs/1604.02892v1,"['cs.CY', 'cs.HC']"
"Exploring the Pathways of Adaptation an Avatar 3D Animation Procedures
  and Virtual Reality Arenas in Research of Human Courtship Behaviour and
  Sexual Reactivity in Psychological Research","There are many reasons for utilising 3D animation and virtual reality in
sexuality research. Apart from providing a mean with which to (re)experience
certain situations there are four main advantages: a) bespoke animated stimuli
can be created and customized, which is especially important when researching
paraphilia and sexual preferences, b) stimulus production is less expensive and
easier to produce compared to real world stimuli, c) virtual reality allows us
to capture data such as physiological reasons to stimuli, that we would not be
able to otherwise (without resorting to self-report measures which are
especially problematic in this research domain), d) ethical, legal, and health
and safety issues are less complex since neither physical nor psychological
harm is caused to animated characters allowing for the safe presentation of
stimuli involving vulnerable targets. The animation sub-group has been
exploring so far several production quality levels and various animation
procedures in a number of available software. The aim is to develop static as
well as dynamic, interactive sexual stimuli for sexual diagnostic and
therapeutic purposes. We are aware of number of ethical issues related to the
use of virtual reality in proposed research are analysed in this chapter.","['Jakub Binter', 'Kateřina Klapilová', 'Tereza Zikánová', 'Tommy Nilsson', 'Klára Bártová', 'Lucie Krejcová', 'Renata Androvicová', 'Jitka Lindová', 'Denisa Prušová', 'Timothy Wells', 'Daniel Riha']",2016-11-06T18:27:09Z,http://arxiv.org/abs/1611.01817v1,['cs.HC']
"Physics holo.lab learning experience: Using Smartglasses for Augmented
  Reality labwork to foster the concepts of heat conduction","Fundamental concepts of thermodynamics rely on abstract physical quantities
such as energy, heat and entropy, which play an important role in the process
of interpreting thermal phenomena and statistical mechanics. However, these
quantities are not covered by human (visual) perception and thus, an intuitive
understanding often is lacking. Today immersive technologies like head-mounted
displays of the newest generation, especially HoloLens, allow for high quality
augmented reality learning experiences, which can overcome this perception gap
and simultaneously avoid a split attention effect. In a mixed reality (MR)
scenario as presented in this paper---which we call a holo.lab---human
perception can be extended to the thermal regime by presenting false-color
representations of the temperature of objects as a virtual augmentation
directly on the real object itself in real-time. Direct feedback to
experimental actions of the users in form of different representations allows
for immediate comparison to theoretical principles and predictions and
therefore is supposed to intensify the theory-experiment interactions and to
increase the conceptual understanding. We tested this technology for an
experiment on thermal conduction of metals in the framework of undergraduate
laboratories. A pilot study with treatment and control groups (N = 59) showed a
small positive effect of MR on students' performance measured with a
standardized concept test for thermodynamics, indicating an improvement of the
understanding of the underlying physical concepts.","['M. P. Strzys', 'S. Kapp', 'M. Thees', 'P. Klein', 'P. Lukowicz', 'P. Knierim', 'A. Schmidt', 'J. Kuhn']",2017-11-14T12:50:35Z,http://arxiv.org/abs/1711.05087v2,['physics.ed-ph']
"Generating Classes of 3D Virtual Mandibles for AR-Based Medical
  Simulation","Simulation and modeling represent promising tools for several application
domains from engineering to forensic science and medicine. Advances in 3D
imaging technology convey paradigms such as augmented reality (AR) and mixed
reality inside promising simulation tools for the training industry. Motivated
by the requirement for superimposing anatomically correct 3D models on a Human
Patient Simulator (HPS) and visualizing them in an AR environment, the purpose
of this research effort is to derive method for scaling a source human mandible
to a target human mandible. Results show that, given a distance between two
same landmarks on two different mandibles, a relative scaling factor may be
computed. Using this scaling factor, results show that a 3D virtual mandible
model can be made morphometrically equivalent to a real target-specific
mandible within a 1.30 millimeter average error bound. The virtual mandible may
be further used as a reference target for registering other anatomical models,
such as the lungs, on the HPS. Such registration will be made possible by
physical constraints among the mandible and the spinal column in the horizontal
normal rest position.","['Neha R. Hippalgaonkar', 'Alexa D. Sider', 'Felix G. Hamza-Lup', 'Anand P. Santhanam', 'Bala Jaganathan', 'Celina Imielinska', 'Jannick P. Rolland']",2018-11-20T03:29:56Z,http://arxiv.org/abs/1811.08053v1,['cs.GR']
"RetinaMatch: Efficient Template Matching of Retina Images for
  Teleophthalmology","Retinal template matching and registration is an important challenge in
teleophthalmology with low-cost imaging devices. However, the images from such
devices generally have a small field of view (FOV) and image quality
degradations, making matching difficult. In this work, we develop an efficient
and accurate retinal matching technique that combines dimension reduction and
mutual information (MI), called RetinaMatch. The dimension reduction
initializes the MI optimization as a coarse localization process, which narrows
the optimization domain and avoids local optima. The effectiveness of
RetinaMatch is demonstrated on the open fundus image database STARE with
simulated reduced FOV and anticipated degradations, and on retinal images
acquired by adapter-based optics attached to a smartphone. RetinaMatch achieves
a success rate over 94\% on human retinal images with the matched target
registration errors below 2 pixels on average, excluding the observer
variability. It outperforms the standard template matching solutions. In the
application of measuring vessel diameter repeatedly, single pixel errors are
expected. In addition, our method can be used in the process of image
mosaicking with area-based registration, providing a robust approach when the
feature based methods fail. To the best of our knowledge, this is the first
template matching algorithm for retina images with small template images from
unconstrained retinal areas. In the context of the emerging mixed reality
market, we envision automated retinal image matching and registration methods
as transformative for advanced teleophthalmology and long-term retinal
monitoring.","['Chen Gong', 'N. Benjamin Erichson', 'John P. Kelly', 'Laura Trutoiu', 'Brian T. Schowengerdt', 'Steven L. Brunton', 'Eric J. Seibel']",2018-11-28T23:06:54Z,http://arxiv.org/abs/1811.11874v1,"['eess.IV', 'cs.CV']"
"AirPen: A Touchless Fingertip Based Gestural Interface for Smartphones
  and Head-Mounted Devices","Hand gestures are an intuitive, socially acceptable, and a non-intrusive
interaction modality in Mixed Reality (MR) and smartphone based applications.
Unlike speech interfaces, they tend to perform well even in shared and public
spaces. Hand gestures can also be used to interact with smartphones in
situations where the user's ability to physically touch the device is impaired.
However, accurate gesture recognition can be achieved through state-of-the-art
deep learning models or with the use of expensive sensors. Despite the
robustness of these deep learning models, they are computationally heavy and
memory hungry, and obtaining real-time performance on-device without additional
hardware is still a challenge. To address this, we propose AirPen: an analogue
to pen on paper, but in air, for in-air writing and gestural commands that
works seamlessly in First and Second Person View. The models are trained on a
GPU machine and ported on an Android smartphone. AirPen comprises of three deep
learning models that work in tandem: MobileNetV2 for hand localisation, our
custom fingertip regression architecture followed by a Bi-LSTM model for
gesture classification. The overall framework works in real-time on mobile
devices and achieves a classification accuracy of 80% with an average latency
of only 0.12 s.","['Varun Jain', 'Ramya Hebbalaguppe']",2019-04-12T09:29:01Z,http://arxiv.org/abs/1904.06122v1,['cs.HC']
"GestARLite: An On-Device Pointing Finger Based Gestural Interface for
  Smartphones and Video See-Through Head-Mounts","Hand gestures form an intuitive means of interaction in Mixed Reality (MR)
applications. However, accurate gesture recognition can be achieved only
through state-of-the-art deep learning models or with the use of expensive
sensors. Despite the robustness of these deep learning models, they are
generally computationally expensive and obtaining real-time performance
on-device is still a challenge. To this end, we propose a novel lightweight
hand gesture recognition framework that works in First Person View for wearable
devices. The models are trained on a GPU machine and ported on an Android
smartphone for its use with frugal wearable devices such as the Google
Cardboard and VR Box. The proposed hand gesture recognition framework is driven
by a cascade of state-of-the-art deep learning models: MobileNetV2 for hand
localisation, our custom fingertip regression architecture followed by a
Bi-LSTM model for gesture classification. We extensively evaluate the framework
on our EgoGestAR dataset. The overall framework works in real-time on mobile
devices and achieves a classification accuracy of 80% on EgoGestAR video
dataset with an average latency of only 0.12 s.","['Varun Jain', 'Gaurav Garg', 'Ramakrishna Perla', 'Ramya Hebbalaguppe']",2019-04-19T14:32:40Z,http://arxiv.org/abs/1904.09843v1,['cs.CV']
"Conceptual Design and Preliminary Results of a VR-based Radiation Safety
  Training System for Interventional Radiologists","Recent studies have reported an increased risk of developing brain and neck
tumors, as well as cataracts, in practitioners in interventional radiology
(IR). Occupational radiation protection in IR has been a top concern for
regulatory agencies and professional societies. To help minimize occupational
radiation exposure in IR, we conceptualized a virtual reality (VR) based
radiation safety training system to help operators understand complex radiation
fields and to avoid high radiation areas through game-like interactive
simulations. The preliminary development of the system has yielded results
suggesting that the training system can calculate and report the radiation
exposure after each training session based on a database precalculated from
computational phantoms and Monte Carlo simulations and the position information
provided in real-time by the MS Hololens headset worn by trainee. In addition,
real-time dose rate and cumulative dose will be displayed to the trainee by MS
Hololens to help them adjust their practice. This paper presents the conceptual
design of the overall hardware and software design, as well as preliminary
results to combine MS HoloLens headset and complex 3D X-ray field spatial
distribution data to create a mixed reality environment for safety training
purpose in IR.","['Yi Guo', 'Li Mao', 'Gongsen Zhang', 'Zhi Chen', 'Xi Pei', 'X. George Xu']",2020-01-14T15:02:47Z,http://arxiv.org/abs/2001.04839v1,"['physics.med-ph', 'cs.HC']"
"Toward the Internet of No Things: The Role of O2O Communications and
  Extended Reality","Future fully interconnected virtual reality (VR) systems and the Tactile
Internet diminish the boundary between virtual (online) and real (offline)
worlds, while extending the digital and physical capabilities of humans via
edge computing and teleoperated robots, respectively. In this paper, we focus
on the Internet of No Things as an extension of immersive VR from virtual to
real environments, where human-intended Internet services - either digital or
physical - appear when needed and disappear when not needed. We first introduce
the concept of integrated online-to-offline (O2O) communications, which treats
online and offline channels as complementary to bridge the virtual and physical
worlds and provide O2O multichannel experiences. We then elaborate on the
emerging extended reality (XR), which brings the different forms of
virtual/augmented/mixed reality together to realize the entire
reality-virtuality continuum and, more importantly, supports human-machine
interaction as envisioned by the Tactile Internet, while posing challenges to
conventional handhelds, e.g., smartphones. Building on the so-called
invisible-to-visible (I2V) technology concept, we present our extrasensory
perception network (ESPN) and investigate how O2O communications and XR can be
combined for the nonlocal extension of human ""sixth-sense"" experiences in space
and time. We conclude by putting our ideas in perspective of the 6G vision.","['Martin Maier', 'Amin Ebrahimzadeh']",2019-06-16T17:41:23Z,http://arxiv.org/abs/1906.06738v1,['cs.NI']
LIME: Live Intrinsic Material Estimation,"We present the first end to end approach for real time material estimation
for general object shapes with uniform material that only requires a single
color image as input. In addition to Lambertian surface properties, our
approach fully automatically computes the specular albedo, material shininess,
and a foreground segmentation. We tackle this challenging and ill posed inverse
rendering problem using recent advances in image to image translation
techniques based on deep convolutional encoder decoder architectures. The
underlying core representations of our approach are specular shading, diffuse
shading and mirror images, which allow to learn the effective and accurate
separation of diffuse and specular albedo. In addition, we propose a novel
highly efficient perceptual rendering loss that mimics real world image
formation and obtains intermediate results even during run time. The estimation
of material parameters at real time frame rates enables exciting mixed reality
applications, such as seamless illumination consistent integration of virtual
objects into real world scenes, and virtual material cloning. We demonstrate
our approach in a live setup, compare it to the state of the art, and
demonstrate its effectiveness through quantitative and qualitative evaluation.","['Abhimitra Meka', 'Maxim Maximov', 'Michael Zollhoefer', 'Avishek Chatterjee', 'Hans-Peter Seidel', 'Christian Richardt', 'Christian Theobalt']",2018-01-03T16:55:31Z,http://arxiv.org/abs/1801.01075v2,['cs.CV']
Cost-benefit Analysis of Visualization in Virtual Environments,"Visualization and virtual environments (VEs) have been two interconnected
parallel strands in visual computing for decades. Some VEs have been purposely
developed for visualization applications, while many visualization applications
are exemplary showcases in general-purpose VEs. Because of the development and
operation costs of VEs, the majority of visualization applications in practice
are yet to benefit from the capacity of VEs. In this paper, we examine this
perplexity from an information-theoretic perspective. Our objectives are to
conduct cost-benefit analysis on typical VE systems (including augmented and
mixed reality, theatre-based systems, and large powerwalls), to explain why
some visualization applications benefit more from VEs than others, and to
sketch out pathways for the future development of visualization applications in
VEs. We support our theoretical propositions and analysis using theories and
discoveries in the literature of cognitive sciences and the practical evidence
reported in the literatures of visualization and VEs.","['Min Chen', 'Kelly Gaither', 'Nigel W. John', 'Brian McCann']",2018-02-25T14:14:42Z,http://arxiv.org/abs/1802.09012v2,"['cs.HC', 'cs.GR']"
"Virtualized Application Function Chaining: Maximizing the Wearable
  System Lifetime","The number of smart devices wear and carry by users is growing rapidly which
is driven by innovative new smart wearables and interesting service o erings.
This has led to applications that utilize multiple devices around the body to
provide immersive environments such as mixed reality. These applications rely
on a number of di erent types of functions such as sensing, communication and
various types of processing, that require considerable resources. Thus one of
the major challenges in supporting of these applications is dependent on the
battery lifetime of the devices that provide the necessary functionality. The
battery lifetime can be extended by either incorporating a battery with larger
capacity and/or by utilizing the available resources e ciently. However, the
increases in battery capacity are not keeping up with the demand and larger
batteries add to both the weight and size of the device. Thus, the focus of
this paper is to improve the battery e ciency through intelligent resources
utilization. We show that, when the same resource is available on multiple
devices that form part of the wearable system, and or is in close proximity, it
is possible consider them as a resource pool and further utilize them
intelligently to improve the system lifetime. Speci cally, we formulate the
function allocation algorithm as a Mixed Integer Linear Programming (MILP)
optimization problem and propose an e cient heuristic solution. The
experimental data driven simulation results show that approximately 40-50%
system battery life improvement can be achieved with proper function allocation
and orchestration.","['Harini Kolamunna', 'Kanchana Thilakarathna', 'Aruna Seneviratne']",2018-04-02T21:43:59Z,http://arxiv.org/abs/1804.00739v1,['cs.NI']
"Offline and Online calibration of Mobile Robot and SLAM Device for
  Navigation","Robot navigation technology is required to accomplish difficult tasks in
various environments. In navigation, it is necessary to know the information of
the external environments and the state of the robot under the environment. On
the other hand, various studies have been done on SLAM technology, which is
also used for navigation, but also applied to devices for Mixed Reality and the
like.
  In this paper, we propose a robot-device calibration method for navigation
with a device using SLAM technology on a robot. The calibration is performed by
using the position and orientation information given by the robot and the
device. In the calibration, the most efficient way of movement is clarified
according to the restriction of the robot movement. Furthermore, we also show a
method to dynamically correct the position and orientation of the robot so that
the information of the external environment and the shape information of the
robot maintain consistency in order to reduce the dynamic error occurring
during navigation.
  Our method can be easily used for various kinds of robots and localization
with sufficient precision for navigation is possible with offline calibration
and online position correction. In the experiments, we confirm the parameters
obtained by two types of offline calibration according to the degree of freedom
of robot movement and validate the effectiveness of online correction method by
plotting localized position error during robot's intense movement. Finally, we
show the demonstration of navigation using SLAM device.","['Ryoichi Ishikawa', 'Takeshi Oishi', 'Katsushi Ikeuchi']",2018-04-13T07:48:44Z,http://arxiv.org/abs/1804.04817v1,"['cs.CV', 'cs.RO']"
"Normalized Object Coordinate Space for Category-Level 6D Object Pose and
  Size Estimation","The goal of this paper is to estimate the 6D pose and dimensions of unseen
object instances in an RGB-D image. Contrary to ""instance-level"" 6D pose
estimation tasks, our problem assumes that no exact object CAD models are
available during either training or testing time. To handle different and
unseen object instances in a given category, we introduce a Normalized Object
Coordinate Space (NOCS)---a shared canonical representation for all possible
object instances within a category. Our region-based neural network is then
trained to directly infer the correspondence from observed pixels to this
shared object representation (NOCS) along with other object information such as
class label and instance mask. These predictions can be combined with the depth
map to jointly estimate the metric 6D pose and dimensions of multiple objects
in a cluttered scene. To train our network, we present a new context-aware
technique to generate large amounts of fully annotated mixed reality data. To
further improve our model and evaluate its performance on real data, we also
provide a fully annotated real-world dataset with large environment and
instance variation. Extensive experiments demonstrate that the proposed method
is able to robustly estimate the pose and size of unseen object instances in
real environments while also achieving state-of-the-art performance on standard
6D pose estimation benchmarks.","['He Wang', 'Srinath Sridhar', 'Jingwei Huang', 'Julien Valentin', 'Shuran Song', 'Leonidas J. Guibas']",2019-01-09T23:31:40Z,http://arxiv.org/abs/1901.02970v2,['cs.CV']
Maps and Globes in Virtual Reality,"This paper explores different ways to render world-wide geographic maps in
virtual reality (VR). We compare: (a) a 3D exocentric globe, where the user's
viewpoint is outside the globe; (b) a flat map (rendered to a plane in VR); (c)
an egocentric 3D globe, with the viewpoint inside the globe; and (d) a curved
map, created by projecting the map onto a section of a sphere which curves
around the user. In all four visualisations the geographic centre can be
smoothly adjusted with a standard handheld VR controller and the user, through
a head-tracked headset, can physically move around the visualisation. For
distance comparison, exocentric globe is more accurate than egocentric globe
and flat map. For area comparison, more time is required with exocentric and
egocentric globes than with flat and curved maps. For direction estimation, the
exocentric globe is more accurate and faster than the other visual
presentations. Our study participants had a weak preference for the exocentric
globe. Generally, the curved map had benefits over the flat map. In almost all
cases the egocentric globe was found to be the least effective visualisation.
Overall, our results provide support for the use of exocentric globes for
geographic visualisation in mixed-reality.","['Yalong Yang', 'Bernhard Jenny', 'Tim Dwyer', 'Kim Marriott', 'Haohui Chen', 'Maxime Cordeil']",2019-08-06T11:45:51Z,http://arxiv.org/abs/1908.02088v1,"['cs.HC', 'cs.GR', 'cs.MM']"
Mixing realities for sketch retrieval in Virtual Reality,"Drawing tools for Virtual Reality (VR) enable users to model 3D designs from
within the virtual environment itself. These tools employ sketching and
sculpting techniques known from desktop-based interfaces and apply them to
hand-based controller interaction. While these techniques allow for mid-air
sketching of basic shapes, it remains difficult for users to create detailed
and comprehensive 3D models. In our work, we focus on supporting the user in
designing the virtual environment around them by enhancing sketch-based
interfaces with a supporting system for interactive model retrieval. Through
sketching, an immersed user can query a database containing detailed 3D models
and replace them into the virtual environment. To understand supportive
sketching within a virtual environment, we compare different methods of sketch
interaction, i.e., 3D mid-air sketching, 2D sketching on a virtual tablet, 2D
sketching on a fixed virtual whiteboard, and 2D sketching on a real tablet.
%using a 2D physical tablet, a 2D virtual tablet, a 2D virtual whiteboard, and
3D mid-air sketching. Our results show that 3D mid-air sketching is considered
to be a more intuitive method to search a collection of models while the
addition of physical devices creates confusion due to the complications of
their inclusion within a virtual environment. While we pose our work as a
retrieval problem for 3D models of chairs, our results can be extrapolated to
other sketching tasks for virtual environments.","['Daniele Giunchi', 'Stuart james', 'Donald Degraen', 'Anthony Steed']",2019-10-25T11:52:25Z,http://arxiv.org/abs/1910.11637v2,"['cs.HC', 'cs.CV']"
Rig-space Neural Rendering,"Movie productions use high resolution 3d characters with complex proprietary
rigs to create the highest quality images possible for large displays.
Unfortunately, these 3d assets are typically not compatible with real-time
graphics engines used for games, mixed reality and real-time pre-visualization.
Consequently, the 3d characters need to be re-modeled and re-rigged for these
new applications, requiring weeks of work and artistic approval. Our solution
to this problem is to learn a compact image-based rendering of the original 3d
character, conditioned directly on the rig parameters. Our idea is to render
the character in many different poses and views, and to train a deep neural
network to render high resolution images, from the rig parameters directly.
Many neural rendering techniques have been proposed to render from 2d
skeletons, or geometry and UV maps. However these require manual work, and to
do not remain compatible with the animator workflow of manipulating rig
widgets, as well as the real-time game engine pipeline of interpolating rig
parameters. We extend our architecture to support dynamic re-lighting and
composition with other 3d objects in the scene. We designed a network that
efficiently generates multiple scene feature maps such as normals, depth,
albedo and mask, which are composed with other scene objects to form the final
image.","['Dominik Borer', 'Lu Yuhang', 'Laura Wuelfroth', 'Jakob Buhmann', 'Martin Guay']",2020-03-22T06:28:22Z,http://arxiv.org/abs/2003.09820v1,['cs.GR']
"Enhanced Self-Perception in Mixed Reality: Egocentric Arm Segmentation
  and Database with Automatic Labelling","In this study, we focus on the egocentric segmentation of arms to improve
self-perception in Augmented Virtuality (AV). The main contributions of this
work are: i) a comprehensive survey of segmentation algorithms for AV; ii) an
Egocentric Arm Segmentation Dataset, composed of more than 10, 000 images,
comprising variations of skin color, and gender, among others. We provide all
details required for the automated generation of groundtruth and semi-synthetic
images; iii) the use of deep learning for the first time for segmenting arms in
AV; iv) to showcase the usefulness of this database, we report results on
different real egocentric hand datasets, including GTEA Gaze+, EDSH, EgoHands,
Ego Youtube Hands, THU-Read, TEgO, FPAB, and Ego Gesture, which allow for
direct comparisons with existing approaches utilizing color or depth. Results
confirm the suitability of the EgoArm dataset for this task, achieving
improvement up to 40% with respect to the original network, depending on the
particular dataset. Results also suggest that, while approaches based on color
or depth can work in controlled conditions (lack of occlusion, uniform
lighting, only objects of interest in the near range, controlled background,
etc.), egocentric segmentation based on deep learning is more robust in real AV
applications.","['Ester Gonzalez-Sosa', 'Pablo Perez', 'Ruben Tolosana', 'Redouane Kachach', 'Alvaro Villegas']",2020-03-27T12:09:27Z,http://arxiv.org/abs/2003.12352v1,['cs.CV']
Lookup tables for phase randomisation in hardware generated holograms,"The rise in virtual and mixed reality systems has prompted a resurgence of
interest in two-dimensional and three-dimensional real-time computer generated
holography. Phase randomisation is an integral part of holographic projection
as it ensures independence in sub-frame techniques and reduces the edge
enhancement seen in flat-phase images. Phase randomisation requires, however,
the availability of a pseudo-random number generator as well as trigonometric
functions such as cos and sin. On embedded devices such as field programmable
gate arrays and digital signal processors this can be an unacceptable load and
necessitate the use of proprietary intellectual property cores. Lookup tables
are able to reduce the computational load but can run to many megabytes for
even low-resolution systems.
  This paper introduces the use of lookup tables (LUTs) in the context of two
common algorithms used for real-time holographic projection: Gerchberg-Saxton
and One-Step Phase-Retrieval. A simulated study is carried out to investigate
the use of relatively small lookup tables where random numbers are repeated in
sequence. We find that the increase in error is low and tunable to under 5\%
even for small look up tables. This result is also demonstrated experimentally.
Finally, the implications of this study are discussed and conclusions drawn.","['Peter J. Christopher', 'Timothy D. Wilkinson']",2020-03-31T11:31:55Z,http://arxiv.org/abs/2004.04049v1,"['eess.SP', 'eess.IV']"
"Anchors Based Method for Fingertips Position Estimation from a Monocular
  RGB Image using Deep Neural Network","In Virtual, augmented, and mixed reality, the use of hand gestures is
increasingly becoming popular to reduce the difference between the virtual and
real world. The precise location of the fingertip is essential/crucial for a
seamless experience. Much of the research work is based on using depth
information for the estimation of the fingertips position. However, most of the
work using RGB images for fingertips detection is limited to a single finger.
The detection of multiple fingertips from a single RGB image is very
challenging due to various factors. In this paper, we propose a deep neural
network (DNN) based methodology to estimate the fingertips position. We
christened this methodology as an Anchor based Fingertips Position Estimation
(ABFPE), and it is a two-step process. The fingertips location is estimated
using regression by computing the difference in the location of a fingertip
from the nearest anchor point. The proposed framework performs the best with
limited dependence on hand detection results. In our experiments on the
SCUT-Ego-Gesture dataset, we achieved the fingertips detection error of 2.3552
pixels on a video frame with a resolution of $640 \times 480$ and about
$92.98\%$ of test images have average pixel errors of five pixels.","['Purnendu Mishra', 'Kishor Sarawadekar']",2020-05-04T09:45:56Z,http://arxiv.org/abs/2005.01351v2,"['cs.CV', 'cs.HC', 'eess.IV']"
Deep Lighting Environment Map Estimation from Spherical Panoramas,"Estimating a scene's lighting is a very important task when compositing
synthetic content within real environments, with applications in mixed reality
and post-production. In this work we present a data-driven model that estimates
an HDR lighting environment map from a single LDR monocular spherical panorama.
In addition to being a challenging and ill-posed problem, the lighting
estimation task also suffers from a lack of facile illumination ground truth
data, a fact that hinders the applicability of data-driven methods. We approach
this problem differently, exploiting the availability of surface geometry to
employ image-based relighting as a data generator and supervision mechanism.
This relies on a global Lambertian assumption that helps us overcome issues
related to pre-baked lighting. We relight our training data and complement the
model's supervision with a photometric loss, enabled by a differentiable
image-based relighting technique. Finally, since we predict spherical spectral
coefficients, we show that by imposing a distribution prior on the predicted
coefficients, we can greatly boost performance. Code and models available at
https://vcl3d.github.io/DeepPanoramaLighting.","['Vasileios Gkitsas', 'Nikolaos Zioulis', 'Federico Alvarez', 'Dimitrios Zarpalas', 'Petros Daras']",2020-05-16T14:23:05Z,http://arxiv.org/abs/2005.08000v1,"['cs.CV', 'cs.GR']"
"A survey on applications of augmented, mixed and virtual reality for
  nature and environment","Augmented reality (AR), virtual reality (VR) and mixed reality (MR) are
technologies of great potential due to the engaging and enriching experiences
they are capable of providing. Their use is rapidly increasing in diverse
fields such as medicine, manufacturing or entertainment. However, the
possibilities that AR, VR and MR offer in the area of environmental
applications are not yet widely explored. In this paper we present the outcome
of a survey meant to discover and classify existing AR/VR/MR applications that
can benefit the environment or increase awareness on environmental issues. We
performed an exhaustive search over several online publication access platforms
and past proceedings of major conferences in the fields of AR/VR/MR. Identified
relevant papers were filtered based on novelty, technical soundness, impact and
topic relevance, and classified into different categories. Referring to the
selected papers, we discuss how the applications of each category are
contributing to environmental protection, preservation and sensitization
purposes. We further analyse these approaches as well as possible future
directions in the scope of existing and upcoming AR/VR/MR enabling
technologies.","['Jason Rambach', 'Gergana Lilligreen', 'Alexander Schäfer', 'Ramya Bankanal', 'Alexander Wiebel', 'Didier Stricker']",2020-08-27T09:59:27Z,http://arxiv.org/abs/2008.12024v2,"['cs.HC', 'cs.CV', 'cs.CY', 'cs.GT']"
HoloGen: An open source toolbox for high-speed hologram generation,"The rise of mixed reality systems such as Microsoft HoloLens has prompted an
increase in interest in the fields of 2D and 3D holography. Already applied in
fields including telecommunications, imaging, projection, lithography, beam
shaping and optical tweezing, Computer Generated Holography (CGH) offers an
exciting approach to a wide range of light shaping problems. The numerical
processing required to generate a hologram is high and requires significant
domain expertise. This has historically slowed the adoption of holographic
techniques in emerging fields. In this paper we present HoloGen, an open-source
Cuda C and C ++ framework for computer generated holography. HoloGen unites,
for the first time, a wide array of existing hologram generation algorithms
with state of the art performance while attempting to remain intuitive and easy
to use. This is enabled by a C # and Windows Presentation Framework (WPF)
graphical user interface (GUI). A novel reflection based parameter hierarchy is
used to ensure ease of modification. Extensive use of C ++ templates based on
the Standard Template Library (STL), compile time flexibility is preserved
while maintaining runtime performance. The current release of HoloGen unites
implementations of well known generation algorithms including Gerchberg-Saxton
(GS), Liu-Taghizadeh (LT), Direct Search (DS), Simulated Annealing (SA) and
One-Step Phase-Retrieval (OSPR) with less known specialist variants including
Weighted GS and Adaptive OSPR. Benchmarking results are presented for several
key algorithms. The software is freely available under an MIT license.","['Peter J. Christopher', 'Andrew Kadis', 'George S. D. Gordon', 'Timothy D. Wilkinson']",2020-08-24T11:22:07Z,http://arxiv.org/abs/2008.12214v2,"['eess.IV', 'physics.optics']"
Cross-Descriptor Visual Localization and Mapping,"Visual localization and mapping is the key technology underlying the majority
of mixed reality and robotics systems. Most state-of-the-art approaches rely on
local features to establish correspondences between images. In this paper, we
present three novel scenarios for localization and mapping which require the
continuous update of feature representations and the ability to match across
different feature types. While localization and mapping is a fundamental
computer vision problem, the traditional setup supposes the same local features
are used throughout the evolution of a map. Thus, whenever the underlying
features are changed, the whole process is repeated from scratch. However, this
is typically impossible in practice, because raw images are often not stored
and re-building the maps could lead to loss of the attached digital content. To
overcome the limitations of current approaches, we present the first principled
solution to cross-descriptor localization and mapping. Our data-driven approach
is agnostic to the feature descriptor type, has low computational requirements,
and scales linearly with the number of description algorithms. Extensive
experiments demonstrate the effectiveness of our approach on state-of-the-art
benchmarks for a variety of handcrafted and learned features.","['Mihai Dusmanu', 'Ondrej Miksik', 'Johannes L. Schönberger', 'Marc Pollefeys']",2020-12-02T18:19:51Z,http://arxiv.org/abs/2012.01377v2,['cs.CV']
Evaluating User Experiences in Mixed Reality,"Measure user experience in MR (i.e., AR/VR) user studies is essential.
Researchers apply a wide range of measuring methods using objective (e.g.,
biosignals, time logging), behavioral (e.g., gaze direction, movement
amplitude), and subjective (e.g., standardized questionnaires) metrics. Many of
these measurement instruments were adapted from use-cases outside of MR but
have not been validated for usage in MR experiments. However, researchers are
faced with various challenges and design alternatives when measuring immersive
experiences. These challenges become even more diverse when running out-of-the
lab studies. Measurement methods of VR experience recently received much
attention. For example, research has started embedding questionnaires in the VE
for various applications, allowing users to stay closer to the ongoing
experience while filling out the survey. However, there is a diversity in the
interaction methods and practices on how the assessment procedure is conducted.
This diversity in methods underlines a missing shared agreement of standardized
measurement tools for VR experiences. AR research strongly orients on the
research methods from VR, e.g., using the same type of subjective
questionnaires. However, some crucial technical differences require careful
considerations during the evaluation. This workshop at CHI 2021 provides a
foundation to exchange expertise and address challenges and opportunities of
research methods in MR user studies. By this, our workshop launches a
discussion of research methods that should lead to standardizing assessment
methods in MR user studies. The outcomes of the workshop will be aggregated
into a collective special issue journal article.","['Dmitry Alexandrovsky', 'Susanne Putze', 'Valentin Schwind', 'Elisa D. Mekler', 'Jan David Smeddinck', 'Denise Kahl', 'Antonio Krüger', 'Rainer Malaka']",2021-01-16T12:38:01Z,http://arxiv.org/abs/2101.06444v1,['cs.HC']
"Defining Preferred and Natural Robot Motions in Immersive Telepresence
  from a First-Person Perspective","This paper presents some early work and future plans regarding how the
autonomous motions of a telepresence robot affect a person embodied in the
robot through a head-mounted display. We consider the preferences, comfort, and
the perceived naturalness of aspects of piecewise linear paths compared to the
same aspects on a smooth path. In a user study, thirty-six subjects (eighteen
females) watched panoramic videos of three different paths through a simulated
museum in virtual reality and responded to questionnaires regarding each path.
We found that comfort had a strong effect on path preference, and that the
subjective feeling of naturalness also had a strong effect on path preference,
even though people consider different things as natural. We describe a
categorization of the responses regarding the naturalness of the robot's motion
and provide a recommendation on how this can be applied more broadly. Although
immersive robotic telepresence is increasingly being used for remote education,
clinical care, and to assist people with disabilities or mobility
complications, the full potential of this technology is limited by issues
related to user experience. Our work addresses these shortcomings and will
enable the future personalization of telepresence experiences for the
improvement of overall remote communication and the enhancement of the feeling
of presence in a remote location.","['Katherine J. Mimnaugh', 'Markku Suomalainen', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-02-25T07:40:23Z,http://arxiv.org/abs/2102.12719v1,['cs.RO']
Experiences with User Studies in Augmented Reality,"The research field of augmented reality (AR) is of increasing popularity, as
seen, among others, in several recently published surveys. To produce further
advancements in AR, it is not only necessary to create new systems or
applications, but also to evaluate them. One important aspect in regards to the
evaluation is the general understanding of how users experience a given AR
application, which can also be seen by the increased number of papers focusing
on this topic that were published in the last years. With the steadily growing
understanding and development of AR in general, it is only a matter of time
until AR devices make the leap into the consumer market where such an in-depth
user understanding is even more essential. Thus, a better understanding of
factors that could influence the design and results of user experience studies
can help us to make them more robust and dependable in the future.
  In this position paper, we describe three challenges which researchers face
while designing and conducting AR users studies. We encountered these
challenges in our past and current research, including papers that focus on
perceptual studies of visualizations, interaction studies, and studies
exploring the use of AR applications and their design spaces.","['Marc Satkowski', 'Wolfgang Büschel', 'Raimund Dachselt']",2021-04-08T14:18:51Z,http://arxiv.org/abs/2104.03795v1,['cs.HC']
Neural RGB-D Surface Reconstruction,"Obtaining high-quality 3D reconstructions of room-scale scenes is of
paramount importance for upcoming applications in AR or VR. These range from
mixed reality applications for teleconferencing, virtual measuring, virtual
room planing, to robotic applications. While current volume-based view
synthesis methods that use neural radiance fields (NeRFs) show promising
results in reproducing the appearance of an object or scene, they do not
reconstruct an actual surface. The volumetric representation of the surface
based on densities leads to artifacts when a surface is extracted using
Marching Cubes, since during optimization, densities are accumulated along the
ray and are not used at a single sample point in isolation. Instead of this
volumetric representation of the surface, we propose to represent the surface
using an implicit function (truncated signed distance function). We show how to
incorporate this representation in the NeRF framework, and extend it to use
depth measurements from a commodity RGB-D sensor, such as a Kinect. In
addition, we propose a pose and camera refinement technique which improves the
overall reconstruction quality. In contrast to concurrent work on integrating
depth priors in NeRF which concentrates on novel view synthesis, our approach
is able to reconstruct high-quality, metrical 3D reconstructions.","['Dejan Azinović', 'Ricardo Martin-Brualla', 'Dan B Goldman', 'Matthias Nießner', 'Justus Thies']",2021-04-09T18:00:01Z,http://arxiv.org/abs/2104.04532v3,['cs.CV']
DeepCompress: Efficient Point Cloud Geometry Compression,"Point clouds are a basic data type that is increasingly of interest as 3D
content becomes more ubiquitous. Applications using point clouds include
virtual, augmented, and mixed reality and autonomous driving. We propose a more
efficient deep learning-based encoder architecture for point clouds compression
that incorporates principles from established 3D object detection and image
compression architectures. Through an ablation study, we show that
incorporating the learned activation function from Computational Efficient
Neural Image Compression (CENIC) and designing more parameter-efficient
convolutional blocks yields dramatic gains in efficiency and performance. Our
proposed architecture incorporates Generalized Divisive Normalization
activations and propose a spatially separable InceptionV4-inspired block. We
then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized
Full Bodies dataset to evaluate our model's performance. Our proposed
modifications outperform the baseline approaches by a small margin in terms of
Bjontegard delta rate and PSNR values, yet reduces necessary encoder
convolution operations by 8 percent and reduces total encoder parameters by 20
percent. Our proposed architecture, when considered on its own, has a small
penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit
rate in Point to Plane Distance for the same peak signal-to-noise ratio.","['Ryan Killea', 'Yun Li', 'Saeed Bastani', 'Paul McLachlan']",2021-06-02T23:18:11Z,http://arxiv.org/abs/2106.01504v1,"['cs.CV', 'cs.GR', 'cs.LG', 'eess.IV']"
"3D Visualisations Should Not be Displayed Alone - Encouraging a Need for
  Multivocality in Visualisation","We believe that 3D visualisations should not be used alone; by coincidentally
displaying alternative views the user can gain the best understanding of all
situations. The different presentations signify manifold meanings and afford
different tasks. Natural 3D worlds implicitly tell many stories. For instance,
walking into a living room, seeing the TV, types of magazines, pictures on the
wall, tells us much about the occupiers: their occupation, standards of living,
taste in design, whether they have kids, and so on. How can we similarly create
rich and diverse 3D visualisation presentations? How can we create
visualisations that allow people to understand different stories from the data?
In a multivariate 2D visualisation a developer may coordinate and link many
views together to provide exploratory visualisation functionality. But how can
this be achieved in 3D and in immersive visualisations? Different visualisation
types, each have specific uses, and each has the potential to tell or evoke a
different story. Through several use-cases, we discuss challenges of 3D
visualisation, and present our argument for concurrent and coordinated
visualisations of alternative styles, and encourage developers to consider
using alternative representations with any 3D view, even if that view is
displayed in a virtual, augmented or mixed reality setup.","['J. C. Roberts', 'J. W. Mearman', 'P. W. S. Butcher', 'H. M. Al-Maneea', 'P. D. Ritsos']",2021-08-10T13:37:04Z,http://arxiv.org/abs/2108.04680v1,"['cs.HC', 'cs.GR', 'I.3.0; H.5.2; I.6.3; J.2']"
"Towards Efficient Point Cloud Graph Neural Networks Through
  Architectural Simplification","In recent years graph neural network (GNN)-based approaches have become a
popular strategy for processing point cloud data, regularly achieving
state-of-the-art performance on a variety of tasks. To date, the research
community has primarily focused on improving model expressiveness, with
secondary thought given to how to design models that can run efficiently on
resource constrained mobile devices including smartphones or mixed reality
headsets. In this work we make a step towards improving the efficiency of these
models by making the observation that these GNN models are heavily limited by
the representational power of their first, feature extracting, layer. We find
that it is possible to radically simplify these models so long as the feature
extraction layer is retained with minimal degradation to model performance;
further, we discover that it is possible to improve performance overall on
ModelNet40 and S3DIS by improving the design of the feature extractor. Our
approach reduces memory consumption by 20$\times$ and latency by up to
9.9$\times$ for graph layers in models such as DGCNN; overall, we achieve
speed-ups of up to 4.5$\times$ and peak memory reductions of 72.5%.","['Shyam A. Tailor', 'René de Jong', 'Tiago Azevedo', 'Matthew Mattina', 'Partha Maji']",2021-08-13T17:04:54Z,http://arxiv.org/abs/2108.06317v1,"['cs.CV', 'cs.LG']"
"FreeStyleGAN: Free-view Editable Portrait Rendering with the Camera
  Manifold","Current Generative Adversarial Networks (GANs) produce photorealistic
renderings of portrait images. Embedding real images into the latent space of
such models enables high-level image editing. While recent methods provide
considerable semantic control over the (re-)generated images, they can only
generate a limited set of viewpoints and cannot explicitly control the camera.
Such 3D camera control is required for 3D virtual and mixed reality
applications. In our solution, we use a few images of a face to perform 3D
reconstruction, and we introduce the notion of the GAN camera manifold, the key
element allowing us to precisely define the range of images that the GAN can
reproduce in a stable manner. We train a small face-specific neural implicit
representation network to map a captured face to this manifold and complement
it with a warping scheme to obtain free-viewpoint novel-view synthesis. We show
how our approach - due to its precise camera control - enables the integration
of a pre-trained StyleGAN into standard 3D rendering pipelines, allowing e.g.,
stereo rendering or consistent insertion of faces in synthetic 3D environments.
Our solution proposes the first truly free-viewpoint rendering of realistic
faces at interactive rates, using only a small number of casual photos as
input, while simultaneously allowing semantic editing capabilities, such as
facial expression or lighting changes.","['Thomas Leimkühler', 'George Drettakis']",2021-09-20T08:59:21Z,http://arxiv.org/abs/2109.09378v1,"['cs.GR', 'cs.CV']"
"Struct-MRT: Immersive Learning and Teaching of Design and Verification
  in Structural Civil Engineering using Mixed Reality","Our goal is to transform traditional paper-based instruction into an
immersive lesson. This paper presents the conception, workflow and deployment
of two MR applications for verification of typical yet geometrically complex
structural members: a reinforced concrete corbel and a steel frame. The aim of
this research is threefold: (i) to develop and implement the technological
feasibility of such applications, (ii) to demonstrate possible use cases in the
context of structural engineering lectures and (iii) to evaluate the presented
MR examples and the future potential of such MR applications in structural
engineering lectures through a survey. The workflow and MR teaching
applications were developed with Apple's ARKit. The verification process was
reproduced in the MR applications based on conventional exercises taught on
paper. Users can navigate independently through the applications and review
every single step, including a true-to-scale, spatial representation of the
specific component as well as associated verification formulas in the
respective step. The applications were used to assess the demand and
expectations for immersive teaching techniques among students and instructors
through a survey. The participants were asked to test the MR applications on
their devices or watch pre-recorded video demonstrations, afterwards perception
was elicited through a questionnaire. The results of subsequent data analysis
show generally positive judgement of the MR application over the six questioned
categories (style, usefulness, ease of use, enjoyment, attitude as well as
intention towards using). The statistical analysis revealed (positivity) biases
for users with prior XR experience w.r.t. to usage and navigation, while
inexperienced users underlined increased enjoyment or excitement with this
learning format. The outlook covers identified shortcomings and future
developments in this field.","['Michael Kraus', 'Irfan Custovic', 'Walter Kaufmann']",2021-09-20T12:46:52Z,http://arxiv.org/abs/2109.09489v1,['cs.HC']
"Use of augmented and virtual reality tools in a general secondary
  education institution in the context of blended learning","The study examines the problem of using augmented and virtual reality in the
process of blended learning in general secondary education. The study analyzes
the meaning of the concept of ""blended learning"". The conceptual principles of
blended learning are considered. The definition of augmented and virtual
reality is given. The mixed reality is considered as a separate kind of notion.
Separate applications of virtual and augmented reality that can be used in the
process of blended learning are considered. As a result of the study, the
authors propose possible ways to use augmented reality in the educational
process. The model of using augmented and virtual reality in blended learning
in general secondary education institutions was designed. It consists of the
following blocks: goal; teacher's activity; forms of education; teaching
methods; teaching aids; organizational forms of education; pupil activity and
results. Based on the model, the methodology of using augmented and virtual
reality in blended learning in general secondary education was developed. The
methodology contains the following components: target component, content
component, technological component and resultant component. The methodology is
quite universal and can be used for any subject in general secondary education.
The types of lessons in which it is expedient to use augmented (AR) and virtual
reality(VR) are determined. Recommendations are given at which stage of the
lesson it is better to use AR and VR tools (depending on the type of lesson).","['Valentyna Kovalenko', 'Maiia Marienko', 'Alisa Sukhikh']",2022-01-13T16:54:36Z,http://arxiv.org/abs/2201.07003v1,['cs.CY']
"DeepMix: Mobility-aware, Lightweight, and Hybrid 3D Object Detection for
  Headsets","Mobile headsets should be capable of understanding 3D physical environments
to offer a truly immersive experience for augmented/mixed reality (AR/MR).
However, their small form-factor and limited computation resources make it
extremely challenging to execute in real-time 3D vision algorithms, which are
known to be more compute-intensive than their 2D counterparts. In this paper,
we propose DeepMix, a mobility-aware, lightweight, and hybrid 3D object
detection framework for improving the user experience of AR/MR on mobile
headsets. Motivated by our analysis and evaluation of state-of-the-art 3D
object detection models, DeepMix intelligently combines edge-assisted 2D object
detection and novel, on-device 3D bounding box estimations that leverage depth
data captured by headsets. This leads to low end-to-end latency and
significantly boosts detection accuracy in mobile scenarios. A unique feature
of DeepMix is that it fully exploits the mobility of headsets to fine-tune
detection results and boost detection accuracy. To the best of our knowledge,
DeepMix is the first 3D object detection that achieves 30 FPS (an end-to-end
latency much lower than the 100 ms stringent requirement of interactive AR/MR).
We implement a prototype of DeepMix on Microsoft HoloLens and evaluate its
performance via both extensive controlled experiments and a user study with 30+
participants. DeepMix not only improves detection accuracy by 9.1--37.3% but
also reduces end-to-end latency by 2.68--9.15x, compared to the baseline that
uses existing 3D object detection models.","['Yongjie Guan', 'Xueyu Hou', 'Nan Wu', 'Bo Han', 'Tao Han']",2022-01-15T05:50:18Z,http://arxiv.org/abs/2201.08812v2,['cs.CV']
"Standardization of Extended Reality (XR) over 5G and 5G-Advanced 3GPP
  New Radio","Extended Reality (XR) is one of the major innovations to be introduced in
5G/5G-Advanced communication systems. A combination of augmented reality,
virtual reality, and mixed reality, supplemented by cloud gaming, revisits the
way how humans interact with computers, networks, and each other. However,
efficient support of XR services imposes new challenges for existing and future
wireless networks. This article presents a tutorial on integrating support for
the XR into the 3GPP New Radio (NR), summarizing a range of activities handled
within various 3GPP Service and Systems Aspects (SA) and Radio Access Networks
(RAN) groups. The article also delivers a case study evaluating the performance
of different XR services in state-of-the-art NR Release 17. The paper concludes
with a vision of further enhancements to better support XR in future NR
releases and outlines open problems in this area.","['Margarita Gapeyenko', 'Vitaly Petrov', 'Stefano Paris', 'Andrea Marcano', 'Klaus I. Pedersen']",2022-03-04T11:17:34Z,http://arxiv.org/abs/2203.02242v3,['cs.NI']
Learning Online Multi-Sensor Depth Fusion,"Many hand-held or mixed reality devices are used with a single sensor for 3D
reconstruction, although they often comprise multiple sensors. Multi-sensor
depth fusion is able to substantially improve the robustness and accuracy of 3D
reconstruction methods, but existing techniques are not robust enough to handle
sensors which operate with diverse value ranges as well as noise and outlier
statistics. To this end, we introduce SenFuNet, a depth fusion approach that
learns sensor-specific noise and outlier statistics and combines the data
streams of depth frames from different sensors in an online fashion. Our method
fuses multi-sensor depth streams regardless of time synchronization and
calibration and generalizes well with little training data. We conduct
experiments with various sensor combinations on the real-world CoRBS and
Scene3D datasets, as well as the Replica dataset. Experiments demonstrate that
our fusion strategy outperforms traditional and recent online depth fusion
approaches. In addition, the combination of multiple sensors yields more robust
outlier handling and more precise surface reconstruction than the use of a
single sensor. The source code and data are available at
https://github.com/tfy14esa/SenFuNet.","['Erik Sandström', 'Martin R. Oswald', 'Suryansh Kumar', 'Silvan Weder', 'Fisher Yu', 'Cristian Sminchisescu', 'Luc Van Gool']",2022-04-07T10:45:32Z,http://arxiv.org/abs/2204.03353v2,['cs.CV']
BEHAVE: Dataset and Method for Tracking Human Object Interactions,"Modelling interactions between humans and objects in natural environments is
central to many applications including gaming, virtual and mixed reality, as
well as human behavior analysis and human-robot collaboration. This challenging
operation scenario requires generalization to vast number of objects, scenes,
and human actions. Unfortunately, there exist no such dataset. Moreover, this
data needs to be acquired in diverse natural environments, which rules out 4D
scanners and marker based capture systems. We present BEHAVE dataset, the first
full body human- object interaction dataset with multi-view RGBD frames and
corresponding 3D SMPL and object fits along with the annotated contacts between
them. We record around 15k frames at 5 locations with 8 subjects performing a
wide range of interactions with 20 common objects. We use this data to learn a
model that can jointly track humans and objects in natural environments with an
easy-to-use portable multi-camera setup. Our key insight is to predict
correspondences from the human and the object to a statistical body model to
obtain human-object contacts during interactions. Our approach can record and
track not just the humans and objects but also their interactions, modeled as
surface contacts, in 3D. Our code and data can be found at:
http://virtualhumans.mpi-inf.mpg.de/behave","['Bharat Lal Bhatnagar', 'Xianghui Xie', 'Ilya A. Petrov', 'Cristian Sminchisescu', 'Christian Theobalt', 'Gerard Pons-Moll']",2022-04-14T13:21:19Z,http://arxiv.org/abs/2204.06950v1,['cs.CV']
"Virtual and Augmented Reality-Based Assistive Interfaces for Upper-limb
  Prosthesis Control and Rehabilitation","Functional upper-limb prosthetic training can improve users performance in
controlling prostheses and has been incorporated into occupational therapy for
individuals in need. In recent years, virtual reality (VR) and augmented
reality (AR) technologies have been shown to be promising avenues to improve
the convenience of rehabilitative prosthesis training systems. However, it is
uncertain if the comprehensive efficacy and effectiveness of VR or AR assistive
tools are adequate compared to conventional prosthetic tools and if not,
whether enhancements can be made through incorporation of other technical
paradigms.
  This work first presents a mixed reality system we developed for prosthesis
control and training. Five able-bodied subjects are involved to perform
three-dimensional object manipulation tasks in analogous AR and VR
environments. Multiple evaluation metrics are applied to assess subjects
performances within the two paradigms. Based on the comparative analysis, we
find that VR-based environment promotes more efficient motion along with higher
task completion rate and path efficiency while AR paradigm allows subjects to
perform motor tasks with shorter time consumed. Another study is conducted to
evaluate the efficiency and feasibility of AR-facilitated prosthesis control
system compared to that in real-world and if any technical additions can be
applied to improve the AR-based system. Three able-bodied subjects were engaged
in the experiment to perform object manipulation tasks in a) physical
environment, b) AR-without-bypass environment, and c) AR-with-bypass
environment. Based on the results obtained from the assessment, we conclude
that while our AR-based system modestly lags behind the effectiveness of
physical systems, the study conducted using a bypass prosthesis suggests that
AR system has the potential to improve the efficacy of prosthesis control.",['Yinghe Sun'],2022-04-28T03:26:12Z,http://arxiv.org/abs/2205.02227v1,['cs.HC']
"Visual Guidance for User Placement in Avatar-Mediated Telepresence
  between Dissimilar Spaces","Rapid advances in technology gradually realize immersive mixed-reality (MR)
telepresence between distant spaces. This paper presents a novel visual
guidance system for avatar-mediated telepresence, directing users to optimal
placements that facilitate the clear transfer of gaze and pointing contexts
through remote avatars in dissimilar spaces, where the spatial relationship
between the remote avatar and the interaction targets may differ from that of
the local user. Representing the spatial relationship between the user/avatar
and interaction targets with angle-based interaction features, we assign
recommendation scores of sampled local placements as their maximum feature
similarity with remote placements. These scores are visualized as color-coded
2D sectors to inform the users of better placements for interaction with
selected targets. In addition, virtual objects of the remote space are
overlapped with the local space for the user to better understand the
recommendations. We examine whether the proposed score measure agrees with the
actual user perception of the partner's interaction context and find a score
threshold for recommendation through user experiments in virtual reality (VR).
A subsequent user study in VR investigates the effectiveness and perceptual
overload of different combinations of visualizations. Finally, we conduct a
user study in an MR telepresence scenario to evaluate the effectiveness of our
method in real-world applications.","['Dongseok Yang', 'Jiho Kang', 'Taehei Kim', 'Sung-Hee Lee']",2022-06-20T02:41:39Z,http://arxiv.org/abs/2206.09542v3,"['cs.HC', 'cs.GR']"
Inter-Frame Compression for Dynamic Point Cloud Geometry Coding,"Efficient point cloud compression is essential for applications like virtual
and mixed reality, autonomous driving, and cultural heritage. In this paper, we
propose a deep learning-based inter-frame encoding scheme for dynamic point
cloud geometry compression. We propose a lossy geometry compression scheme that
predicts the latent representation of the current frame using the previous
frame by employing a novel prediction network. Our proposed network utilizes
sparse convolutions with hierarchical multiscale 3D feature learning to encode
the current frame using the previous frame. We employ convolution on target
coordinates to map the latent representation of the previous frame to the
downsampled coordinates of the current frame to predict the current frame's
feature embedding. Our framework transmits the residual of the predicted
features and the actual features by compressing them using a learned
probabilistic factorized entropy model. At the receiver, the decoder
hierarchically reconstructs the current frame by progressively rescaling the
feature embedding. We compared our model to the state-of-the-art Video-based
Point Cloud Compression (V-PCC) and Geometry-based Point Cloud Compression
(G-PCC) schemes standardized by the Moving Picture Experts Group (MPEG). Our
method achieves more than 91% BD-Rate Bjontegaard Delta Rate) reduction against
G-PCC, more than 62% BD-Rate reduction against V-PCC intra-frame encoding mode,
and more than 52% BD-Rate savings against V-PCC P-frame-based inter-frame
encoding mode using HEVC.","['Anique Akhtar', 'Zhu Li', 'Geert Van der Auwera']",2022-07-25T22:17:19Z,http://arxiv.org/abs/2207.12554v1,"['cs.CV', 'cs.MM', 'eess.IV']"
"AvatarPoser: Articulated Full-Body Pose Tracking from Sparse Motion
  Sensing","Today's Mixed Reality head-mounted displays track the user's head pose in
world space as well as the user's hands for interaction in both Augmented
Reality and Virtual Reality scenarios. While this is adequate to support user
input, it unfortunately limits users' virtual representations to just their
upper bodies. Current systems thus resort to floating avatars, whose limitation
is particularly evident in collaborative settings. To estimate full-body poses
from the sparse input sources, prior work has incorporated additional trackers
and sensors at the pelvis or lower body, which increases setup complexity and
limits practical application in mobile settings. In this paper, we present
AvatarPoser, the first learning-based method that predicts full-body poses in
world coordinates using only motion input from the user's head and hands. Our
method builds on a Transformer encoder to extract deep features from the input
signals and decouples global motion from the learned local joint orientations
to guide pose estimation. To obtain accurate full-body motions that resemble
motion capture animations, we refine the arm joints' positions using an
optimization routine with inverse kinematics to match the original tracking
input. In our evaluation, AvatarPoser achieved new state-of-the-art results in
evaluations on large motion capture datasets (AMASS). At the same time, our
method's inference speed supports real-time operation, providing a practical
interface to support holistic avatar control and representation for Metaverse
applications.","['Jiaxi Jiang', 'Paul Streli', 'Huajian Qiu', 'Andreas Fender', 'Larissa Laich', 'Patrick Snape', 'Christian Holz']",2022-07-27T20:52:39Z,http://arxiv.org/abs/2207.13784v1,"['cs.CV', 'cs.AI', 'cs.GR', 'cs.HC', '68T07, 68T45, 68U01', 'I.2; I.3; I.4; I.5']"
Harmonization and Evaluation; Tweaking the Parameters on Human Listeners,"Kansei models were used to study the connotative meaning of music. In
multimedia and mixed reality, automatically generated melodies are increasingly
being used. It is important to consider whether and what feelings are
communicated by this music. Evaluation of computer-generated melodies is not a
trivial task. Considered the difficulty of defining useful quantitative metrics
of the quality of a generated musical piece, researchers often resort to human
evaluation. In these evaluations, often the judges are required to evaluate a
set of generated pieces along with some benchmark pieces. The latter are often
composed by humans. While this kind of evaluation is relatively common, it is
known that care should be taken when designing the experiment, as humans can be
influenced by a variety of factors. In this paper, we examine the impact of the
presence of harmony in audio files that judges must evaluate, to see whether
having an accompaniment can change the evaluation of generated melodies. To do
so, we generate melodies with two different algorithms and harmonize them with
an automatic tool that we designed for this experiment, and ask more than sixty
participants to evaluate the melodies. By using statistical analyses, we show
harmonization does impact the evaluation process, by emphasizing the
differences among judgements.","['Filippo Carnovalini', 'Alessandro Pelizzo', 'Antonio Rodà', 'Sergio Canazza']",2022-08-31T09:54:06Z,http://arxiv.org/abs/2208.14750v1,"['cs.SD', 'cs.MM', 'eess.AS']"
"Haptic Feedback Relocation from the Fingertips to the Wrist for
  Two-Finger Manipulation in Virtual Reality","Relocation of haptic feedback from the fingertips to the wrist has been
considered as a way to enable haptic interaction with mixed reality virtual
environments while leaving the fingers free for other tasks. We present a pair
of wrist-worn tactile haptic devices and a virtual environment to study how
various mappings between fingers and tactors affect task performance. The
haptic feedback rendered to the wrist reflects the interaction forces occurring
between a virtual object and virtual avatars controlled by the index finger and
thumb. We performed a user study comparing four different finger-to-tactor
haptic feedback mappings and one no-feedback condition as a control. We
evaluated users' ability to perform a simple pick-and-place task via the
metrics of task completion time, path length of the fingers and virtual cube,
and magnitudes of normal and shear forces at the fingertips. We found that
multiple mappings were effective, and there was a greater impact when visual
cues were limited. We discuss the limitations of our approach and describe next
steps toward multi-degree-of-freedom haptic rendering for wrist-worn devices to
improve task performance in virtual environments.","['Jasmin E. Palmer', 'Mine Sarac', 'Aaron A. Garza', 'Allison M. Okamura']",2022-09-15T22:46:25Z,http://arxiv.org/abs/2209.07640v2,"['cs.HC', 'cs.RO']"
"A Novel Light Field Coding Scheme Based on Deep Belief Network &
  Weighted Binary Images for Additive Layered Displays","Light-field displays create an immersive experience by providing binocular
depth sensation and motion parallax. Stacking light attenuating layers is one
approach to implement a light field display with a broader depth of field, wide
viewing angles and high resolution. Due to the transparent holographic optical
element (HOE) layers, additive layered displays can be integrated into
augmented reality (AR) wearables to overlay virtual objects onto the real
world, creating a seamless mixed reality (XR) experience. This paper proposes a
novel framework for light field representation and coding that utilizes Deep
Belief Network (DBN) and weighted binary images suitable for additive layered
displays. The weighted binary representation of layers makes the framework more
flexible for adaptive bitrate encoding. The framework effectively captures
intrinsic redundancies in the light field data, and thus provides a scalable
solution for light field coding suitable for XR display applications. The
latent code is encoded by H.265 codec generating a rate-scalable bit-stream. We
achieve adaptive bitrate decoding by varying the number of weighted binary
images and the H.265 quantization parameter, while maintaining an optimal
reconstruction quality. The framework is tested on real and synthetic benchmark
datasets, and the results validate the rate-scalable property of the proposed
scheme.","['Sally Khaidem', 'Mansi Sharma']",2022-10-04T08:18:06Z,http://arxiv.org/abs/2210.01447v2,"['cs.CV', 'eess.IV']"
"A DirectX-Based DICOM Viewer for Multi-User Surgical Planning in
  Augmented Reality","Preoperative medical imaging is an essential part of surgical planning. The
data from medical imaging devices, such as CT and MRI scanners, consist of
stacks of 2D images in DICOM format. Conversely, advances in 3D data
visualization provide further information by assembling cross-sections into 3D
volumetric datasets. As Microsoft unveiled the HoloLens 2 (HL2), which is
considered one of the best Mixed Reality (XR) headsets in the market, it
promised to enhance visualization in 3D by providing an immersive experience to
users. This paper introduces a prototype holographic XR DICOM Viewer for the 3D
visualization of DICOM image sets on HL2 for surgical planning. We first
developed a standalone graphical C++ engine using the native DirectX11 API and
HLSL shaders. Based on that, the prototype further applies the OpenXR API for
potential deployment on a wide range of devices from vendors across the XR
spectrum. With native access to the device, our prototype unravels the
limitation of hardware capabilities on HL2 for 3D volume rendering and
interaction. Moreover, smartphones can act as input devices to provide another
user interaction method by connecting to our server. In this paper, we present
a holographic DICOM viewer for the HoloLens 2 and contribute (i) a prototype
that renders the DICOM image stacks in real-time on HL2, (ii) three types of
user interactions in XR, and (iii) a preliminary qualitative evaluation of our
prototype.","['Menghe Zhang', 'Weichen Liu', 'Nadir Weibel', 'Jurgen Schulze']",2022-10-25T21:22:00Z,http://arxiv.org/abs/2210.14349v1,"['cs.MM', 'cs.HC']"
Big Data Meets Metaverse: A Survey,"We are living in the era of big data. The Metaverse is an emerging technology
in the future, and it has a combination of big data, AI (artificial
intelligence), VR (Virtual Reality), AR (Augmented Reality), MR (mixed
reality), and other technologies that will diminish the difference between
online and real-life interaction. It has the goal of becoming a platform where
we can work, go shopping, play around, and socialize. Each user who enters the
Metaverse interacts with the virtual world in a data way. With the development
and application of the Metaverse, the data will continue to grow, thus forming
a big data network, which will bring huge data processing pressure to the
digital world. Therefore, big data processing technology is one of the key
technologies to implement the Metaverse. In this survey, we provide a
comprehensive review of how Metaverse is changing big data. Moreover, we
discuss the key security and privacy of Metaverse big data in detail. Finally,
we summarize the open problems and opportunities of Metaverse, as well as the
future of Metaverse with big data. We hope that this survey will provide
researchers with the research direction and prospects of applying big data in
the Metaverse.","['Jiayi Sun', 'Wensheng Gan', 'Zefeng Chen', 'Junhui Li', 'Philip S. Yu']",2022-10-28T17:22:20Z,http://arxiv.org/abs/2210.16282v1,"['cs.DB', 'cs.CY']"
Analyzing Performance Issues of Virtual Reality Applications,"Extended Reality (XR) includes Virtual Reality (VR), Augmented Reality (AR)
and Mixed Reality (MR). XR is an emerging technology that simulates a realistic
environment for users. XR techniques have provided revolutionary user
experiences in various application scenarios (e.g., training, education,
product/architecture design, gaming, remote conference/tour, etc.). Due to the
high computational cost of rendering real-time animation in limited-resource
devices and constant interaction with user activity, XR applications often face
performance bottlenecks, and these bottlenecks create a negative impact on the
user experience of XR software. Thus, performance optimization plays an
essential role in many industry-standard XR applications. Even though
identifying performance bottlenecks in traditional software (e.g., desktop
applications) is a widely explored topic, those approaches cannot be directly
applied within XR software due to the different nature of XR applications.
Moreover, XR applications developed in different frameworks such as Unity and
Unreal Engine show different performance bottleneck patterns and thus,
bottleneck patterns of Unity projects can't be applied for Unreal Engine
(UE)-based XR projects. To fill the knowledge gap for XR performance
optimizations of Unreal Engine-based XR projects, we present the first
empirical study on performance optimizations from seven UE XR projects, 78 UE
XR discussion issues and three sources of UE documentation. Our analysis
identified 14 types of performance bugs, including 12 types of bugs related to
UE settings issues and two types of CPP source code-related issues. To further
assist developers in detecting performance bugs based on the identified bug
patterns, we also developed a static analyzer, UEPerfAnalyzer, that can detect
performance bugs in both configuration files and source code.","['Jason Hogan', 'Aaron Salo', 'Dhia Elhaq Rzig', 'Foyzul Hassan', 'Bruce Maxim']",2022-11-03T17:27:36Z,http://arxiv.org/abs/2211.02013v1,['cs.SE']
Twin-S: A Digital Twin for Skull-base Surgery,"Purpose: Digital twins are virtual interactive models of the real world,
exhibiting identical behavior and properties. In surgical applications,
computational analysis from digital twins can be used, for example, to enhance
situational awareness. Methods: We present a digital twin framework for
skull-base surgeries, named Twin-S, which can be integrated within various
image-guided interventions seamlessly. Twin-S combines high-precision optical
tracking and real-time simulation. We rely on rigorous calibration routines to
ensure that the digital twin representation precisely mimics all real-world
processes. Twin-S models and tracks the critical components of skull-base
surgery, including the surgical tool, patient anatomy, and surgical camera.
Significantly, Twin-S updates and reflects real-world drilling of the
anatomical model in frame rate. Results: We extensively evaluate the accuracy
of Twin-S, which achieves an average 1.39 mm error during the drilling process.
We further illustrate how segmentation masks derived from the continuously
updated digital twin can augment the surgical microscope view in a mixed
reality setting, where bone requiring ablation is highlighted to provide
surgeons additional situational awareness. Conclusion: We present Twin-S, a
digital twin environment for skull-base surgery. Twin-S tracks and updates the
virtual model in real-time given measurements from modern tracking
technologies. Future research on complementing optical tracking with
higher-precision vision-based approaches may further increase the accuracy of
Twin-S.","['Hongchao Shu', 'Ruixing Liang', 'Zhaoshuo Li', 'Anna Goodridge', 'Xiangyu Zhang', 'Hao Ding', 'Nimesh Nagururu', 'Manish Sahu', 'Francis X. Creighton', 'Russell H. Taylor', 'Adnan Munawar', 'Mathias Unberath']",2022-11-21T21:33:51Z,http://arxiv.org/abs/2211.11863v2,"['cs.HC', 'cs.CV', 'cs.RO']"
"Performance Analysis of Free-Space Information Sharing in Full-Duplex
  Semantic Communications","In next-generation Internet services, such as Metaverse, the mixed reality
(MR) technique plays a vital role. Yet the limited computing capacity of the
user-side MR headset-mounted device (HMD) prevents its further application,
especially in scenarios that require a lot of computation. One way out of this
dilemma is to design an efficient information sharing scheme among users to
replace the heavy and repetitive computation. In this paper, we propose a
free-space information sharing mechanism based on full-duplex device-to-device
(D2D) semantic communications. Specifically, the view images of MR users in the
same real-world scenario may be analogous. Therefore, when one user (i.e., a
device) completes some computation tasks, the user can send his own calculation
results and the semantic features extracted from the user's own view image to
nearby users (i.e., other devices). On this basis, other users can use the
received semantic features to obtain the spatial matching of the computational
results under their own view images without repeating the computation. Using
generalized small-scale fading models, we analyze the key performance
indicators of full-duplex D2D communications, including channel capacity and
bit error probability, which directly affect the transmission of semantic
information. Finally, the numerical analysis experiment proves the
effectiveness of our proposed methods.","['Hongyang Du', 'Jiacheng Wang', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Dong In Kim', 'Boon Hee Soong']",2022-11-27T09:18:42Z,http://arxiv.org/abs/2211.14771v1,['eess.SP']
Fast and Lightweight Scene Regressor for Camera Relocalization,"Camera relocalization involving a prior 3D reconstruction plays a crucial
role in many mixed reality and robotics applications. Estimating the camera
pose directly with respect to pre-built 3D models can be prohibitively
expensive for several applications with limited storage and/or communication
bandwidth. Although recent scene and absolute pose regression methods have
become popular for efficient camera localization, most of them are
computation-resource intensive and difficult to obtain a real-time inference
with high accuracy constraints. This study proposes a simple scene regression
method that requires only a multi-layer perceptron network for mapping scene
coordinates to achieve accurate camera pose estimations. The proposed approach
uses sparse descriptors to regress the scene coordinates, instead of a dense
RGB image. The use of sparse features provides several advantages. First, the
proposed regressor network is substantially smaller than those reported in
previous studies. This makes our system highly efficient and scalable. Second,
the pre-built 3D models provide the most reliable and robust 2D-3D matches.
Therefore, learning from them can lead to an awareness of equivalent features
and substantially improve the generalization performance. A detailed analysis
of our approach and extensive evaluations using existing datasets are provided
to support the proposed method. The implementation detail is available at
https://github.com/aislab/feat2map","['Thuan B. Bui', 'Dinh-Tuan Tran', 'Joo-Ho Lee']",2022-12-04T14:41:20Z,http://arxiv.org/abs/2212.01830v1,"['cs.CV', 'cs.RO']"
"CNN-based real-time 2D-3D deformable registration from a single X-ray
  projection","Purpose: The purpose of this paper is to present a method for real-time 2D-3D
non-rigid registration using a single fluoroscopic image. Such a method can
find applications in surgery, interventional radiology and radiotherapy. By
estimating a three-dimensional displacement field from a 2D X-ray image,
anatomical structures segmented in the preoperative scan can be projected onto
the 2D image, thus providing a mixed reality view. Methods: A dataset composed
of displacement fields and 2D projections of the anatomy is generated from the
preoperative scan. From this dataset, a neural network is trained to recover
the unknown 3D displacement field from a single projection image. Results: Our
method is validated on lung 4D CT data at different stages of the lung
deformation. The training is performed on a 3D CT using random (non
domain-specific) diffeomorphic deformations, to which perturbations mimicking
the pose uncertainty are added. The model achieves a mean TRE over a series of
landmarks ranging from 2.3 to 5.5 mm depending on the amplitude of deformation.
Conclusion: In this paper, a CNN-based method for real-time 2D-3D non-rigid
registration is presented. This method is able to cope with pose estimation
uncertainties, making it applicable to actual clinical scenarios, such as lung
surgery, where the C-arm pose is planned before the intervention.","['François Lecomte', 'Jean-Louis Dillenseger', 'Stéphane Cotin']",2022-12-15T09:57:19Z,http://arxiv.org/abs/2212.07692v2,"['eess.IV', 'cs.CV']"
"Playing with Data: An Augmented Reality Approach to Interact with
  Visualizations of Industrial Process Tomography","Industrial process tomography (IPT) is a specialized imaging technique widely
used in industrial scenarios for process supervision and control. Today,
augmented/mixed reality (AR/MR) is increasingly being adopted in many
industrial occasions, even though there is still an obvious gap when it comes
to IPT. To bridge this gap, we propose the first systematic AR approach using
optical see-through (OST) head mounted displays (HMDs) with comparative
evaluation for domain users towards IPT visualization analysis. The
proof-of-concept was demonstrated by a within-subject user study (n=20) with
counterbalancing design. Both qualitative and quantitative measurements were
investigated. The results showed that our AR approach outperformed conventional
settings for IPT data visualization analysis in bringing higher
understandability, reduced task completion time, lower error rates for domain
tasks, increased usability with enhanced user experience, and a better
recommendation level. We summarize the findings and suggest future research
directions for benefiting IPT users with AR/MR.","['Yuchong Zhang', 'Yueming Xuan', 'Rahul Yadav', 'Adel Omrani', 'Morten Fjeld']",2023-02-03T12:19:01Z,http://arxiv.org/abs/2302.01686v5,['cs.HC']
"KuberneTSN: a Deterministic Overlay Network for Time-Sensitive
  Containerized Environments","The emerging paradigm of resource disaggregation enables the deployment of
cloud-like services across a pool of physical and virtualized resources,
interconnected using a network fabric. This design embodies several benefits in
terms of resource efficiency and cost-effectiveness, service elasticity and
adaptability, etc. Application domains benefiting from such a trend include
cyber-physical systems (CPS), tactile internet, 5G networks and beyond, or
mixed reality applications, all generally embodying heterogeneous Quality of
Service (QoS) requirements. In this context, a key enabling factor to fully
support those mixed-criticality scenarios will be the network and the
system-level support for time-sensitive communication. Although a lot of work
has been conducted on devising efficient orchestration and CPU scheduling
strategies, the networking aspects of performance-critical components remain
largely unstudied. Bridging this gap, we propose KuberneTSN, an original
solution built on the Kubernetes platform, providing support for time-sensitive
traffic to unmodified application binaries. We define an architecture for an
accelerated and deterministic overlay network, which includes kernel-bypassing
networking features as well as a novel userspace packet scheduler compliant
with the Time-Sensitive Networking (TSN) standard. The solution is implemented
as tsn-cni, a Kubernetes network plugin that can coexist alongside popular
alternatives. To assess the validity of the approach, we conduct an
experimental analysis on a real distributed testbed, demonstrating that
KuberneTSN enables applications to easily meet deterministic deadlines,
provides the same guarantees of bare-metal deployments, and outperforms overlay
networks built using the Flannel plugin.","['Andrea Garbugli', 'Lorenzo Rosa', 'Armir Bujari', 'Luca Foschini']",2023-02-16T16:16:28Z,http://arxiv.org/abs/2302.08398v1,"['cs.NI', 'cs.DC']"
"AI-Generated Incentive Mechanism and Full-Duplex Semantic Communications
  for Information Sharing","The next generation of Internet services, such as Metaverse, rely on mixed
reality (MR) technology to provide immersive user experiences. However, the
limited computation power of MR headset-mounted devices (HMDs) hinders the
deployment of such services. Therefore, we propose an efficient information
sharing scheme based on full-duplex device-to-device (D2D) semantic
communications to address this issue. Our approach enables users to avoid heavy
and repetitive computational tasks, such as artificial intelligence-generated
content (AIGC) in the view images of all MR users. Specifically, a user can
transmit the generated content and semantic information extracted from their
view image to nearby users, who can then use this information to obtain the
spatial matching of computation results under their view images. We analyze the
performance of full-duplex D2D communications, including the achievable rate
and bit error probability, by using generalized small-scale fading models. To
facilitate semantic information sharing among users, we design a contract
theoretic AI-generated incentive mechanism. The proposed diffusion model
generates the optimal contract design, outperforming two deep reinforcement
learning algorithms, i.e., proximal policy optimization and soft actor-critic
algorithms. Our numerical analysis experiment proves the effectiveness of our
proposed methods. The code for this paper is available at
https://github.com/HongyangDu/SemSharing","['Hongyang Du', 'Jiacheng Wang', 'Dusit Niyato', 'Jiawen Kang', 'Zehui Xiong', 'Dong In Kim']",2023-03-03T12:47:34Z,http://arxiv.org/abs/2303.01896v2,['eess.SP']
Modular 3D Interface Design for Accessible VR Applications,"Designed with an accessible first design approach, the presented paper
describes how exploiting humans proprioception ability in 3D space can result
in a more natural interaction experience when using a 3D graphical user
interface in a virtual environment. The modularity of the designed interface
empowers the user to decide where they want to place interface elements in 3D
space allowing for a highly customizable experience, both in the context of the
player and the virtual space. Drawing inspiration from todays tangible
interfaces used, such as those in aircraft cockpits, a modular interface is
presented taking advantage of our natural understanding of interacting with 3D
objects and exploiting capabilities that otherwise have not been used in 2D
interaction. Additionally, the designed interface supports multimodal input
mechanisms which also demonstrates the opportunity for the design to cross over
to augmented reality applications. A focus group study was completed to better
understand the usability and constraints of the designed 3D GUI.","['Corrie Green', 'Dr Yang Jiang', 'Dr John Isaacs']",2023-04-08T17:07:46Z,http://arxiv.org/abs/2304.10541v2,['cs.HC']
"Meta-Optimization for Higher Model Generalizability in Single-Image
  Depth Prediction","Model generalizability to unseen datasets, concerned with in-the-wild
robustness, is less studied for indoor single-image depth prediction. We
leverage gradient-based meta-learning for higher generalizability on zero-shot
cross-dataset inference. Unlike the most-studied image classification in
meta-learning, depth is pixel-level continuous range values, and mappings from
each image to depth vary widely across environments. Thus no explicit task
boundaries exist. We instead propose fine-grained task that treats each RGB-D
pair as a task in our meta-optimization. We first show meta-learning on limited
data induces much better prior (max +29.4\%). Using meta-learned weights as
initialization for following supervised learning, without involving extra data
or information, it consistently outperforms baselines without the method.
Compared to most indoor-depth methods that only train/ test on a single
dataset, we propose zero-shot cross-dataset protocols, closely evaluate
robustness, and show consistently higher generalizability and accuracy by our
meta-initialization. The work at the intersection of depth and meta-learning
potentially drives both research streams to step closer to practical use.","['Cho-Ying Wu', 'Yiqi Zhong', 'Junying Wang', 'Ulrich Neumann']",2023-05-12T06:17:13Z,http://arxiv.org/abs/2305.07269v2,['cs.CV']
Enhancing Transformer Backbone for Egocentric Video Action Segmentation,"Egocentric temporal action segmentation in videos is a crucial task in
computer vision with applications in various fields such as mixed reality,
human behavior analysis, and robotics. Although recent research has utilized
advanced visual-language frameworks, transformers remain the backbone of action
segmentation models. Therefore, it is necessary to improve transformers to
enhance the robustness of action segmentation models. In this work, we propose
two novel ideas to enhance the state-of-the-art transformer for action
segmentation. First, we introduce a dual dilated attention mechanism to
adaptively capture hierarchical representations in both local-to-global and
global-to-local contexts. Second, we incorporate cross-connections between the
encoder and decoder blocks to prevent the loss of local context by the decoder.
We also utilize state-of-the-art visual-language representation learning
techniques to extract richer and more compact features for our transformer. Our
proposed approach outperforms other state-of-the-art methods on the Georgia
Tech Egocentric Activities (GTEA) and HOI4D Office Tools datasets, and we
validate our introduced components with ablation studies. The source code and
supplementary materials are publicly available on
https://www.sail-nu.com/dxformer.","['Sakib Reza', 'Balaji Sundareshan', 'Mohsen Moghaddam', 'Octavia Camps']",2023-05-19T01:00:08Z,http://arxiv.org/abs/2305.11365v2,['cs.CV']
"Design Frameworks for Hyper-Connected Social XRI Immersive Metaverse
  Environments","The metaverse refers to the merger of technologies for providing a digital
twin of the real world and the underlying connectivity and interactions for the
many kinds of agents within. As this set of technology paradigms - involving
artificial intelligence, mixed reality, the internet-of-things and others -
gains in scale, maturity, and utility there are rapidly emerging design
challenges and new research opportunities. In particular is the metaverse
disconnect problem, the gap in task switching that inevitably occurs when a
user engages with multiple virtual and physical environments simultaneously.
Addressing this gap remains an open issue that affects the user experience and
must be overcome to increase overall utility of the metaverse. This article
presents design frameworks that consider how to address the metaverse as a
hyper-connected meta-environment that connects and expands multiple user
environments, modalities, contexts, and the many objects and relationships
within them. This article contributes to i) a framing of the metaverse as a
social XR-IoT (XRI) concept, ii) design Considerations for XRI metaverse
experiences, iii) a design architecture for social multi-user XRI metaverse
environments, and iv) descriptive exploration of social interaction scenarios
within XRI multi-user metaverses. These contribute a new design framework for
metaverse researchers and creators to consider the coming wave of
interconnected and immersive smart environments.","['Jie Guan', 'Alexis Morris']",2023-06-09T20:02:26Z,http://arxiv.org/abs/2306.06230v3,['cs.HC']
"Towards Ubiquitous Semantic Metaverse: Challenges, Approaches, and
  Opportunities","In recent years, ubiquitous semantic Metaverse has been studied to
revolutionize immersive cyber-virtual experiences for augmented reality (AR)
and virtual reality (VR) users, which leverages advanced semantic understanding
and representation to enable seamless, context-aware interactions within
mixed-reality environments. This survey focuses on the intelligence and
spatio-temporal characteristics of four fundamental system components in
ubiquitous semantic Metaverse, i.e., artificial intelligence (AI),
spatio-temporal data representation (STDR), semantic Internet of Things (SIoT),
and semantic-enhanced digital twin (SDT). We thoroughly survey the
representative techniques of the four fundamental system components that enable
intelligent, personalized, and context-aware interactions with typical use
cases of the ubiquitous semantic Metaverse, such as remote education, work and
collaboration, entertainment and socialization, healthcare, and e-commerce
marketing. Furthermore, we outline the opportunities for constructing the
future ubiquitous semantic Metaverse, including scalability and
interoperability, privacy and security, performance measurement and
standardization, as well as ethical considerations and responsible AI.
Addressing those challenges is important for creating a robust, secure, and
ethically sound system environment that offers engaging immersive experiences
for the users and AR/VR applications.","['Kai Li', 'Billy Pik Lik Lau', 'Xin Yuan', 'Wei Ni', 'Mohsen Guizani', 'Chau Yuen']",2023-07-13T11:14:46Z,http://arxiv.org/abs/2307.06687v2,"['cs.HC', 'cs.AI', 'cs.NI']"
"POV-Surgery: A Dataset for Egocentric Hand and Tool Pose Estimation
  During Surgical Activities","The surgical usage of Mixed Reality (MR) has received growing attention in
areas such as surgical navigation systems, skill assessment, and robot-assisted
surgeries. For such applications, pose estimation for hand and surgical
instruments from an egocentric perspective is a fundamental task and has been
studied extensively in the computer vision field in recent years. However, the
development of this field has been impeded by a lack of datasets, especially in
the surgical field, where bloody gloves and reflective metallic tools make it
hard to obtain 3D pose annotations for hands and objects using conventional
methods. To address this issue, we propose POV-Surgery, a large-scale,
synthetic, egocentric dataset focusing on pose estimation for hands with
different surgical gloves and three orthopedic surgical instruments, namely
scalpel, friem, and diskplacer. Our dataset consists of 53 sequences and 88,329
frames, featuring high-resolution RGB-D video streams with activity
annotations, accurate 3D and 2D annotations for hand-object pose, and 2D
hand-object segmentation masks. We fine-tune the current SOTA methods on
POV-Surgery and further show the generalizability when applying to real-life
cases with surgical gloves and tools by extensive evaluations. The code and the
dataset are publicly available at batfacewayne.github.io/POV_Surgery_io/.","['Rui Wang', 'Sophokles Ktistakis', 'Siwei Zhang', 'Mirko Meboldt', 'Quentin Lohmeyer']",2023-07-19T18:00:32Z,http://arxiv.org/abs/2307.10387v1,['cs.CV']
"A Review of Gaps between Web 4.0 and Web 3.0 Intelligent Network
  Infrastructure","World Wide Web is speeding up its pace into an intelligent and decentralized
ecosystem, as seen in the campaign of Web 3.0 and forthcoming Web 4.0. Marked
by the Europe Commission's latest mention of Web 4.0, a race towards strategic
Web 4.0 success has started. Web 4.0 is committed to bringing the next
technological transition with an open, secure, trustworthy fairness and digital
ecosystem for individuals and businesses in private and public sectors. Despite
overlapping scopes and objectives of Web 3.0 and Web 4.0 from academic and
industrial perspectives, there are distinct and definitive features and gaps
for the next generation of WWW. In this review, a brief introduction to WWW
development unravels the entangled but consistent requirement of a more vivid
web experience, enhancing human-centric experience in both societal and
technical aspects. Moreover, the review brings a decentralized intelligence
prospect of view on native AI entities for Web 4.0, envisioning sustainable,
autonomous and decentralized AI services for the entire Web 4.0 environment,
powering a self-sustainable Decentralized Physical and Software Infrastructure
for Computing Force Network, Semantic Network, Virtual/Mixed Reality, and
Privacy-preserving content presumption.
  The review aims to reveal that Web 4.0 offers native intelligence with
focused thinking on utilizing decentralized physical infrastructure, in
addition to sole requirements on decentralization, bridging the gap between Web
4.0 and Web 3.0 advances with the latest future-shaping blockchain-enabled
computing and network routing protocols.","['Zihan Zhou', 'Zihao Li', 'Xiaoshuai Zhang', 'Yunqing Sun', 'Hao Xu']",2023-08-06T02:49:35Z,http://arxiv.org/abs/2308.02996v1,"['cs.NI', 'cs.CY', 'cs.DC']"
"GRIP: Generating Interaction Poses Using Latent Consistency and Spatial
  Cues","Hands are dexterous and highly versatile manipulators that are central to how
humans interact with objects and their environment. Consequently, modeling
realistic hand-object interactions, including the subtle motion of individual
fingers, is critical for applications in computer graphics, computer vision,
and mixed reality. Prior work on capturing and modeling humans interacting with
objects in 3D focuses on the body and object motion, often ignoring hand pose.
In contrast, we introduce GRIP, a learning-based method that takes, as input,
the 3D motion of the body and the object, and synthesizes realistic motion for
both hands before, during, and after object interaction. As a preliminary step
before synthesizing the hand motion, we first use a network, ANet, to denoise
the arm motion. Then, we leverage the spatio-temporal relationship between the
body and the object to extract two types of novel temporal interaction cues,
and use them in a two-stage inference pipeline to generate the hand motion. In
the first stage, we introduce a new approach to enforce motion temporal
consistency in the latent space (LTC), and generate consistent interaction
motions. In the second stage, GRIP generates refined hand poses to avoid
hand-object penetrations. Given sequences of noisy body and object motion, GRIP
upgrades them to include hand-object interaction. Quantitative experiments and
perceptual studies demonstrate that GRIP outperforms baseline methods and
generalizes to unseen objects and motions from different motion-capture
datasets.","['Omid Taheri', 'Yi Zhou', 'Dimitrios Tzionas', 'Yang Zhou', 'Duygu Ceylan', 'Soren Pirk', 'Michael J. Black']",2023-08-22T17:59:51Z,http://arxiv.org/abs/2308.11617v1,['cs.CV']
"Stereo Matching in Time: 100+ FPS Video Stereo Matching for Extended
  Reality","Real-time Stereo Matching is a cornerstone algorithm for many Extended
Reality (XR) applications, such as indoor 3D understanding, video pass-through,
and mixed-reality games. Despite significant advancements in deep stereo
methods, achieving real-time depth inference with high accuracy on a low-power
device remains a major challenge. One of the major difficulties is the lack of
high-quality indoor video stereo training datasets captured by head-mounted
VR/AR glasses. To address this issue, we introduce a novel video stereo
synthetic dataset that comprises photorealistic renderings of various indoor
scenes and realistic camera motion captured by a 6-DoF moving VR/AR
head-mounted display (HMD). This facilitates the evaluation of existing
approaches and promotes further research on indoor augmented reality scenarios.
Our newly proposed dataset enables us to develop a novel framework for
continuous video-rate stereo matching.
  As another contribution, our dataset enables us to proposed a new video-based
stereo matching approach tailored for XR applications, which achieves real-time
inference at an impressive 134fps on a standard desktop computer, or 30fps on a
battery-powered HMD. Our key insight is that disparity and contextual
information are highly correlated and redundant between consecutive stereo
frames. By unrolling an iterative cost aggregation in time (i.e. in the
temporal dimension), we are able to distribute and reuse the aggregated
features over time. This approach leads to a substantial reduction in
computation without sacrificing accuracy. We conducted extensive evaluations
and comparisons and demonstrated that our method achieves superior performance
compared to the current state-of-the-art, making it a strong contender for
real-time stereo matching in VR/AR applications.","['Ziang Cheng', 'Jiayu Yang', 'Hongdong Li']",2023-09-08T07:53:58Z,http://arxiv.org/abs/2309.04183v1,['cs.CV']
"HoloAssist: an Egocentric Human Interaction Dataset for Interactive AI
  Assistants in the Real World","Building an interactive AI assistant that can perceive, reason, and
collaborate with humans in the real world has been a long-standing pursuit in
the AI community. This work is part of a broader research effort to develop
intelligent agents that can interactively guide humans through performing tasks
in the physical world. As a first step in this direction, we introduce
HoloAssist, a large-scale egocentric human interaction dataset, where two
people collaboratively complete physical manipulation tasks. The task performer
executes the task while wearing a mixed-reality headset that captures seven
synchronized data streams. The task instructor watches the performer's
egocentric video in real time and guides them verbally. By augmenting the data
with action and conversational annotations and observing the rich behaviors of
various participants, we present key insights into how human assistants correct
mistakes, intervene in the task completion procedure, and ground their
instructions to the environment. HoloAssist spans 166 hours of data captured by
350 unique instructor-performer pairs. Furthermore, we construct and present
benchmarks on mistake detection, intervention type prediction, and hand
forecasting, along with detailed analysis. We expect HoloAssist will provide an
important resource for building AI assistants that can fluidly collaborate with
humans in the real world. Data can be downloaded at
https://holoassist.github.io/.","['Xin Wang', 'Taein Kwon', 'Mahdi Rad', 'Bowen Pan', 'Ishani Chakraborty', 'Sean Andrist', 'Dan Bohus', 'Ashley Feniello', 'Bugra Tekin', 'Felipe Vieira Frujeri', 'Neel Joshi', 'Marc Pollefeys']",2023-09-29T07:17:43Z,http://arxiv.org/abs/2309.17024v1,['cs.CV']
"Generative AI-driven Semantic Communication Framework for NextG Wireless
  Network","This work designs a novel semantic communication (SemCom) framework for the
next-generation wireless network to tackle the challenges of unnecessary
transmission of vast amounts that cause high bandwidth consumption, more
latency, and experience with bad quality of services (QoS). In particular,
these challenges hinder applications like intelligent transportation systems
(ITS), metaverse, mixed reality, and the Internet of Everything, where
real-time and efficient data transmission is paramount. Therefore, to reduce
communication overhead and maintain the QoS of emerging applications such as
metaverse, ITS, and digital twin creation, this work proposes a novel semantic
communication framework. First, an intelligent semantic transmitter is designed
to capture the meaningful information (e.g., the rode-side image in ITS) by
designing a domain-specific Mobile Segment Anything Model (MSAM)-based
mechanism to reduce the potential communication traffic while QoS remains
intact. Second, the concept of generative AI is introduced for building the
SemCom to reconstruct and denoise the received semantic data frame at the
receiver end. In particular, the Generative Adversarial Network (GAN) mechanism
is designed to maintain a superior quality reconstruction under different
signal-to-noise (SNR) channel conditions. Finally, we have tested and evaluated
the proposed semantic communication (SemCom) framework with the real-world 6G
scenario of ITS; in particular, the base station equipped with an RGB camera
and a mmWave phased array. Experimental results demonstrate the efficacy of the
proposed SemCom framework by achieving high-quality reconstruction across
various SNR channel conditions, resulting in 93.45% data reduction in
communication.","['Avi Deb Raha', 'Md. Shirajum Munir', 'Apurba Adhikary', 'Yu Qiao', 'Choong Seon Hong']",2023-10-13T11:33:54Z,http://arxiv.org/abs/2310.09021v1,['cs.NI']
"Privacy Preservation in Artificial Intelligence and Extended Reality
  (AI-XR) Metaverses: A Survey","The metaverse is a nascent concept that envisions a virtual universe, a
collaborative space where individuals can interact, create, and participate in
a wide range of activities. Privacy in the metaverse is a critical concern as
the concept evolves and immersive virtual experiences become more prevalent.
The metaverse privacy problem refers to the challenges and concerns surrounding
the privacy of personal information and data within Virtual Reality (VR)
environments as the concept of a shared VR space becomes more accessible.
Metaverse will harness advancements from various technologies such as
Artificial Intelligence (AI), Extended Reality (XR), Mixed Reality (MR), and
5G/6G-based communication to provide personalized and immersive services to its
users. Moreover, to enable more personalized experiences, the metaverse relies
on the collection of fine-grained user data that leads to various privacy
issues. Therefore, before the potential of the metaverse can be fully realized,
privacy concerns related to personal information and data within VR
environments must be addressed. This includes safeguarding users' control over
their data, ensuring the security of their personal information, and protecting
in-world actions and interactions from unauthorized sharing. In this paper, we
explore various privacy challenges that future metaverses are expected to face,
given their reliance on AI for tracking users, creating XR and MR experiences,
and facilitating interactions. Moreover, we thoroughly analyze technical
solutions such as differential privacy, Homomorphic Encryption (HE), and
Federated Learning (FL) and discuss related sociotechnical issues regarding
privacy.","['Mahdi Alkaeed', 'Adnan Qayyum', 'Junaid Qadir']",2023-09-19T11:56:12Z,http://arxiv.org/abs/2310.10665v1,"['cs.CR', 'cs.AI', 'cs.LG']"
"Experiencing Urban Air Mobility: How Passengers evaluate a simulated
  flight with an Air Taxi","For the successful development and implementation of novel concepts and
technology, the acceptance of potential users is crucial. Therefore, within the
project HorizonUAM, we investigated passengers' acceptance of air taxis. One
challenge is that not many people have real experiences with urban air mobility
(UAM) at the moment and thus requirements formulated by potential users refer
to rather abstract concepts. To allow participants to gain realistic
impressions of UAM concepts, a Mixed Reality Air Taxi Simulator was set up. It
allows participants to experience an inner-city business shuttle flight. A
study with 30 participants assessed the information needs and the influence of
another person on board on wellbeing in nominal situations (experiment 1) as
well as one non-nominal situation (experiment 2). For the latter, participants
experienced a re-routing of the flight due to an unavailability of landing
sites at the vertidrome. During and after the flights, participants answered
questionnaires and extensive interviews were conducted. The study produced
first empirical data on relevant factors regarding interaction, information
needs and comfort within an air taxi. The findings show that passengers want to
be informed about intentions of the vehicle. The presence of a steward on board
is not necessary but can increase wellbeing especially during non-nominal
situations.","['Anne Papenfuss', 'Maria Stolz', 'Nele Riedesel', 'Franziska Dunkel', 'Johannes Maria Ernst', 'Tim Laudien', 'Helge Lenz', 'Aytek Korkmaz', 'Albert End', 'Bianca Isabella Schuchardt']",2023-11-02T08:43:52Z,http://arxiv.org/abs/2311.01079v1,"['eess.SY', 'cs.SY', 'physics.soc-ph']"
"Enabling In-Situ Resources Utilisation by leveraging collaborative
  robotics and astronaut-robot interaction","Space exploration and establishing human presence on other planets demand
advanced technology and effective collaboration between robots and astronauts.
Efficient space resource utilization is also vital for extraterrestrial
settlements. The Collaborative In-Situ Resources Utilisation (CISRU) project
has developed a software suite comprising five key modules. The first module
manages multi-agent autonomy, facilitating communication between agents and
mission control. The second focuses on environment perception, employing AI
algorithms for tasks like environment segmentation and object pose estimation.
The third module ensures safe navigation, covering obstacle avoidance, social
navigation with astronauts, and cooperation among robots. The fourth module
addresses manipulation functions, including multi-tool capabilities and
tool-changer design for diverse tasks in In-Situ Resources Utilization (ISRU)
scenarios. Finally, the fifth module controls cooperative behaviour,
incorporating astronaut commands, Mixed Reality interfaces, map fusion, task
supervision, and error control. The suite was tested using an astronaut-rover
interaction dataset in a planetary environment and GMV SPoT analogue
environments. Results demonstrate the advantages of E4 autonomy and AI in space
systems, benefiting astronaut-robot collaboration. This paper details CISRU's
development, field test preparation, and analysis, highlighting its potential
to revolutionize planetary exploration through AI-powered technology.","['Silvia Romero-Azpitarte', 'Cristina Luna', 'Alba Guerra', 'Mercedes Alonso', 'Pablo Romeo Manrique', 'Marina L. Seoane', 'Daniel Olayo', 'Almudena Moreno', 'Pablo Castellanos', 'Fernando Gandía', 'Gianfranco Visentin']",2023-11-06T14:43:03Z,http://arxiv.org/abs/2311.03146v1,['cs.RO']
"Viewport Prediction for Volumetric Video Streaming by Exploring Video
  Saliency and Trajectory Information","Volumetric video, also known as hologram video, is a novel medium that
portrays natural content in Virtual Reality (VR), Augmented Reality (AR), and
Mixed Reality (MR). It is expected to be the next-gen video technology and a
prevalent use case for 5G and beyond wireless communication. Considering that
each user typically only watches a section of the volumetric video, known as
the viewport, it is essential to have precise viewport prediction for optimal
performance. However, research on this topic is still in its infancy. In the
end, this paper presents and proposes a novel approach, named Saliency and
Trajectory Viewport Prediction (STVP), which aims to improve the precision of
viewport prediction in volumetric video streaming. The STVP extensively
utilizes video saliency information and viewport trajectory. To our knowledge,
this is the first comprehensive study of viewport prediction in volumetric
video streaming. In particular, we introduce a novel sampling method, Uniform
Random Sampling (URS), to reduce computational complexity while still
preserving video features in an efficient manner. Then we present a saliency
detection technique that incorporates both spatial and temporal information for
detecting static, dynamic geometric, and color salient regions. Finally, we
intelligently fuse saliency and trajectory information to achieve more accurate
viewport prediction. We conduct extensive simulations to evaluate the
effectiveness of our proposed viewport prediction methods using
state-of-the-art volumetric video sequences. The experimental results show the
superiority of the proposed method over existing schemes. The dataset and
source code will be publicly accessible after acceptance.","['Jie Li', 'Zhixin Li', 'Zhi Liu', 'Pengyuan Zhou', 'Richang Hong', 'Qiyue Li', 'Han Hu']",2023-11-28T03:45:29Z,http://arxiv.org/abs/2311.16462v1,"['cs.CV', 'cs.MM']"
MANUS: Markerless Grasp Capture using Articulated 3D Gaussians,"Understanding how we grasp objects with our hands has important applications
in areas like robotics and mixed reality. However, this challenging problem
requires accurate modeling of the contact between hands and objects. To capture
grasps, existing methods use skeletons, meshes, or parametric models that does
not represent hand shape accurately resulting in inaccurate contacts. We
present MANUS, a method for Markerless Hand-Object Grasp Capture using
Articulated 3D Gaussians. We build a novel articulated 3D Gaussians
representation that extends 3D Gaussian splatting for high-fidelity
representation of articulating hands. Since our representation uses Gaussian
primitives, it enables us to efficiently and accurately estimate contacts
between the hand and the object. For the most accurate results, our method
requires tens of camera views that current datasets do not provide. We
therefore build MANUS-Grasps, a new dataset that contains hand-object grasps
viewed from 50+ cameras across 30+ scenes, 3 subjects, and comprising over 7M
frames. In addition to extensive qualitative results, we also show that our
method outperforms others on a quantitative contact evaluation method that uses
paint transfer from the object to the hand.","['Chandradeep Pokhariya', 'Ishaan N Shah', 'Angela Xing', 'Zekun Li', 'Kefan Chen', 'Avinash Sharma', 'Srinath Sridhar']",2023-12-04T18:56:22Z,http://arxiv.org/abs/2312.02137v2,['cs.CV']
"Exploring the current applications and potential of extended reality for
  environmental sustainability in manufacturing","In response to the transformation towards Industry 5.0, there is a growing
call for manufacturing systems that prioritize environmental sustainability,
alongside the emerging application of digital tools. Extended Reality (XR) -
including Virtual Reality (VR), Augmented Reality (AR) and Mixed Reality (MR) -
is one of the technologies identified as an enabler for Industry 5.0. XR could
potentially also be a driver for more sustainable manufacturing: however, its
potential environmental benefits have received limited attention. This paper
aims to explore the current manufacturing applications and research within the
field of XR technology connected to the environmental sustainability principle.
The objectives of this paper are two-fold: (1) Identify the currently explored
use cases of XR technology in literature and research, addressing environmental
sustainability in manufacturing; (2) Provide guidance and references for
industry and companies to use cases, toolboxes, methodologies, and workflows
for implementing XR in environmental sustainable manufacturing practices. Based
on the categorization of sustainability indicators, developed by the National
Institute of Standards and Technology (NIST), the authors analyzed and mapped
the current literature, with criteria of pragmatic XR use cases for
manufacturing. The exploration resulted in a mapping of the current
applications and use cases of XR technology within manufacturing that has the
potential to drive environmental sustainability. The results are presented as
stated use-cases with reference to the literature, contributing as guidance and
inspiration for future researchers or implementations in industry, using XR as
a driver for environmental sustainability. Furthermore, the authors open up the
discussion for future work and research to increase the attention of XR as a
driver for environmental sustainability.","['Huizhong Cao', 'Henrik Söderlund', 'Mélanie Derspeisse', 'Björn Johansson']",2023-12-29T13:18:01Z,http://arxiv.org/abs/2312.17595v1,"['cs.CY', 'cs.HC']"
On the Emergence of Symmetrical Reality,"Artificial intelligence (AI) has revolutionized human cognitive abilities and
facilitated the development of new AI entities capable of interacting with
humans in both physical and virtual environments. Despite the existence of
virtual reality, mixed reality, and augmented reality for several years,
integrating these technical fields remains a formidable challenge due to their
disparate application directions. The advent of AI agents, capable of
autonomous perception and action, further compounds this issue by exposing the
limitations of traditional human-centered research approaches. It is imperative
to establish a comprehensive framework that accommodates the dual perceptual
centers of humans and AI agents in both physical and virtual worlds. In this
paper, we introduce the symmetrical reality framework, which offers a unified
representation encompassing various forms of physical-virtual amalgamations.
This framework enables researchers to better comprehend how AI agents can
collaborate with humans and how distinct technical pathways of physical-virtual
integration can be consolidated from a broader perspective. We then delve into
the coexistence of humans and AI, demonstrating a prototype system that
exemplifies the operation of symmetrical reality systems for specific tasks,
such as pouring water. Subsequently, we propose an instance of an AI-driven
active assistance service that illustrates the potential applications of
symmetrical reality. This paper aims to offer beneficial perspectives and
guidance for researchers and practitioners in different fields, thus
contributing to the ongoing research about human-AI coexistence in both
physical and virtual environments.","['Zhenliang Zhang', 'Zeyu Zhang', 'Ziyuan Jiao', 'Yao Su', 'Hangxin Liu', 'Wei Wang', 'Song-Chun Zhu']",2024-01-26T16:09:39Z,http://arxiv.org/abs/2401.15132v1,"['cs.HC', 'cs.AI']"
"VR-GS: A Physical Dynamics-Aware Interactive Gaussian Splatting System
  in Virtual Reality","As consumer Virtual Reality (VR) and Mixed Reality (MR) technologies gain
momentum, there's a growing focus on the development of engagements with 3D
virtual content. Unfortunately, traditional techniques for content creation,
editing, and interaction within these virtual spaces are fraught with
difficulties. They tend to be not only engineering-intensive but also require
extensive expertise, which adds to the frustration and inefficiency in virtual
object manipulation. Our proposed VR-GS system represents a leap forward in
human-centered 3D content interaction, offering a seamless and intuitive user
experience. By developing a physical dynamics-aware interactive Gaussian
Splatting in a Virtual Reality setting, and constructing a highly efficient
two-level embedding strategy alongside deformable body simulations, VR-GS
ensures real-time execution with highly realistic dynamic responses. The
components of our Virtual Reality system are designed for high efficiency and
effectiveness, starting from detailed scene reconstruction and object
segmentation, advancing through multi-view image in-painting, and extending to
interactive physics-based editing. The system also incorporates real-time
deformation embedding and dynamic shadow casting, ensuring a comprehensive and
engaging virtual experience.Our project page is available at:
https://yingjiang96.github.io/VR-GS/.","['Ying Jiang', 'Chang Yu', 'Tianyi Xie', 'Xuan Li', 'Yutao Feng', 'Huamin Wang', 'Minchen Li', 'Henry Lau', 'Feng Gao', 'Yin Yang', 'Chenfanfu Jiang']",2024-01-30T01:28:36Z,http://arxiv.org/abs/2401.16663v2,"['cs.HC', 'cs.CV']"
"HI-GAN: Hierarchical Inpainting GAN with Auxiliary Inputs for Combined
  RGB and Depth Inpainting","Inpainting involves filling in missing pixels or areas in an image, a crucial
technique employed in Mixed Reality environments for various applications,
particularly in Diminished Reality (DR) where content is removed from a user's
visual environment. Existing methods rely on digital replacement techniques
which necessitate multiple cameras and incur high costs. AR devices and
smartphones use ToF depth sensors to capture scene depth maps aligned with RGB
images. Despite speed and affordability, ToF cameras create imperfect depth
maps with missing pixels. To address the above challenges, we propose
Hierarchical Inpainting GAN (HI-GAN), a novel approach comprising three GANs in
a hierarchical fashion for RGBD inpainting. EdgeGAN and LabelGAN inpaint masked
edge and segmentation label images respectively, while CombinedRGBD-GAN
combines their latent representation outputs and performs RGB and Depth
inpainting. Edge images and particularly segmentation label images as auxiliary
inputs significantly enhance inpainting performance by complementary context
and hierarchical optimization. We believe we make the first attempt to
incorporate label images into inpainting process.Unlike previous approaches
requiring multiple sequential models and separate outputs, our work operates in
an end-to-end manner, training all three models simultaneously and
hierarchically. Specifically, EdgeGAN and LabelGAN are first optimized
separately and further optimized inside CombinedRGBD-GAN to enhance inpainting
quality. Experiments demonstrate that HI-GAN works seamlessly and achieves
overall superior performance compared with existing approaches.","['Ankan Dash', 'Jingyi Gu', 'Guiling Wang']",2024-02-15T21:43:56Z,http://arxiv.org/abs/2402.10334v1,"['cs.CV', 'cs.AI', 'cs.LG']"
"The Value of Extended Reality Techniques to Improve Remote Collaborative
  Maintenance Operations: A User Study","In the Architecture, Engineering and Construction (AEC) sector, data
extracted from building information modelling (BIM) can be used to create a
digital twin (DT). The algorithms of a BIM-based DT can facilitate the
retrieval of information, which can then be used to improve building operation
and maintenance procedures. However, with the increased complexity and
automation of the building, maintenance operations are likely to become more
complex and may require expert intervention. Collaboration and interaction
between the operator and the expert may be limited as the latter may not be on
site or within the company. Recently, extended reality (XR) technologies have
proven to be effective in improving collaboration during maintenance
operations,through data display and shared interactions. This paper presents a
new collaborative solution using these technologies to enhance collaboration
during remote maintenance operations. The proposed approach consists of a mixed
reality (MR) set-up for the operator, a virtual reality (VR) set-up for the
remote expert and a shared Digital Model of a heat exchanger. The MR set-up is
used for tracking and displaying specific information, provided by the VR
module. A user study was carried out to compare the efficiency of our solution
with a standard audio-video collaboration. Our approach demonstrated
substantial enhancements in collaborative inspection, resulting in a
significative reduction in both the overall completion time of the inspection
and the frequency of errors committed by the operators.","['Corentin Coupry', 'Paul Richard', 'David Bigaud', 'Sylvain Noblecourt', 'David Baudry']",2024-02-29T12:28:40Z,http://arxiv.org/abs/2403.05580v1,"['cs.HC', 'cs.GR']"
OPEN TEACH: A Versatile Teleoperation System for Robotic Manipulation,"Open-sourced, user-friendly tools form the bedrock of scientific advancement
across disciplines. The widespread adoption of data-driven learning has led to
remarkable progress in multi-fingered dexterity, bimanual manipulation, and
applications ranging from logistics to home robotics. However, existing data
collection platforms are often proprietary, costly, or tailored to specific
robotic morphologies. We present OPEN TEACH, a new teleoperation system
leveraging VR headsets to immerse users in mixed reality for intuitive robot
control. Built on the affordable Meta Quest 3, which costs $500, OPEN TEACH
enables real-time control of various robots, including multi-fingered hands and
bimanual arms, through an easy-to-use app. Using natural hand gestures and
movements, users can manipulate robots at up to 90Hz with smooth visual
feedback and interface widgets offering closeup environment views. We
demonstrate the versatility of OPEN TEACH across 38 tasks on different robots.
A comprehensive user study indicates significant improvement in teleoperation
capability over the AnyTeleop framework. Further experiments exhibit that the
collected data is compatible with policy learning on 10 dexterous and
contact-rich manipulation tasks. Currently supporting Franka, xArm, Jaco, and
Allegro platforms, OPEN TEACH is fully open-sourced to promote broader
adoption. Videos are available at https://open-teach.github.io/.","['Aadhithya Iyer', 'Zhuoran Peng', 'Yinlong Dai', 'Irmak Guzey', 'Siddhant Haldar', 'Soumith Chintala', 'Lerrel Pinto']",2024-03-12T17:58:38Z,http://arxiv.org/abs/2403.07870v1,['cs.RO']
Segment Any Medical Model Extended,"The Segment Anything Model (SAM) has drawn significant attention from
researchers who work on medical image segmentation because of its
generalizability. However, researchers have found that SAM may have limited
performance on medical images compared to state-of-the-art non-foundation
models. Regardless, the community sees potential in extending, fine-tuning,
modifying, and evaluating SAM for analysis of medical imaging. An increasing
number of works have been published focusing on the mentioned four directions,
where variants of SAM are proposed. To this end, a unified platform helps push
the boundary of the foundation model for medical images, facilitating the use,
modification, and validation of SAM and its variants in medical image
segmentation. In this work, we introduce SAMM Extended (SAMME), a platform that
integrates new SAM variant models, adopts faster communication protocols,
accommodates new interactive modes, and allows for fine-tuning of subcomponents
of the models. These features can expand the potential of foundation models
like SAM, and the results can be translated to applications such as
image-guided therapy, mixed reality interaction, robotic navigation, and data
augmentation.","['Yihao Liu', 'Jiaming Zhang', 'Andres Diaz-Pinto', 'Haowei Li', 'Alejandro Martin-Gomez', 'Amir Kheradmand', 'Mehran Armand']",2024-03-26T21:37:25Z,http://arxiv.org/abs/2403.18114v1,['cs.CV']
3D Human Scan With A Moving Event Camera,"Capturing a 3D human body is one of the important tasks in computer vision
with a wide range of applications such as virtual reality and sports analysis.
However, conventional frame cameras are limited by their temporal resolution
and dynamic range, which imposes constraints in real-world application setups.
Event cameras have the advantages of high temporal resolution and high dynamic
range (HDR), but the development of event-based methods is necessary to handle
data with different characteristics. This paper proposes a novel event-based
method for 3D pose estimation and human mesh recovery. Prior work on
event-based human mesh recovery require frames (images) as well as event data.
The proposed method solely relies on events; it carves 3D voxels by moving the
event camera around a stationary body, reconstructs the human pose and mesh by
attenuated rays, and fit statistical body models, preserving high-frequency
details. The experimental results show that the proposed method outperforms
conventional frame-based methods in the estimation accuracy of both pose and
body mesh. We also demonstrate results in challenging situations where a
conventional camera has motion blur. This is the first to demonstrate
event-only human mesh recovery, and we hope that it is the first step toward
achieving robust and accurate 3D human body scanning from vision sensors.
https://florpeng.github.io/event-based-human-scan/","['Kai Kohyama', 'Shintaro Shiba', 'Yoshimitsu Aoki']",2024-04-12T14:34:24Z,http://arxiv.org/abs/2404.08504v2,['cs.CV']
"Exploring Proactive Interventions toward Harmful Behavior in Embodied
  Virtual Spaces","Technological advancements have undoubtedly revolutionized various aspects of
human life, altering the ways we perceive the world, engage with others, build
relationships, and conduct our daily work routines. Among the recent
advancements, the proliferation of virtual and mixed reality technologies
stands out as a significant leap forward, promising to elevate our experiences
and interactions to unprecedented levels. However, alongside the benefits,
these emerging technologies also introduce novel avenues for harm and misuse,
particularly in virtual and embodied spaces such as Zoom and virtual reality
(VR) environments.
  The immersive nature of virtual reality environments raises unique challenges
regarding psychological and emotional well-being. While VR can offer
captivating and immersive experiences, prolonged exposure to virtual
environments may lead to phenomena like cybersickness, disorientation, and even
psychological distress in susceptible individuals. Additionally, the blurring
of boundaries between virtual and real-world interactions in VR raises ethical
concerns regarding consent, harassment, and the potential for virtual
experiences to influence real-life behavior. Additionally, the increasing
integration of artificial intelligence (AI) and machine learning algorithms in
virtual spaces introduces risks related to algorithmic bias, discrimination,
and manipulation. In VR environments, AI-driven systems may inadvertently
perpetuate stereotypes, amplify inequalities, or manipulate user behavior
through personalized content recommendations and targeted advertising, posing
ethical dilemmas and societal risks.",['Ruchi Panchanadikar'],2024-04-23T17:38:27Z,http://arxiv.org/abs/2405.05920v1,['cs.HC']
"A First Step in Using Machine Learning Methods to Enhance Interaction
  Analysis for Embodied Learning Environments","Investigating children's embodied learning in mixed-reality environments,
where they collaboratively simulate scientific processes, requires analyzing
complex multimodal data to interpret their learning and coordination behaviors.
Learning scientists have developed Interaction Analysis (IA) methodologies for
analyzing such data, but this requires researchers to watch hours of videos to
extract and interpret students' learning patterns. Our study aims to simplify
researchers' tasks, using Machine Learning and Multimodal Learning Analytics to
support the IA processes. Our study combines machine learning algorithms and
multimodal analyses to support and streamline researcher efforts in developing
a comprehensive understanding of students' scientific engagement through their
movements, gaze, and affective responses in a simulated scenario. To facilitate
an effective researcher-AI partnership, we present an initial case study to
determine the feasibility of visually representing students' states, actions,
gaze, affect, and movement on a timeline. Our case study focuses on a specific
science scenario where students learn about photosynthesis. The timeline allows
us to investigate the alignment of critical learning moments identified by
multimodal and interaction analysis, and uncover insights into students'
temporal learning progressions.","['Joyce Fonteles', 'Eduardo Davalos', 'Ashwin T. S.', 'Yike Zhang', 'Mengxi Zhou', 'Efrat Ayalon', 'Alicia Lane', 'Selena Steinberg', 'Gabriella Anton', 'Joshua Danish', 'Noel Enyedy', 'Gautam Biswas']",2024-05-10T02:40:24Z,http://arxiv.org/abs/2405.06203v1,['cs.AI']
"Autonomous Workflow for Multimodal Fine-Grained Training Assistants
  Towards Mixed Reality","Autonomous artificial intelligence (AI) agents have emerged as promising
protocols for automatically understanding the language-based environment,
particularly with the exponential development of large language models (LLMs).
However, a fine-grained, comprehensive understanding of multimodal environments
remains under-explored. This work designs an autonomous workflow tailored for
integrating AI agents seamlessly into extended reality (XR) applications for
fine-grained training. We present a demonstration of a multimodal fine-grained
training assistant for LEGO brick assembly in a pilot XR environment.
Specifically, we design a cerebral language agent that integrates LLM with
memory, planning, and interaction with XR tools and a vision-language agent,
enabling agents to decide their actions based on past experiences. Furthermore,
we introduce LEGO-MRTA, a multimodal fine-grained assembly dialogue dataset
synthesized automatically in the workflow served by a commercial LLM. This
dataset comprises multimodal instruction manuals, conversations, XR responses,
and vision question answering. Last, we present several prevailing
open-resource LLMs as benchmarks, assessing their performance with and without
fine-tuning on the proposed dataset. We anticipate that the broader impact of
this workflow will advance the development of smarter assistants for seamless
user interaction in XR environments, fostering research in both AI and HCI
communities.","['Jiahuan Pei', 'Irene Viola', 'Haochen Huang', 'Junxiao Wang', 'Moonisa Ahsan', 'Fanghua Ye', 'Jiang Yiming', 'Yao Sai', 'Di Wang', 'Zhumin Chen', 'Pengjie Ren', 'Pablo Cesar']",2024-05-16T14:20:30Z,http://arxiv.org/abs/2405.13034v1,"['cs.CL', 'cs.AI', 'cs.HC']"
"HoloDevice: Holographic Cross-Device Interactions for Remote
  Collaboration","This paper introduces holographic cross-device interaction, a new class of
remote cross-device interactions between local physical devices and
holographically rendered remote devices. Cross-device interactions have enabled
a rich set of interactions with device ecologies. Most existing research
focuses on co-located settings (meaning when users and devices are in the same
physical space) to achieve these rich interactions and affordances. In
contrast, holographic cross-device interaction allows remote interactions
between devices at distant locations by providing a rich visual affordance
through real-time holographic rendering of the device's motion, content, and
interactions on mixed reality head-mounted displays. This maintains the
advantages of having a physical device, such as precise input through touch and
pen interaction. Through holographic rendering, not only can remote devices
interact as if they are co-located, but they can also be virtually augmented to
further enrich interactions, going beyond what is possible with existing
cross-device systems. To demonstrate this concept, we developed HoloDevice, a
prototype system for holographic cross-device interaction using the Microsoft
Hololens 2 augmented reality headset. Our contribution is threefold. First, we
introduce the concept of holographic cross-device interaction. Second, we
present a design space containing three unique benefits, which include: (1)
spatial visualization of interaction and motion, (2) rich visual affordances
for intermediate transition, and (3) dynamic and fluid configuration. Last we
discuss a set of implementation demonstrations and use-case scenarios that
further explore the space.","['Neil Chulpongsatorn', 'Thien-Kim Nguyen', 'Nicolai Marquardt', 'Ryo Suzuki']",2024-05-28T22:49:01Z,http://arxiv.org/abs/2405.19377v1,['cs.HC']
"BundleFusion: Real-time Globally Consistent 3D Reconstruction using
  On-the-fly Surface Re-integration","Real-time, high-quality, 3D scanning of large-scale scenes is key to mixed
reality and robotic applications. However, scalability brings challenges of
drift in pose estimation, introducing significant errors in the accumulated
model. Approaches often require hours of offline processing to globally correct
model errors. Recent online methods demonstrate compelling results, but suffer
from: (1) needing minutes to perform online correction preventing true
real-time use; (2) brittle frame-to-frame (or frame-to-model) pose estimation
resulting in many tracking failures; or (3) supporting only unstructured
point-based representations, which limit scan quality and applicability. We
systematically address these issues with a novel, real-time, end-to-end
reconstruction framework. At its core is a robust pose estimation strategy,
optimizing per frame for a global set of camera poses by considering the
complete history of RGB-D input with an efficient hierarchical approach. We
remove the heavy reliance on temporal tracking, and continually localize to the
globally optimized frames instead. We contribute a parallelizable optimization
framework, which employs correspondences based on sparse features and dense
geometric and photometric matching. Our approach estimates globally optimized
(i.e., bundle adjusted) poses in real-time, supports robust tracking with
recovery from gross tracking failures (i.e., relocalization), and re-estimates
the 3D model in real-time to ensure global consistency; all within a single
framework. Our approach outperforms state-of-the-art online systems with
quality on par to offline methods, but with unprecedented speed and scan
completeness. Our framework leads to a comprehensive online scanning solution
for large indoor environments, enabling ease of use and high-quality results.","['Angela Dai', 'Matthias Nießner', 'Michael Zollhöfer', 'Shahram Izadi', 'Christian Theobalt']",2016-04-05T00:06:39Z,http://arxiv.org/abs/1604.01093v3,"['cs.GR', 'cs.CV']"
"XR: Enabling training mode in the human brain XR: Enabling training mode
  in the human brain","The face of simulation-based training has greatly evolved, with the most
recent tools giving the ability to create virtual environments that rival
realism. At first glance, it might appear that what the training sector needs
is the most realistic simulators possible, but traditional simulators are not
necessarily the most efficient or practical training tools. With all that these
new technologies have to offer; the challenge is to go back to the core of
training needs and identify the right vector of sensory cues that will most
effectively enable training mode in the human brain. Bigger and Pricier doesn't
necessarily mean better. Simulation with cross-reality content (XR), which by
definition encompasses virtual reality (VR), mixed reality (MR), and augmented
reality (AR), is the most practical solution for deploying any kind of
simulation-based training. The authors of this paper (a teacher and a
technology expert) share their experiences and expose XR-specific best
practices to maximize learning transfer. ABOUT THE AUTHORS Sebastien Loze :
Starting his career in the modeling and simulation community more than 15 years
ago, S{\'e}bastien has focused on learning about the latest simulation
innovations and sharing information on how experts have solved their
challenges. He worked on the COTS integration at CAE and the Presagis focusing
on Simulation and Visualization products. More recently, Sebastien put together
simulation and training teams and strategies for emerging companies like CM
Labs and D-BOX. He is now the Simulations Industry Manager at Epic Games,
focusing on helping companies develop real-time solutions for simulation-based
training. Philippe Lepinard: Former military helicopter pilot and simulation
officer, Philippe L{\'e}pinard is now an associate professor at the University
of Paris-Est Cr{\'e}teil (UPEC). His research is focusing on playful learning
and training through simulation. He is one of the founding members of the
French simulation association.","['Philippe Lépinard', 'Sébastien Lozé']",2019-04-26T07:55:39Z,http://arxiv.org/abs/1904.11704v1,"['cs.GR', 'cs.HC']"
"All Reality: Virtual, Augmented, Mixed (X), Mediated (X,Y), and
  Multimediated Reality","The contributions of this paper are: (1) a taxonomy of the ""Realities""
(Virtual, Augmented, Mixed, Mediated, etc.), and (2) some new kinds of
""reality"" that come from nature itself, i.e. that expand our notion beyond
synthetic realities to include also phenomenological realities.
  VR (Virtual Reality) replaces the real world with a simulated experience
(virtual world). AR (Augmented Reality) allows a virtual world to be
experienced while also experiencing the real world at the same time. Mixed
Reality provides blends that interpolate between real and virtual worlds in
various proportions, along a ""Virtuality"" axis, and extrapolate to an ""X-axis"".
Mediated Reality goes a step further by mixing/blending and also modifying
reality. This modifying of reality introduces a second axis. Mediated Reality
is useful as a seeing aid (e.g. modifying reality to make it easier to
understand), and for psychology experiments like Stratton's 1896 upside-down
eyeglasses experiment.
  We propose Multimediated Reality as a multidimensional multisensory mediated
reality that includes not just interactive multimedia-based reality for our
five senses, but also includes additional senses (like sensory sonar, sensory
radar, etc.), as well as our human actions/actuators. These extra senses are
mapped to our human senses using synthetic synesthesia. This allows us to
directly experience real (but otherwise invisible) phenomena, such as wave
propagation and wave interference patterns, so that we can see radio waves and
sound waves and how they interact with objects and each other. Multimediated
reality is multidimensional, multimodal, multisensory, and multiscale. It is
also multidisciplinary, in that we must consider not just the user, but also
how the technology affects others, e.g. how its physical appearance affects
social situations.","['Steve Mann', 'Tom Furness', 'Yu Yuan', 'Jay Iorio', 'Zixin Wang']",2018-04-20T15:40:39Z,http://arxiv.org/abs/1804.08386v1,['cs.HC']
SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in the Wild,"Gesture recognition is a fundamental tool to enable novel interaction
paradigms in a variety of application scenarios like Mixed Reality
environments, touchless public kiosks, entertainment systems, and more.
Recognition of hand gestures can be nowadays performed directly from the stream
of hand skeletons estimated by software provided by low-cost trackers
(Ultraleap) and MR headsets (Hololens, Oculus Quest) or by video processing
software modules (e.g. Google Mediapipe). Despite the recent advancements in
gesture and action recognition from skeletons, it is unclear how well the
current state-of-the-art techniques can perform in a real-world scenario for
the recognition of a wide set of heterogeneous gestures, as many benchmarks do
not test online recognition and use limited dictionaries. This motivated the
proposal of the SHREC 2021: Track on Skeleton-based Hand Gesture Recognition in
the Wild. For this contest, we created a novel dataset with heterogeneous
gestures featuring different types and duration. These gestures have to be
found inside sequences in an online recognition scenario. This paper presents
the result of the contest, showing the performances of the techniques proposed
by four research groups on the challenging task compared with a simple baseline
method.","['Ariel Caputo', 'Andrea Giachetti', 'Simone Soso', 'Deborah Pintani', ""Andrea D'Eusanio"", 'Stefano Pini', 'Guido Borghi', 'Alessandro Simoni', 'Roberto Vezzani', 'Rita Cucchiara', 'Andrea Ranieri', 'Franca Giannini', 'Katia Lupinetti', 'Marina Monti', 'Mehran Maghoumi', 'Joseph J. LaViola Jr', 'Minh-Quan Le', 'Hai-Dang Nguyen', 'Minh-Triet Tran']",2021-06-21T10:57:49Z,http://arxiv.org/abs/2106.10980v1,"['cs.CV', 'cs.LG']"
Egocentric Videoconferencing,"We introduce a method for egocentric videoconferencing that enables
hands-free video calls, for instance by people wearing smart glasses or other
mixed-reality devices. Videoconferencing portrays valuable non-verbal
communication and face expression cues, but usually requires a front-facing
camera. Using a frontal camera in a hands-free setting when a person is on the
move is impractical. Even holding a mobile phone camera in the front of the
face while sitting for a long duration is not convenient. To overcome these
issues, we propose a low-cost wearable egocentric camera setup that can be
integrated into smart glasses. Our goal is to mimic a classical video call, and
therefore, we transform the egocentric perspective of this camera into a front
facing video. To this end, we employ a conditional generative adversarial
neural network that learns a transition from the highly distorted egocentric
views to frontal views common in videoconferencing. Our approach learns to
transfer expression details directly from the egocentric view without using a
complex intermediate parametric expressions model, as it is used by related
face reenactment methods. We successfully handle subtle expressions, not easily
captured by parametric blendshape-based solutions, e.g., tongue movement, eye
movements, eye blinking, strong expressions and depth varying movements. To get
control over the rigid head movements in the target view, we condition the
generator on synthetic renderings of a moving neutral face. This allows us to
synthesis results at different head poses. Our technique produces temporally
smooth video-realistic renderings in real-time using a video-to-video
translation network in conjunction with a temporal discriminator. We
demonstrate the improved capabilities of our technique by comparing against
related state-of-the art approaches.","['Mohamed Elgharib', 'Mohit Mendiratta', 'Justus Thies', 'Matthias Nießner', 'Hans-Peter Seidel', 'Ayush Tewari', 'Vladislav Golyanik', 'Christian Theobalt']",2021-07-07T09:49:39Z,http://arxiv.org/abs/2107.03109v1,"['cs.GR', 'cs.CV']"
Fristograms: Revealing and Exploiting Light Field Internals,"In recent years, light field (LF) capture and processing has become an
integral part of media production. The richness of information available in LFs
has enabled novel applications like post-capture depth-of-field editing, 3D
reconstruction, segmentation and matting, saliency detection, object detection
and recognition, and mixed reality. The efficacy of such applications depends
on certain underlying requirements, which are often ignored. For example, some
operations such as noise-reduction, or hyperfan-filtering are only possible if
a scene point Lambertian radiator. Some other operations such as the removal of
obstacles or looking behind objects are only possible if there is at least one
ray capturing the required scene point. Consequently, the ray distribution
representing a certain scene point is an important characteristic for
evaluating processing possibilities. The primary idea in this paper is to
establish a relation between the capturing setup and the rays of the LF. To
this end, we discretize the view frustum. Traditionally, a uniform
discretization of the view frustum results in voxels that represents a single
sample on a regularly spaced, 3-D grid. Instead, we use frustum-shaped voxels
(froxels), by using depth and capturing-setup dependent discretization of the
view frustum. Based on such discretization, we count the number of rays mapping
to the same pixel on the capturing device(s). By means of this count, we
propose histograms of ray-counts over the froxels (fristograms). Fristograms
can be used as a tool to analyze and reveal interesting aspects of the
underlying LF, like the number of rays originating from a scene point and the
color distribution of these rays. As an example, we show its ability by
significantly reducing the number of rays which enables noise reduction while
maintaining the realistic rendering of non-Lambertian or partially occluded
regions.","['Thorsten Herfet', 'Kelvin Chelli', 'Tobias Lange', 'Robin Kremer']",2021-07-22T10:33:13Z,http://arxiv.org/abs/2107.10563v1,"['eess.IV', 'cs.CV']"
"The Impact of Machine Learning on 2D/3D Registration for Image-guided
  Interventions: A Systematic Review and Perspective","Image-based navigation is widely considered the next frontier of minimally
invasive surgery. It is believed that image-based navigation will increase the
access to reproducible, safe, and high-precision surgery as it may then be
performed at acceptable costs and effort. This is because image-based
techniques avoid the need of specialized equipment and seamlessly integrate
with contemporary workflows. Further, it is expected that image-based
navigation will play a major role in enabling mixed reality environments and
autonomous, robotic workflows. A critical component of image guidance is 2D/3D
registration, a technique to estimate the spatial relationships between 3D
structures, e.g., volumetric imagery or tool models, and 2D images thereof,
such as fluoroscopy or endoscopy. While image-based 2D/3D registration is a
mature technique, its transition from the bench to the bedside has been
restrained by well-known challenges, including brittleness of the optimization
objective, hyperparameter selection, and initialization, difficulties around
inconsistencies or multiple objects, and limited single-view performance. One
reason these challenges persist today is that analytical solutions are likely
inadequate considering the complexity, variability, and high-dimensionality of
generic 2D/3D registration problems. The recent advent of machine
learning-based approaches to imaging problems that, rather than specifying the
desired functional mapping, approximate it using highly expressive parametric
models holds promise for solving some of the notorious challenges in 2D/3D
registration. In this manuscript, we review the impact of machine learning on
2D/3D registration to systematically summarize the recent advances made by
introduction of this novel technology. Grounded in these insights, we then
offer our perspective on the most pressing needs, significant open problems,
and possible next steps.","['Mathias Unberath', 'Cong Gao', 'Yicheng Hu', 'Max Judish', 'Russell H Taylor', 'Mehran Armand', 'Robert Grupp']",2021-08-04T18:31:29Z,http://arxiv.org/abs/2108.02238v1,"['cs.CV', 'cs.RO', 'physics.med-ph']"
"Comfort and Sickness while Virtually Aboard an Autonomous Telepresence
  Robot","In this paper, we analyze how different path aspects affect a user's
experience, mainly VR sickness and overall comfort, while immersed in an
autonomously moving telepresence robot through a virtual reality headset. In
particular, we focus on how the robot turns and the distance it keeps from
objects, with the goal of planning suitable trajectories for an autonomously
moving immersive telepresence robot in mind; rotational acceleration is known
for causing the majority of VR sickness, and distance to objects modulates the
optical flow. We ran a within-subjects user study (n = 36, women = 18) in which
the participants watched three panoramic videos recorded in a virtual museum
while aboard an autonomously moving telepresence robot taking three different
paths varying in aspects such as turns, speeds, or distances to walls and
objects. We found a moderate correlation between the users' sickness as
measured by the SSQ and comfort on a 6-point Likert scale across all paths.
However, we detected no association between sickness and the choice of the most
comfortable path, showing that sickness is not the only factor affecting the
comfort of the user. The subjective experience of turn speed did not correlate
with either the SSQ scores or comfort, even though people often mentioned
turning speed as a source of discomfort in the open-ended questions. Through
exploring the open-ended answers more carefully, a possible reason is that the
length and lack of predictability also play a large role in making people
observe turns as uncomfortable. A larger subjective distance from walls and
objects increased comfort and decreased sickness both in quantitative and
qualitative data. Finally, the SSQ subscales and total weighted scores showed
differences by age group and by gender.","['Markku Suomalainen', 'Katherine J. Mimnaugh', 'Israel Becerra', 'Eliezer Lozano', 'Rafael Murrieta-Cid', 'Steven M. LaValle']",2021-09-09T11:30:17Z,http://arxiv.org/abs/2109.04177v1,"['cs.HC', 'cs.MM', 'cs.RO']"
Extended Reality for Mental Health Evaluation -A Scoping Review,"Mental health disorders are the leading cause of health-related problems
globally. It is projected that mental health disorders will be the leading
cause of morbidity among adults as the incidence rates of anxiety and
depression grows globally. Recently, extended reality (XR), a general term
covering virtual reality (VR), augmented reality (AR) and mixed reality (MR),
is paving a new way to deliver mental health care. In this paper, we conduct a
scoping review on the development and application of XR in the area of mental
disorders. We performed a scoping database search to identify the relevant
studies indexed in Google Scholar, PubMed, and the ACM Digital Library. A
search period between August 2016 and December 2023 was defined to select
articles related to the usage of VR, AR, and MR in a mental health context. We
identified a total of 85 studies from 27 countries across the globe. By
performing data analysis, we found that most of the studies focused on
developed countries such as the US (16.47%) and Germany (12.94%). None of the
studies were for African countries. The majority of the articles reported that
XR techniques led to a significant reduction in symptoms of anxiety or
depression. More studies were published in the year 2021, i.e., 31.76% (n =
31). This could indicate that mental disorder intervention received a higher
attention when COVID-19 emerged. Most studies (n = 65) focused on a population
between 18 and 65 years old, only a few studies focused on teenagers (n = 2).
Also, more studies were done experimentally (n = 67, 78.82%) rather than by
analytical and modeling approaches (n = 8, 9.41%). This shows that there is a
rapid development of XR technology for mental health care. Furthermore, these
studies showed that XR technology can effectively be used for evaluating mental
disorders in similar or better way as the conventional approaches.","['Omisore Olatunji', 'Ifeanyi Odenigbo', 'Joseph Orji', 'Amelia Beltran', 'Nilufar Baghaei', 'Meier Sandra', 'Rita Orji']",2022-04-04T09:46:30Z,http://arxiv.org/abs/2204.01348v2,"['cs.HC', 'cs.CV']"
Updating Industrial Robots for Emerging Technologies,"Industrial arms need to evolve beyond their standard shape to embrace new and
emerging technologies. In this paper, we shall first perform an analysis of
four popular but different modern industrial robot arms. By seeing the common
trends we will try to extrapolate and expand these trends for the future. Here,
particular focus will be on interaction based on augmented reality (AR) through
head-mounted displays (HMD), but also through smartphones. Long-term
human-robot interaction and personalization of said interaction will also be
considered. The use of AR in human-robot interaction has proven to enhance
communication and information exchange. A basic addition to industrial arm
design would be the integration of QR markers on the robot, both for accessing
information and adding tracking capabilities to more easily display AR
overlays. In a recent example of information access, Mercedes Benz added QR
markers on their cars to help rescue workers estimate the best places to cut
and evacuate people after car crashes. One has also to deal with safety in an
environment that will be more and more about collaboration. The QR markers can
therefore be combined with RF-based ranging modules, developed in the
EU-project SafeLog, that can be used both for safety as well as for tracking of
human positions while in close proximity interactions with the industrial arms.
The industrial arms of the future should also be intuitive to program and
interact with. This would be achieved through AR and head mounted displays as
well as the already mentioned RF-based person tracking. Finally, a more
personalized interaction between the robots and humans can be achieved through
life-long learning AI and disembodied, personalized agents. We propose a design
that not only exists in the physical world, but also partly in the digital
world of mixed reality.","['David Puljiz', 'Björn Hein']",2022-04-07T16:08:02Z,http://arxiv.org/abs/2204.03538v2,['cs.RO']
"Convergence and Disruption in Digital Society -- Money, Secure
  Communication, Digital Objects and Generative AI in Spatial Mixed Reality","In the digital society's evolving landscape, open-source tooling and
generative AI are pivotal in transforming global collaboration. These
technologies promise to dismantle traditional barriers of accessibility,
language, and governance, fostering an inclusive digital ecosystem. However,
the journey towards a fully integrated digital society faces significant
challenges, including trust, accessibility, and sustainable development.
Emerging technologies like global ledgers and blockchain propose novel methods
for transferring digital goods and personal data across diverse digital spaces.
This development, coupled with augmented intelligence tools, aims to create
decentralized and federated environments where creativity and collaboration can
flourish. An ""open metaverse"" concept is gaining traction, promoting an
alternative to restrictive proprietary platforms and emphasizing user
empowerment and equity. Despite the opportunities, governance and ethical
considerations remain paramount. The digital society must navigate the fine
balance between innovation and the potential risks associated with new
technologies. The drive for an inclusive, innovative, and secure digital
society necessitates a commitment to open-source principles and ethical AI
application. It also involves overcoming cultural, legislative, and technical
barriers that impede global collaboration. The future digital society envisions
a collaborative, inclusive, and innovative global community. By focusing on
augmented intelligence and supported creativity, it aims to unlock new
possibilities for economic empowerment, cultural exchange, and technological
advancement. This vision is not without its challenges, but with continued
commitment to ethical, open, and inclusive development, a more connected and
empowered global community is within reach.","[""John Joseph O'Hare"", 'Allen Fairchild', 'Umran Ali']",2022-07-19T15:33:24Z,http://arxiv.org/abs/2207.09460v11,['cs.CR']
"Sampling, Communication, and Prediction Co-Design for Synchronizing the
  Real-World Device and Digital Model in Metaverse","The metaverse has the potential to revolutionize the next generation of the
Internet by supporting highly interactive services with the help of Mixed
Reality (MR) technologies; still, to provide a satisfactory experience for
users, the synchronization between the physical world and its digital models is
crucial. This work proposes a sampling, communication and prediction co-design
framework to minimize the communication load subject to a constraint on
tracking the Mean Squared Error (MSE) between a real-world device and its
digital model in the metaverse. To optimize the sampling rate and the
prediction horizon, we exploit expert knowledge and develop a constrained Deep
Reinforcement Learning (DRL) algorithm, named Knowledge-assisted Constrained
Twin-Delayed Deep Deterministic (KC-TD3) policy gradient algorithm. We validate
our framework on a prototype composed of a real-world robotic arm and its
digital model. Compared with existing approaches: (1) When the tracking error
constraint is stringent (MSE=0.002 degrees), our policy degenerates into the
policy in the sampling-communication co-design framework. (2) When the tracking
error constraint is mild (MSE=0.007 degrees), our policy degenerates into the
policy in the prediction-communication co-design framework. (3) Our framework
achieves a better trade-off between the average MSE and the average
communication load compared with a communication system without sampling and
prediction. For example, the average communication load can be reduced up to
87% when the track error constraint is 0.002 degrees. (4) Our policy
outperforms the benchmark with the static sampling rate and prediction horizon
optimized by exhaustive search, in terms of the tail probability of the
tracking error. Furthermore, with the assistance of expert knowledge, the
proposed algorithm KC-TD3 achieves better convergence time, stability, and
final policy performance.","['Zhen Meng', 'Changyang She', 'Guodong Zhao', 'Daniele De Martini']",2022-07-31T20:17:31Z,http://arxiv.org/abs/2208.04233v1,"['cs.RO', 'cs.AI', 'cs.HC', 'cs.LG']"
"AI and 6G into the Metaverse: Fundamentals, Challenges and Future
  Research Trends","Since Facebook was renamed Meta, a lot of attention, debate, and exploration
have intensified about what the Metaverse is, how it works, and the possible
ways to exploit it. It is anticipated that Metaverse will be a continuum of
rapidly emerging technologies, usecases, capabilities, and experiences that
will make it up for the next evolution of the Internet. Several researchers
have already surveyed the literature on artificial intelligence (AI) and
wireless communications in realizing the Metaverse. However, due to the rapid
emergence and continuous evolution of technologies, there is a need for a
comprehensive and in-depth survey of the role of AI, 6G, and the nexus of both
in realizing the immersive experiences of Metaverse. Therefore, in this survey,
we first introduce the background and ongoing progress in augmented reality
(AR), virtual reality (VR), mixed reality (MR) and spatial computing, followed
by the technical aspects of AI and 6G. Then, we survey the role of AI in the
Metaverse by reviewing the state-of-the-art in deep learning, computer vision,
and Edge AI to extract the requirements of 6G in Metaverse. Next, we
investigate the promising services of B5G/6G towards Metaverse, followed by
identifying the role of AI in 6G networks and 6G networks for AI in support of
Metaverse applications, and the need for sustainability in Metaverse. Finally,
we enlist the existing and potential applications, usecases, and projects to
highlight the importance of progress in the Metaverse. Moreover, in order to
provide potential research directions to researchers, we underline the
challenges, research gaps, and lessons learned identified from the literature
review of the aforementioned technologies.","['Muhammad Zawish', 'Fayaz Ali Dharejo', 'Sunder Ali Khowaja', 'Kapal Dev', 'Steven Davy', 'Nawab Muhammad Faseeh Qureshi', 'Paolo Bellavista']",2022-08-23T12:48:53Z,http://arxiv.org/abs/2208.10921v2,"['cs.AI', 'cs.HC', 'cs.NI', 'cs.SI']"
"Apple Vision Pro for Healthcare: ""The Ultimate Display""? -- Entering the
  Wonderland of Precision Medicine","At the Worldwide Developers Conference (WWDC) in June 2023, Apple introduced
the Vision Pro. The Vision Pro is a Mixed Reality (MR) headset, more
specifically it is a Virtual Reality (VR) device with an additional Video
See-Through (VST) capability. The VST capability turns the Vision Pro also into
an Augmented Reality (AR) device. The AR feature is enabled by streaming the
real world via cameras to the (VR) screens in front of the user's eyes. This is
of course not unique and similar to other devices, like the Varjo XR-3.
Nevertheless, the Vision Pro has some interesting features, like an inside-out
screen that can show the headset wearers' eyes to ""outsiders"" or a button on
the top, called ""Digital Crown"", that allows you to seamlessly blend digital
content with your physical space by turning it. In addition, it is untethered,
except for the cable to the battery, which makes the headset more agile,
compared to the Varjo XR-3. This could actually come closer to the ""Ultimate
Display"", which Ivan Sutherland had already sketched in 1965. Not available to
the public yet, like the Ultimate Display, we want to take a look into the
crystal ball in this perspective to see if it can overcome some clinical
challenges that - especially - AR still faces in the medical domain, but also
go beyond and discuss if the Vision Pro could support clinicians in essential
tasks to spend more time with their patients.","['Jan Egger', 'Christina Gsaxner', 'Xiaojun Chen', 'Jiang Bian', 'Jens Kleesiek', 'Behrus Puladi']",2023-08-08T15:01:51Z,http://arxiv.org/abs/2308.04313v4,"['cs.AI', 'cs.GR', 'cs.HC']"
"Modeling novel physics in virtual reality labs: An affective analysis of
  student learning","We report on a study of the effects of laboratory activities that model
fictitious laws of physics in a virtual reality environment on (1) students'
epistemology about the role of experimental physics in class and in the world;
(2) students' self-efficacy; and (3) the quality of student engagement with the
lab activities. We create opportunities for students to practice physics as a
means of creating and validating new knowledge by simulating real and
fictitious physics in virtual reality (VR). This approach seeks to steer
students away from a confirmation mindset in labs by eliminating any form of
prior or outside models to confirm. We refer to the activities using this
approach as Novel Observations in Mixed Reality (NOMR) labs. We examined NOMR's
effects in 100-level and 200-level undergraduate courses. Using pre-post
measurements we find that after NOMR labs, students in both populations were
more expertlike in their epistemology about experimental physics and held
stronger self-efficacy about their abilities to do the kinds of things
experimental physicists do. Through the lens of the psychological theory of
flow, we found that students engage as productively with NOMR labs as with
traditional hands-on labs. This engagement persisted after the novelty of VR in
the classroom wore off, suggesting that these effects are due to the
pedagogical design rather than the medium of the intervention. We conclude that
these NOMR labs offer an approach to physics laboratory instruction that
centers the development of students' understanding of and comfort with the
authentic practice of science.","['Jared P. Canright', 'Suzanne White Brahmia']",2023-10-12T00:27:24Z,http://arxiv.org/abs/2310.07952v1,['physics.ed-ph']
"ALSTER: A Local Spatio-Temporal Expert for Online 3D Semantic
  Reconstruction","We propose an online 3D semantic segmentation method that incrementally
reconstructs a 3D semantic map from a stream of RGB-D frames. Unlike offline
methods, ours is directly applicable to scenarios with real-time constraints,
such as robotics or mixed reality. To overcome the inherent challenges of
online methods, we make two main contributions. First, to effectively extract
information from the input RGB-D video stream, we jointly estimate geometry and
semantic labels per frame in 3D. A key focus of our approach is to reason about
semantic entities both in the 2D input and the local 3D domain to leverage
differences in spatial context and network architectures. Our method predicts
2D features using an off-the-shelf segmentation network. The extracted 2D
features are refined by a lightweight 3D network to enable reasoning about the
local 3D structure. Second, to efficiently deal with an infinite stream of
input RGB-D frames, a subsequent network serves as a temporal expert predicting
the incremental scene updates by leveraging 2D, 3D, and past information in a
learned manner. These updates are then integrated into a global scene
representation. Using these main contributions, our method can enable scenarios
with real-time constraints and can scale to arbitrary scene sizes by processing
and updating the scene only in a local region defined by the new measurement.
Our experiments demonstrate improved results compared to existing online
methods that purely operate in local regions and show that complementary
sources of information can boost the performance. We provide a thorough
ablation study on the benefits of different architectural as well as
algorithmic design decisions. Our method yields competitive results on the
popular ScanNet benchmark and SceneNN dataset.","['Silvan Weder', 'Francis Engelmann', 'Johannes L. Schönberger', 'Akihito Seki', 'Marc Pollefeys', 'Martin R. Oswald']",2023-11-29T20:30:18Z,http://arxiv.org/abs/2311.18068v2,['cs.CV']
"Leveraging Artificial Intelligence to Promote Awareness in Augmented
  Reality Systems","Recent developments in artificial intelligence (AI) have permeated through an
array of different immersive environments, including virtual, augmented, and
mixed realities. AI brings a wealth of potential that centers on its ability to
critically analyze environments, identify relevant artifacts to a goal or
action, and then autonomously execute decision-making strategies to optimize
the reward-to-risk ratio. However, the inherent benefits of AI are not without
disadvantages as the autonomy and communication methodology can interfere with
the human's awareness of their environment. More specifically in the case of
autonomy, the relevant human-computer interaction literature cites that high
autonomy results in an ""out-of-the-loop"" experience for the human such that
they are not aware of critical artifacts or situational changes that require
their attention. At the same time, low autonomy of an AI system can limit the
human's own autonomy with repeated requests to approve its decisions. In these
circumstances, humans enter into supervisor roles, which tend to increase their
workload and, therefore, decrease their awareness in a multitude of ways. In
this position statement, we call for the development of human-centered AI in
immersive environments to sustain and promote awareness. It is our position
then that we believe with the inherent risk presented in both AI and AR/VR
systems, we need to examine the interaction between them when we integrate the
two to create a new system for any unforeseen risks, and that it is crucial to
do so because of its practical application in many high-risk environments.","['Wangfan Li', 'Rohit Mallick', 'Carlos Toxtli-Hernandez', 'Christopher Flathmann', 'Nathan J. McNeese']",2024-04-23T17:47:51Z,http://arxiv.org/abs/2405.05916v1,['cs.HC']
"MedShapeNet -- A Large-Scale Dataset of 3D Medical Shapes for Computer
  Vision","Prior to the deep learning era, shape was commonly used to describe the
objects. Nowadays, state-of-the-art (SOTA) algorithms in medical imaging are
predominantly diverging from computer vision, where voxel grids, meshes, point
clouds, and implicit surface models are used. This is seen from numerous
shape-related publications in premier vision conferences as well as the growing
popularity of ShapeNet (about 51,300 models) and Princeton ModelNet (127,915
models). For the medical domain, we present a large collection of anatomical
shapes (e.g., bones, organs, vessels) and 3D models of surgical instrument,
called MedShapeNet, created to facilitate the translation of data-driven vision
algorithms to medical applications and to adapt SOTA vision algorithms to
medical problems. As a unique feature, we directly model the majority of shapes
on the imaging data of real patients. As of today, MedShapeNet includes 23
dataset with more than 100,000 shapes that are paired with annotations (ground
truth). Our data is freely accessible via a web interface and a Python
application programming interface (API) and can be used for discriminative,
reconstructive, and variational benchmarks as well as various applications in
virtual, augmented, or mixed reality, and 3D printing. Exemplary, we present
use cases in the fields of classification of brain tumors, facial and skull
reconstructions, multi-class anatomy completion, education, and 3D printing. In
future, we will extend the data and improve the interfaces. The project pages
are: https://medshapenet.ikim.nrw/ and
https://github.com/Jianningli/medshapenet-feedback","['Jianning Li', 'Zongwei Zhou', 'Jiancheng Yang', 'Antonio Pepe', 'Christina Gsaxner', 'Gijs Luijten', 'Chongyu Qu', 'Tiezheng Zhang', 'Xiaoxi Chen', 'Wenxuan Li', 'Marek Wodzinski', 'Paul Friedrich', 'Kangxian Xie', 'Yuan Jin', 'Narmada Ambigapathy', 'Enrico Nasca', 'Naida Solak', 'Gian Marco Melito', 'Viet Duc Vu', 'Afaque R. Memon', 'Christopher Schlachta', 'Sandrine De Ribaupierre', 'Rajnikant Patel', 'Roy Eagleson', 'Xiaojun Chen', 'Heinrich Mächler', 'Jan Stefan Kirschke', 'Ezequiel de la Rosa', 'Patrick Ferdinand Christ', 'Hongwei Bran Li', 'David G. Ellis', 'Michele R. Aizenberg', 'Sergios Gatidis', 'Thomas Küstner', 'Nadya Shusharina', 'Nicholas Heller', 'Vincent Andrearczyk', 'Adrien Depeursinge', 'Mathieu Hatt', 'Anjany Sekuboyina', 'Maximilian Löffler', 'Hans Liebl', 'Reuben Dorent', 'Tom Vercauteren', 'Jonathan Shapey', 'Aaron Kujawa', 'Stefan Cornelissen', 'Patrick Langenhuizen', 'Achraf Ben-Hamadou', 'Ahmed Rekik', 'Sergi Pujades', 'Edmond Boyer', 'Federico Bolelli', 'Costantino Grana', 'Luca Lumetti', 'Hamidreza Salehi', 'Jun Ma', 'Yao Zhang', 'Ramtin Gharleghi', 'Susann Beier', 'Arcot Sowmya', 'Eduardo A. Garza-Villarreal', 'Thania Balducci', 'Diego Angeles-Valdez', 'Roberto Souza', 'Leticia Rittner', 'Richard Frayne', 'Yuanfeng Ji', 'Vincenzo Ferrari', 'Soumick Chatterjee', 'Florian Dubost', 'Stefanie Schreiber', 'Hendrik Mattern', 'Oliver Speck', 'Daniel Haehn', 'Christoph John', 'Andreas Nürnberger', 'João Pedrosa', 'Carlos Ferreira', 'Guilherme Aresta', 'António Cunha', 'Aurélio Campilho', 'Yannick Suter', 'Jose Garcia', 'Alain Lalande', 'Vicky Vandenbossche', 'Aline Van Oevelen', 'Kate Duquesne', 'Hamza Mekhzoum', 'Jef Vandemeulebroucke', 'Emmanuel Audenaert', 'Claudia Krebs', 'Timo van Leeuwen', 'Evie Vereecke', 'Hauke Heidemeyer', 'Rainer Röhrig', 'Frank Hölzle', 'Vahid Badeli', 'Kathrin Krieger', 'Matthias Gunzer', 'Jianxu Chen', 'Timo van Meegdenburg', 'Amin Dada', 'Miriam Balzer', 'Jana Fragemann', 'Frederic Jonske', 'Moritz Rempe', 'Stanislav Malorodov', 'Fin H. Bahnsen', 'Constantin Seibold', 'Alexander Jaus', 'Zdravko Marinov', 'Paul F. Jaeger', 'Rainer Stiefelhagen', 'Ana Sofia Santos', 'Mariana Lindo', 'André Ferreira', 'Victor Alves', 'Michael Kamp', 'Amr Abourayya', 'Felix Nensa', 'Fabian Hörst', 'Alexander Brehmer', 'Lukas Heine', 'Yannik Hanusrichter', 'Martin Weßling', 'Marcel Dudda', 'Lars E. Podleska', 'Matthias A. Fink', 'Julius Keyl', 'Konstantinos Tserpes', 'Moon-Sung Kim', 'Shireen Elhabian', 'Hans Lamecker', 'Dženan Zukić', 'Beatriz Paniagua', 'Christian Wachinger', 'Martin Urschler', 'Luc Duong', 'Jakob Wasserthal', 'Peter F. Hoyer', 'Oliver Basu', 'Thomas Maal', 'Max J. H. Witjes', 'Gregor Schiele', 'Ti-chiun Chang', 'Seyed-Ahmad Ahmadi', 'Ping Luo', 'Bjoern Menze', 'Mauricio Reyes', 'Thomas M. Deserno', 'Christos Davatzikos', 'Behrus Puladi', 'Pascal Fua', 'Alan L. Yuille', 'Jens Kleesiek', 'Jan Egger']",2023-08-30T16:52:20Z,http://arxiv.org/abs/2308.16139v5,"['cs.CV', 'cs.DB', 'cs.LG', '68T01']"
